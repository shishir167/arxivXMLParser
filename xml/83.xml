<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:54:43Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|82001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01443</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01443</id><created>2015-08-06</created><authors><author><keyname>Bloom</keyname><forenames>Kenneth</forenames></author><author><keyname>Boccali</keyname><forenames>Tommaso</forenames></author><author><keyname>Bockelman</keyname><forenames>Brian</forenames></author><author><keyname>Bradley</keyname><forenames>Daniel</forenames></author><author><keyname>Dasu</keyname><forenames>Sridhara</forenames></author><author><keyname>Dost</keyname><forenames>Jeff</forenames></author><author><keyname>Fanzago</keyname><forenames>Federica</forenames></author><author><keyname>Sfiligoi</keyname><forenames>Igor</forenames></author><author><keyname>Tadel</keyname><forenames>Alja Mrak</forenames></author><author><keyname>Tadel</keyname><forenames>Matevz</forenames></author><author><keyname>Vuosalo</keyname><forenames>Carl</forenames></author><author><keyname>W&#xfc;rthwein</keyname><forenames>Frank</forenames></author><author><keyname>Yagil</keyname><forenames>Avi</forenames></author><author><keyname>Zvada</keyname><forenames>Marian</forenames></author></authors><title>Any Data, Any Time, Anywhere: Global Data Access for Science</title><categories>physics.comp-ph cs.DC hep-ex physics.ins-det</categories><comments>9 pages, 6 figures, submitted to 2nd IEEE/ACM International Symposium
  on Big Data Computing (BDC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data access is key to science driven by distributed high-throughput computing
(DHTC), an essential technology for many major research projects such as High
Energy Physics (HEP) experiments. However, achieving efficient data access
becomes quite difficult when many independent storage sites are involved
because users are burdened with learning the intricacies of accessing each
system and keeping careful track of data location. We present an alternate
approach: the Any Data, Any Time, Anywhere infrastructure. Combining several
existing software products, AAA presents a global, unified view of storage
systems - a &quot;data federation,&quot; a global filesystem for software delivery, and a
workflow management system. We present how one HEP experiment, the Compact Muon
Solenoid (CMS), is utilizing the AAA infrastructure and some simple performance
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01447</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01447</id><created>2015-08-06</created><authors><author><keyname>AlAgha</keyname><forenames>Iyad</forenames></author></authors><title>Using Linguistic Analysis to Translate Arabic Natural Language Queries
  to SPARQL</title><categories>cs.CL cs.AI cs.DB</categories><comments>Journal Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logic-based machine-understandable framework of the Semantic Web often
challenges naive users when they try to query ontology-based knowledge bases.
Existing research efforts have approached this problem by introducing Natural
Language (NL) interfaces to ontologies. These NL interfaces have the ability to
construct SPARQL queries based on NL user queries. However, most efforts were
restricted to queries expressed in English, and they often benefited from the
advancement of English NLP tools. However, little research has been done to
support querying the Arabic content on the Semantic Web by using NL queries.
This paper presents a domain-independent approach to translate Arabic NL
queries to SPARQL by leveraging linguistic analysis. Based on a special
consideration on Noun Phrases (NPs), our approach uses a language parser to
extract NPs and the relations from Arabic parse trees and match them to the
underlying ontology. It then utilizes knowledge in the ontology to group NPs
into triple-based representations. A SPARQL query is finally generated by
extracting targets and modifiers, and interpreting them into SPARQL. The
interpretation of advanced semantic features including negation, conjunctive
and disjunctive modifiers is also supported. The approach was evaluated by
using two datasets consisting of OWL test data and queries, and the obtained
results have confirmed its feasibility to translate Arabic NL queries to
SPARQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01448</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01448</id><created>2015-08-06</created><authors><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>An O(1)-Approximation for Minimum Spanning Tree Interdiction</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interdiction problems are a natural way to study the sensitivity of a
network optimization problem with respect to the removal of a limited set of
edges or vertices. One of the oldest and best-studied interdiction problems is
minimum spanning tree (MST) interdiction. Here, an undirected multigraph with
nonnegative edge weights and positive interdiction costs on its edges is given,
together with a positive budget B. The goal is to find a subset of edges R,
whose total interdiction cost does not exceed B, such that removing R leads to
a graph where the weight of an MST is as large as possible. Frederickson and
Solis-Oba (SODA 1996) presented an O(log m)-approximation for MST interdiction,
where m is the number of edges. Since then, no further progress has been made
regarding approximations, and the question whether MST interdiction admits an
O(1)-approximation remained open.
  We answer this question in the affirmative, by presenting a 14-approximation
that overcomes two main hurdles that hindered further progress so far.
Moreover, based on a well-known 2-approximation for the metric traveling
salesman problem (TSP), we show that our O(1)-approximation for MST
interdiction implies an O(1)-approximation for a natural interdiction version
of metric TSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01454</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01454</id><created>2015-08-06</created><authors><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Dai</keyname><forenames>Xiangming</forenames></author><author><keyname>Wang</keyname><forenames>Jason Min</forenames></author><author><keyname>Bensaou</keyname><forenames>Brahim</forenames></author></authors><title>Energy Efficient Medium Access with Interference Mitigation in LTE
  Femtocell Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapidly increasing number of deployed LTE femtocell base stations
(FBS), energy consumption of femtocell networks has become a serious
environmental issue. Therefore, energy-efficient protocols are needed to
balance the trade-off between energy saving and bandwidth utilization. The key
component of the desired protocol to prevent both energy and bandwidth waste is
interference mitigation, which, nevertheless, most previous work has failed to
properly consider. To this end, in this paper, we manipulate user equipment
(UE) association and OFDMA scheduling with a combination of interference
mitigation. Recognizing the NP-hardness of the problem, we propose two
distributed algorithms with guaranteed convergence. Extensive simulations show
that our algorithms outperform the alternative algorithms in multiple metrics
such as utility, power consumption, and convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01459</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01459</id><created>2015-08-06</created><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Distributed Resource Allocation for Relay-Aided Device-to-Device
  Communication Under Channel Uncertainties: A Stable Matching Approach</title><categories>cs.NI</categories><comments>Accepted for publication, IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless device-to-device (D2D) communication underlaying cellular network is
a promising concept to improve user experience and resource utilization. Unlike
traditional D2D communication where two mobile devices in the proximity
establish a direct local link bypassing the base station, in this work we focus
on relay-aided D2D communication. Relay-aided transmission could enhance the
performance of D2D communication when D2D user equipments (UEs) are far apart
from each other and/or the quality of D2D link is not good enough for direct
communication. Considering the uncertainties in wireless links, we model and
analyze the performance of a relay-aided D2D communication network, where the
relay nodes serve both the cellular and D2D users. In particular, we formulate
the radio resource allocation problem in a two-hop network to guarantee the
data rate of the UEs while protecting other receiving nodes from interference.
Utilizing time sharing strategy, we provide a centralized solution under
bounded channel uncertainty. With a view to reducing the computational burden
at relay nodes, we propose a distributed solution approach using stable
matching to allocate radio resources in an efficient and computationally
inexpensive way. Numerical results show that the performance of the proposed
method is close to the centralized optimal solution and there is a distance
margin beyond which relaying of D2D traffic improves network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01464</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01464</id><created>2015-08-06</created><updated>2015-11-26</updated><authors><author><keyname>Samorodnitsky</keyname><forenames>Alex</forenames></author></authors><title>On the entropy of a noisy function</title><categories>cs.IT math.CO math.IT</categories><comments>Added a strengthening of Mrs. Gerber's lemma</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $0 &lt; \epsilon &lt; 1/2$ be a noise parameter, and let $T_{\epsilon}$ be the
noise operator acting on functions on the boolean cube $\{0,1\}^n$. Let $f$ be
a nonnegative function on $\{0,1\}^n$. We upper bound the entropy of
$T_{\epsilon} f$ by the average entropy of conditional expectations of $f$,
given sets of roughly $(1-2\epsilon)^2 \cdot n$ variables. In
information-theoretic terms, we prove the following strengthening of &quot;Mrs.
Gerber's lemma&quot;: Let $X$ be a random binary vector of length $n$, and let $Z$
be a noise vector, corresponding to a binary symmetric channel with crossover
probability $\epsilon$. Then, setting $v = (1-2\epsilon)^2 \cdot n$, we have
(up to lower-order terms): $$ H\Big(X \oplus Z\Big) \ge n \cdot H\left(\epsilon
~+~ (1-2\epsilon) \cdot H^{-1}\left(\frac{{\mathbb E}_{|B| = v}
H\Big(\{X_i\}_{i\in B}\Big)}{v}\right)\right) $$ As an application, we show
that for a boolean function $f$, which is close to a characteristic function
$g$ of a subcube of dimension $n-1$, the entropy of $T_{\epsilon} f$ is at most
that of $T_{\epsilon} g$. This, combined with a recent result of Ordentlich,
Shayevitz, and Weinstein shows that the &quot;Most informative boolean function&quot;
conjecture of Courtade and Kumar holds for balanced boolean functions and high
noise $\epsilon \ge 1/2 - \delta$, for some absolute constant $\delta &gt; 0$.
Namely, if $X$ is uniformly distributed in $\{0,1\}^n$ and $Y$ is obtained by
flipping each coordinate of $X$ independently with probability $\epsilon$,
then, provided $\epsilon \ge 1/2 - \delta$, for any balanced boolean function
$f$ holds $I\Big(f(X);Y\Big) \le 1 - H(\epsilon)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01476</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01476</id><created>2015-08-06</created><authors><author><keyname>Zhan</keyname><forenames>Qiang</forenames></author><author><keyname>Wang</keyname><forenames>Chunhong</forenames></author></authors><title>Hyponymy extraction of domain ontology concept based on ccrfs and
  hierarchy clustering</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept hierarchy is the backbone of ontology, and the concept hierarchy
acquisition has been a hot topic in the field of ontology learning. this paper
proposes a hyponymy extraction method of domain ontology concept based on
cascaded conditional random field(CCRFs) and hierarchy clustering. It takes
free text as extracting object, adopts CCRFs identifying the domain concepts.
First the low layer of CCRFs is used to identify simple domain concept, then
the results are sent to the high layer, in which the nesting concepts are
recognized. Next we adopt hierarchy clustering to identify the hyponymy
relation between domain ontology concepts. The experimental results demonstrate
the proposed method is efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01485</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01485</id><created>2015-08-06</created><authors><author><keyname>Alotaibi</keyname><forenames>Reem Ahmed</forenames></author><author><keyname>Elrefaei</keyname><forenames>Lamiaa A.</forenames></author></authors><title>Arabic Text Watermarking: A Review</title><categories>cs.MM</categories><comments>16 pages, 4 tables and 19 figures</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA) Vol. 6, No. 4, July 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The using of the internet with its technologies and applications have been
increased rapidly. So, protecting the text from illegal use is too needed .
Text watermarking is used for this purpose. Arabic text has many
characteristics such existing of diacritics , kashida (extension character) and
points above or under its letters .Each of Arabic letters can take different
shapes with different Unicode. These characteristics are utilized in the
watermarking process. In this paper, several methods are discussed in the area
of Arabic text watermarking with its advantages and disadvantages .Comparison
of these methods is done in term of capacity, robustness and Imperceptibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01504</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01504</id><created>2015-08-06</created><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijaya</forenames></author></authors><title>Resource Oblivious Sorting on Multicores</title><categories>cs.DS cs.DC</categories><comments>A preliminary version of this paper appeared in Proc. ICALP, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic sorting algorithm, SPMS (Sample, Partition, and
Merge Sort), that interleaves the partitioning of a sample sort with merging.
Sequentially, it sorts $n$ elements in $O(n \log n)$ time cache-obliviously
with an optimal number of cache misses. The parallel complexity (or critical
path length) of the algorithm is $O(\log n \cdot \log\log n)$, which improves
on previous bounds for optimal cache oblivious sorting. The algorithm also has
low false sharing costs. When scheduled by a work-stealing scheduler in a
multicore computing environment with a global shared memory and $p$ cores, each
having a cache of size $M$ organized in blocks of size $B$, the costs of the
additional cache misses and false sharing misses due to this parallel execution
are bounded by the cost of $O(S\cdot M/B)$ and $O(S \cdot B)$ cache misses
respectively, where $S$ is the number of steals performed during the execution.
Finally, SPMS is resource oblivious in Athat the dependence on machine
parameters appear only in the analysis of its performance, and not within the
algorithm itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01521</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01521</id><created>2015-08-06</created><updated>2015-10-04</updated><authors><author><keyname>Al-Shaikhli</keyname><forenames>Saif Dawood Salman</forenames></author><author><keyname>Yang</keyname><forenames>Michael Ying</forenames></author><author><keyname>Rosenhahn</keyname><forenames>Bodo</forenames></author></authors><title>Automatic 3D Liver Segmentation Using Sparse Representation of Global
  and Local Image Information via Level Set Formulation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel framework for automated liver segmentation via a level
set formulation is presented. A sparse representation of both global
(region-based) and local (voxel-wise) image information is embedded in a level
set formulation to innovate a new cost function. Two dictionaries are build: A
region-based feature dictionary and a voxel-wise dictionary. These dictionaries
are learned, using the K-SVD method, from a public database of liver
segmentation challenge (MICCAI-SLiver07). The learned dictionaries provide
prior knowledge to the level set formulation. For the quantitative evaluation,
the proposed method is evaluated using the testing data of MICCAI-SLiver07
database. The results are evaluated using different metric scores computed by
the challenge organizers. The experimental results demonstrate the superiority
of the proposed framework by achieving the highest segmentation accuracy
(79.6\%) in comparison to the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01534</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01534</id><created>2015-08-06</created><authors><author><keyname>Shi</keyname><forenames>Bibo</forenames></author><author><keyname>Liu</keyname><forenames>Jundong</forenames></author></authors><title>Nonlinear Metric Learning for kNN and SVMs through Geometric
  Transformations</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, research efforts to extend linear metric learning models to
handle nonlinear structures have attracted great interests. In this paper, we
propose a novel nonlinear solution through the utilization of deformable
geometric models to learn spatially varying metrics, and apply the strategy to
boost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS)
are chosen as the geometric model due to their remarkable versatility and
representation power in accounting for high-order deformations. By transforming
the input space through TPS, we can pull same-class neighbors closer while
pushing different-class points farther away in kNN, as well as make the input
data points more linearly separable in SVMs. Improvements in the performance of
kNN classification are demonstrated through experiments on synthetic and real
world datasets, with comparisons made with several state-of-the-art metric
learning solutions. Our SVM-based models also achieve significant improvements
over traditional linear and kernel SVMs with the same datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01545</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01545</id><created>2015-08-06</created><authors><author><keyname>Trautman</keyname><forenames>Pete</forenames></author></authors><title>A Unified Approach to 3 Basic Challenges in Shared Autonomy</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss some of the challenges facing shared autonomy. In particular, we
explore 1) shared autonomy over unreliable networks, 2) how we can model
individual human operators (in contrast to the average of a human operator),
and 3) how our approach naturally models and integrates sliding autonomy into
the joint human-machine system. We include a Background Section for
completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01549</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01549</id><created>2015-08-06</created><authors><author><keyname>Kamath</keyname><forenames>Uday</forenames></author><author><keyname>Domeniconi</keyname><forenames>Carlotta</forenames></author><author><keyname>De Jong</keyname><forenames>Kenneth</forenames></author></authors><title>Theoretical and Empirical Analysis of a Parallel Boosting Algorithm</title><categories>cs.LG cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Many real-world problems involve massive amounts of data. Under these
circumstances learning algorithms often become prohibitively expensive, making
scalability a pressing issue to be addressed. A common approach is to perform
sampling to reduce the size of the dataset and enable efficient learning.
Alternatively, one customizes learning algorithms to achieve scalability. In
either case, the key challenge is to obtain algorithmic efficiency without
compromising the quality of the results. In this paper we discuss a
meta-learning algorithm (PSBML) which combines features of parallel algorithms
with concepts from ensemble and boosting methodologies to achieve the desired
scalability property. We present both theoretical and empirical analyses which
show that PSBML preserves a critical property of boosting, specifically,
convergence to a distribution centered around the margin. We then present
additional empirical analyses showing that this meta-level algorithm provides a
general and effective framework that can be used in combination with a variety
of learning classifiers. We perform extensive experiments to investigate the
tradeoff achieved between scalability and accuracy, and robustness to noise, on
both synthetic and real-world data. These empirical results corroborate our
theoretical analysis, and demonstrate the potential of PSBML in achieving
scalability without sacrificing accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01553</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01553</id><created>2015-08-06</created><updated>2015-12-01</updated><authors><author><keyname>Yang</keyname><forenames>Yaoqing</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author></authors><title>Graph Codes for Distributed Instant Message Collection in an Arbitrary
  Noisy Broadcast Network</title><categories>cs.IT math.IT</categories><comments>54 pages. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing the number of broadcasts for collecting
all sensor measurements at a sink node in a noisy broadcast sensor network.
Focusing first on arbitrary network topologies, we provide (i) fundamental
limits on the required number of broadcasts of data gathering, and (ii) a
general in-network computing strategy to achieve an upper bound within factor
$\log N$ of the fundamental limits, where $N$ is the number of agents in the
network. Next, focusing on two example networks, namely,
\textcolor{black}{arbitrary geometric networks and random
Erd$\ddot{o}$s-R$\acute{e}$nyi networks}, we provide improved in-network
computing schemes that are optimal in that they attain the fundamental limits,
i.e., the lower and upper bounds are tight \textcolor{black}{in order sense}.
Our main techniques are three distributed encoding techniques, called graph
codes, which are designed respectively for the above-mentioned three scenarios.
Our work thus extends and unifies previous works such as those of Gallager [1]
and Karamchandani~\emph{et. al.} [2] on number of broadcasts for distributed
function computation in special network topologies, while bringing in novel
techniques, e.g., from error-control coding and noisy circuits, for both upper
and lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01554</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01554</id><created>2015-08-06</created><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author><author><keyname>Subrahmanyam</keyname><forenames>K. V.</forenames></author></authors><title>On generating the ring of matrix semi-invariants</title><categories>cs.CC math.AC math.RA</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a field $\mathbb{F}$, let $R(n, m)$ be the ring of invariant polynomials
for the action of $\mathrm{SL}(n, \mathbb{F}) \times \mathrm{SL}(n,
\mathbb{F})$ on tuples of matrices -- $(A, C)\in\mathrm{SL}(n, \mathbb{F})
\times \mathrm{SL}(n, \mathbb{F})$ sends $(B_1, \dots, B_m)\in M(n,
\mathbb{F})^{\oplus m}$ to $(AB_1C^{-1}, \dots, AB_mC^{-1})$. In this paper we
call $R(n, m)$ the \emph{ring of matrix semi-invariants}. Let $\beta(R(n, m))$
be the smallest $D$ s.t. matrix semi-invariants of degree $\leq D$ generate
$R(n, m)$. Guided by the Procesi-Razmyslov-Formanek approach of proving a
strong degree bound for generating matrix invariants, we exhibit several
interesting structural results for the ring of matrix semi-invariants $R(n, m)$
over fields of characteristic $0$. Using these results, we prove that
$\beta(R(n, m))=\Omega(n^{3/2})$, and $\beta(R(2, m))\leq 4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01571</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01571</id><created>2015-08-06</created><authors><author><keyname>Corona</keyname><forenames>Humberto</forenames></author><author><keyname>O'Mahony</keyname><forenames>Michael P.</forenames></author></authors><title>A Mood-based Genre Classification of Television Content</title><categories>cs.IR cs.CL</categories><comments>in ACM Workshop on Recommendation Systems for Television and Online
  Video 2014 Foster City, California USA</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classification of television content helps users organise and navigate
through the large list of channels and programs now available. In this paper,
we address the problem of television content classification by exploiting text
information extracted from program transcriptions. We present an analysis which
adapts a model for sentiment that has been widely and successfully applied in
other fields such as music or blog posts. We use a real-world dataset obtained
from the Boxfish API to compare the performance of classifiers trained on a
number of different feature sets. Our experiments show that, over a large
collection of television content, program genres can be represented in a
three-dimensional space of valence, arousal and dominance, and that promising
classification results can be achieved using features based on this
representation. This finding supports the use of the proposed representation of
television content as a feature space for similarity computation and
recommendation generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01572</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01572</id><created>2015-08-06</created><updated>2015-08-31</updated><authors><author><keyname>Hayashi</keyname><forenames>Yukio</forenames></author></authors><title>Recoverable DTN Routing based on a Relay of Cyclic Message-Ferries on a
  MSQ Network</title><categories>cs.DC cs.NI cs.SI nlin.AO physics.soc-ph</categories><comments>6 pages, 12 figures, The 3rd Workshop on the FoCAS(Fundamentals of
  Collective Adaptive Systems) at The 9th IEEE International Conference on
  SASO(Self-Adaptive and Self-Organizing systems), Boston, USA, Sept.21, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interrelation between a topological design of network and efficient
algorithm on it is important for its applications to communication or
transportation systems. In this paper, we propose a design principle for a
reliable routing in a store-carry-forward manner based on autonomously moving
message-ferries on a special structure of fractal-like network, which consists
of a self-similar tiling of equilateral triangles. As a collective adaptive
mechanism, the routing is realized by a relay of cyclic message-ferries
corresponded to a concatenation of the triangle cycles and using some good
properties of the network structure. It is recoverable for local accidents in
the hierarchical network structure. Moreover, the design principle is
theoretically supported with a calculation method for the optimal service rates
of message-ferries derived from a tandem queue model for stochastic processes
on a chain of edges in the network. These results obtained from a combination
of complex network science and computer science will be useful for developing a
resilient network system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01575</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01575</id><created>2015-08-06</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Hu</keyname><forenames>Chuanyan</forenames></author><author><keyname>Wu</keyname><forenames>Qianhong</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Qin</keyname><forenames>Bo</forenames></author></authors><title>On the Security of Privacy-Preserving Vehicular Communication
  Authentication with Hierarchical Aggregation and Fast Response</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [3], the authors proposed a highly efficient secure and privacy-preserving
scheme for secure vehicular communications. The proposed scheme consists of
four protocols: system setup, protocol for STP and STK distribution, protocol
for common string synchronization, and protocol for vehicular communications.
Here we define the security models for the protocol for STP and STK
distribution, and the protocol for vehicular communications,respectively. We
then prove that these two protocols are secure in our models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01577</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01577</id><created>2015-08-06</created><authors><author><keyname>Vera</keyname><forenames>Javier</forenames></author><author><keyname>Urbina</keyname><forenames>Felipe</forenames></author><author><keyname>Goles</keyname><forenames>Eric</forenames></author></authors><title>Automata networks model for alignment and least effort on vocabulary
  formation</title><categories>cs.CL physics.soc-ph</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can artificial communities of agents develop language with scaling relations
close to the Zipf law? As a preliminary answer to this question, we propose an
Automata Networks model of the formation of a vocabulary on a population of
individuals, under two in principle opposite strategies: the alignment and the
least effort principle. Within the previous account to the emergence of
linguistic conventions (specially, the Naming Game), we focus on modeling
speaker and hearer efforts as actions over their vocabularies and we study the
impact of these actions on the formation of a shared language. The numerical
simulations are essentially based on an energy function, that measures the
amount of local agreement between the vocabularies. The results suggests that
on one dimensional lattices the best strategy to the formation of shared
languages is the one that minimizes the efforts of speakers on communicative
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01580</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01580</id><created>2015-08-06</created><updated>2015-10-08</updated><authors><author><keyname>Vera</keyname><forenames>Javier</forenames></author><author><keyname>Goles</keyname><forenames>Eric</forenames></author></authors><title>Automata networks for memory loss effects in the formation of linguistic
  conventions</title><categories>cs.CL physics.soc-ph</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work attempts to give new theoretical insights to the absence of
intermediate stages in the evolution of language. In particular, it is
developed an automata networks approach to a crucial question: how a population
of language users can reach agreement on a linguistic convention? To describe
the appearance of sharp transitions in the self-organization of language, it is
adopted an extremely simple model of (working) memory. At each time step,
language users simply loss part of their word-memories. Through computer
simulations of low-dimensional lattices, it appear sharp transitions at
critical values that depend on the size of the vicinities of the individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01585</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01585</id><created>2015-08-06</created><updated>2015-10-02</updated><authors><author><keyname>Feng</keyname><forenames>Minwei</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Glass</keyname><forenames>Michael R.</forenames></author><author><keyname>Wang</keyname><forenames>Lidan</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Applying Deep Learning to Answer Selection: A Study and An Open Task</title><categories>cs.CL cs.LG</categories><comments>To appear in the proceedings of ASRU 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We apply a general deep learning framework to address the non-factoid
question answering task. Our approach does not rely on any linguistic tools and
can be applied to different languages or domains. Various architectures are
presented and compared. We create and release a QA corpus and setup a new QA
task in the insurance domain. Experimental results demonstrate superior
performance compared to the baseline methods and various technologies give
further improvements. For this highly challenging task, the top-1 accuracy can
reach up to 65.3% on a test set, which indicates a great potential for
practical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01586</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01586</id><created>2015-08-06</created><authors><author><keyname>Kohli</keyname><forenames>Rajeev</forenames></author><author><keyname>Krishnamurti</keyname><forenames>Ramesh</forenames></author></authors><title>Lower Bound for the Unique Games Problem</title><categories>cs.CC</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a randomized algorithm for the unique games problem, using
independent multinomial probabilities to assign labels to the vertices of a
graph. The expected value of the solution obtained by the algorithm is
expressed as a function of the probabilities. Finding probabilities that
maximize this expected value is shown to be equivalent to obtaining an optimal
solution to the unique games problem. We attain an upper bound on the optimal
solution value by solving a semidefinite programming relaxation of the problem
in polynomial time. We use a different but related formulation to show that
this upper bound is no greater than {\pi}/2 times the value of the optimal
solution to the unique games problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01596</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01596</id><created>2015-08-06</created><authors><author><keyname>Rastogi</keyname><forenames>Pushpendre</forenames></author><author><keyname>Van Durme</keyname><forenames>Benjamin</forenames></author></authors><title>Sublinear Partition Estimation</title><categories>stat.ML cs.LG</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The output scores of a neural network classifier are converted to
probabilities via normalizing over the scores of all competing categories.
Computing this partition function, $Z$, is then linear in the number of
categories, which is problematic as real-world problem sets continue to grow in
categorical types, such as in visual object recognition or discriminative
language modeling. We propose three approaches for sublinear estimation of the
partition function, based on approximate nearest neighbor search and kernel
feature maps and compare the performance of the proposed approaches
empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01600</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01600</id><created>2015-08-07</created><updated>2015-11-25</updated><authors><author><keyname>Sterling</keyname><forenames>Jonathan</forenames></author></authors><title>Remark on the hypothetical judgment</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is the proper explanation of intuitionistic hypothetical judgment, and
thence propositional implication? The answer is unclear from the writings of
Brouwer and Heyting, who in their lifetimes propounded multiple (sometimes
conflicting) explanations of the hypothetical judgment. To my mind, the
determination of an acceptable explanation must take into account its adequacy
for the expression of the bar theorem and, more generally, the development of
an open-ended framework for transcendental arguments in mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01602</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01602</id><created>2015-08-07</created><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>A Stronger Soft-Covering Lemma and Applications</title><categories>cs.IT math.IT</categories><comments>IEEE CNS 2015, 2nd Workshop on Physical-layer Methods for Wireless
  Security, 4 pages</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wyner's soft-covering lemma is a valuable tool for achievability proofs of
information theoretic security, resolvability, channel synthesis, and source
coding. The result herein sharpens the claim of soft-covering by moving away
from an expected value analysis. Instead, a random codebook is shown to achieve
the soft-covering phenomenon with high probability. The probability of failure
is doubly-exponentially small in the block-length, enabling more powerful
applications through the union bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01617</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01617</id><created>2015-08-07</created><authors><author><keyname>Ma</keyname><forenames>Yuanye</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Distributed and Optimal Resource Allocation for Power Beacon-Assisted
  Wireless-Powered Communications</title><categories>cs.IT math.IT</categories><comments>Accepted to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate optimal resource allocation in a power
beacon-assisted wireless-powered communication network (PB-WPCN), which
consists of a set of hybrid access point (AP)-source pairs and a power beacon
(PB). Each source, which has no embedded power supply, first harvests energy
from its associated AP and/or the PB in the downlink (DL) and then uses the
harvested energy to transmit information to its AP in the uplink (UL). We
consider both cooperative and non-cooperative scenarios based on whether the PB
is cooperative with the APs or not. For the cooperative scenario, we formulate
a social welfare maximization problem to maximize the weighted sum-throughput
of all AP-source pairs, which is subsequently solved by a water-filling based
distributed algorithm. In the non-cooperative scenario, all the APs and the PB
are assumed to be rational and self-interested such that incentives from each
AP are needed for the PB to provide wireless charging service. We then
formulate an auction game and propose an auction based distributed algorithm by
considering the PB as the auctioneer and the APs as the bidders. Finally,
numerical results are performed to validate the convergence of both the
proposed algorithms and demonstrate the impacts of various system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01621</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01621</id><created>2015-08-07</created><authors><author><keyname>Behera</keyname><forenames>Adyasha</forenames></author><author><keyname>Panigrahi</keyname><forenames>Amrutanshu</forenames></author></authors><title>Determining the network throughput and flow rate using GSR And AAL2R</title><categories>cs.NI</categories><doi>10.5121/iju.2015.6302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-radio wireless mesh networks, one node is eligible to transmit
packets over multiple channels to different destination nodes simultaneously.
This feature of multi-radio wireless mesh network makes high throughput for the
network and increase the chance for multi path routing. This is because the
multiple channel availability for transmission decreases the probability of the
most elegant problem called as interference problem which is either of
interflow and intraflow type. For avoiding the problem like interference and
maintaining the constant network performance or increasing the performance the
WMN need to consider the packet aggregation and packet forwarding. Packet
aggregation is process of collecting several packets ready for transmission and
sending them to the intended recipient through the channel, while the packet
forwarding holds the hop-by-hop routing. But choosing the correct path among
different available multiple paths is most the important factor in the both
case for a routing algorithm. Hence the most challenging factor is to determine
a forwarding strategy which will provide the schedule for each node for
transmission within the channel. In this research work we have tried to
implement two forwarding strategies for the multi path multi radio WMN as the
approximate solution for the above said problem. We have implemented Global
State Routing (GSR) which will consider the packet forwarding concept and
Aggregation Aware Layer 2 Routing (AAL2R) which considers the both concept i.e.
both packet forwarding and packet aggregation. After the successful
implementation the network performance has been measured by means of simulation
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01623</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01623</id><created>2015-08-07</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz</forenames></author></authors><title>Requirements Engineering Aspects of a Geographically Distributed
  Architecture</title><categories>cs.SE</categories><comments>Preprint. Accepted to the 10th International Conference on Evaluation
  of Novel Approaches to Software Engineering (ENASE 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our ongoing work on requirements specification and analysis for
the geographically distributed software and systems. Developing software and
systems within/for different countries or states or even within/for different
organisations means that the requirements to them can differ in each particular
case. These aspects naturally impact on the software architecture and on the
development process as a whole. The challenge is to deal with this diversity in
a systematic way, avoiding contradictions and non-compliance. In this paper, we
present a formal framework for the analysis of the requirements diversity,
which comes from the differences in the regulations, laws and cultural aspects
for different countries or organisations. The framework also provides the
corresponding architectural view and the methods for requirements structuring
and optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01625</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01625</id><created>2015-08-07</created><authors><author><keyname>Todros</keyname><forenames>Koby</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>Robust Multiple Signal Classification via Probability Measure
  Transformation</title><categories>stat.ME cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new framework for robust multiple signal
classification (MUSIC). The proposed framework, called robust
measure-transformed (MT) MUSIC, is based on applying a transform to the
probability distribution of the received signals, i.e., transformation of the
probability measure defined on the observation space. In robust MT-MUSIC, the
sample covariance is replaced by the empirical MT-covariance. By judicious
choice of the transform we show that: 1) the resulting empirical MT-covariance
is B-robust, with bounded influence function that takes negligible values for
large norm outliers, and 2) under the assumption of spherically contoured noise
distribution, the noise subspace can be determined from the eigendecomposition
of the MT-covariance. Furthermore, we derive a new robust measure-transformed
minimum description length (MDL) criterion for estimating the number of
signals, and extend the MT-MUSIC framework to the case of coherent signals. The
proposed approach is illustrated in simulation examples that show its
advantages as compared to other robust MUSIC and MDL generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01633</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01633</id><created>2015-08-07</created><updated>2015-12-04</updated><authors><author><keyname>Zhang</keyname><forenames>Ruiliang</forenames></author><author><keyname>Zheng</keyname><forenames>Shuai</forenames></author><author><keyname>Kwok</keyname><forenames>James T.</forenames></author></authors><title>Asynchronous Distributed Semi-Stochastic Gradient Optimization</title><categories>cs.LG cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent proliferation of large-scale learning problems,there have
been a lot of interest on distributed machine learning algorithms, particularly
those that are based on stochastic gradient descent (SGD) and its variants.
However, existing algorithms either suffer from slow convergence due to the
inherent variance of stochastic gradients, or have a fast linear convergence
rate but at the expense of poorer solution quality. In this paper, we combine
their merits by proposing a fast distributed asynchronous SGD-based algorithm
with variance reduction. A constant learning rate can be used, and it is also
guaranteed to converge linearly to the optimal solution. Experiments on the
Google Cloud Computing Platform demonstrate that the proposed algorithm
outperforms state-of-the-art distributed asynchronous algorithms in terms of
both wall clock time and solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01640</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01640</id><created>2015-08-07</created><updated>2015-09-29</updated><authors><author><keyname>Horii</keyname><forenames>Shunsuke</forenames></author><author><keyname>Matsushima</keyname><forenames>Toshiyasu</forenames></author><author><keyname>Hirasawa</keyname><forenames>Shigeichi</forenames></author></authors><title>Linear Programming Decoding of Binary Linear Codes for Symbol-Pair Read
  Channels</title><categories>cs.IT math.IT</categories><comments>15pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new decoding algorithm of a binary linear codes
for symbol-pair read channels. Symbol-pair read channel has recently been
introduced by Cassuto and Blaum to model channels with high write resolution
but low read resolution. The proposed decoding algorithm is based on a linear
programming (LP). It is proved that the proposed LP decoder has the
maximum-likelihood (ML) certificate property, i.e., the output of the decoder
is guaranteed to be the ML codeword when it is integral. We also introduce the
fractional pair distance $d_{fp}$ of a code which is a lower bound on the pair
distance. It is proved that the proposed LP decoder will correct up to $\lceil
d_{fp}/2\rceil-1$ pair errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01648</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01648</id><created>2015-08-07</created><authors><author><keyname>Asadianfam</keyname><forenames>Shiva</forenames></author><author><keyname>Shamsi</keyname><forenames>Mahboubeh</forenames></author><author><keyname>Asadianfam</keyname><forenames>Sima</forenames></author></authors><title>Predicting academic major of students using bayesian networks to the
  case of iran</title><categories>cs.CY cs.IR</categories><comments>7 pages, 4 figures. http://airccse.org/journal/ijcax/. in
  International Journal of Computer-Aided Technologies (IJCAx) Vol.2, No.3,
  July 2015</comments><doi>10.5121/ijcax.2015.2304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, which took place current year in the city of Maragheh in IRAN.
Number of high school students in the fields of study: mathematics,
Experimental Sciences, humanities, vocational, business and science were
studied and compared. The purpose of this research is to predict the academic
major of high school students using Bayesian networks. The effective factors
have been used in academic major selection for the first time as an effective
indicator of Bayesian networks. Evaluation of Impacts of indicators on each
other, discretization data and processing them was performed by GeNIe. The
proper course would be advised for students to continue their education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01651</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01651</id><created>2015-08-07</created><authors><author><keyname>Barrera</keyname><forenames>David</forenames></author><author><keyname>Reischuk</keyname><forenames>Raphael M.</forenames></author><author><keyname>Szalachowski</keyname><forenames>Pawel</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author></authors><title>SCION Five Years Later: Revisiting Scalability, Control, and Isolation
  on Next-Generation Networks</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SCION (Scalability, Control, and Isolation on Next-generation Networks)
inter-domain network architecture was proposed to address the availability,
scalability, and security shortcomings of the current Internet. This paper
presents a retrospective of the SCION goals and design decisions, its attacker
model and limitations, and research highlights of work conducted in the 5 years
following SCION's initial publication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01657</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01657</id><created>2015-08-07</created><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Such&#xfd;</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>A parameterized complexity view on non-preemptively scheduling
  interval-constrained jobs: few machines, small looseness, and small slack</title><categories>cs.DS cs.DM math.CO</categories><msc-class>90B35</msc-class><acm-class>F.2.2; I.2.8; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of non-preemptively scheduling $n$ jobs, each job $j$
with a release time $t_j$, a deadline $d_j$, and a processing time $p_j$, on a
minimum number $m$ of parallel identical machines. Cieliebak et al. (2004)
considered the two constraints $|d_j-t_j|\leq \lambda p_j$ and $|d_j-t_j|\leq
p_j +\sigma$ and showed the problem to be NP-hard for any $\lambda&gt;1$ and for
any $\sigma\geq 2$. We complement their results by parameterized complexity
studies: we show that, for any $\lambda&gt;1$, the problem remains weakly NP-hard
even for $m=2$ and strongly W[1]-hard parameterized by $m$. We present a
pseudo-polynomial-time algorithm for constant $m$ and $\lambda$ and a
fixed-parameter tractability result for the parameter $m$ combined with
$\sigma$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01660</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01660</id><created>2015-08-07</created><updated>2015-11-02</updated><authors><author><keyname>Salem</keyname><forenames>Iosif</forenames></author><author><keyname>Schiller</keyname><forenames>Elad M.</forenames></author><author><keyname>Papatriantafilou</keyname><forenames>Marina</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>Shared-object System Equilibria: Delay and Throughput Analysis</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider shared-object systems that require their threads to fulfill the
system jobs by first acquiring sequentially the objects needed for the jobs and
then holding on to them until the job completion. Such systems are in the core
of a variety of shared-resource allocation and synchronization systems. This
work opens a new perspective to study the expected job delay and throughput
analytically, given the possible set of jobs that may join the system
dynamically.
  We identify the system dependencies that cause contention among the threads
as they try to acquire the job objects. We use these observations to define the
shared-object system equilibria. We note that the system is in equilibrium
whenever the rate in which jobs arrive at the system matches the job completion
rate. These equilibria consider not only the job delay but also the job
throughput, as well as the time in which each thread blocks other threads in
order to complete its job. We then further study in detail the thread work
cycles and, by using a graph representation of the problem, we are able to
propose procedures for finding and estimating equilibria, i.e., discovering the
job delay and throughput, as well as the blocking time.
  To the best of our knowledge, this is a new perspective, that can provide
better analytical tools for the problem, in order to estimate performance
measures similar to ones that can be acquired through experimentation on
working systems and simulations, e.g., as job delay and throughput in
(distributed) shared-object systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01667</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01667</id><created>2015-08-07</created><authors><author><keyname>Wang</keyname><forenames>Limin</forenames></author><author><keyname>Guo</keyname><forenames>Sheng</forenames></author><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Places205-VGGNet Models for Scene Recognition</title><categories>cs.CV</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VGGNets have turned out to be effective for object recognition in still
images. However, it is unable to yield good performance by directly adapting
the VGGNet models trained on the ImageNet dataset for scene recognition. This
report describes our implementation of training the VGGNets on the large-scale
Places205 dataset. Specifically, we train three VGGNet models, namely
VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe
toolbox with high computational efficiency. We verify the performance of
trained Places205-VGGNet models on three datasets: MIT67, SUN397, and
Places205. Our trained models achieve the state-of-the-art performance on these
datasets and are made public available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01668</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01668</id><created>2015-08-07</created><authors><author><keyname>Meghanathan</keyname><forenames>Natarajan</forenames></author></authors><title>Distribution of maximal clique size of the vertices for theoretical
  small-world networks and real-world networks</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, 13 figures, 3 tables</comments><journal-ref>International Journal of Computer Networks and Communications,
  vol. 7, no. 4, pp. 21-41, July 2015</journal-ref><doi>10.5121/ijcnc.2015.7402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our primary objective in this paper is to study the distribution of the
maximal clique size of the vertices in complex networks. We define the maximal
clique size for a vertex as the maximum size of the clique that the vertex is
part of and such a clique need not be the maximum size clique for the entire
network. We determine the maximal clique size of the vertices using a modified
version of a branch-and-bound based exact algorithm that has been originally
proposed to determine the maximum size clique for an entire network graph. We
then run this algorithm on two categories of complex networks: One category of
networks capture the evolution of small-world networks from regular network
(according to the wellknown Watts-Strogatz model) and their subsequent
evolution to random networks; we show that the distribution of the maximal
clique size of the vertices follows a Poisson-style distribution at different
stages of the evolution of the small-world network to a random network; on the
other hand, the maximal clique size of the vertices is observed to be
in-variant and to be very close to that of the maximum clique size for the
entire network graph as the regular network is transformed to a small-world
network. The second category of complex networks studied are real-world
networks (ranging from random networks to scale-free networks) and we observe
the maximal clique size of the vertices in five of the six real-world networks
to follow a Poisson-style distribution. In addition to the above case studies,
we also analyze the correlation between the maximal clique size and clustering
coefficient as well as analyze the assortativity index of the vertices with
respect to maximal clique size and node degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01669</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01669</id><created>2015-08-07</created><authors><author><keyname>Zaremohzzabieh</keyname><forenames>Zeinab</forenames></author><author><keyname>Samah</keyname><forenames>Bahaman Abu</forenames></author><author><keyname>Omar</keyname><forenames>Siti Zobidah</forenames></author><author><keyname>Bolong</keyname><forenames>Jusang</forenames></author><author><keyname>Kamarudin</keyname><forenames>Nurul Akhtar</forenames></author></authors><title>Addictive Facebook Use among University Students</title><categories>cs.CY</categories><comments>10 pages</comments><doi>10.5539/ass.v10n6p107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Facebook has become an essential part of almost every university students
daily life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01672</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01672</id><created>2015-08-07</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Yeung</keyname><forenames>Chi Ho</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Modeling mutual feedback between users and recommender systems</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>13 pages, 5 figures</comments><journal-ref>Journal of Statistical Mechanics P07020 (2015)</journal-ref><doi>10.1088/1742-5468/2015/07/P07020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems daily influence our decisions on the Internet. While
considerable attention has been given to issues such as recommendation accuracy
and user privacy, the long-term mutual feedback between a recommender system
and the decisions of its users has been neglected so far. We propose here a
model of network evolution which allows us to study the complex dynamics
induced by this feedback, including the hysteresis effect which is typical for
systems with non-linear dynamics. Despite the popular belief that
recommendation helps users to discover new things, we find that the long-term
use of recommendation can contribute to the rise of extremely popular items and
thus ultimately narrow the user choice. These results are supported by
measurements of the time evolution of item popularity inequality in real
systems. We show that this adverse effect of recommendation can be tamed by
sacrificing part of short-term recommendation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01691</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01691</id><created>2015-08-07</created><authors><author><keyname>Petiot</keyname><forenames>Guillaume</forenames></author><author><keyname>Kosmatov</keyname><forenames>Nikolai</forenames></author><author><keyname>Botella</keyname><forenames>Bernard</forenames></author><author><keyname>Giorgetti</keyname><forenames>Alain</forenames></author><author><keyname>Julliand</keyname><forenames>Jacques</forenames></author></authors><title>Your Proof Fails? Testing Helps to Find the Reason</title><categories>cs.SE</categories><comments>11 pages, 10 figures</comments><acm-class>D.2.4; D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying deductive verification to formally prove that a program respects its
formal specification is a very complex and time-consuming task due in
particular to the lack of feedback in case of proof failures. Along with a
non-compliance between the code and its specification (due to an error in at
least one of them), possible reasons of a proof failure include a missing or
too weak specification for a called function or a loop, and lack of time or
simply incapacity of the prover to finish a particular proof. This work
proposes a new methodology where test generation helps to identify the reason
of a proof failure and to exhibit a counter-example clearly illustrating the
issue. We describe how to transform an annotated C program into C code suitable
for testing and illustrate the benefits of the method on comprehensive
examples. The method has been implemented in STADY, a plugin of the software
analysis platform FRAMA-C. Initial experiments show that detecting
non-compliances and contract weaknesses allows to precisely diagnose most proof
failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01696</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01696</id><created>2015-08-07</created><authors><author><keyname>Madadipouya</keyname><forenames>Kasra</forenames></author></authors><title>A Location-Based Movie Recommender System Using Collaborative Filtering</title><categories>cs.IR</categories><comments>7 pages in International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.5, No.4, July 2015</comments><doi>10.5121/ijfcst.2015.5402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Available recommender systems mostly provide recommendations based on the
users preferences by utilizing traditional methods such as collaborative
filtering which only relies on the similarities between users and items.
However, collaborative filtering might lead to provide poor recommendation
because it does not rely on other useful available data such as users locations
and hence the accuracy of the recommendations could be very low and
inefficient. This could be very obvious in the systems that locations would
affect users preferences highly such as movie recommender systems. In this
paper a new location-based movie recommender system based on the collaborative
filtering is introduced for enhancing the accuracy and the quality of
recommendations. In this approach, users locations have been utilized and take
in consideration in the entire processing of the recommendations and peer
selections. The potential of the proposed approach in providing novel and
better quality recommendations have been discussed through experiments in real
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01703</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01703</id><created>2015-08-07</created><authors><author><keyname>Ahmadi</keyname><forenames>Mohammad</forenames></author><author><keyname>Vali</keyname><forenames>Mostafa</forenames></author><author><keyname>Moghaddam</keyname><forenames>Farez</forenames></author><author><keyname>Hakemi</keyname><forenames>Aida</forenames></author><author><keyname>Madadipouya</keyname><forenames>Kasra</forenames></author></authors><title>A Reliable User Authentication and Data Protection Model in Cloud
  Computing Environments</title><categories>cs.DC cs.CR</categories><comments>4 pages in International Conference on Information, System and
  Convergence Applications June 24-27, 2015 in Kuala Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security issues are the most challenging problems in cloud computing
environments as an emerging technology. Regarding to this importance, an
efficient and reliable user authentication and data protection model has been
presented in this paper to increase the rate of reliability cloud-based
environments. Accordingly, two encryption procedures have been established in
an independent middleware (Agent) to perform the process of user
authentication, access control, and data protection in cloud servers. AES has
been used as a symmetric cryptography algorithm in cloud servers and RSA has
been used as an asymmetric cryptography algorithm in Agent servers. The
theoretical evaluation of the proposed model shows that the ability of
resistance in face with possible attacks and unpredictable events has been
enhanced considerably in comparison with similar models because of using dual
encryption and an independent middleware during user authentication and data
protection procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01706</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01706</id><created>2015-08-07</created><authors><author><keyname>Morteza</keyname><forenames>Jaderian</forenames></author><author><keyname>Hossein</keyname><forenames>Moradzadeh</forenames></author><author><keyname>Kasra</keyname><forenames>Madadipouya</forenames></author><author><keyname>Mohammad</keyname><forenames>Firoozinia</forenames></author><author><keyname>Shahaboddin</keyname><forenames>Shamshirband</forenames></author></authors><title>A Method in Security of Wireless Sensor Network based on Optimized
  Artificial immune system in Multi-Agent Environments</title><categories>cs.NI cs.CR</categories><comments>8 pages</comments><journal-ref>Research Journal of Recent Sciences, Vol. 2(10), 99-106, October
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security in computer networks is one of the most interesting aspects of
computer systems. It is typically represented by the initials CIA:
confidentiality, integrity, and authentication or availability. Although, many
access levels for data protection have been identified in computer networks,
the intruders would still find lots of ways to harm sites and systems. The
accommodation proceedings and the security supervision in the network systems,
especially wireless sensor networks have been changed into a challenging point.
One of the newest security algorithms for wireless sensor networks is
Artificial Immune System (AIS) algorithm. Human lymphocytes play the main role
in recognizing and destroying the unknown elements. In this article, we focus
on the inspiration of these defective systems to guarantee the complications
security using two algorithms; the first algorithms proposed to distinguish
self-nodes from non-self ones by the related factors and the second one is to
eliminate the enemy node danger.The results showed a high rate success and good
rate of detecting for unknown object; it could present the best nodes with high
affinity and fitness to be selected to confront the unknown agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01707</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01707</id><created>2015-08-07</created><authors><author><keyname>Li</keyname><forenames>Wanpeng</forenames></author><author><keyname>Mitchell</keyname><forenames>Chris J</forenames></author></authors><title>Analysing the Security of Google's implementation of OpenID Connect</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many millions of users routinely use their Google accounts to log in to
relying party (RP) websites supporting the Google OpenID Connect service.
OpenID Connect, a newly standardised single-sign-on protocol, builds an
identity layer on top of the OAuth 2.0 protocol, which has itself been widely
adopted to support identity management services. It adds identity management
functionality to the OAuth 2.0 system and allows an RP to obtain assurances
regarding the authenticity of an end user. A number of authors have analysed
the security of the OAuth 2.0 protocol, but whether OpenID Connect is secure in
practice remains an open question. We report on a large-scale practical study
of Google's implementation of OpenID Connect, involving forensic examination of
103 RP websites which support its use for sign-in. Our study reveals serious
vulnerabilities of a number of types, all of which allow an attacker to log in
to an RP website as a victim user. Further examination suggests that these
vulnerabilities are caused by a combination of Google's design of its OpenID
Connect service and RP developers making design decisions which sacrifice
security for simplicity of implementation. We also give practical
recommendations for both RPs and OPs to help improve the security of real world
OpenID Connect systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01718</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01718</id><created>2015-08-07</created><authors><author><keyname>Amami</keyname><forenames>Rimah</forenames></author><author><keyname>Ellouze</keyname><forenames>Noureddine</forenames></author></authors><title>Study of Phonemes Confusions in Hierarchical Automatic Phoneme
  Recognition System</title><categories>cs.CL</categories><comments>08 pages in Journal of Convergence Information Technology 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have analyzed the impact of confusions on the robustness of
phoneme recognitions system. The confusions are detected at the pronunciation
and the confusions matrices of the phoneme recognizer. The confusions show that
some similarities between phonemes at the pronunciation affect significantly
the recognition rates. This paper proposes to understand those confusions in
order to improve the performance of the phoneme recognition system by isolating
the problematic phonemes. Confusion analysis leads to build a new hierarchical
recognizer using new phoneme distribution and the information from the
confusion matrices. This new hierarchical phoneme recognition system shows
significant improvements of the recognition rates on TIMIT database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01719</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01719</id><created>2015-08-07</created><authors><author><keyname>Fett</keyname><forenames>Daniel</forenames></author><author><keyname>Kuesters</keyname><forenames>Ralf</forenames></author><author><keyname>Schmitz</keyname><forenames>Guido</forenames></author></authors><title>SPRESSO: A Secure, Privacy-Respecting Single Sign-On System for the Web</title><categories>cs.CR</categories><comments>Parts of this work extend the web model presented in arXiv:1411.7210
  and arXiv:1403.1866</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single sign-on (SSO) systems, such as OpenID and OAuth, allow web sites,
so-called relying parties (RPs), to delegate user authentication to identity
providers (IdPs), such as Facebook or Google. These systems are very popular,
as they provide a convenient means for users to log in at RPs and move much of
the burden of user authentication from RPs to IdPs.
  There is, however, a downside to current systems, as they do not respect
users' privacy: IdPs learn at which RP a user logs in. With one exception,
namely Mozilla's BrowserID system (a.k.a. Mozilla Persona), current SSO systems
were not even designed with user privacy in mind. Unfortunately, recently
discovered attacks, which exploit design flaws of BrowserID, show that
BrowserID does not provide user privacy either.
  In this paper, we therefore propose the first privacy-respecting SSO system
for the web, called SPRESSO (for Secure Privacy-REspecting Single Sign-On). The
system is easy to use, decentralized, and platform independent. It is based
solely on standard HTML5 and web features and uses no browser extensions,
plug-ins, or other executables.
  Existing SSO systems and the numerous attacks on such systems illustrate that
the design of secure SSO systems is highly non-trivial. We therefore also carry
out a formal analysis of SPRESSO based on an expressive model of the web in
order to formally prove that SPRESSO enjoys strong authentication and privacy
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01720</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01720</id><created>2015-08-07</created><updated>2016-02-18</updated><authors><author><keyname>Sokolic</keyname><forenames>Jure</forenames></author><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Mismatch in the Classification of Linear Subspaces: Sufficient
  Conditions for Reliable Classification</title><categories>cs.IT cs.CV math.IT stat.ML</categories><comments>17 pages, 7 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the classification of linear subspaces with mismatched
classifiers. In particular, we assume a model where one observes signals in the
presence of isotropic Gaussian noise and the distribution of the signals
conditioned on a given class is Gaussian with a zero mean and a low-rank
covariance matrix. We also assume that the classifier knows only a mismatched
version of the parameters of input distribution in lieu of the true parameters.
By constructing an asymptotic low-noise expansion of an upper bound to the
error probability of such a mismatched classifier, we provide sufficient
conditions for reliable classification in the low-noise regime that are able to
sharply predict the absence of a classification error floor. Such conditions
are a function of the geometry of the true signal distribution, the geometry of
the mismatched signal distributions as well as the interplay between such
geometries, namely, the principal angles and the overlap between the true and
the mismatched signal subspaces. Numerical results demonstrate that our
conditions for reliable classification can sharply predict the behavior of a
mismatched classifier both with synthetic data and in a motion segmentation and
a hand-written digit classification applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01722</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01722</id><created>2015-08-07</created><updated>2016-03-02</updated><authors><author><keyname>Chen</keyname><forenames>Jun-Cheng</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Unconstrained Face Verification using Deep CNN Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm for unconstrained face verification
based on deep convolutional features and evaluate it on the newly released
IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world
unconstrained faces from 500 subjects with full pose and illumination
variations which are much harder than the traditional Labeled Face in the Wild
(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network
(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the
IJB-A dataset are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01727</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01727</id><created>2015-08-06</created><authors><author><keyname>Bartoli</keyname><forenames>Daniele</forenames></author><author><keyname>Bonini</keyname><forenames>Matteo</forenames></author><author><keyname>Giulietti</keyname><forenames>Massimo</forenames></author></authors><title>Constant dimension codes from Riemann-Roch spaces</title><categories>cs.IT math.AG math.CO math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some families of constant dimension codes arising from Riemann-Roch spaces
associated to particular divisors of a curve $\X$ are constructed. These
families are generalizations of the one constructed by Hansen
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01733</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01733</id><created>2015-08-07</created><authors><author><keyname>Sahin</keyname><forenames>Sureyya</forenames></author></authors><title>Position Equations of a 3RPR Planar Manipulator</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study parametric equations, which describe the position of an in-parallel
planar manipulator. We discuss isometries in the Gauss plane, then we write the
loop-closure equations in terms of the rotations as the parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01742</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01742</id><created>2015-08-07</created><authors><author><keyname>Angelakis</keyname><forenames>Vangelis</forenames></author><author><keyname>Avgouleas</keyname><forenames>Ioannis</forenames></author><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Fitzgerald</keyname><forenames>Emma</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Allocation of Heterogeneous Resources of an IoT Device to Flexible
  Services</title><categories>cs.NI</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) devices can be equipped with multiple heterogeneous
network interfaces. An overwhelmingly large amount of services may demand some
or all of these interfaces' available resources. Herein, we present a precise
mathematical formulation of assigning services to interfaces with heterogeneous
resources in one or more rounds. For reasonable instance sizes, the presented
formulation produces optimal solutions for this computationally hard problem.
We prove the NP-Completeness of the problem and develop two algorithms to
approximate the optimal solution for big instance sizes. The first algorithm
allocates the most demanding service requirements first, considering the
average cost of interfaces resources. The second one calculates the demanding
resource shares and allocates first the most demanding of them by choosing
randomly among equally demanding shares. Finally, we provide simulation results
giving insight into services splitting over different interfaces for both
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01745</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01745</id><created>2015-08-07</created><updated>2015-08-26</updated><authors><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Gasic</keyname><forenames>Milica</forenames></author><author><keyname>Mrksic</keyname><forenames>Nikola</forenames></author><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Semantically Conditioned LSTM-based Natural Language Generation for
  Spoken Dialogue Systems</title><categories>cs.CL</categories><comments>To be appear in EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language generation (NLG) is a critical component of spoken dialogue
and it has a significant impact both on usability and perceived quality. Most
NLG systems in common use employ rules and heuristics and tend to generate
rigid and stylised responses without the natural variation of human language.
They are also not easily scaled to systems covering multiple domains and
languages. This paper presents a statistical language generator based on a
semantically controlled Long Short-term Memory (LSTM) structure. The LSTM
generator can learn from unaligned data by jointly optimising sentence planning
and surface realisation using a simple cross entropy training criterion, and
language variation can be easily achieved by sampling from output candidates.
With fewer heuristics, an objective evaluation in two differing test domains
showed the proposed method improved performance compared to previous methods.
Human judges scored the LSTM system higher on informativeness and naturalness
and overall preferred it to the other systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01746</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01746</id><created>2015-08-07</created><updated>2016-01-19</updated><authors><author><keyname>Godoy</keyname><forenames>Alan</forenames></author><author><keyname>Sim&#xf5;es</keyname><forenames>Fl&#xe1;vio</forenames></author><author><keyname>Stuchi</keyname><forenames>Jos&#xe9; Augusto</forenames></author><author><keyname>Angeloni</keyname><forenames>Marcus de Assis</forenames></author><author><keyname>Uliani</keyname><forenames>M&#xe1;rio</forenames></author><author><keyname>Violato</keyname><forenames>Ricardo</forenames></author></authors><title>Using Deep Learning for Detecting Spoofing Attacks on Speech Signals</title><categories>cs.SD cs.CL cs.CR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that speaker verification systems are subject to spoofing
attacks. The Automatic Speaker Verification Spoofing and Countermeasures
Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing
attacks based on synthetic speech, along with a protocol for experiments. This
paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based
on deep neural networks, working both as a classifier and as a feature
extraction module for a GMM and a SVM classifier. Results show the validity of
this approach, achieving less than 0.5\% EER for known attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01753</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01753</id><created>2015-08-07</created><authors><author><keyname>Marinov</keyname><forenames>Martin</forenames></author><author><keyname>Nash</keyname><forenames>Nicholas</forenames></author><author><keyname>Gregg</keyname><forenames>David</forenames></author></authors><title>Practical Algorithms for Finding Extremal Sets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimal sets within a collection of sets are defined as the ones which do
not have a proper subset within the collection, and the maximal sets are the
ones which do not have a proper superset within the collection. Identifying
extremal sets is a fundamental problem with a wide-range of applications in SAT
solvers, data-mining and social network analysis. In this paper, we present two
novel improvements of the high-quality extremal set identification algorithm,
\textit{AMS-Lex}, described by Bayardo and Panda. The first technique uses
memoization to improve the execution time of the single-threaded variant of the
AMS-Lex, whilst our second improvement uses parallel programming methods. In a
subset of the presented experiments our memoized algorithm executes more than
$400$ times faster than the highly efficient publicly available implementation
of AMS-Lex. Moreover, we show that our modified algorithm's speedup is not
bounded above by a constant and that it increases as the length of the common
prefixes in successive input \textit{itemsets} increases. We provide
experimental results using both real-world and synthetic data sets, and show
our multi-threaded variant algorithm out-performing AMS-Lex by $3$ to $6$
times. We find that on synthetic input datasets when executed using $16$ CPU
cores of a $32$-core machine, our multi-threaded program executes about as fast
as the state of the art parallel GPU-based program using an NVIDIA GTX 580
graphics processing unit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01755</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01755</id><created>2015-08-07</created><authors><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Gasic</keyname><forenames>Milica</forenames></author><author><keyname>Kim</keyname><forenames>Dongho</forenames></author><author><keyname>Mrksic</keyname><forenames>Nikola</forenames></author><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Stochastic Language Generation in Dialogue using Recurrent Neural
  Networks with Convolutional Sentence Reranking</title><categories>cs.CL</categories><comments>To be appear in SigDial 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The natural language generation (NLG) component of a spoken dialogue system
(SDS) usually needs a substantial amount of handcrafting or a well-labeled
dataset to be trained on. These limitations add significantly to development
costs and make cross-domain, multi-lingual dialogue systems intractable.
Moreover, human languages are context-aware. The most natural response should
be directly learned from data rather than depending on predefined syntaxes or
rules. This paper presents a statistical language generator based on a joint
recurrent and convolutional neural network structure which can be trained on
dialogue act-utterance pairs without any semantic alignments or predefined
grammar trees. Objective metrics suggest that this new model outperforms
previous methods under the same experimental conditions. Results of an
evaluation by human judges indicate that it produces not only high quality but
linguistically varied utterances which are preferred compared to n-gram and
rule-based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01761</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01761</id><created>2015-08-07</created><authors><author><keyname>Vega</keyname><forenames>Gerardo</forenames></author></authors><title>Reducible Cyclic Codes Constructed as the Direct Sum of Two
  Semiprimitive Cyclic Codes</title><categories>cs.IT math.IT</categories><comments>13 pages, 4 weight distribution tables. Creation date April 10, 2015</comments><msc-class>11T71, 11T55, 12E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a family of reducible cyclic codes constructed as the direct sum
of two different semiprimitive two-weight irreducible cyclic codes. This family
generalizes the class of reducible cyclic codes that was reported in the main
result of B. Wang, {\em et al.} \cite{once}. Moreover, despite of what was
stated therein, we show that, at least for the codes studied here, it is still
possible to compute the frequencies of their weight distributions through the
cyclotomic numbers in a very easy way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01773</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01773</id><created>2015-08-07</created><updated>2016-02-12</updated><authors><author><keyname>Simmons</keyname><forenames>David</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author><author><keyname>Warsi</keyname><forenames>Naqueeb</forenames></author></authors><title>Capacity and Power Scaling Laws for Finite Antenna MIMO
  Amplify-and-Forward Relay Networks</title><categories>cs.IT math.IT</categories><comments>16 pages, 9 figures. Accepted for publication in IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel framework that can be used to study the
capacity and power scaling properties of linear multiple-input multiple-output
(MIMO) $d\times d$ antenna amplify-and-forward (AF) relay networks. In
particular, we model these networks as random dynamical systems (RDS) and
calculate their $d$ Lyapunov exponents. Our analysis can be applied to systems
with any per-hop channel fading distribution, although in this contribution we
focus on Rayleigh fading. Our main results are twofold: 1) the total transmit
power at the $n$th node will follow a deterministic trajectory through the
network governed by the network's maximum Lyapunov exponent, 2) the capacity of
the $i$th eigenchannel at the $n$th node will follow a deterministic trajectory
through the network governed by the network's $i$th Lyapunov exponent. Before
concluding, we concentrate on some applications of our results. In particular,
we show how the Lyapunov exponents are intimately related to the rate at which
the eigenchannel capacities diverge from each other, and how this relates to
the amplification strategy and number of antennas at each relay. We also use
them to determine the extra cost in power associated with each extra
multiplexed data stream.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01774</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01774</id><created>2015-08-07</created><updated>2016-02-11</updated><authors><author><keyname>Sigtia</keyname><forenames>Siddharth</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Dixon</keyname><forenames>Simon</forenames></author></authors><title>An End-to-End Neural Network for Polyphonic Piano Music Transcription</title><categories>stat.ML cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a supervised neural network model for polyphonic piano music
transcription. The architecture of the proposed model is analogous to speech
recognition systems and comprises an acoustic model and a music language model.
The acoustic model is a neural network used for estimating the probabilities of
pitches in a frame of audio. The language model is a recurrent neural network
that models the correlations between pitch combinations over time. The proposed
model is general and can be used to transcribe polyphonic music without
imposing any constraints on the polyphony. The acoustic and language model
predictions are combined using a probabilistic graphical model. Inference over
the output variables is performed using the beam search algorithm. We perform
two sets of experiments. We investigate various neural network architectures
for the acoustic models and also investigate the effect of combining acoustic
and music language model predictions using the proposed architecture. We
compare performance of the neural network based acoustic models with two
popular unsupervised acoustic models. Results show that convolutional neural
network acoustic models yields the best performance across all evaluation
metrics. We also observe improved performance with the application of the music
language models. Finally, we present an efficient variant of beam search that
improves performance and reduces run-times by an order of magnitude, making the
model suitable for real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01775</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01775</id><created>2015-08-07</created><authors><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author><author><keyname>Dobson</keyname><forenames>Ian</forenames></author><author><keyname>Rezaei</keyname><forenames>Pooya</forenames></author></authors><title>Cascading Power Outages Propagate Locally in an Influence Graph that is
  not the Actual Grid Topology</title><categories>physics.soc-ph cs.SY</categories><comments>Submitted to the IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a cascading power transmission outage, component outages propagate
non-locally; after one component outages, the next failure may be very distant,
both topologically and geographically. As a result, simple models of
topological contagion do not accurately represent the propagation of cascades
in power systems. However, cascading power outages do follow patterns, some of
which are useful in understanding and reducing blackout risk. This paper
describes a method by which the data from many cascading failure simulations
can be transformed into a graph-based model of influences that provides
actionable information about the many ways that cascades propagate in a
particular system. The resulting &quot;influence graph&quot; model is Markovian, since
component outage probabilities depend only on the outages that occurred in the
prior generation. To validate the model we compare the distribution of cascade
sizes resulting from n-2 contingencies in a 2896 branch test case to cascade
sizes in the influence graph. The two distributions are remarkably similar. In
addition, we derive an equation with which one can quickly identify
modifications to the proposed system that will substantially reduce cascade
propagation. With this equation one can quickly identify critical components
that can be improved to substantially reduce the risk of large cascading
blackouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01786</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01786</id><created>2015-08-07</created><authors><author><keyname>Romero</keyname><forenames>Daniel M.</forenames></author><author><keyname>Swaab</keyname><forenames>Roderick I.</forenames></author><author><keyname>Uzzi</keyname><forenames>Brian</forenames></author><author><keyname>Galinsky</keyname><forenames>Adam D.</forenames></author></authors><title>Mimicry Is Presidential: Linguistic Style Matching in Presidential
  Debates and Improved Polling Numbers</title><categories>cs.CL cs.SI</categories><comments>in the Personality and Social Psychology Bulletin (2015)</comments><doi>10.1177/0146167215591168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current research used the contexts of U.S. presidential debates and
negotiations to examine whether matching the linguistic style of an opponent in
a two-party exchange affects the reactions of third-party observers. Building
off communication accommodation theory (CAT), interaction alignment theory
(IAT), and processing fluency, we propose that language style matching (LSM)
will improve subsequent third-party evaluations because matching an opponent's
linguistic style reflects greater perspective taking and will make one's
arguments easier to process. In contrast, research on status inferences
predicts that LSM will negatively impact third-party evaluations because LSM
implies followership. We conduct two studies to test these competing
hypotheses. Study 1 analyzed transcripts of U.S. presidential debates between
1976 and 2012 and found that candidates who matched their opponent's linguistic
style increased their standing in the polls. Study 2 demonstrated a causal
relationship between LSM and third-party observer evaluations using negotiation
transcripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01795</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01795</id><created>2015-08-07</created><updated>2015-12-01</updated><authors><author><keyname>Kobayashi</keyname><forenames>Teruyoshi</forenames></author></authors><title>Trend-driven information cascades on random networks</title><categories>cs.SI physics.soc-ph</categories><comments>Physical Review E, in press</comments><doi>10.1103/PhysRevE.92.062823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threshold models of global cascades have been extensively used to model
real-world collective behavior, such as the contagious spread of fads and the
adoption of new technologies. A common property of those cascade models is that
a vanishingly small seed fraction can spread to a finite fraction of an
infinitely large network through local infections.
  In social and economic networks, however, individuals' behavior is often
influenced not only by what their direct neighbors are doing, but also by what
the majority of people are doing as a trend. A trend affects individuals'
behavior while individuals' behavior creates a trend.
  To analyze such a complex interplay between local- and global-scale
phenomena, I generalize the standard threshold model by introducing a new type
of node, called \textit{global nodes} (or \textit{trend followers}), whose
activation probability depends on a global-scale trend; specifically the
percentage of activated nodes in the population. The model shows that global
nodes play a role as accelerating cascades once a trend emerges while reducing
the probability of a trend emerging. Global nodes thus either facilitate or
inhibit cascades, suggesting that a moderate share of trend followers may
maximize the average size of cascades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01797</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01797</id><created>2015-08-07</created><authors><author><keyname>Haah</keyname><forenames>Jeongwan</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author><author><keyname>Ji</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodi</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author></authors><title>Sample-optimal tomography of quantum states</title><categories>quant-ph cs.IT math.IT</categories><comments>revtex, 11 pages</comments><report-no>MIT-CTP/4699</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a fundamental problem to decide how many copies of an unknown mixed
quantum state are necessary and sufficient to determine the state. Previously,
it was known only that estimating states to error $\epsilon$ in trace distance
required $O(dr^2/\epsilon^2)$ copies for a $d$-dimensional density matrix of
rank $r$. Here, we give a theoretical measurement scheme (POVM) that requires
$O( (dr/ \delta ) \ln (d/\delta) )$ copies of $\rho$ to error $\delta$ in
infidelity, and a matching lower bound up to logarithmic factors. This implies
$O( (dr / \epsilon^2) \ln (d/\epsilon) )$ copies suffice to achieve error
$\epsilon$ in trace distance. For fixed $d$, our measurement can be implemented
on a quantum computer in time polynomial in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01813</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01813</id><created>2015-08-07</created><authors><author><keyname>Sundar</keyname><forenames>Kaarthik</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author></authors><title>Generalized multiple depot traveling salesmen problem - polyhedral study
  and exact algorithm</title><categories>cs.DS</categories><comments>26 pages</comments><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized multiple depot traveling salesmen problem (GMDTSP) is a
variant of the multiple depot traveling salesmen problem (MDTSP), where each
salesman starts at a distinct depot, the targets are partitioned into clusters
and at least one target in each cluster is visited by some salesman. The GMDTSP
is an NP-hard problem as it generalizes the MDTSP and has practical
applications in design of ring networks, vehicle routing, flexible
manufacturing scheduling and postal routing. We present an integer programming
formulation for the GMDTSP and valid inequalities to strengthen the linear
programming relaxation. Furthermore, we present a polyhedral analysis of the
convex hull of feasible solutions to the GMDTSP and derive facet-defining
inequalities that strengthen the linear programming relaxation of the GMDTSP.
All these results are then used to develop a branch-and-cut algorithm to obtain
optimal solutions to the problem. The performance of the algorithm is evaluated
through extensive computational experiments on several benchmark instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01818</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01818</id><created>2015-08-07</created><updated>2015-09-23</updated><authors><author><keyname>Huang</keyname><forenames>Chong</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author></authors><title>Designing Incentive Schemes For Privacy-Sensitive Users</title><categories>cs.GT cs.CR</categories><comments>25 pages, 10 figures, submitted to journal of privacy and
  confidentiality</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Businesses (retailers) often wish to offer personalized advertisements
(coupons) to individuals (consumers), but run the risk of strong reactions from
consumers who want a customized shopping experience but feel their privacy has
been violated. Existing models for privacy such as differential privacy or
information theory try to quantify privacy risk but do not capture the
subjective experience and heterogeneous expression of privacy-sensitivity. We
propose a Markov decision process (MDP) model to capture (i) different consumer
privacy sensitivities via a time-varying state; (ii) different coupon types
(action set) for the retailer; and (iii) the action-and-state-dependent cost
for perceived privacy violations. For the simple case with two states (&quot;Normal&quot;
and &quot;Alerted&quot;), two coupons (targeted and untargeted) model, and consumer
behavior statistics known to the retailer, we show that a stationary
threshold-based policy is the optimal coupon-offering strategy for a retailer
that wishes to minimize its expected discounted cost. The threshold is a
function of all model parameters; the retailer offers a targeted coupon if
their belief that the consumer is in the &quot;Alerted&quot; state is below the
threshold. We extend this two-state model to consumers with multiple
privacy-sensitivity states as well as coupon-dependent state transition
probabilities. Furthermore, we study the case with imperfect (noisy) cost
feedback from consumers and uncertain initial belief state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01819</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01819</id><created>2015-08-07</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Sharmodeep</forenames></author><author><keyname>Bickel</keyname><forenames>Peter J.</forenames></author></authors><title>Spectral Clustering and Block Models: A Review And A New Algorithm</title><categories>math.ST cs.SI stat.ML stat.TH</categories><comments>27 pages</comments><msc-class>62F12, 68R10, 05C12, 62M99, 60J80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on spectral clustering of unlabeled graphs and review some results
on clustering methods which achieve weak or strong consistent identification in
data generated by such models. We also present a new algorithm which appears to
perform optimally both theoretically using asymptotic theory and empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01834</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01834</id><created>2015-08-07</created><authors><author><keyname>Chen</keyname><forenames>Bryant</forenames></author></authors><title>Decomposition and Identification of Linear Structural Equation Models</title><categories>cs.AI stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of identifying linear structural
equation models. We first extend the edge set half-trek criterion to cover a
broader class of models. We then show that any semi-Markovian linear model can
be recursively decomposed into simpler sub-models, resulting in improved
identification power. Finally, we show that, unlike the existing methods
developed for linear models, the resulting method subsumes the identification
algorithm of non-parametric models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01835</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01835</id><created>2015-08-07</created><updated>2016-02-04</updated><authors><author><keyname>Coulier</keyname><forenames>Pieter</forenames></author><author><keyname>Pouransari</keyname><forenames>Hadi</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>The inverse fast multipole method: using a fast approximate direct
  solver as a preconditioner for dense linear systems</title><categories>math.NA cs.NA</categories><comments>Revised version Submitted to the SIAM Journal on Scientific
  Computing. 35 pages, 29 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although some preconditioners are available for solving dense linear systems,
there are still many matrices for which preconditioners are lacking, in
particular in cases where the size of the matrix $N$ becomes very large. There
remains hence a great need to develop general purpose preconditioners whose
cost scales well with the matrix size $N$. In this paper, we propose a
preconditioner with broad applicability and with cost $\mathcal{O}(N)$ for
dense matrices, when the matrix is given by a smooth kernel. Extending the
method using the same framework to general $\mathcal{H}^2$-matrices is
relatively straightforward. These preconditioners have a controlled accuracy
(machine accuracy can be achieved if needed) and scale linearly with $N$. They
are based on an approximate direct solve of the system. The linear scaling of
the algorithm is achieved by means of two key ideas. First, the
$\mathcal{H}^2$-structure of the dense matrix is exploited to obtain an
extended sparse system of equations. Second, fill-ins arising when performing
the elimination are compressed as low-rank matrices if they correspond to
well-separated interactions. This ensures that the sparsity pattern of the
extended sparse matrix is preserved throughout the elimination, hence resulting
in a very efficient algorithm with $\mathcal{O}(N \log(1/\varepsilon)^2 )$
computational cost and $\mathcal{O}(N \log 1/\varepsilon )$ memory requirement,
for an error tolerance $0 &lt; \varepsilon &lt; 1$. The solver is inexact, although
the error can be controlled and made as small as needed. These solvers are
related to ILU in the sense that the fill-in is controlled. However, in ILU,
most of the fill-in is simply discarded whereas here it is approximated using
low-rank blocks, with a prescribed tolerance. Numerical examples are discussed
to demonstrate the linear scaling of the method and to illustrate its
effectiveness as a preconditioner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01836</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01836</id><created>2015-08-07</created><authors><author><keyname>Kedlaya</keyname><forenames>Kiran S.</forenames></author></authors><title>On the algebraicity of generalized power series</title><categories>math.AC cs.FL math.NT</categories><comments>22 pages</comments><msc-class>12J25, 12F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let K be an algebraically closed field of characteristic p. We exhibit a
counterexample against a theorem asserted in one of our earlier papers, which
claims to characterize the integral closure of K((t)) within the field of
Hahn-Mal'cev-Neumann generalized power series. We then give a corrected
characterization, generalizing our earlier description in terms of finite
automata in the case where K is the algebraic closure of a finite field. We
also characterize the integral closure of K(t), thus generalizing a well-known
theorem of Christol and suggesting a possible framework for computing in this
integral closure. We recover various corollaries on the structure of algebraic
generalized power series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01841</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01841</id><created>2015-08-07</created><authors><author><keyname>Ayre</keyname><forenames>Peter</forenames></author><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Greenhill</keyname><forenames>Catherine</forenames></author></authors><title>Hypergraph coloring up to condensation</title><categories>cs.DM math.CO</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving a result of Dyer, Frieze and Greenhill [Journal of Combinatorial
Theory, Series B, 2015], we determine the $q$-colorability threshold in random
$k$-uniform hypergraphs up to an additive error of $\ln 2+\varepsilon_q$, where
$\lim_{q\to\infty}\varepsilon_q=0$. The new lower bound on the threshold
matches the &quot;condensation phase transition&quot; predicted by statistical physics
considerations [Krzakala et al., PNAS~2007].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01842</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01842</id><created>2015-08-07</created><authors><author><keyname>Aghagolzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>New Guarantees for Blind Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>To appear in the 53rd Annual Allerton Conference on Communication,
  Control and Computing, University of Illinois at Urbana-Champaign, IL, USA,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind Compressed Sensing (BCS) is an extension of Compressed Sensing (CS)
where the optimal sparsifying dictionary is assumed to be unknown and subject
to estimation (in addition to the CS sparse coefficients). Since the emergence
of BCS, dictionary learning, a.k.a. sparse coding, has been studied as a matrix
factorization problem where its sample complexity, uniqueness and
identifiability have been addressed thoroughly. However, in spite of the strong
connections between BCS and sparse coding, recent results from the sparse
coding problem area have not been exploited within the context of BCS. In
particular, prior BCS efforts have focused on learning constrained and complete
dictionaries that limit the scope and utility of these efforts. In this paper,
we develop new theoretical bounds for perfect recovery for the general
unconstrained BCS problem. These unconstrained BCS bounds cover the case of
overcomplete dictionaries, and hence, they go well beyond the existing BCS
theory. Our perfect recovery results integrate the combinatorial theories of
sparse coding with some of the recent results from low-rank matrix recovery. In
particular, we propose an efficient CS measurement scheme that results in
practical recovery bounds for BCS. Moreover, we discuss the performance of BCS
under polynomial-time sparse coding algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01843</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01843</id><created>2015-08-07</created><updated>2016-03-05</updated><authors><author><keyname>Clark</keyname><forenames>Eric M.</forenames></author><author><keyname>Jones</keyname><forenames>Chris A.</forenames></author><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Kurti</keyname><forenames>Allison N.</forenames></author><author><keyname>Nortotsky</keyname><forenames>Michell Craig</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Vaporous Marketing: Uncovering Pervasive Electronic Cigarette
  Advertisements on Twitter</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Twitter has become the &quot;wild-west&quot; of marketing and promotional
strategies for advertisement agencies. Electronic cigarettes have been heavily
marketed across Twitter feeds, offering discounts, &quot;kid-friendly&quot; flavors,
algorithmically generated false testimonials, and free samples. Methods:All
electronic cigarette keyword related tweets from a 10% sample of Twitter
spanning January 2012 through December 2014 (approximately 850,000 total
tweets) were identified and categorized as Automated or Organic by combining a
keyword classification and a machine trained Human Detection algorithm. A
sentiment analysis using Hedonometrics was performed on Organic tweets to
quantify the change in consumer sentiments over time. Commercialized tweets
were topically categorized with key phrasal pattern matching. Results:The
overwhelming majority (80%) of tweets were classified as automated or
promotional in nature. The majority of these tweets were coded as
commercialized (83.65% in 2013), up to 33% of which offered discounts or free
samples and appeared on over a billion twitter feeds as impressions. The
positivity of Organic (human) classified tweets has decreased over time (5.84
in 2013 to 5.77 in 2014) due to a relative increase in the negative words
ban,tobacco,doesn't,drug,against,poison,tax and a relative decrease in the
positive words like haha,good,cool. Automated tweets are more positive than
organic (6.17 versus 5.84) due to a relative increase in the marketing words
best,win,buy,sale,health,discount and a relative decrease in negative words
like bad, hate, stupid, don't. Conclusions:Due to the youth presence on Twitter
and the clinical uncertainty of the long term health complications of
electronic cigarette consumption, the protection of public health warrants
scrutiny and potential regulation of social media marketing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01847</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01847</id><created>2015-08-07</created><updated>2015-10-09</updated><authors><author><keyname>Xuan</keyname><forenames>Pengfei</forenames></author><author><keyname>Denton</keyname><forenames>Jeffrey</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Srimani</keyname><forenames>Pradip K.</forenames></author><author><keyname>Luo</keyname><forenames>Feng</forenames></author></authors><title>Big Data Analytics on Traditional HPC Infrastructure Using Two-Level
  Storage</title><categories>cs.DC</categories><comments>Submitted to SC15, 8 pages, 7 figures, 3 tables</comments><acm-class>D.4.3</acm-class><doi>10.1145/2831244.2831253</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-intensive computing has become one of the major workloads on traditional
high-performance computing (HPC) clusters. Currently, deploying data-intensive
computing software framework on HPC clusters still faces performance and
scalability issues. In this paper, we develop a new two-level storage system by
integrating Tachyon, an in-memory file system with OrangeFS, a parallel file
system. We model the I/O throughputs of four storage structures: HDFS,
OrangeFS, Tachyon and two-level storage. We conduct computational experiments
to characterize I/O throughput behavior of two-level storage and compare its
performance to that of HDFS and OrangeFS, using TeraSort benchmark. Theoretical
models and experimental tests both show that the two-level storage system can
increase the aggregate I/O throughputs. This work lays a solid foundation for
future work in designing and building HPC systems that can provide a better
support on I/O intensive workloads with preserving existing computing
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01859</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01859</id><created>2015-08-08</created><authors><author><keyname>Illeperuma</keyname><forenames>G. D.</forenames></author><author><keyname>Sonnadara</keyname><forenames>D. U. J.</forenames></author></authors><title>Simulation of optical flow and fuzzy based obstacle avoidance system for
  mobile robots</title><categories>cs.CV cs.RO</categories><comments>4 pages, Published in 30 April 2015</comments><report-no>ISSN : 2250-3749</report-no><journal-ref>International Journal of Artificial Intelligence and Neural
  Networks, 5-1 (2015) 53-56</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Honey bees use optical flow to avoid obstacles effectively. In this research
work similar methodology was tested on a simulated mobile robot. Simulation
framework was based on VRML and Simulink in a 3D world. Optical flow vectors
were calculated from a video scene captured by a virtual camera which was used
as inputs to a fuzzy logic controller. Fuzzy logic controller decided the
locomotion of the robot. Different fuzzy logic rules were evaluated. The robot
was able to navigate through complex static and dynamic environments
effectively, avoiding obstacles on its path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01869</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01869</id><created>2015-08-08</created><updated>2015-10-19</updated><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>On environments as systemic exoskeletons: Crosscutting optimizers and
  antifragility enablers</title><categories>cs.OH</categories><comments>Accepted for publication in the Journal of Reliable Intelligent
  Environments. Extends conference papers [10,12,15]. The final publication is
  available at Springer via http://dx.doi.org/10.1007/s40860-015-0006-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classic approaches to General Systems Theory often adopt an individual
perspective and a limited number of systemic classes. As a result, those
classes include a wide number and variety of systems that result equivalent to
each other. This paper introduces a different approach: First, systems
belonging to a same class are further differentiated according to five major
general characteristics. This introduces a &quot;horizontal dimension&quot; to system
classification. A second component of our approach considers systems as nested
compositional hierarchies of other sub-systems. The resulting &quot;vertical
dimension&quot; further specializes the systemic classes and makes it easier to
assess similarities and differences regarding properties such as resilience,
performance, and quality-of-experience. Our approach is exemplified by
considering a telemonitoring system designed in the framework of Flemish
project &quot;Little Sister&quot;. We show how our approach makes it possible to design
intelligent environments able to closely follow a system's horizontal and
vertical organization and to artificially augment its features by serving as
crosscutting optimizers and as enablers of antifragile behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01872</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01872</id><created>2015-08-08</created><authors><author><keyname>Levin</keyname><forenames>Stanislav</forenames></author><author><keyname>Yehudai</keyname><forenames>Amiram</forenames></author></authors><title>Alleviating Merge Conflicts with Fine-grained Visual Awareness</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Merge conflicts created by software team members working on the same code can
be costly to resolve, and adversely affect productivity. In this work, we
suggest the approach of fine-grained merge conflict awareness, where software
team members are notified of potential merge conflicts via graphical decoration
of the relevant semantic elements, in near real-time. The novelty of this
approach is that it allows software developers to pinpoint the element in
conflict, such as a method's body, parameter, return value, and so on,
promoting communication about conflicting changes soon after they take place
and on a semantic level. We have also conducted a preliminary qualitative
evaluation of our approach, the results of which we report in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01878</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01878</id><created>2015-08-08</created><updated>2015-10-01</updated><authors><author><keyname>Li</keyname><forenames>Ming</forenames></author><author><keyname>Deng</keyname><forenames>Youjin</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Clique percolation in random graphs</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>6 pages, 5 figures</comments><journal-ref>Phys. Rev. E 92, 042116 (2015)</journal-ref><doi>10.1103/PhysRevE.92.042116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a generation of the classical percolation, clique percolation focuses on
the connection of cliques in a graph, where the connection of two $k$-cliques
means that they share at least $l&lt;k$ vertices. In this paper, we develop a
theoretical approach to study clique percolation in Erd\H{o}s-R\'{e}nyi graphs,
which gives not only the exact solutions of the critical point, but also the
corresponding order parameter. Based on this, we prove theoretically that the
fraction $\psi$ of cliques in the giant clique cluster always makes a
continuous phase transition as the classical percolation. However, the fraction
$\phi$ of vertices in the giant clique cluster for $l&gt;1$ makes a
step-function-like discontinuous phase transition in the thermodynamic limit
and a continuous phase transition for $l=1$. More interesting, our analysis
shows that at the critical point, the order parameter $\phi_c$ for $l&gt;1$ is
neither $0$ nor $1$, but a constant depending on $k$ and $l$. All these
theoretical findings are in agreement with the simulation results, which give
theoretical support and clarification for previous simulation studies of clique
percolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01880</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01880</id><created>2015-08-08</created><authors><author><keyname>Bracher</keyname><forenames>Annina</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Feedback and Partial Message Side-Information on the Semideterministic
  Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>50 pages, 2 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of the semideterministic discrete memoryless broadcast channel
(SD-BC) with partial message side-information (P-MSI) at the receivers is
established. For the setting without a common message, it is shown that P-MSI
to the deterministic receiver can only increase capacity if also the stochastic
receiver has P-MSI, while P-MSI to the stochastic receiver alone can increase
capacity. For the setting where the encoder also conveys a common message, it
is shown that P-MSI to the deterministic receiver only can also increase
capacity.
  The capacity results are used to show that on the SD-BC with or without P-MSI
feedback from the stochastic receiver can increase capacity; with P-MSI at the
deterministic receiver it can, in particular, increase the sum-rate capacity.
In contrast, as we also show, when the stochastic receiver has full MSI
(F-MSI), feedback cannot increase capacity.
  The link between feedback and P-MSI is a new feedback code for the SD-BC,
which---roughly speaking---turns feedback into P-MSI at the stochastic
receiver. The P-MSI allows the stochastic receiver to better mitigate
experienced interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01887</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01887</id><created>2015-08-08</created><updated>2015-08-11</updated><authors><author><keyname>Peng</keyname><forenames>Zhanglin</forenames></author><author><keyname>Li</keyname><forenames>Ya</forenames></author><author><keyname>Cai</keyname><forenames>Zhaoquan</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning
  in Hierarchy</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates how the traditional image classification pipelines can
be extended into a deep architecture, inspired by recent successes of deep
neural networks. We propose a deep boosting framework based on layer-by-layer
joint feature boosting and dictionary learning. In each layer, we construct a
dictionary of filters by combining the filters from the lower layer, and
iteratively optimize the image representation with a joint
discriminative-generative formulation, i.e. minimization of empirical
classification error plus regularization of analysis image generation over
training images. For optimization, we perform two iterating steps: i) to
minimize the classification error, select the most discriminative features
using the gentle adaboost algorithm; ii) according to the feature selection,
update the filters to minimize the regularization on analysis image
representation using the gradient descent method. Once the optimization is
converged, we learn the higher layer representation in the same way. Our model
delivers several distinct advantages. First, our layer-wise optimization
provides the potential to build very deep architectures. Second, the generated
image representation is compact and meaningful. In several visual recognition
tasks, our framework outperforms existing state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01890</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01890</id><created>2015-08-08</created><authors><author><keyname>Tafazzoli</keyname><forenames>Tala</forenames></author><author><keyname>Salahi</keyname><forenames>Elham</forenames></author><author><keyname>Gharaee</keyname><forenames>Hossein</forenames></author></authors><title>A proposed architecture for network forensic system in large-scale
  networks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cybercrime is increasing at a faster pace and sometimes causes billions of
dollars of business- losses so investigating attackers after commitment is of
utmost importance and become one of the main concerns of network managers.
Network forensics as the process of Collecting, identifying, extracting and
analyzing data and systematically monitoring traffic of network is one of the
main requirements in detection and tracking of criminals. In this paper, we
propose an architecture for network forensic system. Our proposed architecture
consists of five main components: collection and indexing, database management,
analysis component, SOC communication component and the database. The main
difference between our proposed architecture and other systems is in analysis
component. This component is composed of four parts: Analysis and investigation
subsystem, Reporting subsystem, Alert and visualization subsystem and the
malware analysis subsystem. The most important differentiating factors of the
proposed system with existing systems are: clustering and ranking of malware,
dynamic analysis of malware, collecting and analysis of network flows and
anomalous behaviour analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01892</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01892</id><created>2015-08-08</created><authors><author><keyname>Gu</keyname><forenames>Yifan</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>An Adaptive Transmission Protocol for Wireless-Powered Cooperative
  Communications</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. of ICC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a wireless-powered cooperative communication
network, which consists of one hybrid access point (AP), one source and one
relay to assist information transmission. Unlike conventional cooperative
networks, the source and relay are assumed to have no embedded energy supplies
in the considered system. Hence, they need to first harvest energy from the
radio-frequency (RF) signals radiated by the AP in the downlink (DL) before
information transmission in the uplink (UL). Inspired by the recently proposed
harvest-then-transmit (HTT) and harvest-then-cooperate (HTC) protocols, we
develop a new adaptive transmission (AT) protocol. In the proposed protocol, at
the beginning of each transmission block, the AP charges the source. AP and
source then perform channel estimation to acquire the channel state information
(CSI) between them. Based on the CSI estimate, the AP adaptively chooses the
source to perform UL information transmission either directly or cooperatively
with the relay. We derive an approximate closed-form expression for the average
throughput of the proposed AT protocol over Nakagami-m fading channels. The
analysis is then verified by Monte Carlo simulations. Results show that the
proposed AT protocol considerably outperforms both the HTT and HTC protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01893</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01893</id><created>2015-08-08</created><authors><author><keyname>Amiri</keyname><forenames>Ryan</forenames></author><author><keyname>Andersson</keyname><forenames>Erika</forenames></author></authors><title>Unconditionally Secure Quantum Signatures</title><categories>quant-ph cs.CR</categories><comments>16 pages; an accessible short review paper in a special issue on
  Quantum Cryptography</comments><journal-ref>Entropy 2015, 17(8), 5635-5659</journal-ref><doi>10.3390/e17085635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signature schemes, proposed in 1976 by Diffie and Hellman, have become
ubiquitous across modern communications. They allow for the exchange of
messages from one sender to multiple recipients, with the guarantees that
messages cannot be forged or tampered with and that messages also can be
forwarded from one recipient to another without compromising their validity.
Signatures are different from, but no less important than encryption, which
ensures the privacy of a message. Commonly used signature protocols -
signatures based on the Rivest-Adleman-Shamir (RSA) algorithm, the digital
signature algorithm (DSA), and the elliptic curve digital signature algorithm
(ECDSA) - are only computationally secure, similar to public key encryption
methods. In fact, since these rely on the difficulty of finding discrete
logarithms or factoring large primes, it is known that they will become
completely insecure with the emergence of quantum computers. We may therefore
see a shift towards signature protocols that will remain secure even in a
post-quantum world. Ideally, such schemes would provide unconditional or
information-theoretic security. In this paper, we aim to provide an accessible
and comprehensive review of existing unconditionally secure signature schemes
for signing classical messages, with a focus on unconditionally secure quantum
signature schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01898</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01898</id><created>2015-08-08</created><updated>2015-08-16</updated><authors><author><keyname>Liu</keyname><forenames>Jingchu</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Redesigning Fronthaul for Next-Generation Networks: Beyond Baseband
  Samples and Point-to-Point Links</title><categories>cs.IT cs.NI math.IT</categories><comments>16 pages, 5 figures, accepted by IEEE Wireless Communications
  Magazine Oct 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fronthaul (FH) is an indispensable enabler for 5G networks. However, the
classical fronthauling method demands for large bandwidth, low latency, and
tightly synchronized on the transport network, and only allows for
point-to-point logical topology. This greatly limits the usage of FH in many 5G
scenarios. In this paper, we introduce a new perspective to understand and
design FH for next-generation wireless access. We allow the renovated FH to
transport information other than time-domain I/Q samples and to support logical
topologies beyond point-to-point links. In this way, different function
splitting scheme could be incorporated into the radio access network to satisfy
the bandwidth and latency requirements of ultra-dense networks, control/data
(C/D) decoupling architectures, and delay-sensitive communications. At the same
time, massive cooperation and device-centric networking could be effectively
enabled with point-to-multi-point FH transportation. We analyze three unique
design requirements for the renovated FH, including the ability to handle
various payload traffic, support different logical topology, and provide
differentiated latency guarantee. Following this analysis, we propose a
reference architecture for designing the renovated FH. The required
functionalities are categorized into four logical layers and realized using
novel technologies such as decoupled synchronization layer, packet switching,
and session-based control. We also discuss some important future research
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01899</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01899</id><created>2015-08-08</created><updated>2015-08-24</updated><authors><author><keyname>Liu</keyname><forenames>Jingchu</forenames></author><author><keyname>Deng</keyname><forenames>Ruichen</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Seeing the Unobservable: Channel Learning for Wireless Communication
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 4 figures, accepted by GlobeCom'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication networks rely heavily on channel state information
(CSI) to make informed decision for signal processing and network operations.
However, the traditional CSI acquisition methods is facing many difficulties:
pilot-aided channel training consumes a great deal of channel resources and
reduces the opportunities for energy saving, while location-aided channel
estimation suffers from inaccurate and insufficient location information. In
this paper, we propose a novel channel learning framework, which can tackle
these difficulties by inferring unobservable CSI from the observable one. We
formulate this framework theoretically and illustrate a special case in which
the learnability of the unobservable CSI can be guaranteed. Possible
applications of channel learning are then described, including cell selection
in multi-tier networks, device discovery for device-to-device (D2D)
communications, as well as end-to-end user association for load balancing. We
also propose a neuron-network-based algorithm for the cell selection problem in
multi-tier networks. The performance of this algorithm is evaluated using
geometry-based stochastic channel model (GSCM). In settings with 5 small cells,
the average cell-selection accuracy is 73% - only a 3.9% loss compared with a
location-aided algorithm which requires genuine location information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01903</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01903</id><created>2015-08-08</created><updated>2016-02-03</updated><authors><author><keyname>Ma</keyname><forenames>Wentao</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Duan</keyname><forenames>Jiandong</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author></authors><title>Diffusion Maximum Correntropy Criterion Algorithms for Robust
  Distributed Estimation</title><categories>stat.ML cs.LG</categories><comments>17 pages,10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust diffusion adaptive estimation algorithms based on the maximum
correntropy criterion (MCC), including adaptation to combination MCC and
combination to adaptation MCC, are developed to deal with the distributed
estimation over network in impulsive (long-tailed) noise environments. The cost
functions used in distributed estimation are in general based on the mean
square error (MSE) criterion, which is desirable when the measurement noise is
Gaussian. In non-Gaussian situations, such as the impulsive-noise case, MCC
based methods may achieve much better performance than the MSE methods as they
take into account higher order statistics of error distribution. The proposed
methods can also outperform the robust diffusion least mean p-power(DLMP) and
diffusion minimum error entropy (DMEE) algorithms. The mean and mean square
convergence analysis of the new algorithms are also carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01907</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01907</id><created>2015-08-08</created><updated>2015-09-12</updated><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Efficient quantum tomography</title><categories>quant-ph cs.DS</categories><comments>25 pages. This version includes a new section on principal component
  analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the quantum state tomography problem, one wishes to estimate an unknown
$d$-dimensional mixed quantum state $\rho$, given few copies. We show that
$O(d/\epsilon)$ copies suffice to obtain an estimate $\hat{\rho}$ that
satisfies $\|\hat{\rho} - \rho\|_F^2 \leq \epsilon$ (with high probability). An
immediate consequence is that $O(\mathrm{rank}(\rho) \cdot d/\epsilon^2) \leq
O(d^2/\epsilon^2)$ copies suffice to obtain an $\epsilon$-accurate estimate in
the standard trace distance. This improves on the best known prior result of
$O(d^3/\epsilon^2)$ copies for full tomography, and even on the best known
prior result of $O(d^2\log(d/\epsilon)/\epsilon^2)$ copies for spectrum
estimation. Our result is the first to show that nontrivial tomography can be
obtained using a number of copies that is just linear in the dimension.
  Next, we generalize these results to show that one can perform efficient
principal component analysis on $\rho$. Our main result is that $O(k
d/\epsilon^2)$ copies suffice to output a rank-$k$ approximation $\hat{\rho}$
whose trace distance error is at most $\epsilon$ more than that of the best
rank-$k$ approximator to $\rho$. This subsumes our above trace distance
tomography result and generalizes it to the case when $\rho$ is not guaranteed
to be of low rank. A key part of the proof is the analogous generalization of
our spectrum-learning results: we show that the largest $k$ eigenvalues of
$\rho$ can be estimated to trace-distance error $\epsilon$ using
$O(k^2/\epsilon^2)$ copies. In turn, this result relies on a new coupling
theorem concerning the Robinson-Schensted-Knuth algorithm that should be of
independent combinatorial interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01926</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01926</id><created>2015-08-08</created><authors><author><keyname>Aijaz</keyname><forenames>Adnan</forenames></author><author><keyname>Uddin</keyname><forenames>Nazir</forenames></author><author><keyname>Holland</keyname><forenames>Oliver</forenames></author><author><keyname>Aghvami</keyname><forenames>A. Hamid</forenames></author></authors><title>On Practical Aspects of Mobile Data Offloading to Wi-Fi Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data traffic over cellular networks is exhibiting an ongoing exponential
growth, increasing by an order of magnitude every year and has already
surpassed voice traffic. This increase in data traffic demand has led to a need
for solutions to enhance capacity provision, whereby traffic offloading to
Wi-Fi is one means that can enhance realised capacity. Though offloading to
Wi-Fi networks has matured over the years, a number of challenges are still
being faced by operators to its realization. In this article, we carry out a
survey of the practical challenges faced by operators in data traffic
offloading to Wi-Fi networks. We also provide recommendations to successfully
address these challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01927</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01927</id><created>2015-08-08</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Incorporating Inductions and Game Semantics into Logic Programming</title><categories>cs.LO</categories><comments>11 pages. arXiv admin note: substantial text overlap with
  arXiv:1507.07228</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inductions and game semantics are two useful extensions to traditional logic
programming. To be specific, inductions can capture a wider class of provable
formulas in logic programming. Adopting game semantics can make logic
programming more interactive.
  In this paper, we propose an execution model for a logic language with these
features. This execution model follows closely the reasoning process in real
life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01928</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01928</id><created>2015-08-08</created><authors><author><keyname>Trillos</keyname><forenames>Nicol&#xe1;s Garc&#xed;a</forenames></author><author><keyname>Slep&#x10d;ev</keyname><forenames>Dejan</forenames></author></authors><title>A variational approach to the consistency of spectral clustering</title><categories>math.ST cs.LG stat.ML stat.TH</categories><msc-class>49J55, 49J45, 60D05, 68R10, 62G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes the consistency of spectral approaches to data
clustering. We consider clustering of point clouds obtained as samples of a
ground-truth measure. A graph representing the point cloud is obtained by
assigning weights to edges based on the distance between the points they
connect. We investigate the spectral convergence of both unnormalized and
normalized graph Laplacians towards the appropriate operators in the continuum
domain. We obtain sharp conditions on how the connectivity radius can be scaled
with respect to the number of sample points for the spectral convergence to
hold.
  We also show that the discrete clusters obtained via spectral clustering
converge towards a continuum partition of the ground truth measure. Such
continuum partition minimizes a functional describing the continuum analogue of
the graph-based spectral partitioning. Our approach, based on variational
convergence, is general and flexible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01929</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01929</id><created>2015-08-08</created><authors><author><keyname>L&#xed;&#x161;ka</keyname><forenames>Martin</forenames></author><author><keyname>Sojka</keyname><forenames>Petr</forenames></author><author><keyname>R&#x16f;&#x17e;i&#x10d;ka</keyname><forenames>Michal</forenames></author></authors><title>Combining Text and Formula Queries in Math Information Retrieval:
  Evaluation of Query Results Merging Strategies</title><categories>cs.IR</categories><acm-class>H.3.3; I.7</acm-class><doi>10.1145/2810355.2810359</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specific to Math Information Retrieval is combining text with mathematical
formulae both in documents and in queries. Rigorous evaluation of query
expansion and merging strategies combining math and standard textual keyword
terms in a query are given. It is shown that techniques similar to those known
from textual query processing may be applied in math information retrieval as
well, and lead to a cutting edge performance. Striping and merging partial
results from subqueries is one technique that improves results measured by
information retrieval evaluation metrics like Bpref.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01950</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01950</id><created>2015-08-08</created><updated>2016-01-26</updated><authors><author><keyname>Zhang</keyname><forenames>Ming</forenames></author><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>A Game Theoretic Model for Defending Against Stealthy Attacks with
  Limited Resources</title><categories>cs.GT</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stealthy attacks are a major threat to cyber security. In practice, both
attackers and defenders have resource constraints that could limit their
capabilities. Hence, to develop robust defense strategies, a promising approach
is to utilize game theory to understand the fundamental trade-offs involved.
Previous works in this direction, however, mainly focus on the single-node case
without considering strict resource constraints. In this paper, a
game-theoretic model for protecting a system of multiple nodes against stealthy
attacks is proposed. We consider the practical setting where the frequencies of
both attack and defense are constrained by limited resources, and an asymmetric
feedback structure where the attacker can fully observe the states of nodes
while largely hiding its actions from the defender. We characterize the best
response strategies for both attacker and defender, and study the Nash
Equilibria of the game. We further study a sequential game where the defender
first announces its strategy and the attacker then responds accordingly, and
design an algorithm that finds a nearly optimal strategy for the defender to
commit to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01951</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01951</id><created>2015-08-08</created><updated>2015-08-11</updated><authors><author><keyname>Nushi</keyname><forenames>Besmira</forenames></author><author><keyname>Singla</keyname><forenames>Adish</forenames></author><author><keyname>Gruenheid</keyname><forenames>Anja</forenames></author><author><keyname>Zamanian</keyname><forenames>Erfan</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Kossmann</keyname><forenames>Donald</forenames></author></authors><title>Crowd Access Path Optimization: Diversity Matters</title><categories>cs.LG cs.DB</categories><comments>10 pages, 3rd AAAI Conference on Human Computation and Crowdsourcing
  (HCOMP 2015)</comments><acm-class>H.1.2; I.2.6; H.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality assurance is one the most important challenges in crowdsourcing.
Assigning tasks to several workers to increase quality through redundant
answers can be expensive if asking homogeneous sources. This limitation has
been overlooked by current crowdsourcing platforms resulting therefore in
costly solutions. In order to achieve desirable cost-quality tradeoffs it is
essential to apply efficient crowd access optimization techniques. Our work
argues that optimization needs to be aware of diversity and correlation of
information within groups of individuals so that crowdsourcing redundancy can
be adequately planned beforehand. Based on this intuitive idea, we introduce
the Access Path Model (APM), a novel crowd model that leverages the notion of
access paths as an alternative way of retrieving information. APM aggregates
answers ensuring high quality and meaningful confidence. Moreover, we devise a
greedy optimization algorithm for this model that finds a provably good
approximate plan to access the crowd. We evaluate our approach on three
crowdsourced datasets that illustrate various aspects of the problem. Our
results show that the Access Path Model combined with greedy optimization is
cost-efficient and practical to overcome common difficulties in large-scale
crowdsourcing like data sparsity and anonymity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01954</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01954</id><created>2015-08-08</created><authors><author><keyname>Sultan</keyname><forenames>Mujahid</forenames></author><author><keyname>Miranskyy</keyname><forenames>Andriy</forenames></author></authors><title>Ordering Interrogative Questions for Effective Requirements Engineering:
  The W6H Pattern</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements elicitation and requirements analysis are important practices of
Requirements Engineering. Elicitation techniques, such as interviews and
questionnaires, rely on formulating interrogative questions and asking these in
a proper order to maximize the accuracy of the information being gathered.
Information gathered during requirements elicitation then has to be
interpreted, analyzed, and validated. Requirements analysis involves analyzing
the problem and solutions spaces. In this paper, we describe a method to
formulate interrogative questions for effective requirements elicitation based
on the lexical and semantic principles of the English language interrogatives,
and propose a pattern to organize stakeholder viewpoint concerns for better
requirements analysis. This helps requirements engineer thoroughly describe
problem and solutions spaces.
  Most of the previous requirements elicitation studies included six out of the
seven English language interrogatives 'what', 'where', 'when', 'who', 'why',
and 'how' (denoted by W5H) and did not propose any order in the interrogatives.
We show that extending the set of six interrogatives with 'which' (denoted by
W6H) improves the generation and formulation of questions for requirements
elicitation and facilitates better requirements analysis via arranging
stakeholder views. We discuss the interdependencies among interrogatives (for
requirements engineer to consider while eliciting the requirements) and suggest
an order for the set of W6H interrogatives. The proposed W6H-based reusable
pattern also aids requirements engineer in organizing viewpoint concerns of
stakeholders, making this pattern an effective tool for requirements analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01962</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01962</id><created>2015-08-08</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Species Trees from Gene Trees Despite a High Rate of Lateral Genetic
  Transfer: A Tight Bound</title><categories>math.PR cs.CE q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstructing the tree of life from molecular sequences is a fundamental
problem in computational biology. Modern data sets often contain a large number
of genes which can complicate the reconstruction problem due to the fact that
different genes may undergo different evolutionary histories. This is the case
in particular in the presence of lateral genetic transfer (LGT), whereby a gene
is inherited from a distant species rather than an immediate ancestor. Such an
event produces a gene tree which is distinct from (but related to) the species
phylogeny.
  In previous work, a stochastic model of LGT was introduced and it was shown
that the species phylogeny can be reconstructed from gene trees despite
surprisingly high rates of LGT. Both lower and upper bounds on this rate were
obtained, but a large gap remained. Here we close this gap, up to a constant.
Specifically, we show that the species phylogeny can be reconstructed perfectly
even when each edge of the tree has a constant probability of being the
location of an LGT event. Our new reconstruction algorithm builds the tree
recursively from the leaves. We also provide a matching bound in the negative
direction (up to a constant).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01975</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01975</id><created>2015-08-08</created><authors><author><keyname>Dorling</keyname><forenames>Kevin</forenames></author><author><keyname>Messier</keyname><forenames>Geoffrey G.</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Magierowski</keyname><forenames>Sebastian</forenames></author></authors><title>Minimizing the Net Present Cost of Deploying and Operating Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>14 pages, 9 figures, 2 algorithms. Accepted by IEEE Transactions on
  Network and Service Management</comments><doi>10.1109/TNSM.2015.2464071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing the cost of deploying and operating a Wireless Sensor Network
(WSN) involves deciding how to partition a budget between competing expenses
such as node hardware, energy, and labor. Most commercial network operators
account for interest rates in their budgeting exercises, providing a financial
incentive to defer some costs until a later time. In this paper, we propose a
net present cost (NPC) model for WSN capital and operating expenses that
accounts for interest rates. Our model optimizes the number, size, and spacing
between expenditures in order to minimize the NPC required for the network to
achieve a desired operational lifetime. In general this optimization problem is
non-convex, but if the spacing between expenditures is linearly proportional to
the size of the expenditures, and the number of maintenance cycles is known in
advance, the problem becomes convex and can be solved to global optimality. If
non-deferrable recurring costs are low, then evenly spacing the expenditures
can provide near-optimal results. With the provided models and methods, network
operators can now derive a payment schedule to minimize NPC while accounting
for various operational parameters. The numerical examples show substantial
cost benefits under practical assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01977</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01977</id><created>2015-08-08</created><authors><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author><author><keyname>Vishnoi</keyname><forenames>Nisheeth K.</forenames></author></authors><title>A Simple Analysis of the Dikin Walk</title><categories>cs.DS math.OC</categories><comments>8 pages</comments><msc-class>68W20, 90C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling points from the uniform distribution on a polytope is a well-studied
problem, and is an important ingredient in several computational tasks
involving polytopes, such as volume estimation. This is achieved by setting up
a random walk inside the polytope, with its stationary distribution being
uniform in the interior of the polytope. Kannan-Narayanan and Narayanan
proposed the Dikin walk based on interior point methods, where the next point
is sampled, roughly, from the Dikin ellipsoid at the current point. In this
paper, we give a simple proof of the mixing time of the Dikin walk, using
well-known properties of Gaussians, and concentration of Gaussian polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01982</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01982</id><created>2015-08-08</created><updated>2016-02-28</updated><authors><author><keyname>Dunning</keyname><forenames>Iain</forenames></author><author><keyname>Huchette</keyname><forenames>Joey</forenames></author><author><keyname>Lubin</keyname><forenames>Miles</forenames></author></authors><title>JuMP: A Modeling Language for Mathematical Optimization</title><categories>math.OC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  JuMP is an open-source modeling language that allows users to express a wide
range of optimization problems (linear, mixed-integer, quadratic,
conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic
syntax. JuMP takes advantage of advanced features of the Julia programming
language to offer unique functionality while achieving performance on par with
commercial modeling tools for standard tasks. In this work we will provide
benchmarks, present the novel aspects of the implementation, and discuss how
JuMP can be extended to new problem classes and composed with state-of-the-art
tools for visualization and interactivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01983</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01983</id><created>2015-08-09</created><updated>2016-01-08</updated><authors><author><keyname>Bakry</keyname><forenames>Amr</forenames></author><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>El-Gaaly</keyname><forenames>Tarek</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View
  Invariance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is focused on studying the view-manifold structure in the feature
spaces implied by the different layers of Convolutional Neural Networks (CNN).
There are several questions that this paper aims to answer: Does the learned
CNN representation achieve viewpoint invariance? How does it achieve viewpoint
invariance? Is it achieved by collapsing the view manifolds, or separating them
while preserving them? At which layer is view invariance achieved? How can the
structure of the view manifold at each layer of a deep convolutional neural
network be quantified experimentally? How does fine-tuning of a pre-trained CNN
on a multi-view dataset affect the representation at each layer of the network?
In order to answer these questions we propose a methodology to quantify the
deformation and degeneracy of view manifolds in CNN layers. We apply this
methodology and report interesting results in this paper that answer the
aforementioned questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01991</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01991</id><created>2015-08-09</created><authors><author><keyname>Huang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author></authors><title>Bidirectional LSTM-CRF Models for Sequence Tagging</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a variety of Long Short-Term Memory (LSTM) based
models for sequence tagging. These models include LSTM networks, bidirectional
LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer
(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is
the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to
NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model
can efficiently use both past and future input features thanks to a
bidirectional LSTM component. It can also use sentence level tag information
thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or
close to) accuracy on POS, chunking and NER data sets. In addition, it is
robust and has less dependence on word embedding as compared to previous
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01993</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01993</id><created>2015-08-09</created><authors><author><keyname>Fehrer</keyname><forenames>Ralph</forenames></author><author><keyname>Feuerriegel</keyname><forenames>Stefan</forenames></author></authors><title>Improving Decision Analytics with Deep Learning: The Case of Financial
  Disclosures</title><categories>stat.ML cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision analytics commonly focuses on the text mining of financial news
sources in order to provide managerial decision support and to predict stock
market movements. Existing predictive frameworks almost exclusively apply
traditional machine learning methods, whereas recent research indicates that
traditional machine learning methods are not sufficiently capable of extracting
suitable features and capturing the non-linear nature of complex tasks. As a
remedy, novel deep learning models aim to overcome this issue by extending
traditional neural network models with additional hidden layers. Indeed, deep
learning has been shown to outperform traditional methods in terms of
predictive performance. In this paper, we adapt the novel deep learning
technique to financial decision support. In this instance, we aim to predict
the direction of stock movements following financial disclosures. As a result,
we show how deep learning can outperform the accuracy of random forests as a
benchmark for machine learning by 5.66%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.01996</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.01996</id><created>2015-08-09</created><authors><author><keyname>Yu</keyname><forenames>Hui</forenames></author><author><keyname>Wu</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Jiang</keyname><forenames>Wenbin</forenames></author><author><keyname>Liu</keyname><forenames>Qun</forenames></author><author><keyname>Lin</keyname><forenames>ShouXun</forenames></author></authors><title>An Automatic Machine Translation Evaluation Metric Based on Dependency
  Parsing Model</title><categories>cs.CL</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the syntax-based metrics obtain the similarity by comparing the
sub-structures extracted from the trees of hypothesis and reference. These
sub-structures are defined by human and can't express all the information in
the trees because of the limited length of sub-structures. In addition, the
overlapped parts between these sub-structures are computed repeatedly. To avoid
these problems, we propose a novel automatic evaluation metric based on
dependency parsing model, with no need to define sub-structures by human.
First, we train a dependency parsing model by the reference dependency tree.
Then we generate the hypothesis dependency tree and the corresponding
probability by the dependency parsing model. The quality of the hypothesis can
be judged by this probability. In order to obtain the lexicon similarity, we
also introduce the unigram F-score to the new metric. Experiment results show
that the new metric gets the state-of-the-art performance on system level, and
is comparable with METEOR on sentence level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02015</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02015</id><created>2015-08-09</created><authors><author><keyname>Pattanayak</keyname><forenames>Sukhamoy</forenames></author><author><keyname>Singh</keyname><forenames>Abhay Kumar</forenames></author></authors><title>On cyclic DNA codes over the Ring $\Z_4 + u \Z_4$</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><msc-class>94B05, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the theory for constructing DNA cyclic codes of odd
length over $\Z_4[u]/\langle u^2 \rangle$ which play an important role in DNA
computing. Cyclic codes of odd length over $\Z_4 + u \Z_4$ satisfy the reverse
constraint and the reverse-complement constraint are studied in this paper. The
structure and existence of such codes are also studied. The paper concludes
with some DNA example obtained via the family of cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02017</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02017</id><created>2015-08-09</created><authors><author><keyname>Chiasserini</keyname><forenames>C. F</forenames></author><author><keyname>Garetto</keyname><forenames>M.</forenames></author><author><keyname>Leonardi</keyname><forenames>E.</forenames></author></authors><title>Impact of Clustering on the Performance of Network De-anonymization</title><categories>cs.SI</categories><acm-class>G.3; G.2.2; H.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, graph matching algorithms have been successfully applied to the
problem of network de-anonymization, in which nodes (users) participating to
more than one social network are identified only by means of the structure of
their links to other members. This procedure exploits an initial set of seed
nodes large enough to trigger a percolation process which correctly matches
almost all other nodes across the different social networks. Our main
contribution is to show the crucial role played by clustering, which is a
ubiquitous feature of realistic social network graphs (and many other systems).
Clustering has both the effect of making matching algorithms more vulnerable to
errors, and the potential to dramatically reduce the number of seeds needed to
trigger percolation, thanks to a wave-like propagation effect. We demonstrate
these facts by considering a fairly general class of random geometric graphs
with variable clustering level, and showing how clever algorithms can achieve
surprisingly good performance while containing matching errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02024</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02024</id><created>2015-08-09</created><authors><author><keyname>Wang</keyname><forenames>Weixi</forenames></author><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Xu</keyname><forenames>Weiping</forenames></author><author><keyname>Zhang</keyname><forenames>Baoyun</forenames></author></authors><title>Preprint Virtual Reality Based GIS Analysis Platform</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on ICONIP2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on ICONIP2015. The proposed
platform supports the integrated VRGIS functions including 3D spatial analysis
functions, 3D visualization for spatial process and serves for 3D globe and
digital city. The 3D analysis and visualization of the concerned city massive
information are conducted in the platform. The amount of information that can
be visualized with this platform is overwhelming, and the GIS based
navigational scheme allows to have great flexibility to access the different
available data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02027</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02027</id><created>2015-08-09</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The graph spectrum of barycentric refinements</title><categories>cs.DM math.CO math.SP</categories><comments>20 pages 12 figures</comments><msc-class>05C50, 57M15, 37Dxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite simple graph G, let G' be its barycentric refinement: it is
the graph in which the vertices are the complete subgraphs of G and in which
two such subgraphs are connected, if one is contained into the other. If
L(0)=0&lt;L(1) &lt;= L(2) ... &lt;= L(n) are the eigenvalues of the Laplacian of G,
define the spectral function F(x) as the function F(x) = L([n x]) on the
interval [0,1], where [r] is the floor function giving the largest integer
smaller or equal than r. The graph G' is known to be homotopic to G with Euler
characteristic chi(G')=chi(G) and dim(G') &gt;= dim(G). Let G(m) be the sequence
of barycentric refinements of G=G(0). We prove that for any finite simple graph
G, the spectral functions F(G(m)) of successive refinements converge for m to
infinity uniformly on compact subsets of (0,1) and exponentially fast to a
universal limiting eigenvalue distribution function F which only depends on the
clique number respectively the dimension d of the largest complete subgraph of
G and not on the starting graph G. In the case d=1, where we deal with graphs
without triangles, the limiting distribution is the smooth function F(x) = 4
sin^2(pi x/2). This is related to the Julia set of the quadratic map T(z) =
4z-z^2 which has the one dimensional Julia set [0,4] and F satisfies
T(F(k/n))=F(2k/n) as the Laplacians satisfy such a renormalization recursion.
The spectral density in the d=1 case is then the arc-sin distribution which is
the equilibrium measure on the Julia set. In higher dimensions, where the
limiting function F still remains unidentified, F' appears to have a discrete
or singular component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02028</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02028</id><created>2015-08-09</created><authors><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>Jin</keyname><forenames>Jie</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Reduce the Complexity of List Decoding of Polar Codes by Tree-Pruning</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes under cyclic redundancy check aided successive cancellation list
(CA-SCL) decoding can outperform the turbo codes and the LDPC codes when code
lengths are configured to be several kilobits. In order to reduce the decoding
complexity, a novel tree-pruning scheme for the \mbox{SCL/CA-SCL} decoding
algorithms is proposed in this paper. In each step of the decoding procedure,
the candidate paths with metrics less than a threshold are dropped directly to
avoid the unnecessary computations for the path searching on the descendant
branches of them. Given a candidate path, an upper bound of the path metric of
its descendants is proposed to determined whether the pruning of this candidate
path would affect frame error rate (FER) performance. By utilizing this upper
bounding technique and introducing a dynamic threshold, the proposed scheme
deletes the redundant candidate paths as many as possible while keeping the
performance deterioration in a tolerant region, thus it is much more efficient
than the existing pruning scheme. With only a negligible loss of FER
performance, the computational complexity of the proposed pruned decoding
scheme is only about $40\%$ of the standard algorithm in the low
signal-to-noise ratio (SNR) region (where the FER under CA-SCL decoding is
about $0.1 \sim 0.001$), and it can be very close to that of the successive
cancellation (SC) decoder in the moderate and high SNR regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02031</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02031</id><created>2015-08-09</created><updated>2015-08-11</updated><authors><author><keyname>Dzerzhinskiy</keyname><forenames>Fedor</forenames></author><author><keyname>Raykov</keyname><forenames>Leonid D.</forenames></author></authors><title>What Is Software Engineering?</title><categories>cs.SE</categories><comments>14 pages, In English</comments><acm-class>D.2.0</acm-class><journal-ref>Programmirovanie (Programming and Computer Software), 1990, No. 2,
  pp. 67-79 (In Russian)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A later translation (2015) of the article in Russian published in 1990. The
article proposes an approach to defining a set of basic notions for subject
area of software engineering discipline. The set of notions is intended to
serve as a basis for detection and correction of some widespread conceptual
mistakes in the efforts aimed at improving the quality and work productivity in
creation and operation of software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02035</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02035</id><created>2015-08-09</created><authors><author><keyname>Khani</keyname><forenames>Hossein</forenames></author><author><keyname>Afsharchi</keyname><forenames>Mohsen</forenames></author></authors><title>Security Games with Ambiguous Beliefs of Agents</title><categories>cs.AI cs.CR cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently the Dempster-Shafer based algorithm and Uniform Random Probability
based algorithm are the preferred method of resolving security games, in which
defenders are able to identify attackers and only strategy remained ambiguous.
However this model is inefficient in situations where resources are limited and
both the identity of the attackers and their strategies are ambiguous. The
intent of this study is to find a more effective algorithm to guide the
defenders in choosing which outside agents with which to cooperate given both
ambiguities. We designed an experiment where defenders were compelled to engage
with outside agents in order to maximize protection of their targets. We
introduced two important notions: the behavior of each agent in target
protection and the tolerance threshold in the target protection process. From
these, we proposed an algorithm that was applied by each defender to determine
the best potential assistant(s) with which to cooperate. Our results showed
that our proposed algorithm is safer than the Dempster-Shafer based algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02050</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02050</id><created>2015-08-09</created><authors><author><keyname>Almanie</keyname><forenames>Tahani</forenames></author><author><keyname>Mirza</keyname><forenames>Rsha</forenames></author><author><keyname>Lor</keyname><forenames>Elizabeth</forenames></author></authors><title>Crime Prediction Based On Crime Types And Using Spatial And Temporal
  Criminal Hotspots</title><categories>cs.AI cs.CY cs.DB</categories><comments>19 pages, 18 figures, 7 tables</comments><acm-class>H.2.8</acm-class><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.5, No.4, July 2015</journal-ref><doi>10.5121/ijdkp.2015.5401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on finding spatial and temporal criminal hotspots. It
analyses two different real-world crimes datasets for Denver, CO and Los
Angeles, CA and provides a comparison between the two datasets through a
statistical analysis supported by several graphs. Then, it clarifies how we
conducted Apriori algorithm to produce interesting frequent patterns for
criminal hotspots. In addition, the paper shows how we used Decision Tree
classifier and Naive Bayesian classifier in order to predict potential crime
types. To further analyse crimes datasets, the paper introduces an analysis
study by combining our findings of Denver crimes dataset with its demographics
information in order to capture the factors that might affect the safety of
neighborhoods. The results of this solution could be used to raise awareness
regarding the dangerous locations and to help agencies to predict future crimes
in a specific location within a particular time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02052</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02052</id><created>2015-08-09</created><updated>2015-09-18</updated><authors><author><keyname>Khan</keyname><forenames>Farooq</forenames></author></authors><title>Coreless 5G Mobile Network</title><categories>cs.NI</categories><comments>7 pages, 5 Figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Todays mobile networks contain an increasing variety of proprietary hardware
stifling innovation and leading to longer time-to-market for introduction of
new services. We propose to replace the mobile core network nodes and
interfaces with an Open Source SW implementation running on general purpose
commodity hardware. The proposed open source approach referred to as coreless
mobile network is expected to reduce cost, increase flexibility, improve
innovation speed and accelerate time-to-market for introduction of new features
and functionalities. A common Open Source SW framework will also enable
automatic discovery and selection, seamless data mobility as well as unified
charging and billing across cellular, WiFi, UAV and satellite access networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02055</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02055</id><created>2015-03-26</created><authors><author><keyname>Karmakar</keyname><forenames>Prasenjit</forenames></author><author><keyname>Gopinath</keyname><forenames>K.</forenames></author></authors><title>Scalable Reliability Modelling of RAID Storage Subsystems</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliability modelling of RAID storage systems with its various components
such as RAID controllers, enclosures, expanders, interconnects and disks is
important from a storage system designer's point of view. A model that can
express all the failure characteristics of the whole RAID storage system can be
used to evaluate design choices, perform cost reliability trade-offs and
conduct sensitivity analyses. However, including such details makes the
computational models of reliability quickly infeasible.
  We present a CTMC reliability model for RAID storage systems that scales to
much larger systems than heretofore reported and we try to model all the
components as accurately as possible. We use several state-space reduction
techniques at the user level, such as aggregating all in-series components and
hierarchical decomposition, to reduce the size of our model. To automate
computation of reliability, we use the PRISM model checker as a CTMC solver
where appropriate. Our modelling techniques using PRISM are more practical (in
both time and effort) compared to previously reported Monte-Carlo simulation
techniques.
  Our model for RAID storage systems (that includes, for example, disks,
expanders, enclosures) uses Weibull distributions for disks and, where
appropriate, correlated failure modes for disks, while we use exponential
distributions with independent failure modes for all other components. To use
the CTMC solver, we approximate the Weibull distribution for a disk using sum
of exponentials and we confirm that this model gives results that are in
reasonably good agreement with those from the sequential Monte Carlo simulation
methods for RAID disk subsystems reported in literature earlier. Using a
combination of scalable techniques, we are able to model and compute
reliability for fairly large configurations with upto 600 disks using this
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02060</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02060</id><created>2015-04-13</created><authors><author><keyname>Medhat</keyname><forenames>Walaa</forenames></author><author><keyname>Yousef</keyname><forenames>Ahmed H.</forenames></author><author><keyname>Korashy</keyname><forenames>Hoda</forenames></author></authors><title>Egyptian Dialect Stopword List Generation from Social Network Data</title><categories>cs.CL</categories><comments>The paper is an extension to the old paper found in the language
  engineering conference, arXiv:1410.1135. It is accepted by the language
  engineeringjournal. Although it has nearly the same structure, it is
  different because extensive cross validation is added any many negation words
  are added to dataset of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a methodology for generating a stopword list from online
social network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper is
to investigate the effect of removingED stopwords on the Sentiment Analysis
(SA) task. The stopwords lists generated before were on Modern Standard Arabic
(MSA) which is not the common language used in OSN. We have generated a
stopword list of Egyptian dialect to be used with the OSN corpora. We compare
the efficiency of text classification when using the generated list along with
previously generated lists of MSA and combining the Egyptian dialect list with
the MSA list. The text classification was performed using Na\&quot;ive Bayes and
Decision Tree classifiers and two feature selection approaches, unigram and
bigram. The experiments show that removing ED stopwords give better performance
than using lists of MSA stopwords only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02061</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02061</id><created>2015-05-07</created><authors><author><keyname>Jabbar</keyname><forenames>M. A.</forenames></author><author><keyname>Deekshatulu</keyname><forenames>B. L</forenames></author><author><keyname>Chandra</keyname><forenames>Priti</forenames></author></authors><title>Classification of Heart Disease Using K- Nearest Neighbor and Genetic
  Algorithm</title><categories>cs.CY cs.DB</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Data mining techniques have been widely used to mine knowledgeable
information from medical data bases. In data mining classification is a
supervised learning that can be used to design models describing important data
classes, where class attribute is involved in the construction of the
classifier. Nearest neighbor (KNN) is very simple, most popular, highly
efficient and effective algorithm for pattern recognition.KNN is a straight
forward classifier, where samples are classified based on the class of their
nearest neighbor. Medical data bases are high volume in nature. If the data set
contains redundant and irrelevant attributes, classification may produce less
accurate result. Heart disease is the leading cause of death in INDIA. In
Andhra Pradesh heart disease was the leading cause of mortality accounting for
32%of all deaths, a rate as high as Canada (35%) and USA.Hence there is a need
to define a decision support system that helps clinicians decide to take
precautionary steps. In this paper we propose a new algorithm which combines
KNN with genetic algorithm for effective classification. Genetic algorithms
perform global search in complex large and multimodal landscapes and provide
optimal solution. Experimental results shows that our algorithm enhance the
accuracy in diagnosis of heart disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02063</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02063</id><created>2015-08-09</created><updated>2015-09-15</updated><authors><author><keyname>Khan</keyname><forenames>Farooq</forenames></author></authors><title>Multi-Comm-Core Architecture for Terabit/s Wireless</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1508.02383</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communications along with the Internet has been the most
transformative technology in the past 50 years. We expect that wireless data
growth driven by new mobile applications, need to connect all humankind (not
just 1/3) as well as Billions of things to the Internet will require Terabit/s
shared links for ground based local area and wide area wireless access, for
wireless backhaul as well as access via unmanned aerial vehicles (UAVs) and
satellites. We present a new scalable radio architecture that we refer to
multi-comm-core (MCC) to enable low-cost ultra-high speed wireless
communications using both traditional and millimeter wave spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02064</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02064</id><created>2015-08-09</created><authors><author><keyname>Mytidis</keyname><forenames>Antonis</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Athanasios A.</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Orestis P.</forenames></author><author><keyname>Whiting</keyname><forenames>Bernard</forenames></author></authors><title>Sensitivity study using machine learning algorithms on simulated r-mode
  gravitational wave signals from newborn neutron stars</title><categories>astro-ph.IM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a follow-up sensitivity study on r-mode gravitational wave signals
from newborn neutron stars illustrating the applicability of machine learning
algorithms for the detection of long-lived gravitational-wave transients. In
this sensitivity study we examine three machine learning algorithms (MLAs):
artificial neural networks (ANNs), support vector machines (SVMs) and
constrained subspace classifiers (CSCs). The objective of this study is to
compare the detection efficiency that MLAs can achieve with the efficiency of
conventional detection algorithms discussed in an earlier paper. Comparisons
are made using 2 distinct r-mode waveforms. For the training of the MLAs we
assumed that some information about the distance to the source is given so that
the training was performed over distance ranges not wider than half an order of
magnitude. The results of this study suggest that machine learning algorithms
are suitable for the detection of long-lived gravitational-wave transients and
that when assuming knowledge of the distance to the source, MLAs are at least
as efficient as conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02071</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02071</id><created>2015-08-09</created><authors><author><keyname>Reichman</keyname><forenames>Daniel</forenames></author><author><keyname>Shinkar</keyname><forenames>Igor</forenames></author></authors><title>On Percolation and $NP$-Hardness</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the robustness of computational hardness of problems whose input
is obtained by applying independent random deletions to worst-case instances.
For some classical $NP$-hard problems on graphs, such as Coloring,
Vertex-Cover, and Hamiltonicity, we examine the complexity of these problems
when edges (or vertices) of an arbitrary graph are deleted independently with
probability $1-p &gt; 0$. We prove that for $n$-vertex graphs, these problems
remain as hard as in the worst-case, as long as $p &gt; \frac{1}{n^{1-\epsilon}}$
for arbitrary $\epsilon \in (0,1)$, unless $NP \subseteq BPP$.
  We also prove hardness results for Constraint Satisfaction Problems, where
random deletions are applied to clauses or variables, as well as the Subset-Sum
problem, where items of a given instance are deleted at random.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02074</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02074</id><created>2015-08-09</created><updated>2015-11-30</updated><authors><author><keyname>Schaeffer</keyname><forenames>Luke</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Closed, Palindromic, Rich, Privileged, Trapezoidal, and Balanced Words
  in Automatic Sequences</title><categories>cs.FL cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the property of being closed (resp., palindromic, rich,
privileged trapezoidal, balanced) is expressible in first-order logic for
automatic (and some related) sequences. It therefore follows that the
characteristic function of those n for which an automatic sequence x has a
closed (resp., palindromic, privileged, rich, trape- zoidal, balanced) factor
of length n is automatic. For privileged words this requires a new
characterization of the privileged property. We compute the corresponding
characteristic functions for various famous sequences, such as the Thue-Morse
sequence, the Rudin-Shapiro sequence, the ordinary paperfolding sequence, the
period-doubling sequence, and the Fibonacci sequence. Finally, we also show
that the function counting the total number of palindromic factors in a prefix
of length n of a k-automatic sequence is not k-synchronized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02078</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02078</id><created>2015-08-09</created><authors><author><keyname>Tran</keyname><forenames>Tuyen X.</forenames></author><author><keyname>Pompili</keyname><forenames>Dario</forenames></author></authors><title>Dynamic Radio Cooperation for Downlink Cloud-RANs with Computing
  Resource Sharing</title><categories>cs.IT math.IT</categories><comments>9 pages, 6 figures, accepted to IEEE MASS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel dynamic radio-cooperation strategy is proposed for Cloud Radio Access
Networks (C-RANs) consisting of multiple Remote Radio Heads (RRHs) connected to
a central Virtual Base Station (VBS) pool. In particular, the key capabilities
of C-RANs in computing-resource sharing and real-time communication among the
VBSs are leveraged to design a joint dynamic radio clustering and cooperative
beamforming scheme that maximizes the downlink weighted sum-rate system utility
(WSRSU). Due to the combinatorial nature of the radio clustering process and
the non-convexity of the cooperative beamforming design, the underlying
optimization problem is NP-hard, and is extremely difficult to solve for a
large network. Our approach aims for a suboptimal solution by transforming the
original problem into a Mixed-Integer Second-Order Cone Program (MI-SOCP),
which can be solved efficiently using a proposed iterative algorithm. Numerical
simulation results show that our low-complexity algorithm provides
close-to-optimal performance in terms of WSRSU while significantly
outperforming conventional radio clustering and beamforming schemes.
Additionally, the results also demonstrate the significant improvement in
computing-resource utilization of C-RANs over traditional RANs with distributed
computing resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02079</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02079</id><created>2015-08-09</created><authors><author><keyname>Kalyanam</keyname><forenames>Janani</forenames></author><author><keyname>Velupillai</keyname><forenames>Sumithra</forenames></author><author><keyname>Doan</keyname><forenames>Son</forenames></author><author><keyname>Conway</keyname><forenames>Mike</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert</forenames></author></authors><title>Facts and Fabrications about Ebola: A Twitter Based Study</title><categories>cs.SI cs.CY</categories><comments>Appears in SIGKDD BigCHat Workshop 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Microblogging websites like Twitter have been shown to be immensely useful
for spreading information on a global scale within seconds. The detrimental
effect, however, of such platforms is that misinformation and rumors are also
as likely to spread on the network as credible, verified information. From a
public health standpoint, the spread of misinformation creates unnecessary
panic for the public. We recently witnessed several such scenarios during the
outbreak of Ebola in 2014 [14, 1]. In order to effectively counter the medical
misinformation in a timely manner, our goal here is to study the nature of such
misinformation and rumors in the United States during fall 2014 when a handful
of Ebola cases were confirmed in North America. It is a well known convention
on Twitter to use hashtags to give context to a Twitter message (a tweet). In
this study, we collected approximately 47M tweets from the Twitter streaming
API related to Ebola. Based on hashtags, we propose a method to classify the
tweets into two sets: credible and speculative. We analyze these two sets and
study how they differ in terms of a number of features extracted from the
Twitter API. In conclusion, we infer several interesting differences between
the two sets. We outline further potential directions to using this material
for monitoring and separating speculative tweets from credible ones, to enable
improved public health information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02082</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02082</id><created>2015-08-09</created><authors><author><keyname>Lim</keyname><forenames>Benjamin</forenames></author></authors><title>Vulnerability Analysis of GWireless</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networking has become very popular in recent years due to the
increase in adoption of mobile devices. As more and more employees demand for
Wi-Fi access for their devices, more companies have been jumping onto the
&quot;Bring Your Own Device&quot; (BYOD) bandwagon[1] to appease their employees. One
such example of an enterprise wireless infrastructure is the George Washington
University's GWireless.
  For this project, I will attempt to capture hashes of authentication
credentials from users who are connecting to the GWireless network using what
is commonly known as the &quot;evil twin&quot; attack. I will document the hardware,
software used and steps taken to configure the devices. I will then evaluate
the feasibility of such an attack, explore variations of the attack and
document measures that can be taken to prevent such an attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02086</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02086</id><created>2015-08-09</created><authors><author><keyname>Kingravi</keyname><forenames>Hassan A.</forenames></author><author><keyname>Maske</keyname><forenames>Harshal</forenames></author><author><keyname>Chowdhary</keyname><forenames>Girish</forenames></author></authors><title>Kernel Controllers: A Systems-Theoretic Approach for Data-Driven
  Modeling and Control of Spatiotemporally Evolving Processes</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of modeling, estimating, and controlling the latent
state of a spatiotemporally evolving continuous function using very few sensor
measurements and actuator locations. Our solution to the problem consists of
two parts: a predictive model of functional evolution, and feedback based
estimator and controllers that can robustly recover the state of the model and
drive it to a desired function. We show that layering a dynamical systems prior
over temporal evolution of weights of a kernel model is a valid approach to
spatiotemporal modeling that leads to systems theoretic, control-usable,
predictive models. We provide sufficient conditions on the number of sensors
and actuators required to guarantee observability and controllability. The
approach is validated on a large real dataset, and in simulation for the
control of spatiotemporally evolving function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02087</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02087</id><created>2015-08-09</created><authors><author><keyname>Moritz</keyname><forenames>Philipp</forenames></author><author><keyname>Nishihara</keyname><forenames>Robert</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>A Linearly-Convergent Stochastic L-BFGS Algorithm</title><categories>math.OC cs.LG math.NA stat.CO stat.ML</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new stochastic L-BFGS algorithm and prove a linear convergence
rate for strongly convex functions. Our algorithm draws heavily from a recent
stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as a recent
approach to variance reduction for stochastic gradient descent from Johnson and
Zhang (2013). We demonstrate experimentally that our algorithm performs well on
large-scale convex and non-convex optimization problems, exhibiting linear
convergence and rapidly solving the optimization problems to high levels of
precision. Furthermore, we show that our algorithm performs well for a
wide-range of step sizes, often differing by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02091</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02091</id><created>2015-08-09</created><authors><author><keyname>Hessel</keyname><forenames>Jack</forenames></author><author><keyname>Savva</keyname><forenames>Nicolas</forenames></author><author><keyname>Wilber</keyname><forenames>Michael J.</forenames></author></authors><title>Image Representations and New Domains in Neural Image Captioning</title><categories>cs.CL cs.CV</categories><comments>11 Pages, 5 Images, To appear at EMNLP 2015's Vision + Learning
  workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the possibility that recent promising results in automatic caption
generation are due primarily to language models. By varying image
representation quality produced by a convolutional neural network, we find that
a state-of-the-art neural captioning algorithm is able to produce quality
captions even when provided with surprisingly poor image representations. We
replicate this result in a new, fine-grained, transfer learned captioning
domain, consisting of 66K recipe image/title pairs. We also provide some
experiments regarding the appropriateness of datasets for automatic captioning,
and find that having multiple captions per image is beneficial, but not an
absolute requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02096</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02096</id><created>2015-08-09</created><authors><author><keyname>Ling</keyname><forenames>Wang</forenames></author><author><keyname>Lu&#xed;s</keyname><forenames>Tiago</forenames></author><author><keyname>Marujo</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Astudillo</keyname><forenames>Ram&#xf3;n Fernandez</forenames></author><author><keyname>Amir</keyname><forenames>Silvio</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Black</keyname><forenames>Alan W.</forenames></author><author><keyname>Trancoso</keyname><forenames>Isabel</forenames></author></authors><title>Finding Function in Form: Compositional Character Models for Open
  Vocabulary Word Representation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for constructing vector representations of words by
composing characters using bidirectional LSTMs. Relative to traditional word
representation models that have independent vectors for each word type, our
model requires only a single vector per character type and a fixed set of
parameters for the compositional model. Despite the compactness of this model
and, more importantly, the arbitrary nature of the form-function relationship
in language, our &quot;composed&quot; word representations yield state-of-the-art results
in language modeling and part-of-speech tagging. Benefits over traditional
baselines are particularly pronounced in morphologically rich languages (e.g.,
Turkish).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02101</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02101</id><created>2015-08-09</created><updated>2015-08-21</updated><authors><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Lafrance</keyname><forenames>Philip</forenames></author></authors><title>Avoidability index for binary patterns with reversal</title><categories>math.CO cs.FL</categories><comments>15 pages, 1 figure</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every pattern $p$ over the alphabet $\{x,y,x^R,y^R\}$, we specify the
least $k$ such that $p$ is $k$-avoidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02103</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02103</id><created>2015-08-09</created><updated>2015-08-17</updated><authors><author><keyname>Lee</keyname><forenames>Sanghack</forenames></author><author><keyname>Honavar</keyname><forenames>Vasant</forenames></author></authors><title>Lifted Representation of Relational Causal Models Revisited:
  Implications for Reasoning and Structure Learning</title><categories>cs.AI cs.LG</categories><comments>Workshop on Advances in Causal Inference, Conference on Uncertainty
  in Artificial Intelligence, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maier et al. (2010) introduced the relational causal model (RCM) for
representing and inferring causal relationships in relational data. A lifted
representation, called abstract ground graph (AGG), plays a central role in
reasoning with and learning of RCM. The correctness of the algorithm proposed
by Maier et al. (2013a) for learning RCM from data relies on the soundness and
completeness of AGG for relational d-separation to reduce the learning of an
RCM to learning of an AGG. We revisit the definition of AGG and show that AGG,
as defined in Maier et al. (2013b), does not correctly abstract all ground
graphs. We revise the definition of AGG to ensure that it correctly abstracts
all ground graphs. We further show that AGG representation is not complete for
relational d-separation, that is, there can exist conditional independence
relations in an RCM that are not entailed by AGG. A careful examination of the
relationship between the lack of completeness of AGG for relational
d-separation and faithfulness conditions suggests that weaker notions of
completeness, namely adjacency faithfulness and orientation faithfulness
between an RCM and its AGG, can be used to learn an RCM from data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02108</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02108</id><created>2015-08-01</created><authors><author><keyname>Khalili</keyname><forenames>Azam</forenames></author><author><keyname>Rastegarnia</keyname><forenames>Amir</forenames></author></authors><title>Steady-state Performance of Incremental LMS Strategies For Parameter
  Estimation Over Fading Wireless Channels</title><categories>cs.SY cs.IT math.IT</categories><comments>4 pages; 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of fading in the communication channels between nodes on
the performance of the incremental least mean square (ILMS) algorithm. We
derive steady-state performance metrics, including the mean-square deviation
(MSD), excess mean-square error (EMSE), and mean-square error (MSE). We obtain
the sufficient conditions to ensure mean-square convergence, and verify our
results through simulations. Simulation results show that our theoretical
analysis closely matches the actual steady state performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02111</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02111</id><created>2015-08-09</created><updated>2015-08-11</updated><authors><author><keyname>Zhu</keyname><forenames>Yuqing</forenames></author><author><keyname>Wang</keyname><forenames>Yilei</forenames></author><author><keyname>Wang</keyname><forenames>Fan</forenames></author></authors><title>10 Observations on Google Cluster Trace + 2 Measures for Cluster
  Utilization Enhancement</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilization enhancement is a key concern to cluster owners. Google's cluster
manager named Borg manages its clusters at an overall utilization higher than
many others' clusters. Recently, Google has disclosed the details of its
powerful cluster manager Borg. Quite a few lessons are summarized from the Borg
experiences. Nevertheless, we find that more can be learned if the Borg design
is correlated with the trace analysis of a Google cluster managed by Borg.
There is one such trace released four years ago. In this paper, we analyze the
Google cluster trace and make 10 observations not found in previous analyses.
We also correlates the results of our analysis and previous analyses to the
Borg design, such that we find two measures that can possibly further improve
cluster utilization over Borg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02117</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02117</id><created>2015-08-09</created><authors><author><keyname>Rajanna</keyname><forenames>Amogh</forenames></author><author><keyname>Kaveh</keyname><forenames>Mos</forenames></author></authors><title>Cooperative Relaying for Large Random Multihop Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures, IEEE SPAWC 2013 and IEEE WCNC 2016 submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new relaying protocol for large multihop networks
combining the concepts of cooperative diversity and opportunistic relaying. The
cooperative relaying protocol is based on two diversity mechanisms, incremental
redundancy combining and repetition combining. We assume that nodes in the
large multihop network are modeled by a homogeneous Poisson Point Process and
operate under Rayleigh fading and constant power transmission per node. The
performance of the proposed relaying protocol is evaluated through the progress
rate density (PRD) of the multihop network and compared to the conventional
multihop relaying with no cooperation. We develop an analytic approximation to
the PRD based on the concept of decoding cells. The protocol parameters are
optimized to maximize the PRD of network. We show that the cooperative relaying
protocol provides significant throughput improvements over conventional
relaying with no cooperation in a large multihop network. It is also shown that
incremental redundancy combining provides a higher gain in PRD relative to
repetition combining. The gain in PRD has near constant value at all values of
the path loss exponent and is monotonic in diversity order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02127</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02127</id><created>2015-08-10</created><authors><author><keyname>Manvi</keyname></author><author><keyname>Bhatia</keyname><forenames>Komal Kumar</forenames></author><author><keyname>Dixit</keyname><forenames>Ashutosh</forenames></author></authors><title>A novel design of hidden web crawler using ontology</title><categories>cs.IR</categories><comments>7 pages,8 figures,2 tables, International Journal of Engineering
  Trends &amp; Technology (IJETT),August 2015, ISSN: 2231-5381</comments><doi>10.14445/22315381/IJETT-V26P204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Web is content hidden behind HTML forms. Since it represents a large
portion of the structured, unstructured and dynamic data on the Web, accessing
Deep-Web content has been a long challenge for the database community. This
paper describes a crawler for accessing Deep-Web using Ontologies. Performance
evaluation of the proposed work showed that this new approach has promising
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02131</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02131</id><created>2015-08-10</created><authors><author><keyname>Beck</keyname><forenames>Daniel</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Hardmeier</keyname><forenames>Christian</forenames></author><author><keyname>Specia</keyname><forenames>Lucia</forenames></author></authors><title>Learning Structural Kernels for Natural Language Processing</title><categories>cs.CL cs.LG</categories><comments>Transactions of the Association for Computational Linguistics, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural kernels are a flexible learning paradigm that has been widely used
in Natural Language Processing. However, the problem of model selection in
kernel-based methods is usually overlooked. Previous approaches mostly rely on
setting default values for kernel hyperparameters or using grid search, which
is slow and coarse-grained. In contrast, Bayesian methods allow efficient model
selection by maximizing the evidence on the training data through
gradient-based methods. In this paper we show how to perform this in the
context of structural kernels by using Gaussian Processes. Experimental results
on tree kernels show that this procedure results in better prediction
performance compared to hyperparameter optimization via grid search. The
framework proposed in this paper can be adapted to other structures besides
trees, e.g., strings and graphs, thereby extending the utility of kernel-based
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02133</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02133</id><created>2015-08-10</created><authors><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>On the Number of Synchronizing Colorings of Digraphs</title><categories>cs.FL</categories><comments>CIAA 2015. The final publication is available at
  http://link.springer.com/chapter/10.1007/978-3-319-22360-5_11</comments><journal-ref>In Implementation and Application of Automata (CIAA 2015), volume
  9223 of LNCS, pages 127-139, Springer, 2015</journal-ref><doi>10.1007/978-3-319-22360-5_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We deal with $k$-out-regular directed multigraphs with loops (called simply
\emph{digraphs}). The edges of such a digraph can be colored by elements of
some fixed $k$-element set in such a way that outgoing edges of every vertex
have different colors. Such a coloring corresponds naturally to an automaton.
The road coloring theorem states that every primitive digraph has a
synchronizing coloring.
  In the present paper we study how many synchronizing colorings can exist for
a digraph with $n$ vertices. We performed an extensive experimental
investigation of digraphs with small number of vertices. This was done by using
our dedicated algorithm exhaustively enumerating all small digraphs. We also
present a series of digraphs whose fraction of synchronizing colorings is equal
to $1-1/k^d$, for every $d \ge 1$ and the number of vertices large enough.
  On the basis of our results we state several conjectures and open problems.
In particular, we conjecture that $1-1/k$ is the smallest possible fraction of
synchronizing colorings, except for a single exceptional example on 6 vertices
for $k=2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02136</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02136</id><created>2015-08-10</created><authors><author><keyname>Brown</keyname><forenames>Donald L.</forenames></author><author><keyname>Vasilyeva</keyname><forenames>Maria</forenames></author></authors><title>A Generalized Multiscale Finite Element Method for Poroelasticity
  Problems I: Linear Problems</title><categories>math.NA cs.CE cs.NA physics.comp-ph</categories><comments>arXiv admin note: text overlap with arXiv:1309.6030 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the numerical solution of poroelasticity problems
that are of Biot type and develop a general algorithm for solving coupled
systems. We discuss the challenges associated with mechanics and flow problems
in heterogeneous media. The two primary issues being the multiscale nature of
the media and the solutions of the fluid and mechanics variables traditionally
developed with separate grids and methods. For the numerical solution we
develop and implement a Generalized Multiscale Finite Element Method (GMsFEM)
that solves problem on a coarse grid by constructing local multiscale basis
functions. The procedure begins with construction of multiscale bases for both
displacement and pressure in each coarse block. Using a snapshot space and
local spectral problems, we construct a basis of reduced dimension. Finally,
after multiplying by a multiscale partitions of unity, the multiscale basis is
constructed in the offline phase and the coarse grid problem then can be solved
for arbitrary forcing and boundary conditions. We implement this algorithm on
two heterogenous media and compute error between the multiscale solution with
the fine-scale solutions. Randomized oversampling and forcing strategies are
also tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02138</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02138</id><created>2015-08-10</created><authors><author><keyname>Brown</keyname><forenames>Donald L.</forenames></author><author><keyname>Vasilyeva</keyname><forenames>Maria</forenames></author></authors><title>A Generalized Multiscale Finite Element Method for Poroelasticity
  Problems II: Nonlinear Coupling</title><categories>math.NA cs.CE cs.NA</categories><comments>arXiv admin note: text overlap with arXiv:1304.5188 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the numerical solution of some nonlinear
poroelasticity problems that are of Biot type and develop a general algorithm
for solving nonlinear coupled systems. We discuss the difficulties associated
with flow and mechanics in heterogenous media with nonlinear coupling. The
central issue being how to handle the nonlinearities and the multiscale scale
nature of the media. To compute an efficient numerical solution we develop and
implement a Generalized Multiscale Finite Element Method (GMsFEM) that solves
nonlinear problems on a coarse grid by constructing local multiscale basis
functions and treating part of the nonlinearity locally as a parametric value.
After linearization with a Picard Iteration, the procedure begins with
construction of multiscale bases for both displacement and pressure in each
coarse block by treating the staggered nonlinearity as a parametric value.
Using a snapshot space and local spectral problems, we construct an offline
basis of reduced dimension. From here an online, parametric dependent, space is
constructed. Finally, after multiplying by a multiscale partitions of unity,
the multiscale basis is constructed and the coarse grid problem then can be
solved for arbitrary forcing and boundary conditions. We implement this
algorithm on a geometry with a linear and nonlinear pressure dependent
permeability field and compute error between the multiscale solution with the
fine-scale solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02142</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02142</id><created>2015-08-10</created><authors><author><keyname>Naim</keyname><forenames>Iftekhar</forenames></author><author><keyname>Gildea</keyname><forenames>Daniel</forenames></author></authors><title>Feature-based Decipherment for Large Vocabulary Machine Translation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthographic similarities across languages provide a strong signal for
probabilistic decipherment, especially for closely related language pairs. The
existing decipherment models, however, are not well-suited for exploiting these
orthographic similarities. We propose a log-linear model with latent variables
that incorporates orthographic similarity features. Maximum likelihood training
is computationally expensive for the proposed log-linear model. To address this
challenge, we perform approximate inference via MCMC sampling and contrastive
divergence. Our results show that the proposed log-linear model with
contrastive divergence scales to large vocabularies and outperforms the
existing generative decipherment models by exploiting the orthographic
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02149</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02149</id><created>2015-08-10</created><authors><author><keyname>Ciobanu</keyname><forenames>Laura</forenames></author><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Elder</keyname><forenames>Murray</forenames></author></authors><title>Solution sets for equations over free groups are EDT0L languages</title><categories>math.GR cs.CC cs.FL cs.LO</categories><comments>A conference version of this paper was presented at ICALP 2015, Kyoto
  (Japan), July 4-10, 2015, see http://arxiv.org/abs/1502.03426</comments><msc-class>03D05, 20F65, 20F70, 68Q25, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, given an equation over a finitely generated free group, the set
of all solutions in reduced words forms an effectively constructible EDT0L
language. Thus, for each equation there is a construction of a Lindenmayer
system which generates all solutions in reduced words. In particular, the set
of all solutions in reduced words is an indexed language in the sense of Aho.
The language characterization we give, as well as further questions about the
existence or finiteness of solutions, follow from our explicit construction of
a finite directed graph which encodes all the solutions. Our result
incorporates the recently invented recompression technique of Je\.z, and a new
way to integrate solutions of linear Diophantine equations into the process.
  As a byproduct of our techniques, we improve the complexity from quadratic
nondeterministic space in previous works to quasi-linear nondeterministic space
here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02153</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02153</id><created>2015-08-10</created><authors><author><keyname>Abou-Jaoudeh</keyname><forenames>John</forenames></author><author><keyname>Dak-Al-Bab</keyname><forenames>Kinan</forenames></author><author><keyname>El-Katerji</keyname><forenames>Mostafa</forenames></author><author><keyname>Falcone</keyname><forenames>Yli&#xe8;s</forenames></author><author><keyname>Jaber</keyname><forenames>Mohamad</forenames></author></authors><title>A High-Level Modeling Language for the Efficient Design, Implementation,
  and Testing of Android Applications</title><categories>cs.SE</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing mobile applications remains difficult, time consuming, and
error-prone, in spite of the number of existing platforms and tools. In this
paper, we define MoDroid, a high-level modeling language to ease the
development of Android applications. MoDroid allows developing models
representing the core of applications. MoDroid provides Android programmers
with the following advantages: (1) Models are built using high-level primitives
that abstract away several implementation details; (2) It allows the definition
of interfaces between models to automatically compose them; (3) Java native
android can be automatically generated along with the required permissions; (4)
It supports efficient model-based testing that operates on models. MoDroid is
fully implemented and was used to develop several non-trivial Android
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02157</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02157</id><created>2015-08-10</created><authors><author><keyname>Buchbinder</keyname><forenames>Niv</forenames></author><author><keyname>Feldman</keyname><forenames>Moran</forenames></author></authors><title>Deterministic Algorithms for Submodular Maximization Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomization is a fundamental tool used in many theoretical and practical
areas of computer science. We study here the role of randomization in the area
of submodular function maximization. In this area most algorithms are
randomized, and in almost all cases the approximation ratios obtained by
current randomized algorithms are superior to the best results obtained by
known deterministic algorithms. Derandomization of algorithms for general
submodular function maximization seems hard since the access to the function is
done via a value oracle. This makes it hard, for example, to apply standard
derandomization techniques such as conditional expectations. Therefore, an
interesting fundamental problem in this area is whether randomization is
inherently necessary for obtaining good approximation ratios.
  In this work we give evidence that randomization is not necessary for
obtaining good algorithms by presenting a new technique for derandomization of
algorithms for submodular function maximization. Our high level idea is to
maintain explicitly a (small) distribution over the states of the algorithm,
and carefully update it using marginal values obtained from an extreme point
solution of a suitable linear formulation. We demonstrate our technique on two
recent algorithms for unconstrained submodular maximization and for maximizing
submodular function subject to a cardinality constraint. In particular, for
unconstrained submodular maximization we obtain an optimal deterministic
$1/2$-approximation showing that randomization is unnecessary for obtaining
optimal results for this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02158</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02158</id><created>2015-08-10</created><authors><author><keyname>Tsang</keyname><forenames>Hing Yin</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Fourier Sparsity of GF(2) Polynomials</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a conjecture called &quot;linear rank conjecture&quot; recently raised in
(Tsang et al., FOCS'13), which asserts that if many linear constraints are
required to lower the degree of a GF(2) polynomial, then the Fourier sparsity
(i.e. number of non-zero Fourier coefficients) of the polynomial must be large.
We notice that the conjecture implies a surprising phenomenon that if the
highest degree monomials of a GF(2) polynomial satisfy a certain condition,
then the Fourier sparsity of the polynomial is large regardless of the
monomials of lower degrees -- whose number is generally much larger than that
of the highest degree monomials. We develop a new technique for proving lower
bound on the Fourier sparsity of GF(2) polynomials, and apply it to certain
special classes of polynomials to showcase the above phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02166</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02166</id><created>2015-08-10</created><updated>2015-08-13</updated><authors><author><keyname>Lim</keyname><forenames>Yeon-Geun</forenames></author><author><keyname>Hong</keyname><forenames>Daesik</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Performance Analysis of Self-Interference Cancellation Methods in
  Full-Duplex Large-Scale MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a performance analysis of self-interference cancellation
methods in full-duplex large-scale multiple-input multiple-output (MIMO)
systems. To support huge data traffic demands, we assume that the base station
is assumed to be located in the small cell, giving it compact antenna arrays
with a high channel correlation. From the analysis and the numerical results,
the time-domain-cancellation (TDC) outperforms the spatial suppression in the
perfect channel estimation cases. It is also concluded that the ergodic
performance of the spatial suppression is better than those of the TDC in the
imperfect channel estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02171</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02171</id><created>2015-08-10</created><authors><author><keyname>Gyarmati</keyname><forenames>Laszlo</forenames></author><author><keyname>Anguera</keyname><forenames>Xavier</forenames></author></authors><title>Automatic Extraction of the Passing Strategies of Soccer Teams</title><categories>cs.CV stat.ML</categories><comments>2015 KDD Workshop on Large-Scale Sports Analytics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology offers new ways to measure the locations of the players and of the
ball in sports. This translates to the trajectories the ball takes on the field
as a result of the tactics the team applies. The challenge professionals in
soccer are facing is to take the reverse path: given the trajectories of the
ball is it possible to infer the underlying strategy/tactic of a team? We
propose a method based on Dynamic Time Warping to reveal the tactics of a team
through the analysis of repeating series of events. Based on the analysis of an
entire season, we derive insights such as passing strategies for maintaining
ball possession or counter attacks, and passing styles with a focus on the team
or on the capabilities of the individual players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02175</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02175</id><created>2015-08-10</created><authors><author><keyname>Sharma</keyname><forenames>Shubha</forenames></author><author><keyname>Vashistha</keyname><forenames>Ankush</forenames></author><author><keyname>Bohara</keyname><forenames>Vivek Ashok</forenames></author></authors><title>On the Performance of Multiple Antenna Cooperative Spectrum Sharing
  Protocol under Nakagami-m Fading</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted in the proceedings of IEEE PIMRC 2015 Hong Kong, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a cooperative spectrum sharing (CSS) protocol, two wireless systems
operate over the same frequency band albeit with different priorities. The
secondary (or cognitive) system which has a lower priority, helps the higher
priority primary system to achieve its target rate by acting as a relay and
allocating a fraction of its power to forward the primary signal. The secondary
system in return is benefited by transmitting its own data on primary system's
spectrum. In this paper, we have analyzed the performance of multiple antenna
cooperative spectrum sharing protocol under Nakagami-m Fading. Closed form
expressions for outage probability have been obtained by varying the parameters
m and Omega of the Nakagami-m fading channels. Apart from above, we have shown
the impact of power allocation factor (alpha) and parameter m on the region of
secondary spectrum access, conventionally defined as critical radius for the
secondary system. A comparison between theoretical and simulated results is
also presented to corroborate the theoretical results obtained in this paper
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02177</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02177</id><created>2015-08-10</created><authors><author><keyname>Ning</keyname><forenames>Xuemei</forenames></author><author><keyname>Liu</keyname><forenames>Zhaoqi</forenames></author><author><keyname>Zhang</keyname><forenames>Shihua</forenames></author></authors><title>Local community extraction in directed networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network is a simple but powerful representation of real-world complex
systems. Network community analysis has become an invaluable tool to explore
and reveal the internal organization of nodes. However, only a few methods were
directly designed for community-detection in directed networks. In this
article, we introduce the concept of local community structure in directed
networks and provide a generic criterion to describe a local community with two
properties. We further propose a stochastic optimization algorithm to rapidly
detect a local community, which allows for uncovering the directional modular
characteristics in directed networks. Numerical results show that the proposed
method can resolve detailed local communities with directional information and
provide more structural characteristics of directed networks than previous
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02179</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02179</id><created>2015-08-10</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Haunschild</keyname><forenames>Robin</forenames></author></authors><title>t factor: A metric for measuring impact on Twitter</title><categories>cs.DL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the definition of the well-known h index we propose a t factor for
measuring the impact of publications (and other entities) on Twitter. The new
index combines tweet and retweet data in a balanced way whereby retweets are
seen as data reflecting the impact of initial tweets. The t factor is defined
as follows: A unit (single publication, journal, researcher, research group
etc.) has factor t if t of its Nt tweets have at least t retweets each and the
other (Nt-t) tweets have &lt;=t retweets each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02187</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02187</id><created>2015-08-10</created><authors><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>Irene</forenames></author><author><keyname>Pellikaan</keyname><forenames>Ruud</forenames></author></authors><title>A characterization of MDS codes that have an error correcting pair</title><categories>math.AG cs.IT math.IT</categories><msc-class>13P10, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error-correcting pairs were introduced in 1988 by R. Pellikaan, and were
found independently by R. K\&quot;otter (1992), as a general algebraic method of
decoding linear codes. These pairs exist for several classes of codes. However
little or no study has been made for characterizing those codes. This article
is an attempt to fill the vacuum left by the literature concerning this
subject. Since every linear code is contained in an MDS code of the same
minimum distance over some finite field extension we have focused our study on
the class of MDS codes.
  Our main result states that an MDS code of minimum distance $2t+1$ has a
$t$-ECP if and only if it is a generalized Reed-Solomon code. A second proof is
given using recent results Mirandola and Z\'emor (2015) on the Schur product of
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02196</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02196</id><created>2015-08-10</created><authors><author><keyname>Moloudi</keyname><forenames>Saeedeh</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>Threshold Saturation for Spatially Coupled Turbo-like Codes over the
  Binary Erasure Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove threshold saturation for spatially coupled turbo codes
(SC-TCs) and braided convolutional codes (BCCs) over the binary erasure
channel. We introduce a compact graph representation for the ensembles of SC-TC
and BCC codes which simplifies their description and the analysis of the
message passing decoding. We demonstrate that by few assumptions in the
ensembles of these codes, it is possible to rewrite their vector recursions in
a form which places these ensembles under the category of scalar admissible
systems. This allows us to define potential functions and prove threshold
saturation using the proof technique introduced by Yedla et al..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02198</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02198</id><created>2015-08-10</created><authors><author><keyname>Chun</keyname><forenames>Young Jin</forenames></author><author><keyname>Omri</keyname><forenames>Aymen</forenames></author><author><keyname>Hasna</keyname><forenames>Mazen O.</forenames></author></authors><title>Joint Optimization of Area Spectral Efficiency and Delay Over PPP
  Interfered Ad-hoc Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication, IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing demand on user data rates, future wireless
communication networks require higher spectral efficiency. To reach higher
spectral efficiencies, wireless network technologies collaborate and construct
a seamless interconnection between multiple tiers of architectures at the cost
of increased co-channel interference. To evaluate the performance of the
co-channel transmission based communication, we propose a new metric for area
spectral efficiency (ASE) of interference limited Ad-hoc network by assuming
that the nodes are randomly distributed according to a Poisson point processes
(PPP). We introduce a utility function, U = ASE/delay and derive the optimal
ALOHA transmission probability p and the SIR threshold \tau that jointly
maximize the ASE and minimize the local delay. Finally numerical results has
been conducted to confirm that the joint optimization based on the U metric
achieves a significant performance gain compared to conventional systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02206</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02206</id><created>2015-08-10</created><updated>2015-10-01</updated><authors><author><keyname>Xing</keyname><forenames>Pengbo</forenames></author><author><keyname>Liu</keyname><forenames>Ju</forenames></author><author><keyname>Zhai</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Xinhua</forenames></author><author><keyname>Zheng</keyname><forenames>Lina</forenames></author></authors><title>Self-Interference Suppression for the Full-Duplex Wireless Communication
  with Large-Scale Antenna</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we proposed a shared-antenna full-duplex massive MIMO model
for the multiuser MIMO system. This model exploits a single antenna array at
the base station (BS) to transmit and receive the signals simultaneously. It
has the merits of both the full-duplex system and the time-division duplex
(TDD) massive MIMO system, i.e., the high spectral efficiency and the channel
reciprocity. We focus on the zero-forcing (ZF) and the maximal-ratio
transmission/maximal-ratio combining (MRT/MRC) linear processing methods, which
are commonly used in the massive MIMO system. As the main finding, we prove
that the self-interference (SI) in a shared-antenna full-duplex massive MU-MIMO
system can be suppressed in the completely dependent uplink and downlink
channels when the number of antennas becomes large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02212</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02212</id><created>2015-08-10</created><authors><author><keyname>Zhang</keyname><forenames>Weiyu</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Joint Robust Transmit/Receive Adaptive Beamforming for MIMO Radar Using
  Probability-Constrained Optimization</title><categories>cs.IT math.IT</categories><comments>14 pages, 1 figure, Submitted to IEEE Signal Processing Letters on
  May 2015</comments><journal-ref>IEEE Signal Processing Letters, vol. 23, no. 1, pp. 112-116, Jan.
  2016</journal-ref><doi>10.1109/LSP.2015.2504386</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A joint robust transmit/receive adaptive beamforming for multiple-input
multipleoutput (MIMO) radar based on probability-constrained optimization
approach is developed in the case of Gaussian and arbitrary distributed
mismatch present in both the transmit and receive signal steering vectors. A
tight lower bound of the probability constraint is also derived by using
duality theory. The formulated probability-constrained robust beamforming
problem is nonconvex and NP-hard. However, we reformulate its cost function
into a bi-quadratic function while the probability constraint splits into
transmit and receive parts. Then, a block coordinate descent method based on
second-order cone programming is developed to address the biconvex problem.
Simulation results show an improved robustness of the proposed beamforming
method as compared to the worst-case and other existing state-of-the-art joint
transmit/receive robust adaptive beamforming methods for MIMO radar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02219</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02219</id><created>2015-08-10</created><authors><author><keyname>Carpentieri</keyname><forenames>Bruno</forenames></author><author><keyname>Liao</keyname><forenames>Jia</forenames></author><author><keyname>Sosonkina</keyname><forenames>Masha</forenames></author><author><keyname>Bonfiglioli</keyname><forenames>Aldo</forenames></author></authors><title>Using the VBARMS method in parallel computing</title><categories>math.NA cs.MA cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes an improved parallel MPI-based implementation of VBARMS,
a variable block variant of the pARMS preconditioner proposed by Li,~Saad and
Sosonkina [NLAA, 2003] for solving general nonsymmetric linear systems. The
parallel VBARMS solver can detect automatically exact or approximate dense
structures in the linear system, and exploits this information to achieve
improved reliability and increased throughput during the factorization. A novel
graph compression algorithm is discussed that finds these approximate dense
blocks structures and requires only one simple to use parameter. A complete
study of the numerical and parallel performance of parallel VBARMS is presented
for the analysis of large turbulent Navier-Stokes equations on a suite of
three-dimensional test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02225</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02225</id><created>2015-08-10</created><authors><author><keyname>Yu</keyname><forenames>Hui</forenames></author><author><keyname>Wu</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Jiang</keyname><forenames>Wenbin</forenames></author><author><keyname>Liu</keyname><forenames>Qun</forenames></author><author><keyname>Lin</keyname><forenames>Shouxun</forenames></author></authors><title>Improve the Evaluation of Translation Fluency by Using Entropy of
  Matched Sub-segments</title><categories>cs.CL</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widely-used automatic evaluation metrics cannot adequately reflect the
fluency of the translations. The n-gram-based metrics, like BLEU, limit the
maximum length of matched fragments to n and cannot catch the matched fragments
longer than n, so they can only reflect the fluency indirectly. METEOR, which
is not limited by n-gram, uses the number of matched chunks but it does not
consider the length of each chunk. In this paper, we propose an entropy-based
method, which can sufficiently reflect the fluency of translations through the
distribution of matched words. This method can easily combine with the
widely-used automatic evaluation metrics to improve the evaluation of fluency.
Experiments show that the correlations of BLEU and METEOR are improved on
sentence level after combining with the entropy-based method on WMT 2010 and
WMT 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02240</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02240</id><created>2015-08-10</created><authors><author><keyname>Lee</keyname><forenames>Taeho</forenames></author><author><keyname>Szalachowski</keyname><forenames>Pawel</forenames></author><author><keyname>Barrera</keyname><forenames>David</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author><author><keyname>Lee</keyname><forenames>Heejo</forenames></author><author><keyname>Watrin</keyname><forenames>David</forenames></author></authors><title>Bootstrapping Real-world Deployment of Future Internet Architectures</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past decade has seen many proposals for future Internet architectures.
Most of these proposals require substantial changes to the current networking
infrastructure and end-user devices, resulting in a failure to move from theory
to real-world deployment. This paper describes one possible strategy for
bootstrapping the initial deployment of future Internet architectures by
focusing on providing high availability as an incentive for early adopters.
Through large-scale simulation and real-world implementation, we show that with
only a small number of adopting ISPs, customers can obtain high availability
guarantees. We discuss design, implementation, and evaluation of an
availability device that allows customers to bridge into the future Internet
architecture without modifications to their existing infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02246</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02246</id><created>2015-08-10</created><authors><author><keyname>Nguyen</keyname><forenames>Ngu</forenames></author></authors><title>Feature Learning for Interaction Activity Recognition in RGBD Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a human activity recognition method which is based on
features learned from 3D video data without incorporating domain knowledge. The
experiments on data collected by RGBD cameras produce results outperforming
other techniques. Our feature encoding method follows the bag-of-visual-word
model, then we use a SVM classifier to recognise the activities. We do not use
skeleton or tracking information and the same technique is applied on color and
depth data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02253</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02253</id><created>2015-08-10</created><updated>2016-02-16</updated><authors><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Ramezanipour</keyname><forenames>Iran</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Average Error Probability in Wireless Sensor Networks with Imperfect
  Sensing and Communication for Different Decision Rules</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework to evaluate the probability that a decision
error event occurs in wireless sensor networks, including sensing and
communication errors. We consider a scenario where sensors need to identify
whether a given event has occurred based on its periodic, noisy, observations
of a given signal. Such information about the signal needs to be sent to a
fusion center that decides about the actual state at that specific observation
time. The communication links -- single- or multi-hop -- are modeled as binary
symmetric channels, which may have different error probabilities. The decision
at the fusion center is based on OR, AND, K-OUT-OF-N and MAJORITY Boolean
operations on the received signals associated to individual sensor
observations. We derive closed-form equations for the average decision error
probability as a function of the system parameters (e.g. number of sensors and
hops) and the input signal characterization. Our analyses show the best
decision rule is closely related to the frequency that the observed events
occur and the number of sensors. In our numerical example, we show that the AND
rule outperforms MAJORITY if such an event is rare and there is only a handful
number of sensors. Conversely, if there is a large number of sensors or more
evenly distributed event occurrences, the MAJORITY is the best choice. We
further show that, while the error probability using the MAJORITY rule
asymptotically goes to 0 with increasing number of sensors, it is also more
susceptible to higher channel error probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02268</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02268</id><created>2015-08-10</created><authors><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Chen</keyname><forenames>Ting</forenames></author></authors><title>Dropout Training for SVMs with Data Augmentation</title><categories>cs.LG</categories><comments>15 pages. arXiv admin note: substantial text overlap with
  arXiv:1404.4171</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout and other feature noising schemes have shown promising results in
controlling over-fitting by artificially corrupting the training data. Though
extensive theoretical and empirical studies have been performed for generalized
linear models, little work has been done for support vector machines (SVMs),
one of the most successful approaches for supervised learning. This paper
presents dropout training for both linear SVMs and the nonlinear extension with
latent representation learning. For linear SVMs, to deal with the intractable
expectation of the non-smooth hinge loss under corrupting distributions, we
develop an iteratively re-weighted least square (IRLS) algorithm by exploring
data augmentation techniques. Our algorithm iteratively minimizes the
expectation of a re-weighted least square problem, where the re-weights are
analytically updated. For nonlinear latent SVMs, we consider learning one layer
of latent representations in SVMs and extend the data augmentation technique in
conjunction with first-order Taylor-expansion to deal with the intractable
expected non-smooth hinge loss and the nonlinearity of latent representations.
Finally, we apply the similar data augmentation ideas to develop a new IRLS
algorithm for the expected logistic loss under corrupting distributions, and we
further develop a non-linear extension of logistic regression by incorporating
one layer of latent representations. Our algorithms offer insights on the
connection and difference between the hinge loss and logistic loss in dropout
training. Empirical results on several real datasets demonstrate the
effectiveness of dropout training on significantly boosting the classification
accuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMs
further improve the prediction performance on several image datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02272</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02272</id><created>2015-08-10</created><authors><author><keyname>Zwolak</keyname><forenames>Justyna P.</forenames></author><author><keyname>Brewe</keyname><forenames>Eric</forenames></author></authors><title>The impact of social integration on student persistence in introductory
  Modeling Instruction courses</title><categories>physics.ed-ph cs.SI</categories><comments>4 pages, 5 figures, 3 tables, submitter to 2015 Physics Education
  Research Conference Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing student retention and persistence -- in particular classes or in
their major area of study -- is a challenge for universities. Students'
academic and social integration into an institution seems to be vital for
student retention, yet, research on the effect of interpersonal interactions is
rare. Social network analysis is an approach that can be used to identify
patterns of interaction that contribute to integration into the university. We
analyze how students position within a social network in a Modeling Instruction
(MI) course that strongly emphasizes interactive learning impacts their
persistence in taking a subsequent MI course. We find that students with higher
centrality at the end of the first semester of MI are more likely to enroll in
a second semester of MI. While the correlation with increased persistence is an
ongoing study, these findings suggest that student social integration
influences persistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02284</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02284</id><created>2015-08-10</created><authors><author><keyname>Samardjiska</keyname><forenames>Simona</forenames></author><author><keyname>Gligoroski</keyname><forenames>Danilo</forenames></author></authors><title>Approaching Maximum Embedding Efficiency on Small Covers Using
  Staircase-Generator Codes</title><categories>cs.MM cs.IT math.IT</categories><comments>Extended version of the paper presented at ISIT 2015</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new family of binary linear codes suitable for steganographic
matrix embedding. The main characteristic of the codes is the staircase random
block structure of the generator matrix. We propose an efficient list decoding
algorithm for the codes that finds a close codeword to a given random word. We
provide both theoretical analysis of the performance and stability of the
decoding algorithm, as well as practical results. Used for matrix embedding,
these codes achieve almost the upper theoretical bound of the embedding
efficiency for covers in the range of 1000 - 1500 bits, which is at least an
order of magnitude smaller than the values reported in related works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02285</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02285</id><created>2015-08-10</created><authors><author><keyname>Limsopatham</keyname><forenames>Nut</forenames></author><author><keyname>Collier</keyname><forenames>Nigel</forenames></author></authors><title>Adapting Phrase-based Machine Translation to Normalise Medical Terms in
  Social Media Messages</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies have shown that health reports in social media, such as
DailyStrength and Twitter, have potential for monitoring health conditions
(e.g. adverse drug reactions, infectious diseases) in particular communities.
However, in order for a machine to understand and make inferences on these
health conditions, the ability to recognise when laymen's terms refer to a
particular medical concept (i.e.\ text normalisation) is required. To achieve
this, we propose to adapt an existing phrase-based machine translation (MT)
technique and a vector representation of words to map between a social media
phrase and a medical concept. We evaluate our proposed approach using a
collection of phrases from tweets related to adverse drug reactions. Our
experimental results show that the combination of a phrase-based MT technique
and the similarity between word vector representations outperforms the
baselines that apply only either of them by up to 55%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02297</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02297</id><created>2015-08-10</created><authors><author><keyname>Schakel</keyname><forenames>Adriaan M. J.</forenames></author><author><keyname>Wilson</keyname><forenames>Benjamin J.</forenames></author></authors><title>Measuring Word Significance using Distributed Representations of Words</title><categories>cs.CL</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed representations of words as real-valued vectors in a relatively
low-dimensional space aim at extracting syntactic and semantic features from
large text corpora. A recently introduced neural network, named word2vec
(Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic
information in the direction of the word vectors. In this brief report, it is
proposed to use the length of the vectors, together with the term frequency, as
measure of word significance in a corpus. Experimental evidence using a
domain-specific corpus of abstracts is presented to support this proposal. A
useful visualization technique for text corpora emerges, where words are mapped
onto a two-dimensional plane and automatically ranked by significance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02301</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02301</id><created>2015-08-10</created><authors><author><keyname>Klein</keyname><forenames>Edouard</forenames></author></authors><title>Easy steps towards a sane IT policy in hospitals</title><categories>cs.CY</categories><comments>17 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We witnessed the low quality of IT solutions in Paris hospitals. The price
paid to private companies for these solutions and the cost incurred from their
inefficiency constitute a gross and appalling waste of public resources. We
propose to bootstrap a change in IT policy by having heads of department hire
IT workers ; we give advice to the central decision making body on how to
incentivize them. Easily measurable efficiency gains as well as
hard-to-quantify positive externalities will follow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02303</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02303</id><created>2015-08-10</created><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Fu</keyname><forenames>Lingkun</forenames></author><author><keyname>Chen</keyname><forenames>Min</forenames></author><author><keyname>Chi</keyname><forenames>Kaikai</forenames></author><author><keyname>Zhu</keyname><forenames>Yi-hua</forenames></author></authors><title>RF-Based Charger Placement for Duty Cycle Guarantee in Battery-Free
  Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Battery-free sensor networks have emerged as a promising solution to conquer
the lifetime limitation of battery-powered systems. In this paper, we study a
sensor network built from battery-free sensor nodes which harvest energy from
radio frequency (RF) signals transmitted by RF-based chargers, e.g., radio
frequency identification (RFID) readers. Due to the insufficiency of harvested
energy, the sensor nodes have to work in duty cycles to harvest enough energy
before turning active and performing tasks. One fundamental issue in this kind
of network design is how to deploy the chargers to ensure that the battery-free
nodes can maintain a designated duty cycle for continuous operation. Based on a
new wireless recharge model, we formulate the charger placement problem for
node's duty cycle guarantee as a constrained optimization problem. We develop
both greedy and efficient heuristics for solving the problem and validate our
solutions through extensive simulations. The simulation results show that the
proposed particle swarm optimization (PSO)-based divide-and-conquer approach
can effectively reduce the number of chargers compared with the greedy
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02307</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02307</id><created>2015-08-10</created><authors><author><keyname>Khambekar</keyname><forenames>Nilesh</forenames></author><author><keyname>Spooner</keyname><forenames>Chad M.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Vipin</forenames></author></authors><title>MUSE: A Methodology for Characterizing and Quantifying the Use of
  Spectrum</title><categories>cs.NI</categories><comments>Under submission to a Journal. Additional Contact:
  nilesh@spectrumfi.com or spectrumfi@outlook.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum sharing paradigm is envisaged to meet the growing demand for
the Radio Frequency (RF) spectrum. There exist several technical, regulatory,
and business impediments for adopting the new paradigm. In this regard, we
underscore the need of characterizing and quantifying the use of spectrum by
each of the individual transmitters and receivers.
  We propose MUSE, a methodology to characterize and quantify the use of
spectrum in the space, time, and frequency dimensions. MUSE characterizes the
use of spectrum by a transmitter at a point in terms of the RF power occupied
by the transmitter. It characterizes the use of spectrum by a receiver at a
point in terms of the constraints on the RF-power that can be occupied by any
of the transmitters in the system in order to ensure successful reception. It
divides the spectrum-space into discrete unit-spectrum-spaces and quantifies
the spectrum used by the individual transceivers in the discretized spectrum
space.
  We characterize the performance of the spectrum management functions in the
discretized spectrum-space and illustrate maximizing the use of spectrum. In
order to address the challenges for the dynamic spectrum sharing paradigm, we
emphasize on articulating, defining, and enforcing the spectrum-access rights
in the discretized spectrum-space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02315</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02315</id><created>2015-08-10</created><authors><author><keyname>Brunelle</keyname><forenames>Justin F.</forenames></author><author><keyname>Weigle</keyname><forenames>Michele C.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Archiving Deferred Representations Using a Two-Tiered Crawling Approach</title><categories>cs.DL cs.IR</categories><comments>To appear at iPRES2015 11 pages</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web resources are increasingly interactive, resulting in resources that are
increasingly difficult to archive. The archival difficulty is based on the use
of client-side technologies (e.g., JavaScript) to change the client-side state
of a representation after it has initially loaded. We refer to these
representations as deferred representations. We can better archive deferred
representations using tools like headless browsing clients. We use 10,000 seed
Universal Resource Identifiers (URIs) to explore the impact of including
PhantomJS -- a headless browsing tool -- into the crawling process by comparing
the performance of wget (the baseline), PhantomJS, and Heritrix. Heritrix
crawled 2.065 URIs per second, 12.15 times faster than PhantomJS and 2.4 times
faster than wget. However, PhantomJS discovered 531,484 URIs, 1.75 times more
than Heritrix and 4.11 times more than wget. To take advantage of the
performance benefits of Heritrix and the URI discovery of PhantomJS, we
recommend a tiered crawling strategy in which a classifier predicts whether a
representation will be deferred or not, and only resources with deferred
representations are crawled with PhantomJS while resources without deferred
representations are crawled with Heritrix. We show that this approach is 5.2
times faster than using only PhantomJS and creates a frontier (set of URIs to
be crawled) 1.8 times larger than using only Heritrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02324</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02324</id><created>2015-08-10</created><updated>2015-12-01</updated><authors><author><keyname>Liu</keyname><forenames>Xiao-Yang</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wu</keyname><forenames>Min-You</forenames></author></authors><title>Adaptive Sampling of RF Fingerprints for Fine-grained Indoor
  Localization</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>To appear in IEEE Transactions on Mobile Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indoor localization is a supporting technology for a broadening range of
pervasive wireless applications. One promis- ing approach is to locate users
with radio frequency fingerprints. However, its wide adoption in real-world
systems is challenged by the time- and manpower-consuming site survey process,
which builds a fingerprint database a priori for localization. To address this
problem, we visualize the 3-D RF fingerprint data as a function of locations
(x-y) and indices of access points (fingerprint), as a tensor and use tensor
algebraic methods for an adaptive tubal-sampling of this fingerprint space. In
particular using a recently proposed tensor algebraic framework in [1] we
capture the complexity of the fingerprint space as a low-dimensional
tensor-column space. In this formulation the proposed scheme exploits
adaptivity to identify reference points which are highly informative for
learning this low-dimensional space. Further, under certain incoherency
conditions we prove that the proposed scheme achieves bounded recovery error
and near-optimal sampling complexity. In contrast to several existing work that
rely on random sampling, this paper shows that adaptivity in sampling can lead
to significant improvements in localization accuracy. The approach is validated
on both data generated by the ray-tracing indoor model which accounts for the
floor plan and the impact of walls and the real world data. Simulation results
show that, while maintaining the same localization accuracy of existing
approaches, the amount of samples can be cut down by 71% for the high SNR case
and 55% for the low SNR case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02326</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02326</id><created>2015-08-10</created><updated>2015-08-14</updated><authors><author><keyname>Bulling</keyname><forenames>Nils</forenames></author><author><keyname>Nguyen</keyname><forenames>Hoang Nga</forenames></author></authors><title>Model Checking Resource Bounded Systems with Shared Resources via
  Alternating B\&quot;uchi Pushdown Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the verification of resource-constrained multiagent
systems is undecidable in general. In many such settings, resources are private
to agents. In this paper, we investigate the model checking problem for a
resource logic based on Alternating-Time Temporal Logic (ATL) with shared
resources. Resources can be consumed and produced up to any amount. We show
that the model checking problem is undecidable if two or more of such unbounded
resources are available. Our main technical result is that in the case of a
single shared resource, the problem becomes decidable. Although intuitive, the
proof of decidability is non-trivial. We reduce model checking to a problem
over alternating B\&quot;uchi pushdown systems. An intermediate result connects to
general automata-based verification: we show that model checking Computation
Tree Logic (CTL) over (compact) alternating B\&quot;uchi pushdown systems is
decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02336</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02336</id><created>2015-08-06</created><authors><author><keyname>Ukil</keyname><forenames>Abhisek</forenames></author></authors><title>Theoretical Analysis of Tuned HVAC Line for Low Loss Long Distance Bulk
  Power Transmission</title><categories>cs.SY</categories><comments>Published version is 5 pages</comments><journal-ref>International Journal of Electrical power &amp; Energy Systems, vol.
  73, pp. 433-437, 2015</journal-ref><doi>10.1016/j.ijepes.2015.05.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main objectives of the smart grid initiative is to enable bulk
power transmission over long distance, with reduced transmission losses.
Besides the traditional high-voltage alternating current (HVAC) transmission,
with the advancement in power electronics, high-voltage direct current (HVDC)
transmission is increasingly becoming important. One of the main factors
impacting the transmission line parameters and the losses is the length of the
transmission line (overhead). In this paper, a concept of tuned high-voltage AC
line is presented for long (&gt; 250 km) transmission line. A tuned line is where
the receiving-end voltage and current are numerically equal to the
corresponding sending-end values. This paper presents the detailed theoretical
analysis of the tuned HVAC line, suggesting adaptation of the transmission
frequency as per the length of the line. The simulation of a tuned HVAC line is
performed using the PSCAD/EMTDC. Simulation results for two different line
lengths, substantiate the theoretical analysis of reducing the reactive power
absorbed in the line, while increasing the active power transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02340</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02340</id><created>2015-08-07</created><updated>2015-11-11</updated><authors><author><keyname>Tauchnitz</keyname><forenames>Nico</forenames></author></authors><title>The Pontryagin Maximum Principle for Nonlinear Infinite Horizon Optimal
  Control Problems with State Constraints</title><categories>math.OC cs.SY</categories><comments>75 pages, in german. V2: corrections in formula 3.22, example 3.23,
  remark 5.5. One part is published in JOTA (see DOI)</comments><msc-class>34, 46, 49</msc-class><doi>10.1007/s10957-015-0723-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous proof of the Pontryagin maximum principle for control problems on
a finite horizon bases on the needle variation technique, as well as the
separability concept of cones created by disturbances of the trajectories. In
this preprint, we develop this method for infinite horizon optimal control
problems. The results are necessary conditions for a strong local minimizer in
form of the Pontryagin maximum principle, Arrow type sufficiency conditions and
the validity of diverse transversality conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02341</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02341</id><created>2015-08-10</created><authors><author><keyname>Shoari</keyname><forenames>Arian</forenames></author></authors><title>On the Definition and Existence of an MVU Estimator for Target Location
  Estimation</title><categories>cs.IT math.IT</categories><comments>27 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of target localization with ideal binary detectors is considered
in one dimensional space. The problem is investigated in both a censored and
non-censored scheme. In the censored setting, the problem is equivalent to
estimating the center of a uniform distribution by knowing samples of data. It
does not admit an MVU estimator according to the previous results of
Lehmann-Sheffe. However, it is proven that if the radius of detection is known
and sensor deployment region is very large, both censored and non-censored
cases will have an MVU estimator among the functions that are invariant to
Euclidean motion. In addition, it is shown that when the radius of detection is
not known, the censored case still has an MVU estimator whereas in the
non-censored case, an MVU estimator does not exist, even under the assumption
that the estimators are invariant to Euclidean motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02344</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02344</id><created>2015-08-10</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Local Algorithms for Block Models with Side Information</title><categories>stat.ML cs.CC cs.DC math.PR</categories><comments>Due to the limitation &quot;The abstract field cannot be longer than 1,920
  characters&quot;, the abstract here is shorter than that in the PDF file</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a recent interest in understanding the power of local
algorithms for optimization and inference problems on sparse graphs. Gamarnik
and Sudan (2014) showed that local algorithms are weaker than global algorithms
for finding large independent sets in sparse random regular graphs. Montanari
(2015) showed that local algorithms are suboptimal for finding a community with
high connectivity in the sparse Erd\H{o}s-R\'enyi random graphs. For the
symmetric planted partition problem (also named community detection for the
block models) on sparse graphs, a simple observation is that local algorithms
cannot have non-trivial performance.
  In this work we consider the effect of side information on local algorithms
for community detection under the binary symmetric stochastic block model. In
the block model with side information each of the $n$ vertices is labeled $+$
or $-$ independently and uniformly at random; each pair of vertices is
connected independently with probability $a/n$ if both of them have the same
label or $b/n$ otherwise. The goal is to estimate the underlying vertex
labeling given 1) the graph structure and 2) side information in the form of a
vertex labeling positively correlated with the true one. Assuming that the
ratio between in and out degree $a/b$ is $\Theta(1)$ and the average degree $
(a+b) / 2 = n^{o(1)}$, we characterize three different regimes under which a
local algorithm, namely, belief propagation run on the local neighborhoods,
maximizes the expected fraction of vertices labeled correctly. Thus, in
contrast to the case of symmetric block models without side information, we
show that local algorithms can achieve optimal performance for the block model
with side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02345</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02345</id><created>2015-08-10</created><authors><author><keyname>Syrakos</keyname><forenames>Alexandros</forenames></author><author><keyname>Efthimiou</keyname><forenames>Georgios</forenames></author><author><keyname>Bartzis</keyname><forenames>John G.</forenames></author><author><keyname>Goulas</keyname><forenames>Apostolos</forenames></author></authors><title>Numerical experiments on the efficiency of local grid refinement based
  on truncation error estimates</title><categories>physics.comp-ph cs.NA</categories><journal-ref>Journal of Computational Physics 231 (2012) 6725-6753</journal-ref><doi>10.1016/j.jcp.2012.06.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local grid refinement aims to optimise the relationship between accuracy of
the results and number of grid nodes. In the context of the finite volume
method no single local refinement criterion has been globally established as
optimum for the selection of the control volumes to subdivide, since it is not
easy to associate the discretisation error with an easily computable quantity
in each control volume. Often the grid refinement criterion is based on an
estimate of the truncation error in each control volume, because the truncation
error is a natural measure of the discrepancy between the algebraic
finite-volume equations and the original differential equations. However, it is
not a straightforward task to associate the truncation error with the optimum
grid density because of the complexity of the relationship between truncation
and discretisation errors. In the present work several criteria based on a
truncation error estimate are tested and compared on a regularised lid-driven
cavity case at various Reynolds numbers. It is shown that criteria where the
truncation error is weighted by the volume of the grid cells perform better
than using just the truncation error as the criterion. Also it is observed that
the efficiency of local refinement increases with the Reynolds number. The
truncation error is estimated by restricting the solution to a coarser grid and
applying the coarse grid discrete operator. The complication that high
truncation error develops at grid level interfaces is also investigated and
several treatments are tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02349</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02349</id><created>2015-08-10</created><authors><author><keyname>Mabillard</keyname><forenames>Isaac</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Eliminating Higher-Multiplicity Intersections, I. A Whitney Trick for
  Tverberg-Type Problems</title><categories>math.GT cs.CG cs.DM math.CO</categories><comments>46 pages, 14 figures</comments><msc-class>57Q35, 55S35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by topological Tverberg-type problems and by classical results
about embeddings (maps without double points), we study the question whether a
finite simplicial complex K can be mapped into R^d without triple, quadruple,
or, more generally, r-fold points. Specifically, we are interested in maps f
from K to R^d that have no r-Tverberg points, i.e., no r-fold points with
preimages in r pairwise disjoint simplices of K, and we seek necessary and
sufficient conditions for the existence of such maps.
  We present a higher-multiplicity analogue of the completeness of the Van
Kampen obstruction for embeddability in twice the dimension. Specifically, we
show that under suitable restrictions on the dimensions, a well-known Deleted
Product Criterion (DPC) is not only necessary but also sufficient for the
existence of maps without r-Tverberg points. Our main technical tool is a
higher-multiplicity version of the classical Whitney trick.
  An important guiding idea for our work was that sufficiency of the DPC,
together with an old result of Ozaydin on the existence of equivariant maps,
might yield an approach to disproving the remaining open cases of the
long-standing topological Tverberg conjecture. Unfortunately, our proof of the
sufficiency of the DPC requires a &quot;codimension 3&quot; proviso, which is not
satisfied for when K is the N-simplex.
  Recently, Frick found an extremely elegant way to overcome this last
&quot;codimension 3&quot; obstacle and to construct counterexamples to the topological
Tverberg conjecture for d at least 3r+1 (r not a prime power). Here, we present
a different construction that yields counterexamples for d at least 3r (r not a
prime power).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02353</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02353</id><created>2015-08-06</created><authors><author><keyname>Chuan</keyname><forenames>Luo</forenames></author><author><keyname>Ukil</keyname><forenames>Abhisek</forenames></author></authors><title>Modeling and Validation of Electrical Load Profiling in Residential
  Buildings in Singapore</title><categories>cs.SY</categories><journal-ref>IEEE Transactions on Power Systems, vol. 30, no. 5, pp. 2800-2809,
  2015</journal-ref><doi>10.1109/TPWRS.2014.2367509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand of electricity keeps increasing in this modern society and the
behavior of customers vary greatly from time to time, city to city, type to
type, etc. Generally, buildings are classified into residential, commercial and
industrial. This study is aimed to distinguish the types of residential
buildings in Singapore and establish a mathematical model to represent and
model the load profile of each type. Modeling household energy consumption is
the first step in exploring the possible demand response and load reduction
opportunities under the smart grid initiative. Residential electricity load
profiling includes the details on the electrical appliances, its energy
requirement, and consumption pattern. The model is generated with a bottom-up
load model. Simulation is performed for daily load profiles of 1 or 2 rooms, 3
rooms, 4 rooms and 5 rooms public housing. The simulated load profile is
successfully validated against the measured electricity consumption data, using
a web-based Customer Energy Portal (CEP) at the campus housings of Nanyang
Technological University, Singapore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02354</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02354</id><created>2015-08-10</created><updated>2015-08-13</updated><authors><author><keyname>Cheng</keyname><forenames>Jianpeng</forenames></author><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author></authors><title>Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models
  of Meaning</title><categories>cs.CL cs.AI cs.NE</categories><comments>Accepted for presentation at EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep compositional models of meaning acting on distributional representations
of words in order to produce vectors of larger text constituents are evolving
to a popular area of NLP research. We detail a compositional distributional
framework based on a rich form of word embeddings that aims at facilitating the
interactions between words in the context of a sentence. Embeddings and
composition layers are jointly learned against a generic objective that
enhances the vectors with syntactic information from the surrounding context.
Furthermore, each word is associated with a number of senses, the most
plausible of which is selected dynamically during the composition process. We
evaluate the produced vectors qualitatively and quantitatively with positive
results. At the sentence level, the effectiveness of the framework is
demonstrated on the MSRPar task, for which we report results within the
state-of-the-art range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02373</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02373</id><created>2015-08-10</created><authors><author><keyname>Cao</keyname><forenames>Yuan</forenames></author></authors><title>Training Conditional Random Fields with Natural Gradient Descent</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel parameter estimation procedure that works efficiently for
conditional random fields (CRF). This algorithm is an extension to the maximum
likelihood estimation (MLE), using loss functions defined by Bregman
divergences which measure the proximity between the model expectation and the
empirical mean of the feature vectors. This leads to a flexible training
framework from which multiple update strategies can be derived using natural
gradient descent (NGD). We carefully choose the convex function inducing the
Bregman divergence so that the types of updates are reduced, while making the
optimization procedure more effective by transforming the gradients of the
log-likelihood loss function. The derived algorithms are very simple and can be
easily implemented on top of the existing stochastic gradient descent (SGD)
optimization procedure, yet it is very effective as illustrated by experimental
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02374</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02374</id><created>2015-08-10</created><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Somekh-Baruch</keyname><forenames>Anelia</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>A Counter-Example to the Mismatched Decoding Converse for Binary-Input
  Discrete Memoryless Channels</title><categories>cs.IT math.IT</categories><comments>Extended version of paper accepted to IEEE Transactions on
  Information Theory; rate derivation and numerical algorithms included in
  appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the mismatched decoding problem for binary-input discrete
memoryless channels. An example is provided for which an achievable rate based
on superposition coding exceeds the LM rate (Hui, 1983; Csisz\'ar-K\&quot;orner,
1981), thus providing a counter-example to a previously reported converse
result (Balakirsky, 1995). Both numerical evaluations and theoretical results
are used in establishing this claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02375</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02375</id><created>2015-08-10</created><authors><author><keyname>Gormley</keyname><forenames>Matthew R.</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author><author><keyname>Eisner</keyname><forenames>Jason</forenames></author></authors><title>Approximation-Aware Dependency Parsing by Belief Propagation</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to train the fast dependency parser of Smith and Eisner (2008)
for improved accuracy. This parser can consider higher-order interactions among
edges while retaining O(n^3) runtime. It outputs the parse with maximum
expected recall -- but for speed, this expectation is taken under a posterior
distribution that is constructed only approximately, using loopy belief
propagation through structured factors. We show how to adjust the model
parameters to compensate for the errors introduced by this approximation, by
following the gradient of the actual loss on training data. We find this
gradient by back-propagation. That is, we treat the entire parser
(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)
and Domke (2010) did for loopy CRFs. The resulting trained parser obtains
higher accuracy with fewer iterations of belief propagation than one trained by
conditional log-likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02383</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02383</id><created>2015-08-09</created><authors><author><keyname>Khan</keyname><forenames>Farooq</forenames></author></authors><title>Mobile Internet from the Heavens</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1508.02063</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost two-thirds of the humankind currently does not have access to the
Internet, wired or wireless. We present a Space Internet proposal capable of
providing Zetabyte/ month capacity which is equivalent to 200GB/month for 5
Billion users Worldwide. Our proposal is based on deploying thousands of
low-cost micro-satellites in Low-Earth Orbit (LEO), each capable of providing
Terabit/s data rates with signal latencies better than or equal to ground based
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02387</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02387</id><created>2015-08-10</created><authors><author><keyname>Situngkir</keyname><forenames>Hokky</forenames></author></authors><title>Indonesia embraces the Data Science</title><categories>cs.CY</categories><comments>Paper presented in South East Asian Mathematical Society (SEAMS) 7th
  Conference, 10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information era is the time when information is not only largely
generated, but also vastly processed in order to extract and generated more
information. The complex nature of modern living is represented by the various
kind of data. Data can be in the forms of signals, images, texts, or manifolds
resembling the horizon of observation. The task of the emerging data sciences
are to extract information from the data, for people gain new insights of the
complex world. The insights may came from the new way of the data
representation, be it a visualizations, mapping, or other. The insights may
also come from the implementation of mathematical analysis and or computational
processing giving new insights of what the states of the nature represented by
the data. Both ways implement the methodologies reducing the dimensionality of
the data. The relations between the two functions, representation and analysis
are the heart of how information in data is transformed mathematically and
computationally into new information. The paper discusses some practices, along
with various data coming from the social life in Indonesia to gain new insights
about Indonesia in the emerging data sciences. The data sciences in Indonesia
has made Indonesian Data Cartograms, Indonesian Celebrity Sentiment Mapping,
Ethno-Clustering Maps, social media community detection, and a lot more to
come, become possible. All of these are depicted as the exemplifications on how
Data Science has become integral part of the technology bringing data closer to
people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02388</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02388</id><created>2015-08-10</created><authors><author><keyname>Myasnikov</keyname><forenames>Alexei</forenames></author><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author><author><keyname>Ushakov</keyname><forenames>Alexander</forenames></author></authors><title>Non-commutative lattice problems</title><categories>math.GR cs.CC math.CO</categories><comments>17 pages, 2 figures</comments><msc-class>03D15, 20F65, 20F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider several subgroup-related algorithmic questions in groups, modeled
after the classic computational lattice problems, and study their computational
complexity. We find polynomial time solutions to problems like finding a
subgroup element closest to a given group element, or finding a shortest
non-trivial element of a subgroup in the case of nilpotent groups, and a large
class of surface groups and Coxeter groups. We also provide polynomial time
algorithm to compute geodesics in given generators of a subgroup of a free
group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02405</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02405</id><created>2015-08-10</created><authors><author><keyname>Gholami</keyname><forenames>Farnood</forenames></author><author><keyname>Trojan</keyname><forenames>Daria A.</forenames></author><author><keyname>Kovecses</keyname><forenames>Jozsef</forenames></author><author><keyname>Haddad</keyname><forenames>Wassim M.</forenames></author><author><keyname>Gholami</keyname><forenames>Behnood</forenames></author></authors><title>Gait Assessment for Multiple Sclerosis Patients Using Microsoft Kinect</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gait analysis of patients with neurological disorders, including multiple
sclerosis (MS), is important for rehabilitation and treatment. The Mircrosoft
Kinect sensor, which was developed for motion recognition in gaming
applications, is an ideal candidate for an inexpensive system providing the
capability for human gait analysis. In this research, we develop a framework to
quantify the gait abnormality of MS patients using a Kinect for Windows camera.
In addition to the previously introduced gait indices, a novel set of MS gait
indices based on the concept of dynamic time warping is introduced. The newly
introduced indices can characterize a patient's gait pattern as a whole and
quantify a subject's gait distance from the healthy population. We will
investigate the correlation of gait indices with the multiple sclerosis walking
scale (MSWS) and the clinical ambulation score. This work establishes the
feasibility of using the Kinect sensor for clinical gait assessment for MS
patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02407</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02407</id><created>2015-08-10</created><authors><author><keyname>Yagan</keyname><forenames>Osman</forenames></author></authors><title>Zero-one laws for connectivity in inhomogeneous random key graphs</title><categories>math.PR cs.IT math.IT</categories><comments>Paper submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new random key predistribution scheme for securing
heterogeneous wireless sensor networks. Each of the n sensors in the network is
classified into r classes according to some probability distribution {\mu} =
{{\mu}_1 , . . . , {\mu}_r }. Before deployment, a class-i sensor is assigned
K_i cryptographic keys that are selected uniformly at random from a common pool
of P keys. Once deployed, a pair of sensors can communicate securely if and
only if they have a key in common. We model the communication topology of this
network by a newly defined inhomogeneous random key graph. We establish scaling
conditions on the parameters P and {K_1 , . . . , K_r } so that this graph i)
has no isolated nodes; and ii) is connected, both with high probability. The
results are given in the form of zero-one laws with the number of sensors n
growing unboundedly large; critical scalings are identified and shown to
coincide for both graph properties. Our results are shown to complement and
improve those given by Godehardt et al. and Zhao et al. for the same model,
therein referred to as the general random intersection graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02420</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02420</id><created>2015-08-10</created><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Smooth Boolean functions are easy: efficient algorithms for
  low-sensitivity functions</title><categories>cs.CC</categories><msc-class>68Q15, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A natural measure of smoothness of a Boolean function is its sensitivity (the
largest number of Hamming neighbors of a point which differ from it in function
value). The structure of smooth or equivalently low-sensitivity functions is
still a mystery. A well-known conjecture states that every such Boolean
function can be computed by a shallow decision tree. While this conjecture
implies that smooth functions are easy to compute in the simplest computational
model, to date no non-trivial upper bounds were known for such functions in any
computational model, including unrestricted Boolean circuits. Even a bound on
the description length of such functions better than the trivial $2^n$ does not
seem to have been known.
  In this work, we establish the first computational upper bounds on smooth
Boolean functions:
  1) We show that every sensitivity s function is uniquely specified by its
values on a Hamming ball of radius 2s. We use this to show that such functions
can be computed by circuits of size $n^{O(s)}$.
  2) We show that sensitivity s functions satisfy a strong pointwise
noise-stability guarantee for random noise of rate O(1/s). We use this to show
that these functions have formulas of depth O(s log n).
  3) We show that sensitivity s functions can be (locally) self-corrected from
worst-case noise of rate $\exp(-O(s))$.
  All our results are simple, and follow rather directly from (variants of) the
basic fact that that the function value at few points in small neighborhoods of
a given point determine its function value via a majority vote. Our results
confirm various consequences of the conjecture. They may be viewed as providing
a new form of evidence towards its validity, as well as new directions towards
attacking it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02428</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02428</id><created>2015-08-10</created><authors><author><keyname>Schulte</keyname><forenames>Oliver</forenames></author><author><keyname>Qian</keyname><forenames>Zhensong</forenames></author></authors><title>FactorBase: SQL for Learning A Multi-Relational Graphical Model</title><categories>cs.DB cs.LG</categories><comments>14 pages, 10 figures, 10 tables, Published on 2015 IEEE International
  Conference on Data Science and Advanced Analytics (IEEE DSAA'2015), Oct
  19-21, 2015, Paris, France</comments><acm-class>H.2.8; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe FactorBase, a new SQL-based framework that leverages a relational
database management system to support multi-relational model discovery. A
multi-relational statistical model provides an integrated analysis of the
heterogeneous and interdependent data resources in the database. We adopt the
BayesStore design philosophy: statistical models are stored and managed as
first-class citizens inside a database. Whereas previous systems like
BayesStore support multi-relational inference, FactorBase supports
multi-relational learning. A case study on six benchmark databases evaluates
how our system supports a challenging machine learning application, namely
learning a first-order Bayesian network model for an entire database. Model
learning in this setting has to examine a large number of potential statistical
associations across data tables. Our implementation shows how the SQL
constructs in FactorBase facilitate the fast, modular, and reliable development
of highly scalable model learning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02435</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02435</id><created>2015-08-10</created><updated>2015-10-05</updated><authors><author><keyname>Weinzierl</keyname><forenames>T.</forenames></author><author><keyname>Verleye</keyname><forenames>B.</forenames></author><author><keyname>Henri</keyname><forenames>P.</forenames></author><author><keyname>Roose</keyname><forenames>D.</forenames></author></authors><title>Two Particle-in-Grid Realisations on Spacetrees</title><categories>cs.DS</categories><journal-ref>Parallel Computing, 2016</journal-ref><doi>10.1016/j.parco.2015.12.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper studies two particle management strategies for dynamically
adaptive Cartesian grids at hands of a particle-in-cell code. One holds the
particles within the grid cells, the other within the grid vertices. The
fundamental challenge for the algorithmic strategies results from the fact that
particles may run through the grid without velocity constraints. To facilitate
this, we rely on multiscale grid representations. They allow us to lift and
drop particles between different spatial resolutions. We call this cell-based
strategy particle in tree (PIT). Our second approach assigns particles to
vertices describing a dual grid (PIDT) and augments the lifts and drops with
multiscale linked cells.
  Our experiments validate the two schemes at hands of an electrostatic
particle-in-cell code by retrieving the dispersion relation of Langmuir waves
in a thermal plasma. They reveal that different particle and grid
characteristics favour different realisations. The possibility that particles
can tunnel through an arbitrary number of grid cells implies that most data is
exchanged between neighbouring ranks, while very few data is transferred
non-locally. This constraints the scalability as the code potentially has to
realise global communication. We show that the merger of an analysed tree
grammar with PIDT allows us to predict particle movements among several levels
and to skip parts of this global communication a priori. It is capable to
outperform several established implementations based upon trees and/or
space-filling curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02439</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02439</id><created>2015-08-10</created><updated>2015-10-06</updated><authors><author><keyname>Wang</keyname><forenames>Di</forenames></author><author><keyname>Rao</keyname><forenames>Satish</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Unified Acceleration Method for Packing and Covering Problems via
  Diameter Reduction</title><categories>cs.DS cs.NA</categories><comments>Fixed typo in packing LP formulation (page 1), and wrong citation in
  the discussion of earlier works on page 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear coupling method was introduced recently by Allen-Zhu and Orecchia
for solving convex optimization problems with first order methods, and it
provides a conceptually simple way to integrate a gradient descent step and
mirror descent step in each iteration. The high-level approach of the linear
coupling method is very flexible, and it has shown initial promise by providing
improved algorithms for packing and covering linear programs. Somewhat
surprisingly, however, while the dependence of the convergence rate on the
error parameter $\epsilon$ for packing problems was improved to
$O(1/\epsilon)$, which corresponds to what accelerated gradient methods are
designed to achieve, the dependence for covering problems was only improved to
$O(1/\epsilon^{1.5})$, and even that required a different more complicated
algorithm. Given the close connections between packing and covering problems
and since previous algorithms for these very related problems have led to the
same $\epsilon$ dependence, this discrepancy is surprising, and it leaves open
the question of the exact role that the linear coupling is playing in
coordinating the complementary gradient and mirror descent step of the
algorithm. In this paper, we clarify these issues for linear coupling
algorithms for packing and covering linear programs, illustrating that the
linear coupling method can lead to improved $O(1/\epsilon)$ dependence for both
packing and covering problems in a unified manner, i.e., with the same
algorithm and almost identical analysis. Our main technical result is a novel
diameter reduction method for covering problems that is of independent interest
and that may be useful in applying the accelerated linear coupling method to
other combinatorial problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02440</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02440</id><created>2015-08-10</created><updated>2015-09-07</updated><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author></authors><title>Energy Structure of Optimal Positional Strategies in Mean Payoff Games</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies structural aspects concerning optimal positional strategies
in Mean Payoff Games, it is a contribution to understanding the relationship
between optimal positional strategies in MPGs and small energy-progress
measures in reweighted energy games. In particular, it is observed an energy
decomposition theorem which describes the whole space of all optimal positional
strategies in terms of so-called extremal small energy-progress measures. This
points out what we called the &quot;energy structure&quot; of the space of all optimal
positional strategies in MPGs, as it allows for a disjoint-set decomposition
(of that space) which is expressed in terms of small energy-progress measures
in reweighted energy games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02445</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02445</id><created>2015-08-10</created><authors><author><keyname>Stanojevi&#x107;</keyname><forenames>Milo&#x161;</forenames></author></authors><title>Removing Biases from Trainable MT Metrics by Using Self-Training</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most trainable machine translation (MT) metrics train their weights on human
judgments of state-of-the-art MT systems outputs. This makes trainable metrics
biases in many ways. One of them is preferring longer translations. These
biased metrics when used for tuning are evaluating different types of
translations -- n-best lists of translations with very diverse quality. Systems
tuned with these metrics tend to produce overly long translations that are
preferred by the metric but not by humans. This is usually solved by manually
tweaking metric's weights to equally value recall and precision. Our solution
is more general: (1) it does not address only the recall bias but also all
other biases that might be present in the data and (2) it does not require any
knowledge of the types of features used which is useful in cases when manual
tuning of metric's weights is not possible. This is accomplished by
self-training on unlabeled n-best lists by using metric that was initially
trained on standard human judgments. One way of looking at this is as domain
adaptation from the domain of state-of-the-art MT translations to diverse
n-best list translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02448</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02448</id><created>2015-08-10</created><authors><author><keyname>Chowdhury</keyname><forenames>Omar</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Jia</keyname><forenames>Limin</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>Equivalence-based Security for Querying Encrypted Databases: Theory and
  Application to Privacy Policy Audits</title><categories>cs.CR</categories><comments>CCS 2015 paper technical report, in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of simultaneously preserving confidentiality and
usability of data outsourced to third-party clouds, we present two different
database encryption schemes that largely hide data but reveal enough
information to support a wide-range of relational queries. We provide a
security definition for database encryption that captures confidentiality based
on a notion of equivalence of databases from the adversary's perspective. As a
specific application, we adapt an existing algorithm for finding violations of
privacy policies to run on logs encrypted under our schemes and observe low to
moderate overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02449</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02449</id><created>2015-08-10</created><updated>2015-10-01</updated><authors><author><keyname>Owhadi</keyname><forenames>Houman</forenames></author><author><keyname>Scovel</keyname><forenames>Clint</forenames></author></authors><title>Towards Machine Wald</title><categories>math.ST cs.LG stat.TH</categories><comments>37 pages</comments><msc-class>62C99, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past century has seen a steady increase in the need of estimating and
predicting complex systems and making (possibly critical) decisions with
limited information. Although computers have made possible the numerical
evaluation of sophisticated statistical models, these models are still designed
\emph{by humans} because there is currently no known recipe or algorithm for
dividing the design of a statistical model into a sequence of arithmetic
operations. Indeed enabling computers to \emph{think} as \emph{humans} have the
ability to do when faced with uncertainty is challenging in several major ways:
(1) Finding optimal statistical models remains to be formulated as a well posed
problem when information on the system of interest is incomplete and comes in
the form of a complex combination of sample data, partial knowledge of
constitutive relations and a limited description of the distribution of input
random variables. (2) The space of admissible scenarios along with the space of
relevant information, assumptions, and/or beliefs, tend to be infinite
dimensional, whereas calculus on a computer is necessarily discrete and finite.
With this purpose, this paper explores the foundations of a rigorous framework
for the scientific computation of optimal statistical estimators/models and
reviews their connections with Decision Theory, Machine Learning, Bayesian
Inference, Stochastic Optimization, Robust Optimization, Optimal Uncertainty
Quantification and Information Based Complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02452</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02452</id><created>2015-08-10</created><authors><author><keyname>Han</keyname><forenames>Zheng</forenames></author><author><keyname>Curtis</keyname><forenames>Frank E.</forenames></author></authors><title>Primal-Dual Active-Set Methods for Isotonic Regression and Trend
  Filtering</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isotonic regression (IR) is a non-parametric calibration method used in
supervised learning. For performing large-scale IR, we propose a primal-dual
active-set (PDAS) algorithm which, in contrast to the state-of-the-art Pool
Adjacent Violators (PAV) algorithm, can be parallized and is easily
warm-started thus well-suited in the online settings. We prove that, like the
PAV algorithm, our PDAS algorithm for IR is convergent and has a work
complexity of O(n), though our numerical experiments suggest that our PDAS
algorithm is often faster than PAV. In addition, we propose PDAS variants (with
safeguarding to ensure convergence) for solving related trend filtering (TF)
problems, providing the results of experiments to illustrate their
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02454</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02454</id><created>2015-08-10</created><updated>2016-01-20</updated><authors><author><keyname>Wang</keyname><forenames>Xing</forenames></author><author><keyname>Liang</keyname><forenames>Jie</forenames></author></authors><title>Multi-Resolution Compressed Sensing via Approximate Message Passing</title><categories>cs.IT math.IT</categories><comments>36 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of multi-resolution compressed sensing
(MR-CS) reconstruction, which has received little attention in the literature.
Instead of always reconstructing the signal at the original high resolution
(HR), we enable the reconstruction of a low-resolution (LR) signal when there
are not enough CS samples to recover a HR signal. We propose an approximate
message passing (AMP)-based framework dubbed MR-AMP, and derive its state
evolution, phase transition, and noise sensitivity, which show that in addition
to reduced complexity, our method can recover a LR signal with bounded noise
sensitivity even when the noise sensitivity of the conventional HR
reconstruction is unbounded. We then apply the MR-AMP to image reconstruction
using either soft-thresholding or total variation denoiser, and develop three
pairs of up-/down-sampling operators in transform or spatial domain. The
performance of the proposed scheme is demonstrated by both 1D synthetic data
and 2D images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02470</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02470</id><created>2015-08-10</created><authors><author><keyname>Isaac</keyname><forenames>Tobin</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author></authors><title>Support for Non-conformal Meshes in PETSc's DMPlex Interface</title><categories>cs.MS</categories><comments>16 pages, 13 figures, 5 code examples</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PETSc's DMPlex interface for unstructured meshes has been extended to support
non-conformal meshes. The topological construct that DMPlex implements---the
CW-complex---is by definition conformal, so representing non- conformal meshes
in a way that hides complexity requires careful attention to the interface
between DMPlex and numerical methods such as the finite element method. Our
approach---which combines a tree structure for subset- superset relationships
and a &quot;reference tree&quot; describing the types of non-conformal
interfaces---allows finite element code written for conformal meshes to extend
automatically: in particular, all &quot;hanging-node&quot; constraint calculations are
handled behind the scenes. We give example code demonstrating the use of this
extension, and use it to convert forests of quadtrees and forests of octrees
from the p4est library to DMPlex meshes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02471</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02471</id><created>2015-08-10</created><authors><author><keyname>Miller</keyname><forenames>Avery</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Time Versus Cost Tradeoffs for Deterministic Rendezvous in Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two mobile agents, starting from different nodes of a network at possibly
different times, have to meet at the same node. This problem is known as
$\mathit{rendezvous}$. Agents move in synchronous rounds. Each agent has a
distinct integer label from the set $\{1,\dots,L\}$. Two main efficiency
measures of rendezvous are its $\mathit{time}$ (the number of rounds until the
meeting) and its $\mathit{cost}$ (the total number of edge traversals). We
investigate tradeoffs between these two measures. A natural benchmark for both
time and cost of rendezvous in a network is the number of edge traversals
needed for visiting all nodes of the network, called the exploration time.
Hence we express the time and cost of rendezvous as functions of an upper bound
$E$ on the time of exploration (where $E$ and a corresponding exploration
procedure are known to both agents) and of the size $L$ of the label space. We
present two natural rendezvous algorithms. Algorithm $\mathtt{Cheap}$ has cost
$O(E)$ (and, in fact, a version of this algorithm for the model where the
agents start simultaneously has cost exactly $E$) and time $O(EL)$. Algorithm
$\mathtt{Fast}$ has both time and cost $O(E\log L)$. Our main contributions are
lower bounds showing that, perhaps surprisingly, these two algorithms capture
the tradeoffs between time and cost of rendezvous almost tightly. We show that
any deterministic rendezvous algorithm of cost asymptotically $E$ (i.e., of
cost $E+o(E)$) must have time $\Omega(EL)$. On the other hand, we show that any
deterministic rendezvous algorithm with time complexity $O(E\log L)$ must have
cost $\Omega (E\log L)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02477</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02477</id><created>2015-08-10</created><updated>2015-11-10</updated><authors><author><keyname>Banerjee</keyname><forenames>Indranil</forenames></author><author><keyname>Richards</keyname><forenames>Dana</forenames></author></authors><title>Computing Maximal Layers Of Points in $E^{f(n)}$</title><categories>cs.CG cs.DS</categories><comments>13 pages, submitted to LATIN 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a randomized algorithm for computing the collection
of maximal layers for a point set in $E^{k}$ ($k = f(n)$). The input to our
algorithm is a point set $P = \{p_1,...,p_n\}$ with $p_i \in E^{k}$. The
proposed algorithm achieves a runtime of $O\left(kn^{2 - {1 \over \log{k}} +
\log_k{\left(1 + {2 \over {k+1}}\right)}}\log{n}\right)$ when $P$ is a random
order and a runtime of $O(k^2 n^{3/2 + (\log_{k}{(k-1)})/2}\log{n})$ for an
arbitrary $P$. Both bounds hold in expectation. Additionally, the run time is
bounded by $O(kn^2)$ in the worst case. This is the first non-trivial algorithm
whose run-time remains polynomial whenever $f(n)$ is bounded by some polynomial
in $n$ while remaining sub-quadratic in $n$ for constant $k$. The algorithm is
implemented using a new data-structure for storing and answering dominance
queries over the set of incomparable points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02479</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02479</id><created>2015-08-10</created><updated>2016-03-04</updated><authors><author><keyname>Choi</keyname><forenames>Heejin</forenames></author><author><keyname>Sasaki</keyname><forenames>Yutaka</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Normalized Hierarchical SVM</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present improved methods of using structured SVMs in a large-scale
hierarchical classification problem, that is when labels are leaves, or sets of
leaves, in a tree or a DAG. We examine the need to normalize both the
regularization and the margin and show how doing so significantly improves
performance, including allowing achieving state-of-the-art results where
unnormalized structured SVMs do not perform better than flat models. We also
describe a further extension of hierarchical SVMs that highlight the connection
between hierarchical SVMs and matrix factorization models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02483</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02483</id><created>2015-08-11</created><authors><author><keyname>van der Veen</keyname><forenames>Han</forenames></author><author><keyname>Hiemstra</keyname><forenames>Djoerd</forenames></author><author><keyname>Broek</keyname><forenames>Tijs van den</forenames></author><author><keyname>Ehrenhard</keyname><forenames>Michel</forenames></author><author><keyname>Need</keyname><forenames>Ariana</forenames></author></authors><title>Determine the User Country of a Tweet</title><categories>cs.SI</categories><comments>CTIT Technical Report, University of Twente</comments><report-no>tr-ctit-15-05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the widely used message platform Twitter, about 2% of the tweets contains
the geographical location through exact GPS coordinates (latitude and
longitude). Knowing the location of a tweet is useful for many data analytics
questions. This research is looking at the determination of a location for
tweets that do not contain GPS coordinates. An accuracy of 82% was achieved
using a Naive Bayes model trained on features such as the users' timezone, the
user's language, and the parsed user location. The classifier performs well on
active Twitter countries such as the Netherlands and United Kingdom. An
analysis of errors made by the classifier shows that mistakes were made due to
limited information and shared properties between countries such as shared
timezone. A feature analysis was performed in order to see the effect of
different features. The features timezone and parsed user location were the
most informative features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02485</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02485</id><created>2015-08-11</created><authors><author><keyname>Xiang</keyname><forenames>Chengdi</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author></authors><title>Guaranteed Cost Dynamic Coherent Control for Uncertain Linear Quantum
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns a class of uncertain linear quantum systems subject to
quadratic perturbations in the system Hamiltonian. A small gain approach is
used to evaluate the performance of the given quantum system. In order to get
improved control performance, we propose two methods to design a coherent
controller for the system. One is to formulate a static quantum controller by
adding a controller Hamiltonian to the given system, and the other is to build
a dynamic quantum controller which is directly coupled to the given system.
Both controller design methods are given in terms of LMIs and a non-convex
equality. Hence, a rank constrained LMI method is used as a numerical
procedure. An illustrative example is given to demonstrate the proposed methods
and also to make a performance comparison with different controller design
methods. Results show that for the same uncertain quantum system, the dynamic
quantum controller can offer an improvement in performance over the static
quantum controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02489</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02489</id><created>2015-08-11</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author><author><keyname>Nguyen</keyname><forenames>Hung Dinh</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Probabilistic Power Flow Computation via Low-Rank and Sparse Tensor
  Recovery</title><categories>cs.CE math.ST stat.TH</categories><comments>8 pages, 10 figures, submitted to IEEE Trans. Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tensor-recovery method to solve probabilistic power
flow problems. Our approach generates a high-dimensional and sparse generalized
polynomial-chaos expansion that provides useful statistical information. The
result can also speed up other essential routines in power systems (e.g.,
stochastic planning, operations and controls).
  Instead of simulating a power flow equation at all quadrature points, our
approach only simulates an extremely small subset of samples. We suggest a
model to exploit the underlying low-rank and sparse structure of
high-dimensional simulation data arrays, making our technique applicable to
power systems with many random parameters. We also present a numerical method
to solve the resulting nonlinear optimization problem.
  Our algorithm is implemented in MATLAB and is verified by several benchmarks
in MATPOWER $5.1$. Accurate results are obtained for power systems with up to
$50$ independent random parameters, with a speedup factor up to $9\times
10^{20}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02492</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02492</id><created>2015-08-11</created><authors><author><keyname>Rosnes</keyname><forenames>Eirik</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>Analysis of Spatially-Coupled Counter Braids</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Information Theory Workshop, Jeju Island,
  Korea, October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A counter braid (CB) is a novel counter architecture introduced by Lu et al.
in 2007 for per-flow measurements on high-speed links. CBs achieve an
asymptotic compression rate (under optimal decoding) that matches the entropy
lower bound of the flow size distribution. Spatially-coupled CBs (SC-CBs) have
recently been proposed. In this work, we further analyze single-layer CBs and
SC-CBs using an equivalent bipartite graph representation of CBs. On this
equivalent representation, we show that the potential and area thresholds are
equal. We also show that the area under the extended belief propagation (BP)
extrinsic information transfer curve (defined for the equivalent graph),
computed for the expected residual CB graph when a peeling decoder equivalent
to the BP decoder stops, is equal to zero precisely at the area threshold.
This, combined with simulations and an asymptotic analysis of the Maxwell
decoder, leads to the conjecture that the area threshold is in fact equal to
the Maxwell decoding threshold and hence a lower bound on the maximum a
posteriori (MAP) decoding threshold. Finally, we present some numerical results
and give some insight into the apparent gap of the BP decoding threshold of
SC-CBs to the conjectured lower bound on the MAP decoding threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02495</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02495</id><created>2015-08-11</created><authors><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Movahednasab</keyname><forenames>Mohammad</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>On ISI-free Modulations for Diffusion based Molecular Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A diffusion molecular channel is a channel with memory, as molecules released
into the medium hit the receptors after a random delay. Coding over the
diffusion channel is performed by choosing the type, intensity, or the released
time of molecules diffused in the environment over time. To avoid intersymbol
interference (ISI), molecules of the same type should be released at time
instances that are sufficiently far apart. This ensures that molecules of a
previous transmission are faded in the environment, before molecules of the
same type are reused for signaling. In this paper, we consider ISI-free
time-slotted modulation schemes. The maximum reliable transmission rate for
these modulations is given by the constrained coding capacity of the graph that
represents the permissible transmission sequences. However, achieving the
constrained coding capacity requires long blocklengths and delays at the
decoder, making it impractical for simple nanomachines. The main contribution
of this paper is to consider modulations with small delay (short blocklength)
and show that they get very close to constrained coding capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02496</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02496</id><created>2015-08-11</created><updated>2015-08-25</updated><authors><author><keyname>Chandrasekhar</keyname><forenames>Vijay</forenames></author><author><keyname>Lin</keyname><forenames>Jie</forenames></author><author><keyname>Mor&#xe8;re</keyname><forenames>Olivier</forenames></author><author><keyname>Goh</keyname><forenames>Hanlin</forenames></author><author><keyname>Veillard</keyname><forenames>Antoine</forenames></author></authors><title>A Practical Guide to CNNs and Fisher Vectors for Image Instance
  Retrieval</title><categories>cs.CV cs.IR</categories><comments>Deep Convolutional Neural Networks for instance retrieval, Fisher
  Vectors, instance retrieval</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With deep learning becoming the dominant approach in computer vision, the use
of representations extracted from Convolutional Neural Nets (CNNs) is quickly
gaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global
image descriptors for image instance retrieval. While the good performance of
CNNs for image classification are unambiguously recognised, which of the two
has the upper hand in the image retrieval context is not entirely clear yet. In
this work, we propose a comprehensive study that systematically evaluates FVs
and CNNs for image retrieval. The first part compares the performances of FVs
and CNNs on multiple publicly available data sets. We investigate a number of
details specific to each method. For FVs, we compare sparse descriptors based
on interest point detectors with dense single-scale and multi-scale variants.
For CNNs, we focus on understanding the impact of depth, architecture and
training data on retrieval results. Our study shows that no descriptor is
systematically better than the other and that performance gains can usually be
obtained by using both types together. The second part of the study focuses on
the impact of geometrical transformations such as rotations and scale changes.
FVs based on interest point detectors are intrinsically resilient to such
transformations while CNNs do not have a built-in mechanism to ensure such
invariance. We show that performance of CNNs can quickly degrade in presence of
rotations while they are far less affected by changes in scale. We then propose
a number of ways to incorporate the required invariances in the CNN pipeline.
Overall, our work is intended as a reference guide offering practically useful
and simply implementable guidelines to anyone looking for state-of-the-art
global descriptors best suited to their specific image instance retrieval
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02497</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02497</id><created>2015-08-11</created><updated>2015-08-12</updated><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Elwes</keyname><forenames>Richard</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andy</forenames></author></authors><title>Minority population in the one-dimensional Schelling model of
  segregation</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Schelling model of segregation looks to explain the way in which a
population of agents or particles of two types may come to organise itself into
large homogeneous clusters, and can be seen as a variant of the Ising model in
which the system is subjected to rapid cooling. While the model has been very
extensively studied, the unperturbed (noiseless) version has largely resisted
rigorous analysis, with most results in the literature pertaining to versions
of the model in which noise is introduced into the dynamics so as to make it
amenable to standard techniques from statistical mechanics or stochastic
evolutionary game theory. We rigorously analyse the one-dimensional version of
the model in which one of the two types is in the minority, and establish
various forms of threshold behaviour. Our results are in sharp contrast with
the case when the distribution of the two types is uniform (i.e. each agent has
equal chance of being of each type in the initial configuration), which was
studied by Brandt, Immorlica, Kamath, and Kleinberg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02505</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02505</id><created>2015-08-11</created><authors><author><keyname>Keyvanara</keyname><forenames>Maryam</forenames></author><author><keyname>Monadjemi</keyname><forenames>Seyed Amirhassan</forenames></author></authors><title>Simulating Brain Reaction to Methamphetamine Regarding Consumer
  Personality</title><categories>cs.NE q-bio.NC</categories><comments>10 Pages, 4 Figures, Journal Paper</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA) Vol. 6, No. 4, July 2015, pp. 63-72</journal-ref><doi>10.5121/ijaia.2015.6406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Addiction, as a nervous disease, can be analysed using mathematical modelling
and computer simulations. In this paper, we use an existing mathematical model
to predict and simulate human brain response to the consumption of a single
dose of methamphetamine. The model is implemented and coded in Matlab. Three
types of personalities including introverts, ambiverts and extroverts are
studied. The parameters of the mathematical model are calibrated and optimized,
according to psychological theories, using a real coded genetic algorithm. The
simulations show significant correlation between people response to
methamphetamine abuse and their personality. They also show that one of the
causes of tendency to stimulants roots in consumers personality traits. The
results can be used as a tool for reducing attitude towards addiction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02506</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02506</id><created>2015-08-11</created><authors><author><keyname>Martins</keyname><forenames>R. C.</forenames></author><author><keyname>Fachada</keyname><forenames>N.</forenames></author></authors><title>Finite Element Procedures for Enzyme, Chemical Reaction and 'In-Silico'
  Genome Scale Networks</title><categories>cs.CE q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity to predict and control bioprocesses is perhaps one of the most
important objectives of biotechnology. Computational simulation is an
established methodology for the design and optimization of bioprocesses, where
the finite elements method (FEM) is at the state-of-art engineering
multi-physics simulation system, with tools such as Finite Element Analysis
(FEA) and Computational Fluid Dynamics (CFD). Although FEA and CFD are
currently applied to bioreactor design, most simulations are restricted to the
multi-physics capabilities of the existing sofware packages. This manuscript is
a contribution for the consolidation of FEM in computational biotechnology, by
presenting a comprehensive review of finite element procedures of the most
common enzymatic mechanisms found in biotechnological processes, such as,
enzyme activation, Michaelis Menten, competitive inhibition, non-competitive
inhibition, anti-competitive inhibition, competition by substrate, sequential
random mechanism, ping-pong bi-bi and Theorel-Chance. Most importantly, the
manuscript opens the possibility for the use of FEM in conjunction with
{\guillemotleft}in-silico{\guillemotright} models of metabolic networks, as
well as, chemical networks in order to simulate complex bioprocesses in
biotechnology, putting emphasis into flux balance analysis, pheno-metabolomics
space exploration in time and space, overcoming the limitations of assuming
chemostat conditions in systems biology computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02517</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02517</id><created>2015-08-11</created><authors><author><keyname>Bos</keyname><forenames>Arie</forenames></author><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author></authors><title>Hyperorthogonal well-folded Hilbert curves</title><categories>cs.CG</categories><comments>Manuscript submitted to Journal of Computational Geometry. An
  abstract appeared in the 31st Int Symp on Computational Geometry (SoCG 2015),
  LIPIcs 34:812-826</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  R-trees can be used to store and query sets of point data in two or more
dimensions. An easy way to construct and maintain R-trees for two-dimensional
points, due to Kamel and Faloutsos, is to keep the points in the order in which
they appear along the Hilbert curve. The R-tree will then store bounding boxes
of points along contiguous sections of the curve, and the efficiency of the
R-tree depends on the size of the bounding boxes---smaller is better. Since
there are many different ways to generalize the Hilbert curve to higher
dimensions, this raises the question which generalization results in the
smallest bounding boxes. Familiar methods, such as the one by Butz, can result
in curve sections whose bounding boxes are a factor $\Omega(2^{d/2})$ larger
than the volume traversed by that section of the curve. Most of the volume
bounded by such bounding boxes would not contain any data points. In this paper
we present a new way of generalizing Hilbert's curve to higher dimensions,
which results in much tighter bounding boxes: they have at most 4 times the
volume of the part of the curve covered, independent of the number of
dimensions. Moreover, we prove that a factor 4 is asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02521</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02521</id><created>2015-08-11</created><authors><author><keyname>Ullah</keyname><forenames>Sajid</forenames></author><author><keyname>Wahid</keyname><forenames>Mussarat</forenames></author></authors><title>Topology Control of wireless sensor network using Quantum Inspired
  Genetic algorithm</title><categories>cs.NE cs.NI</categories><comments>4 Figures/6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, an evolving Linked Quantum register has been introduced, which
are group vector of binary pair of genes, which in its local proximity
represent those nodes that will have high connectivity and keep the energy
consumption at low, and which are taken into account for topology control. The
register works in higher dimension. Here order-2 Quantum inspired genetic
algorithm has been used and also higher order can be used to achieve greater
versatility in topology control of nodes. Numerical result has been obtained,
analysis is done as how the result has previously been obtained with Quantum
genetic algorithm and results are compared too. For future work, factor is
hinted which would exploit the algorithm to work in more computational
intensive problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02526</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02526</id><created>2015-08-11</created><authors><author><keyname>Grispos</keyname><forenames>George</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author><author><keyname>Storer</keyname><forenames>Tim</forenames></author></authors><title>Security Incident Response Criteria: A Practitioner's Perspective</title><categories>cs.CR cs.CY</categories><comments>The 21st Americas Conference on Information Systems (AMCIS 2015),
  Puerto Rico, USA.
  http://aisel.aisnet.org/amcis2015/ISSecurity/GeneralPresentations/35/. August
  13-15, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial reports indicate that security incidents continue to inflict large
financial losses on organizations. Researchers and industrial analysts contend
that there are fundamental problems with existing security incident response
process solutions. This paper presents the Security Incident Response Criteria
(SIRC) which can be applied to a variety of security incident response
approaches. The criteria are derived from empirical data based on in-depth
interviews conducted within a Global Fortune 500 organization and supporting
literature. The research contribution of this paper is twofold. First, the
criteria presented in this paper can be used to evaluate existing security
incident response solutions and second, as a guide, to support future security
incident response improvement initiatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02535</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02535</id><created>2015-08-11</created><authors><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Rybicki</keyname><forenames>Joel</forenames></author></authors><title>Efficient counting with optimal resilience</title><categories>cs.DC</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the synchronous $c$-counting problem, we are given a synchronous system of
$n$ nodes, where up to $f$ of the nodes may be Byzantine, that is, have
arbitrary faulty behaviour. The task is to have all of the correct nodes count
modulo $c$ in unison in a self-stabilising manner: regardless of the initial
state of the system and the faulty nodes' behavior, eventually rounds are
consistently labelled by a counter modulo $c$ at all correct nodes.
  We provide a deterministic solution with resilience $f&lt;n/3$ that stabilises
in $O(f)$ rounds and every correct node broadcasts $O(\log^2 f)$ bits per
round. We build and improve on a recent result offering stabilisation time
$O(f)$ and communication complexity $O(\log^2 f /\log \log f)$ but with
sub-optimal resilience $f = n^{1-o(1)}$ (PODC 2015). Our new algorithm has
optimal resilience, asymptotically optimal stabilisation time, and low
communication complexity.
  Finally, we modify the algorithm to guarantee that after stabilisation very
little communication occurs. In particular, for optimal resilience and
polynomial counter size $c=n^{O(1)}$, the algorithm broadcasts only $O(1)$ bits
per node every $\Theta(n)$ rounds without affecting the other properties of the
algorithm; communication-wise this is asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02550</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02550</id><created>2015-08-11</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Sir&#xe9;n</keyname><forenames>Jouni</forenames></author></authors><title>Relative Compressed Suffix Trees</title><categories>cs.DS</categories><comments>The implementation is available at
  https://github.com/jltsiren/relative-fm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suffix trees are one of the most versatile data structures in stringology,
with many applications in bioinformatics. Their main drawback is their size,
which can be tens of times larger than the input sequence. Much effort has been
put into reducing the space usage, leading ultimately to compressed suffix
trees. These compressed data structures can efficiently simulate the suffix
tree, while using space proportional to a compressed representation of the
sequence. In this work, we take a new approach to compressed suffix trees for
repetitive sequence collections, such as collections of individual genomes. We
compress the suffix trees of individual sequences relative to the suffix tree
of a reference sequence. These relative data structures provide competitive
time/space trade-offs, being almost as small as the smallest compressed suffix
trees for repetitive collections, and almost as fast as the largest and fastest
compressed suffix trees. They also provide access to the suffix trees of
individual sequences, instead of storing all sequences in the same tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02552</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02552</id><created>2015-08-11</created><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Sadaf</keyname><forenames>Kishwar</forenames></author></authors><title>Web Search Result Clustering based on Heuristic Search and k-means</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Giving user a simple and well organized web search result has been a topic of
active information Retrieval (IR) research. Irrespective of how small or
ambiguous a query is, a user always wants the desired result on the first
display of an IR system. Clustering of an IR system result can render a way,
which fulfills the actual information need of a user. In this paper, an
approach to cluster an IR system result is presented.The approach is a
combination of heuristics and k-means technique using cosine similarity. Our
heuristic approach detects the initial value of k for creating initial
centroids. This eliminates the problem of external specification of the value
k, which may lead to unwanted result if wrongly specified. The centroids
created in this way are more specific and meaningful in the context of web
search result. Another advantage of the proposed method is the removal of the
objective means function of k-means which makes cluster sizes same. The end
result of the proposed approach consists of different clusters of documents
having different sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02556</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02556</id><created>2015-08-11</created><updated>2015-11-30</updated><authors><author><keyname>Madue&#xf1;o</keyname><forenames>Germ&#xe1;n C.</forenames></author><author><keyname>Nielsen</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>&#x10c;edomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Assessment of LTE Wireless Access for Monitoring of Energy Distribution
  in the Smart Grid</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted; v2: revised after review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While LTE is becoming widely rolled out for human-type services, it is also a
promising solution for cost-efficient connectivity of the smart grid monitoring
equipment. This is a type of machine-to-machine (M2M) traffic that consists
mainly of sporadic uplink transmissions. In such a setting, the amount of
traffic that can be served in a cell is not constrained by the data capacity,
but rather by the signaling constraints in the random access channel and
control channel. In this paper we explore these limitations using a detailed
simulation of the LTE access reservation protocol (ARP). We find that 1)
assigning more random access opportunities may actually worsen performance; and
2) the additional signaling that follows the ARP has very large impact on the
capacity in terms of the number of supported devices; we observed a reduction
in the capacity by almost a factor of 3. This suggests that a lightweight
access method, with a reduced number of signaling messages, needs to be
considered in standardization for M2M applications. Additionally we propose a
tractable analytical model to calculate the outage that can be rapidly
implemented and evaluated. The model accounts for the features of the random
access, control channel and uplink and downlink data channels, as well as
retransmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02557</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02557</id><created>2015-08-11</created><authors><author><keyname>Bothra</keyname><forenames>Neha</forenames></author><author><keyname>Jain</keyname><forenames>Kritika</forenames></author><author><keyname>Chakraborty</keyname><forenames>Sanjay</forenames></author></authors><title>Can JSP Code be Generated Using XML Tags?</title><categories>cs.OH</categories><doi>10.15864/ajac.v2i3.138</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Over the years, a variety of web services have started using server-side
scripting to deliver results back to a client as a paid or free service; one
such server-side scripting language is Java Server Pages (JSP). Also Extensible
markup language (XML), is being adopted by most web developers as a tool to
describe data.Therefore, we present a conversion method which uses predefined
XML tags as input and generates the corresponding JSP code. However, the end
users are required to have a basic experience with web pages. This conversion
method aims to reduce the time and effort spent by the user (web developer) to
get acquainted with JSP. The conversion process abstracts the user from the
intricacies of JSP and enables him to focus on the business logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02558</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02558</id><created>2015-08-11</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Prades</keyname><forenames>Javier</forenames></author><author><keyname>Reano</keyname><forenames>Carlos</forenames></author><author><keyname>Silla</keyname><forenames>Federico</forenames></author></authors><title>Acceleration-as-a-Service: Exploiting Virtualised GPUs for a Financial
  Application</title><categories>cs.DC cs.CE</categories><comments>11th IEEE International Conference on eScience (IEEE eScience) -
  Munich, Germany, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  'How can GPU acceleration be obtained as a service in a cluster?' This
question has become increasingly significant due to the inefficiency of
installing GPUs on all nodes of a cluster. The research reported in this paper
is motivated to address the above question by employing rCUDA (remote CUDA), a
framework that facilitates Acceleration-as-a-Service (AaaS), such that the
nodes of a cluster can request the acceleration of a set of remote GPUs on
demand. The rCUDA framework exploits virtualisation and ensures that multiple
nodes can share the same GPU. In this paper we test the feasibility of the
rCUDA framework on a real-world application employed in the financial risk
industry that can benefit from AaaS in the production setting. The results
confirm the feasibility of rCUDA and highlight that rCUDA achieves similar
performance compared to CUDA, provides consistent results, and more
importantly, allows for a single application to benefit from all the GPUs
available in the cluster without loosing efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02566</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02566</id><created>2015-08-11</created><authors><author><keyname>Kisseleff</keyname><forenames>S.</forenames></author><author><keyname>Akyildiz</keyname><forenames>I. F.</forenames></author><author><keyname>Gerstacker</keyname><forenames>W.</forenames></author></authors><title>Beamforming for Magnetic Induction based Wireless Power Transfer Systems
  with Multiple Receivers</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted for presentation at IEEE GLOBECOM 2015.
  It has 7 pages and 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic induction (MI) based communication and power transfer systems have
gained an increased attention in the recent years. Typical applications for
these systems lie in the area of wireless charging, near-field communication,
and wireless sensor networks. For an optimal system performance, the power
efficiency needs to be maximized. Typically, this optimization refers to the
impedance matching and tracking of the split-frequencies. However, an important
role of magnitude and phase of the input signal has been mostly overlooked.
Especially for the wireless power transfer systems with multiple transmitter
coils, the optimization of the transmit signals can dramatically improve the
power efficiency. In this work, we propose an iterative algorithm for the
optimization of the transmit signals for a transmitter with three orthogonal
coils and multiple single coil receivers. The proposed scheme significantly
outperforms the traditional baseline algorithms in terms of power efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02570</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02570</id><created>2015-08-11</created><authors><author><keyname>Nyirenda</keyname><forenames>Mwawi M.</forenames></author><author><keyname>Ng</keyname><forenames>Siaw-Lynn</forenames></author><author><keyname>Martin</keyname><forenames>Keith M.</forenames></author></authors><title>A Combinatorial Model of Interference in Frequency Hopping Schemes</title><categories>cs.IT math.CO math.IT</categories><comments>15 pages, conference</comments><msc-class>94A05, 94A55, 94B60</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a frequency hopping multiple access (FHMA) system a set of users
communicate simultaneously using frequency hopping sequences defined on the
same set of frequency channels. A frequency hopping sequence specifies which
channels to use as communication progresses. A set of frequency hopping
sequences used in an FHMA is called a frequency hopping scheme (FHS). Much of
the research on the performance of FHS is based on either pairwise mutual
interference or adversarial interference but not both. In this paper, we
evaluate the performance of an FHS with respect to both group-wise mutual
interference and adversarial interference (jamming), bearing in mind that more
than two users may be transmitting simultaneously in the presence of an
adversary. We point out a correspondence between a cover-free code and a
frequency hopping scheme. Cover-free codes give a lower bound on the
transmission capacity of the FHS. Furthermore, we specify a jammer model and
consider what additional properties a cover-free code should have to resist the
jammer. We show that a purely combinatorial approach is inadequate against such
a jammer, but that with the use of pseudorandomness, we can have a system that
has high throughput as well as secure against jamming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02577</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02577</id><created>2015-08-11</created><authors><author><keyname>Aref</keyname><forenames>Vahid</forenames></author><author><keyname>Buelow</keyname><forenames>Henning</forenames></author><author><keyname>Schuh</keyname><forenames>Karsten</forenames></author><author><keyname>Idler</keyname><forenames>Wilfried</forenames></author></authors><title>Experimental Demonstration of Nonlinear Frequency Division Multiplexed
  Transmission</title><categories>cs.IT math.IT physics.optics</categories><comments>Will be presented in ECOC 2015, Sept. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We experimentally demonstrate an NFDM optical system with modulation over
nonlinear discrete spectrum. Particularly, each symbol carries 4-bits from
multiplexing two eigenvalues modulated by QPSK constellation. We show a low
error performance using NFT detection with 4Gbps rate over 640km.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02593</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02593</id><created>2015-08-11</created><updated>2015-08-28</updated><authors><author><keyname>Krompa&#xdf;</keyname><forenames>Denis</forenames></author><author><keyname>Baier</keyname><forenames>Stephan</forenames></author><author><keyname>Tresp</keyname><forenames>Volker</forenames></author></authors><title>Type-Constrained Representation Learning in Knowledge Graphs</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large knowledge graphs increasingly add value to various applications that
require machines to recognize and understand queries and their semantics, as in
search or question answering systems. Latent variable models have increasingly
gained attention for the statistical modeling of knowledge graphs, showing
promising results in tasks related to knowledge graph completion and cleaning.
Besides storing facts about the world, schema-based knowledge graphs are backed
by rich semantic descriptions of entities and relation-types that allow
machines to understand the notion of things and their semantic relationships.
In this work, we study how type-constraints can generally support the
statistical modeling with latent variable models. More precisely, we integrated
prior knowledge in form of type-constraints in various state of the art latent
variable approaches. Our experimental results show that prior knowledge on
relation-types significantly improves these models up to 77% in link-prediction
tasks. The achieved improvements are especially prominent when a low model
complexity is enforced, a crucial requirement when these models are applied to
very large datasets. Unfortunately, type-constraints are neither always
available nor always complete e.g., they can become fuzzy when entities lack
proper typing. We show that in these cases, it can be beneficial to apply a
local closed-world assumption that approximates the semantics of relation-types
based on observations made in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02598</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02598</id><created>2015-08-11</created><updated>2016-01-27</updated><authors><author><keyname>Kelk</keyname><forenames>Steven</forenames></author></authors><title>A note on convex characters and Fibonacci numbers</title><categories>q-bio.PE cs.DS math.CO</categories><comments>added a number of footnotes concerning classical Fibonaci identities</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an unrooted, binary phylogenetic tree T on a set of n &gt;= 2 taxa, a
closed expression for the number of convex characters on T has been known since
1992, and this is independent of the exact topology of T. Here we prove that
this number is equal to the (2n-1)th Fibonacci number. Moreover, we show that
the number of convex characters in which each state appears on at least two
taxa, is also independent of topology, and equal to the (n-1)th Fibonacci
number. We use this insight to give a simple but effective algorithm for the
NP-hard &quot;maximum parsimony distance&quot; problem that runs in time $\Theta(
\phi^{n} \cdot \text{poly}(n) )$, where $\phi \approx 1.618...$ is the golden
ratio. Finally, we give an explicit example demonstrating that topological
neutrality no longer holds when counting the number of convex characters in
which each state appears on at least three taxa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02606</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02606</id><created>2015-08-11</created><authors><author><keyname>Hu</keyname><forenames>Hao</forenames></author><author><keyname>Cui</keyname><forenames>Hainan</forenames></author></authors><title>InAR:Inverse Augmented Reality</title><categories>cs.CV</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02608</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02608</id><created>2015-08-11</created><authors><author><keyname>Lens</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Boigelot</keyname><forenames>Bernard</forenames></author></authors><title>Efficient Path Interpolation and Speed Profile Computation for
  Nonholonomic Mobile Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies path synthesis for nonholonomic mobile robots moving in
two-dimensional space. We first address the problem of interpolating paths
expressed as sequences of straight line segments, such as those produced by
some planning algorithms, into smooth curves that can be followed without
stopping. Our solution has the advantage of being simpler than other existing
approaches, and has a low computational cost that allows a real-time
implementation. It produces discretized paths on which curvature and variation
of curvature are bounded at all points, and preserves obstacle clearance. Then,
we consider the problem of computing a time-optimal speed profile for such
paths. We introduce an algorithm that solves this problem in linear time, and
that is able to take into account a broader class of physical constraints than
other solutions. Our contributions have been implemented and evaluated in the
framework of the Eurobot contest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02617</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02617</id><created>2015-08-11</created><updated>2015-12-15</updated><authors><author><keyname>Lopez-Suarez</keyname><forenames>Miquel</forenames></author><author><keyname>Neri</keyname><forenames>Igor</forenames></author><author><keyname>Gammaitoni</keyname><forenames>Luca</forenames></author></authors><title>Sub kBT micro electromechanical irreversible logic gate</title><categories>cond-mat.mes-hall cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern computers, computation is performed by assembling together sets of
logic gates. Popular gates like AND, OR, XOR, processing two logic inputs and
yielding one logic output, are often addressed as irreversible logic gates; a
sole knowledge of the output logic value, is not sufficient to infer the logic
value of the two inputs. This lack of linkage between logical and physical
irreversibility has animated a long debate [1,2] and been, recently,
clarified[3] from a purely theoretical point of view; however, it is still
missing experimental verification. In the following we present an experiment
wherein we show that a combinational device that implements the OR gate logic,
realized with a micro electromechanical cantilever, can be operated with energy
as low as 0.05 kB T (with kB the Boltzman constant and T the room temperature),
provided the operation is slow enough and frictional phenomena are properly
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02626</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02626</id><created>2015-08-11</created><updated>2015-10-14</updated><authors><author><keyname>Borgwardt</keyname><forenames>Stefan</forenames></author><author><keyname>Mailis</keyname><forenames>Theofilos</forenames></author><author><keyname>Pe&#xf1;aloza</keyname><forenames>Rafael</forenames></author><author><keyname>Turhan</keyname><forenames>Anni-Yasmin</forenames></author></authors><title>Answering Fuzzy Conjunctive Queries over Finitely Valued Fuzzy
  Ontologies</title><categories>cs.LO cs.AI</categories><comments>submitted to the Journal on Data Semantics, v1: 19 pages, v2: 20
  pages, improved evaluation section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy Description Logics (DLs) provide a means for representing vague
knowledge about an application domain. In this paper, we study fuzzy extensions
of conjunctive queries (CQs) over the DL $\mathcal{SROIQ}$ based on finite
chains of degrees of truth. To answer such queries, we extend a well-known
technique that reduces the fuzzy ontology to a classical one, and use classical
DL reasoners as a black box. We improve the complexity of previous reduction
techniques for finitely valued fuzzy DLs, which allows us to prove tight
complexity results for answering certain kinds of fuzzy CQs. We conclude with
an experimental evaluation of a prototype implementation, showing the
feasibility of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02636</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02636</id><created>2015-08-03</created><updated>2016-02-01</updated><authors><author><keyname>Ye</keyname><forenames>Maojiao</forenames></author><author><keyname>Hu</keyname><forenames>Guoqiang</forenames></author></authors><title>Game Design and Analysis for Price based Demand Response: An Aggregate
  Game Approach</title><categories>q-fin.EC cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an aggregate game approach is proposed for the modeling and
analysis of energy consumption control in smart grid. Since the electricity
user's cost function depends on the aggregate load, which is unknown to the end
users, an aggregate load estimator is employed to estimate it. Based on the
communication among the users about their estimations on the aggregate load,
Nash equilibrium seeking strategies are proposed for the electricity users. By
using singular perturbation analysis and Lyapunov stability analysis, a local
convergence result to the Nash equilibrium is presented for the energy
consumption game that may have multiple Nash equilibria. For the energy
consumption game with a unique Nash equilibrium, it is shown that the players'
strategies converge to the Nash equilibrium non-locally. More specially, if the
unique Nash equilibrium is an inner Nash equilibrium, then the convergence rate
can be quantified. Energy consumption game with stubborn players is also
investigated. Convergence to the best response strategies for the rational
players is ensured. Numerical examples are provided to verify the effectiveness
of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02645</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02645</id><created>2015-08-08</created><authors><author><keyname>Bailey</keyname><forenames>J. Phillip</forenames></author><author><keyname>Beal</keyname><forenames>Aubrey N.</forenames></author><author><keyname>Dean</keyname><forenames>Robert N.</forenames></author><author><keyname>Hamilton</keyname><forenames>Michael C.</forenames></author></authors><title>A Digital Matched Filter for Reverse Time Chaos</title><categories>physics.data-an cs.SY nlin.CD</categories><comments>9 pages, 16 figures</comments><msc-class>94C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of reverse time chaos allows the realization of hardware chaotic
systems that can operate at speeds equivalent to existing state of the art
while requiring significantly less complex circuitry. Matched filter decoding
is possible for the reverse time system since it exhibits a closed form
solution formed partially by a linear basis pulse. Coefficients have been
calculated and are used to realize the matched filter digitally as a finite
impulse response filter. Numerical simulations confirm that this correctly
implements a matched filter that can be used for detection of the chaotic
signal. In addition, the direct form of the filter has been implemented in
hardware description language and demonstrates performance in agreement with
numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02668</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02668</id><created>2015-08-11</created><updated>2015-08-11</updated><authors><author><keyname>Afshang</keyname><forenames>Mehrnaz</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Chong</keyname><forenames>Peter Han Joo</forenames></author></authors><title>Modeling and Performance Analysis of Clustered Device-to-Device Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>34 double-spaced pages, 10 figures. Submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication enables direct communication between
proximate devices thereby improving the overall spectrum utilization and
offloading traffic from cellular networks. This paper develops a new spatial
model for D2D networks in which the device locations are modeled as a Poisson
cluster process. Using this model, we study the performance of a typical D2D
receiver in terms of coverage probability under two realistic content
availability setups: (i) content of interest for a typical device is available
at a device chosen uniformly at random from the same cluster, which we term
uniform content availability, and (ii) content of interest is available at the
$k^{th}$ closest device from the typical device inside the same cluster, which
we term $k$-closest content availability. Using these coverage probability
results, we also characterize the area spectral efficiency (ASE) of the whole
network for the two setups. A key intermediate step in this analysis is the
derivation of the distributions of distances from a typical device to both the
intra- and inter-cluster devices. Our analysis reveals that an optimum number
of D2D transmitters must be simultaneously activated per cluster in order to
maximize ASE. This can be interpreted as the classical tradeoff between more
aggressive frequency reuse and higher interference power. The optimum number of
simultaneously transmitting devices and the resulting ASE increase as the
content is made available closer to the receivers. Our analysis also quantifies
the best and worst case performance of clustered D2D networks both in terms of
coverage and ASE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02674</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02674</id><created>2015-08-11</created><authors><author><keyname>Van Bien</keyname><forenames>Dinh Doan</forenames></author><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Collier</keyname><forenames>Rem W.</forenames></author></authors><title>Space-Time Diagram Generation for Profiling Multi Agent Systems</title><categories>cs.MA cs.SE</categories><journal-ref>In L. Braubach, J.-P. Briot, and J. Thangarajah, editors,
  Programming Multi-Agent Systems, volume 5919 of Lecture Notes in Computer
  Science, pages 170--184. Springer Berlin Heidelberg, Budapest, Hungary, May
  2009</journal-ref><doi>10.1007/978-3-642-14843-9_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in Agent Oriented Software Engineering have focused on the provision
of frameworks and toolkits to aid in the creation of Multi Agent Systems
(MASs). However, despite the need to address the inherent complexity of such
systems, little progress has been made in the development of tools to allow for
the debugging and understanding of their inner workings.
  This paper introduces a novel performance analysis system, named
AgentSpotter, which facilitates such analysis. AgentSpotter was developed by
mapping conventional profiling concepts to the domain of MASs. We outline its
integration into the Agent Factory multi agent framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02677</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02677</id><created>2015-08-11</created><authors><author><keyname>Van Bien</keyname><forenames>Dinh Doan</forenames></author><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Collier</keyname><forenames>Rem W.</forenames></author></authors><title>Call Graph Profiling for Multi Agent Systems</title><categories>cs.MA cs.SE</categories><journal-ref>In Languages, Methodologies, and Development Tools for Multi-Agent
  Systems - 2nd International Workshop, LADS 2009, Revised Selected Papers,
  Lecture Notes in Computer Science vol. 6039, pp. 153--167. Springer Berlin
  Heidelberg, 2010</journal-ref><doi>10.1007/978-3-642-22723-3_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design, implementation and testing of Multi Agent Systems is typically a
very complex task. While a number of specialist agent programming languages and
toolkits have been created to aid in the development of such systems, the
provision of associated development tools still lags behind those available for
other programming paradigms. This includes tools such as debuggers and
profilers to help analyse system behaviour, performance and efficiency.
AgentSpotter is a profiling tool designed specifically to operate on the
concepts of agent-oriented programming. This paper extends previous work on
AgentSpotter by discussing its Call Graph View, which presents system
performance information, with reference to the communication between the agents
in the system. This is aimed at aiding developers in examining the effect that
agent communication has on the processing requirements of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02679</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02679</id><created>2015-08-11</created><authors><author><keyname>Ibn-Khedher</keyname><forenames>Hatem</forenames></author><author><keyname>Abd-Elrahman</keyname><forenames>Emad</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Forestier</keyname><forenames>Jacky</forenames></author></authors><title>Network Issues in Virtual Machine Migration</title><categories>cs.NI</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN) is based basically on three features:
centralization of the control plane, programmability of network functions and
traffic engineering. The network function migration poses interesting problems
that we try to expose and solve in this paper. Content Distribution Network
virtualization is presented as use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02681</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02681</id><created>2015-08-11</created><authors><author><keyname>Jahedpari</keyname><forenames>Fatemeh</forenames></author><author><keyname>De Vos</keyname><forenames>Marina</forenames></author><author><keyname>Hashemi</keyname><forenames>Sattar</forenames></author><author><keyname>Hirsch</keyname><forenames>Benjamin</forenames></author><author><keyname>Padget</keyname><forenames>Julian</forenames></author></authors><title>Artificial Prediction Markets for Online Prediction of Continuous
  Variables-A Preliminary Report</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the Artificial Continuous Prediction Market (ACPM) as a means to
predict a continuous real value, by integrating a range of data sources and
aggregating the results of different machine learning (ML) algorithms. ACPM
adapts the concept of the (physical) prediction market to address the
prediction of real values instead of discrete events. Each ACPM participant has
a data source, a ML algorithm and a local decision-making procedure that
determines what to bid on what value. The contributions of ACPM are: (i)
adaptation to changes in data quality by the use of learning in: (a) the
market, which weights each market participant to adjust the influence of each
on the market prediction and (b) the participants, which use a Q-learning based
trading strategy to incorporate the market prediction into their subsequent
predictions, (ii) resilience to a changing population of low- and
high-performing participants. We demonstrate the effectiveness of ACPM by
application to an influenza-like illnesses data set, showing ACPM out-performs
a range of well-known regression models and is resilient to variation in data
source quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02685</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02685</id><created>2015-08-11</created><authors><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Collier`</keyname><forenames>Rem W.</forenames></author></authors><title>Augmenting Agent Platforms to Facilitate Conversation Reasoning</title><categories>cs.MA</categories><journal-ref>In Languages, Methodologies, and Development Tools for Multi-Agent
  Systems - Third International Workshop, LADS 2010, Revised Selected Papers,
  Lecture Notes in Computer Science vol. 6822, pp. 56--75. Springer Berlin
  Heidelberg, 2011</journal-ref><doi>10.1007/978-3-642-22723-3_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within Multi Agent Systems, communication by means of Agent Communication
Languages (ACLs) has a key role to play in the co-operation, co-ordination and
knowledge-sharing between agents. Despite this, complex reasoning about agent
messaging, and specifically about conversations between agents, tends not to
have widespread support amongst general-purpose agent programming languages.
  ACRE (Agent Communication Reasoning Engine) aims to complement the existing
logical reasoning capabilities of agent programming languages with the
capability of reasoning about complex interaction protocols in order to
facilitate conversations between agents. This paper outlines the aims of the
ACRE project and gives details of the functioning of a prototype implementation
within the Agent Factory multi agent framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02705</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02705</id><created>2015-08-11</created><authors><author><keyname>Filippidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Murray</keyname><forenames>Richard M.</forenames></author></authors><title>Symbolic construction of GR(1) contracts for synchronous systems with
  full information</title><categories>cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a symbolic algorithm for the construction of
assume-guarantee specifications that allow multiple agents to cooperate. Each
agent is assigned goals expressed in a fragment of linear temporal logic known
as generalized reactivity of rank 1 (GR(1)). These goals may be unrealizable,
unless additional assumptions are made by each agent about the behavior of the
other agents. The proposed algorithm constructs weakly fair assumptions for
each agent, to ensure that they can cooperate successfully. A necessary
requirement is that the given goals be cooperatively satisfiable. We prove that
there exist games for which the GR(1) fragment with liveness properties over
states is not sufficient to ensure realizability from any state in the
cooperatively winning set. The obstruction is due to circular dependencies of
liveness goals. To prevent circularity, we introduce nested games as a
formalism to express specifications with conditional assumptions. The algorithm
is symbolic, with fixpoint structure similar to the GR(1) synthesis algorithm,
implying time complexity polynomial in the number of states, and linear in the
number of recurrence goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02708</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02708</id><created>2015-08-11</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author><author><keyname>Lee</keyname><forenames>Sanghyuk</forenames></author><author><keyname>Lim</keyname><forenames>Eng Gee</forenames></author></authors><title>On enery-efficient time synchronization based on source clock frequency
  recovery in wireless sensor networks</title><categories>cs.NI cs.SY</categories><comments>2 pages, 2 figures, extended abstract of the presentation at
  International Conference on Information, System and Convergence Applications
  (ICISCA) 2015 (awarded Best Paper Award)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study energy-efficient time synchronization schemes with
focus on asymmetric wireless sensor networks, where a head node, which is
connected to both wired &amp; wireless networks, is equipped with a powerful
processor and supplied power from outlet, and sensor nodes, which are connected
only through wireless channels, are limited in processing and battery-powered.
It is this asymmetry that we focus our study on; unlike existing schemes saving
the power of all sensor nodes in the network (including the head node), we
concentrate on battery-powered sensor nodes in minimizing energy consumption
for synchronization. Specifically, we discuss a time synchronization scheme
based on source clock frequency recovery, where we minimize the number of
message transmissions from sensor nodes to the head node, and its extension to
network-wide, multi-hop synchronization through gateway nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02733</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02733</id><created>2015-08-11</created><authors><author><keyname>Syrakos</keyname><forenames>Alexandros</forenames></author><author><keyname>Goulas</keyname><forenames>Apostolos</forenames></author></authors><title>Estimate of the truncation error of a finite volume discretisation of
  the Navier-Stokes equations on colocated grids</title><categories>physics.comp-ph cs.NA physics.flu-dyn</categories><journal-ref>International Journal for Numerical Methods in Fluids 50 (2006)
  103-130</journal-ref><doi>10.1002/fld.1038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A methodology is proposed for the calculation of the truncation error of
finite volume discretisations of the incompressible Navier-Stokes equations on
colocated grids. The truncation error is estimated by restricting the solution
obtained on a given grid to a coarser grid and calculating the image of the
discrete Navier-Stokes operator of the coarse grid on the restricted velocity
and pressure field. The proposed methodology is not a new concept but its
application to colocated finite volume discretisations of the incompressible
Navier-Stokes equations is made possible by the introduction of a variant of
the momentum interpolation technique for mass fluxes where the pressure-part of
the mass fluxes is not dependent on the coefficients of the linearised momentum
equations. The theory presented is supported by a number of numerical
experiments. The methodology is developed for two-dimensional flows, but
extension to three-dimensional cases should not pose problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02759</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02759</id><created>2015-08-11</created><updated>2015-08-14</updated><authors><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author></authors><title>Technical Note: Split Algorithm in O(n) for the Vehicle Routing Problem</title><categories>cs.DS</categories><comments>14 pages, Working Paper, PUC-Rio</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Split algorithm is an essential building block for route-first
cluster-second heuristics and modern genetic algorithms for vehicle routing
problems. The recent survey of [Prins, Lacomme and Prodhon, Transport Res. C
(40), 179-200] lists more than 70 articles that use this technique. In the
vehicle routing literature, Split is usually assimilated to the search for a
shortest path in a directed acyclic graph $\mathcal{G}$ and solved in $O(nB)$
using Bellman's algorithm, where $n$ is the number of delivery points and $B$
is a bound on the number of customers per route, due to the capacity
constraints. Some linear-time algorithms are also known for this problem as a
consequence of a Monge property of $\mathcal{G}$. In this article, we highlight
a stronger property of this graph, leading to a simpler alternative algorithm
in $O(n)$. Experimentally, we observe that the approach is faster than the
classical Split for problem instances of practical size. We also extend the
method to deal with a limited fleet and soft capacity constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02760</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02760</id><created>2015-08-11</created><authors><author><keyname>Mahoney</keyname><forenames>J. R.</forenames></author><author><keyname>Aghamohammadi</keyname><forenames>C.</forenames></author><author><keyname>Crutchfield</keyname><forenames>J. P.</forenames></author></authors><title>Occam's Quantum Strop: Synchronizing and Compressing Classical Cryptic
  Processes via a Quantum Channel</title><categories>quant-ph cond-mat.stat-mech cs.IT math.IT</categories><comments>10 pages, 6 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/oqs.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic process's statistical complexity stands out as a fundamental
property: the minimum information required to synchronize one process generator
to another. How much information is required, though, when synchronizing over a
quantum channel? Recent work demonstrated that representing causal similarity
as quantum state-indistinguishability provides a quantum advantage. We
generalize this to synchronization and offer a sequence of constructions that
exploit extended causal structures, finding substantial increase of the quantum
advantage. We demonstrate that maximum compression is determined by the
process's cryptic order---a classical, topological property closely allied to
Markov order, itself a measure of historical dependence. We introduce an
efficient algorithm that computes the quantum advantage and close noting that
the advantage comes at a cost---one trades off prediction for generation
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02765</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02765</id><created>2015-08-11</created><updated>2015-11-10</updated><authors><author><keyname>Gamal</keyname><forenames>Mostafa El</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>Are Slepian-Wolf Rates Necessary for Distributed Parameter Estimation?</title><categories>cs.IT math.IT stat.ML</categories><comments>Accepted in Allerton 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a distributed parameter estimation problem, in which multiple
terminals send messages related to their local observations using limited rates
to a fusion center who will obtain an estimate of a parameter related to
observations of all terminals. It is well known that if the transmission rates
are in the Slepian-Wolf region, the fusion center can fully recover all
observations and hence can construct an estimator having the same performance
as that of the centralized case. One natural question is whether Slepian-Wolf
rates are necessary to achieve the same estimation performance as that of the
centralized case. In this paper, we show that the answer to this question is
negative. We establish our result by explicitly constructing an asymptotically
minimum variance unbiased estimator (MVUE) that has the same performance as
that of the optimal estimator in the centralized case while requiring
information rates less than the conditions required in the Slepian-Wolf rate
region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02767</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02767</id><created>2015-08-11</created><authors><author><keyname>Bonifacio</keyname><forenames>Adilson Luiz</forenames></author><author><keyname>Moura</keyname><forenames>Arnaldo Vieira</forenames></author></authors><title>Intrinsic Properties of Complete Test Suites</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Completeness is a desirable property of test suites. Roughly, completeness
guarantees that a non-equivalent implementation under test will always be
identified. Several approaches proposed sufficient, and sometimes also
necessary, conditions on the specification model and on the test suite in order
to guarantee completeness. Usually, these approaches impose several
restrictions on the specification and on the implementations, such as requiring
them to be reduced or complete. Further, test cases are required to be
non-blocking --- that is, they must run to completion --- on both the
specification and the implementation models. In this work we deal test cases
that can be blocking, we define a new notion that captures completeness, and we
characterize test suite completeness in this new scenario. We establish an
upper bound on the number of states of implementations beyond which no test
suite can be complete, both in the classical sense and in the new scenario with
blocking test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02773</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02773</id><created>2015-08-11</created><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Paulusma</keyname><forenames>Daniel</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Editing to a Planar Graph of Given Degrees</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following graph modification problem. Let the input consist
of a graph $G=(V,E)$, a weight function $w\colon V\cup E\rightarrow
\mathbb{N}$, a cost function $c\colon V\cup E\rightarrow \mathbb{N}$ and a
degree function $\delta\colon V\rightarrow \mathbb{N}_0$, together with three
integers $k_v, k_e$ and $C$. The question is whether we can delete a set of
vertices of total weight at most $k_v$ and a set of edges of total weight at
most $k_e$ so that the total cost of the deleted elements is at most $C$ and
every non-deleted vertex $v$ has degree $\delta(v)$ in the resulting graph
$G'$. We also consider the variant in which $G'$ must be connected. Both
problems are known to be NP-complete and W[1]-hard when parameterized by
$k_v+k_e$. We prove that, when restricted to planar graphs, they stay
NP-complete but have polynomial kernels when parameterized by $k_v+k_e$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02774</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02774</id><created>2015-08-11</created><authors><author><keyname>Breuel</keyname><forenames>Thomas M.</forenames></author></authors><title>Benchmarking of LSTM Networks</title><categories>cs.NE</categories><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LSTM (Long Short-Term Memory) recurrent neural networks have been highly
successful in a number of application areas. This technical report describes
the use of the MNIST and UW3 databases for benchmarking LSTM networks and
explores the effect of di?erent architectural and hyperparameter choices on
performance. Significant ?ndings include: (1) LSTM performance depends smoothly
on learning rates, (2) batching and momentum has no significant effect on
performance, (3) softmax training outperforms least square training, (4)
peephole units are not useful, (5) the standard non-linearities (tanh and
sigmoid) perform best, (6) bidirectional training combined with CTC performs
better than other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02788</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02788</id><created>2015-08-11</created><authors><author><keyname>Breuel</keyname><forenames>Thomas M.</forenames></author></authors><title>The Effects of Hyperparameters on SGD Training of Neural Networks</title><categories>cs.NE cs.LG</categories><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of neural network classifiers is determined by a number of
hyperparameters, including learning rate, batch size, and depth. A number of
attempts have been made to explore these parameters in the literature, and at
times, to develop methods for optimizing them. However, exploration of
parameter spaces has often been limited. In this note, I report the results of
large scale experiments exploring these different parameters and their
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02790</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02790</id><created>2015-08-11</created><authors><author><keyname>Breuel</keyname><forenames>Thomas M.</forenames></author></authors><title>On the Convergence of SGD Training of Neural Networks</title><categories>cs.NE cs.LG</categories><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are usually trained by some form of stochastic gradient
descent (SGD)). A number of strategies are in common use intended to improve
SGD optimization, such as learning rate schedules, momentum, and batching.
These are motivated by ideas about the occurrence of local minima at different
scales, valleys, and other phenomena in the objective function. Empirical
results presented here suggest that these phenomena are not significant factors
in SGD optimization of MLP-related objective functions, and that the behavior
of stochastic gradient descent in these problems is better described as the
simultaneous convergence at different rates of many, largely non-interacting
subproblems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02792</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02792</id><created>2015-08-11</created><authors><author><keyname>Breuel</keyname><forenames>Thomas M.</forenames></author></authors><title>Possible Mechanisms for Neural Reconfigurability and their Implications</title><categories>cs.NE q-bio.NC</categories><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a biologically and evolutionarily plausible neural
architecture that allows a single group of neurons, or an entire cortical
pathway, to be dynamically reconfigured to perform multiple, potentially very
different computations. The paper shows that reconfigurability can account for
the observed stochastic and distributed coding behavior of neurons and provides
a parsimonious explanation for timing phenomena in psychophysical experiments.
It also shows that reconfigurable pathways correspond to classes of statistical
classifiers that include decision lists, decision trees, and hierarchical
Bayesian methods. Implications for the interpretation of neurophysiological and
psychophysical results are discussed, and future experiments for testing the
reconfigurability hypothesis are explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02793</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02793</id><created>2015-08-11</created><updated>2015-12-25</updated><authors><author><keyname>Zhuang</keyname><forenames>Yan</forenames></author></authors><title>A generalized Goulden-Jackson cluster method and lattice path
  enumeration</title><categories>math.CO cs.DM cs.FL</categories><comments>31 pages</comments><msc-class>05A15, 05A05, 05C50, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Goulden-Jackson cluster method is a powerful tool for obtaining
generating functions for counting words in a free monoid by occurrences of a
set of subwords. We introduce a generalization of the cluster method for monoid
networks, which generalize the combinatorial framework of free monoids. As a
sample application of the generalized cluster method, we compute bivariate and
multivariate generating functions counting Motzkin paths---both with height
bounded and unbounded---by statistics corresponding to the number of
occurrences of various subwords, yielding both closed-form and continued
fraction formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02796</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02796</id><created>2015-08-11</created><authors><author><keyname>Zhou</keyname><forenames>Xinyang</forenames></author><author><keyname>Farivar</keyname><forenames>Masoud</forenames></author><author><keyname>Chen</keyname><forenames>Lijun</forenames></author></authors><title>Pseudo-gradient Based Local Voltage Control in Distribution Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voltage regulation is critical for power grids. However, it has become a much
more challenging problem as distributed energy resources (DERs) such as
photovoltaic and wind generators are increasingly deployed, causing rapid
voltage fluctuations beyond what can be handled by the traditional voltage
regulation methods. In this paper, motivated by two previously proposed
inverter-based local volt/var control algorithms, we propose a pseudo-gradient
based voltage control algorithm for the distribution network that does not
constrain the allowable control functions and has low implementation
complexity. We characterize the convergence of the proposed voltage control
scheme, and compare it against the two previous algorithms in terms of the
convergence condition as well as the convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02797</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02797</id><created>2015-08-11</created><authors><author><keyname>Yang</keyname><forenames>Chenchen</forenames></author><author><keyname>Yao</keyname><forenames>Yao</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author></authors><title>Analysis on Cache-enabled Wireless Heterogeneous Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching the popular multimedia content is a promising way to unleash the
ultimate potential of wireless networks. In this paper, we contribute to
proposing and analyzing the cache-based content delivery in a three-tier
heterogeneous network (HetNet), where base stations (BSs), relays and
device-to-device (D2D) pairs are included. We advocate to proactively cache the
popular contents in the relays and parts of the users with caching ability when
the network is off-peak. The cached contents can be reused for frequent access
to offload the cellular network traffic. The node locations are first modeled
as mutually independent Poisson Point Processes (PPPs) and the corresponding
content access protocol is developed. The average ergodic rate and outage
probability in the downlink are then analyzed theoretically. We further derive
the throughput and the delay based on the \emph{multiclass processor-sharing
queue} model and the continuous-time Markov process. According to the critical
condition of the steady state in the HetNet, the maximum traffic load and the
global throughput gain are investigated. Moreover, impacts of some key network
characteristics, e.g., the heterogeneity of multimedia contents, node densities
and the limited caching capacities, on the system performance are elaborated to
provide a valuable insight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02798</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02798</id><created>2015-08-11</created><authors><author><keyname>Khor</keyname><forenames>Susan</forenames></author></authors><title>The short-cut network within protein residue networks</title><categories>q-bio.MN cs.DM</categories><comments>24 pages. arXiv admin note: text overlap with arXiv:1412.2155</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A protein residue network (PRN) is a network of interacting amino acids
within a protein. We describe characteristics of a sparser, highly central and
more volatile sub-network of a PRN called the short-cut network (SCN), as a
protein folds under molecular dynamics (MD) simulation with the goal of
understanding how proteins form navigable small-world networks within
themselves. The edges of an SCN are found via a local greedy search on a PRN.
SCNs grow in size and transitivity strength as a protein folds, and SCNs from
successful MD trajectories are better formed in these terms. Findings from an
investigation on how to model the formation of SCNs using dynamic graph theory,
and suggestions to move forward are presented. A SCN is enriched with
short-range contacts and its formation correlates positively with secondary
structure formation. Thus our approach to modeling PRN formation, in essence
protein folding from a graph theoretic view point, is more in tune with the
notion of increasing order to a random graph than the other way around, and
this increase in order coincides with improved navigability of PRNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02808</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02808</id><created>2015-08-11</created><authors><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>Microscopic Analysis of the Uplink Interference in FDMA Small Cell
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>30 pages, 6 figures, 5 tables, IEEE TWC [J]. arXiv admin note: text
  overlap with arXiv:1505.01924</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analytically derive an upper bound on the error in
approximating the uplink (UL) single-cell interference by a lognormal
distribution in frequency division multiple access (FDMA) small cell networks
(SCNs). Such an upper bound is measured by the Kolmogorov Smirnov (KS) distance
between the actual cumulative density function (CDF) and the approximate CDF.
The lognormal approximation is important because it allows tractable network
performance analysis. Our results are more general than the existing works in
the sense that we do not pose any requirement on (i) the shape and/or size of
cell coverage areas, (ii) the uniformity of user equipment (UE) distribution,
and (iii) the type of multi-path fading. Based on our results, we propose a new
framework to directly and analytically investigate a complex network with
practical deployment of multiple BSs placed at irregular locations, using a
power lognormal approximation of the aggregate UL interference. The proposed
network performance analysis is particularly useful for the 5th generation (5G)
systems with general cell deployment and UE distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02812</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02812</id><created>2015-08-12</created><authors><author><keyname>Liu</keyname><forenames>Jiamou</forenames></author><author><keyname>Wei</keyname><forenames>Ziheng</forenames></author></authors><title>A Game of Attribute Decomposition for Software Architecture Design</title><categories>cs.GT cs.SE</categories><comments>23 pages, 5 figures, a shorter version to appear at 12th
  International Colloquium on Theoretical Aspects of Computing (ICTAC 2015)</comments><acm-class>D.2.11; F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute-driven software architecture design aims to provide decision
support by taking into account the quality attributes of softwares. A central
question in this process is: What architecture design best fulfills the
desirable software requirements? To answer this question, a system designer
needs to make tradeoffs among several potentially conflicting quality
attributes. Such decisions are normally ad-hoc and rely heavily on experiences.
We propose a mathematical approach to tackle this problem. Game theory
naturally provides the basic language: Players represent requirements, and
strategies involve setting up coalitions among the players. In this way we
propose a novel model, called decomposition game, for attribute-driven design.
We present its solution concept based on the notion of cohesion and
expansion-freedom and prove that a solution always exists. We then investigate
the computational complexity of obtaining a solution. The game model and the
algorithms may serve as a general framework for providing useful guidance for
software architecture design. We present our results through running examples
and a case study on a real-life software project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02820</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02820</id><created>2015-08-12</created><authors><author><keyname>Jaganathan</keyname><forenames>Kishore</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>STFT Phase Retrieval: Uniqueness Guarantees and Recovery Algorithms</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of signal reconstruction from its Fourier magnitude is of
paramount importance in various fields of engineering and applied physics. Due
to the absence of Fourier phase information, some form of additional
information is required in order to be able to uniquely, efficiently and
robustly identify the underlying signal. Inspired by practical methods in
optical imaging, we consider the problem of signal reconstruction from its
Short-Time Fourier Transform (STFT) magnitude. We first develop conditions
under which the STFT magnitude is an almost surely unique signal
representation. Then, we consider a semidefinite relaxation-based algorithm
(STliFT) and provide recovery guarantees. Numerical simulations complement our
theoretical analysis and provide directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02823</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02823</id><created>2015-08-12</created><authors><author><keyname>Singla</keyname><forenames>Adish</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Learning to Hire Teams</title><categories>cs.HC cs.CY cs.LG</categories><comments>Short version of this paper will appear in HCOMP'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing and human computation has been employed in increasingly
sophisticated projects that require the solution of a heterogeneous set of
tasks. We explore the challenge of building or hiring an effective team, for
performing tasks required for such projects on an ongoing basis, from an
available pool of applicants or workers who have bid for the tasks. The
recruiter needs to learn workers' skills and expertise by performing online
tests and interviews, and would like to minimize the amount of budget or time
spent in this process before committing to hiring the team. How can one
optimally spend budget to learn the expertise of workers as part of recruiting
a team? How can one exploit the similarities among tasks as well as underlying
social ties or commonalities among the workers for faster learning? We tackle
these decision-theoretic challenges by casting them as an instance of online
learning for best action selection. We present algorithms with PAC bounds on
the required budget to hire a near-optimal team with high confidence.
Furthermore, we consider an embedding of the tasks and workers in an underlying
graph that may arise from task similarities or social ties, and that can
provide additional side-observations for faster learning. We then quantify the
improvement in the bounds that we can achieve depending on the characteristic
properties of this graph structure. We evaluate our methodology on simulated
problem instances as well as on real-world crowdsourcing data collected from
the oDesk platform. Our methodology and results present an interesting
direction of research to tackle the challenges faced by a recruiter for
contract-based crowdsourcing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02826</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02826</id><created>2015-08-12</created><authors><author><keyname>Ray</keyname><forenames>Nicolas</forenames></author><author><keyname>Sokolov</keyname><forenames>Dmitry</forenames></author></authors><title>Inappropriate use of L-BFGS, Illustrated on frame field design</title><categories>cs.GR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  L-BFGS is a hill climbing method that is guarantied to converge only for
convex problems. In computer graphics, it is often used as a black box solver
for a more general class of non linear problems, including problems having many
local minima. Some works obtain very nice results by solving such difficult
problems with L-BFGS. Surprisingly, the method is able to escape local minima:
our interpretation is that the approximation of the Hessian is smoother than
the real Hessian, making it possible to evade the local minima. We analyse the
behavior of L-BFGS on the design of 2D frame fields. It involves an energy
function that is infinitly continuous, strongly non linear and having many
local minima. Moreover, the local minima have a clear visual interpretation:
they corresponds to differents frame field topologies. We observe that the
performances of LBFGS are almost unpredictables: they are very competitive when
the field is sampled on the primal graph, but really poor when they are sampled
on the dual graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02834</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02834</id><created>2015-08-12</created><authors><author><keyname>Kumar</keyname><forenames>Sudhir</forenames></author><author><keyname>Dixit</keyname><forenames>Rishabh</forenames></author><author><keyname>Hegde</keyname><forenames>Rajesh M.</forenames></author></authors><title>Second Order Cone Programming for Sensor Node Localization in Mixed
  LOS/NLOS Conditions</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel method for sensor node localization under mixed
line-of-sight/non-line-of-sight (LOS/NLOS) conditions based on second order
cone programming (SOCP) is presented. SOCP methods have, hitherto, not been
utilized in the node localization under mixed LOS/NLOS conditions. Unlike
semidefinite programming (SDP) formulation, SOCP is computationally efficient
for resource constrained ad-hoc sensor network. The proposed method can work
seamlessly in mixed LOS/NLOS conditions. The robustness of the method is due to
the fair utilization of all measurements obtained under LOS and NLOS
conditions. The computational complexity of this method is quadratic in the
number of nearest neighbours of the unknown node. Extensive simulations and
real field deployments are used to evaluate the performance of the proposed
method. The experimental results of the proposed method is reasonably better
when compared to similar methods in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02844</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02844</id><created>2015-08-12</created><updated>2015-08-18</updated><authors><author><keyname>Pepik</keyname><forenames>Bojan</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Ritschel</keyname><forenames>Tobias</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>What is Holding Back Convnets for Detection?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks have recently shown excellent results in
general object detection and many other tasks. Albeit very effective, they
involve many user-defined design choices. In this paper we want to better
understand these choices by inspecting two key aspects &quot;what did the network
learn?&quot;, and &quot;what can the network learn?&quot;. We exploit new annotations
(Pascal3D+), to enable a new empirical analysis of the R-CNN detector. Despite
common belief, our results indicate that existing state-of-the-art convnet
architectures are not invariant to various appearance factors. In fact, all
considered networks have similar weak points which cannot be mitigated by
simply increasing the training data (architectural changes are needed). We show
that overall performance can improve when using image renderings for data
augmentation. We report the best known results on the Pascal3D+ detection and
view-point estimation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02848</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02848</id><created>2015-08-12</created><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast
  and Effective Image Restoration</title><categories>cs.CV</categories><comments>14 pages, 13 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image restoration is a long-standing problem in low-level computer vision
with many interesting applications. We describe a flexible learning framework
to obtain simple but effective models for various image restoration problems.
The proposed approach is based on the concept of nonlinear reaction diffusion,
but we extend conventional nonlinear reaction diffusion models by highly
parametrized linear filters as well as highly parametrized influence functions.
In contrast to previous nonlinear diffusion models, all the parameters,
including the filters and the influence functions, are learned from training
data through a loss based approach. We call this approach TNRD -- Trainable
Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of
image restoration tasks by incorporating appropriate reaction force. We
demonstrate its capabilities with three representative applications, Gaussian
image denoising, single image super resolution and JPEG deblocking. Experiments
show that our trained nonlinear diffusion models largely benefit from the
training of the parameters and finally lead to the best reported performance on
common test datasets with respect to the tested applications. Our trained
models retain the structural simplicity of diffusion models and take only a
small number of steps, thus are highly efficient. Moreover, they are also
well-suited for parallel computation on GPUs, which makes the inference
procedure extremely fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02849</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02849</id><created>2015-08-12</created><authors><author><keyname>Jiang</keyname><forenames>Fei</forenames></author><author><keyname>Jia</keyname><forenames>Lili</forenames></author><author><keyname>Sheng</keyname><forenames>Xiaobao</forenames></author><author><keyname>LeMieux</keyname><forenames>Riley</forenames></author></authors><title>Manifold regularization in structured output space for semi-supervised
  structured output prediction</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured output prediction aims to learn a predictor to predict a
structured output from a input data vector. The structured outputs include
vector, tree, sequence, etc. We usually assume that we have a training set of
input-output pairs to train the predictor. However, in many real-world appli-
cations, it is difficult to obtain the output for a input, thus for many
training input data points, the structured outputs are missing. In this paper,
we dis- cuss how to learn from a training set composed of some input-output
pairs, and some input data points without outputs. This problem is called semi-
supervised structured output prediction. We propose a novel method for this
problem by constructing a nearest neighbor graph from the input space to
present the manifold structure, and using it to regularize the structured out-
put space directly. We define a slack structured output for each training data
point, and proposed to predict it by learning a structured output predictor.
The learning of both slack structured outputs and the predictor are unified
within one single minimization problem. In this problem, we propose to mini-
mize the structured loss between the slack structured outputs of neighboring
data points, and the prediction error measured by the structured loss. The
problem is optimized by an iterative algorithm. Experiment results over three
benchmark data sets show its advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02851</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02851</id><created>2015-08-12</created><authors><author><keyname>Khachatrian</keyname><forenames>Hrant</forenames></author><author><keyname>Mamikonyan</keyname><forenames>Tigran</forenames></author></authors><title>On interval edge-colorings of bipartite graphs of small order</title><categories>cs.DM math.CO</categories><comments>Accepted for the CSIT 2015 conference</comments><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge-coloring of a graph $G$ with colors $1,\ldots,t$ is an interval
$t$-coloring if all colors are used, and the colors of edges incident to each
vertex of $G$ are distinct and form an interval of integers. A graph $G$ is
interval colorable if it has an interval $t$-coloring for some positive integer
$t$. The problem of deciding whether a bipartite graph is interval colorable is
NP-complete. The smallest known examples of interval non-colorable bipartite
graphs have $19$ vertices. On the other hand it is known that the bipartite
graphs on at most $14$ vertices are interval colorable. In this work we observe
that several classes of bipartite graphs of small order have an interval
coloring. In particular, we show that all bipartite graphs on $15$ vertices are
interval colorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02864</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02864</id><created>2015-08-12</created><updated>2015-09-30</updated><authors><author><keyname>Goldberg</keyname><forenames>Mayer</forenames><affiliation>Ben-Gurion University</affiliation></author></authors><title>Ellipses and Lambda Definability</title><categories>cs.LO</categories><comments>31 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:25) 2015</journal-ref><doi>10.2168/LMCS-11(3:25)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ellipses are a meta-linguistic notation for denoting terms the size of which
are specified by a meta-variable that ranges over the natural numbers. In this
work, we present a systematic approach for encoding such meta-expressions in
the &#xce;&#xbb;-calculus, without ellipses: Terms that are parameterized by
meta-variables are replaced with corresponding &#xce;&#xbb;-abstractions over actual
variables. We call such &#xce;&#xbb;-terms arity-generic. Concrete terms, for particular
choices of the parameterizing variable are obtained by applying an
arity-generic &#xce;&#xbb;-term to the corresponding numeral, obviating the need to use
ellipses. For example, to find the multiple fixed points of n equations, n
different &#xce;&#xbb;-terms are needed, every one of which is indexed by two
meta-variables, and defined using three levels of ellipses. A single
arity-generic &#xce;&#xbb;-abstraction that takes two Church numerals, one for the number
of fixed-point equations, and one for their arity, replaces all these multiple
fixed-point combinators. We show how to define arity-generic generalizations of
two historical fixed-point combinators, the first by Curry, and the second by
Turing, for defining multiple fixed points. These historical fixed-point
combinators are related by a construction due to B&#xc3;&#xb6;hm: We show that likewise,
their arity-generic generalizations are related by an arity-generic
generalization of B&#xc3;&#xb6;hm's construction. We further demonstrate this approach to
arity-generic &#xce;&#xbb;-definability with additional &#xce;&#xbb;-terms that create, project,
extend, reverse, and map over ordered n-tuples, as well as an arity-generic
generator for one-point bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02865</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02865</id><created>2015-08-12</created><authors><author><keyname>Prando</keyname><forenames>Giulia</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author></authors><title>Maximum Entropy Vector Kernels for MIMO system identification</title><categories>cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent contributions have framed linear system identification as a
nonparametric regularized inverse problem, which in some situations have proved
to be advantageous w.r.t classical parametric methods. Typical formulations
exploit an $\ell_2$-type regularization which accounts for the stability and
smoothness of the impulse response to be estimated. In this paper, adopting
Maximum Entropy arguments, we derive a new type of $\ell_2$-regularization
which results in a vector-valued kernel; our aim is to introduce regularization
on the block Hankel matrix built with Markov coefficients, thus controlling the
complexity of the identified model, measured by its McMillan degree. As a
special case we recover the standard nuclear norm penalty. Combining this
Hankel-based regularization with the standard $\ell_2$-type regularization
adopted in previous literature we design a kernel which, at the same time,
encodes stability, smoothness and low McMillan degree. In contrast with
previous literature on reweighed nuclear norm penalties, our kernel is
described by a small number of hyper-prameters, which are iteratively updated
through marginal likelihood maximization. To this purpose we also adapt a
Scaled Gradient Projection (SGP) algorithm which is proved to be significantly
computationally cheaper than other first and second order off-the-shelf
optimization methods. The effectiveness of the identification technique we
propose is confirmed by several Monte-Carlo studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02868</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02868</id><created>2015-08-12</created><authors><author><keyname>Situngkir</keyname><forenames>Hokky</forenames></author></authors><title>Cellular-Automata and Innovation within Indonesian Traditional Weaving
  Crafts: A Discourse of Human-Computer Interaction</title><categories>cs.CY</categories><comments>8 pages, 5 figures</comments><report-no>BFI Working Paper Series WP-2013-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper reports the possibility of Indonesian traditional artisans of
weaving designs and crafts to explore the cellular automata, a dynamical model
in computation that may yield similar patterns. The reviews of the cellular
automata due to the perspective of weaving process reveals that the latter
would focus on macro-properties, i.e.: the strength of structural construction
beside the aesthetic patterns and designs. The meeting of traditional weaving
practice and the computational model is delivered and open the door for
interesting discourse of computer-aided designs for the traditional artists and
designers to come.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02873</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02873</id><created>2015-08-12</created><authors><author><keyname>Gupta</keyname><forenames>Aditya</forenames></author><author><keyname>Shamra</keyname><forenames>Abhishek</forenames></author></authors><title>Modeling and Analysis of Walking Pattern for a Biped Robot</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the design and development of an autonomous biped robot
using master and worker combination of controllers. In addition, the bot is
wirelessly controllable. The work presented here explains the walking pattern,
system control and actuator control techniques for 10 Degree of Freedom (DOF)
biped humanoid. Bi-pedal robots have better mobility than conventional wheeled
robots, but they tend to topple easily. In order to walk stably in various
environments, such as on rough terrain, up and down slopes, or in regions
containing obstacles, it is necessary, that robot should adapt to the ground
conditions with a foot motion, as well as maintain its stability with a torso
motion. It is desirable to select a walking pattern that requires small torque
and velocity of the joint actuators. The work proposed a low cost solution
using open source hardware-software and application. The work extends to
develop and implement new algorithms by adding gyroscope and accelerometer to
further the research in the field of biped robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02878</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02878</id><created>2015-08-12</created><authors><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>McKay</keyname><forenames>Brendan D.</forenames></author></authors><title>Fullerenes with distant pentagons</title><categories>math.CO cs.DM</categories><comments>15 pages, submitted for publication. arXiv admin note: text overlap
  with arXiv:1501.02680</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For each $d&gt;0$, we find all the smallest fullerenes for which the least
distance between two pentagons is $d$. We also show that for each $d$ there is
an $h_d$ such that fullerenes with pentagons at least distance $d$ apart and
any number of hexagons greater than or equal to $h_d$ exist.
  We also determine the number of fullerenes where the minimum distance between
any two pentagons is at least $d$, for $1 \le d \le 5$, up to 400 vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02884</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02884</id><created>2015-08-12</created><updated>2015-09-24</updated><authors><author><keyname>Diaz-Aviles</keyname><forenames>Ernesto</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Pinelli</keyname><forenames>Fabio</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Lynch</keyname><forenames>Karol</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Nabi</keyname><forenames>Zubair</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Gkoufas</keyname><forenames>Yiannis</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Bouillet</keyname><forenames>Eric</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Calabrese</keyname><forenames>Francesco</forenames><affiliation>IBM Research -- Ireland</affiliation></author><author><keyname>Coughlan</keyname><forenames>Eoin</forenames><affiliation>IBM Now Factory -- Ireland</affiliation></author><author><keyname>Holland</keyname><forenames>Peter</forenames><affiliation>IBM Now Factory -- Ireland</affiliation></author><author><keyname>Salzwedel</keyname><forenames>Jason</forenames><affiliation>IBM Now Factory -- Ireland</affiliation></author></authors><title>Towards Real-time Customer Experience Prediction for Telecommunication
  Operators</title><categories>cs.CY cs.IR stat.ML</categories><comments>IEEE 2015 BigData Conference (to appear). Keywords: Telecom
  operators; Customer Care; Big Data; Predictive Analytics</comments><acm-class>I.2.6; K.4.0; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telecommunications operators (telcos) traditional sources of income, voice
and SMS, are shrinking due to customers using over-the-top (OTT) applications
such as WhatsApp or Viber. In this challenging environment it is critical for
telcos to maintain or grow their market share, by providing users with as good
an experience as possible on their network.
  But the task of extracting customer insights from the vast amounts of data
collected by telcos is growing in complexity and scale everey day. How can we
measure and predict the quality of a user's experience on a telco network in
real-time? That is the problem that we address in this paper.
  We present an approach to capture, in (near) real-time, the mobile customer
experience in order to assess which conditions lead the user to place a call to
a telco's customer care center. To this end, we follow a supervised learning
approach for prediction and train our 'Restricted Random Forest' model using,
as a proxy for bad experience, the observed customer transactions in the telco
data feed before the user places a call to a customer care center.
  We evaluate our approach using a rich dataset provided by a major African
telecommunication's company and a novel big data architecture for both the
training and scoring of predictive models. Our empirical study shows our
solution to be effective at predicting user experience by inferring if a
customer will place a call based on his current context.
  These promising results open new possibilities for improved customer service,
which will help telcos to reduce churn rates and improve customer experience,
both factors that directly impact their revenue growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02909</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02909</id><created>2015-08-12</created><updated>2015-08-25</updated><authors><author><keyname>AlAmmouri</keyname><forenames>Ahmad</forenames></author><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>In-Band Full-Duplex Communications for Cellular Networks with Partial
  Uplink/Downlink Overlap</title><categories>cs.IT math.IT</categories><comments>To be presented in IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-band full-duplex (FD) communications have been optimistically promoted to
improve the spectrum utilization in cellular networks. However, the explicit
impact of spatial interference, imposed by FD communications, on uplink and
downlink transmissions has been overlooked in the literature. This paper
presents an extensive study of the explicit effect of FD communications on the
uplink and downlink performances. For the sake of rigorous analysis, we develop
a tractable framework based on stochastic geometry toolset. The developed model
accounts for uplink truncated channel inversion power control in FD cellular
networks. The study shows that FD communications improve the downlink
throughput at the expense of significant degradation in the uplink throughput.
Therefore, we propose a novel fine-grained duplexing scheme, denoted as
$\alpha$-duplex scheme, which allows a partial overlap between uplink and
downlink frequency bands. To this end, we show that the amount of the overlap
can be optimized via adjusting $\alpha$ to achieve a certain design objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02933</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02933</id><created>2015-08-12</created><updated>2015-08-23</updated><authors><author><keyname>Nishihara</keyname><forenames>Robert</forenames></author><author><keyname>Lopez-Paz</keyname><forenames>David</forenames></author><author><keyname>Bottou</keyname><forenames>L&#xe9;on</forenames></author></authors><title>No Regret Bound for Extreme Bandits</title><categories>stat.ML cs.LG math.OC math.ST stat.TH</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for hyperparameter optimization abound, all of which work well
under different and often unverifiable assumptions. Motivated by the general
challenge of sequentially choosing which algorithm to use, we study the more
specific task of choosing among distributions to use for random hyperparameter
optimization. This work is naturally framed in the extreme bandit setting,
which deals with sequentially choosing which distribution from a collection to
sample in order to minimize (maximize) the single best cost (reward). Whereas
the distributions in the standard bandit setting are primarily characterized by
their means, a number of subtleties arise when we care about the minimal cost
as opposed to the average cost. For example, there may not be a well-defined
&quot;best&quot; distribution as there is in the standard bandit setting. The best
distribution depends on the rewards that have been obtained and on the
remaining time horizon. Whereas in the standard bandit setting, it is sensible
to compare policies with an oracle which plays the single best arm, in the
extreme bandit setting, there are multiple sensible oracle models. We define a
sensible notion of regret in the extreme bandit setting, which closely
parallels the concept of regret in the standard bandit setting. We then prove
that no policy can asymptotically achieve no regret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02935</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02935</id><created>2015-08-12</created><authors><author><keyname>Schleicher</keyname><forenames>Dierk</forenames></author><author><keyname>Stoll</keyname><forenames>Robin</forenames></author></authors><title>Newton's method in practice: finding all roots of polynomials of degree
  one million efficiently</title><categories>math.NA cs.NA math.DS</categories><comments>32 pages, 21 figures</comments><msc-class>49M15, 65H04, 37F10, 37N30, 37-04, 65-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Newton's method to find all roots of several polynomials in one
complex variable of degree up to and exceeding one million and show that the
method, applied to appropriately chosen starting points, can be turned into an
algorithm that can be applied routinely to find all roots without deflation and
with the inherent numerical stability of Newton's method.
  We specify an algorithm that provably terminates and finds all roots of any
polynomial of arbitrary degree, provided all roots are distinct and exact
computation is available. It is known that Newton's method is inherently
stable, so computing errors do not accumulate; we provide an exact bound on how
much numerical precision is sufficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02946</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02946</id><created>2015-08-12</created><authors><author><keyname>Alonso</keyname><forenames>Juan M.</forenames></author></authors><title>A Hausdorff dimension for finite sets</title><categories>cs.DM math.CA</categories><msc-class>68R99 (primary), 28A78, 68P10 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical Hausdorff dimension of finite or countable sets is zero. We
define an analog for finite sets, called finite Hausdorff dimension which is
non-trivial. It turns out that a finite bound for the finite Hausdorff
dimension guarantees that every point of the set has &quot;nearby&quot; neighbors. This
property is important for many computer algorithms of great practical value,
that obtain solutions by finding nearest neighbors. We also define an analog
for finite sets of the classical box-counting dimension, and compute examples.
The main result of the paper is a Convergence Theorem. It gives conditions
under which, if a sequence of finite sets converges to a compact set
(convergence of compact subsets of Euclidean space under the Hausdorff metric),
then the finite Hausdorff dimension of the finite sets will converge to the
classical Hausdorff dimension of the compact set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02959</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02959</id><created>2015-08-12</created><authors><author><keyname>Fedorov</keyname><forenames>Roman</forenames></author></authors><title>Mountain Peak Detection in Online Social Media</title><categories>cs.CV cs.MM</categories><comments>M.Sc. Thesis, Politecnico di Milano, Italy, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system for the classification of mountain panoramas from
user-generated photographs followed by identification and extraction of
mountain peaks from those panoramas. We have developed an automatic technique
that, given as input a geo-tagged photograph, estimates its FOV (Field Of View)
and the direction of the camera using a matching algorithm on the photograph
edge maps and a rendered view of the mountain silhouettes that should be seen
from the observer's point of view. The extraction algorithm then identifies the
mountain peaks present in the photograph and their profiles. We discuss
possible applications in social fields such as photograph peak tagging on
social portals, augmented reality on mobile devices when viewing a mountain
panorama, and generation of collective intelligence systems (such as
environmental models) from massive social media collections (e.g. snow water
availability maps based on mountain peak states extracted from photograph
hosting services).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02960</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02960</id><created>2015-08-12</created><authors><author><keyname>Fattahia</keyname><forenames>Ehsan</forenames></author><author><keyname>Waluga</keyname><forenames>Christian</forenames></author><author><keyname>Wohlmuth</keyname><forenames>Barbara</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author><author><keyname>Manhart</keyname><forenames>Michael</forenames></author><author><keyname>Helmig</keyname><forenames>Rainer</forenames></author></authors><title>Pore-scale lattice Boltzmann simulation of laminar and turbulent flow
  through a sphere pack</title><categories>cs.CE physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lattice Boltzmann method can be used to simulate flow through porous
media with full geometrical resolution. With such a direct numerical
simulation, it becomes possible to study fundamental effects which are
difficult to assess either by developing macroscopic mathematical models or
experiments. We first evaluate the lattice Boltzmann method with various
boundary handling of the solid-wall and various collision operators to assess
their suitability for large scale direct numerical simulation of porous media
flow. A periodic pressure drop boundary condition is used to mimic the pressure
driven flow through the simple sphere pack in a periodic domain. The evaluation
of the method is done in the Darcy regime and the results are compared to a
semi-analytic solution. Taking into account computational cost and accuracy, we
choose the most efficient combination of the solid boundary condition and
collision operator. We apply this method to perform simulations for a wide
range of Reynolds numbers from Stokes flow over seven orders of magnitude to
turbulent flow. Contours and streamlines of the flow field are presented to
show the flow behavior in different flow regimes. Moreover, unknown parameters
of the Forchheimer, the Barree--Conway and friction factor models are evaluated
numerically for the considered flow regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02964</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02964</id><created>2015-08-12</created><authors><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>Binary words avoiding xx^Rx and strongly unimodal sequences</title><categories>math.CO cs.FL</categories><comments>4 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, Currie and Rampersad showed that the growth of the number
of binary words avoiding the pattern xxx^R was intermediate between polynomial
and exponential. We now show that the same holds for the growth of the number
of binary words avoiding the pattern xx^Rx. Curiously, the analysis for xx^Rx
is much simpler than that for xxx^R. We derive our results by giving a
bijection between the set of binary words avoiding xx^Rx and a class of
sequences closely related to the class of &quot;strongly unimodal sequences.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02968</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02968</id><created>2015-08-12</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Cunial</keyname><forenames>Fabio</forenames></author></authors><title>Space-efficient detection of unusual words</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1502.06370</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting all the strings that occur in a text more frequently or less
frequently than expected according to an IID or a Markov model is a basic
problem in string mining, yet current algorithms are based on data structures
that are either space-inefficient or incur large slowdowns, and current
implementations cannot scale to genomes or metagenomes in practice. In this
paper we engineer an algorithm based on the suffix tree of a string to use just
a small data structure built on the Burrows-Wheeler transform, and a stack of
$O(\sigma^2\log^2 n)$ bits, where $n$ is the length of the string and $\sigma$
is the size of the alphabet. The size of the stack is $o(n)$ except for very
large values of $\sigma$. We further improve the algorithm by removing its time
dependency on $\sigma$, by reporting only a subset of the maximal repeats and
of the minimal rare words of the string, and by detecting and scoring candidate
under-represented strings that $\textit{do not occur}$ in the string. Our
algorithms are practical and work directly on the BWT, thus they can be
immediately applied to a number of existing datasets that are available in this
form, returning this string mining problem to a manageable scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02977</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02977</id><created>2015-08-12</created><authors><author><keyname>Gilliocq-Hirtz</keyname><forenames>Diane</forenames></author><author><keyname>Belhachmi</keyname><forenames>Zakaria</forenames></author></authors><title>A massively parallel multi-level approach to a domain decomposition
  method for the optical flow estimation with varying illumination</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variational method to solve the optical flow problem with
varying illumination. We apply an adaptive control of the regularization
parameter which allows us to preserve the edges and fine features of the
computed flow. To reduce the complexity of the estimation for high resolution
images and the time of computations, we implement a multi-level parallel
approach based on the domain decomposition with the Schwarz overlapping method.
The second level of parallelism uses the massively parallel solver MUMPS. We
perform some numerical simulations to show the efficiency of our approach and
to validate it on classical and real-world image sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02982</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02982</id><created>2015-07-25</created><authors><author><keyname>Nebeling</keyname><forenames>Michael</forenames></author><author><keyname>Guo</keyname><forenames>Anhong</forenames></author><author><keyname>Murray</keyname><forenames>Kyle</forenames></author><author><keyname>Tostengard</keyname><forenames>Annika</forenames></author><author><keyname>Giannopoulos</keyname><forenames>Angelos</forenames></author><author><keyname>Mihajlov</keyname><forenames>Martin</forenames></author><author><keyname>Dow</keyname><forenames>Steven</forenames></author><author><keyname>Teevan</keyname><forenames>Jaime</forenames></author><author><keyname>Bigham</keyname><forenames>Jeffrey P.</forenames></author></authors><title>WearWrite: Orchestrating the Crowd to Complete Complex Tasks from
  Wearables (We Wrote This Paper on a Watch)</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a paradigm for completing complex tasks from
wearable devices by leveraging crowdsourcing, and demonstrate its validity for
academic writing. We explore this paradigm using a collaborative authoring
system, called WearWrite, which is designed to enable authors and crowd workers
to work together using an Android smartwatch and Google Docs to produce
academic papers, including this one. WearWrite allows expert authors who do not
have access to large devices to contribute bits of expertise and big picture
direction from their watch, while freeing them of the obligation of integrating
their contributions into the overall document. Crowd workers on desktop
computers actually write the document. We used this approach to write several
simple papers, and found it was effective at producing reasonable drafts.
However, the workers often needed more structure and the authors more context.
WearWrite addresses these issues by focusing workers on specific tasks and
providing select context to authors on the watch. We demonstrate the system's
feasibility by writing this paper using it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02984</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02984</id><created>2015-08-11</created><updated>2015-10-26</updated><authors><author><keyname>Chen</keyname><forenames>Hsien-Pu</forenames></author><author><keyname>Gonzalez</keyname><forenames>Elias</forenames></author><author><keyname>Saez</keyname><forenames>Yessica</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Cable Capacitance Attack against the KLJN Secure Key Exchange</title><categories>cs.ET cs.CR physics.class-ph</categories><comments>Accepted for publication in the journal: Information</comments><journal-ref>Information 2015, 6(4), 719-732</journal-ref><doi>10.3390/info6040719</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The security of the Kirchhoff-law-Johnson-(like)-noise (KLJN) key exchange
system is based on the Fluctuation-Dissipation-Theorem of classical statistical
physics. Similarly to quantum key distribution, in practical situations, due to
the non-idealities of the building elements, there is a small information leak,
which can be mitigated by privacy amplification or other techniques so that the
unconditional (information theoretic) security is preserved. In this paper, the
industrial cable and circuit simulator LTSPICE is used to validate the
information leak due to one of the non-idealities in KLJN, the parasitic
(cable) capacitance. Simulation results show that privacy amplification and/or
capacitor killer (capacitance compensation) arrangements can effectively
eliminate the leak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.02986</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.02986</id><created>2015-08-12</created><authors><author><keyname>Ralaivola</keyname><forenames>Liva</forenames></author><author><keyname>Louche</keyname><forenames>Ugo</forenames></author></authors><title>From Cutting Planes Algorithms to Compression Schemes and Active
  Learning</title><categories>cs.LG</categories><comments>IJCNN 2015, Jul 2015, Killarney, Ireland. 2015,
  \&amp;lt;http://www.ijcnn.org/\&amp;gt</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cutting-plane methods are well-studied localization(and optimization)
algorithms. We show that they provide a natural framework to perform
machinelearning ---and not just to solve optimization problems posed by
machinelearning--- in addition to their intended optimization use. In
particular, theyallow one to learn sparse classifiers and provide good
compression schemes.Moreover, we show that very little effort is required to
turn them intoeffective active learning methods. This last property provides a
generic way todesign a whole family of active learning algorithms from existing
passivemethods. We present numerical simulations testifying of the relevance
ofcutting-plane methods for passive and active learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03000</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03000</id><created>2015-08-12</created><authors><author><keyname>Parasuraman</keyname><forenames>Ramviyas</forenames></author></authors><title>Few common failure cases in mobile robots</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A mobile robot deployed for remote inspection, surveying or rescue missions
can fail due to various possibilities and can be hardware or software related.
These failure scenarios necessitate manual recovery (self-rescue) of the robot
from the environment. It would bring unforeseen challenges to recover the
mobile robot if the environment where it was deployed had hazardous or harmful
conditions (e.g. ionizing radiations). While it is not fully possible to
predict all the failures in the robot, failures can be reduced by employing
certain design/usage considerations. Few example failure cases based on real
experiences are presented in this short article along with generic suggestions
on overcoming the illustrated failure situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03011</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03011</id><created>2015-08-12</created><authors><author><keyname>El-Bardan</keyname><forenames>Raghed</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Matching-based Spectrum Allocation in Cognitive Radio Networks</title><categories>cs.NI cs.GT cs.IT math.IT</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel spectrum association approach for cognitive radio
networks (CRNs) is proposed. Based on a measure of both inference and
confidence as well as on a measure of quality-of-service, the association
between secondary users (SUs) in the network and frequency bands licensed to
primary users (PUs) is investigated. The problem is formulated as a matching
game between SUs and PUs. In this game, SUs employ a soft-decision Bayesian
framework to detect PUs' signals and, eventually, rank them based on the
logarithm of the a posteriori ratio. A performance measure that captures both
the ranking metric and rate is further computed by the SUs. Using this
performance measure, a PU evaluates its own utility function that it uses to
build its own association preferences. A distributed algorithm that allows both
SUs and PUs to interact and self-organize into a stable match is proposed.
Simulation results show that the proposed algorithm can improve the sum of SUs'
rates by up to 20 % and 60 % relative to the deferred acceptance algorithm and
random channel allocation approach, respectively. The results also show an
improved convergence time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03020</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03020</id><created>2015-08-12</created><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Bounds for codes on pentagon and other cycles</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of a graph is defined as the rate of exponential grow of
independent sets in the strong powers of the graph. In strong power, an edge
connects two sequences if at each position letters are equal or adjacent. We
consider a variation of the problem where edges in the power graphs are removed
among sequences which differ in more than a fraction $\delta$ of coordinates.
For odd cycles, we derive an upper bound on the corresponding rate which
combines Lov\'asz' bound on the capacity with Delsarte's linear programming
bounds on the minimum distance of codes in Hamming spaces. For the pentagon,
this shows that for $\delta \ge {1-{1\over\sqrt{5}}}$ the Lov\'asz rate is the
best possible, while we prove by a Gilbert-Varshamov-type bound that a higher
rate is achievable for $\delta &lt; {2\over 5}$.
  Communication interpretation of this question is the problem of sending
quinary symbols subject to $\pm 1\mod 5$ disturbance. The maximal communication
rate subject to the zero undetected-error equals capacity of a pentagon. The
question addressed here is how much this rate can be increased if only a
fraction $\delta$ of symbols is allowed to be disturbed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03032</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03032</id><created>2015-08-12</created><authors><author><keyname>Falkner</keyname><forenames>Andreas</forenames></author><author><keyname>Ryabokon</keyname><forenames>Anna</forenames></author><author><keyname>Schenner</keyname><forenames>Gottfried</forenames></author><author><keyname>Shchekotykhin</keyname><forenames>Kostyantyn</forenames></author></authors><title>OOASP: Connecting Object-oriented and Logic Programming</title><categories>cs.AI cs.SE</categories><comments>13 pages, 4 figures, accepted for publication at LPNMR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of contemporary software systems are implemented using an
object-oriented approach. Modeling phases -- during which software engineers
analyze requirements to the future system using some modeling language -- are
an important part of the development process, since modeling errors are often
hard to recognize and correct.
  In this paper we present a framework which allows the integration of Answer
Set Programming into the object-oriented software development process. OOASP
supports reasoning about object-oriented software models and their
instantiations. Preliminary results of the OOASP application in CSL Studio,
which is a Siemens internal modeling environment for product configurators,
show that it can be used as a lightweight approach to verify, create and
transform instantiations of object models at runtime and to support the
software development process during design and testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03040</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03040</id><created>2015-08-12</created><updated>2016-02-16</updated><authors><author><keyname>Casares</keyname><forenames>Ram&#xf3;n</forenames></author></authors><title>Syntax Evolution: Problems and Recursion</title><categories>cs.CL</categories><comments>30 pages</comments><msc-class>91F20, 68T20, 68T50, 92D15</msc-class><acm-class>I.2.7; I.2.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We are Turing complete, and natural language parsing is decidable, so our
syntactic abilities are in excess to those needed to speak a natural language.
This is an anomaly, because evolution would not keep an overqualified feature
for long. We solve this anomaly by using a coincidence, both syntax and problem
solving are computing, and a difference, Turing completeness is not a
requirement of syntax, but of problem solving. Then computing should have been
shaped by evolutionary requirements coming from both syntax and problem
solving, but the last one, Turing completeness, only from problem solving. So
we propose and analyze a hypothesis: syntax and problem solving co-evolved in
humans towards Turing completeness. Finally, we argue that Turing completeness,
also known as recursion, is our most singular feature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03061</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03061</id><created>2015-08-12</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Oliveira</keyname><forenames>Igor C.</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Addition is exponentially harder than counting for shallow monotone
  circuits</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $U_{k,N}$ denote the Boolean function which takes as input $k$ strings of
$N$ bits each, representing $k$ numbers $a^{(1)},\dots,a^{(k)}$ in
$\{0,1,\dots,2^{N}-1\}$, and outputs 1 if and only if $a^{(1)} + \cdots +
a^{(k)} \geq 2^N.$ Let THR$_{t,n}$ denote a monotone unweighted threshold gate,
i.e., the Boolean function which takes as input a single string $x \in
\{0,1\}^n$ and outputs $1$ if and only if $x_1 + \cdots + x_n \geq t$. We refer
to circuits that are composed of THR gates as monotone majority circuits.
  The main result of this paper is an exponential lower bound on the size of
bounded-depth monotone majority circuits that compute $U_{k,N}$. More
precisely, we show that for any constant $d \geq 2$, any depth-$d$ monotone
majority circuit computing $U_{d,N}$ must have size
$\smash{2^{\Omega(N^{1/d})}}$. Since $U_{k,N}$ can be computed by a single
monotone weighted threshold gate (that uses exponentially large weights), our
lower bound implies that constant-depth monotone majority circuits require
exponential size to simulate monotone weighted threshold gates. This answers a
question posed by Goldmann and Karpinski (STOC'93) and recently restated by
Hastad (2010, 2014). We also show that our lower bound is essentially best
possible, by constructing a depth-$d$, size-$2^{O(N^{1/d})}$ monotone majority
circuit for $U_{d,N}$.
  As a corollary of our lower bound, we significantly strengthen a classical
theorem in circuit complexity due to Ajtai and Gurevich (JACM'87). They
exhibited a monotone function that is in AC$^0$ but requires super-polynomial
size for any constant-depth monotone circuit composed of unbounded fan-in AND
and OR gates. We describe a monotone function that is in depth-$3$ AC$^0$ but
requires exponential size monotone circuits of any constant depth, even if the
circuits are composed of THR gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03062</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03062</id><created>2015-08-12</created><authors><author><keyname>Cameron</keyname><forenames>Kathie</forenames></author><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Hoang</keyname><forenames>Chinh T.</forenames></author></authors><title>On the structure of (pan, even hole)-free graphs</title><categories>cs.DM</categories><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hole is a chordless cycle with at least four vertices. A pan is a graph
which consists of a hole and a single vertex with precisely one neighbor on the
hole. An even hole is a hole with an even number of vertices. We prove that a
(pan, even hole)-free graph can be decomposed by clique cutsets into
essentially unit circular-arc graphs. This structure theorem is the basis of
our $O(nm)$-time certifying algorithm for recognizing (pan, even hole)-free
graphs and for our $O(n^{2.5}+nm)$-time algorithm to optimally color them.
Using this structure theorem, we show that the tree-width of a (pan, even
hole)-free graph is at most 1.5 times the clique number minus 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03064</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03064</id><created>2015-07-30</created><authors><author><keyname>Pushak</keyname><forenames>Yasha</forenames></author><author><keyname>Hare</keyname><forenames>Warren</forenames></author><author><keyname>Lucet</keyname><forenames>Yves</forenames></author></authors><title>Multiple-Path Selection for new Highway Alignments using Discrete
  Algorithms</title><categories>cs.DS cs.AI math.OC</categories><comments>to be published in European Journal of Operational Research</comments><msc-class>52B05, 90C35</msc-class><acm-class>G.1.6; G.2.2</acm-class><doi>10.1016/j.ejor.2015.07.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of finding multiple near-optimal,
spatially-dissimilar paths that can be considered as alternatives in the
decision making process, for finding optimal corridors in which to construct a
new road. We further consider combinations of techniques for reducing the costs
associated with the computation and increasing the accuracy of the cost
formulation. Numerical results for five algorithms to solve the dissimilar
multipath problem show that a &quot;bidirectional approach&quot; yields the fastest
running times and the most robust algorithm. Further modifications of the
algorithms to reduce the running time were tested and it is shown that running
time can be reduced by an average of 56 percent without compromising the
quality of the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03080</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03080</id><created>2015-08-12</created><authors><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Pai</keyname><forenames>Mallesh M.</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>The Strange Case of Privacy in Equilibrium Models</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how privacy technologies affect user and advertiser behavior in a
simple economic model of targeted advertising. In our model, a consumer first
decides whether or not to buy a good, and then an advertiser chooses an
advertisement to show the consumer. The consumer's value for the good is
correlated with her type, which determines which ad the advertiser would prefer
to show to her---and hence, the advertiser would like to use information about
the consumer's purchase decision to target the ad that he shows.
  In our model, the advertiser is given only a differentially private signal
about the consumer's behavior---which can range from no signal at all to a
perfect signal, as we vary the differential privacy parameter. This allows us
to study equilibrium behavior as a function of the level of privacy provided to
the consumer. We show that this behavior can be highly counter-intuitive, and
that the effect of adding privacy in equilibrium can be completely different
from what we would expect if we ignored equilibrium incentives. Specifically,
we show that increasing the level of privacy can actually increase the amount
of information about the consumer's type contained in the signal the advertiser
receives, lead to decreased utility for the consumer, and increased profit for
the advertiser, and that generally these quantities can be non-monotonic and
even discontinuous in the privacy level of the signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03087</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03087</id><created>2015-08-12</created><authors><author><keyname>Subramanian</keyname><forenames>Lavanya</forenames></author></authors><title>Providing High and Controllable Performance in Multicore Systems Through
  Shared Resource Management</title><categories>cs.DC</categories><comments>CMU PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple applications executing concurrently on a multicore system interfere
with each other at different shared resources such as main memory and shared
caches. Such inter-application interference, if uncontrolled, results in high
system performance degradation and unpredictable application slowdowns. While
previous work has proposed application-aware memory scheduling as a solution to
mitigate inter-application interference and improve system performance,
previously proposed memory scheduling techniques incur high hardware complexity
and unfairly slowdown some applications. Furthermore, previously proposed
memory-interference mitigation techniques are not designed to precisely control
application performance.
  This dissertation seeks to achieve high and controllable performance in
multicore systems by mitigating and quantifying the impact of shared resource
interference. First, towards mitigating memory interference and achieving high
performance, we propose the Blacklisting memory scheduler that achieves high
performance and fairness at low complexity. Next, towards quantifying the
impact of memory interference and achieving controllable performance in the
presence of memory bandwidth interference, we propose the Memory Interference
induced Slowdown Estimation (MISE) model. We propose and demonstrate two use
cases that can leverage MISE to provide soft performance guarantees and high
overall performance/fairness. Finally, we seek to quantify the impact of shared
cache interference on application slowdowns, in addition to memory bandwidth
interference. Towards this end, we propose the Application Slowdown Model
(ASM). We propose and demonstrate several use cases of ASM that leverage it to
provide soft performance guarantees and improve performance and fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03096</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03096</id><created>2015-08-12</created><updated>2015-09-03</updated><authors><author><keyname>Saxe</keyname><forenames>Joshua</forenames></author><author><keyname>Berlin</keyname><forenames>Konstantin</forenames></author></authors><title>Deep Neural Network Based Malware Detection Using Two Dimensional Binary
  Program Features</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malware remains a serious problem for corporations, government agencies, and
individuals, as attackers continue to use it as a tool to effect frequent and
costly network intrusions. Machine learning holds the promise of automating the
work required to detect newly discovered malware families, and could
potentially learn generalizations about malware and benign software that
support the detection of entirely new, unknown malware families. Unfortunately,
few proposed machine learning based malware detection methods have achieved the
low false positive rates required to deliver deployable detectors.
  In this paper we a deep neural network malware classifier that achieves a
usable detection rate at an extremely low false positive rate and scales to
real world training example volumes on commodity hardware. Specifically, we
show that our system achieves a 95% detection rate at 0.1% false positive rate
(FPR), based on more than 400,000 software binaries sourced directly from our
customers and internal malware databases. We achieve these results by directly
learning on all binaries, without any filtering, unpacking, or manually
separating binary files into categories. Further, we confirm our false positive
rates directly on a live stream of files coming in from Invincea's deployed
endpoint solution, provide an estimate of how many new binary files we expected
to see a day on an enterprise network, and describe how that relates to the
false positive rate and translates into an intuitive threat score.
  Our results demonstrate that it is now feasible to quickly train and deploy a
low resource, highly accurate machine learning classification model, with false
positive rates that approach traditional labor intensive signature based
methods, while also detecting previously unseen malware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03101</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03101</id><created>2015-08-12</created><updated>2016-01-08</updated><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>B&#xfc;ttner</keyname><forenames>M.</forenames></author><author><keyname>Leistedt</keyname><forenames>B.</forenames></author><author><keyname>Peiris</keyname><forenames>H. V.</forenames></author><author><keyname>Wiaux</keyname><forenames>Y.</forenames></author></authors><title>A novel sampling theorem on the rotation group</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>5 pages, 2 figures, minor changes to match version accepted for
  publication. Code available at http://www.sothree.org</comments><journal-ref>IEEE Signal Processing Letters. Vol. 22, No. 12, 2015, pp
  2425-2429</journal-ref><doi>10.1109/LSP.2015.2490676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel sampling theorem for functions defined on the
three-dimensional rotation group SO(3) by connecting the rotation group to the
three-torus through a periodic extension. Our sampling theorem requires $4L^3$
samples to capture all of the information content of a signal band-limited at
$L$, reducing the number of required samples by a factor of two compared to
other equiangular sampling theorems. We present fast algorithms to compute the
associated Fourier transform on the rotation group, the so-called Wigner
transform, which scale as $O(L^4)$, compared to the naive scaling of $O(L^6)$.
For the common case of a low directional band-limit $N$, complexity is reduced
to $O(N L^3)$. Our fast algorithms will be of direct use in speeding up the
computation of directional wavelet transforms on the sphere. We make our SO3
code implementing these algorithms publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03110</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03110</id><created>2015-08-12</created><updated>2016-01-10</updated><authors><author><keyname>Winlaw</keyname><forenames>Manda</forenames></author><author><keyname>Hynes</keyname><forenames>Michael B.</forenames></author><author><keyname>Caterini</keyname><forenames>Anthony</forenames></author><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author></authors><title>Algorithmic Acceleration of Parallel ALS for Collaborative Filtering:
  Speeding up Distributed Big Data Recommendation in Spark</title><categories>math.NA cs.DC cs.IR cs.NA</categories><comments>Proceedings of ICPADS 2015, Melbourne, AU. 10 pages; 6 figures; 4
  tables</comments><msc-class>65K05</msc-class><acm-class>G.1.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering algorithms are important building blocks in many
practical recommendation systems. For example, many large-scale data processing
environments include collaborative filtering models for which the Alternating
Least Squares (ALS) algorithm is used to compute latent factor matrix
decompositions. In this paper, we propose an approach to accelerate the
convergence of parallel ALS-based optimization methods for collaborative
filtering using a nonlinear conjugate gradient (NCG) wrapper around the ALS
iterations. We also provide a parallel implementation of the accelerated
ALS-NCG algorithm in the Apache Spark distributed data processing environment,
and an efficient line search technique as part of the ALS-NCG implementation
that requires only one pass over the data on distributed datasets. In serial
numerical experiments on a linux workstation and parallel numerical experiments
on a 16 node cluster with 256 computing cores, we demonstrate that the combined
ALS-NCG method requires many fewer iterations and less time than standalone ALS
to reach movie rankings with high accuracy on the MovieLens 20M dataset. In
parallel, ALS-NCG can achieve an acceleration factor of 4 or greater in clock
time when an accurate solution is desired; furthermore, the acceleration factor
increases as greater numerical precision is required in the solution. In
addition, the NCG acceleration mechanism is efficient in parallel and scales
linearly with problem size on synthetic datasets with up to nearly 1 billion
ratings. The acceleration mechanism is general and may also be applicable to
other optimization methods for collaborative filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03112</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03112</id><created>2015-08-12</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author></authors><title>Capacity-Achieving Rateless Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rateless coding scheme transmits incrementally more and more coded bits
over an unknown channel until all the information bits are decoded reliably by
the receiver. We propose a new rateless coding scheme based on polar codes, and
we show that this scheme is capacity-achieving, i.e. its information rate is as
good as the best code specifically designed for the unknown channel. Previous
rateless coding schemes are designed for specific classes of channels such as
AWGN channels, binary erasure channels, etc. but the proposed rateless coding
scheme is capacity-achieving for broad classes of channels as long as they are
ordered via degradation. Moreover, it inherits the conceptual and computational
simplicity of polar codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03113</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03113</id><created>2015-08-12</created><updated>2016-01-15</updated><authors><author><keyname>Xu</keyname><forenames>Jian</forenames></author><author><keyname>Wickramarathne</keyname><forenames>Thanuka L.</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Representing higher-order dependencies in networks</title><categories>cs.SI physics.soc-ph</categories><comments>51 pages, 8 figures, 2 tables, 2 algorithms</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To ensure the correctness of network analysis methods, the network (as the
input) has to be a sufficiently accurate representation of the underlying data.
However, when representing sequential data from complex systems such as global
shipping traffic or web clickstream traffic as networks, the conventional
network representation implicitly assuming the Markov property (first-order
dependency) can quickly become limiting. That is, when movements are simulated
on the network, the next movement depends only on the current node, failing to
capture the fact that the movement may depend on multiple previous steps. We
show that data derived from some complex systems show up to fifth-order
dependencies, such that the oversimplification in the first-order network
representation can later result in inaccurate network analysis results. To that
end, we propose the Higher-Order Network (HON) representation that can discover
and embed variable orders of dependencies in a network representation. Through
a comprehensive empirical evaluation and analysis, we establish several
desirable characteristics of HON -- accuracy, scalability, and direct
compatibility with the existing suite of network analysis methods. We
illustrate the broad applicability of HON by using it as the input to a variety
of tasks, such as random walking, clustering and ranking, where these methods
yield more accurate results without modification. Our approach brings the
representative power of networks for handling the increasingly complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03116</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03116</id><created>2015-08-13</created><authors><author><keyname>Grant</keyname><forenames>Christan</forenames></author><author><keyname>Wang</keyname><forenames>Daisy Zhe</forenames></author><author><keyname>Wick</keyname><forenames>Michael L.</forenames></author></authors><title>Query-Driven Sampling for Collective Entity Resolution</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic databases play a preeminent role in the processing and
management of uncertain data. Recently, many database research efforts have
integrated probabilistic models into databases to support tasks such as
information extraction and labeling. Many of these efforts are based on batch
oriented inference which inhibits a realtime workflow. One important task is
entity resolution (ER). ER is the process of determining records (mentions) in
a database that correspond to the same real-world entity. Traditional pairwise
ER methods can lead to inconsistencies and low accuracy due to localized
decisions. Leading ER systems solve this problem by collectively resolving all
records using a probabilistic graphical model and Markov chain Monte Carlo
(MCMC) inference. However, for large datasets this is an extremely expensive
process. One key observation is that, such exhaustive ER process incurs a huge
up-front cost, which is wasteful in practice because most users are interested
in only a small subset of entities. In this paper, we advocate pay-as-you-go
entity resolution by developing a number of query-driven collective ER
techniques. We introduce two classes of SQL queries that involve ER operators
--- selection-driven ER and join-driven ER. We implement novel variations of
the MCMC Metropolis Hastings algorithm to generate biased samples and
selectivity-based scheduling algorithms to support the two classes of ER
queries. Finally, we show that query-driven ER algorithms can converge and
return results within minutes over a database populated with the extraction
from a newswire dataset containing 71 million mentions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03117</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03117</id><created>2015-08-13</created><authors><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Li</keyname><forenames>Huan</forenames></author></authors><title>Optimized Projections for Compressed Sensing via Direct Mutual Coherence
  Minimization</title><categories>cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Sensing (CS) is a novel technique for simultaneous signal sampling
and compression based on the existence of a sparse representation of signal and
a projected dictionary $\PP\D$, where $\PP\in\mathbb{R}^{m\times d}$ is the
projection matrix and $\D\in\mathbb{R}^{d\times n}$ is the dictionary. To
exactly recover the signal with a small number of measurements $m$, the
projected dictionary $\PP\D$ is expected to be of low mutual coherence. Several
previous methods attempt to find the projection $\PP$ such that the mutual
coherence of $\PP\D$ can be as low as possible. However, they do not minimize
the mutual coherence directly and thus their methods are far from optimal. Also
the solvers they used lack of the convergence guarantee and thus there has no
guarantee on the quality of their obtained solutions. This work aims to address
these issues. We propose to find an optimal projection by minimizing the mutual
coherence of $\PP\D$ directly. This leads to a nonconvex nonsmooth minimization
problem. We then approximate it by smoothing and solve it by alternate
minimization. We further prove the convergence of our algorithm. To the best of
our knowledge, this is the first work which directly minimizes the mutual
coherence of the projected dictionary with a convergence guarantee. Numerical
experiments demonstrate that the proposed method can recover sparse signals
better than existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03130</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03130</id><created>2015-08-13</created><authors><author><keyname>Edakunni</keyname><forenames>Narayanan U.</forenames></author><author><keyname>Raghunathan</keyname><forenames>Aditi</forenames></author><author><keyname>Tripathi</keyname><forenames>Abhishek</forenames></author><author><keyname>Handley</keyname><forenames>John</forenames></author><author><keyname>Roulland</keyname><forenames>Fredric</forenames></author></authors><title>Probabilistic Dependency Networks for Prediction and Diagnostics</title><categories>cs.LG</categories><comments>Presented at the Transportation Research Board Annual Meeting 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in transportation frequently involve modelling and predicting
attributes of events that occur at regular intervals. The event could be
arrival of a bus at a bus stop, the volume of a traffic at a particular point,
the demand at a particular bus stop etc. In this work, we propose a specific
implementation of probabilistic graphical models to learn the probabilistic
dependency between the events that occur in a network. A dependency graph is
built from the past observed instances of the event and we use the graph to
understand the causal effects of some events on others in the system. The
dependency graph is also used to predict the attributes of future events and is
shown to have a good prediction accuracy compared to the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03148</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03148</id><created>2015-08-13</created><authors><author><keyname>Laufer-Goldshtein</keyname><forenames>Bracha</forenames></author><author><keyname>Talmon</keyname><forenames>Ronen</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author></authors><title>Semi-Supervised Sound Source Localization Based on Manifold
  Regularization</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional speaker localization algorithms, based merely on the received
microphone signals, are often sensitive to adverse conditions, such as: high
reverberation or low signal to noise ratio (SNR). In some scenarios, e.g. in
meeting rooms or cars, it can be assumed that the source position is confined
to a predefined area, and the acoustic parameters of the environment are
approximately fixed. Such scenarios give rise to the assumption that the
acoustic samples from the region of interest have a distinct geometrical
structure. In this paper, we show that the high dimensional acoustic samples
indeed lie on a low dimensional manifold and can be embedded into a low
dimensional space. Motivated by this result, we propose a semi-supervised
source localization algorithm which recovers the inverse mapping between the
acoustic samples and their corresponding locations. The idea is to use an
optimization framework based on manifold regularization, that involves
smoothness constraints of possible solutions with respect to the manifold. The
proposed algorithm, termed Manifold Regularization for Localization (MRL), is
implemented in an adaptive manner. The initialization is conducted with only
few labelled samples attached with their respective source locations, and then
the system is gradually adapted as new unlabelled samples (with unknown source
locations) are received. Experimental results show superior localization
performance when compared with a recently presented algorithm based on a
manifold learning approach and with the generalized cross-correlation (GCC)
algorithm as a baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03152</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03152</id><created>2015-08-13</created><authors><author><keyname>Srivastava</keyname><forenames>Amit</forenames></author><author><keyname>Maheshwari</keyname><forenames>Shikha</forenames></author></authors><title>A New Weighted Information Generating Function for Discrete Probability
  Distributions</title><categories>cs.IT math.IT</categories><journal-ref>Cybernetics and Information Technologies, 11(4),2011, pp. 24-30</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The object of this paper is to introduce a new weighted information
generating function whose derivative at point 1 gives some well known measures
of information. Some properties and particular cases of the proposed generating
function have also been studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03165</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03165</id><created>2015-08-13</created><authors><author><keyname>Amor</keyname><forenames>B.</forenames></author><author><keyname>Vuik</keyname><forenames>S.</forenames></author><author><keyname>Callahan</keyname><forenames>R.</forenames></author><author><keyname>Darzi</keyname><forenames>A.</forenames></author><author><keyname>Yaliraki</keyname><forenames>S. N.</forenames></author><author><keyname>Barahona</keyname><forenames>M.</forenames></author></authors><title>Community detection and role identification in directed networks:
  understanding the Twitter network of the care.data debate</title><categories>cs.SI physics.soc-ph</categories><comments>27 pages, 6 figures, to appear in 'Dynamic Networks and
  Cyber-Security'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of social media as an important channel for the debate and
discussion of public affairs, online social networks such as Twitter have
become important platforms for public information and engagement by policy
makers. To communicate effectively through Twitter, policy makers need to
understand how influence and interest propagate within its network of users. In
this chapter we use graph-theoretic methods to analyse the Twitter debate
surrounding NHS England's controversial care.data scheme. Directionality is a
crucial feature of the Twitter social graph - information flows from the
followed to the followers - but is often ignored in social network analyses;
our methods are based on the behaviour of dynamic processes on the network and
can be applied naturally to directed networks. We uncover robust communities of
users and show that these communities reflect how information flows through the
Twitter network. We are also able to classify users by their differing roles in
directing the flow of information through the network. Our methods and results
will be useful to policy makers who would like to use Twitter effectively as a
communication medium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03167</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03167</id><created>2015-08-13</created><authors><author><keyname>Bacher</keyname><forenames>Axel</forenames></author><author><keyname>Bodini</keyname><forenames>Olivier</forenames></author><author><keyname>Hollender</keyname><forenames>Alexandros</forenames></author><author><keyname>Lumbroso</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>MergeShuffle: A Very Fast, Parallel Random Permutation Algorithm</title><categories>cs.DS cs.DM</categories><comments>Preliminary draft. 12 pages, 1 figure, 3 algorithms, implementation
  code at https://github.com/axel-bacher/mergeshuffle</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces an algorithm, MergeShuffle, which is an extremely
efficient algorithm to generate random permutations (or to randomly permute an
existing array). It is easy to implement, runs in $n\log_2 n + O(1)$ time, is
in-place, uses $n\log_2 n + \Theta(n)$ random bits, and can be parallelized
accross any number of processes, in a shared-memory PRAM model. Finally, our
preliminary simulations using OpenMP suggest it is more efficient than the
Rao-Sandelius algorithm, one of the fastest existing random permutation
algorithms.
  We also show how it is possible to further reduce the number of random bits
consumed, by introducing a second algorithm BalancedShuffle, a variant of the
Rao-Sandelius algorithm which is more conservative in the way it recursively
partitions arrays to be shuffled. While this algorithm is of lesser practical
interest, we believe it may be of theoretical value.
  Our full code is available at: https://github.com/axel-bacher/mergeshuffle
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03170</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03170</id><created>2015-08-13</created><authors><author><keyname>Figueiredo</keyname><forenames>Paulo</forenames></author><author><keyname>Apar&#xed;cio</keyname><forenames>Marta</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author></authors><title>Generation of Multimedia Artifacts: An Extractive Summarization-based
  Approach</title><categories>cs.AI cs.CL cs.MM</categories><comments>7 pages, 2 figures</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore methods for content selection and address the issue of coherence
in the context of the generation of multimedia artifacts. We use audio and
video to present two case studies: generation of film tributes, and
lecture-driven science talks. For content selection, we use centrality-based
and diversity-based summarization, along with topic analysis. To establish
coherence, we use the emotional content of music, for film tributes, and ensure
topic similarity between lectures and documentaries, for science talks.
Composition techniques for the production of multimedia artifacts are addressed
as a means of organizing content, in order to improve coherence. We discuss our
results considering the above aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03174</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03174</id><created>2015-08-13</created><authors><author><keyname>Hernandez-Urbina</keyname><forenames>Victor</forenames></author></authors><title>Logical N-AND Gate on a Molecular Turing Machine</title><categories>cs.ET cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Boolean algebra, it is known that the logical function that corresponds to
the negation of the conjunction --NAND-- is universal in the sense that any
other logical function can be built based on it. This property makes it
essential to modern digital electronics and computer processor design. Here, we
design a molecular Turing machine that computes the NAND function over binary
strings of arbitrary length. For this purpose, we will perform a mathematical
abstraction of the kind of operations that can be done over a double-stranded
DNA molecule, as well as presenting a molecular encoding of the input symbols
for such a machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03181</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03181</id><created>2015-08-13</created><updated>2016-02-25</updated><authors><author><keyname>Boland</keyname><forenames>Natashia</forenames></author><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Rigterink</keyname><forenames>Fabian</forenames></author></authors><title>A polynomially solvable case of the pooling problem</title><categories>math.OC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answering a question of Haugland, we show that the pooling problem with one
pool and a bounded number of inputs can be solved in polynomial time by solving
a polynomial number of linear programs of polynomial size. We also give an
overview of known complexity results and remaining open problems to further
characterize the border between (strongly) NP-hard and polynomially solvable
cases of the pooling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03186</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03186</id><created>2015-08-13</created><authors><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Network-Assisted Device-to-Device (D2D) Direct Proximity Discovery with
  Underlay Communication</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted for presentation at Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-Device communications are expected to play an important role in
current and future cellular generations, by increasing the spatial reuse of
spectrum resources and enabling lower latency communication links. This
paradigm has two fundamental building blocks: (i) proximity discovery and (ii)
direct communication between proximate devices. While (ii) is treated
extensively in the recent literature, (i) has received relatively little
attention. In this paper we analyze a network-assisted underlay proximity
discovery protocol, where a cellular device can take the role of: announcer
(which announces its interest in establishing a D2D connection) or monitor
(which listens for the transmissions from the announcers). Traditionally, the
announcers transmit their messages over dedicated channel resources. In
contrast, inspired by recent advances on receivers with multiuser decoding
capabilities, we consider the case where the announcers underlay their messages
in the downlink transmissions that are directed towards the monitoring devices.
We propose a power control scheme applied to the downlink transmission, which
copes with the underlay transmission via additional power expenditure, while
guaranteeing both reliable downlink transmissions and underlay proximity
discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03204</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03204</id><created>2015-08-13</created><authors><author><keyname>Dumitrescu</keyname><forenames>Constantin</forenames></author></authors><title>On the Design of a User-in-the-Loop Channel. With Application to
  Emergency Egress</title><categories>cs.CY</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The herein approach addresses the case when the user is part of the
communication channel. A purpose of the abstraction is to provide a framework
for the field of communications, with the user included in the system,
reckoning the current 7 Layer model allows safeguards for data only. Solid
grounds for application were identified as contribution to ensuring the need
for safety which becomes tight during emergency events where timely evacuation
is critical. One of the components of a communication system [1] as described
in information theory, is the communication channel. It allows the transmitter
signal representing the message, to reach the receiver. A hardware setup with
two different communication systems in a user-in-the-loop (UIL) [2]
configuration is described and assessed pertinent to the specific application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03211</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03211</id><created>2015-08-13</created><authors><author><keyname>Myklebust</keyname><forenames>Tor G. J.</forenames></author></authors><title>Computing accurate Horner form approximations to special functions in
  finite precision arithmetic</title><categories>cs.NA cs.MS math.NA</categories><comments>10 pages, 6 figures</comments><msc-class>33F05</msc-class><acm-class>G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various applications, computers are required to compute approximations to
univariate elementary and special functions such as $\exp$ and $\arctan$ to
modest accuracy. This paper proposes a new heuristic for automating the design
of such implementations. This heuristic takes a certain restricted
specification of program structure and the desired error properties as input
and takes explicit account of roundoff error during evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03216</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03216</id><created>2015-08-13</created><authors><author><keyname>De Maio</keyname><forenames>Antonio</forenames></author><author><keyname>Orlando</keyname><forenames>Danilo</forenames></author></authors><title>Adaptive Radar Detection of a Subspace Signal Embedded in Subspace
  Structured plus Gaussian Interference Via Invariance</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with adaptive radar detection of a subspace signal competing
with two sources of interference. The former is Gaussian with unknown
covariance matrix and accounts for the joint presence of clutter plus thermal
noise. The latter is structured as a subspace signal and models coherent pulsed
jammers impinging on the radar antenna. The problem is solved via the Principle
of Invariance which is based on the identification of a suitable group of
transformations leaving the considered hypothesis testing problem invariant. A
maximal invariant statistic, which completely characterizes the class of
invariant decision rules and significantly compresses the original data domain,
as well as its statistical characterization are determined. Thus, the existence
of the optimum invariant detector is addressed together with the design of
practically implementable invariant decision rules. At the analysis stage, the
performance of some receivers belonging to the new invariant class is
established through the use of analytic expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03235</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03235</id><created>2015-08-12</created><authors><author><keyname>Kumar</keyname><forenames>Navin</forenames></author><author><keyname>Sahu</keyname><forenames>Aryabartta</forenames></author></authors><title>Bufferless NOC Simulation of Large Multicore System on GPU Hardware</title><categories>cs.DC</categories><comments>14 pages, 6 figures, Indicon14</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Last level cache management and core interconnection network play important
roles in performance and power consumption in multicore system. Large scale
chip multicore uses mesh interconnect widely due to scalability and simplicity
of the mesh interconnection design. As interconnection network occupied
significant area and consumes significant percent of system power, bufferless
network is an appealing alternative design to reduce power consumption and
hardware cost. We have designed and implemented a simulator for simulation of
distributed cache management of large chip multicore where cores are connected
using bufferless interconnection network. Also, we have redesigned and
implemented the our simulator which is a GPU compatible parallel version of the
same simulator using CUDA programming model. We have simulated target large
chip multicore with up to 43,000 cores and achieved up to 25 times speedup on
NVIDIA GeForce GTX 690 GPU over serial simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03236</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03236</id><created>2015-08-12</created><authors><author><keyname>Agrawal</keyname><forenames>T. K.</forenames></author><author><keyname>Sharma</keyname><forenames>R.</forenames></author><author><keyname>Ghose</keyname><forenames>M.</forenames></author><author><keyname>Sahu</keyname><forenames>A.</forenames></author></authors><title>Scheduling Chained Multiprocessor Tasks onto Large Multiprocessor System</title><categories>cs.DC</categories><comments>14 pages, 21 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper, we proposed an effective approach for scheduling of
multiprocessor unit time tasks with chain precedence on to large multiprocessor
system. The proposed longest chain maximum processor scheduling algorithm is
proved to be optimal for uniform chains and monotone
(non-increasing/non-decreasing) chains for both splitable and non-splitable
multiprocessor unit time tasks chain. Scheduling arbitrary chains of
non-splitable multiprocessor unit time tasks is proved to be NP-complete
problem. But scheduling arbitrary chains of splitable multiprocessor unit time
tasks is still an open problem to be proved whether it is NP-complete or can be
solved in polynomial time. We have used three heuristics (a) maximum
criticality first, (b) longest chain maximum criticality first and (c) longest
chain maximum processor first for scheduling of arbitrary chains. Also compared
performance of all three scheduling heuristics and found out that the proposed
longest chain maximum processor first performs better in most of the cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03261</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03261</id><created>2015-08-13</created><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Sun</keyname><forenames>He</forenames></author></authors><title>Constructing Linear-Sized Spectral Sparsification in Almost-Linear Time</title><categories>cs.DS cs.DM</categories><comments>22 pages. A preliminary version of this paper is to appear in
  proceedings of the 56th Annual IEEE Symposium on Foundations of Computer
  Science (FOCS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first almost-linear time algorithm for constructing
linear-sized spectral sparsification for graphs. This improves all previous
constructions of linear-sized spectral sparsification, which requires
$\Omega(n^2)$ time.
  A key ingredient in our algorithm is a novel combination of two techniques
used in literature for constructing spectral sparsification: Random sampling by
effective resistance, and adaptive constructions based on barrier functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03263</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03263</id><created>2015-08-13</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Logic Programming with Macro Connective</title><categories>cs.PL</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic programming such as Prolog is often sequential and slow because each
execution step processes only a single, $micro$ connective. To fix this
problem, we propose to use $macro$ connectives as the means of improving both
readability and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03269</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03269</id><created>2015-08-13</created><authors><author><keyname>Corsolini</keyname><forenames>Mario</forenames></author><author><keyname>Carta</keyname><forenames>Andrea</forenames></author></authors><title>A New Approach to an Old Problem: The Reconstruction of a Go Game
  through a Series of Photographs</title><categories>cs.CV</categories><comments>13 pages, 18 figures, datasets available from
  http://www.oipaz.net/PhotoKifu.html Published in the &quot;Proceedings of the
  Second International Go Game Science Conference&quot;, a part of the &quot;European Go
  Congress 2015&quot;, Liberec, Czech Republic, July 30 2015 ISBN 978-80-7378-299-3</comments><acm-class>I.2.10; I.4.8; I.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a series of photographs taken during a Go game, we describe the
techniques we successfully employ for pinpointing the grid lines of the Go
board and for tracking their small movements between consecutive photographs;
then we discuss how to approximate the location and orientation of the
observer's point of view, in order to compensate for projection effects.
Finally we describe the different criteria that jointly form the algorithm for
stones' detection, thus enabling us to automatically reconstruct the whole move
sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03273</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03273</id><created>2015-08-13</created><updated>2016-01-05</updated><authors><author><keyname>Maslov</keyname><forenames>Dmitri</forenames></author></authors><title>On the advantages of using relative phase Toffolis with an application
  to multiple control Toffoli optimization</title><categories>quant-ph cs.ET</categories><comments>14 pages</comments><journal-ref>Phys. Rev. A 93, 022311 (2016)</journal-ref><doi>10.1103/PhysRevA.93.022311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various implementations of the Toffoli gate up to a relative phase have been
known for years. The advantage over regular Toffoli gate is their smaller
circuit size. However, their use has been often limited to a demonstration of
quantum control in designs such as those where the Toffoli gate is being
applied last or otherwise for some specific reasons the relative phase does not
matter. It was commonly believed that the relative phase deviations would
prevent the relative phase Toffolis from being very helpful in practical
large-scale designs.
  In this paper, we report three circuit identities that provide the means for
replacing certain configurations of the multiple control Toffoli gates with
their simpler relative phase implementations, up to a selectable unitary on
certain qubits, and without changing the overall functionality. We illustrate
the advantage via applying those identities to the optimization of the known
circuits implementing multiple control Toffoli gates, and report the reductions
in the CNOT-count, T-count, as well as the number of ancillae used. We suggest
that a further study of the relative phase Toffoli implementations and their
use may yield other optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03276</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03276</id><created>2015-08-13</created><authors><author><keyname>Suchan</keyname><forenames>Jakob</forenames></author><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author><author><keyname>Jhavar</keyname><forenames>Harshita</forenames></author></authors><title>Talking about the Moving Image: A Declarative Model for Image Schema
  Based Embodied Perception Grounding and Language Generation</title><categories>cs.AI cs.CL cs.CV cs.HC</categories><comments>19 pages. Unpublished report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general theory and corresponding declarative model for the
embodied grounding and natural language based analytical summarisation of
dynamic visuo-spatial imagery. The declarative model ---ecompassing
spatio-linguistic abstractions, image schemas, and a spatio-temporal feature
based language generator--- is modularly implemented within Constraint Logic
Programming (CLP). The implemented model is such that primitives of the theory,
e.g., pertaining to space and motion, image schemata, are available as
first-class objects with `deep semantics' suited for inference and query. We
demonstrate the model with select examples broadly motivated by areas such as
film, design, geography, smart environments where analytical natural language
based externalisations of the moving image are central from the viewpoint of
human interaction, evidence-based qualitative analysis, and sensemaking.
  Keywords: moving image, visual semantics and embodiment, visuo-spatial
cognition and computation, cognitive vision, computational models of narrative,
declarative spatial reasoning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03280</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03280</id><created>2015-08-13</created><authors><author><keyname>Ben-Artzi</keyname><forenames>Jonathan</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author><author><keyname>Nevanlinna</keyname><forenames>Olavi</forenames></author><author><keyname>Seidel</keyname><forenames>Markus</forenames></author></authors><title>Can everything be computed? - On the Solvability Complexity Index and
  Towers of Algorithms</title><categories>cs.CC math-ph math.LO math.MP math.NA math.SP</categories><msc-class>68Q17 (Primary), 47A10, 81Q05, 81Q12, 03D15, 65J22 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes some of the fundamental barriers in the theory of
computations and finally settles the long standing computational spectral
problem. Due to these barriers, there are problems at the heart of
computational theory that do not fit into classical complexity theory. Many
computational problems can be solved as follows: a sequence of approximations
is created by an algorithm, and the solution to the problem is the limit of
this sequence. However, as we demonstrate, for several basic problems in
computations (computing spectra of operators, inverse problems or roots of
polynomials using rational maps) such a procedure based on one limit is
impossible. Yet, one can compute solutions to these problems, but only by using
several limits. This may come as a surprise, however, this touches onto the
boundaries of computational mathematics. To analyze this phenomenon we use the
Solvability Complexity Index (SCI). The SCI is the smallest number of limits
needed in the computation. We show that the SCI of spectra and essential
spectra of operators is equal to three, and that the SCI of spectra of
self-adjoint operators is equal to two, thus providing the lower bound barriers
and the first algorithms to compute such spectra in two and three limits. This
finally settles the long standing computational spectral problem. In addition,
we provide bounds for the SCI of spectra of classes of Schr\&quot;{o}dinger
operators, thus we affirmatively answer the long standing question on whether
or not these spectra can actually be computed. The SCI yields a framework for
understanding barriers in computations. It has a direct link to the
Arithmetical Hierarchy, and we demonstrate how the impossibility result of
McMullen on polynomial root finding with rational maps in one limit and the
results of Doyle and McMullen on solving the quintic in several limits can be
put in the SCI framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03285</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03285</id><created>2015-08-13</created><updated>2015-08-18</updated><authors><author><keyname>Huang</keyname><forenames>Yinjie</forenames></author><author><keyname>Georgiopoulos</keyname><forenames>Michael</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Georgios C.</forenames></author></authors><title>Hash Function Learning via Codewords</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel hash learning framework that has two main
distinguishing features, when compared to past approaches. First, it utilizes
codewords in the Hamming space as ancillary means to accomplish its hash
learning task. These codewords, which are inferred from the data, attempt to
capture similarity aspects of the data's hash codes. Secondly and more
importantly, the same framework is capable of addressing supervised,
unsupervised and, even, semi-supervised hash learning tasks in a natural
manner. A series of comparative experiments focused on content-based image
retrieval highlights its performance advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03298</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03298</id><created>2015-08-13</created><authors><author><keyname>Katz</keyname><forenames>Gilad</forenames></author><author><keyname>Shapira</keyname><forenames>Bracha</forenames></author></authors><title>Enabling Complex Wikipedia Queries - Technical Report</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report we present a database schema used to store Wikipedia
so it can be easily used in query-intensive applications. In addition to
storing the information in a way that makes it highly accessible, our schema
enables users to easily formulate complex queries using information such as the
anchor-text of links and their location in the page, the titles and number of
redirect pages for each page and the paragraph structure of entity pages. We
have successfully used the schema in domains such as recommender systems,
information retrieval and sentiment analysis. In order to assist other
researchers, we now make the schema and its content available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03309</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03309</id><created>2015-08-13</created><authors><author><keyname>Syrakos</keyname><forenames>Alexandros</forenames></author><author><keyname>Goulas</keyname><forenames>Apostolos</forenames></author></authors><title>Finite volume adaptive solutions using SIMPLE as smoother</title><categories>physics.comp-ph cs.NA physics.flu-dyn</categories><journal-ref>International Journal for Numerical Methods in Fluids 52 (2006)
  1215-1245</journal-ref><doi>10.1002/fld.1228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new multilevel procedure that can solve the discrete
Navier-Stokes system arising from finite volume discretizations on composite
grids, which may consist of more than one level. SIMPLE is used and tested as
the smoother, but the multilevel procedure is such that it does not exclude the
use of other smoothers. Local refinement is guided by a criterion based on an
estimate of the truncation error. The numerical experiments presented test not
only the behaviour of the multilevel algebraic solver, but also the efficiency
of local refinement based on this particular criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03310</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03310</id><created>2015-08-13</created><authors><author><keyname>Couceiro</keyname><forenames>Miguel</forenames></author><author><keyname>Marichal</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Teheux</keyname><forenames>Bruno</forenames></author></authors><title>Relaxations of associativity and preassociativity for variadic functions</title><categories>math.GR cs.DM math.RA</categories><msc-class>20M05, 20M32, 39B72, 68R99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider two properties of variadic functions, namely
associativity and preassociativity, that are pertaining to several data and
language processing tasks. We propose parameterized relaxations of these
properties and provide their descriptions in terms of factorization results. We
also give an example where these parameterized notions give rise to natural
hierarchies of functions and indicate their potential use in measuring the
degrees of associativeness and preassociativeness. We illustrate these results
by several examples and constructions and discuss some open problems that lead
to further directions of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03311</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03311</id><created>2015-08-13</created><authors><author><keyname>Mandal</keyname><forenames>Dibyendu</forenames></author><author><keyname>Boyd</keyname><forenames>Alexander B.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Memoryless Thermodynamics? A Reply</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>4 pages; http://csc.ucdavis.edu/~cmg/compmech/pubs/MerhavReply.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reply to arXiv:1508.00203 `Comment on &quot;Identifying Functional
Thermodynamics in Autonomous Maxwellian Ratchets&quot; (arXiv:1507.01537v2)'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03325</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03325</id><created>2015-08-13</created><authors><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Competition and cooperation among different punishing strategies in the
  spatial public goods game</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>6 two-column pages, 5 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 92 (2015) 012819</journal-ref><doi>10.1103/PhysRevE.92.012819</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the fact that people have diverse propensities to punish
wrongdoers, we study a spatial public goods game with defectors and different
types of punishing cooperators. During the game, cooperators punish defectors
with class-specific probabilities and subsequently share the associated costs
of sanctioning. We show that in the presence of different punishing cooperators
the highest level of public cooperation is always attainable through a
selection mechanism. Interestingly, the selection not necessarily favors the
evolution of punishers who would be able to prevail on their own against the
defectors, nor does it always hinder the evolution of punishers who would be
unable to prevail on their own. Instead, the evolutionary success of punishing
strategies depends sensitively on their invasion velocities, which in turn
reveals fascinating examples of both competition and cooperation among them.
Furthermore, we show that under favorable conditions, when punishment is not
strictly necessary for the maintenance of public cooperation, the less
aggressive, mild form of sanctioning is the sole victor of selection process.
Our work reveals that natural strategy selection can not only promote, but
sometimes also hinder competition among prosocial strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03326</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03326</id><created>2015-08-13</created><updated>2016-02-01</updated><authors><author><keyname>Zhou</keyname><forenames>Li</forenames></author></authors><title>A Survey on Contextual Multi-armed Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this survey we cover a few stochastic and adversarial contextual bandit
algorithms. We analyze each algorithm's assumption and regret bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03329</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03329</id><created>2015-08-13</created><authors><author><keyname>Yousefi</keyname><forenames>Niloofar</forenames></author><author><keyname>Georgiopoulos</keyname><forenames>Michael</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Georgios C.</forenames></author></authors><title>Multi-Task Learning with Group-Specific Feature Space Sharing</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When faced with learning a set of inter-related tasks from a limited amount
of usable data, learning each task independently may lead to poor
generalization performance. Multi-Task Learning (MTL) exploits the latent
relations between tasks and overcomes data scarcity limitations by co-learning
all these tasks simultaneously to offer improved performance. We propose a
novel Multi-Task Multiple Kernel Learning framework based on Support Vector
Machines for binary classification tasks. By considering pair-wise task
affinity in terms of similarity between a pair's respective feature spaces, the
new framework, compared to other similar MTL approaches, offers a high degree
of flexibility in determining how similar feature spaces should be, as well as
which pairs of tasks should share a common feature space in order to benefit
overall performance. The associated optimization problem is solved via a block
coordinate descent, which employs a consensus-form Alternating Direction Method
of Multipliers algorithm to optimize the Multiple Kernel Learning weights and,
hence, to determine task affinities. Empirical evaluation on seven data sets
exhibits a statistically significant improvement of our framework's results
compared to the ones of several other Clustered Multi-Task Learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03337</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03337</id><created>2015-08-13</created><updated>2016-02-16</updated><authors><author><keyname>Fountoulakis</keyname><forenames>Kimon</forenames></author><author><keyname>Kundu</keyname><forenames>Abhisek</forenames></author><author><keyname>Kontopoulou</keyname><forenames>Eugenia-Maria</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>A Randomized Rounding Algorithm for Sparse PCA</title><categories>cs.DS cs.LG stat.ML</categories><comments>22 pages, 31 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and analyze a simple, two-step algorithm to approximate the
optimal solution of the sparse PCA problem. Our approach first solves a convex
L1 relaxation of the NP-hard sparse PCA optimization problem and then uses a
randomized rounding strategy to sparsify the resulting dense solution. Our main
theoretical result guarantees an additive error approximation and provides a
tradeoff between sparsity and accuracy. Our experimental evaluation indicates
that our approach is competitive in practice, even compared to state-of-the-art
toolboxes such as Spasm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03348</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03348</id><created>2015-08-13</created><updated>2015-08-17</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Ramnath</keyname><forenames>Rajiv</forenames></author></authors><title>Looking at Software Sustainability and Productivity Challenges from NSF</title><categories>cs.CY cs.SE</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper is a contribution to the Computational Science &amp; Engineering
Software Sustainability and Productivity Challenges (CSESSP Challenges)
Workshop (https://www.nitrd.gov/csessp/), sponsored by the Networking and
Information Technology Research and Development (NITRD) Software Design and
Productivity (SDP) Coordinating Group, held October 15th-16th 2015 in
Washington DC, USA. It introduces the role of software at the National Science
Foundation (NSF) and the NSF Software Infrastructure for Sustained Innovation
(SI2) program, then describes challenges that the SI2 program has identified,
including funding models, career paths, incentives, training, interdisciplinary
work, portability, and dissemination, as well as lesson that have been learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03349</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03349</id><created>2015-08-13</created><updated>2016-01-21</updated><authors><author><keyname>Noorzad</keyname><forenames>Parham</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>The Multivariate Covering Lemma and its Converse</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multivariate covering lemma states that given a collection of $k$
codebooks, each of sufficiently large cardinality and independently generated
according to one of the marginals of a joint distribution, one can always
choose one codeword from each codebook such that the resulting $k$-tuple of
codewords is jointly typical with respect to the joint distribution. We give a
proof of this lemma for weakly typical sets. This allows achievability proofs
that rely on the covering lemma to go through for continuous channels (e.g.,
Gaussian) without the need for quantization. The covering lemma and its
converse are widely used in information theory, including in rate-distortion
theory and in achievability results for multi-user channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03351</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03351</id><created>2015-08-13</created><authors><author><keyname>Wang</keyname><forenames>Shaocheng</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author></authors><title>On the Consistency and Confidence of Distributed Dynamic State
  Estimation in Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>This paper is to appear at the 54th IEEE Conference on Decision and
  Control in December, 2015 at Osaka, Japan. This version contains detailed
  proofs and simulation examples</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The problem of distributed dynamic state estimation in wireless sensor
networks is studied. Two important properties of local estimates, namely, the
consistency and confidence, are emphasized. On one hand, the consistency, which
means that the approximated error covariance is lower bounded by the true
unknown one, has to be guaranteed so that the estimate is not over-confident.
On the other hand, since the confidence indicates the accuracy of the estimate,
the estimate should be as confident as possible. We first analyze two different
information fusion strategies used in the case of information sources with,
respectively, uncorrelated errors and unknown but correlated errors. Then a
distributed hybrid information fusion algorithm is proposed, where each agent
uses the information obtained not only by itself, but also from its neighbors
through communication. The proposed algorithm not only guarantees the
consistency of the estimates, but also utilizes the available information
sources in a more efficient manner and hence improves the confidence. Besides,
the proposed algorithm is fully distributed and guarantees convergence with the
sufficient condition formulated. The comparisons with existing algorithms are
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03353</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03353</id><created>2015-08-13</created><authors><author><keyname>Khalid</keyname><forenames>Zubair</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author></authors><title>Gauss-Legendre Sampling on the Rotation Group</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figures</comments><doi>10.1109/LSP.2015.2503295</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Gauss-Legendre quadrature based sampling on the rotation group
for the representation of a band-limited signal such that the Fourier transform
(FT) of a signal can be exactly computed from its samples. Our figure of merit
is the sampling efficiency, which is defined as a ratio of the degrees of
freedom required to represent a band-limited signal in harmonic domain to the
number of samples required to accurately compute the FT. The proposed sampling
scheme is asymptotically as efficient as the most efficient scheme developed
very recently. For the computation of FT and inverse FT, we also develop fast
algorithms of complexity similar to the complexity attained by the fast
algorithms for the existing sampling schemes. The developed algorithms are
stable, accurate and do not have any pre-computation requirements. We also
analyse the computation time and numerical accuracy of the proposed algorithms
and show, through numerical experiments, that the proposed Fourier transforms
are accurate with errors on the order of numerical precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03371</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03371</id><created>2015-08-13</created><authors><author><keyname>Guo</keyname><forenames>Ruocheng</forenames></author><author><keyname>Shaabani</keyname><forenames>Elham</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Abhinav</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author></authors><title>Toward Order-of-Magnitude Cascade Prediction</title><categories>cs.SI physics.soc-ph</categories><comments>4 pages, 15 figures, ASONAM 2015 poster paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a piece of information (microblog, photograph, video, link, etc.) starts
to spread in a social network, an important question arises: will it spread to
&quot;viral&quot; proportions -- where &quot;viral&quot; is defined as an order-of-magnitude
increase. However, several previous studies have established that cascade size
and frequency are related through a power-law - which leads to a severe
imbalance in this classification problem. In this paper, we devise a suite of
measurements based on &quot;structural diversity&quot; -- the variety of social contexts
(communities) in which individuals partaking in a given cascade engage. We
demonstrate these measures are able to distinguish viral from non-viral
cascades, despite the severe imbalance of the data for this problem. Further,
we leverage these measurements as features in a classification approach,
successfully predicting microblogs that grow from 50 to 500 reposts with
precision of 0.69 and recall of 0.52 for the viral class - despite this class
comprising under 2\% of samples. This significantly outperforms our baseline
approach as well as the current state-of-the-art. Our work also demonstrates
how we can tradeoff between precision and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03374</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03374</id><created>2015-08-13</created><authors><author><keyname>Vakilian</keyname><forenames>Vida</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Roy</keyname><forenames>Sebastien</forenames></author></authors><title>Space-Frequency Block Code for MIMO-OFDM Communication Systems with
  Reconfigurable Antennas</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a space-frequency (SF) block coding scheme for a multiple-input
multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) system
using antennas with reconfigurable radiation patterns. In this system, each
element of the antenna array at the transmitter side is assumed to be
reconfigurable so that it can independently change the physical characteristics
of its radiation pattern. The proposed block code is full rate and benefits
from spatial, frequency, and reconfigurable radiation pattern state diversity
over frequency-selective fading channels. We provide simulation results to
demonstrate the performance of the proposed block coding technique and make
comparisons with that of the previous SF coding schemes in MIMO-OFDM systems.
The results indicate that the proposed code achieves higher diversity and
coding gain compared to other available SF codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03379</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03379</id><created>2015-08-13</created><authors><author><keyname>Leskel&#xe4;</keyname><forenames>Lasse</forenames></author><author><keyname>Ngo</keyname><forenames>Hoa</forenames></author></authors><title>The impact of degree variability on connectivity properties of large
  networks</title><categories>cs.SI math.PR physics.soc-ph</categories><msc-class>91D30, 60E15</msc-class><acm-class>J.4; G.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of is to study how increased variability in the degree distribution
impacts the global connectivity properties of a large network. We approach this
question by modeling the network as a uniform random graph with a given degree
sequence. We analyze the effect of the degree variability on the approximate
size of the largest connected component using stochastic ordering techniques. A
counterexample shows that a higher degree variability may lead to a larger
connected component, contrary to basic intuition about branching processes.
When certain extremal cases are ruled out, the higher degree variability is
shown to decrease the limiting approximate size of the largest connected
component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03381</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03381</id><created>2015-08-13</created><authors><author><keyname>Xu</keyname><forenames>Hangjun</forenames></author></authors><title>An Algorithm for Comparing Similarity Between Two Trees</title><categories>cs.CG</categories><comments>M.S. degree Project, Department of Computer Science, Duke University,
  April, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  An important problem in geometric computing is defining and computing
similarity between two geometric shapes, e.g. point sets, curves and surfaces,
etc. Important geometric and topological information of many shapes can be
captured by defining a tree structure on them (e.g. medial axis and contour
trees). Hence, it is natural to study the problem of comparing similarity
between trees. We study gapped edit distance between two ordered labeled trees,
first proposed by Touzet \cite{Touzet2003}.
  Given two binary trees $T_{1}$ and $T_{2}$ with $m$ and $n$ nodes. We compute
the general gap edit distance in $O(m^{3}n^{2} + m^{2}n^{3})$ time. The
computation of this distance in the case of arbitrary trees has shown to be
NP-hard \cite{Touzet2003}. We also give an algorithm for computing the complete
subtree gap edit distance, which can be applied to comparing contour trees of
terrains in $\mathbb{R}^{3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03386</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03386</id><created>2015-08-13</created><authors><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Gasic</keyname><forenames>Milica</forenames></author><author><keyname>Kim</keyname><forenames>Dongho</forenames></author><author><keyname>Mrksic</keyname><forenames>Nikola</forenames></author><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Learning from Real Users: Rating Dialogue Success with Neural Networks
  for Reinforcement Learning in Spoken Dialogue Systems</title><categories>cs.LG cs.CL</categories><comments>Accepted for publication in INTERSPEECH 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To train a statistical spoken dialogue system (SDS) it is essential that an
accurate method for measuring task success is available. To date training has
relied on presenting a task to either simulated or paid users and inferring the
dialogue's success by observing whether this presented task was achieved or
not. Our aim however is to be able to learn from real users acting under their
own volition, in which case it is non-trivial to rate the success as any prior
knowledge of the task is simply unavailable. User feedback may be utilised but
has been found to be inconsistent. Hence, here we present two neural network
models that evaluate a sequence of turn-level features to rate the success of a
dialogue. Importantly these models make no use of any prior knowledge of the
user's task. The models are trained on dialogues generated by a simulated user
and the best model is then used to train a policy on-line which is shown to
perform at least as well as a baseline system using prior knowledge of the
user's task. We note that the models should also be of interest for evaluating
SDS and for monitoring a dialogue in rule-based SDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03388</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03388</id><created>2015-08-13</created><authors><author><keyname>Dubois</keyname><forenames>Catherine</forenames><affiliation>ENSIIE</affiliation></author><author><keyname>Masci</keyname><forenames>Paolo</forenames><affiliation>Queen Mary University of London</affiliation></author><author><keyname>M&#xe9;ry</keyname><forenames>Dominique</forenames><affiliation>LORIA, Universit&#xe9; de Lorraine</affiliation></author></authors><title>Proceedings Second International Workshop on Formal Integrated
  Development Environment</title><categories>cs.PL cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015</journal-ref><doi>10.4204/EPTCS.187</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of F-IDE 2015, the second international
workshop on Formal Integrated Development Environment, which was held as an FM
2015 satellite event, on June 22, 2015, in Oslo (Norway). High levels of
safety, security and also privacy standards require the use of formal methods
to specify and develop compliant software (sub)systems. Any standard comes with
an assessment process, which requires a complete documentation of the
application in order to ease the justification of design choices and the review
of code and proofs. Thus tools are needed for handling specifications, program
constructs and verification artifacts. The aim of the F-IDE workshop is to
provide a forum for presenting and discussing research efforts as well as
experience returns on design, development and usage of formal IDE aiming at
making formal methods &quot;easier&quot; for both specialists and non-specialists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03389</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03389</id><created>2015-08-13</created><authors><author><keyname>ter Beek</keyname><forenames>Maurice H.</forenames><affiliation>ISTI-CNR, Pisa, Italy</affiliation></author><author><keyname>Lafuente</keyname><forenames>Alberto Lluch</forenames><affiliation>DTU, Denmark</affiliation></author></authors><title>Proceedings 11th International Workshop on Automated Specification and
  Verification of Web Systems</title><categories>cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 188, 2015</journal-ref><doi>10.4204/EPTCS.188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These proceedings contain the papers presented at the 11th International
Workshop on Automated Specification and Verification of Web Systems (WWV 2015),
which was held on 23 June 2015 in Oslo, Norway, as a satellite workshop of the
20th International Symposium on Formal Methods (FM 2015). WWV is a yearly
interdisciplinary forum for researchers originating from the following areas:
declarative, rule-based programming, formal methods, software engineering and
web-based systems. The workshop fosters the cross-fertilisation and advancement
of hybrid methods from such areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03390</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03390</id><created>2015-08-13</created><authors><author><keyname>Yu</keyname><forenames>Adams Wei</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author></authors><title>Doubly Stochastic Primal-Dual Coordinate Method for Regularized
  Empirical Risk Minimization with Factorized Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed a doubly stochastic primal-dual coordinate optimization algorithm
for regularized empirical risk minimization that can be formulated as a
saddle-point problem using conjugate function. Different from existing
coordinate methods, the proposed method randomly samples both primal and dual
coordinates to update solutions, which is a desirable property when applied to
data with both a high dimension and a large size. The convergence of our method
is established not only in terms of the solution's distance to optimality but
also in terms of the primal-dual objective gap. When applied to the data matrix
factorized as a product of two smaller matrices, we show that the proposed
method has a lower overall complexity than other coordinate methods,
especially, when data size is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03391</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03391</id><created>2015-08-13</created><updated>2015-08-18</updated><authors><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Gasic</keyname><forenames>Milica</forenames></author><author><keyname>Mrksic</keyname><forenames>Nikola</forenames></author><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Reward Shaping with Recurrent Neural Networks for Speeding up On-Line
  Policy Learning in Spoken Dialogue Systems</title><categories>cs.LG cs.CL</categories><comments>Accepted for publication in SigDial 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical spoken dialogue systems have the attractive property of being
able to be optimised from data via interactions with real users. However in the
reinforcement learning paradigm the dialogue manager (agent) often requires
significant time to explore the state-action space to learn to behave in a
desirable manner. This is a critical issue when the system is trained on-line
with real users where learning costs are expensive. Reward shaping is one
promising technique for addressing these concerns. Here we examine three
recurrent neural network (RNN) approaches for providing reward shaping
information in addition to the primary (task-orientated) environmental
feedback. These RNNs are trained on returns from dialogues generated by a
simulated user and attempt to diffuse the overall evaluation of the dialogue
back down to the turn level to guide the agent towards good behaviour faster.
In both simulated and real user scenarios these RNNs are shown to increase
policy learning speed. Importantly, they do not require prior knowledge of the
user's goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03395</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03395</id><created>2015-08-13</created><authors><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author></authors><title>Information-theoretic Bounds on Matrix Completion under Union of
  Subspaces Model</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note we extend some of the recent results on matrix completion
under the assumption that the columns of the matrix can be grouped (clustered)
into subspaces (not necessarily disjoint or independent). This model deviates
from the typical assumption prevalent in the literature dealing with
compression and recovery for big-data applications. The results have a direct
bearing on the problem of subspace clustering under missing or incomplete
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03398</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03398</id><created>2015-08-13</created><updated>2015-11-01</updated><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>He</keyname><forenames>Ji</forenames></author><author><keyname>Shen</keyname><forenames>Yelong</forenames></author><author><keyname>Xiao</keyname><forenames>Lin</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Song</keyname><forenames>Xinying</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>End-to-end Learning of LDA by Mirror-Descent Back Propagation over a
  Deep Architecture</title><categories>cs.LG</categories><comments>Proc. NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a fully discriminative learning approach for supervised Latent
Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which
maximizes the posterior probability of the prediction variable given the input
document. Different from traditional variational learning or Gibbs sampling
approaches, the proposed learning method applies (i) the mirror descent
algorithm for maximum a posterior inference and (ii) back propagation over a
deep architecture together with stochastic gradient/mirror descent for model
parameter estimation, leading to scalable and end-to-end discriminative
learning of the model. As a byproduct, we also apply this technique to develop
a new learning method for the traditional unsupervised LDA model (i.e.,
BP-LDA). Experimental results on three real-world regression and classification
tasks show that the proposed methods significantly outperform the previous
supervised topic models, neural networks, and is on par with deep neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03401</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03401</id><created>2015-08-13</created><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Binary Compressive Sensing via Analog Fountain Coding</title><categories>cs.IT math.IT</categories><comments>The paper is accepted to publish in IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a compressive sensing (CS) approach is proposed for sparse
binary signals' compression and reconstruction based on analog fountain codes
(AFCs). In the proposed scheme, referred to as the analog fountain compressive
sensing (AFCS), each measurement is generated from a linear combination of L
randomly selected binary signal elements with real weight coefficients. The
weight coefficients are chosen from a finite weight set and L, called
measurement degree, is obtained based on a predefined degree distribution
function. We propose a simple verification based reconstruction algorithm for
the AFCS in the noiseless case. The proposed verification based decoder is
analyzed through SUM-OR tree analytical approach and an optimization problem is
formulated to find the optimum measurement degree to minimize the number of
measurements required for the reconstruction of binary sparse signals. We show
that in the AFCS, the number of required measurements is of O(-n log(1-k/n)),
where n is the signal length and k is the signal sparsity level. We then
consider the signal reconstruction of AFCS in the presence of additive white
Gaussian noise (AWGN) and the standard message passing decoder is then used for
the signal recovery. Simulation results show that the AFCS can perfectly
recover all non-zero elements of the sparse binary signal with a significantly
reduced number of measurements, compared to the conventional binary CS and
L1-minimization approaches in a wide range of signal to noise ratios (SNRs).
Finally, we show a practical application of the AFCS for the sparse event
detection in wireless sensor networks (WSNs), where the sensors' readings can
be treated as measurements from the CS point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03410</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03410</id><created>2015-08-13</created><authors><author><keyname>Karami</keyname><forenames>Mohammad</forenames></author><author><keyname>Park</keyname><forenames>Youngsam</forenames></author><author><keyname>McCoy</keyname><forenames>Damon</forenames></author></authors><title>Stress Testing the Booters: Understanding and Undermining the Business
  of DDoS Services</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DDoS-for-hire services, also known as booters, have commoditized DDoS attacks
and enabled abusive subscribers of these services to cheaply extort, harass and
intimidate businesses and people by knocking them offline. However, due to the
underground nature of these booters, little is known about their underlying
technical and business structure. In this paper we empirically measure many
facets of their technical and payment infrastructure. We also perform an
analysis of leaked and scraped data from three major booters---Asylum Stresser,
Lizard Stresser and VDO---which provides us with an in-depth view of their
customers and victims. Finally, we conduct a large-scale payment intervention
in collaboration with PayPal and evaluate its effectiveness. Based on our
analysis we show that these services are responsible for hundreds of thousands
of DDoS attacks and identify potentially promising methods of increasing
booters' costs and undermining these services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03411</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03411</id><created>2015-08-13</created><updated>2015-08-23</updated><authors><author><keyname>Hallak</keyname><forenames>Assaf</forenames></author><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Emphatic TD Bellman Operator is a Contraction</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, \citet{SuttonMW15} introduced the emphatic temporal differences
(ETD) algorithm for off-policy evaluation in Markov decision processes. In this
short note, we show that the projected fixed-point equation that underlies ETD
involves a contraction operator, with a $\sqrt{\gamma}$-contraction modulus
(where $\gamma$ is the discount factor). This allows us to provide error bounds
on the approximation error of ETD. To our knowledge, these are the first error
bounds for an off-policy evaluation algorithm under general target and behavior
policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03415</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03415</id><created>2015-08-14</created><authors><author><keyname>Xu</keyname><forenames>Guangkui</forenames></author><author><keyname>Cao</keyname><forenames>Xiwang</forenames></author></authors><title>Several classes of bent, near-bent and 2-plateaued functions over finite
  fields of odd characteristic</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a recent work of Mesnager, we present several new infinite
families of quadratic ternary bent, near-bent and 2-plateaued functions from
some known quadratic ternary bent functions. Meanwhile, the distribution of the
Walsh spectrum of two class of 2-plateaued functions obtained in this paper is
completely determined. Additionally, we construct the first class of $p$-ary
bent functions of algebraic degree $p$ over the fields of an arbitrary odd
characteristic. The proposed class contains non-quadratic $p$-ary bent
functions that are affinely inequivalent to known monomial and binomial ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03420</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03420</id><created>2015-08-14</created><authors><author><keyname>Ravner</keyname><forenames>Liron</forenames></author><author><keyname>Haviv</keyname><forenames>Moshe</forenames></author><author><keyname>Vu</keyname><forenames>Hai L.</forenames></author></authors><title>A strategic timing of arrivals to a linear slowdown processor sharing
  system</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a discrete population of users with homogeneous service demand
who need to decide when to arrive to a system in which the service rate
deteriorates linearly with the number of users in the system. The users have
heterogeneous desired departure times from the system, and their goal is to
minimize a weighted sum of the travel time and square deviation from the
desired departure times. Users join the system sequentially, according to the
order of their desired departure times. We model this scenario as a
non-cooperative game in which each user selects his actual arrival time. We
present explicit equilibria solutions for a two-user example, namely the
subgame perfect and Nash equilibria and show that multiple equilibria may
exist. We further explain why a general solution for any number of users is
computationally challenging. The difficulty lies in the fact that the objective
functions are piecewise convex, i.e., non-smooth and non-convex. As a result,
the minimization of the costs relies on checking all arrival and departure
order permutations, which is exponentially large with respect to the population
size. Instead we propose an iterated best-response algorithm which can be
efficiently studied numerically. Finally, we compare the equilibrium arrival
profiles to a socially optimal solution and discuss the implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03422</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03422</id><created>2015-08-14</created><updated>2015-12-08</updated><authors><author><keyname>Khan</keyname><forenames>Salman H.</forenames></author><author><keyname>Bennamoun</keyname><forenames>Mohammed</forenames></author><author><keyname>Sohel</keyname><forenames>Ferdous</forenames></author><author><keyname>Togneri</keyname><forenames>Roberto</forenames></author></authors><title>Cost Sensitive Learning of Deep Feature Representations from Imbalanced
  Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Class imbalance is a common problem in the case of real-world object
detection and classification tasks. Data of some classes is abundant making
them an over-represented majority, and data of other classes is scarce, making
them an under-represented minority. This imbalance makes it challenging for a
classifier to appropriately learn the discriminating boundaries of the majority
and minority classes. In this work, we propose a cost sensitive deep neural
network which can automatically learn robust feature representations for both
the majority and minority classes. During training, our learning procedure
jointly optimizes the class dependent costs and the neural network parameters.
The proposed approach is applicable to both binary and multi-class problems
without any modification. Moreover, as opposed to data level approaches, we do
not alter the original data distribution which results in a lower computational
cost during the training process. We report the results of our experiments on
six major image classification datasets and show that the proposed approach
significantly outperforms the baseline algorithms. Comparisons with popular
data sampling techniques and cost sensitive classifiers demonstrate the
superior performance of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03428</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03428</id><created>2015-08-14</created><authors><author><keyname>Papamichail</keyname><forenames>Dimitris</forenames></author><author><keyname>Liu</keyname><forenames>Hongmei</forenames></author><author><keyname>Machado</keyname><forenames>Vitor</forenames></author><author><keyname>Gould</keyname><forenames>Nathan</forenames></author><author><keyname>Coleman</keyname><forenames>J. Robert</forenames></author><author><keyname>Papamichail</keyname><forenames>Georgios</forenames></author></authors><title>Codon Context Optimization in Synthetic Gene Design</title><categories>cs.DS cs.CE q-bio.GN</categories><comments>9 pages, 5 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Advances in de novo synthesis of DNA and computational gene design methods
make possible the customization of genes by direct manipulation of features
such as codon bias and mRNA secondary structure. Codon context is another
feature significantly affecting mRNA translational efficiency, but existing
methods and tools for evaluating and designing novel optimized protein coding
sequences utilize untested heuristics and do not provide quantifiable
guarantees on design quality. In this study we examine statistical properties
of codon context measures in an effort to better understand the phenomenon. We
analyze the computational complexity of codon context optimization and design
exact and efficient heuristic gene recoding algorithms under reasonable
constraint models. We also present a web-based tool for evaluating codon
context bias in the appropriate context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03431</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03431</id><created>2015-08-14</created><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Gurvich</keyname><forenames>Vladimir</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author></authors><title>A Pseudo-Polynomial Algorithm for Mean Payoff Stochastic Games with
  Perfect Information and Few Random Positions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-person zero-sum stochastic mean payoff games with perfect
information, or BWR-games, given by a digraph $G = (V, E)$, with local rewards
$r: E \to \ZZ$, and three types of positions: black $V_B$, white $V_W$, and
random $V_R$ forming a partition of $V$. It is a long-standing open question
whether a polynomial time algorithm for BWR-games exists, or not, even when
$|V_R|=0$. In fact, a pseudo-polynomial algorithm for BWR-games would already
imply their polynomial solvability. In this paper, we show that BWR-games with
a constant number of random positions can be solved in pseudo-polynomial time.
More precisely, in any BWR-game with $|V_R|=O(1)$, a saddle point in uniformly
optimal pure stationary strategies can be found in time polynomial in
$|V_W|+|V_B|$, the maximum absolute local reward, and the common denominator of
the transition probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03441</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03441</id><created>2015-08-14</created><authors><author><keyname>Mduma</keyname><forenames>Neema</forenames></author><author><keyname>Kalegele</keyname><forenames>Khamisi</forenames></author></authors><title>An Integrated Mobile Application for Enhancing Management of Nutrition
  Information in Arusha Tanzania</title><categories>cs.CY</categories><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 13, No. 7, July 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the fact that management of nutrition information is still a problem
in many developing countries including Tanzania and nutrition information is
only verbally provided without emphasis, this study proposes mobile application
for enhancing management of nutrition information. The paper discusses the
implementation of an integrated mobile application for enhancing management of
nutrition information based on literature review and interviews, which were
conducted in Arusha region for the collection of key information and details
required for designing the mobile application. In this application, PHP
technique has been used to build the application logic and MySQL technology for
developing the back-end database. Using XML and Java, we have built an
application interface that provides easy interactive view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03446</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03446</id><created>2015-08-14</created><authors><author><keyname>Bastug</keyname><forenames>Mert</forenames></author><author><keyname>Petreczky</keyname><forenames>Mihaly</forenames></author><author><keyname>Toth</keyname><forenames>Roland</forenames></author><author><keyname>Wisniewski</keyname><forenames>Rafael</forenames></author><author><keyname>Leth</keyname><forenames>John</forenames></author><author><keyname>Efimov</keyname><forenames>Denis</forenames></author></authors><title>Moment Matching Based Model Reduction for LPV State-Space Models</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm for reducing the state dimension, i.e. order, of
linear parameter varying (LPV) discrete-time state-space (SS) models with
affine dependence on the scheduling variable. The input-output behavior of the
reduced order model approximates that of the original model. In fact, for input
and scheduling sequences of a certain length, the input-output behaviors of the
reduced and original model coincide. The proposed method can also be
interpreted as a reachability and observability reduction (minimization)
procedure for LPV-SS representations with affine dependence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03455</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03455</id><created>2015-08-14</created><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Gurvich</keyname><forenames>Vladimir</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author></authors><title>A Potential Reduction Algorithm for Two-person Zero-sum Mean Payoff
  Stochastic Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest a new algorithm for two-person zero-sum undiscounted stochastic
games focusing on stationary strategies. Given a positive real $\epsilon$, let
us call a stochastic game $\epsilon$-ergodic, if its values from any two
initial positions differ by at most $\epsilon$. The proposed new algorithm
outputs for every $\epsilon&gt;0$ in finite time either a pair of stationary
strategies for the two players guaranteeing that the values from any initial
positions are within an $\epsilon$-range, or identifies two initial positions
$u$ and $v$ and corresponding stationary strategies for the players proving
that the game values starting from $u$ and $v$ are at least $\epsilon/24$
apart. In particular, the above result shows that if a stochastic game is
$\epsilon$-ergodic, then there are stationary strategies for the players
proving $24\epsilon$-ergodicity. This result strengthens and provides a
constructive version of an existential result by Vrieze (1980) claiming that if
a stochastic game is $0$-ergodic, then there are $\epsilon$-optimal stationary
strategies for every $\epsilon &gt; 0$. The suggested algorithm is based on a
potential transformation technique that changes the range of local values at
all positions without changing the normal form of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03458</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03458</id><created>2015-08-14</created><authors><author><keyname>Shiraga</keyname><forenames>Takeharu</forenames></author><author><keyname>Yamauchi</keyname><forenames>Yukiko</forenames></author><author><keyname>Kijima</keyname><forenames>Shuji</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>Total Variation Discrepancy of Deterministic Random Walks for Ergodic
  Markov Chains</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a derandomization of Markov chain Monte Carlo (MCMC), this paper
investigates deterministic random walks, which is a deterministic process
analogous to a random walk. While there are several progresses on the analysis
of the vertex-wise discrepancy (i.e., $L_\infty$ discrepancy), little is known
about the {\em total variation discrepancy} (i.e., $L_1$ discrepancy), which
plays a significant role in the analysis of an FPRAS based on MCMC. This paper
investigates upper bounds of the $L_1$ discrepancy between the expected number
of tokens in a Markov chain and the number of tokens in its corresponding
deterministic random walk. First, we give a simple but nontrivial upper bound
${\rm O}(mt^*)$ of the $L_1$ discrepancy for any ergodic Markov chains, where
$m$ is the number of edges of the transition diagram and $t^*$ is the mixing
time of the Markov chain. Then, we give a better upper bound ${\rm
O}(m\sqrt{t^*\log t^*})$ for non-oblivious deterministic random walks, if the
corresponding Markov chain is ergodic and lazy. We also present some lower
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03473</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03473</id><created>2015-08-14</created><authors><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author></authors><title>A Lower Bound on the Diameter of the Flip Graph</title><categories>cs.CG cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The flip graph is the graph whose nodes correspond to non-isomorphic
combinatorial triangulations and whose edges connect pairs of triangulations
that can be obtained one from the other by flipping a single edge. In this note
we show that the diameter of the flip graph is at least $\frac{7n}{3} +
\Theta(1)$, improving upon the previous $2n + \Theta(1)$ lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03479</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03479</id><created>2015-08-14</created><authors><author><keyname>Kumar</keyname><forenames>Manish</forenames></author><author><keyname>Kaul</keyname><forenames>Shubham</forenames></author><author><keyname>Singh</keyname><forenames>Vibhutesh Kumar</forenames></author><author><keyname>Bohara</keyname><forenames>Vivek Ashok</forenames></author></authors><title>iDART-Intruder Detection and Alert in Real Time</title><categories>cs.CY</categories><comments>Submitted as an entry to India Innovation Initiative - i3, 2015. 2
  Page demo paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we design and develop a smart intruder detection and alert
system which aims to elevate the security as well as the likelihood of true
positive identification of trespassers and intruders as compared to other
commonly deployed electronic security systems. Using multiple sensors, this
system can gauge the extent of danger exhibited by a person or animal in or
around the home premises, and can forward various critical information
regarding the event to home owners as well as other specified entities, such as
relevant security authorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03498</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03498</id><created>2015-08-14</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author></authors><title>Lensless Compressive Imaging</title><categories>cs.CV stat.AP stat.ME</categories><comments>37 pages, 10 figures. Submitted to SIAM Journal on Imaging Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a lensless compressive imaging architecture, which consists of an
aperture assembly and a single sensor, without using any lens. An anytime
algorithm is proposed to reconstruct images from the compressive measurements;
the algorithm produces a sequence of solutions that monotonically converge to
the true signal (thus, anytime). The algorithm is developed based on the
sparsity of local overlapping patches (in the transformation domain) and
state-of-the-art results have been obtained. Experiments on real data
demonstrate that encouraging results are obtained by measuring about 10% (of
the image pixels) compressive measurements. The reconstruction results of the
proposed algorithm are compared with the JPEG compression (based on file sizes)
and the reconstructed image quality is close to the JPEG compression, in
particular at a high compression rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03517</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03517</id><created>2015-08-14</created><updated>2016-02-27</updated><authors><author><keyname>Bharath</keyname><forenames>B. N.</forenames></author><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Learning-Based Approach to Caching in Heterogenous Small Cell Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 5 figures, published in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heterogenous network with base stations (BSs), small base stations (SBSs)
and users distributed according to independent Poisson point processes is
considered. SBS nodes are assumed to possess high storage capacity and to form
a distributed caching network. Popular files are stored in local cache of SBS,
so that a user can download the desired files from one of the SBSs in its
vicinity. The offloading-loss is captured via a cost function that depends on
the random caching strategy we propose. The popularity profile of cached
content is unknown and estimated using instantaneous demands from users within
a specified time interval. An estimate of the cost function is obtained from
which an optimal random caching strategy is devised. The training time to
achieve an $\epsilon&gt;0$ difference between the achieved and optimal costs is
finite provided the user density is greater than a predefined threshold, and
scales as $N^2$, where $N$ is the support of the popularity profile. A transfer
learning-based approach to improve this estimate is proposed. The training time
is reduced when the popularity profile is modeled using a parametric family of
distributions; the delay is independent of $N$ and scales linearly with the
dimension of the distribution parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03519</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03519</id><created>2015-08-14</created><authors><author><keyname>Kaaser</keyname><forenames>Dominik</forenames></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author></authors><title>On the Voting Time of the Deterministic Majority Process</title><categories>cs.DC</categories><comments>full version of brief announcement accepted at DISC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the deterministic binary majority process we are given a simple graph
where each node has one out of two initial opinions. In every round, every node
adopts the majority opinion among its neighbors. By using a potential argument
first discovered by Goles and Olivos (1980), it is known that this process
always converges in $O(|E|)$ rounds to a two-periodic state in which every node
either keeps its opinion or changes it in every round.
  It has been shown by Frischknecht, Keller, and Wattenhofer (2013) that the
$O(|E|)$ bound on the convergence time of the deterministic binary majority
process is indeed tight even for dense graphs. However, in many graphs such as
the complete graph, from any initial opinion assignment, the process converges
in just a constant number of rounds.
  By carefully exploiting the structure of the potential function by Goles and
Olivos (1980), we derive a new upper bound on the convergence time of the
deterministic binary majority process that accounts for such exceptional cases.
We show that it is possible to identify certain modules of a graph $G$ in order
to obtain a new graph $G^\Delta$ with the property that the worst-case
convergence time of $G^\Delta$ is an upper bound on that of $G$. Moreover, even
though our upper bound can be computed in linear time, we show that, given an
integer $k$, it is NP-hard to decide whether there exists an initial opinion
assignment for which it takes more than $k$ rounds to converge to the
two-periodic state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03523</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03523</id><created>2015-08-14</created><authors><author><keyname>Roca-Lacostena</keyname><forenames>Jordi</forenames></author><author><keyname>Cerquides</keyname><forenames>Jesus</forenames></author><author><keyname>Pouly</keyname><forenames>Marc</forenames></author></authors><title>Sufficient and necessary conditions for Dynamic Programming in
  Valuation-Based Systems</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valuation algebras abstract a large number of formalisms for automated
reasoning and enable the definition of generic inference procedures. Many of
these formalisms provide some notion of solution. Typical examples are
satisfying assignments in constraint systems, models in logics or solutions to
linear equation systems.
  Many widely used dynamic programming algorithms for optimization problems
rely on low treewidth decompositions and can be understood as particular cases
of a single algorithmic scheme for finding solutions in a valuation algebra.
The most encompassing description of this algorithmic scheme to date has been
proposed by Pouly and Kohlas together with sufficient conditions for its
correctness. Unfortunately, the formalization relies on a theorem for which we
provide counterexamples. In spite of that, the mainline of Pouly and Kohlas'
theory is correct, although some of the necessary conditions have to be
revised. In this paper we analyze the impact that the counter-examples have on
the theory, and rebuild the theory providing correct sufficient conditions for
the algorithms. Furthermore, we also provide necessary conditions for the
algorithms, allowing for a sharper characterization of when the algorithmic
scheme can be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03528</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03528</id><created>2015-07-29</created><updated>2016-03-07</updated><authors><author><keyname>Sendi&#xf1;a-Nadal</keyname><forenames>I.</forenames></author><author><keyname>Danziger</keyname><forenames>M. M.</forenames></author><author><keyname>Wang</keyname><forenames>Z.</forenames></author><author><keyname>Havlin</keyname><forenames>S.</forenames></author><author><keyname>Boccaletti</keyname><forenames>S.</forenames></author></authors><title>Assortativity and leadership emergence from anti-preferential attachment
  in heterogeneous networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><journal-ref>Scientific Reports 6, 21297 (2016)</journal-ref><doi>10.1038/srep21297</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world networks exhibit degree-assortativity, with nodes of similar
degree more likely to link to one another. Particularly in social networks, the
contribution to the total assortativity varies with degree, featuring a
distinctive peak slightly past the average degree. The way traditional models
imprint assortativity on top of pre-defined topologies is via degree-preserving
link permutations, which however destroy the particular graph's hierarchical
traits of clustering. Here, we propose the first generative model which creates
heterogeneous networks with scale-free-like properties and tunable realistic
assortativity. In our approach, two distinct populations of nodes are added to
an initial network seed: one (the followers) that abides by usual preferential
rules, and one (the potential leaders) connecting via anti-preferential
attachments, i.e. selecting lower degree nodes for their initial links. The
latter nodes come to develop a higher average degree, and convert eventually
into the final hubs. Examining the evolution of links in Facebook, we present
empirical validation for the connection between the initial anti-preferential
attachment and long term high degree. Thus, our work sheds new light on the
structure and evolution of social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03530</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03530</id><created>2015-07-29</created><authors><author><keyname>Hern&#xe1;ndez</keyname><forenames>Dami&#xe1;n G.</forenames></author><author><keyname>Zanette</keyname><forenames>Dami&#xe1;n H.</forenames></author><author><keyname>Samengo</keyname><forenames>In&#xe9;s</forenames></author></authors><title>Information-theoretical analysis of the statistical dependencies among
  three variables: Applications to written language</title><categories>cs.CL physics.data-an physics.soc-ph</categories><doi>10.1103/PhysRevE.92.022813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the information-theoretical concepts required to study the
statistical dependencies among three variables. Some of such dependencies are
pure triple interactions, in the sense that they cannot be explained in terms
of a combination of pairwise correlations. We derive bounds for triple
dependencies, and characterize the shape of the joint probability distribution
of three binary variables with high triple interaction. The analysis also
allows us to quantify the amount of redundancy in the mutual information
between pairs of variables, and to assess whether the information between two
variables is or is not mediated by a third variable. These concepts are applied
to the analysis of written texts. We find that the probability that a given
word is found in a particular location within the text is not only modulated by
the presence or absence of other nearby words, but also, on the presence or
absence of nearby pairs of words. We identify the words enclosing the key
semantic concepts of the text, the triplets of words with high pairwise and
triple interactions, and the words that mediate the pairwise interactions
between other words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03536</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03536</id><created>2015-08-14</created><authors><author><keyname>van der Storm</keyname><forenames>Tijs</forenames></author><author><keyname>Erdweg</keyname><forenames>Sebastian</forenames></author></authors><title>Proceedings of the 3rd Workshop on Domain-Specific Language Design and
  Implementation (DSLDI 2015)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the DSLDI workshop is to bring together researchers and
practitioners interested in sharing ideas on how DSLs should be designed,
implemented, supported by tools, and applied in realistic application contexts.
We are both interested in discovering how already known domains such as graph
processing or machine learning can be best supported by DSLs, but also in
exploring new domains that could be targeted by DSLs. More generally, we are
interested in building a community that can drive forward the development of
modern DSLs. These informal post-proceedings contain the submitted talk
abstracts to the 3rd DSLDI workshop (DSLDI'15), and a summary of the panel
discussion on Language Composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03538</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03538</id><created>2015-08-14</created><authors><author><keyname>Brandl</keyname><forenames>Florian</forenames></author><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Hofbauer</keyname><forenames>Johannes</forenames></author></authors><title>Welfare Maximization Entices Participation</title><categories>cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider randomized mechanisms with optional participation. Preferences
over lotteries are modeled using skew-symmetric bilinear (SSB) utility
functions, a generalization of classic von Neumann-Morgenstern utility
functions. We show that every welfare-maximizing mechanism entices
participation and that the converse holds under additional assumptions. Two
important corollaries of our results are characterizations of an attractive
randomized voting rule that satisfies Condorcet-consistency and entices
participation. This stands in contrast to a well-known result by Moulin (1988),
who proves that no deterministic voting rule can satisfy both properties
simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03545</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03545</id><created>2015-08-10</created><updated>2015-12-14</updated><authors><author><keyname>Kim</keyname><forenames>Hyewon</forenames></author><author><keyname>Ha</keyname><forenames>Meesoon</forenames></author><author><keyname>Jeong</keyname><forenames>Hawoong</forenames></author></authors><title>Scaling Properties in Time-Varying Networks with Memory</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>8 pages, 10 figures (published version)</comments><journal-ref>Eur. Phys. J. B (2015) 88: 315</journal-ref><doi>10.1140/epjb/e2015-60662-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formation of network structure is mainly influenced by an individual
node's activity and its memory, where activity can usually be interpreted as
the individual inherent property and memory can be represented by the
interaction strength between nodes. In our study, we define the activity
through the appearance pattern in the time-aggregated network representation,
and quantify the memory through the contact pattern of empirical temporal
networks. To address the role of activity and memory in epidemics on
time-varying networks, we propose temporal-pattern coarsening of
activity-driven growing networks with memory. In particular, we focus on the
relation between time-scale coarsening and spreading dynamics in the context of
dynamic scaling and finite-size scaling. Finally, we discuss the universality
issue of spreading dynamics on time-varying networks for various
memory-causality tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03554</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03554</id><created>2015-08-14</created><authors><author><keyname>Derakhshani</keyname><forenames>Mahsa</forenames></author><author><keyname>Wang</keyname><forenames>Xiaowei</forenames></author><author><keyname>Le-Ngoc</keyname><forenames>Tho</forenames></author><author><keyname>Leon-Garcia</keyname><forenames>Alberto</forenames></author></authors><title>Virtualization of Multi-Cell 802.11 Networks: Association and Airtime
  Control</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the virtualization and optimization of a multi-cell
WLAN. We consider the station (STA)-access point (AP) association and airtime
control for virtualized 802.11 networks to provide service customization and
fairness across multiple internet service providers (ISPs) sharing the common
physical infrastructure and network capacity. More specifically, an
optimization problem is formulated on the STAs transmission probabilities to
maximize the overall network throughput, while providing airtime usage
guarantees for the ISPs. Subsequently, an algorithm to reach the optimal
solution is developed by applying monomial approximation and geometric
programming iteratively. Based on the proposed three-dimensional Markov-chain
model of the enhanced distributed channel access (EDCA) protocol, the detailed
implementation of the optimal transmission probability is also discussed. The
accuracy of the proposed Markov-chain model and the performance of the
developed association and airtime control scheme are evaluated through
numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03559</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03559</id><created>2015-08-14</created><updated>2016-01-11</updated><authors><author><keyname>Angulo</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Moreno</keyname><forenames>Jaime A.</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author></authors><title>Fundamental limitations of network reconstruction</title><categories>cs.SY math.OC physics.bio-ph physics.soc-ph</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction is the first step towards understanding, diagnosing
and controlling the dynamics of complex networked systems. It allows us to
infer properties of the interaction matrix, which characterizes how nodes in a
system directly interact with each other. Despite a decade of extensive
studies, network reconstruction remains an outstanding challenge. The
fundamental limitations governing which properties of the interaction matrix
(e.g., adjacency pattern, sign pattern and degree sequence) can be inferred
from given temporal data of individual nodes remain unknown. Here we rigorously
derive necessary conditions to reconstruct any property of the interaction
matrix. These conditions characterize how uncertain can we be about the
coupling functions that characterize the interactions between nodes, and how
informative does the measured temporal data need to be; rendering two classes
of fundamental limitations of network reconstruction. Counterintuitively, we
find that reconstructing any property of the interaction matrix is generically
as difficult as reconstructing the interaction matrix itself, requiring equally
informative temporal data. Revealing these fundamental limitations shed light
on the design of better network reconstruction algorithms, which offer
practical improvements over existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03566</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03566</id><created>2015-08-14</created><authors><author><keyname>Atalar</keyname><forenames>Aras</forenames></author><author><keyname>Renaud-Goud</keyname><forenames>Paul</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>Analyzing the Performance of Lock-Free Data Structures: A Conflict-based
  Model</title><categories>cs.DS cs.DC</categories><comments>Short version to appear in DISC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the modeling and the analysis of the performance of
lock-free concurrent data structures. Lock-free designs employ an optimistic
conflict control mechanism, allowing several processes to access the shared
data object at the same time. They guarantee that at least one concurrent
operation finishes in a finite number of its own steps regardless of the state
of the operations. Our analysis considers such lock-free data structures that
can be represented as linear combinations of fixed size retry loops. Our main
contribution is a new way of modeling and analyzing a general class of
lock-free algorithms, achieving predictions of throughput that are close to
what we observe in practice. We emphasize two kinds of conflicts that shape the
performance: (i) hardware conflicts, due to concurrent calls to atomic
primitives; (ii) logical conflicts, caused by simultaneous operations on the
shared data structure. We show how to deal with these hardware and logical
conflicts separately, and how to combine them, so as to calculate the
throughput of lock-free algorithms. We propose also a common framework that
enables a fair comparison between lock-free implementations by covering the
whole contention domain, together with a better understanding of the
performance impacting factors. This part of our analysis comes with a method
for calculating a good back-off strategy to finely tune the performance of a
lock-free algorithm. Our experimental results, based on a set of widely used
concurrent data structures and on abstract lock-free designs, show that our
analysis follows closely the actual code behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03567</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03567</id><created>2015-08-14</created><authors><author><keyname>Hamdi</keyname><forenames>Rami</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author></authors><title>Sum-rate Maximizing in Downlink Massive MIMO Systems with Circuit Power
  Consumption</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Wireless and Mobile Computing,
  Networking and Communications (WiMob 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The downlink of a single cell base station (BS) equipped with large-scale
multiple-input multiple-output (MIMO) system is investigated in this paper. As
the number of antennas at the base station becomes large, the power consumed at
the RF chains cannot be anymore neglected. So, a circuit power consumption
model is introduced in this work. It involves that the maximal sum-rate is not
obtained when activating all the available RF chains. Hence, the aim of this
work is to find the optimal number of activated RF chains that maximizes the
sum-rate. Computing the optimal number of activated RF chains must be
accompanied by an adequate antenna selection strategy. First, we derive
analytically the optimal number of RF chains to be activated so that the
average sum-rate is maximized under received equal power. Then, we propose an
efficient greedy algorithm to select the sub-optimal set of RF chains to be
activated with regards to the system sum-rate. It allows finding the balance
between the power consumed at the RF chains and the transmitted power. The
performance of the proposed algorithm is compared with the optimal performance
given by brute force search (BFS) antenna selection. Simulations allow to
compare the performance given by greedy, optimal and random antenna selection
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03572</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03572</id><created>2015-08-14</created><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Fast Witness Extraction Using a Decision Oracle</title><categories>cs.DS</categories><comments>Journal version, 16 pages. Extended abstract presented at ESA'14</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The gist of many (NP-)hard combinatorial problems is to decide whether a
universe of $n$ elements contains a witness consisting of $k$ elements that
match some prescribed pattern. For some of these problems there are known
advanced algebra-based FPT algorithms which solve the decision problem but do
not return the witness. We investigate techniques for turning such a
YES/NO-decision oracle into an algorithm for extracting a single witness, with
an objective to obtain practical scalability for large values of $n$. By
relying on techniques from combinatorial group testing, we demonstrate that a
witness may be extracted with $O(k\log n)$ queries to either a deterministic or
a randomized set inclusion oracle with one-sided probability of error.
Furthermore, we demonstrate through implementation and experiments that the
algebra-based FPT algorithms are practical, in particular in the setting of the
$k$-path problem. Also discussed are engineering issues such as optimizing
finite field arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03575</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03575</id><created>2015-08-14</created><authors><author><keyname>Lorber</keyname><forenames>Florian</forenames></author><author><keyname>Rosenmann</keyname><forenames>Amnon</forenames></author><author><keyname>Nickovic</keyname><forenames>Dejan</forenames></author><author><keyname>Aichernig</keyname><forenames>Bernhard</forenames></author></authors><title>Bounded Determinization of Timed Automata with Silent Transitions</title><categories>cs.FL</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic timed automata are strictly less expressive than their
non-deterministic counterparts, which are again less expressive than those with
silent transitions. As a consequence, timed automata are in general
non-determinizable. This is unfortunate since deterministic automata play a
major role in model-based testing, observability and implementability. However,
by bounding the length of the traces in the automaton, effective
determinization becomes possible. We propose a novel procedure for bounded
determinization of timed automata. The procedure unfolds the automata to
bounded trees, removes all silent transitions and determinizes via disjunction
of guards. The proposed algorithms are optimized to the bounded setting and
thus are more efficient and can handle a larger class of timed automata than
the general algorithms. The approach is implemented in a prototype tool and
evaluated on several examples. To our best knowledge, this is the first
implementation of this type of procedure for timed automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03579</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03579</id><created>2015-08-14</created><authors><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Fineman</keyname><forenames>Jeremy T.</forenames></author><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author><author><keyname>Newport</keyname><forenames>Calvin</forenames></author></authors><title>Smoothed Analysis of Dynamic Networks</title><categories>cs.DC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the technique of smoothed analysis to distributed algorithms in
dynamic network models. Whereas standard smoothed analysis studies the impact
of small random perturbations of input values on algorithm performance metrics,
dynamic graph smoothed analysis studies the impact of random perturbations of
the underlying changing network graph topologies. Similar to the original
application of smoothed analysis, our goal is to study whether known strong
lower bounds in dynamic network models are robust or fragile: do they withstand
small (random) perturbations, or do such deviations push the graphs far enough
from a precise pathological instance to enable much better performance? Fragile
lower bounds are likely not relevant for real-world deployment, while robust
lower bounds represent a true difficulty caused by dynamic behavior. We apply
this technique to three standard dynamic network problems with known strong
worst-case lower bounds: random walks, flooding, and aggregation. We prove that
these bounds provide a spectrum of robustness when subjected to
smoothing---some are extremely fragile (random walks), some are moderately
fragile / robust (flooding), and some are extremely robust (aggregation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03583</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03583</id><created>2015-08-14</created><authors><author><keyname>Barker</keyname><forenames>Timothy</forenames></author><author><keyname>Zhai</keyname><forenames>Chao</forenames></author><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author></authors><title>A Coverage Based Decentralised Routing Algorithm for Vehicular Traffic
  Networks</title><categories>cs.NI</categories><comments>8 Pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple yet effective routing strategy inspired by coverage
control, which delays the onset of congestion on traffic networks, by
introducing a control parameter. The routing algorithm allows a trade-off
between the congestion level and the distance to the destination. Numerical
verification of the strategy is provided on a number of representative examples
in SUMO, a well known micro agent simulator used for the analysis of traffic
networks. We find that it is crucial in many cases to tune the given control
parameters to some optimal value in order to reduce congestion in the most
effective way. The effects of different network structural properties are
connected to the level of congestion and the optimal range for setting the
control parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03590</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03590</id><created>2015-05-04</created><updated>2015-12-07</updated><authors><author><keyname>Mignard-Debise</keyname><forenames>Lois</forenames></author><author><keyname>Ihrke</keyname><forenames>Ivo</forenames></author></authors><title>Light-field Microscopy with a Consumer Light-field Camera</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the use of inexpensive consumer light- field camera technology for
the purpose of light-field mi- croscopy. Our experiments are based on the Lytro
(first gen- eration) camera. Unfortunately, the optical systems of the Lytro
and those of microscopes are not compatible, lead- ing to a loss of light-field
information due to angular and spatial vignetting when directly recording
microscopic pic- tures. We therefore consider an adaptation of the Lytro op-
tical system. We demonstrate that using the Lytro directly as an oc- ular
replacement, leads to unacceptable spatial vignetting. However, we also found a
setting that allows the use of the Lytro camera in a virtual imaging mode which
prevents the information loss to a large extent. We analyze the new vir- tual
imaging mode and use it in two different setups for im- plementing light-field
microscopy using a Lytro camera. As a practical result, we show that the camera
can be used for low magnification work, as e.g. common in quality control,
surface characterization, etc. We achieve a maximum spa- tial resolution of
about 6.25{\mu}m, albeit at a limited SNR for the side views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03592</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03592</id><created>2015-08-14</created><authors><author><keyname>Noghani</keyname><forenames>Kyoomars Alizadeh</forenames></author><author><keyname>Sunay</keyname><forenames>M. Oguz</forenames></author></authors><title>Streaming Multicast Video over Software-Defined Networks</title><categories>cs.NI</categories><comments>6 pages, 5 figures, 1 table, First Workshop on Software-Defined
  Internets of the Future - WSDIF 2014. Proceedings of the 11th IEEE
  International Conference on Mobile Ad hoc and Sensor Systems (MASS) 2014 -
  Philadelphia, Pennsylvania - October 28 - 30, 2014</comments><doi>10.1109/MASS.2014.125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the video streaming applications in today's Internet involve the
distribution of content from a CDN source to a large population of interested
clients. However, widespread support of IP multicast is unavailable due to
technical and economical reasons, leaving the floor to application layer
multicast which introduces excessive delays for the clients and increased
traffic load for the network. This paper is concerned with the introduction of
an SDN-based framework that allows the network controller to not only deploy IP
multicast between a source and subscribers, but also control, via a simple
northbound interface, the distributed set of sources where multiple-
description coded (MDC) video content is available. We observe that for medium
to heavy network loads, relative to the state-of-the-art, the SDN-based
streaming multicast video framework increases the PSNR of the received video
significantly, from a level that is practically unwatchable to one that has
good quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03593</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03593</id><created>2015-08-14</created><authors><author><keyname>Assadi</keyname><forenames>Sepehr</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Jabbari</keyname><forenames>Shahin</forenames></author></authors><title>Online Assignment of Heterogeneous Tasks in Crowdsourcing Markets</title><categories>cs.DS cs.HC</categories><comments>Extended version of paper in HCOMP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of heterogeneous task assignment in crowdsourcing
markets from the point of view of the requester, who has a collection of tasks.
Workers arrive online one by one, and each declare a set of feasible tasks they
can solve, and desired payment for each feasible task. The requester must
decide on the fly which task (if any) to assign to the worker, while assigning
workers only to feasible tasks. The goal is to maximize the number of assigned
tasks with a fixed overall budget.
  We provide an online algorithm for this problem and prove an upper bound on
the competitive ratio of this algorithm against an arbitrary (possibly
worst-case) sequence of workers who want small payments relative to the
requester's total budget. We further show an almost matching lower bound on the
competitive ratio of any algorithm in this setting. Finally, we propose a
different algorithm that achieves an improved competitive ratio in the random
permutation model, where the order of arrival of the workers is chosen
uniformly at random. Apart from these strong theoretical guarantees, we carry
out experiments on simulated data which demonstrates the practical
applicability of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03594</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03594</id><created>2015-05-28</created><authors><author><keyname>Ch'ng</keyname><forenames>Eugene</forenames></author></authors><title>Local Interactions and the Emergence of a Twitter Small-World Network</title><categories>physics.soc-ph cs.SI</categories><comments>Ch'ng E. (2015) Local Interactions and the Emergence and Maintenance
  of a Twitter Small-World Network, Social Networking 4(2), p.33-40</comments><journal-ref>2015 Social Networking 4(2), p.33-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The small-world phenomenon is found in many self-organising systems. Systems
configured in small-world networks spread information more easily than in
random or regular lattice-type networks. Whilst it is a known fact that
small-world networks have short average path length and high clustering
coefficient in self-organising systems, the ego centralities that maintain the
cohesiveness of small-world network have not been formally defined. Here we
show that instantaneous events such as the release of news items via Twitter,
coupled with active community arguments related to the news item form a
particular type of small-world network. Analysis of the centralities in the
network reveals that community arguments maintain the small-world network
whilst actively maintaining the cohesiveness and boundary of the group. The
results demonstrate how an active Twitter community unconsciously forms a
small-world network whilst interacting locally with a bordering community. Over
time, such local interactions brought about the global emergence of the
small-world network, connecting media channels with human activities.
Understanding the small-world phenomenon in relation to online social or civic
movement is important, as evident in the spate of online activists that tipped
the power of governments for the better or worst in recent times. The support,
or removal of high centrality nodes in such networks has important
ramifications in the self-expression of society and civic discourses. The
presentation in this article anticipates further exploration of man-made
self-organising systems where a larger cluster of ad-hoc and active community
maintains the overall cohesiveness of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03599</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03599</id><created>2015-08-14</created><updated>2015-10-27</updated><authors><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Efficient Redundancy Techniques for Latency Reduction in Cloud Systems</title><categories>cs.DC cs.IT math.IT</categories><comments>submitted to ACM Transactions on Modeling and Performance Evaluation
  of Computing Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud computing systems, assigning a task to multiple servers and waiting
for the earliest copy to finish is an effective method to combat the
variability in response time of individual servers, and thus reduce average
latency. But adding redundancy may result in higher cost of computing
resources, as well as an increase in queueing delay due to higher traffic load.
This work helps understand when and how redundancy gives a cost-efficient
reduction in latency. For a general task service time distribution, we compare
different redundancy strategies, for e.g. the number of redundant tasks, and
time when they are issued and canceled. We get the insight that the
log-concavity of the task service distribution is a key factor in determining
whether adding redundancy helps. If the service distribution is log-convex,
then adding maximum redundancy reduces both latency and cost. And if it is
log-concave, then less redundancy, and early cancellation of redundant tasks is
more effective. Using these insights, we design a general redundancy strategy
that achieves a good latency-cost trade-off for an arbitrary service
distribution. This work also generalizes and extends some results in the
analysis of fork-join queues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03600</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03600</id><created>2015-08-14</created><authors><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Metric embedding with outliers</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of metric embeddings with \emph{outliers}. Given some
metric space $(X,\rho)$ we wish to find a small set of outlier points $K
\subset X$ and either an isometric or a low-distortion embedding of
$(X\setminus K,\rho)$ into some target metric space. This is a natural problem
that captures scenarios where a small fraction of points in the input
corresponds to noise.
  For the case of isometric embeddings we derive polynomial-time approximation
algorithms for minimizing the number of outliers when the target space is an
ultrametric, a tree metric, or constant-dimensional Euclidean space. The
approximation factors are 3, 4 and 2, respectively. For the case of embedding
into an ultrametric or tree metric, we further improve the running time to
$O(n^2)$ for an $n$-point input metric space, which is optimal. We complement
these upper bounds by showing that outlier embedding into ultrametrics, trees,
and $d$-dimensional Euclidean space for any $d\geq 2$ are all NP-hard, as well
as NP-hard to approximate within a factor better than 2 assuming the Unique
Game Conjecture.
  For the case of non-isometries we consider embeddings with small
$\ell_{\infty}$ distortion. We present polynomial-time \emph{bi-criteria}
approximation algorithms. Specifically, given some $\epsilon &gt; 0$, let
$k_\epsilon$ denote the minimum number of outliers required to obtain an
embedding with distortion $\epsilon$. For the case of embedding into
ultrametrics we obtain a polynomial-time algorithm which computes a set of at
most $3k_{\epsilon}$ outliers and an embedding of the remaining points into an
ultrametric with distortion $O(\epsilon \log n)$. For embedding a metric of
unit diameter into constant-dimensional Euclidean space we present a
polynomial-time algorithm which computes a set of at most $2k_{\epsilon}$
outliers and an embedding of the remaining points with distortion
$O(\sqrt{\epsilon})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03601</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03601</id><created>2015-08-14</created><authors><author><keyname>K.</keyname><forenames>Ranjitha R.</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Is Stack Overflow Overflowing With Questions and Tags</title><categories>cs.SI cs.CL</categories><comments>11 pages, 7 figures, 3 tables Presented at Third International
  Symposium on Women in Computing and Informatics (WCI-2015)</comments><acm-class>H.5.3; I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming question and answer (Q &amp; A) websites, such as Quora, Stack
Overflow, and Yahoo! Answer etc. helps us to understand the programming
concepts easily and quickly in a way that has been tested and applied by many
software developers. Stack Overflow is one of the most frequently used
programming Q\&amp;A website where the questions and answers posted are presently
analyzed manually, which requires a huge amount of time and resource. To save
the effort, we present a topic modeling based technique to analyze the words of
the original texts to discover the themes that run through them. We also
propose a method to automate the process of reviewing the quality of questions
on Stack Overflow dataset in order to avoid ballooning the stack overflow with
insignificant questions. The proposed method also recommends the appropriate
tags for the new post, which averts the creation of unnecessary tags on Stack
Overflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03604</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03604</id><created>2015-08-14</created><authors><author><keyname>Drawert</keyname><forenames>Brian</forenames></author><author><keyname>Trogdon</keyname><forenames>Michael</forenames></author><author><keyname>Toor</keyname><forenames>Salman</forenames></author><author><keyname>Petzold</keyname><forenames>Linda</forenames></author><author><keyname>Hellander</keyname><forenames>Andreas</forenames></author></authors><title>MOLNs: A cloud platform for interactive, reproducible and scalable
  spatial stochastic computational experiments in systems biology using PyURDME</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational experiments using spatial stochastic simulations have led to
important new biological insights, but they require specialized tools, a
complex software stack, as well as large and scalable compute and data analysis
resources due to the large computational cost associated with Monte Carlo
computational workflows. The complexity of setting up and managing a
large-scale distributed computation environment to support productive and
reproducible modeling can be prohibitive for practitioners in systems biology.
This results in a barrier to the adoption of spatial stochastic simulation
tools, effectively limiting the type of biological questions addressed by
quantitative modeling. In this paper, we present PyURDME, a new, user-friendly
spatial modeling and simulation package, and MOLNs, a cloud computing appliance
for distributed simulation of stochastic reaction-diffusion models. MOLNs is
based on IPython and provides an interactive programming platform for
development of sharable and reproducible distributed parallel computational
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03605</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03605</id><created>2015-08-14</created><authors><author><keyname>Kala</keyname><forenames>Srikant Manas</forenames></author><author><keyname>Musham</keyname><forenames>Ranadheer</forenames></author><author><keyname>Reddy</keyname><forenames>M Pavan Kumar</forenames></author><author><keyname>Tamma</keyname><forenames>Bheemarjuna Reddy</forenames></author></authors><title>Reliable Prediction of Channel Assignment Performance in Wireless Mesh
  Networks</title><categories>cs.NI</categories><comments>Accepted in ICACCI-2015</comments><journal-ref>ICACCI, Aug. 2015, 13 - 19</journal-ref><doi>10.1109/ICACCI.2015.7275577</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancements in wireless mesh networks (WMN), and the surge in
multi-radio multi-channel (MRMC) WMN deployments have spawned a multitude of
network performance issues. These issues are intricately linked to the adverse
impact of endemic interference. Thus, interference mitigation is a primary
design objective in WMNs. Interference alleviation is often effected through
efficient channel allocation (CA) schemes which fully utilize the potential of
MRMC environment and also restrain the detrimental impact of interference.
However, numerous CA schemes have been proposed in research literature and
there is a lack of CA performance prediction techniques which could assist in
choosing a suitable CA for a given WMN. In this work, we propose a reliable
interference estimation and CA performance prediction approach. We demonstrate
its efficacy by substantiating the CA performance predictions for a given WMN
with experimental data obtained through rigorous simulations on an ns-3 802.11g
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03606</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03606</id><created>2015-08-14</created><updated>2016-03-07</updated><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author></authors><title>Hierarchical Models as Marginals of Hierarchical Models</title><categories>math.PR cs.LG cs.NE math.ST stat.TH</categories><comments>18 pages, 4 figures, 2 tables, WUPES'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the representation of hierarchical models in terms of
marginals of other hierarchical models with smaller interactions. We focus on
binary variables and marginals of pairwise interaction models whose hidden
variables are conditionally independent given the visible variables. In this
case the problem is equivalent to the representation of linear subspaces of
polynomials by feedforward neural networks with soft-plus computational units.
We show that every hidden variable can freely model multiple interactions among
the visible variables, which allows us to generalize and improve previous
results. In particular, we show that a restricted Boltzmann machine with less
than $[ 2(\log(v)+1) / (v+1) ] 2^v-1$ hidden binary variables can approximate
every distribution of $v$ visible binary variables arbitrarily well, compared
to $2^{v-1}-1$ from the best previously known result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03607</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03607</id><created>2015-08-14</created><authors><author><keyname>A.</keyname><forenames>Anusha</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Is That Twitter Hashtag Worth Reading</title><categories>cs.SI</categories><comments>10 pages, 6 figures, Presented at the Third International Symposium
  on Women in Computing and Informatics (WCI-2015)</comments><acm-class>H.5.3; I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social media such as Twitter, Facebook, Wikis and Linkedin have made a
great impact on the way we consume information in our day to day life. Now it
has become increasingly important that we come across appropriate content from
the social media to avoid information explosion. In case of Twitter, popular
information can be tracked using hashtags. Studying the characteristics of
tweets containing hashtags becomes important for a number of tasks, such as
breaking news detection, personalized message recommendation, friends
recommendation, and sentiment analysis among others.
  In this paper, we have analyzed Twitter data based on trending hashtags,
which is widely used nowadays. We have used event based hashtags to know users'
thoughts on those events and to decide whether the rest of the users might find
it interesting or not. We have used topic modeling, which reveals the hidden
thematic structure of the documents (tweets in this case) in addition to
sentiment analysis in exploring and summarizing the content of the documents. A
technique to find the interestingness of event based twitter hashtag and the
associated sentiment has been proposed. The proposed technique helps twitter
follower to read, relevant and interesting hashtag.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03610</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03610</id><created>2015-08-13</created><authors><author><keyname>Situngkir</keyname><forenames>Hokky</forenames></author></authors><title>Exploring Ancient Architectural Designs with Cellular Automata</title><categories>cs.CY nlin.CG</categories><comments>9 pages, 5 figures</comments><report-no>BFI Working Paper Series WP-092010</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses the utilization of three-dimensional cellular automata
employing the two-dimensional totalistic cellular automata to simulate how
simple rules could emerge a highly complex architectural designs of some
Indonesian heritages. A detailed discussion is brought to see the simple rules
applied in Borobudur Temple, the largest ancient Buddhist temple in the country
with very complex detailed designs within. The simulation confirms some
previous findings related to measurement of the temple as well as some other
ancient buildings in Indonesia. This happens to open further exploitation of
the explanatory power presented by cellular automata for complex architectural
designs built by civilization not having any supporting sophisticated tools,
even standard measurement systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03619</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03619</id><created>2015-08-14</created><updated>2015-10-06</updated><authors><author><keyname>Beamer</keyname><forenames>Scott</forenames></author><author><keyname>Asanovi&#x107;</keyname><forenames>Krste</forenames></author><author><keyname>Patterson</keyname><forenames>David</forenames></author></authors><title>The GAP Benchmark Suite</title><categories>cs.DC cs.DS</categories><comments>previous upload missing benchmark (spec.tex)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a graph processing benchmark suite with the goal of helping to
standardize graph processing evaluations. Fewer differences between graph
processing evaluations will make it easier to compare different research
efforts and quantify improvements. The benchmark not only specifies graph
kernels, input graphs, and evaluation methodologies, but it also provides
optimized baseline implementations. These baseline implementations are
representative of state-of-the-art performance, and thus new contributions
should outperform them to demonstrate an improvement.
  The input graphs are sized appropriately for shared memory platforms, but any
implementation on any platform that conforms to the benchmark's specifications
could be compared. This benchmark suite can be used in a variety of settings.
Graph framework developers can demonstrate the generality of their programming
model by implementing all of the benchmark's kernels and delivering competitive
performance on all of the benchmark's graphs. Algorithm designers can use the
input graphs and the baseline implementations to demonstrate their
contribution. Platform designers and performance analysts can use the suite as
a workload representative of graph processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03629</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03629</id><created>2015-08-14</created><updated>2015-08-28</updated><authors><author><keyname>Fournier</keyname><forenames>Laurent</forenames></author></authors><title>RFC 7800 - Money Over IP</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Request For Comment (RFC) is a proposal for a new protocol to use money
over the Internet. Features like a distributed architecture, a published
cryptographic algorithm, a minimal authority responsibility, the absence of
fees will make this protocol a perfect tool for citizens in today's digital
World. An implementation has validated the main principles and we entering now
a testing phase (v0.1). Depending of the results, a released date for the 1.0
revision will to decided to allow anybody to send or to receive money to/from
anyone, in any currency, with a regular and personal smart-phone. A distributed
hash table (DHT) is used to store all transactions, public keys and
certificates redundantly on several nodes. The all system may replace coins,
banknotes and classical checks in the future. We also argue that the Bitcoin
technology does not satisfy the requirements for a digital mean of payment.
This proposal is expected to be reviewed and commented by the Internet
Engineering Task Force.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03630</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03630</id><created>2015-08-14</created><authors><author><keyname>Hoffmann</keyname><forenames>Guillaume</forenames></author></authors><title>Undecidability of a Very Simple Modal Logic with Binding</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show undecidability of the satisfiability problem of what is arguably the
simplest non-sub-Boolean modal logic with an implicit notion of binding. This
work enriches the series of existing results of undecidability of modal logics
with binders, which started with Hybrid Logics and continued with Memory
Logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03649</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03649</id><created>2015-08-13</created><authors><author><keyname>Situngkir</keyname><forenames>Hokky</forenames></author></authors><title>Borobudur was Built Algorithmically</title><categories>cs.CY cs.CV cs.GR</categories><comments>9 pages, 6 figures</comments><report-no>BFI Working Paper Series, WP082010</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The self-similarity of Indonesian Borobudur Temple is observed through the
dimensionality of stupa that is hypothetically closely related to whole
architectural body. Fractal dimension is calculated by using the cube counting
method and found that the dimension is 2.325, which is laid between the
two-dimensional plane and three dimensional space. The applied fractal geometry
and self-similarity of the building is emerged as the building process
implement the metric rules, since there is no universal metric standard known
in ancient traditional Javanese culture thus the architecture is not based on
final master plan. The paper also proposes how the hypothetical algorithmic
architecture might be applied computationally in order to see some experimental
generations of similar building. The paper ends with some conjectures for
further challenge and insights related to fractal geometry in Javanese
traditional cultural heritages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03650</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03650</id><created>2015-08-14</created><authors><author><keyname>Shahrivar</keyname><forenames>Ebrahim Moradi</forenames></author><author><keyname>Pirani</keyname><forenames>Mohammad</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>Robustness and Algebraic Connectivity of Random Interdependent Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate certain structural properties of random interdependent
networks. We start by studying a property known as $r$-robustness, which is a
strong indicator of the ability of a network to tolerate structural
perturbations and dynamical attacks. We show that random $k$-partite graphs
exhibit a threshold for $r$-robustness, and that this threshold is the same as
the one for the graph to have minimum degree $r$. We then extend this
characterization to random interdependent networks with arbitrary intra-layer
topologies. Finally, we characterize the algebraic connectivity of such
networks, and provide an asymptotically tight rate of growth of this quantity
for a certain range of inter-layer edge formation probabilities. Our results
arise from a characterization of the isoperimetric constant of random
interdependent networks, and yield new insights into the structure and
robustness properties of such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03651</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03651</id><created>2015-04-18</created><updated>2015-08-30</updated><authors><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author></authors><title>A conjecture about the efficiency of first price mechanisms</title><categories>q-fin.EC cs.GT</categories><comments>The conjecture is disproved in its original form by the author. The
  current status of the question is a part of the paper &quot;Efficient Teamwork&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present different versions of a conjecture which would express that first
price mechanisms never work very badly in a very general class of problems. The
definitions include most of the problems where there is a principal (seller)
who has the right to exclude others from the game. The exact definitions are
motivated by the &quot;first price mechanism&quot; in E Cs: &quot;Efficient Teamwork&quot;, but the
conjecture is relevant for most auction problems, e.g. for combinatorial
auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03653</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03653</id><created>2015-04-24</created><authors><author><keyname>Tsukada</keyname><forenames>Koji</forenames></author><author><keyname>Oki</keyname><forenames>Maho</forenames></author><author><keyname>Kurihara</keyname><forenames>Kazutaka</forenames></author><author><keyname>Furudate</keyname><forenames>Yuko</forenames></author></authors><title>AnimalCatcher: a digital camera to capture various reactions of animals</title><categories>cs.HC</categories><comments>Written in Japanese, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People often have difficulty to take pictures of animals, since animals
usually do not react with cameras nor understand verbal directions. To solve
this problem, we developed a new interaction technique, AnimalCatcher, which
can attract animals' attention easily. The AnimalCatcher shoots various sounds
using directional speaker to capture various reactions of animals. This paper
describes concepts, implementation, and example pictures taken in a zoo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03657</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03657</id><created>2015-08-12</created><authors><author><keyname>Agnarsson</keyname><forenames>Geir</forenames></author><author><keyname>Greenlaw</keyname><forenames>Raymond</forenames></author><author><keyname>Kantabutra</keyname><forenames>Sanpawat</forenames></author></authors><title>The complexity of cyber attacks in a new layered-security model and the
  maximum-weight, rooted-subtree problem</title><categories>cs.DS cs.CR math.CO</categories><comments>18 pages, 2 tables</comments><msc-class>05C22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our cyber security model we define the concept of {\em penetration cost},
which is the cost that must be paid in order to break into the next layer of
security. Given a tree $T$ rooted at a vertex $r$, a {\em penetrating cost}
edge function $c$ on $T$, a {\em target-acquisition} vertex function $p$ on
$T$, the attacker's {\em budget} and the {\em game-over threshold} $B,G \in
{\mathbb{Q}}^{+}$ respectively, we consider the problem of determining the
existence of a rooted subtree $T'$ of $T$ within the attacker's budget (that
is, the sum of the costs of the edges in $T'$ is less than or equal to $B$)
with total acquisition value more than the game-over threshold (that is, the
sum of the target values of the nodes in $T'$ is greater than or equal to $G$).
We prove that the general version of this problem is intractable, but does
admit a polynomial time approximation scheme. We also analyze the complexity of
three restricted versions of the problems, where the penetration cost is the
constant function, integer-valued, and rational-valued among a given fixed
number of distinct values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03658</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03658</id><created>2015-08-04</created><authors><author><keyname>Kalokidou</keyname><forenames>V.</forenames></author><author><keyname>Johnson</keyname><forenames>O.</forenames></author><author><keyname>Piechocki</keyname><forenames>R.</forenames></author></authors><title>A hybrid TIM-NOMA scheme for the Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>11 pages, Published at &quot;EAI Endorsed Transactions on Wireless
  Spectrum&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future mobile communication networks will require enhanced network efficiency
and reduced system overhead. Research on Blind Interference Alignment and
Topological Interference Management (TIM) has shown that optimal Degrees of
Freedom can be achieved, in the absence of Channel State Information at the
transmitters. Moreover, the recently emerged Non-Orthogonal Multiple Access
(NOMA) scheme suggests a different multiple access approach, compared to the
orthogonal methods employed in 4G, resulting in high capacity gains. Our
contribution is a hybrid TIM-NOMA scheme in K-user cells, where users are
divided into T groups. By superimposing users in the power domain, we introduce
a two-stage decoding process, managing inter-group interference based on the
TIM principles, and intra-group interference based on Successful Interference
Cancellation, as proposed by NOMA. We show that the hybrid scheme can improve
the sum rate by at least 100% compared to Time Division Multiple Access, for
high SNR values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03660</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03660</id><created>2015-08-14</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Kantor</keyname><forenames>Erez</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>Parter</keyname><forenames>Merav</forenames></author></authors><title>Computing in Additive Networks with Bounded-Information Codes</title><categories>cs.DC cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the theory of the additive wireless network model, in
which the received signal is abstracted as an addition of the transmitted
signals. Our central observation is that the crucial challenge for computing in
this model is not high contention, as assumed previously, but rather
guaranteeing a bounded amount of \emph{information} in each neighborhood per
round, a property that we show is achievable using a new random coding
technique.
  Technically, we provide efficient algorithms for fundamental distributed
tasks in additive networks, such as solving various symmetry breaking problems,
approximating network parameters, and solving an \emph{asymmetry revealing}
problem such as computing a maximal input.
  The key method used is a novel random coding technique that allows a node to
successfully decode the received information, as long as it does not contain
too many distinct values. We then design our algorithms to produce a limited
amount of information in each neighborhood in order to leverage our enriched
toolbox for computing in additive networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03664</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03664</id><created>2015-08-14</created><authors><author><keyname>Khan</keyname><forenames>Amjad Saeed</forenames></author><author><keyname>Tassi</keyname><forenames>Andrea</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author></authors><title>Rethinking the Intercept Probability of Random Linear Network Coding</title><categories>cs.CR cs.IT cs.NI cs.PF math.IT</categories><comments>IEEE Communications Letters, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter considers a network comprising a transmitter, which employs
random linear network coding to encode a message, a legitimate receiver, which
can recover the message if it gathers a sufficient number of linearly
independent coded packets, and an eavesdropper. Closed-form expressions for the
probability of the eavesdropper intercepting enough coded packets to recover
the message are derived. Transmission with and without feedback is studied.
Furthermore, an optimization model that minimizes the intercept probability
under delay and reliability constraints is presented. Results validate the
proposed analysis and quantify the secrecy gain offered by a feedback link from
the legitimate receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03671</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03671</id><created>2015-08-14</created><authors><author><keyname>Ozkan</keyname><forenames>Ibrahim</forenames></author><author><keyname>Turksen</keyname><forenames>I. Burhan</forenames></author></authors><title>Fuzzy Longest Common Subsequence Matching With FCM</title><categories>cs.AI</categories><comments>Prepared April 17, 2013. 26 Pages</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capturing the interdependencies between real valued time series can be
achieved by finding common similar patterns. The abstraction of time series
makes the process of finding similarities closer to the way as humans do.
Therefore, the abstraction by means of a symbolic levels and finding the common
patterns attracts researchers. One particular algorithm, Longest Common
Subsequence, has been used successfully as a similarity measure between two
sequences including real valued time series. In this paper, we propose Fuzzy
Longest Common Subsequence matching for time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03679</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03679</id><created>2015-08-14</created><authors><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Cheung</keyname><forenames>Ho Yee</forenames></author><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Emamjomeh-Zadeh</keyname><forenames>Ehsan</forenames></author><author><keyname>Han</keyname><forenames>Li</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Mixture Selection, Mechanism Design, and Signaling</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We pose and study a fundamental algorithmic problem which we term mixture
selection, arising as a building block in a number of game-theoretic
applications: Given a function $g$ from the $n$-dimensional hypercube to the
bounded interval $[-1,1]$, and an $n \times m$ matrix $A$ with bounded entries,
maximize $g(Ax)$ over $x$ in the $m$-dimensional simplex. This problem arises
naturally when one seeks to design a lottery over items for sale in an auction,
or craft the posterior beliefs for agents in a Bayesian game through the
provision of information (a.k.a. signaling).
  We present an approximation algorithm for this problem when $g$
simultaneously satisfies two smoothness properties: Lipschitz continuity with
respect to the $L^\infty$ norm, and noise stability. The latter notion, which
we define and cater to our setting, controls the degree to which
low-probability errors in the inputs of $g$ can impact its output. When $g$ is
both $O(1)$-Lipschitz continuous and $O(1)$-stable, we obtain an (additive)
PTAS for mixture selection. We also show that neither assumption suffices by
itself for an additive PTAS, and both assumptions together do not suffice for
an additive FPTAS.
  We apply our algorithm to different game-theoretic applications from
mechanism design and optimal signaling. We make progress on a number of open
problems suggested in prior work by easily reducing them to mixture selection:
we resolve an important special case of the small-menu lottery design problem
posed by Dughmi, Han, and Nisan; we resolve the problem of revenue-maximizing
signaling in Bayesian second-price auctions posed by Emek et al. and Miltersen
and Sheffet; we design a quasipolynomial-time approximation scheme for the
optimal signaling problem in normal form games suggested by Dughmi; and we
design an approximation algorithm for the optimal signaling problem in the
voting model of Alonso and C\^{a}mara.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03686</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03686</id><created>2015-08-14</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>de Bianchi</keyname><forenames>Massimiliano Sassoli</forenames></author></authors><title>Beyond-Quantum Modeling of Question Order Effects and Response
  Replicability in Psychological Measurements</title><categories>cs.AI quant-ph</categories><comments>32 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general tension-reduction (GTR) model was recently considered to derive
quantum probabilities as (universal) averages over all possible forms of
non-uniform fluctuations, and explain their considerable success in describing
experimental situations also outside of the domain of physics, for instance in
the ambit of quantum models of cognition and decision. Yet, this result also
highlighted the possibility of observing violations of the predictions of the
Born rule, in those situations where the averaging would not be large enough,
or would be altered because of the combination of multiple measurements. In
this article we show that this is indeed the case in typical psychological
measurements exhibiting question order effects, by showing that their
statistics of outcomes are inherently non-Hilbertian, and require the larger
framework of the GTR-model to receive an exact mathematical description. We
also consider another unsolved problem of quantum cognition: response
replicability. It is has been observed that when question order effects and
response replicability occur together, the situation cannot be handled anymore
by quantum theory. However, we show that it can be easily and naturally
described in the GTR-model. Based on these findings, we motivate the adoption
in cognitive science of a hidden-measurements interpretation of the quantum
formalism, and of its GTR-model generalization, as the natural interpretational
framework explaining the data of psychological measurements on conceptual
entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03687</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03687</id><created>2015-08-14</created><authors><author><keyname>Siboni</keyname><forenames>Shachar</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author></authors><title>Universal Anomaly Detection: Algorithms and Applications</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern computer threats are far more complicated than those seen in the past.
They are constantly evolving, altering their appearance, perpetually changing
disguise. Under such circumstances, detecting known threats, a fortiori
zero-day attacks, requires new tools, which are able to capture the essence of
their behavior, rather than some fixed signatures. In this work, we propose
novel universal anomaly detection algorithms, which are able to learn the
normal behavior of systems and alert for abnormalities, without any prior
knowledge on the system model, nor any knowledge on the characteristics of the
attack. The suggested method utilizes the Lempel-Ziv universal compression
algorithm in order to optimally give probability assignments for normal
behavior (during learning), then estimate the likelihood of new data (during
operation) and classify it accordingly. The suggested technique is generic, and
can be applied to different scenarios. Indeed, we apply it to key problems in
computer security. The first is detecting Botnets Command and Control (C&amp;C)
channels. A Botnet is a logical network of compromised machines which are
remotely controlled by an attacker using a C&amp;C infrastructure, in order to
perform malicious activities. We derive a detection algorithm based on timing
data, which can be collected without deep inspection, from open as well as
encrypted flows. We evaluate the algorithm on real-world network traces,
showing how a universal, low complexity C&amp;C identification system can be built,
with high detection rates and low false-alarm probabilities. Further
applications include malicious tools detection via system calls monitoring and
data leakage identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03693</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03693</id><created>2015-08-14</created><authors><author><keyname>Zheng</keyname><forenames>Weiye</forenames></author><author><keyname>Wu</keyname><forenames>Wenchuan</forenames></author><author><keyname>Gomez-Exposito</keyname><forenames>Antonio</forenames></author><author><keyname>Zhang</keyname><forenames>Boming</forenames></author></authors><title>Distributed Robust Bilinear State Estimation for Power Systems with
  Nonlinear Measurements</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a fully distributed robust state-estimation (D-RBSE)
method that is applicable to multi-area power systems with nonlinear
measurements. We extend the recently introduced bilinear formulation of state
estimation problems to a robust model. A distributed bilinear state-estimation
procedure is developed. In both linear stages, the state estimation problem in
each area is solved locally, with minimal data exchange with its neighbors. The
intermediate nonlinear transformation can be performed by all areas in parallel
without any need of inter-regional communication. This algorithm does not
require a central coordinator and can compress bad measurements by introducing
a robust state estimation model. Numerical tests on IEEE 14-bus and 118-bus
benchmark systems demonstrate the validity of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03698</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03698</id><created>2015-08-15</created><updated>2015-11-10</updated><authors><author><keyname>Banerjee</keyname><forenames>Indranil</forenames></author><author><keyname>Richards</keyname><forenames>Dana</forenames></author></authors><title>Sorting Under 1-$\infty$ Cost Model</title><categories>cs.DS</categories><comments>12 pages, 1 figure, submitted to STOC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of sorting under non-uniform comparison
costs, where costs are either 1 or $\infty$. If comparing a pair has an
associated cost of $\infty$ then we say that such a pair cannot be compared
(forbidden pairs). Along with the set of elements $V$ the input to our problem
is a graph $G(V, E)$, whose edges represents the pairs that we can compare
incurring an unit of cost. Given a graph with $n$ vertices and $q$ forbidden
edges we propose the first non-trivial deterministic algorithm which makes
$O((q + n)\log{n})$ comparisons with a total complexity of $O(n^2 +
q^{\omega/2})$, where $\omega$ is the exponent in the complexity of matrix
multiplication. We also propose a simple randomized algorithm for the problem
which makes $\widetilde{O}(n^2/\sqrt{q + n} + n\sqrt{q})$ probes with high
probability. When the input graph is random we show that
$\widetilde{O}(\min{(n^{3/2}, pn^2)})$ probes suffice, where $p$ is the edge
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03710</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03710</id><created>2015-08-15</created><authors><author><keyname>Fayyaz</keyname><forenames>Mohsen</forenames></author><author><keyname>PourReza</keyname><forenames>Masoud</forenames></author><author><keyname>Saffar</keyname><forenames>Mohammad Hajizadeh</forenames></author><author><keyname>Sabokrou</keyname><forenames>Mohammad</forenames></author><author><keyname>Fathy</keyname><forenames>Mahmood</forenames></author></authors><title>A Novel Approach For Finger Vein Verification Based on Self-Taught
  Learning</title><categories>cs.CV</categories><comments>4 pages, 4 figures, Submitted Iranian Conference on Machine Vision
  and Image Processing</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a method for user Finger Vein Authentication (FVA)
as a biometric system. Using the discriminative features for classifying theses
finger veins is one of the main tips that make difference in related works,
Thus we propose to learn a set of representative features, based on
autoencoders. We model the user finger vein using a Gaussian distribution.
Experimental results show that our algorithm perform like a state-of-the-art on
SDUMLA-HMT benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03712</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03712</id><created>2015-08-15</created><authors><author><keyname>Thomann</keyname><forenames>Philipp</forenames></author><author><keyname>Steinwart</keyname><forenames>Ingo</forenames></author><author><keyname>Schmid</keyname><forenames>Nico</forenames></author></authors><title>Towards an Axiomatic Approach to Hierarchical Clustering of Measures</title><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><msc-class>Primary 62H30, Secondary 91C20, 62G07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose some axioms for hierarchical clustering of probability measures
and investigate their ramifications. The basic idea is to let the user
stipulate the clusters for some elementary measures. This is done without the
need of any notion of metric, similarity or dissimilarity. Our main results
then show that for each suitable choice of user-defined clustering on
elementary measures we obtain a unique notion of clustering on a large set of
distributions satisfying a set of additivity and continuity axioms. We
illustrate the developed theory by numerous examples including some with and
some without a density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03713</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03713</id><created>2015-08-15</created><authors><author><keyname>Bilir</keyname><forenames>S.</forenames></author><author><keyname>Gogus</keyname><forenames>E.</forenames></author><author><keyname>Tas</keyname><forenames>O. Onal</forenames></author><author><keyname>Yontan</keyname><forenames>T.</forenames></author></authors><title>A New Ranking Scheme for the Institutional Scientific Performance</title><categories>astro-ph.IM cs.DL physics.soc-ph</categories><comments>12 pages, 3 figures and 2 tables, accepted for publication in Journal
  of Scientometric Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new performance indicator to evaluate the productivity of
research institutions by their disseminated scientific papers. The new quality
measure includes two principle components: the normalized impact factor of the
journal in which paper was published, and the number of citations received per
year since it was published. In both components, the scientific impacts are
weighted by the contribution of authors from the evaluated institution. As a
whole, our new metric, namely, the institutional performance score takes into
account both journal based impact and articles specific impacts. We apply this
new scheme to evaluate research output performance of Turkish institutions
specialized in astronomy and astrophysics in the period of 1998-2012. We
discuss the implications of the new metric, and emphasize the benefits of it
along with comparison to other proposed institutional performance indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03714</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03714</id><created>2015-08-15</created><authors><author><keyname>Bramas</keyname><forenames>Quentin</forenames><affiliation>NPA, LIP6, UPMC, LINCS</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, IUF, LINCS, NPA, UPMC</affiliation></author></authors><title>Asynchronous Pattern Formation without Chirality</title><categories>cs.DC cs.MA cs.RO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new probabilistic pattern formation algorithm for oblivious
mobile robots that operate in the ASYNC model. Unlike previous work, our
algorithm makes no assumptions about the local coordinate systems of robots
(the robots do not share a common &quot; North &quot; nor a common &quot; Right &quot;), yet it
preserves the ability to form any general pattern (and not just patterns that
satisfy symmetricity predicates). Our proposal also gets rid of the previous
assumption (in the same model) that robots do not pause while moving (so, our
robots really are fully asynchronous), and the amount of randomness is kept low
-- a single random bit per robot per Look-Compute-Move cycle is used. Our
protocol consists in the combination of two phases, a probabilistic leader
election phase, and a deterministic pattern formation one. As the deterministic
phase does not use chirality, it may be of independent interest in the
determin-istic context. A straightforward extension of our algorithm permits to
form patterns with multiplicity points (provided robots are endowed with
multiplicity detection), a new feature in the context of pattern formation that
we believe is an important feature of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03715</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03715</id><created>2015-08-15</created><authors><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Naldi</keyname><forenames>Simone</forenames><affiliation>LAAS</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>LIP6, PolSys</affiliation></author></authors><title>Exact algorithms for linear matrix inequalities</title><categories>math.OC cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A(x) = A_0 + x_1 A_1+ \cdots +x_nA_n$ be a linear matrix, or pencil,
generated by given symmetric matrices A0, A1, ... An of size m with rational
entries. The set of real vectors x such that the pencil is positive
semidefinite is a convex semi-algebraic set called spectrahedron, described by
a linear matrix inequality (LMI). We design an exact algorithm that, up to
genericity assumptions on the input matrices, computes an exact algebraic
representation of at least one point in the spectrahedron, or decides that it
is empty. The algorithm does not assume the existence of an interior point, and
the computed point minimizes the rank of the pencil on the spectrahedron. The
degree d of the algebraic representation of the point coincides experimentally
with the algebraic degree of a generic semidefinite program associated to the
pencil. We provide explicit bounds for the complexity of our algorithm, proving
that the maximum number of arithmetic operations that are performed is
essentially quadratic in a multilinear Bezout bound of d. When the size m of
the pencil is fixed, such a bound, and hence the complexity, is polynomial in
n, the number of variables. We conclude by providing results of experiments
showing practical improvements with respect to state-of-the-art computer
algebra algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03716</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03716</id><created>2015-08-15</created><authors><author><keyname>Stai</keyname><forenames>Eleni</forenames></author><author><keyname>Loulakis</keyname><forenames>Michail</forenames></author><author><keyname>Papavassiliou</keyname><forenames>Symeon</forenames></author></authors><title>Cross-Layer Design of Wireless Multihop Networks over Stochastic
  Channels with Time-Varying Statistics</title><categories>cs.SY math.PR</categories><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Utility Maximization (NUM) is often applied for the cross-layer
design of wireless networks considering known wireless channels. However,
realistic wireless channel capacities are stochastic bearing time-varying
statistics, necessitating the redesign and solution of NUM problems to capture
such effects. Based on NUM theory we develop a framework for scheduling,
routing, congestion and power control in wireless multihop networks that
considers stochastic Long or Short Term Fading wireless channels. Specifically,
the wireless channel is modeled via stochastic differential equations
alleviating several assumptions that exist in state-of-the-art channel modeling
within the NUM framework such as the finite number of states or the
stationarity. Our consideration of wireless channel modeling leads to a NUM
problem formulation that accommodates non-convex and time-varying utilities. We
consider both cases of non orthogonal and orthogonal access of users to the
medium. In the first case, scheduling is performed via power control, while the
latter separates scheduling and power control and the role of power control is
to further increase users' optimal utility by exploiting random reductions of
the stochastic channel power loss while also considering energy efficiency.
Finally, numerical results evaluate the performance and operation of the
proposed approach and study the impact of several involved parameters on
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03720</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03720</id><created>2015-08-15</created><authors><author><keyname>Yan</keyname><forenames>Xu</forenames></author><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Chen</keyname><forenames>Yunchuan</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Classifying Relations via Long Short Term Memory Networks along Shortest
  Dependency Path</title><categories>cs.CL cs.LG</categories><comments>EMNLP '15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relation classification is an important research arena in the field of
natural language processing (NLP). In this paper, we present SDP-LSTM, a novel
neural network to classify the relation of two entities in a sentence. Our
neural architecture leverages the shortest dependency path (SDP) between two
entities; multichannel recurrent neural networks, with long short term memory
(LSTM) units, pick up heterogeneous information along the SDP. Our proposed
model has several distinct features: (1) The shortest dependency paths retain
most relevant information (to relation classification), while eliminating
irrelevant words in the sentence. (2) The multichannel LSTM networks allow
effective information integration from heterogeneous sources over the
dependency paths. (3) A customized dropout strategy regularizes the neural
network to alleviate overfitting. We test our model on the SemEval 2010
relation classification task, and achieve an $F_1$-score of 83.7\%, higher than
competing methods in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03721</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03721</id><created>2015-08-15</created><authors><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Chen</keyname><forenames>Yunchuan</forenames></author><author><keyname>Lu</keyname><forenames>Yangyang</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>A Comparative Study on Regularization Strategies for Embedding-based
  Neural Networks</title><categories>cs.CL cs.LG</categories><comments>EMNLP '15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to compare different regularization strategies to address a
common phenomenon, severe overfitting, in embedding-based neural networks for
NLP. We chose two widely studied neural models and tasks as our testbed. We
tried several frequently applied or newly proposed regularization strategies,
including penalizing weights (embeddings excluded), penalizing embeddings,
re-embedding words, and dropout. We also emphasized on incremental
hyperparameter tuning, and combining different regularizations. The results
provide a picture on tuning hyperparameters for neural NLP models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03722</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03722</id><created>2015-08-15</created><authors><author><keyname>Pejovic</keyname><forenames>Veljko</forenames></author><author><keyname>Mehrotra</keyname><forenames>Abhinav</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Anticipatory Mobile Digital Health: Towards Personalised Proactive
  Therapies and Prevention Strategies</title><categories>cs.CY cs.HC</categories><comments>2 figures, 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last two centuries saw groundbreaking advances in the field of
healthcare: from the invention of the vaccine to organ transplant, and
eradication of numerous deadly diseases. Yet, these breakthroughs have only
illuminated the role that individual traits and behaviours play in the health
state of a person. Continuous patient monitoring and individually-tailored
therapies can help in early detection and efficient tackling of health issues.
However, even the most developed nations cannot afford proactive personalised
healthcare at scale. Mobile computing devices, nowadays equipped with an array
of sensors, high-performance computing power, and carried by their owners at
all time, promise to revolutionise modern healthcare. These devices can enable
continuous patient monitoring, and, with the help of machine learning, can
build predictive models of patient's health and behaviour. Finally, through
their close integration with a user's lifestyle mobiles can be used to deliver
personalised proactive therapies. In this article, we develop the concept of
anticipatory mobile-based healthcare - anticipatory mobile digital health - and
examine the opportunities and challenges associated with its practical
realisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03725</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03725</id><created>2015-08-15</created><authors><author><keyname>Pejovic</keyname><forenames>Veljko</forenames></author><author><keyname>Lathia</keyname><forenames>Neal</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Mobile-Based Experience Sampling for Behaviour Research</title><categories>cs.HC cs.CY</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Experience Sampling Method (ESM) introduces in-situ sampling of human
behaviour, and provides researchers and behavioural therapists with
ecologically valid and timely assessments of a person's psychological state.
This, in turn, opens up new opportunities for understanding behaviour at a
scale and granularity that was not possible just a few years ago. The practical
applications are many, such as the delivery of personalised and agile behaviour
interventions. Mobile computing devices represent a revolutionary platform for
improving ESM. They are an inseparable part of our daily lives, context-aware,
and can interact with people at suitable moments. Furthermore, these devices
are equipped with sensors, and can thus take part of the reporting burden off
the participant, and collect data automatically. The goal of this survey is to
discuss recent advancements in using mobile technologies for ESM (mESM), and
present our vision of the future of mobile experience sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03728</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03728</id><created>2015-08-15</created><updated>2015-12-14</updated><authors><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Zhu</keyname><forenames>Guangxu</forenames></author></authors><title>Some New Research Trends in Wirelessly Powered Communications</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vision of seamlessly integrating information transfer (IT) and microwave
based power transfer (PT) in the same system has led to the emergence of a new
research area, called wirelessly power communications (WPC). Extensive research
has been conducted on developing WPC theory and techniques, building on the
extremely rich wireless communications litera- ture covering diversified topics
such as transmissions, resource allocations, medium access control and network
protocols and architectures. Despite these research efforts, transforming WPC
from theory to practice still faces many unsolved prob- lems concerning issues
such as mobile complexity, power transfer efficiency, and safety. Furthermore,
the fundamental limits of WPC remain largely unknown. Recent attempts to
address these open issues has resulted in the emergence of numerous new
research trends in the WPC area. A few promising trends are introduced in this
article. From the practical perspective, the use of backscatter antennas can
support WPC for low-complexity passive devices, the design of spiky waveforms
can improve the PT efficiency, and analog spatial decoupling is proposed for
solving the PT-IT near-far problem in WPC. From the theoretic perspective, the
fundamental limits of WPC can be quantified by leveraging recent results on
super-directivity and the limit can be improved by the deployment of
large-scale distributed antenna arrays. Specific research problems along these
trends are discussed, whose solutions can lead to significant advancements in
WPC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03735</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03735</id><created>2015-08-15</created><updated>2016-01-05</updated><authors><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Coordination Complexity: Small Information Coordinating Large
  Populations</title><categories>cs.DS cs.GT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of a quantity that we call coordination complexity. In
a distributed optimization problem, the information defining a problem instance
is distributed among $n$ parties, who need to each choose an action, which
jointly will form a solution to the optimization problem. The coordination
complexity represents the minimal amount of information that a centralized
coordinator, who has full knowledge of the problem instance, needs to broadcast
in order to coordinate the $n$ parties to play a nearly optimal solution.
  We show that upper bounds on the coordination complexity of a problem imply
the existence of good jointly differentially private algorithms for solving
that problem, which in turn are known to upper bound the price of anarchy in
certain games with dynamically changing populations.
  We show several results. We fully characterize the coordination complexity
for the problem of computing a many-to-one matching in a bipartite graph by
giving almost matching lower and upper bounds.Our upper bound in fact extends
much more generally, to the problem of solving a linearly separable convex
program. We also give a different upper bound technique, which we use to bound
the coordination complexity of coordinating a Nash equilibrium in a routing
game, and of computing a stable matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03755</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03755</id><created>2015-08-15</created><authors><author><keyname>Potapov</keyname><forenames>Danila</forenames><affiliation>LEAR</affiliation></author><author><keyname>Douze</keyname><forenames>Matthijs</forenames><affiliation>LEAR</affiliation></author><author><keyname>Revaud</keyname><forenames>Jerome</forenames><affiliation>LEAR</affiliation></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames><affiliation>LEAR, CIMS</affiliation></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames><affiliation>LEAR</affiliation></author></authors><title>Beat-Event Detection in Action Movie Franchises</title><categories>cs.CV</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While important advances were recently made towards temporally localizing and
recognizing specific human actions or activities in videos, efficient detection
and classification of long video chunks belonging to semantically defined
categories such as &quot;pursuit&quot; or &quot;romance&quot; remains challenging.We introduce a
new dataset, Action Movie Franchises, consisting of a collection of Hollywood
action movie franchises. We define 11 non-exclusive semantic categories -
called beat-categories - that are broad enough to cover most of the movie
footage. The corresponding beat-events are annotated as groups of video shots,
possibly overlapping.We propose an approach for localizing beat-events based on
classifying shots into beat-categories and learning the temporal constraints
between shots. We show that temporal constraints significantly improve the
classification performance. We set up an evaluation protocol for beat-event
localization as well as for shot classification, depending on whether movies
from the same franchise are present or not in the training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03762</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03762</id><created>2015-08-15</created><authors><author><keyname>Chockler</keyname><forenames>Gregory</forenames></author><author><keyname>Dobre</keyname><forenames>Dan</forenames></author><author><keyname>Shraer</keyname><forenames>Alexander</forenames></author><author><keyname>Spiegelman</keyname><forenames>Alexander</forenames></author></authors><title>Space Bounds for Reliable Multi-Writer Data Store: Inherent Cost of
  Read/Write Primitives</title><categories>cs.DC</categories><acm-class>C.2.4; C.4; D.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable storage emulations from fault-prone components have established
themselves as an algorithmic foundation of modern storage services and
applications. Most existing reliable storage emulations are built from storage
services supporting arbitrary read-modify-write primitives. Since such
primitives are not typically exposed by pre-existing or off-the-shelf
components (such as cloud storage services or network-attached disks) it is
natural to ask if they are indeed essential for efficient storage emulations.
In this paper, we answer this question in the affirmative. We show that
relaxing the underlying storage to only support read/write operations leads to
a linear blow-up in the emulation space requirements. We also show that the
space complexity is not adaptive to concurrency, which implies that the storage
cannot be reliably reclaimed even in sequential runs. On a positive side, we
show that Compare-and-Swap primitives, which are commonly available with many
off-the-shelf storage services, can be used to emulate a reliable multi-writer
atomic register with constant storage and adaptive time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03763</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03763</id><created>2015-08-15</created><authors><author><keyname>Chodpathumwan</keyname><forenames>Yodsawalai</forenames></author><author><keyname>Aleyasin</keyname><forenames>Amirhossein</forenames></author><author><keyname>Termehchy</keyname><forenames>Arash</forenames></author><author><keyname>Sun</keyname><forenames>Yizhou</forenames></author></authors><title>Representation Independent Proximity and Similarity Search</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding similar or strongly related entities in a graph database is a
fundamental problem in data management and analytics with applications in
similarity query processing, entity resolution, and pattern matching.
Similarity search algorithms usually leverage the structural properties of the
data graph to quantify the degree of similarity or relevance between entities.
Nevertheless, the same information can be represented in many different
structures and the structural properties observed over particular
representations do not necessarily hold for alternative structures. Thus, these
algorithms are effective on some representations and ineffective on others. We
postulate that a similarity search algorithm should return essentially the same
answers over different databases that represent the same information. We
formally define the property of representation independence for similarity
search algorithms as their robustness against transformations that modify the
structure of databases and preserve their information content. We formalize two
widespread groups of such transformations called {\it relationship
reorganizing} and {\it entity rearranging} transformations. We show that
current similarity search algorithms are not representation independent under
these transformations and propose an algorithm called {\bf R-PathSim}, which is
provably robust under these transformations. We perform an extensive empirical
study on the representation independence of current similarity search
algorithms under relationship reorganizing and entity rearranging
transformations. Our empirical results suggest that current similarity search
algorithms except for R-PathSim are highly sensitive to the data
representation. These results also indicate that R-PathSim is as effective or
more effective than other similarity search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03765</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03765</id><created>2015-08-15</created><authors><author><keyname>Everett</keyname><forenames>Evan</forenames></author><author><keyname>Shepard</keyname><forenames>Clayton</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Measurement-driven Evaluation of All-digital Many-antenna Full-duplex
  Communication</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communication on August
  13, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present and study an all-digital method, called \softNull,
to enable full-duplex in many-antenna systems. Unlike most designs that rely on
analog cancelers to suppress self-interference, SoftNull relies on digital
transmit beamforming to reduce self-interference. SoftNull does not attempt to
perfectly null self-interference, but instead seeks to reduce self-interference
sufficiently to prevent swamping the receiver's dynamic range. Residual
self-interference is then cancelled digitally by the receiver. We evaluate the
performance of SoftNull using measurements from a 72-element antenna array in
both indoor and outdoor environments. We find that SoftNull can significantly
outperform half-duplex for small cells operating in the many-antenna regime,
where the number of antennas is many more than the number of users served
simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03767</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03767</id><created>2015-08-15</created><authors><author><keyname>Wei</keyname><forenames>Zhipeng</forenames></author><author><keyname>Cui</keyname><forenames>Zehan</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author></authors><title>Cracking Intel Sandy Bridge's Cache Hash Function</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On Intel Sandy Bridge processor, last level cache (LLC) is divided into cache
slices and all physical addresses are distributed across the cache slices using
an hash function. With this undocumented hash function existing, it is
impossible to implement cache partition based on page coloring. This article
cracks the hash functions on two types of Intel Sandy processors by converting
the problem of cracking the hash function to the problem of classifying data
blocks into different groups based on eviction relationship existing between
data blocks that are mapped to the same cache set. Based on the cracking
result, this article proves that it's possible to implement cache partition
based on page coloring on cache indexed by hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03769</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03769</id><created>2015-08-15</created><authors><author><keyname>Andrew</keyname><forenames>Lachlan L. H.</forenames></author><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Lin</keyname><forenames>Minghong</forenames></author><author><keyname>Meyerson</keyname><forenames>Adam</forenames></author><author><keyname>Roytman</keyname><forenames>Alan</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>A Tale of Two Metrics: Simultaneous Bounds on Competitiveness and Regret</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider algorithms for &quot;smoothed online convex optimization&quot; problems, a
variant of the class of online convex optimization problems that is strongly
related to metrical task systems. Prior literature on these problems has
focused on two performance metrics: regret and the competitive ratio. There
exist known algorithms with sublinear regret and known algorithms with constant
competitive ratios; however, no known algorithm achieves both simultaneously.
We show that this is due to a fundamental incompatibility between these two
metrics - no algorithm (deterministic or randomized) can achieve sublinear
regret and a constant competitive ratio, even in the case when the objective
functions are linear. However, we also exhibit an algorithm that, for the
important special case of one-dimensional decision spaces, provides sublinear
regret while maintaining a competitive ratio that grows arbitrarily slowly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03773</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03773</id><created>2015-08-15</created><authors><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author></authors><title>No acute tetrahedron is an 8-reptile</title><categories>cs.CG math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $r$-gentiling is a dissection of a shape into $r \geq 2$ parts which are
all similar to the original shape. An $r$-reptiling is an $r$-gentiling of
which all parts are mutually congruent. This article shows that no acute
tetrahedron is an $r$-gentile or $r$-reptile for any $r &lt; 9$, by showing that
no acute spherical diangle can be dissected into less than nine acute spherical
triangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03776</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03776</id><created>2015-08-15</created><authors><author><keyname>Wang</keyname><forenames>Weimin</forenames></author><author><keyname>Xu</keyname><forenames>Chong</forenames></author></authors><title>Analysis of Information Theoretic Limitation for Linear Time Invariant
  Feedback Systems</title><categories>cs.IT math.IT</categories><comments>in Chinese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-theoretic fundamental limitation in feedback control system is an
important topic for decades. In this paper, a new bode-like fundamental
inequality in causal feedback control system is developed. This inequality
relates directed information, mutual information and bode sensitivity
functions. This inequality recovers previous known results in feedback system
as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03787</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03787</id><created>2015-08-15</created><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>Information-theoretically Secure Erasure Codes for Distributed Storage</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repair operations in distributed storage systems potentially expose the data
to malicious acts of passive eavesdroppers or active adversaries, which can be
detrimental to the security of the system. This paper presents erasure codes
and repair algorithms that ensure security of the data in the presence of
passive eavesdroppers and active adversaries, while maintaining high
availability, reliability and efficiency in the system. Our codes are optimal
in that they meet previously proposed lower bounds on the storage,
network-bandwidth, and reliability requirements for a wide range of system
parameters. Our results thus establish the capacity of such systems. Our codes
for security from active adversaries provide an additional appealing feature of
`on-demand security' where the desired level of security can be chosen
separately for each instance of repair, and our algorithms remain optimal
simultaneously for all possible levels. The paper also provides necessary and
sufficient conditions governing the transformation of any (non-secure) code
into one providing on-demand security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03790</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03790</id><created>2015-08-16</created><updated>2015-08-25</updated><authors><author><keyname>Yao</keyname><forenames>Kaisheng</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Vylomova</keyname><forenames>Katerina</forenames></author><author><keyname>Duh</keyname><forenames>Kevin</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Depth-Gated LSTM</title><categories>cs.NE cs.CL</categories><comments>Content presented in 2015 Jelinek Summer Workshop on Speech and
  Language Technology on August 14th 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, we present an extension of long short-term memory (LSTM)
neural networks to using a depth gate to connect memory cells of adjacent
layers. Doing so introduces a linear dependence between lower and upper layer
recurrent units. Importantly, the linear dependence is gated through a gating
function, which we call depth gate. This gate is a function of the lower layer
memory cell, the input to and the past memory cell of this layer. We conducted
experiments and verified that this new architecture of LSTMs was able to
improve machine translation and language modeling performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03800</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03800</id><created>2015-08-16</created><updated>2016-01-26</updated><authors><author><keyname>Lv</keyname><forenames>Qian</forenames></author><author><keyname>Yi</keyname><forenames>Yuhao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongzhi</forenames></author></authors><title>Corona graphs as a model of small-world networks</title><categories>cs.DM</categories><comments>21 pages, 2 figures</comments><acm-class>G.2.2</acm-class><doi>10.1088/1742-5468/2015/11/P11024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce recursive corona graphs as a model of small-world networks. We
investigate analytically the critical characteristics of the model, including
order and size, degree distribution, average path length, clustering
coefficient, and the number of spanning trees, as well as Kirchhoff index.
Furthermore, we study the spectra for the adjacency matrix and the Laplacian
matrix for the model. We obtain explicit results for all the quantities of the
recursive corona graphs, which are similar to those observed in real-life
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03810</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03810</id><created>2015-08-16</created><authors><author><keyname>Catanzaro</keyname><forenames>Daniele</forenames></author><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author><author><keyname>Halld&#xf3;rsson</keyname><forenames>Bjarni V.</forenames></author><author><keyname>Halld&#xf3;rsson</keyname><forenames>Magn&#xfa;s M.</forenames></author><author><keyname>Hixon</keyname><forenames>Thomas</forenames></author><author><keyname>Stacho</keyname><forenames>Juraj</forenames></author></authors><title>Max Point-Tolerance Graphs</title><categories>cs.DM math.CO</categories><comments>Accepted to the Journal of Discrete Applied Mathematics</comments><doi>10.1016/j.dam.2015.08.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is a \emph{max point-tolerance (MPT)} graph if each vertex $v$ of
$G$ can be mapped to a \emph{pointed-interval} $(I_v, p_v)$ where $I_v$ is an
interval of $\mathbb{R}$ and $p_v \in I_v$ such that $uv$ is an edge of $G$ iff
$I_u \cap I_v \supseteq \{p_u, p_v\}$. MPT graphs model relationships among DNA
fragments in genome-wide association studies as well as basic transmission
problems in telecommunications. We formally introduce this graph class,
characterize it, study combinatorial optimization problems on it, and relate it
to several well known graph classes. We characterize MPT graphs as a special
case of several 2D geometric intersection graphs; namely, triangle, rectangle,
L-shape, and line segment intersection graphs. We further characterize MPT as
having certain linear orders on their vertex set. Our last characterization is
that MPT graphs are precisely obtained by intersecting special pairs of
interval graphs. We also show that, on MPT graphs, the maximum weight
independent set problem can be solved in polynomial time, the coloring problem
is NP-complete, and the clique cover problem has a 2-approximation. Finally, we
demonstrate several connections to known graph classes; e.g., MPT graphs
strictly contain interval graphs and outerplanar graphs, but are incomparable
to permutation, chordal, and planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03812</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03812</id><created>2015-08-16</created><authors><author><keyname>Li</keyname><forenames>Jiuyong</forenames></author><author><keyname>Ma</keyname><forenames>Saisai</forenames></author><author><keyname>Le</keyname><forenames>Thuc Duy</forenames></author><author><keyname>Liu</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Jixue</forenames></author></authors><title>Causal Decision Trees</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncovering causal relationships in data is a major objective of data
analytics. Causal relationships are normally discovered with designed
experiments, e.g. randomised controlled trials, which, however are expensive or
infeasible to be conducted in many cases. Causal relationships can also be
found using some well designed observational studies, but they require domain
experts' knowledge and the process is normally time consuming. Hence there is a
need for scalable and automated methods for causal relationship exploration in
data. Classification methods are fast and they could be practical substitutes
for finding causal signals in data. However, classification methods are not
designed for causal discovery and a classification method may find false causal
signals and miss the true ones. In this paper, we develop a causal decision
tree where nodes have causal interpretations. Our method follows a well
established causal inference framework and makes use of a classic statistical
test. The method is practical for finding causal signals in large data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03819</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03819</id><created>2015-08-16</created><authors><author><keyname>Li</keyname><forenames>Jiuyong</forenames></author><author><keyname>Le</keyname><forenames>Thuc Duy</forenames></author><author><keyname>Liu</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Jixue</forenames></author><author><keyname>Jin</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Bingyu</forenames></author><author><keyname>Ma</keyname><forenames>Saisai</forenames></author></authors><title>From Observational Studies to Causal Rule Mining</title><categories>cs.AI</categories><comments>This paper has been accepted by ACM TIST journal and will be
  available soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomised controlled trials (RCTs) are the most effective approach to causal
discovery, but in many circumstances it is impossible to conduct RCTs.
Therefore observational studies based on passively observed data are widely
accepted as an alternative to RCTs. However, in observational studies, prior
knowledge is required to generate the hypotheses about the cause-effect
relationships to be tested, hence they can only be applied to problems with
available domain knowledge and a handful of variables. In practice, many data
sets are of high dimensionality, which leaves observational studies out of the
opportunities for causal discovery from such a wealth of data sources. In
another direction, many efficient data mining methods have been developed to
identify associations among variables in large data sets. The problem is,
causal relationships imply associations, but the reverse is not always true.
However we can see the synergy between the two paradigms here. Specifically,
association rule mining can be used to deal with the high-dimensionality
problem while observational studies can be utilised to eliminate non-causal
associations. In this paper we propose the concept of causal rules (CRs) and
develop an algorithm for mining CRs in large data sets. We use the idea of
retrospective cohort studies to detect CRs based on the results of association
rule mining. Experiments with both synthetic and real world data sets have
demonstrated the effectiveness and efficiency of CR mining. In comparison with
the commonly used causal discovery methods, the proposed approach in general is
faster and has better or competitive performance in finding correct or sensible
causes. It is also capable of finding a cause consisting of multiple variables,
a feature that other causal discovery methods do not possess.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03826</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03826</id><created>2015-08-16</created><authors><author><keyname>Li</keyname><forenames>Shaohua</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Miao</keyname><forenames>Chunyan</forenames></author></authors><title>A Generative Word Embedding Model and its Low Rank Positive Semidefinite
  Solution</title><categories>cs.CL cs.LG stat.ML</categories><comments>Proceedings of the Conference on Empirical Methods in Natural
  Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing word embedding methods can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-based methods. However some models are
opaque to probabilistic interpretation, and MF-based methods, typically solved
using Singular Value Decomposition (SVD), may incur loss of corpus information.
In addition, it is desirable to incorporate global latent factors, such as
topics, sentiments or writing styles, into the word embedding model. Since
generative models provide a principled way to incorporate latent factors, we
propose a generative word embedding model, which is easy to interpret, and can
serve as a basis of more sophisticated latent factor models. The model
inference reduces to a low rank weighted positive semidefinite approximation
problem. Its optimization is approached by eigendecomposition on a submatrix,
followed by online blockwise regression, which is scalable and avoids the
information loss in SVD. In experiments on 7 common benchmark datasets, our
vectors are competitive to word2vec, and better than other MF-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03837</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03837</id><created>2015-08-16</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Incorporating User Interaction into Imperative Languages</title><categories>cs.PL</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adding versatile interactions to imperative programming -- C, Java and
Android -- is an essential task. Unfortunately, existing languages provide
little constructs for user interaction.
  We propose a computability-logical approach to user interaction. We
illustrate our idea via C^I, an extension of the core C with a new choice
statement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03838</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03838</id><created>2015-08-16</created><authors><author><keyname>Merz</keyname><forenames>Stephan</forenames></author><author><keyname>Vanzetto</keyname><forenames>Hern&#xe1;n</forenames></author></authors><title>Encoding TLA+ set theory into many-sorted first-order logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an encoding of Zermelo-Fraenkel set theory into many-sorted
first-order logic, the input language of state-of-the-art SMT solvers. This
translation is the main component of a back-end prover based on SMT solvers in
the TLA+ Proof System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03840</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03840</id><created>2015-08-16</created><authors><author><keyname>Adeyanju</keyname><forenames>Ibrahim</forenames></author><author><keyname>Babalola</keyname><forenames>Comfort</forenames></author><author><keyname>Salaudeen</keyname><forenames>Kareemat</forenames></author><author><keyname>Oyediran</keyname><forenames>Biola</forenames></author></authors><title>3D-Computer Animation for a Yoruba Native Folktale</title><categories>cs.GR cs.MM</categories><comments>9 pages</comments><journal-ref>International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol.5, No.3, July 2015</journal-ref><doi>10.5121/ijcga.2015.5302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer graphics has wide range of applications which are implemented into
computer animation, computer modeling among others. Since the invention of
computer graphics researchers have not paid much of attentions toward the
possibility of converting oral tales otherwise known as folktales into possible
cartoon animated videos. This paper is based on how to develop cartoons of
local folktales that will be of huge benefits to Nigerians. The activities were
divided into 5 stages; analysis, design, development, implementation and
evaluation which involved various processes and use of various specialized
software and hardware. After the implementation of this project, the video
characteristics were evaluated using likert scale. Analysis of 30 user
responses indicated that 17 users (56.7 percent) rated the image quality as
excellent, the video and image synchronization was rated as excellent by 9
users (30 percent), the Background noise was rated excellent by 18 users (60
percent), the Character Impression was rated Excellent by 11 users (36.67
percent), the general assessment of the storyline was rated excellent by 17
users (56.7 percent), the video Impression was rated excellent by 11 users
(36.67 percent) and the voice quality was rated by 10 users (33.33 percent) as
excellent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03843</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03843</id><created>2015-08-16</created><authors><author><keyname>Rodriguez</keyname><forenames>Marko A.</forenames></author></authors><title>The Gremlin Graph Traversal Machine and Language</title><categories>cs.DB cs.DM</categories><comments>To appear in the Proceedings of the 2015 ACM Database Programming
  Languages Conference</comments><acm-class>G.2</acm-class><doi>10.1145/2815072.2815073</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Gremlin is a graph traversal machine and language designed, developed, and
distributed by the Apache TinkerPop project. Gremlin, as a graph traversal
machine, is composed of three interacting components: a graph $G$, a traversal
$\Psi$, and a set of traversers $T$. The traversers move about the graph
according to the instructions specified in the traversal, where the result of
the computation is the ultimate locations of all halted traversers. A Gremlin
machine can be executed over any supporting graph computing system such as an
OLTP graph database and/or an OLAP graph processor. Gremlin, as a graph
traversal language, is a functional language implemented in the user's native
programming language and is used to define the $\Psi$ of a Gremlin machine.
This article provides a mathematical description of Gremlin and details its
automaton and functional properties. These properties enable Gremlin to
naturally support imperative and declarative querying, host language
agnosticism, user-defined domain specific languages, an extensible
compiler/optimizer, single- and multi-machine execution models, hybrid depth-
and breadth-first evaluation, as well as the existence of a Universal Gremlin
Machine and its respective entailments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03846</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03846</id><created>2015-08-16</created><authors><author><keyname>Picado</keyname><forenames>Jose</forenames></author><author><keyname>Termehchy</keyname><forenames>Arash</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author></authors><title>Schema Independent Relational Learning</title><categories>cs.DB cs.AI cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning novel and interesting concepts and relations from relational
databases is an important problem with many applications in database systems
and machine learning. Relational learning algorithms generally leverage the
properties of the database schema to find the definition of the target concept
in terms of the existing relations in the database. Nevertheless, it is well
established that the same data set may be represented under different schemas
for various reasons, such as efficiency, data quality, and usability.
Unfortunately, many current learning algorithms tend to vary quite
substantially over the choice of schema, both in terms of learning accuracy and
efficiency, which complicates their off-the-shelf application. In this paper,
we formalize the property of schema independence of relational learning
algorithms, and study both the theoretical and empirical dependence of existing
algorithms on the common class of vertical (de)composition schema
transformations. We study both sample-based learning algorithms, which learn
from sets of labeled examples, and query-based algorithms, which learn by
asking queries to a user. For sample-based algorithms we consider the two main
algorithm classes: top-down and bottom-up. We prove that practical top-down
algorithms are generally not schema independent, while, in contrast, two
bottom-up algorithms Golem and ProGolem are schema independent with some
modifications. For query-based learning algorithms we show that the vertical
(de)composition transformations influence their learning efficiency. We support
the theoretical results with an empirical study that demonstrates the schema
dependence/independence of several algorithms on existing benchmark data sets
under natural vertical (de)compositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03854</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03854</id><created>2015-08-16</created><authors><author><keyname>Rei</keyname><forenames>Marek</forenames></author></authors><title>Online Representation Learning in Recurrent Neural Language Models</title><categories>cs.CL cs.LG cs.NE</categories><comments>In Proceedings of EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate an extension of continuous online learning in recurrent neural
network language models. The model keeps a separate vector representation of
the current unit of text being processed and adaptively adjusts it after each
prediction. The initial experiments give promising results, indicating that the
method is able to increase language modelling accuracy, while also decreasing
the parameters needed to store the model along with the computation required at
each step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03856</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03856</id><created>2015-08-16</created><authors><author><keyname>Sarwar</keyname><forenames>Sheikh Muhammad</forenames></author><author><keyname>Hasan</keyname><forenames>Mahamudul</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author></authors><title>Two-stage Cascaded Classifier for Purchase Prediction</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe our machine learning solution for the RecSys
Challenge, 2015. We have proposed a time efficient two-stage cascaded
classifier for the prediction of buy sessions and purchased items within such
sessions. Based on the model, several interesting features found, and formation
of our own test bed, we have achieved a reasonable score. Usage of Random
Forests helps us to cope with the effect of the multiplicity of good models
depending on varying subsets of features in the purchased items prediction and,
in its turn, boosting is used as a suitable technique to overcome severe class
imbalance of the buy-session prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03859</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03859</id><created>2015-08-16</created><authors><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author><author><keyname>Newport</keyname><forenames>Calvin</forenames></author></authors><title>The Computational Power of Beeps</title><categories>cs.DC cs.DS</categories><comments>Extended abstract to appear in the Proceedings of the International
  Symposium on Distributed Computing (DISC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the quantity of computational resources (state
machine states and/or probabilistic transition precision) needed to solve
specific problems in a single hop network where nodes communicate using only
beeps. We begin by focusing on randomized leader election. We prove a lower
bound on the states required to solve this problem with a given error bound,
probability precision, and (when relevant) network size lower bound. We then
show the bound tight with a matching upper bound. Noting that our optimal upper
bound is slow, we describe two faster algorithms that trade some state
optimality to gain efficiency. We then turn our attention to more general
classes of problems by proving that once you have enough states to solve leader
election with a given error bound, you have (within constant factors) enough
states to simulate correctly, with this same error bound, a logspace TM with a
constant number of unary input tapes: allowing you to solve a large and
expressive set of problems. These results identify a key simplicity threshold
beyond which useful distributed computation is possible in the beeping model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03863</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03863</id><created>2015-08-16</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Discrete Route/Trajectory Decision Making Problems</title><categories>cs.AI cs.SY math.OC</categories><comments>25 pages, 34 figures, 16 tables</comments><msc-class>68T20, 93A13, 93B51, 90B50</msc-class><acm-class>I.2.8; J.6; K.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on composite multistage decision making problems which are
targeted to design a route/trajectory from an initial decision situation
(origin) to goal (destination) decision situation(s). Automobile routing
problem is considered as a basic physical metaphor. The problems are based on a
discrete (combinatorial) operations/states design/solving space (e.g.,
digraph). The described types of discrete decision making problems can be
considered as intelligent design of a route (trajectory, strategy) and can be
used in many domains: (a) education (planning of student educational
trajectory), (b) medicine (medical treatment), (c) economics (trajectory of
start-up development). Several types of the route decision making problems are
described: (i) basic route decision making, (ii) multi-goal route decision
making, (iii) multi-route decision making, (iv) multi-route decision making
with route/trajectory change(s), (v) composite multi-route decision making
(solution is a composition of several routes/trajectories at several
corresponding domains), and (vi) composite multi-route decision making with
coordinated routes/trajectories. In addition, problems of modeling and building
the design spaces are considered. Numerical examples illustrate the suggested
approach. Three applications are considered: educational trajectory
(orienteering problem), plan of start-up company (modular three-stage design),
and plan of medical treatment (planning over digraph with two-component
vertices).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03865</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03865</id><created>2015-08-16</created><authors><author><keyname>Meier</keyname><forenames>Yannick</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Atan</keyname><forenames>Onur</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Predicting Grades</title><categories>cs.LG</categories><comments>13 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To increase efficacy in traditional classroom courses as well as in Massive
Open Online Courses (MOOCs), automated systems supporting the instructor are
needed. One important problem is to automatically detect students that are
going to do poorly in a course early enough to be able to take remedial
actions. Existing grade prediction systems focus on maximizing the accuracy of
the prediction while overseeing the importance of issuing timely and
personalized predictions. This paper proposes an algorithm that predicts the
final grade of each student in a class. It issues a prediction for each student
individually, when the expected accuracy of the prediction is sufficient. The
algorithm learns online what is the optimal prediction and time to issue a
prediction based on past history of students' performance in a course. We
derive a confidence estimate for the prediction accuracy and demonstrate the
performance of our algorithm on a dataset obtained based on the performance of
approximately 700 UCLA undergraduate students who have taken an introductory
digital signal processing over the past 7 years. We demonstrate that for 85% of
the students we can predict with 76% accuracy whether they are going do well or
poorly in the class after the 4th course week. Using data obtained from a pilot
course, our methodology suggests that it is effective to perform early in-class
assessments such as quizzes, which result in timely performance prediction for
each student, thereby enabling timely interventions by the instructor (at the
student or class level) when necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03868</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03868</id><created>2015-08-16</created><updated>2015-10-07</updated><authors><author><keyname>Jou</keyname><forenames>Brendan</forenames></author><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Redi</keyname><forenames>Miriam</forenames></author><author><keyname>Topkara</keyname><forenames>Mercan</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Visual Affect Around the World: A Large-scale Multilingual Visual
  Sentiment Ontology</title><categories>cs.MM cs.CL cs.CV cs.IR</categories><comments>11 pages, to appear at ACM MM'15</comments><acm-class>H.1.2; H.5.1; H.5.4; I.2.10</acm-class><doi>10.1145/2733373.2806246</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every culture and language is unique. Our work expressly focuses on the
uniqueness of culture and language in relation to human affect, specifically
sentiment and emotion semantics, and how they manifest in social multimedia. We
develop sets of sentiment- and emotion-polarized visual concepts by adapting
semantic structures called adjective-noun pairs, originally introduced by Borth
et al. (2013), but in a multilingual context. We propose a new
language-dependent method for automatic discovery of these adjective-noun
constructs. We show how this pipeline can be applied on a social multimedia
platform for the creation of a large-scale multilingual visual sentiment
concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our
unified ontology is organized hierarchically by multilingual clusters of
visually detectable nouns and subclusters of emotionally biased versions of
these nouns. In addition, we present an image-based prediction task to show how
generalizable language-specific models are in a multilingual context. A new,
publicly available dataset of &gt;15.6K sentiment-biased visual concepts across 12
languages with language-specific detector banks, &gt;7.36M images and their
metadata is also released.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03871</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03871</id><created>2015-08-16</created><updated>2015-10-05</updated><authors><author><keyname>Heidarzadeh</keyname><forenames>Anoosheh</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>Cooperative Data Exchange with Unreliable Clients</title><categories>cs.IT math.IT</categories><comments>8 pages; in Proc. 53rd Annual Allerton Conference on Communication,
  Control, and Computing (Allerton 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of clients in a broadcast network, each of which holds a
subset of packets in the ground set X. In the (coded) cooperative data exchange
problem, the clients need to recover all packets in X by exchanging coded
packets over a lossless broadcast channel. Several previous works analyzed this
problem under the assumption that each client initially holds a random subset
of packets in X. In this paper we consider a generalization of this problem for
settings in which an unknown (but of a certain size) subset of clients are
unreliable and their packet transmissions are subject to arbitrary erasures.
For the special case of one unreliable client, we derive a closed-form
expression for the minimum number of transmissions required for each reliable
client to obtain all packets held by other reliable clients (with probability
approaching 1 as the number of packets tends to infinity). Furthermore, for the
cases with more than one unreliable client, we provide an approximation
solution in which the number of transmissions per packet is within an
arbitrarily small additive factor from the value of the optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03878</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03878</id><created>2015-08-16</created><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>A Pessimistic Approximation for the Fisher Information Measure</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem how to determine the intrinsic quality of a signal processing
system with respect to the inference of an unknown deterministic parameter
$\theta$ is considered. While Fisher's information measure $F(\theta)$ forms a
classical analytical tool for such a problem, direct computation of the
information measure can become difficult in certain situations. This in
particular forms an obstacle for the estimation theoretic performance analysis
of non-linear measurement systems, where the form of the conditional output
probability function can make calculation of the information measure
$F(\theta)$ difficult. Based on the Cauchy-Schwarz inequality, we establish an
alternative information measure $S(\theta)$. It forms a pessimistic
approximation to the Fisher information $F(\theta)$ and has the property that
it can be evaluated with the first four output moments at hand. These entities
usually exhibit good mathematical tractability or can be determined at
low-complexity by output measurements in a calibrated setup or via numerical
simulations. With various examples we show that $S(\theta)$ provides a good
conservative approximation for $F(\theta)$ and outline different estimation
theoretic problems where the presented information bound turns out to be
useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03881</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03881</id><created>2015-08-16</created><updated>2015-11-24</updated><authors><author><keyname>Xia</keyname><forenames>Fangting</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Pose-Guided Human Parsing with Deep Learned Features</title><categories>cs.CV</categories><comments>12 pages, 10 figures, a shortened version of this paper was accepted
  by AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parsing human body into semantic regions is crucial to human-centric
analysis. In this paper, we propose a segment-based parsing pipeline that
explores human pose information, i.e. the joint location of a human model,
which improves the part proposal, accelerates the inference and regularizes the
parsing process at the same time. Specifically, we first generate part segment
proposals with respect to human joints predicted by a deep model, then part-
specific ranking models are trained for segment selection using both pose-based
features and deep-learned part potential features. Finally, the best ensemble
of the proposed part segments are inferred though an And-Or Graph.
  We evaluate our approach on the popular Penn-Fudan pedestrian parsing
dataset, and demonstrate the effectiveness of using the pose information for
each stage of the parsing pipeline. Finally, we show that our approach yields
superior part segmentation accuracy comparing to the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03882</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03882</id><created>2015-08-16</created><authors><author><keyname>Rasheed</keyname><forenames>Muhibur</forenames></author><author><keyname>Clement</keyname><forenames>Nathan</forenames></author><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Bajaj</keyname><forenames>Chandrajit</forenames></author></authors><title>Quantifying and Visualizing Uncertainties in Molecular Models</title><categories>cs.CE</categories><comments>19 figures, 30 pages</comments><msc-class>62, 65, 68</msc-class><acm-class>F.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational molecular modeling and visualization has seen significant
progress in recent years with sev- eral molecular modeling and visualization
software systems in use today. Nevertheless the molecular biology community
lacks techniques and tools for the rigorous analysis, quantification and
visualization of the associated errors in molecular structure and its
associated properties. This paper attempts at filling this vacuum with the
introduction of a systematic statistical framework where each source of
structural uncertainty is modeled as a ran- dom variable (RV) with a known
distribution, and properties of the molecules are defined as dependent RVs. The
framework consists of a theoretical basis, and an empirical implementation
where the uncertainty quantification (UQ) analysis is achieved by using
Chernoff-like bounds. The framework enables additionally the propagation of
input structural data uncertainties, which in the molecular protein world are
described as B-factors, saved with almost all X-ray models deposited in the
Protein Data Bank (PDB). Our statistical framework is also able and has been
applied to quantify and visualize the uncertainties in molecular properties,
namely solvation interfaces and solvation free energy estimates. For each of
these quantities of interest (QOI) of the molecular models we provide several
novel and intuitive visualizations of the input, intermediate, and final
propagated uncertainties. These methods should enable the end user achieve a
more quantitative and visual evaluation of various molecular PDB models for
structural and property correctness, or the lack thereof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03890</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03890</id><created>2015-08-16</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author></authors><title>On connectivity in a general random intersection graph</title><categories>cs.DM cs.SI math.PR physics.soc-ph</categories><comments>Conference version of a full paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been growing interest in studies of general random intersection
graphs. In this paper, we consider a general random intersection graph
$\mathbb{G}(n,\overrightarrow{a}, \overrightarrow{K_n},P_n)$ defined on a set
$\mathcal{V}_n$ comprising $n$ vertices, where $\overrightarrow{a}$ is a
probability vector $(a_1,a_2,\ldots,a_m)$ and $\overrightarrow{K_n}$ is
$(K_{1,n},K_{2,n},\ldots,K_{m,n})$. This graph has been studied in the
literature including a most recent work by Ya\u{g}an [arXiv:1508.02407].
Suppose there is a pool $\mathcal{P}_n$ consisting of $P_n$ distinct objects.
The $n$ vertices in $\mathcal{V}_n$ are divided into $m$ groups $\mathcal{A}_1,
\mathcal{A}_2, \ldots, \mathcal{A}_m$. Each vertex $v$ is independently
assigned to exactly a group according to the probability distribution with
$\mathbb{P}[v \in \mathcal{A}_i]= a_i$, where $i=1,2,\ldots,m$. Afterwards,
each vertex in group $\mathcal{A}_i$ independently chooses $K_{i,n}$ objects
uniformly at random from the object pool $\mathcal{P}_n$. Finally, an
undirected edge is drawn between two vertices in $\mathcal{V}_n$ that share at
least one object. This graph model $\mathbb{G}(n,\overrightarrow{a},
\overrightarrow{K_n},P_n)$ has applications in secure sensor networks and
social networks. We investigate connectivity in this general random
intersection graph $\mathbb{G}(n,\overrightarrow{a}, \overrightarrow{K_n},P_n)$
and present a sharp zero-one law. Our result is also compared with the zero-one
law established by Ya\u{g}an.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03891</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03891</id><created>2015-08-16</created><authors><author><keyname>Sridharan</keyname><forenames>Mohan</forenames></author><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author><author><keyname>Zhang</keyname><forenames>Shiqi</forenames></author><author><keyname>Wyatt</keyname><forenames>Jeremy</forenames></author></authors><title>A Refinement-Based Architecture for Knowledge Representation and
  Reasoning in Robotics</title><categories>cs.RO cs.AI cs.LO</categories><comments>27 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an architecture that combines the complementary
strengths of probabilistic graphical models and declarative programming to
enable robots to represent and reason with qualitative and quantitative
descriptions of uncertainty and domain knowledge. The architecture comprises
three components: a controller, a logician and an executor. The logician uses a
coarse-resolution, qualitative model of the world and logical reasoning to plan
a sequence of abstract actions for an assigned goal. For each abstract action
to be executed, the executor uses probabilistic information at finer
granularity and probabilistic algorithms to execute a sequence of concrete
actions, and to report observations of the environment. The controller
dispatches goals and relevant information to the logician and executor,
collects and processes information from these components, refines the
coarse-resolution description to obtain the probabilistic information at finer
granularity for each abstract action, and computes the policy for executing the
abstract action using probabilistic algorithms. The architecture is evaluated
in simulation and on a mobile robot moving objects in an indoor domain, to show
that it supports reasoning with violation of defaults, noisy observations and
unreliable actions, in complex domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03892</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03892</id><created>2015-08-16</created><authors><author><keyname>Chaudhari</keyname><forenames>Dipak L.</forenames></author><author><keyname>Damani</keyname><forenames>Om</forenames></author></authors><title>Building an IDE for the Calculational Derivation of Imperative Programs</title><categories>cs.PL cs.LO</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 1-13</journal-ref><doi>10.4204/EPTCS.187.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an IDE called CAPS (Calculational Assistant for
Programming from Specifications) for the interactive, calculational derivation
of imperative programs. In building CAPS, our aim has been to make the IDE
accessible to non-experts while retaining the overall flavor of the
pen-and-paper calculational style. We discuss the overall architecture of the
CAPS system, the main features of the IDE, the GUI design, and the trade-offs
involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03893</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03893</id><created>2015-08-16</created><authors><author><keyname>Couto</keyname><forenames>Lu&#xed;s Diogo</forenames><affiliation>Aarhus University</affiliation></author><author><keyname>Larsen</keyname><forenames>Peter Gorm</forenames><affiliation>Aarhus University</affiliation></author><author><keyname>Hasanagi&#x107;</keyname><forenames>Miran</forenames><affiliation>Aarhus University</affiliation></author><author><keyname>Kanakis</keyname><forenames>Georgios</forenames><affiliation>Aarhus University</affiliation></author><author><keyname>Lausdahl</keyname><forenames>Kenneth</forenames><affiliation>Aarhus University</affiliation></author><author><keyname>Tran-J&#xf8;rgensen</keyname><forenames>Peter W. V.</forenames><affiliation>Aarhus University</affiliation></author></authors><title>Towards Enabling Overture as a Platform for Formal Notation IDEs</title><categories>cs.SE</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 14-27</journal-ref><doi>10.4204/EPTCS.187.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal Methods tools will never have as many users as tools for popular
programming languages and so the effort spent on constructing Integrated
Development Environments (IDEs) will be orders of magnitudes lower than that of
programming languages such as Java. This means newcomers to formal methods do
not get the same user experience as with their favourite programming IDE. In
order to improve this situation it is essential that efforts are combined so it
is possible to reuse common features and thus not start from scratch every
time. This paper presents the Overture platform where such a reuse philosophy
is present. We give an overview of the platform itself as well as the
extensibility principles that enable much of the reuse. The paper also contains
several examples platform extensions, both in the form of new features and a
new IDE supporting a new language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03894</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03894</id><created>2015-08-16</created><authors><author><keyname>Dordowsky</keyname><forenames>Frank</forenames><affiliation>ESG Elektroniksystem- und Logistik GmbH</affiliation></author></authors><title>An experimental Study using ACSL and Frama-C to formulate and verify
  Low-Level Requirements from a DO-178C compliant Avionics Project</title><categories>cs.SE cs.LO</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 28-41</journal-ref><doi>10.4204/EPTCS.187.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safety critical avionics software is a natural application area for formal
verification. This is reflected in the formal method's inclusion into the
certification guideline DO-178C and its formal methods supplement DO-333.
Airbus and Dassault-Aviation, for example, have conducted studies in using
formal verification. A large German national research project, Verisoft XT,
also examined the application of formal methods in the avionics domain.
  However, formal methods are not yet mainstream, and it is questionable if
formal verification, especially formal deduction, can be integrated into the
software development processes of a resource constrained small or medium
enterprise (SME). ESG, a Munich based medium sized company, has conducted a
small experimental study on the application of formal verification on a small
portion of a real avionics project. The low level specification of a software
function was formalized with ACSL, and the corresponding source code was
partially verified using Frama-C and the WP plugin, with Alt-Ergo as automated
prover.
  We established a couple of criteria which a method should meet to be fit for
purpose for industrial use in SME, and evaluated these criteria with the
experience gathered by using ACSL with Frama-C on a real world example. The
paper reports on the results of this study but also highlights some issues
regarding the method in general which, in our view, will typically arise when
using the method in the domain of embedded real-time programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03895</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03895</id><created>2015-08-16</created><authors><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Poskitt</keyname><forenames>Christopher M.</forenames></author><author><keyname>Tschannen</keyname><forenames>Julian</forenames></author></authors><title>The AutoProof Verifier: Usability by Non-Experts and on Standard Code</title><categories>cs.SE cs.HC cs.LO</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 42-55</journal-ref><doi>10.4204/EPTCS.187.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal verification tools are often developed by experts for experts; as a
result, their usability by programmers with little formal methods experience
may be severely limited. In this paper, we discuss this general phenomenon with
reference to AutoProof: a tool that can verify the full functional correctness
of object-oriented software. In particular, we present our experiences of using
AutoProof in two contrasting contexts representative of non-expert usage.
First, we discuss its usability by students in a graduate course on software
verification, who were tasked with verifying implementations of various sorting
algorithms. Second, we evaluate its usability in verifying code developed for
programming assignments of an undergraduate course. The first scenario
represents usability by serious non-experts; the second represents usability on
&quot;standard code&quot;, developed without full functional verification in mind. We
report our experiences and lessons learnt, from which we derive some general
suggestions for furthering the development of verification tools with respect
to improving their usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03896</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03896</id><created>2015-08-16</created><authors><author><keyname>Kabbani</keyname><forenames>Nabil M.</forenames><affiliation>Clemson University</affiliation></author><author><keyname>Welch</keyname><forenames>Daniel</forenames><affiliation>Clemson University</affiliation></author><author><keyname>Priester</keyname><forenames>Caleb</forenames><affiliation>Clemson University</affiliation></author><author><keyname>Schaub</keyname><forenames>Stephen</forenames><affiliation>Clemson University</affiliation></author><author><keyname>Durkee</keyname><forenames>Blair</forenames><affiliation>Clemson University</affiliation></author><author><keyname>Sun</keyname><forenames>Yu-Shan</forenames><affiliation>Clemson University</affiliation></author><author><keyname>Sitaraman</keyname><forenames>Murali</forenames><affiliation>Clemson University</affiliation></author></authors><title>Formal Reasoning Using an Iterative Approach with an Integrated Web IDE</title><categories>cs.SE cs.HC cs.PL</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 56-71</journal-ref><doi>10.4204/EPTCS.187.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes our experience in communicating the elements of
reasoning about correctness, and the central role of formal specifications in
reasoning about modular, component-based software using a language and an
integrated Web IDE designed for the purpose. Our experience in using such an
IDE, supported by a 'push-button' verifying compiler in a classroom setting,
reveals the highly iterative process learners use to arrive at suitably
specified, automatically provable code. We explain how the IDE facilitates
reasoning at each step of this process by providing human readable verification
conditions (VCs) and feedback from an integrated prover that clearly indicates
unprovable VCs to help identify obstacles to completing proofs. The paper
discusses the IDE's usage in verified software development using several
examples drawn from actual classroom lectures and student assignments to
illustrate principles of design-by-contract and the iterative process of
creating and subsequently refining assertions, such as loop invariants in
object-based code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03897</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03897</id><created>2015-08-16</created><authors><author><keyname>Nokovic</keyname><forenames>Bojan</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Sekerinski</keyname><forenames>Emil</forenames><affiliation>McMaster University</affiliation></author></authors><title>A Holistic Approach in Embedded System Development</title><categories>cs.SE</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 72-85</journal-ref><doi>10.4204/EPTCS.187.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present pState, a tool for developing &quot;complex&quot; embedded systems by
integrating validation into the design process. The goal is to reduce
validation time. To this end, qualitative and quantitative properties are
specified in system models expressed as pCharts, an extended version of
hierarchical state machines. These properties are specified in an intuitive way
such that they can be written by engineers who are domain experts, without
needing to be familiar with temporal logic. From the system model, executable
code that preserves the verified properties is generated. The design is
documented on the model and the documentation is passed as comments into the
generated code. On the series of examples we illustrate how models and
properties are specified using pState.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03898</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03898</id><created>2015-08-16</created><authors><author><keyname>Signoles</keyname><forenames>Julien</forenames><affiliation>CEA LIST, Software Security Lab</affiliation></author></authors><title>Software Architecture of Code Analysis Frameworks Matters: The Frama-C
  Example</title><categories>cs.SE</categories><comments>In Proceedings F-IDE 2015, arXiv:1508.03388</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 187, 2015, pp. 86-96</journal-ref><doi>10.4204/EPTCS.187.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing large software, as software analyzers which aim to be used in
industrial settings, requires a well-engineered software architecture in order
to ease its daily development and its maintenance process during its lifecycle.
If the analyzer is not only a single tool, but an open extensible collaborative
framework in which external developers may develop plug-ins collaborating with
each other, such a well designed architecture even becomes more important.
  In this experience report, we explain difficulties of developing and
maintaining open extensible collaborative analysis frameworks, through the
example of Frama-C, a platform dedicated to the analysis of code written in C.
We also present the new upcoming software architecture of Frama-C and how it
aims to solve some of these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03901</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03901</id><created>2015-08-16</created><authors><author><keyname>Francalanza</keyname><forenames>Adrian</forenames><affiliation>CS, ICT, University of Malta</affiliation></author><author><keyname>Giunti</keyname><forenames>Marco</forenames><affiliation>RELEASE, DI, Universidade da Beira Interior &amp; NOVA LINCS, DI-FCT, Universidade NOVA de Lisboa</affiliation></author><author><keyname>Ravara</keyname><forenames>Ant&#xf3;nio</forenames><affiliation>NOVA LINCS, DI-FCT, Universidade NOVA de Lisboa</affiliation></author></authors><title>Unlocking Blocked Communicating Processes</title><categories>cs.PL cs.LO</categories><comments>In Proceedings WWV 2015, arXiv:1508.03389</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 188, 2015, pp. 23-32</journal-ref><doi>10.4204/EPTCS.188.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of disentangling locked processes via code refactoring.
We identify and characterise a class of processes that is not lock-free; then
we formalise an algorithm that statically detects potential locks and propose
refactoring procedures that disentangle detected locks. Our development is cast
within a simple setting of a finite linear CCS variant &#xe2;&#x80;&#x94; although it suffices
to illustrate the main concepts, we also discuss how our work extends to other
language extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03902</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03902</id><created>2015-08-16</created><authors><author><keyname>Hoang</keyname><forenames>Van Tien</forenames><affiliation>IMT Lucca, Italy</affiliation></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames><affiliation>IIT-CNR, Pisa, Italy</affiliation></author><author><keyname>Tiezzi</keyname><forenames>Francesco</forenames><affiliation>University of Camerino, Italy</affiliation></author><author><keyname>Petrocchi</keyname><forenames>Marinella</forenames><affiliation>IIT-CNR, Pisa, Italy</affiliation></author><author><keyname>De Nicola</keyname><forenames>Rocco</forenames><affiliation>IMT Lucca, Italy</affiliation></author></authors><title>Domain-specific queries and Web search personalization: some
  investigations</title><categories>cs.IR</categories><comments>In Proceedings WWV 2015, arXiv:1508.03389</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 188, 2015, pp. 51-58</journal-ref><doi>10.4204/EPTCS.188.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Major search engines deploy personalized Web results to enhance users'
experience, by showing them data supposed to be relevant to their interests.
Even if this process may bring benefits to users while browsing, it also raises
concerns on the selection of the search results. In particular, users may be
unknowingly trapped by search engines in protective information bubbles, called
&quot;filter bubbles&quot;, which can have the undesired effect of separating users from
information that does not fit their preferences. This paper moves from early
results on quantification of personalization over Google search query results.
Inspired by previous works, we have carried out some experiments consisting of
search queries performed by a battery of Google accounts with differently
prepared profiles. Matching query results, we quantify the level of
personalization, according to topics of the queries and the profile of the
accounts. This work reports initial results and it is a first step a for more
extensive investigation to measure Web search personalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03903</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03903</id><created>2015-08-16</created><authors><author><keyname>Margheri</keyname><forenames>Andrea</forenames><affiliation>Universit&#xe0; degli Studi di Firenze, Universit&#xe0; di Pisa</affiliation></author><author><keyname>Pugliese</keyname><forenames>Rosario</forenames><affiliation>Universit&#xe0; degli Studi di Firenze</affiliation></author><author><keyname>Tiezzi</keyname><forenames>Francesco</forenames><affiliation>Universit&#xe0; di Camerino</affiliation></author></authors><title>On Properties of Policy-Based Specifications</title><categories>cs.SE cs.CR</categories><comments>In Proceedings WWV 2015, arXiv:1508.03389</comments><proxy>EPTCS</proxy><acm-class>D.2.1;D.4.6</acm-class><journal-ref>EPTCS 188, 2015, pp. 33-50</journal-ref><doi>10.4204/EPTCS.188.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of large-scale, complex computing systems has dramatically
increased the difficulties of securing accesses to systems' resources. To
ensure confidentiality and integrity, the exploitation of access control
mechanisms has thus become a crucial issue in the design of modern computing
systems. Among the different access control approaches proposed in the last
decades, the policy-based one permits to capture, by resorting to the concept
of attribute, all systems' security-relevant information and to be, at the same
time, sufficiently flexible and expressive to represent the other approaches.
In this paper, we move a step further to understand the effectiveness of
policy-based specifications by studying how they permit to enforce traditional
security properties. To support system designers in developing and maintaining
policy-based specifications, we formalise also some relevant properties
regarding the structure of policies. By means of a case study from the banking
domain, we present real instances of such properties and outline an approach
towards their automatised verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03905</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03905</id><created>2015-08-16</created><authors><author><keyname>Guo</keyname><forenames>Hai-Feng</forenames><affiliation>University of Nebraska at Omaha</affiliation></author><author><keyname>Ouyang</keyname><forenames>Qing</forenames><affiliation>University of Nebraska at Omaha</affiliation></author><author><keyname>Siy</keyname><forenames>Harvey</forenames><affiliation>University of Nebraska at Omaha</affiliation></author></authors><title>Semantics-based Automated Web Testing</title><categories>cs.SE cs.PL</categories><comments>In Proceedings WWV 2015, arXiv:1508.03389</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 188, 2015, pp. 59-74</journal-ref><doi>10.4204/EPTCS.188.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present TAO, a software testing tool performing automated test and oracle
generation based on a semantic approach. TAO entangles grammar-based test
generation with automated semantics evaluation using a denotational semantics
framework. We show how TAO can be incorporated with the Selenium automation
tool for automated web testing, and how TAO can be further extended to support
automated delta debugging, where a failing web test script can be
systematically reduced based on grammar-directed strategies. A real-life
parking website is adopted throughout the paper to demonstrate the effectivity
of our semantics-based web testing approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03906</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03906</id><created>2015-08-16</created><authors><author><keyname>Bacciu</keyname><forenames>Davide</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa</affiliation></author><author><keyname>Gnesi</keyname><forenames>Stefania</forenames><affiliation>Istituto di Scienza e Tecnologie dell'Informazione, CNR</affiliation></author><author><keyname>Semini</keyname><forenames>Laura</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa</affiliation></author></authors><title>Using a Machine Learning Approach to Implement and Evaluate Product Line
  Features</title><categories>cs.SE cs.LG</categories><comments>In Proceedings WWV 2015, arXiv:1508.03389</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 188, 2015, pp. 75-83</journal-ref><doi>10.4204/EPTCS.188.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bike-sharing systems are a means of smart transportation in urban
environments with the benefit of a positive impact on urban mobility. In this
paper we are interested in studying and modeling the behavior of features that
permit the end user to access, with her/his web browser, the status of the
Bike-Sharing system. In particular, we address features able to make a
prediction on the system state. We propose to use a machine learning approach
to analyze usage patterns and learn computational models of such features from
logs of system usage.
  On the one hand, machine learning methodologies provide a powerful and
general means to implement a wide choice of predictive features. On the other
hand, trained machine learning models are provided with a measure of predictive
performance that can be used as a metric to assess the cost-performance
trade-off of the feature. This provides a principled way to assess the runtime
behavior of different components before putting them into operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03908</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03908</id><created>2015-08-16</created><authors><author><keyname>Gul</keyname><forenames>Nosheen</forenames><affiliation>University of Leicester, England</affiliation></author></authors><title>A Calculus of Mobility and Communication for Ubiquitous Computing</title><categories>cs.DC cs.LO</categories><comments>In Proceedings WWV 2015, arXiv:1508.03389</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 188, 2015, pp. 6-22</journal-ref><doi>10.4204/EPTCS.188.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Calculus of Mobility and Communication (CMC) for the modelling
of mobility, communication and context-awareness in the setting of ubiquitous
computing. CMC is an ambient calculus with the in and out capabilities of
Cardelli and Gordon's Mobile Ambients. The calculus has a new form of global
communication similar to that in Milner's CCS. In CMC an ambient is tagged with
a set of ports that agents executing inside the ambient are allowed to
communicate on. It also has a new context-awareness feature that allows
ambients to query their location. We present reduction semantics and labelled
transition system semantics of CMC and prove that the semantics coincide. A new
notion of behavioural equivalence is given by defining capability barbed
bisimulation and congruence which is proved to coincide with barbed
bisimulation congruence. The expressiveness of the calculus is illustrated by
two case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03928</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03928</id><created>2015-08-17</created><authors><author><keyname>Li</keyname><forenames>Hongyang</forenames></author><author><keyname>Lu</keyname><forenames>Huchuan</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Price</keyname><forenames>Brian</forenames></author></authors><title>LCNN: Low-level Feature Embedded CNN for Salient Object Detection</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose a novel deep neural network framework embedded with
low-level features (LCNN) for salient object detection in complex images. We
utilise the advantage of convolutional neural networks to automatically learn
the high-level features that capture the structured information and semantic
context in the image. In order to better adapt a CNN model into the saliency
task, we redesign the network architecture based on the small-scale datasets.
Several low-level features are extracted, which can effectively capture
contrast and spatial information in the salient regions, and incorporated to
compensate with the learned high-level features at the output of the last fully
connected layer. The concatenated feature vector is further fed into a
hinge-loss SVM detector in a joint discriminative learning manner and the final
saliency score of each region within the bounding box is obtained by the linear
combination of the detector's weights. Experiments on three challenging
benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be
effective and superior than most low-level oriented state-of-the-arts in terms
of P-R curves, F-measure and mean absolute errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03929</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03929</id><created>2015-08-17</created><updated>2016-03-04</updated><authors><author><keyname>Kheradpisheh</keyname><forenames>Saeed Reza</forenames></author><author><keyname>Ghodrati</keyname><forenames>Masoud</forenames></author><author><keyname>Ganjtabesh</keyname><forenames>Mohammad</forenames></author><author><keyname>Masquelier</keyname><forenames>Timoth&#xe9;e</forenames></author></authors><title>Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object
  Recognition</title><categories>cs.CV q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (DCNNs) have attracted much attention
recently, and have shown to be able to recognize thousands of object categories
in natural image databases. Their architecture is somewhat similar to that of
the human visual system: both use restricted receptive fields, and a hierarchy
of layers which progressively extract more and more abstracted features. Yet it
is unknown whether DCNNs match human performance at the task of view-invariant
object recognition, whether they make similar errors and use similar
representations for this task, and whether the answers depend on the magnitude
of the viewpoint variations. To investigate these issues, we benchmarked eight
state-of-the-art DCNNs, the HMAX model, and a baseline shallow model and
compared their results to those of humans with backward masking. Unlike in all
previous DCNN studies, we carefully controlled the magnitude of the viewpoint
variations to demonstrate that shallow nets can outperform deep nets and humans
when variations are weak. When facing larger variations, however, more layers
were needed to match human performance and error distributions, and to have
representations that are consistent with human behavior. A very deep net with
18 layers even outperformed humans at the highest variation level, using the
most human-like representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03931</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03931</id><created>2015-08-17</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Johnson</keyname><forenames>Timothy</forenames></author><author><keyname>Torres</keyname><forenames>Manuel</forenames></author></authors><title>Knuthian Drawings of Series-Parallel Flowcharts</title><categories>cs.CG cs.DS</categories><comments>Full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a classic paper by Knuth, we revisit the problem of drawing
flowcharts of loop-free algorithms, that is, degree-three series-parallel
digraphs. Our drawing algorithms show that it is possible to produce Knuthian
drawings of degree-three series-parallel digraphs with good aspect ratios and
small numbers of edge bends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03938</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03938</id><created>2015-08-17</created><authors><author><keyname>Townsend</keyname><forenames>Samuel</forenames></author><author><keyname>Larsen</keyname><forenames>Mark E.</forenames></author><author><keyname>Boonstra</keyname><forenames>Tjeerd W.</forenames></author><author><keyname>Christensen</keyname><forenames>Helen</forenames></author></authors><title>Using Bluetooth Low Energy in smartphones to map social networks</title><categories>cs.SI cs.CY</categories><comments>6 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have an important role in an individual's health, with the
propagation of health-related features through a network, and correlations
between network structures and symptomatology. Using Bluetooth-enabled
smartphones to measure social connectivity is an alternative to traditional
paper-based data collection; however studies employing this technology have
been restricted to limited sets of homogenous handsets. We investigated the
feasibility of using the Bluetooth Low Energy (BLE) protocol, present on users'
own smartphones, to measure social connectivity. A custom application was
designed for Android and iOS handsets. The app was configured to simultaneously
broadcast via BLE and perform periodic discovery scans for other nearby
devices. The app was installed on two Android handsets and two iOS handsets,
and each combination of devices was tested in the foreground, background and
locked states. Connectivity was successfully measured in all test cases, except
between two iOS devices when both were in a locked state with their screens
off. As smartphones are in a locked state for the majority of a day, this
severely limits the ability to measure social connectivity on users' own
smartphones. It is not currently feasible to use Bluetooth Low Energy to map
social networks, due to the inability of iOS devices to detect another iOS
device when both are in a locked state. While the technology was successfully
implemented on Android devices, this represents a smaller market share of
partially or fully compatible devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03940</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03940</id><created>2015-08-17</created><updated>2015-11-27</updated><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Mi</keyname><forenames>De</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Imran</keyname><forenames>Muhammad Ali</forenames></author><author><keyname>Shakir</keyname><forenames>Muhammad Zeeshan</forenames></author></authors><title>MmWave Massive MIMO Based Wireless Backhaul for 5G Ultra-Dense Network</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted by IEEE Wireless Communications
  Magazine. This paper is related to 5G, ultra-dense network (UDN), millimeter
  waves (mmWave) fronthaul/backhaul, massive MIMO, sparsity/low-rank property
  of mmWave massive MIMO channels, sparse channel estimation, compressive
  sensing (CS), hybrid digital/analog precoding/combining, and hybrid
  beamforming. http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7306533</comments><journal-ref>IEEE Wireless Communications, vol. 22, no. 5, pp. 13-21, Oct. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra-dense network (UDN) has been considered as a promising candidate for
future 5G network to meet the explosive data demand. To realize UDN, a
reliable, Gigahertz bandwidth, and cost-effective backhaul connecting
ultra-dense small-cell base stations (BSs) and macro-cell BS is prerequisite.
Millimeter-wave (mmWave) can provide the potential Gbps traffic for wireless
backhaul. Moreover, mmWave can be easily integrated with massive MIMO for the
improved link reliability. In this article, we discuss the feasibility of
mmWave massive MIMO based wireless backhaul for 5G UDN, and the benefits and
challenges are also addressed. Especially, we propose a digitally-controlled
phase-shifter network (DPSN) based hybrid precoding/combining scheme for mmWave
massive MIMO, whereby the low-rank property of mmWave massive MIMO channel
matrix is leveraged to reduce the required cost and complexity of transceiver
with a negligible performance loss. One key feature of the proposed scheme is
that the macro-cell BS can simultaneously support multiple small-cell BSs with
multiple streams for each smallcell BS, which is essentially different from
conventional hybrid precoding/combining schemes typically limited to
single-user MIMO with multiple streams or multi-user MIMO with single stream
for each user. Based on the proposed scheme, we further explore the fundamental
issues of developing mmWave massive MIMO for wireless backhaul, and the
associated challenges, insight, and prospect to enable the mmWave massive MIMO
based wireless backhaul for 5G UDN are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03950</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03950</id><created>2015-08-17</created><updated>2016-01-12</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Stefaner</keyname><forenames>Moritz</forenames></author><author><keyname>Anegon</keyname><forenames>Felix de Moya</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>Excellence networks in science: A Web-based application based on
  Bayesian multilevel logistic regression (BMLR) for the identification of
  institutions collaborating successfully</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>accepted for publication in the Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we present an application which can be accessed via
www.excellence-networks.net and which represents networks of scientific
institutions worldwide. The application is based on papers (articles, reviews
and conference papers) published between 2007 and 2011. It uses (network) data,
on which the SCImago Institutions Ranking is based (Scopus data from Elsevier).
Using this data, institutional networks have been estimated with statistical
models (Bayesian multilevel logistic regression, BMLR) for a number of Scopus
subject areas. Within single subject areas, we have investigated and visualized
how successfully overall an institution (reference institution) has
collaborated (compared to all the other institutions in a subject area), and
with which other institutions (network institutions) a reference institution
has collaborated particularly successfully. The &quot;best paper rate&quot;
(statistically estimated) was used as an indicator for evaluating the
collaboration success of an institution. This gives the proportion of highly
cited papers from an institution, and is considered generally as an indicator
for measuring impact in bibliometrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03952</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03952</id><created>2015-08-17</created><updated>2016-02-23</updated><authors><author><keyname>Jamali</keyname><forenames>Mohammad Vahid</forenames></author><author><keyname>Salehi</keyname><forenames>Jawad A.</forenames></author></authors><title>Performance Studies of Underwater Wireless Optical Communication Systems
  with Spatial Diversity: MIMO Scheme</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analytically study the performance of multiple-input
multiple-output underwater wireless optical communication (MIMO-UWOC) systems
with on-off keying (OOK) modulation. To mitigate turbulence-induced fading,
which is amongst the major degrading effects of underwater channels on the
propagating optical signal, we use spatial diversity over UWOC links.
Furthermore, the effects of absorption and scattering are considered in our
analysis. We analytically obtain the exact and the upper bound bit error rate
(BER) expressions for both optimal and equal gain combining. In order to more
effectively calculate the system BER, we apply Gauss-Hermite quadrature formula
as well as approximation to the sum of log-normal random variables. We also
apply photon-counting method to evaluate the system BER in the presence of shot
noise. Our numerical results indicate an excellent match between the exact and
the upper bound BER curves. Also well matches between analytical results and
numerical simulations confirm the accuracy of our derived expressions.
Moreover, our results show that spatial diversity can considerably improve the
system performance, especially for more turbulent channels, e.g., a $3\times1$
MISO transmission in a $25$ m coastal water link with log-amplitude variance of
$0.16$ can introduce $8$ dB performance improvement at the BER of $10^{-9}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03953</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03953</id><created>2015-08-17</created><authors><author><keyname>Wang</keyname><forenames>Kang</forenames></author><author><keyname>Nguyen</keyname><forenames>Tam V.</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Sepulveda</keyname><forenames>Jose</forenames></author></authors><title>Sense Beyond Expressions: Cuteness</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of Internet culture, cuteness has become a popular
concept. Many people are curious about what factors making a person look cute.
However, there is rare research to answer this interesting question. In this
work, we construct a dataset of personal images with comprehensively annotated
cuteness scores and facial attributes to investigate this high-level concept in
depth. Based on this dataset, through an automatic attributes mining process,
we find several critical attributes determining the cuteness of a person. We
also develop a novel Continuous Latent Support Vector Machine (C-LSVM) method
to predict the cuteness score of one person given only his image. Extensive
evaluations validate the effectiveness of the proposed method for cuteness
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03954</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03954</id><created>2015-08-17</created><authors><author><keyname>Reps</keyname><forenames>Bram</forenames></author><author><keyname>Weinzierl</keyname><forenames>Tobias</forenames></author></authors><title>Complex additive geometric multilevel solvers for Helmholtz equations on
  spacetrees</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a family of implementations of low order, additive, geometric
multilevel solvers for systems of Helmholtz equations. Both grid spacing and
arithmetics may comprise complex numbers and we thus can apply complex scaling
techniques to the indefinite Helmholtz operator. Our implementations are based
upon the notion of a spacetree and work exclusively with a finite number of
precomputed local element matrices. They are globally matrix-free.
  Combining various relaxation factors with two grid transfer operators allows
us to switch from pure additive multigrid over a hierarchical basis method into
BPX with several multiscale smoothing variants within one code base. Pipelining
allows us to realise a full approximation storage (FAS) scheme within the
additive environment where, amortised, each grid vertex carrying degrees of
freedom is read/written only once per iteration. The codes thus realise a
single-touch policy. Among the features facilitated by matrix-free FAS is
arbitrary dynamic mesh refinement (AMR) for all solver variants. AMR as enabler
for full multigrid (FMG) cycling---the grid unfolds throughout the
computation---allows us to reduce the cost per unknown per order of accuracy.
  The present paper primary contributes towards software realisation and design
questions. Our experiments show that the consolidation of single-touch FAS,
dynamic AMR and vectorisation-friendly, complex scaled, matrix-free FMG cycles
delivers a mature implementation blueprint for solvers for a non-trivial class
of problems such as Helmholtz equations. Besides this validation, we put
particular emphasis on a strict implementation formalism as well as some
implementation correctness proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03959</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03959</id><created>2015-08-17</created><authors><author><keyname>Romeo</keyname><forenames>Ortega</forenames></author><author><keyname>Alexey</keyname><forenames>Bobtsov</forenames></author><author><keyname>Anton</keyname><forenames>Pyrkin</forenames></author><author><keyname>Stanislav</keyname><forenames>Aranovskiy</forenames></author></authors><title>A Parameter Estimation Approach to State Observation of Nonlinear
  Systems</title><categories>cs.SY</categories><comments>This paper is submitted to Systems &amp; Control Letters Journal. The
  abridged version of this paper will be presented at Conference on Descision
  and Control 2015, Osaka, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach to the problem of partial state estimation of nonlinear
systems is proposed. The main idea is to translate the state estimation problem
into one of estimation of constant, unknown parameters related to the systems
initial conditions. The class of systems for which the method is applicable is
identified via two assumptions related to the transformability of the system
into a suitable cascaded form and our ability to estimate the unknown
parameters. The first condition involves the solvability of a partial
differential equation while the second one requires some persistency of
excitation--like conditions. The proposed observer is shown to be applicable to
position estimation of a class of electromechanical systems, for the
reconstruction of the state of power converters and for speed observation of a
class of mechanical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03961</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03961</id><created>2015-08-17</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author><author><keyname>Su</keyname><forenames>Hongye</forenames></author></authors><title>Immersion and Invariance Stabilization of Nonlinear Systems: A
  Horizontal Contraction Approach</title><categories>cs.SY</categories><comments>5 pages, the 54th IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper is to propose an alternative procedure to
carry out one of the key steps of immersion and invariance stabilising
controller design. Namely, the one that ensures attractivity of the manifold
whose internal dynamics contains a copy of the desired system behaviour.
Towards this end we invoke contraction theory principles and ensure the
attractivity of the manifold rendering it horizontally contractive. The main
advantage of adopting this alternative approach is to make more systematic the
last step of the design with more explicit degrees of freedom to accomplish the
task. The classical case of systems in feedback form is used to illustrate the
proposed controller design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03965</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03965</id><created>2015-08-17</created><authors><author><keyname>Shaabani</keyname><forenames>Elham</forenames></author><author><keyname>Aleali</keyname><forenames>Ashkan</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Bertetto</keyname><forenames>John</forenames></author></authors><title>Early Identification of Violent Criminal Gang Members</title><categories>cs.SI physics.soc-ph</categories><comments>SIGKDD 2015</comments><acm-class>J.4</acm-class><doi>10.1145/2783258.2788618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gang violence is a major problem in the United States accounting for a large
fraction of homicides and other violent crime. In this paper, we study the
problem of early identification of violent gang members. Our approach relies on
modified centrality measures that take into account additional data of the
individuals in the social network of co-arrestees which together with other
arrest metadata provide a rich set of features for a classification algorithm.
We show our approach obtains high precision and recall (0.89 and 0.78
respectively) in the case where the entire network is known and out-performs
current approaches used by law-enforcement to the problem in the case where the
network is discovered overtime by virtue of new arrests - mimicking real-world
law-enforcement operations. Operational issues are also discussed as we are
preparing to leverage this method in an operational environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03975</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03975</id><created>2015-08-17</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author><author><keyname>Hong</keyname><forenames>Choonog Seon</forenames></author><author><keyname>Choi</keyname><forenames>Mi Jung</forenames></author></authors><title>Autonomic Resilient Internet-of-Things(IoT)Management</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Resilient IoT, the revenue of service provider is resilient to uncertain
usage-contexts(e.g. emotion, environmental contexts) of Smart-device users.
Hence, Autonomic Resilient IoT Management problem is decomposed into two
subproblems, namely m-connectivity and k-dominance, such that m-alternations on
revenue making process is resilient to users common interests, which might be
depicted through k-1 alternations of usage-contexts. In this context, a greedy
approximation scheme Bee is proposed, which resolves aforementioned
sub-problems with five consecutive models, namely Maverick, Siren, Pigmy, Arkeo
and Augeas, respectively. Theoretical analysis justifies the problem as
NP-hard, combinatorial optimization problem, which is amenable to greedy
approximation. Moreover, Bee lays out the theoretical foundation of Resilient
Fact-finding, followed by theoretical and experimental(i.e synthetic) proof,
which show how Bee-resilience resolves acute CDS measurement problem.
Accordingly, experiments on real Social rumor dataset extract dominator and
dominate to justify how Bee resilience improves CDS measurement. Finally,
case-study and prototype development are performed on Android and Web platforms
in a Resilient IoT scenario, where service provider recommends personalized
services for Smart-device users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03981</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03981</id><created>2015-08-17</created><authors><author><keyname>Kolchyna</keyname><forenames>Olga</forenames></author><author><keyname>Souza</keyname><forenames>Th'arsis T. P.</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author><author><keyname>Treleaven</keyname><forenames>Philip C.</forenames></author></authors><title>In Quest of Significance: Identifying Types of Twitter Sentiment Events
  that Predict Spikes in Sales</title><categories>cs.SI cs.CY cs.IR stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of Twitter events to predict consumer sales events by
analysing sales for 75 companies from the retail sector and over 150 million
tweets mentioning those companies along with their sentiment. We suggest an
approach for events identification on Twitter extending existing methodologies
of event study. We also propose a robust method for clustering Twitter events
into different types based on their shape, which captures the varying dynamics
of information propagation through the social network. We provide empirical
evidence that through events differentiation based on their shape we can
clearly identify types of Twitter events that have a more significant power to
predict spikes in sales than the aggregated Twitter signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03992</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03992</id><created>2015-08-17</created><authors><author><keyname>Twigg</keyname><forenames>Andrew</forenames></author><author><keyname>Xavier</keyname><forenames>Eduardo C.</forenames></author></authors><title>Locality-preserving allocations Problems and coloured Bin Packing</title><categories>cs.DS</categories><doi>10.1016/j.tcs.2015.06.036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following problem, introduced by Chung et al. in 2006. We are
given, online or offline, a set of coloured items of different sizes, and wish
to pack them into bins of equal size so that we use few bins in total (at most
$\alpha$ times optimal), and that the items of each colour span few bins (at
most $\beta$ times optimal). We call such allocations $(\alpha,
\beta)$-approximate. As usual in bin packing problems, we allow additive
constants and consider $(\alpha,\beta)$ as the asymptotic performance ratios.
We prove that for $\eps&gt;0$, if we desire small $\alpha$, no scheme can beat
$(1+\eps, \Omega(1/\eps))$-approximate allocations and similarly as we desire
small $\beta$, no scheme can beat $(1.69103, 1+\eps)$-approximate allocations.
We give offline schemes that come very close to achieving these lower bounds.
For the online case, we prove that no scheme can even achieve
$(O(1),O(1))$-approximate allocations. However, a small restriction on item
sizes permits a simple online scheme that computes $(2+\eps, 1.7)$-approximate
allocations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03993</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03993</id><created>2015-08-17</created><authors><author><keyname>Jensen</keyname><forenames>Kristian E.</forenames></author></authors><title>Simulating Viscous Fingering with a Timespace Method and Anisotropic
  Mesh Adaptation</title><categories>cs.CE math.NA</categories><comments>Rejected by Journal of Computational Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report findings related to a two dimensional viscous fingering problem
solved with a timespace method and anisotropic elements. Timespace methods have
attracted interest for solution of time dependent partial differential
equations due to the implications of parallelism in the temporal dimension, but
there are also attractive features in the context of anisotropic mesh
adaptation; not only are heuristics and interpolation errors avoided, but
slanted elements in timespace also correspond to long and accurate timesteps,
i.e. the anisotropy in timespace can be exploited. We show that our timespace
method is restricted by a minimum timestep size, which is due to the growth of
numerical perturbations. The lower bound on the timestep is, however, quite
high, which is indicative that the number of timesteps can be reduced with
several orders of magnitude for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.03996</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.03996</id><created>2015-08-17</created><updated>2015-09-30</updated><authors><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Hajiesmaili</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Cai</keyname><forenames>Sinan</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author><author><keyname>Zhu</keyname><forenames>Qi</forenames></author></authors><title>Peak-Aware Online Economic Dispatching for Microgrids</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By employing local renewable energy sources and power generation units while
connected to the central grid, microgrid can usher in great benefits in terms
of cost efficiency, power reliability, and environmental awareness. Economic
dispatching is a central problem in microgrid operation, which aims at
effectively scheduling various energy sources to minimize the operating cost
while satisfying the electricity demand. Designing intelligent economic
dispatching strategies for microgrids, however, is drastically different from
that for conventional central grids, due to two unique challenges. First, the
erratic renewable energy emphasizes the need for online algorithms. Second, the
widely-adopted peak-based pricing scheme brings out the need for new peak-aware
strategy design. In this paper, we tackle these critical challenges and devise
peak-aware online economic dispatching algorithms. For microgrids with
fast-responding generators, we prove that our deterministic and randomized
algorithms achieve the best possible competitive ratios $2-\beta$ and
$e/(e-1+\beta)$, respectively, where $\beta\in[0,1]$ is the ratio between the
minimum grid spot price and the local-generation price. Our results
characterize the fundamental \emph{price of uncertainty} of the problem. For
microgrids with slow-responding generators, we first show that a large
competitive ratio is inevitable. Then we leverage limited prediction of
electricity demand and renewable generation to improve the competitiveness of
the algorithms. By extensive empirical evaluations using real-world traces, we
show that our online algorithms achieve near offline-optimal performance. In a
representative scenario, our algorithm achieves $17.5\%$ and $9.24\%$ cost
reduction as compared to the case without local generation units and the case
using peak-oblivious algorithms, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04006</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04006</id><created>2015-08-17</created><authors><author><keyname>Gueuning</keyname><forenames>Martin</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>Imperfect diffusion on temporal networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study diffusion on networks where the contact dynamics between the nodes
is governed by a random process and where the inter-contact time distribution
may differ from the exponential. We consider a process of imperfect diffusion,
where transmission is successful with a determined probability at each contact.
We first derive an expression for the inter-success time distribution,
determining the speed of the propagation, and then focus on a problem related
to epidemic spreading, by estimating the epidemic threshold in a system where
nodes remain infectious during a finite, random period of time. Finally, we
discuss the implications of our work to design an efficient strategy to enhance
spreading on temporal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04022</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04022</id><created>2015-08-17</created><authors><author><keyname>Bahrami</keyname><forenames>Sajjad</forenames></author><author><keyname>Razeghi</keyname><forenames>Behrooz</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Mostafa</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>Joint Source-Channel Coding for Broadcast Channel with Cooperating
  Receivers</title><categories>cs.IT math.IT</categories><comments>to appear in Proceedings of IEEE Information Theory Workshop - Fall
  (ITW'2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that, as opposed to point-to-point channel, separate source and
channel coding is not optimal in general for sending correlated sources over
multiuser channels. In some works joint source-channel coding has been
investigated for some certain multiuser channels; i.g., multiple access channel
(MAC) and broadcast channel (BC). In this paper, we obtain a sufficient
condition for transmitting arbitrarily correlated sources over a discrete
memoryless BC with cooperating receivers, where the receivers are allowed to
exchange messages via a pair of noisy cooperative links. It is seen that our
results is a general form of previous ones and includes them as its special
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04024</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04024</id><created>2015-08-17</created><authors><author><keyname>Lafuerza</keyname><forenames>Luis F.</forenames></author><author><keyname>Dyson</keyname><forenames>Louise</forenames></author><author><keyname>Edmonds</keyname><forenames>Bruce</forenames></author><author><keyname>McKane</keyname><forenames>Alan J.</forenames></author></authors><title>Simplification and analysis of a model of social interaction in voting</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently proposed model of social interaction in voting is investigated by
simplifying it down into a version that is more analytically tractable and
which allows a mathematical analysis to be performed. This analysis clarifies
the interplay of the different elements present in the system --- social
influence, heterogeneity and noise --- and leads to a better understanding of
its properties. The origin of a regime of bistability is identified. The
insight gained in this way gives further intuition into the behaviour of the
original model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04025</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04025</id><created>2015-08-17</created><updated>2015-09-20</updated><authors><author><keyname>Luong</keyname><forenames>Minh-Thang</forenames></author><author><keyname>Pham</keyname><forenames>Hieu</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>Effective Approaches to Attention-based Neural Machine Translation</title><categories>cs.CL</categories><comments>11 pages, 7 figures, EMNLP 2015 camera-ready version, more training
  details</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An attentional mechanism has lately been used to improve neural machine
translation (NMT) by selectively focusing on parts of the source sentence
during translation. However, there has been little work exploring useful
architectures for attention-based NMT. This paper examines two simple and
effective classes of attentional mechanism: a global approach which always
attends to all source words and a local one that only looks at a subset of
source words at a time. We demonstrate the effectiveness of both approaches
over the WMT translation tasks between English and German in both directions.
With local attention, we achieve a significant gain of 5.0 BLEU points over
non-attentional systems which already incorporate known techniques such as
dropout. Our ensemble model using different attention architectures has
established a new state-of-the-art result in the WMT'15 English to German
translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over
the existing best system backed by NMT and an n-gram reranker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04028</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04028</id><created>2015-08-17</created><updated>2016-02-13</updated><authors><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Lee</keyname><forenames>Joonbum</forenames></author><author><keyname>Reimer</keyname><forenames>Bryan</forenames></author><author><keyname>Victor</keyname><forenames>Trent</forenames></author></authors><title>&quot;Owl&quot; and &quot;Lizard&quot;: Patterns of Head Pose and Eye Pose in Driver Gaze
  Classification</title><categories>cs.CV cs.HC cs.LG</categories><comments>Accepted for Publication in IET Computer Vision. arXiv admin note:
  text overlap with arXiv:1507.04760</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate, robust, inexpensive gaze tracking in the car can help keep a driver
safe by facilitating the more effective study of how to improve (1) vehicle
interfaces and (2) the design of future Advanced Driver Assistance Systems. In
this paper, we estimate head pose and eye pose from monocular video using
methods developed extensively in prior work and ask two new interesting
questions. First, how much better can we classify driver gaze using head and
eye pose versus just using head pose? Second, are there individual-specific
gaze strategies that strongly correlate with how much gaze classification
improves with the addition of eye pose information? We answer these questions
by evaluating data drawn from an on-road study of 40 drivers. The main insight
of the paper is conveyed through the analogy of an &quot;owl&quot; and &quot;lizard&quot; which
describes the degree to which the eyes and the head move when shifting gaze.
When the head moves a lot (&quot;owl&quot;), not much classification improvement is
attained by estimating eye pose on top of head pose. On the other hand, when
the head stays still and only the eyes move (&quot;lizard&quot;), classification accuracy
increases significantly from adding in eye pose. We characterize how that
accuracy varies between people, gaze strategies, and gaze regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04030</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04030</id><created>2015-08-17</created><updated>2016-02-22</updated><authors><author><keyname>Jamali</keyname><forenames>Mohammad Vahid</forenames></author><author><keyname>Akhoundi</keyname><forenames>Farhad</forenames></author><author><keyname>Salehi</keyname><forenames>Jawad A.</forenames></author></authors><title>Performance Characterization of Relay-Assisted Wireless Optical CDMA
  Networks in Turbulent Underwater Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the performance of relay-assisted underwater
wireless optical code division multiple access (OCDMA) networks over turbulent
channels. In addition to scattering and absorption effects of underwater
channels, we also consider optical turbulence as a log-normal fading
coefficient in our analysis. To simultaneously and asynchronously share medium
among many users, we assign a unique optical orthogonal code (OOC) to each user
in order to actualize OCDMA-based underwater network. The most significant
challenge in underwater optical communication is in the ability to extend the
short range of its coverage. In order to expand the viable communication range,
we consider multi-hop transmission to the destination. Moreover, we evaluate
the performance of a relay-assisted point-to-point UWOC system as a special
case of the proposed relay-assisted OCDMA network. Our numerical results
indicate significant performance improvement by employing intermediate relays,
e.g., one can achieve $32$ {dB} improvement in the bit error rate (BER) of
$10^{-6}$ using only a dual-hop transmission in a $90$ {m} point-to-point clear
ocean link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04032</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04032</id><created>2015-08-17</created><authors><author><keyname>Xue</keyname><forenames>Yexiang</forenames></author><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author><author><keyname>Lebras</keyname><forenames>Ronan</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>Variable Elimination in Fourier Domain</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic inference is a key computational challenge in statistical
machine learning and artificial intelligence. The ability to represent complex
high dimensional probability distributions in a compact form is the most
important insight in the field of graphical models.
  In this paper, we explore a novel way to exploit compact representations of
high-dimensional probability distributions in approximate probabilistic
inference algorithms. Our approach is based on discrete Fourier Representation
of weighted Boolean Functions, complementing the classical method to exploit
conditional independence between the variables. We show that a large class of
probabilistic graphical models have a compact Fourier representation. This
theoretical result opens up an entirely new way of approximating a probability
distribution. We demonstrate the significance of this approach by applying it
to the variable elimination algorithm and comparing the results with the bucket
representation and other approximate inference algorithms, obtaining very
encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04035</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04035</id><created>2015-08-17</created><authors><author><keyname>Osegi</keyname><forenames>Emmanuel N.</forenames></author></authors><title>A Generative Model for Multi-Dialect Representation</title><categories>cs.CV cs.LG stat.ML</categories><comments>19 pages, 3 figures, 2 tables, Appendix</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In the era of deep learning several unsupervised models have been developed
to capture the key features in unlabeled handwritten data. Popular among them
is the Restricted Boltzmann Machines RBM. However, due to the novelty in
handwritten multidialect data, the RBM may fail to generate an efficient
representation. In this paper we propose a generative model, the Mode
Synthesizing Machine MSM for on-line representation of real life handwritten
multidialect language data. The MSM takes advantage of the hierarchical
representation of the modes of a data distribution using a two-point error
update to learn a sequence of representative multidialects in a generative way.
Experiments were performed to evaluate the performance of the MSM over the RBM
with the former attaining much lower error values than the latter on both
independent and mixed data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04040</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04040</id><created>2015-08-17</created><updated>2015-12-15</updated><authors><author><keyname>Determe</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Louveaux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Horlin</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>On The Exact Recovery Condition of Simultaneous Orthogonal Matching
  Pursuit</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2015.2506989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several exact recovery criteria (ERC) ensuring that orthogonal matching
pursuit (OMP) identifies the correct support of sparse signals have been
developed in the last few years. These ERC rely on the restricted isometry
property (RIP), the associated restricted isometry constant (RIC) and sometimes
the restricted orthogonality constant (ROC). In this paper, three of the most
recent ERC for OMP are examined. The contribution is to show that these ERC
remain valid for a generalization of OMP, entitled simultaneous orthogonal
matching pursuit (SOMP), that is capable to process several measurement vectors
simultaneously and return a common support estimate for the underlying sparse
vectors. The sharpness of the bounds is also briefly discussed in light of
previous works focusing on OMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04044</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04044</id><created>2015-08-17</created><authors><author><keyname>Damevski</keyname><forenames>Kostadin</forenames></author><author><keyname>Shepherd</keyname><forenames>David</forenames></author><author><keyname>Kraft</keyname><forenames>Nicholas</forenames></author><author><keyname>Pollock</keyname><forenames>Lori</forenames></author></authors><title>Supporting Developers in Porting Software via Combined Textual and
  Structural Analysis of Software Artifacts</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is position paper accepted to the Computational Science &amp; Engineering
Software Sustainability and Productivity Challenges (CSESSP Challenges)
Workshop, sponsored by the Networking and Information Technology Research and
Development (NITRD) Software Design and Productivity (SDP) Coordinating Group,
held October 15th-16th 2015 in Washington DC, USA. It discusses the role
recommendation systems, based on textual and structural information in source
code, and further enhanced by mining related applications, can have in
improving the portability of scientific and engineering software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04048</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04048</id><created>2015-08-17</created><authors><author><keyname>Esmaeili</keyname><forenames>Leila</forenames></author><author><keyname>Mutallebi</keyname><forenames>Muhamad</forenames></author><author><keyname>Mardani</keyname><forenames>Shahla</forenames></author><author><keyname>Golpayegani</keyname><forenames>Seyyed Alireza Hashemi</forenames></author></authors><title>Studying the Affecting Factors on Trust in Social Commerce</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, e-commerce has enjoyed the appearance of many novel opportunities
created as a result of the increasing growth of social networks. Social
commerce (s-commerce), an offspring of e-commerce, interconnects users and
helps with the commerce process facilitated by social networks as well as other
media. Uncertainty and trust are always an issue in any type of commerce and
especially in s-commerce where, due to its nature, a higher level of risk and
uncertainty exists. Despite the history for trust in virtual commercial
transactions, there are a limited number of researches on the topic; and the
current shift towards commerce in virtual communities and networks begs further
attention. Therefore, in this paper, by scrutinizing the available literature
on the topic, the affecting factors on trust in scommerce are identified and
introduced. Among the reviewed papers, 7 papers matched the criteria, and
overall, 19 factors were extracted. Firm size, trust disposition, information
quality, and familiarity with other members were among the most cited factors
in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04053</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04053</id><created>2015-08-17</created><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>The odd Hadwiger's conjecture is &quot;almost'' decidable</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The odd Hadwiger's conjecture, made by Gerads and Seymour in early 1990s, is
an analogue of the famous Hadwiger's conjecture. It says that every graph with
no odd $K_t$-minor is $(t-1)$-colorable. This conjecture is known to be true
for $t \leq 5$, but the cases $t \geq 5$ are wide open. So far, the most
general result says that every graph with no odd $K_t$-minor is $O(t \sqrt{\log
t})$-colorable.
  In this paper, we tackle this conjecture from an algorithmic view, and show
the following:
  For a given graph $G$ and any fixed $t$, there is a polynomial time algorithm
to output one of the following:
  \begin{enumerate} \item a $(t-1)$-coloring of $G$, or \item an odd
$K_{t}$-minor of $G$, or \item after making all &quot;reductions&quot; to $G$, the
resulting graph $H$ (which is an odd minor of $G$ and which has no reductions)
has a tree-decomposition $(T, Y)$ such that torso of each bag $Y_t$ is either
\begin{itemize} \item of size at most $f_1(t) \log n$ for some function $f_1$
of $t$, or \item a graph that has a vertex $X$ of order at most $f_2(t)$ for
some function $f_2$ of $t$ such that $Y_t-X$ is bipartite. Moreover, degree of
$t$ in $T$ is at most $f_3(t)$ for some function $f_3$ of $t$. \end{itemize}
\end{enumerate}
  Let us observe that the last odd minor $H$ is indeed a minimal counterexample
to the odd Hadwiger's conjecture for the case $t$. So our result says that a
minimal counterexample satisfies the lsat conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04065</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04065</id><created>2015-08-17</created><authors><author><keyname>Mousavi</keyname><forenames>Ali</forenames></author><author><keyname>Patel</keyname><forenames>Ankit B.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>A Deep Learning Approach to Structured Signal Recovery</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new framework for sensing and recovering
structured signals. In contrast to compressive sensing (CS) systems that employ
linear measurements, sparse representations, and computationally complex
convex/greedy algorithms, we introduce a deep learning framework that supports
both linear and mildly nonlinear measurements, that learns a structured
representation from training data, and that efficiently computes a signal
estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an
unsupervised feature learner. SDA enables us to capture statistical
dependencies between the different elements of certain signals and improve
signal recovery performance as compared to the CS approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04066</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04066</id><created>2015-08-17</created><updated>2015-08-18</updated><authors><author><keyname>Weninger</keyname><forenames>Tim</forenames></author><author><keyname>Palacios</keyname><forenames>Rodrigo</forenames></author><author><keyname>Crescenzi</keyname><forenames>Valter</forenames></author><author><keyname>Gottron</keyname><forenames>Thomas</forenames></author><author><keyname>Merialdo</keyname><forenames>Paolo</forenames></author></authors><title>Web Content Extraction - a Meta-Analysis of its Past and Thoughts on its
  Future</title><categories>cs.IR cs.DB</categories><comments>Accepted for publication in SIGKDD Explorations</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present a meta-analysis of several Web content extraction
algorithms, and make recommendations for the future of content extraction on
the Web. First, we find that nearly all Web content extractors do not consider
a very large, and growing, portion of modern Web pages. Second, it is well
understood that wrapper induction extractors tend to break as the Web changes;
heuristic/feature engineering extractors were thought to be immune to a Web
site's evolution, but we find that this is not the case: heuristic content
extractor performance also tends to degrade over time due to the evolution of
Web site forms and practices. We conclude with recommendations for future work
that address these and other findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04073</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04073</id><created>2015-08-17</created><authors><author><keyname>Mousavi</keyname><forenames>Ali</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>An Information-Theoretic Measure of Dependency Among Variables in Large
  Datasets</title><categories>cs.IT math.IT stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximal information coefficient (MIC), which measures the amount of
dependence between two variables, is able to detect both linear and non-linear
associations. However, computational cost grows rapidly as a function of the
dataset size. In this paper, we develop a computationally efficient
approximation to the MIC that replaces its dynamic programming step with a much
simpler technique based on the uniform partitioning of data grid. A variety of
experiments demonstrate the quality of our approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04087</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04087</id><created>2015-08-17</created><updated>2015-12-20</updated><authors><author><keyname>Wolff</keyname><forenames>J. G.</forenames></author></authors><title>The SP theory of intelligence: distinctive features and advantages</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper highlights distinctive features of the &quot;SP theory of intelligence&quot;
and its apparent advantages compared with some AI-related alternatives.
Distinctive features and advantages are: simplification and integration of
observations and concepts; simplification and integration of structures and
processes in computing systems; the theory is itself a theory of computing; it
can be the basis for new architectures for computers; information compression
via the matching and unification of patterns and, more specifically, via
multiple alignment, is fundamental; transparency in the representation and
processing of knowledge; the discovery of 'natural' structures via information
compression (DONSVIC); interpretations of mathematics; interpretations in human
perception and cognition; and realisation of abstract concepts in terms of
neurons and their inter-connections (&quot;SP-neural&quot;). These things relate to
AI-related alternatives: minimum length encoding and related concepts; deep
learning in neural networks; unified theories of cognition and related
research; universal search; Bayesian networks and more; pattern recognition and
vision; the analysis, production, and translation of natural language;
Unsupervised learning of natural language; exact and inexact forms of
reasoning; representation and processing of diverse forms of knowledge; IBM's
Watson; software engineering; solving problems associated with big data, and in
the development of intelligence in autonomous robots. In conclusion, the SP
system can provide a firm foundation for the long-term development of AI, with
many potential benefits and applications. It may also deliver useful results on
relatively short timescales. A high-parallel, open-source version of the SP
machine, derived from the SP computer model, would be a means for researchers
everywhere to explore what can be done with the system, and to create new
versions of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04089</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04089</id><created>2015-08-17</created><updated>2015-10-26</updated><authors><author><keyname>Madiman</keyname><forenames>Mokshay</forenames></author><author><keyname>Kontoyiannis</keyname><forenames>Ioannis</forenames></author></authors><title>Entropy bounds on abelian groups and the Ruzsa divergence</title><categories>cs.IT math.CO math.IT math.PR</categories><comments>26 pages. Changes in v2: Added Section V on entropies of products of
  random variables, corrected several typos, added some references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, a family of interesting new inequalities for the
entropies of sums and differences of random variables has been developed by
Ruzsa, Tao and others, motivated by analogous results in additive
combinatorics. The present work extends these earlier results to the case of
random variables taking values in $\mathbb{R}^n$ or, more generally, in
arbitrary locally compact and Polish abelian groups. We isolate and study a key
quantity, the Ruzsa divergence between two probability distributions, and we
show that its properties can be used to extend the earlier inequalities to the
present general setting. The new results established include several variations
on the theme that the entropies of the sum and the difference of two
independent random variables severely constrain each other. Although the
setting is quite general, the result are already of interest (and new) for
random vectors in $\mathbb{R}^n$. In that special case, quantitative bounds are
provided for the stability of the equality conditions in the entropy power
inequality; a reverse entropy power inequality for log-concave random vectors
is proved; an information-theoretic analog of the Rogers-Shephard inequality
for convex bodies is established; and it is observed that some of these results
lead to new inequalities for the determinants of positive-definite matrices.
Moreover, by considering the multiplicative subgroups of the complex plane, one
obtains new inequalities for the differential entropies of products and ratios
of nonzero, complex-valued random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04093</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04093</id><created>2015-08-17</created><updated>2015-10-26</updated><authors><author><keyname>Fradelizi</keyname><forenames>Matthieu</forenames></author><author><keyname>Madiman</keyname><forenames>Mokshay</forenames></author><author><keyname>Wang</keyname><forenames>Liyao</forenames></author></authors><title>Optimal Concentration of Information Content For Log-Concave Densities</title><categories>math.PR cs.IT math.FA math.IT</categories><comments>15 pages. Changes in v2: Remark 2.5 (due to C. Saroglou) added with
  more general sufficient conditions for equality in Theorem 2.3. Also some
  minor corrections and added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An elementary proof is provided of sharp bounds for the varentropy of random
vectors with log-concave densities, as well as for deviations of the
information content from its mean. These bounds significantly improve on the
bounds obtained by Bobkov and Madiman ({\it Ann. Probab.}, 39(4):1528--1543,
2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04095</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04095</id><created>2015-08-17</created><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author></authors><title>Algorithmic Aspects of Optimal Channel Coding</title><categories>cs.IT cs.DS math.IT quant-ph</categories><comments>14 pages, comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central question in information theory is to determine the maximum success
probability that can be achieved in sending a fixed number of messages over a
noisy channel. This was first studied in the pioneering work of Shannon who
established a simple expression characterizing this quantity in the limit of
multiple independent uses of the channel. Here we consider the general setting
with only one use of the channel. We observe that the maximum success
probability can be expressed as the maximum value of a submodular function.
Using this connection, we establish the following results:
  1. There is a simple greedy polynomial-time algorithm that computes a code
achieving a (1-1/e)-approximation of the maximum success probability. Moreover,
for this problem it is NP-hard to obtain an approximation ratio strictly better
than (1-1/e).
  2. Shared quantum entanglement between the sender and the receiver can
increase the success probability by a factor of at most 1/(1-1/e). In addition,
this factor is tight if one allows an arbitrary non-signaling box between the
sender and the receiver.
  3. We give tight bounds on the one-shot performance of the meta-converse of
Polyanskiy-Poor-Verdu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04099</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04099</id><created>2015-08-17</created><updated>2015-09-07</updated><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames></author></authors><title>Permanents, Bosons and Linear Optics</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>v2: revtex4, 8 pages, extended by a third, new references, title
  changed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expressions with permanents in quantum processes with bosons deserved
recently certain attention. A difference between couple relevant models is
discussed in presented work. An oscillator model has certain resemblance with
matchgate circuits also known as &quot;fermionic linear optics&quot; and effectively
simulated on classical computer. The possibility of effective classical
computations of average particle numbers in single-mode measurement for bosonic
linear optical networks is treated using the analogy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04105</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04105</id><created>2015-08-17</created><authors><author><keyname>Ogbogu</keyname><forenames>Nelson O.</forenames></author><author><keyname>Madueme</keyname><forenames>Theophilus C.</forenames></author><author><keyname>Osegi</keyname><forenames>Emmanuel N.</forenames></author></authors><title>PTILE: A framework for the Evaluation of Power Transformer Insulation
  Life in Electric Power System</title><categories>cs.CE</categories><comments>5 pages, 6 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, a framework is developed for power transformer (Generator Step
up Unit) insulation life evaluation (PTILE) study on power system Network.
Parameters used for studies include real time sample data obtained from power
transformer field studies in the South-South Niger Delta region of Nigeria. It
is used for performing simulations over varying number of years. Simulation
reports shows a polynomial running time complexity and validates the stochastic
Hot Spot theory indicating that the transformers in such region should be
replaced sooner due to higher hot spots and transformer loading in such regions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04112</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04112</id><created>2015-08-17</created><updated>2015-08-17</updated><authors><author><keyname>Lei</keyname><forenames>Tao</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>Molding CNNs for text: non-linear, non-consecutive convolutions</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of deep learning often derives from well-chosen operational
building blocks. In this work, we revise the temporal convolution operation in
CNNs to better adapt it to text processing. Instead of concatenating word
representations, we appeal to tensor algebra and use low-rank n-gram tensors to
directly exploit interactions between words already at the convolution stage.
Moreover, we extend the n-gram convolution to non-consecutive words to
recognize patterns with intervening words. Through a combination of low-rank
tensors, and pattern weighting, we can efficiently evaluate the resulting
convolution operation via dynamic programming. We test the resulting
architecture on standard sentiment classification and news categorization
tasks. Our model achieves state-of-the-art performance both in terms of
accuracy and training speed. For instance, we obtain 51.2% accuracy on the
fine-grained sentiment classification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04123</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04123</id><created>2015-08-17</created><authors><author><keyname>Mbaziira</keyname><forenames>Alex V.</forenames></author><author><keyname>Abozinadah</keyname><forenames>Ehab</forenames></author><author><keyname>Jones</keyname><forenames>James H.</forenames><suffix>Jr</suffix></author></authors><title>Evaluating Classifiers in Detecting 419 Scams in Bilingual Cybercriminal
  Communities</title><categories>cs.SI cs.CY cs.LG</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Incidents of organized cybercrime are rising because of criminals are reaping
high financial rewards while incurring low costs to commit crime. As the
digital landscape broadens to accommodate more internet-enabled devices and
technologies like social media, more cybercriminals who are not native English
speakers are invading cyberspace to cash in on quick exploits. In this paper we
evaluate the performance of three machine learning classifiers in detecting 419
scams in a bilingual Nigerian cybercriminal community. We use three popular
classifiers in text processing namely: Na\&quot;ive Bayes, k-nearest neighbors (IBK)
and Support Vector Machines (SVM). The preliminary results on a real world
dataset reveal the SVM significantly outperforms Na\&quot;ive Bayes and IBK at 95%
confidence level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04124</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04124</id><created>2015-08-17</created><updated>2015-09-08</updated><authors><author><keyname>Altendorfer</keyname><forenames>Richard</forenames></author><author><keyname>Wirkert</keyname><forenames>Sebastian</forenames></author></authors><title>A Complete Derivation Of The Association Log-Likelihood Distance For
  Multi-Object Tracking</title><categories>cs.SY cs.RO</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Mahalanobis distance is commonly used in multi-object trackers for
measurement-to-track association. Starting with the original definition of the
Mahalanobis distance we review its use in association. Given that there is no
principle in multi-object tracking that sets the Mahalanobis distance apart as
a distinguished statistical distance we revisit the global association
hypotheses of multiple hypothesis tracking as the most general association
setting. Those association hypotheses induce a distance-like quantity for
assignment which we refer to as association log-likelihood distance. We compare
the ability of the Mahalanobis distance to the association log-likelihood
distance to yield correct association relations in Monte-Carlo simulations. It
turns out that on average the distance based on association log-likelihood
performs better than the Mahalanobis distance, confirming that the maximization
of global association hypotheses is a more fundamental approach to association
than the minimization of a certain statistical distance measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04125</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04125</id><created>2015-08-17</created><updated>2015-09-02</updated><authors><author><keyname>Case</keyname><forenames>Adam</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Stull</keyname><forenames>D. M.</forenames></author></authors><title>Reachability Problems for Continuous Chemical Reaction Networks</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemical reaction networks (CRNs) model the behavior of molecules in a
well-mixed system. The emerging field of molecular programming uses CRNs not
only as a descriptive tool, but as a programming language for chemical
computation. Recently, Chen, Doty and Soloveichik introduced a new model of
chemical kinetics, rate-independent continuous CRNs (CCRNs), to study the
chemical computation of continuous functions. A fundamental question of a CRN
is whether a state of the system is reachable through a sequence of reactions
in the network. This is known as the reachability problem. In this paper, we
investigate CCRN-REACH, the reachability problem for this model of chemical
reaction networks. We show that, for continuous CRNs, constructing a path to a
state of the network is computable in polynomial time. We also prove that a
related problem, Sub-CCRN-REACH, is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04126</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04126</id><created>2015-06-09</created><authors><author><keyname>Akhlaq</keyname><forenames>Assad</forenames></author><author><keyname>McKilliam</keyname><forenames>R. G.</forenames></author><author><keyname>Subramanian</keyname><forenames>R.</forenames></author></authors><title>Basis construction for range estimation by phase unwrapping</title><categories>stat.AP cs.IT math.IT</categories><comments>submitted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2015.2465153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the distance, or range, between two
locations by measuring the phase of a sinusoidal signal transmitted between the
locations. This method is only capable of unambiguously measuring range within
an interval of length equal to the wavelength of the signal. To address this
problem signals of multiple different wavelengths can be transmitted. The range
can then be measured within an interval of length equal to the least common
multiple of these wavelengths. Estimation of the range requires solution of a
problem from computational number theory called the closest lattice point
problem. Algorithms to solve this problem require a basis for this lattice.
Constructing a basis is non-trivial and an explicit construction has only been
given in the case that the wavelengths can be scaled to pairwise relatively
prime integers. In this paper we present an explicit construction of a basis
without this assumption on the wavelengths. This is important because the
accuracy of the range estimator depends upon the wavelengths. Simulations
indicate that significant improvement in accuracy can be achieved by using
wavelengths that cannot be scaled to pairwise relatively prime integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04127</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04127</id><created>2015-08-17</created><authors><author><keyname>Ding</keyname><forenames>Huanyu</forenames></author><author><keyname>Casta&#xf1;&#xf3;n</keyname><forenames>David A.</forenames></author></authors><title>Optimal Solutions for Adaptive Search Problems with Entropy Objectives</title><categories>cs.IT cs.SY math.IT</categories><msc-class>93E35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of searching for an unknown object occurs in important
applications ranging from security, medicine and defense. Sensors with the
capability to process information rapidly require adaptive algorithms to
control their search in response to noisy observations. In this paper, we
discuss classes of dynamic, adaptive search problems, and formulate the
resulting sensor control problems as stochastic control problems with imperfect
information, based on previous work on noisy search problems. The structure of
these problems, with objective functions related to information entropy, allows
for a complete characterization of the optimal strategies and the optimal cost
for the resulting finite-horizon stochastic control problems. We study the
problem where an individual sensor is capable of searching over multiple
sub-regions in a time, and provide a constructive algorithm for determining
optimal policies in real time based on convex optimization. We also study the
problem in which there are multiple sensors, each of which is only capable of
detecting over one sub-region in a time, jointly searching for an object.
Whereas this can be viewed as a special case of our multi-region results, we
show that the computation of optimal policies can be decoupled into
single-sensor individual scalar convex optimization problems, and provide
simple symmetry conditions where the solutions can be determined analytically.
We also consider the case where individual sensors can select the accuracy of
their sensing modes with different costs, and derive optimal strategies for
these problems in terms of the solutions of scalar convex optimization
problems. We illustrate our results with experiments using multiple sensors
searching for a single object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04131</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04131</id><created>2015-08-17</created><authors><author><keyname>Joshi</keyname><forenames>Pushkar</forenames></author></authors><title>Contextually proximate approach to develop smart user interface</title><categories>cs.HC</categories><comments>4 pages 2 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Researchers and experts are taking efforts in delivering an optimal user
experience from a long time. Computer interfaces are being developed to keep
user 'in the flow' as well as for making users more connected to the real world
wile using virtual environment. Developing ubiquitous user interfaces for
novices and experts at the same time is crucial work for interaction designers.
This paper molds the designing approach of user interfaces in bit different
parameters by reviewing the existing literature and proposing a different way
to develop a smart user interface to make user more familiar with the design
and to keep user 'in the flow'. Contextually proximate approach (CPA) will help
users to minimize their feeling of insecurity as designing process includes
local resources of users to develop the user interfaces. These various
resources and parameters are explained further in the paper by giving different
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04145</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04145</id><created>2015-08-17</created><authors><author><keyname>Fallenstein</keyname><forenames>Benja</forenames></author><author><keyname>Taylor</keyname><forenames>Jessica</forenames></author><author><keyname>Christiano</keyname><forenames>Paul F.</forenames></author></authors><title>Reflective Oracles: A Foundation for Classical Game Theory</title><categories>cs.AI cs.GT</categories><comments>Extended version of &quot;Reflective Oracles: A Foundation for Game Theory
  in Artificial Intelligence&quot; accepted to LORI-V</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical game theory treats players as special---a description of a game
contains a full, explicit enumeration of all players---even though in the real
world, &quot;players&quot; are no more fundamentally special than rocks or clouds. It
isn't trivial to find a decision-theoretic foundation for game theory in which
an agent's coplayers are a non-distinguished part of the agent's environment.
Attempts to model both players and the environment as Turing machines, for
example, fail for standard diagonalization reasons.
  In this paper, we introduce a &quot;reflective&quot; type of oracle, which is able to
answer questions about the outputs of oracle machines with access to the same
oracle. These oracles avoid diagonalization by answering some queries randomly.
We show that machines with access to a reflective oracle can be used to define
rational agents using causal decision theory. These agents model their
environment as a probabilistic oracle machine, which may contain other agents
as a non-distinguished part.
  We show that if such agents interact, they will play a Nash equilibrium, with
the randomization in mixed strategies coming from the randomization in the
oracle's answers. This can be seen as providing a foundation for classical game
theory in which players aren't special.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04153</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04153</id><created>2015-06-23</created><authors><author><keyname>Boulanger</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Seifert</keyname><forenames>Ludovic</forenames></author><author><keyname>H&#xe9;rault</keyname><forenames>Romain</forenames></author><author><keyname>Coeurjolly</keyname><forenames>Jean-Francois</forenames></author></authors><title>Automatic sensor-based detection and classification of climbing
  activities</title><categories>stat.AP cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a method to automatically detect and classify climbing
activities using inertial measurement units (IMUs) attached to the wrists, feet
and pelvis of the climber. The IMUs record limb acceleration and angular
velocity. Detection requires a learning phase with manual annotation to
construct the statistical models used in the cusum algorithm. Full-body
activity is then classified based on the detection of each IMU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04158</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04158</id><created>2015-07-12</created><authors><author><keyname>Fantacci</keyname><forenames>Claudio</forenames></author></authors><title>Distributed multi-object tracking over sensor networks: a random finite
  set approach</title><categories>stat.ME cs.SY</categories><comments>Ph.D. thesis of Claudio Fantacci, Universit\`a di Firenze,
  Dipartimento di Ingegneria dell'Informazione (DINFO), Florence, Italy
  Successfully defended on the 5th of March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the present dissertation is to address distributed tracking over a
network of heterogeneous and geographically dispersed nodes (or agents) with
sensing, communication and processing capabilities. Tracking is carried out in
the Bayesian framework and its extension to a distributed context is made
possible via an information-theoretic approach to data fusion which exploits
consensus algorithms and the notion of Kullback-Leibler Average (KLA) of the
Probability Density Functions (PDFs) to be fused. The first step toward
distributed tracking considers a single moving object. Consensus takes place in
each agent for spreading information over the network so that each node can
track the object. To achieve such a goal, consensus is carried out on the local
single-object posterior distribution, which is the result of local data
processing, in the Bayesian setting, exploiting the last available measurement
about the object. The next step is in the direction of distributed estimation
of multiple moving objects. In order to model, in a rigorous and elegant way, a
possibly time-varying number of objects present in a given area of interest,
the Random Finite Set (RFS) formulation is adopted since it provides the notion
of probability density for multi-object states that allows to directly extend
existing tools in distributed estimation to multi-object tracking. The last
theoretical step of the present dissertation is toward distributed filtering
with the further requirement of unique object identities. To this end the
labeled RFS framework is adopted as it provides a tractable approach to the
multi-object Bayesian recursion. A generalization of the KLA to the labeled RFS
framework, enables the development of novel consensus multi-object tracking
filters which are fully distributed, scalable and computationally efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04159</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04159</id><created>2015-08-17</created><updated>2015-10-04</updated><authors><author><keyname>Dietrich</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Zug</keyname><forenames>Sebastian</forenames></author><author><keyname>Nardi</keyname><forenames>Luigi</forenames></author><author><keyname>Kaiser</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Reasoning in complex environments with the SelectScript declarative
  language</title><categories>cs.PL cs.AI cs.DB cs.RO</categories><comments>15 pages, 7 figures, 6th International Workshop on Domain-Specific
  Languages and models for ROBotic systems (DSLRob-15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SelectScript is an extendable, adaptable, and declarative domain-specific
language aimed at information retrieval from simulation environments and
robotic world models in an SQL-like manner. In this work we have extended the
language in two directions. First, we have implemented hierarchical queries;
second, we improve efficiency enabling manual design space exploration on
different &quot;search&quot; strategies. We demonstrate the applicability of such
extensions in two application problems; the basic language concepts are
explained by solving the classical problem of the Towers of Hanoi and then a
common path planning problem in a complex 3D environment is implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04163</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04163</id><created>2015-08-17</created><authors><author><keyname>Hosseinloo</keyname><forenames>Ashkan Haji</forenames></author><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Optimal control strategies for efficient energy harvesting from ambient
  vibrations</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ease of miniaturization and minimal maintenance are among the advantages for
replacing conventional batteries with vibratory energy harvesters in a wide of
range of disciplines and applications, from wireless communication sensors to
medical implants. However, the current harvesters do not extract energy from
the ambient vibrations in a very efficient and robust fashion, and hence, there
need to be more optimal harvesting approaches. In this paper, we introduce a
generic architecture for vibration energy harvesting and delineate the key
challenges in the field. Then, we formulate an optimal control problem to
maximize the harvested energy. Though possessing similar structure to that of
the standard LQG problem, this optimal control problem is inherently different
from the LQG problem and poses theoretical challenges to control community. As
the first step, we simplify it to a tractable problem of optimizing control
gains for a linear system subjected to Gaussian white noise excitation, and
show that this optimal problem has non-trivial optimal solutions in both time
and frequency domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04172</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04172</id><created>2015-08-17</created><updated>2015-11-29</updated><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Grant</keyname><forenames>Steven L.</forenames></author></authors><title>Proportionate Adaptive Filtering for Block Sparse System Identification</title><categories>cs.IT math.IT</categories><doi>10.1109/TASLP.2015.2499602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new family of proportionate normalized least mean square
(PNLMS) adaptive algorithms that improve the performance of identifying
block-sparse systems is proposed. The main proposed algorithm, called
block-sparse PNLMS (BS-PNLMS), is based on the optimization of a mixed l2,1
norm of the adaptive filter coefficients. It is demonstrated that both the NLMS
and the traditional PNLMS are special cases of BS-PNLMS. Meanwhile, a
block-sparse improved PNLMS (BS-IPNLMS) is also derived for both sparse and
dispersive impulse responses. Simulation results demonstrate that the proposed
BS-PNLMS and BS-IPNLMS algorithms outperformed the NLMS, PNLMS and IPNLMS
algorithms with only a modest increase in computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04180</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04180</id><created>2015-08-17</created><updated>2016-03-05</updated><authors><author><keyname>Turilli</keyname><forenames>Matteo</forenames></author><author><keyname>Santcroos</keyname><forenames>Mark</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>A Comprehensive Perspective on Pilot-Job Systems</title><categories>cs.DC cs.SE</categories><msc-class>68Nxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pilot-Job systems play an important role in supporting distributed scientific
computing. They are used to consume more than 700 million CPU hours a year by
the Open Science Grid communities, and by processing up to 1 million jobs a day
for the ATLAS experiment on the Worldwide LHC Computing Grid. With the
increasing importance of task-level parallelism in high-performance computing,
Pilot-Job systems are also witnessing an adoption beyond traditional domains.
Notwithstanding the growing impact on scientific research, there is no
agreement upon a definition of Pilot-Job system and no clear understanding of
the underlying abstraction and paradigm. Pilot-Job implementations have
proliferated with no shared best practices or open interfaces and little
interoperability. Ultimately, this is hindering the realization of the full
impact of Pilot-Jobs by limiting their robustness, portability, and
maintainability. This paper offers a comprehensive analysis of Pilot-Job
systems critically assessing their motivations, evolution, properties, and
implementation. The three main contributions of this paper are: (i) an analysis
of the motivations and evolution of Pilot-Job systems; (ii) an outline of the
Pilot abstraction, its distinguishing logical components and functionalities,
its terminology, and its architecture pattern; and (iii) the description of
core and auxiliary properties of Pilot-Jobs systems and the analysis of seven
exemplar Pilot-Job implementations. Together, these contributions illustrate
the Pilot paradigm, its generality, and how it helps to address some challenges
in distributed scientific computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04185</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04185</id><created>2015-08-17</created><authors><author><keyname>Park</keyname><forenames>Jaehyuk</forenames></author><author><keyname>Ciampaglia</keyname><forenames>Giovanni Luca</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author></authors><title>Style in the Age of Instagram: Predicting Success within the Fashion
  Industry using Social Media</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>10 pages, 5 figures, accepted for presentation at CSCW'16</comments><journal-ref>CSCW '16, February 27-March 02, 2016</journal-ref><doi>10.1145/2818048.2820065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fashion is a multi-billion dollar industry with social and economic
implications worldwide. To gain popularity, brands want to be represented by
the top popular models. As new faces are selected using stringent (and often
criticized) aesthetic criteria, \emph{a priori} predictions are made difficult
by information cascades and other fundamental trend-setting mechanisms.
However, the increasing usage of social media within and without the industry
may be affecting this traditional system. We therefore seek to understand the
ingredients of success of fashion models in the age of Instagram. Combining
data from a comprehensive online fashion database and the popular mobile
image-sharing platform, we apply a machine learning framework to predict the
tenure of a cohort of new faces for the 2015 Spring\,/\,Summer season
throughout the subsequent 2015-16 Fall\,/\,Winter season. Our framework
successfully predicts most of the new popular models who appeared in 2015. In
particular, we find that a strong social media presence may be more important
than being under contract with a top agency, or than the aesthetic standards
sought after by the industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04186</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04186</id><created>2015-08-17</created><updated>2015-10-15</updated><authors><author><keyname>Ong</keyname><forenames>Hao Yi</forenames></author><author><keyname>Chavez</keyname><forenames>Kevin</forenames></author><author><keyname>Hong</keyname><forenames>Augustus</forenames></author></authors><title>Distributed Deep Q-Learning</title><categories>cs.LG cs.AI cs.DC cs.NE</categories><comments>Updated figure of distributed deep learning architecture, updated
  content throughout paper including dealing with minor grammatical issues and
  highlighting differences of our paper with respect to prior work. arXiv admin
  note: text overlap with arXiv:1312.5602 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is based on the deep Q-network, a convolutional neural
network trained with a variant of Q-learning. Its input is raw pixels and its
output is a value function estimating future rewards from taking an action
given a system state. To distribute the deep Q-network training, we adapt the
DistBelief software framework to the context of efficiently training
reinforcement learning agents. As a result, the method is completely
asynchronous and scales well with the number of machines. We demonstrate that
the deep Q-network agent, receiving only the pixels and the game score as
inputs, was able to achieve reasonable success on a simple game with minimal
parameter tuning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04190</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04190</id><created>2015-08-17</created><authors><author><keyname>Zongbo</keyname><forenames>Hao</forenames></author><author><keyname>Linlin</keyname><forenames>Lu</forenames></author><author><keyname>Qianni</keyname><forenames>Zhang</forenames></author><author><keyname>Jie</keyname><forenames>Wu</forenames></author><author><keyname>Ebroul</keyname><forenames>Izquierdo</forenames></author><author><keyname>Juanyu</keyname><forenames>Yang</forenames></author><author><keyname>Jun</keyname><forenames>Zhao</forenames></author></authors><title>Action Recognition based on Subdivision-Fusion Model</title><categories>cs.CV</categories><comments>Accepted by BMVC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel Subdivision-Fusion Model (SFM) to recognize human
actions. In most action recognition tasks, overlapping feature distribution is
a common problem leading to overfitting. In the subdivision stage of the
proposed SFM, samples in each category are clustered. Then, such samples are
grouped into multiple more concentrated subcategories. Boundaries for the
subcategories are easier to find and as consequence overfitting is avoided. In
the subsequent fusion stage, the multi-subcategories classification results are
converted back to the original category recognition problem. Two methods to
determine the number of clusters are provided. The proposed model has been
thoroughly tested with four popular datasets. In the Hollywood2 dataset, an
accuracy of 79.4% is achieved, outperforming the state-of-the-art accuracy of
64.3%. The performance on the YouTube Action dataset has been improved from
75.8% to 82.5%, while considerably improvements are also observed on the KTH
and UCF50 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04198</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04198</id><created>2015-08-17</created><authors><author><keyname>Fu</keyname><forenames>Yifan</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Hong</keyname><forenames>Xia</forenames></author><author><keyname>Tien</keyname><forenames>David</forenames></author></authors><title>Low Rank Representation on Riemannian Manifold of Square Root Densities</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel low rank representation (LRR) algorithm for
data lying on the manifold of square root densities. Unlike traditional LRR
methods which rely on the assumption that the data points are vectors in the
Euclidean space, our new algorithm is designed to incorporate the intrinsic
geometric structure and geodesic distance of the manifold. Experiments on
several computer vision datasets showcase its noise robustness and superior
performance on classification and subspace clustering compared to other
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04210</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04210</id><created>2015-08-18</created><authors><author><keyname>Hu</keyname><forenames>Changwei</forenames></author><author><keyname>Rai</keyname><forenames>Piyush</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors</title><categories>stat.ML cs.LG</categories><comments>UAI (Uncertainty in Artificial Intelligence) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable Bayesian model for low-rank factorization of massive
tensors with binary observations. The proposed model has the following key
properties: (1) in contrast to the models based on the logistic or probit
likelihood, using a zero-truncated Poisson likelihood for binary data allows
our model to scale up in the number of \emph{ones} in the tensor, which is
especially appealing for massive but sparse binary tensors; (2)
side-information in form of binary pairwise relationships (e.g., an adjacency
network) between objects in any tensor mode can also be leveraged, which can be
especially useful in &quot;cold-start&quot; settings; and (3) the model admits simple
Bayesian inference via batch, as well as \emph{online} MCMC; the latter allows
scaling up even for \emph{dense} binary data (i.e., when the number of ones in
the tensor/network is also massive). In addition, non-negative factor matrices
in our model provide easy interpretability, and the tensor rank can be inferred
from the data. We evaluate our model on several large-scale real-world binary
tensors, achieving excellent computational scalability, and also demonstrate
its usefulness in leveraging side-information provided in form of
mode-network(s).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04211</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04211</id><created>2015-08-18</created><authors><author><keyname>Hu</keyname><forenames>Changwei</forenames></author><author><keyname>Rai</keyname><forenames>Piyush</forenames></author><author><keyname>Chen</keyname><forenames>Changyou</forenames></author><author><keyname>Harding</keyname><forenames>Matthew</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Scalable Bayesian Non-Negative Tensor Factorization for Massive Count
  Data</title><categories>stat.ML cs.LG</categories><comments>ECML PKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian non-negative tensor factorization model for
count-valued tensor data, and develop scalable inference algorithms (both batch
and online) for dealing with massive tensors. Our generative model can handle
overdispersed counts as well as infer the rank of the decomposition. Moreover,
leveraging a reparameterization of the Poisson distribution as a multinomial
facilitates conjugacy in the model and enables simple and efficient Gibbs
sampling and variational Bayes (VB) inference updates, with a computational
cost that only depends on the number of nonzeros in the tensor. The model also
provides a nice interpretability for the factors; in our model, each factor
corresponds to a &quot;topic&quot;. We develop a set of online inference algorithms that
allow further scaling up the model to massive tensors, for which batch
inference methods may be infeasible. We apply our framework on diverse
real-world applications, such as \emph{multiway} topic modeling on a scientific
publications database, analyzing a political science data set, and analyzing a
massive household transactions data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04221</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04221</id><created>2015-08-18</created><authors><author><keyname>Liu</keyname><forenames>Xuejie</forenames></author><author><keyname>Wang</keyname><forenames>Jingbin</forenames></author><author><keyname>Yin</keyname><forenames>Ming</forenames></author><author><keyname>Edwards</keyname><forenames>Benjamin</forenames></author><author><keyname>Xu</keyname><forenames>Peijuan</forenames></author></authors><title>Supervised learning of sparse context reconstruction coefficients for
  data representation and classification</title><categories>cs.LG cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.00019</comments><doi>10.1007/s00521-015-2042-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context of data points, which is usually defined as the other data points in
a data set, has been found to play important roles in data representation and
classification. In this paper, we study the problem of using context of a data
point for its classification problem. Our work is inspired by the observation
that actually only very few data points are critical in the context of a data
point for its representation and classification. We propose to represent a data
point as the sparse linear combination of its context, and learn the sparse
context in a supervised way to increase its discriminative ability. To this
end, we proposed a novel formulation for context learning, by modeling the
learning of context parameter and classifier in a unified objective, and
optimizing it with an alternative strategy in an iterative algorithm.
Experiments on three benchmark data set show its advantage over
state-of-the-art context-based data representation and classification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04224</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04224</id><created>2015-08-18</created><authors><author><keyname>Wang</keyname><forenames>Jingyan</forenames></author><author><keyname>Zhou</keyname><forenames>Yihua</forenames></author><author><keyname>Wang</keyname><forenames>Haoxiang</forenames></author><author><keyname>Yang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Yang</keyname><forenames>Feng</forenames></author><author><keyname>Peterson</keyname><forenames>Austin</forenames></author></authors><title>Image tag completion by local learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of tag completion is to learn the missing tags of an image. In
this paper, we propose to learn a tag scoring vector for each image by local
linear learning. A local linear function is used in the neighborhood of each
image to predict the tag scoring vectors of its neighboring images. We
construct a unified objective function for the learning of both tag scoring
vectors and local linear function parame- ters. In the objective, we impose the
learned tag scoring vectors to be consistent with the known associations to the
tags of each image, and also minimize the prediction error of each local linear
function, while reducing the complexity of each local function. The objective
function is optimized by an alternate optimization strategy and gradient
descent methods in an iterative algorithm. We compare the proposed algorithm
against different state-of-the-art tag completion methods, and the results show
its advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04227</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04227</id><created>2015-08-18</created><updated>2015-08-19</updated><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Second-Order Region for Gray-Wyner Network</title><categories>cs.IT math.IT</categories><comments>24 pages, 2 figures; some minor modifications in v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coding problem over the Gray-Wyner network is studied from the
second-order coding rates perspective. A tilted information density for this
network is introduced in the spirit of Kostina-Verd\'u, and, under a certain
regularity condition, the second-order region is characterized in terms of the
variance of this tilted information density and the tangent vector of the
first-order region. The second-order region is proved by the type method: the
achievability part is proved by the type-covering argument, and the converse
part is proved by a refinement of the perturbation approach that was used by
Gu-Effros to show the strong converse of the Gray-Wyner network. This is the
first instance that the second-order region is characterized for a
multi-terminal problem where the characterization of the first-order region
involves an auxiliary random variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04228</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04228</id><created>2015-08-18</created><updated>2015-08-26</updated><authors><author><keyname>Kim</keyname><forenames>Hyeji</forenames></author><author><keyname>Nachman</keyname><forenames>Benjamin</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Superposition Coding is Almost Always Optimal for the Poisson Broadcast
  Channel</title><categories>cs.IT math.IT</categories><comments>17 pages, 11 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the capacity region of the continuous-time Poisson
broadcast channel is achieved via superposition coding for most channel
parameter values. Interestingly, the channel in some subset of these parameter
values does not belong to any of the existing classes of broadcast channels for
which superposition coding is optimal (e.g., degraded, less noisy, more
capable). In particular, we introduce the notion of effectively less noisy
broadcast channel and show that it implies less noisy but is not in general
implied by more capable. For the rest of the channel parameter values, we show
that there is a gap between Marton's inner bound and the UV outer bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04232</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04232</id><created>2015-08-18</created><authors><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author></authors><title>Differential Positivity on Compact Sets</title><categories>cs.SY math.DS</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies differentially positive systems, that is, systems whose
linearization along an arbitrary trajectory is positive. We illustrate the use
of differential positivity on compact forward invariant sets for the
characterization of bistable and periodic behaviors. Geometric conditions for
differential positivity are provided. The introduction of compact sets
simplifies the use of differential positivity in applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04234</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04234</id><created>2015-08-18</created><authors><author><keyname>Castaneda</keyname><forenames>Armando</forenames></author><author><keyname>Dolev</keyname><forenames>Danny</forenames></author><author><keyname>Trehan</keyname><forenames>Amitabh</forenames></author></authors><title>Compact Routing Messages in Self-Healing Trees</title><categories>cs.DC cs.DS cs.NI</categories><comments>Under Submission</comments><acm-class>E.1; H.3.4; C.2.1; C.2.4; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing compact routing schemes, e.g., Thorup and Zwick [SPAA 2001] and
Chechik [PODC 2013], often have no means to tolerate failures, once the system
has been setup and started. This paper presents, to our knowledge, the first
self-healing compact routing scheme. Besides, our schemes are developed for low
memory nodes, i.e., nodes need only $O(\log^2 n)$ memory, and are thus, compact
schemes.
  We introduce two algorithms of independent interest: The first is CompactFT,
a novel compact version (using only $O(\log n)$ local memory) of the
self-healing algorithm Forgiving Tree of Hayes et al. [PODC 2008]. The second
algorithm (CompactFTZ) combines CompactFT with Thorup-Zwick's tree-based
compact routing scheme [SPAA 2001] to produce a fully compact self-healing
routing scheme. In the self-healing model, the adversary deletes nodes one at a
time with the affected nodes self-healing locally by adding few edges.
CompactFT recovers from each attack in only $O(1)$ time and $\Delta$ messages,
with only +3 degree increase and $O(log \Delta)$ graph diameter increase, over
any sequence of deletions ($\Delta$ is the initial maximum degree).
  Additionally, CompactFTZ guarantees delivery of a packet sent from sender s
as long as the receiver t has not been deleted, with only an additional $O(y
\log \Delta)$ latency, where $y$ is the number of nodes that have been deleted
on the path between $s$ and $t$. If $t$ has been deleted, $s$ gets informed and
the packet removed from the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04238</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04238</id><created>2015-08-18</created><authors><author><keyname>Zhang</keyname><forenames>Xiaolei</forenames></author><author><keyname>Han</keyname><forenames>Yong</forenames></author><author><keyname>Hao</keyname><forenames>DongSheng</forenames></author><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author></authors><title>Preprint ARPPS Augmented Reality Pipeline Prospect System</title><categories>cs.CV</categories><comments>This is the preprint version of our paper on ICONIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04245</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04245</id><created>2015-08-18</created><authors><author><keyname>Lehrenfeld</keyname><forenames>Christoph</forenames></author><author><keyname>Sch&#xf6;berl</keyname><forenames>Joachim</forenames></author></authors><title>High order exactly divergence-free Hybrid Discontinuous Galerkin Methods
  for unsteady incompressible flows</title><categories>math.NA cs.CE cs.NA</categories><comments>21 pages, 3 figures, 4 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an efficient discretization method for the solution
of the unsteady incompressible Navier-Stokes equations based on a high order
(Hybrid) Discontinuous Galerkin formulation. The crucial component for the
efficiency of the discretization method is the disctinction between stiff
linear parts and less stiff non-linear parts with respect to their temporal and
spatial treatment. Exploiting the flexibility of operator-splitting time
integration schemes we combine two spatial discretizations which are tailored
for two simpler sub-problems: a corresponding hyperbolic transport problem and
an unsteady Stokes problem. For the hyperbolic transport problem a spatial
discretization with an Upwind Discontinuous Galerkin method and an explicit
treatment in the time integration scheme is rather natural and allows for an
efficient implementation. The treatment of the Stokes part involves the
solution of linear systems. In this case a discretization with Hybrid
Discontinuous Galerkin methods is better suited. We consider such a
discretization for the Stokes part with two important features:
H(div)-conforming finite elements to garantuee exactly divergence-free velocity
solutions and a projection operator which reduces the number of globally
coupled unknowns. We present the method, discuss implementational aspects and
demonstrate the performance on two and three dimensional benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04250</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04250</id><created>2015-08-18</created><authors><author><keyname>Fadaei</keyname><forenames>Salman</forenames></author></authors><title>Mechanism Design via Dantzig-Wolfe Decomposition</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In random allocation rules, typically first an optimal fractional point is
calculated via solving a linear program. Lying in the feasible region of the
linear program, the fractional point satisfies the underlying constraints. In
effect, the point represents a fractional assignment of objects or more
generally packages of objects to agents. In order to implement an expected
assignment, one must decompose the point into integer solutions, each
satisfying underlying constraints. The resulting convex combination can then be
viewed as a probability distribution over feasible assignments out of which a
random assignment can be sampled. This approach has been successfully employed
in combinatorial optimization \cite{carr2000randomized} as well as mechanism
design with or without money
\cite{Lavi11,budish2013designing,nguyen2015assignment}.
  In this paper, we show that both finding the optimal fractional point as well
as its decomposition into integer solutions can be done at once. We propose an
appropriate linear program which provides the desired solution. We show that
the linear program can be solved via Dantzig-Wolfe decomposition. The resulting
convex decomposition is tight in terms of the number of integer points
according to Carath{\'e}odory theorem. Dantzig-Wolfe decomposition is a direct
implementation of the revised simplex method which is well known to be highly
efficient in practice.
  The method can also find a decomposition into integer solutions when the
fractional point is readily present perhaps as an outcome of other algorithms
rather than linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04257</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04257</id><created>2015-08-18</created><updated>2015-12-30</updated><authors><author><keyname>Yin</keyname><forenames>Wenpeng</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author></authors><title>Learning Meta-Embeddings by Using Ensembles of Embedding Sets</title><categories>cs.CL</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word embeddings -- distributed representations of words -- in deep learning
are beneficial for many tasks in natural language processing (NLP). However,
different embedding sets vary greatly in quality and characteristics of the
captured semantics. Instead of relying on a more advanced algorithm for
embedding learning, this paper proposes an ensemble approach of combining
different public embedding sets with the aim of learning meta-embeddings.
Experiments on word similarity and analogy tasks and on part-of-speech tagging
show better performance of meta-embeddings compared to individual embedding
sets. One advantage of meta-embeddings is the increased vocabulary coverage. We
will release our meta-embeddings publicly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04261</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04261</id><created>2015-08-18</created><updated>2015-08-31</updated><authors><author><keyname>Campigotto</keyname><forenames>Paolo</forenames></author><author><keyname>Battiti</keyname><forenames>Roberto</forenames></author><author><keyname>Passerini</keyname><forenames>Andrea</forenames></author></authors><title>Learning Modulo Theories for preference elicitation in hybrid domains</title><categories>cs.AI</categories><comments>50 pages, 3 figures, submitted to Artificial Intelligence Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces CLEO, a novel preference elicitation algorithm capable
of recommending complex objects in hybrid domains, characterized by both
discrete and continuous attributes and constraints defined over them. The
algorithm assumes minimal initial information, i.e., a set of catalog
attributes, and defines decisional features as logic formulae combining Boolean
and algebraic constraints over the attributes. The (unknown) utility of the
decision maker (DM) is modelled as a weighted combination of features. CLEO
iteratively alternates a preference elicitation step, where pairs of candidate
solutions are selected based on the current utility model, and a refinement
step where the utility is refined by incorporating the feedback received. The
elicitation step leverages a Max-SMT solver to return optimal hybrid solutions
according to the current utility model. The refinement step is implemented as
learning to rank, and a sparsifying norm is used to favour the selection of few
informative features in the combinatorial space of candidate decisional
features.
  CLEO is the first preference elicitation algorithm capable of dealing with
hybrid domains, thanks to the use of Max-SMT technology, while retaining
uncertainty in the DM utility and noisy feedback. Experimental results on
complex recommendation tasks show the ability of CLEO to quickly focus towards
optimal solutions, as well as its capacity to recover from suboptimal initial
choices. While no competitors exist in the hybrid setting, CLEO outperforms a
state-of-the-art Bayesian preference elicitation algorithm when applied to a
purely discrete task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04265</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04265</id><created>2015-08-18</created><authors><author><keyname>Choudhury</keyname><forenames>Neel</forenames></author><author><keyname>Dindokar</keyname><forenames>Ravikant</forenames></author><author><keyname>Dixit</keyname><forenames>Akshay</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author></authors><title>Partitioning Strategies for Load Balancing Subgraph-centric Distributed
  Graph Processing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed graph processing platforms have helped emerging application
domains use commodity clusters and Clouds to analyze large graphs.
Vertex-centric programming models like Google Pregel, and their
subgraph-centric variants, specify data-parallel application logic for a single
vertex or component that execute iteratively. The locality and balancing of
components within partitions affects the performance of such platforms. We
propose three partitioning strategies for a subgraph-centric model, and analyze
their impact on CPU utilization, communication, iterations, and makespan. We
analyze these using Breadth First Search and PageRank algorithms on powerlaw
and spatio-planar graphs. They are validated on a commodity cluster using our
GoFFish subgraph-centric platform, and compared against Apache Giraph
vertex-centric platform. Our experiments show upto 8 times improvement in
utilization resulting to upto 5 times improvement of overall makespan for flat
and hierarchical partitioning over the default strategy due to improved machine
utilization. Further, these also exhibit better horizontal scalability relative
to Giraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04271</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04271</id><created>2015-08-18</created><authors><author><keyname>Botha</keyname><forenames>Jan A.</forenames></author></authors><title>Probabilistic Modelling of Morphologically Rich Languages</title><categories>cs.CL</categories><comments>DPhil thesis, University of Oxford, submitted and accepted 2014.
  http://ora.ox.ac.uk/objects/uuid:8df7324f-d3b8-47a1-8b0b-3a6feb5f45c7</comments><acm-class>I.2.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis investigates how the sub-structure of words can be accounted for
in probabilistic models of language. Such models play an important role in
natural language processing tasks such as translation or speech recognition,
but often rely on the simplistic assumption that words are opaque symbols. This
assumption does not fit morphologically complex language well, where words can
have rich internal structure and sub-word elements are shared across distinct
word forms.
  Our approach is to encode basic notions of morphology into the assumptions of
three different types of language models, with the intention that leveraging
shared sub-word structure can improve model performance and help overcome data
sparsity that arises from morphological processes.
  In the context of n-gram language modelling, we formulate a new Bayesian
model that relies on the decomposition of compound words to attain better
smoothing, and we develop a new distributed language model that learns vector
representations of morphemes and leverages them to link together
morphologically related words. In both cases, we show that accounting for word
sub-structure improves the models' intrinsic performance and provides benefits
when applied to other tasks, including machine translation.
  We then shift the focus beyond the modelling of word sequences and consider
models that automatically learn what the sub-word elements of a given language
are, given an unannotated list of words. We formulate a novel model that can
learn discontiguous morphemes in addition to the more conventional contiguous
morphemes that most previous models are limited to. This approach is
demonstrated on Semitic languages, and we find that modelling discontiguous
sub-word structures leads to improvements in the task of segmenting words into
their contiguous morphemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04278</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04278</id><created>2015-08-18</created><authors><author><keyname>Fuchs</keyname><forenames>Fabian</forenames></author><author><keyname>Wolf</keyname><forenames>Matthias</forenames></author></authors><title>On the Distributed Computation of Fractional Connected Dominating Set
  Packings</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most fundamental problems in wireless networks is to achieve high
throughput. Fractional Connected Dominating Set (FCDS) Packings can achieve a
throughput of ${\Theta}(k/\log n)$ messages for networks with node connectivity
$k$, which is optimal regarding routing-based message transmission. FCDS were
proposed by Censor-Hillel \emph{et al.} [SODA'14,PODC'14] and are a natural
generalization to Connected Dominating Sets (CDS), allowing each node to
participate with a fraction of its weight in multiple FCDS. Thus, $\Omega(k)$
co-existing transmission backbones are established, taking full advantage of
the networks connectivity. We propose a modified distributed algorithm that
improves upon previous algorithms for $k\Delta \in o(\min\{\frac{n \log n}{k}
,D,\sqrt{n \log n} \log^* n\}\log n)$, where $\Delta$ is the maximum node
degree, $D$ the diameter and $n$ the number of nodes in the network. We achieve
this by explicitly computing connections between tentative dominating sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04286</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04286</id><created>2015-08-18</created><authors><author><keyname>Filippou</keyname><forenames>Miltiades C.</forenames></author><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharmalingam</forenames></author><author><keyname>Pastore</keyname><forenames>Adriano</forenames></author><author><keyname>Ropokis</keyname><forenames>George A.</forenames></author></authors><title>Coordinated Shared Spectrum Precoding with Distributed CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the operation of a Licensed Shared Access (LSA) system is
investigated, considering downlink communication. The system comprises of a
Multiple-Input-Single-Output (MISO) incumbent transmitter (TX) - receiver (RX)
pair, which offers a spectrum sharing opportunity to a MISO licensee TX-RX
pair. Our main contribution is the design of a coordinated transmission scheme,
inspired by the underlay Cognitive Radio (CR) approach, with the aim of
maximizing the average rate of the licensee, subject to an average rate
constraint for the incumbent. In contrast to most prior works on underlay CR,
the coordination of the two TXs takes place under a realistic Channel State
Information (CSI) scenario, where each TX has sole access to the instantaneous
direct channel of its served terminal. Such a CSI knowledge setting brings
about a formulation based on the theory of Team Decisions, whereby the TXs aim
at optimizing a common objective given the same constraint set, on the basis of
individual channel information. Consequently, a novel set of applicable
precoding schemes is proposed. Relying on statistical coordination criteria,
the two TXs cooperate in the lack of any instantaneous CSI exchange. We verify
by simulations that our novel coordinated precoding scheme outperforms the
standard underlay CR approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04294</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04294</id><created>2015-08-18</created><authors><author><keyname>Ji</keyname><forenames>Shenggong</forenames></author><author><keyname>Lu</keyname><forenames>Linyuan</forenames></author><author><keyname>Yeung</keyname><forenames>Chi Ho</forenames></author><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author></authors><title>Effective spreading from multiple leaders identified by percolation in
  social networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 4 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks constitute a new platform for information propagation, but
its success is crucially dependent on the choice of spreaders who initiate the
spreading of information. In this paper, we remove edges in a network at random
and the network segments into isolated clusters. The most important nodes in
each cluster then form a group of influential spreaders, such that news
propagating from them would lead to an extensive coverage and minimal
redundancy. The method well utilizes the similarities between the
pre-percolated state and the coverage of information propagation in each social
cluster to obtain a set of distributed and coordinated spreaders. Our tests on
the Facebook networks show that this method outperforms conventional methods
based on centrality. The suggested way of identifying influential spreaders
thus sheds light on a new paradigm of information propagation on social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04306</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04306</id><created>2015-08-18</created><authors><author><keyname>Hershey</keyname><forenames>John R.</forenames></author><author><keyname>Chen</keyname><forenames>Zhuo</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author></authors><title>Deep clustering: Discriminative embeddings for segmentation and
  separation</title><categories>cs.NE cs.LG stat.ML</categories><comments>Originally submitted on June 5, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of acoustic source separation in a deep learning
framework we call &quot;deep clustering.&quot; Rather than directly estimating signals or
masking functions, we train a deep network to produce spectrogram embeddings
that are discriminative for partition labels given in training data. Previous
deep network approaches provide great advantages in terms of learning power and
speed, but previously it has been unclear how to use them to separate signals
in a class-independent way. In contrast, spectral clustering approaches are
flexible with respect to the classes and number of items to be segmented, but
it has been unclear how to leverage the learning power and speed of deep
networks. To obtain the best of both worlds, we use an objective function that
to train embeddings that yield a low-rank approximation to an ideal pairwise
affinity matrix, in a class-independent way. This avoids the high cost of
spectral factorization and instead produces compact clusters that are amenable
to simple clustering methods. The segmentations are therefore implicitly
encoded in the embeddings, and can be &quot;decoded&quot; by clustering. Preliminary
experiments show that the proposed method can separate speech: when trained on
spectrogram features containing mixtures of two speakers, and tested on
mixtures of a held-out set of speakers, it can infer masking functions that
improve signal quality by around 6dB. We show that the model can generalize to
three-speaker mixtures despite training only on two-speaker mixtures. The
framework can be used without class labels, and therefore has the potential to
be trained on a diverse set of sound types, and to generalize to novel sources.
We hope that future work will lead to segmentation of arbitrary sounds, with
extensions to microphone array methods as well as image segmentation and other
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04310</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04310</id><created>2015-08-18</created><authors><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author></authors><title>A State-Dependent Updating Period For Certified Real-Time Model
  Predictive Control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a state-dependent control updating period framework is
proposed that leads to real-time implementable Model Predictive Control with
certified practical stability results and constraints satisfaction. The scheme
is illustrated and validated using new certification bound that is derived in
the case where the Fast Gradient iteration is used through a penalty method to
solve generally constrained convex optimization problems. Both the
certification bound computation and its use in the state-dependent updating
period framework are illustrated in the particular case of linear MPC. An
illustrative example involving a chain of four integrators is used to show the
explicit computation of the state-dependent control updating scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04324</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04324</id><created>2015-08-18</created><updated>2016-01-07</updated><authors><author><keyname>Mladenov</keyname><forenames>Vladislav</forenames></author><author><keyname>Mainka</keyname><forenames>Christian</forenames></author><author><keyname>Schwenk</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>On the security of modern Single Sign-On Protocols: Second-Order
  Vulnerabilities in OpenID Connect</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OAuth is the new de facto standard for delegating authorization in the web.
An important limitation of OAuth is the fact that it was designed for
authorization and not for authentication. The usage of OAuth for authentication
thus leads to serious vulnerabilities as shown by Zhou et. al. in [44] and Chen
et. al. in [9]. OpenID Connect was created on top of OAuth to fill this gap by
providing federated identity management and user authentication. OpenID Connect
was standardized in February 2014, but leading companies like Google,
Microsoft, AOL and PayPal are already using it in their web applications [1],
[2], [3], [30].
  In this paper we describe the OpenID Connect protocol and provide the first
in-depth analysis of one of the key features of OpenID Connect: the Discovery
and the Dynamic Registration extensions.We present a new class of attacks on
OpenID Connect that belong to the category of second-order vulnerabilities.
These attacks consist of two phases: First, the injection payload is stored by
the legitimate application. Later on, this payload is used in a
security-critical operation. Our new class of attacks - called Malicious
Endpoints attacks - exploits the OpenID Connect extensions Discovery and
Dynamic Registration. These attacks break user authentication, compromise user
privacy, and enable Server Side Request Forgery (SSRF), client-side code
injection, and Denial-of-Service (DoS). As a result, the security of the OpenID
Connect protocol cannot be guaranteed when these extensions are enabled in
their present form.
  We contacted the authors of the OpenID Connect and OAuth specifications. They
acknowledged our Malicious Endpoint attacks and recognized the need to improve
the specification [29]. We are currently involved in the discussion regarding
the mitigation of the existing issues and an extension to the OAuth
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04326</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04326</id><created>2015-08-18</created><authors><author><keyname>Pang</keyname><forenames>Yanwei</forenames></author><author><keyname>Cao</keyname><forenames>Jiale</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Cascade Learning by Optimally Partitioning</title><categories>cs.CV cs.LG</categories><comments>17 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascaded AdaBoost classifier is a well-known efficient object detection
algorithm. The cascade structure has many parameters to be determined. Most of
existing cascade learning algorithms are designed by assigning detection rate
and false positive rate to each stage either dynamically or statically. Their
objective functions are not directly related to minimum computation cost. These
algorithms are not guaranteed to have optimal solution in the sense of
minimizing computation cost. On the assumption that a strong classifier is
given, in this paper we propose an optimal cascade learning algorithm (we call
it iCascade) which iteratively partitions the strong classifiers into two parts
until predefined number of stages are generated. iCascade searches the optimal
number ri of weak classifiers of each stage i by directly minimizing the
computation cost of the cascade. Theorems are provided to guarantee the
existence of the unique optimal solution. Theorems are also given for the
proposed efficient algorithm of searching optimal parameters ri. Once a new
stage is added, the parameter ri for each stage decreases gradually as
iteration proceeds, which we call decreasing phenomenon. Moreover, with the
goal of minimizing computation cost, we develop an effective algorithm for
setting the optimal threshold of each stage classifier. In addition, we prove
in theory why more new weak classifiers are required compared to the last
stage. Experimental results on face detection demonstrate the effectiveness and
efficiency of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04333</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04333</id><created>2015-08-18</created><authors><author><keyname>Mondal</keyname><forenames>Shouvick</forenames></author><author><keyname>Banerjee</keyname><forenames>Arko</forenames></author></authors><title>ESDF: Ensemble Selection using Diversity and Frequency</title><categories>cs.LG</categories><comments>Conference: National Conference on Research Trends in Computer
  Science and Application (NCRTCSA-2014) Date: 8th February 2014 Organized by:
  Dept. of Computer Application, Siliguri Institute of Technology, India In
  Association With: Computer Society of India, Siliguri Chapter Technically
  Sponsored By: IEEE, Kolkata Section Paper Id: NCRTCSA118. Shouvick Mondal et
  al.; ESDF: Ensemble Selection using Diversity and Frequency; Proceedings of
  NCRTCSA 2014; pp. 28-33, 2014</comments><msc-class>62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently ensemble selection for consensus clustering has emerged as a
research problem in Machine Intelligence. Normally consensus clustering
algorithms take into account the entire ensemble of clustering, where there is
a tendency of generating a very large size ensemble before computing its
consensus. One can avoid considering the entire ensemble and can judiciously
select few partitions in the ensemble without compromising on the quality of
the consensus. This may result in an efficient consensus computation technique
and may save unnecessary computational overheads. The ensemble selection
problem addresses this issue of consensus clustering. In this paper, we propose
an efficient method of ensemble selection for a large ensemble. We prioritize
the partitions in the ensemble based on diversity and frequency. Our method
selects top K of the partitions in order of priority, where K is decided by the
user. We observe that considering jointly the diversity and frequency helps in
identifying few representative partitions whose consensus is qualitatively
better than the consensus of the entire ensemble. Experimental analysis on a
large number of datasets shows our method gives better results than earlier
ensemble selection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04347</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04347</id><created>2015-08-18</created><authors><author><keyname>Renaville</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Branse</keyname><forenames>Yosef</forenames></author><author><keyname>Chen</keyname><forenames>Xiaotian</forenames></author><author><keyname>Needleman</keyname><forenames>Mark</forenames></author></authors><title>SFX Miscellaneous Free Ejournals Target: Usage Survey among the SFX
  Community</title><categories>cs.DL</categories><journal-ref>Serials Review 41 (2015) 58-68</journal-ref><doi>10.1080/00987913.2015.1031317</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The number of free or open access articles is increasing rapidly, and their
retrieval with library indexes and OpenURL link resolvers has been a challenge.
In June 2014, the SFX MISCELLANEOUS FREE EJOURNALS target contained more than
24,000 portfolios of all kinds. The SFX Knowledge Base Advisory Board (KBAB)
carried out an international survey to get an overview of the usage of this
target by the SFX community and to precisely identify what could be done to
improve it. The target is widely used among the community. However, many
respondents complained about three major problems: (a) incorrect links, (b)
full texts actually not free, and (c) incorrect or missing thresholds (years
and volumes information).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04359</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04359</id><created>2015-08-18</created><updated>2015-10-03</updated><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Informational Bottlenecks in Two-Unicast Wireless Networks with Delayed
  CSIT</title><categories>cs.IT math.IT</categories><comments>In proceedings of the 53rd Annual Allerton Conference on
  Communication, Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the impact of delayed channel state information at the transmitters
(CSIT) in two-unicast wireless networks with a layered topology and arbitrary
connectivity. We introduce a technique to obtain outer bounds to the
degrees-of-freedom (DoF) region through the new graph-theoretic notion of
bottleneck nodes. Such nodes act as informational bottlenecks only under the
assumption of delayed CSIT, and imply asymmetric DoF bounds of the form $mD_1 +
D_2 \leq m$. Combining this outer-bound technique with new achievability
schemes, we characterize the sum DoF of a class of two-unicast wireless
networks, which shows that, unlike in the case of instantaneous CSIT, the DoF
of two-unicast networks with delayed CSIT can take an infinite set of values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04360</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04360</id><created>2015-08-18</created><updated>2015-09-25</updated><authors><author><keyname>Metcalfe</keyname><forenames>George</forenames><affiliation>University of Bern</affiliation></author><author><keyname>Cabrer</keyname><forenames>Leonardo</forenames><affiliation>University of Florence</affiliation></author></authors><title>Exact Unification and Admissibility</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.5583</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:23) 2015</journal-ref><doi>10.2168/LMCS-11(3:23)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new hierarchy of &quot;exact&quot; unification types is introduced, motivated by the
study of admissible rules for equational classes and non-classical logics. In
this setting, unifiers of identities in an equational class are preordered, not
by instantiation, but rather by inclusion over the corresponding sets of
unified identities. Minimal complete sets of unifiers under this new
preordering always have a smaller or equal cardinality than those provided by
the standard instantiation preordering, and in significant cases a dramatic
reduction may be observed. In particular, the classes of distributive lattices,
idempotent semigroups, and MV-algebras, which all have nullary unification
type, have unitary or finitary exact type. These results are obtained via an
algebraic interpretation of exact unification, inspired by Ghilardi's algebraic
approach to equational unification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04364</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04364</id><created>2015-08-18</created><authors><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Panayi</keyname><forenames>Efstathios</forenames></author><author><keyname>Chapelle</keyname><forenames>Ariane</forenames></author></authors><title>Trends in crypto-currencies and blockchain technologies: A monetary
  theory and regulation perspective</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The internet era has generated a requirement for low cost, anonymous and
rapidly verifiable transactions to be used for online barter, and fast settling
money have emerged as a consequence. For the most part, e-money has fulfilled
this role, but the last few years have seen two new types of money emerge.
Centralised virtual currencies, usually for the purpose of transacting in
social and gaming economies, and crypto-currencies, which aim to eliminate the
need for financial intermediaries by offering direct peer-to-peer online
payments.
  We describe the historical context which led to the development of these
currencies and some modern and recent trends in their uptake, in terms of both
usage in the real economy and as investment products. As these currencies are
purely digital constructs, with no government or local authority backing, we
then discuss them in the context of monetary theory, in order to determine how
they may be have value under each. Finally, we provide an overview of the state
of regulatory readiness in terms of dealing with transactions in these
currencies in various regions of the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04372</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04372</id><created>2015-08-18</created><authors><author><keyname>Ghayem</keyname><forenames>Fateme</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>A Fast and Efficient Algorithm for Reconstructing MR images From Partial
  Fourier Samples</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of Magnetic Resonance (MR) image reconstruction
from partial Fourier samples has been considered. To this aim, we leverage the
evidence that MR images are sparser than their zero-filled reconstructed ones
from incomplete Fourier samples. This information can be used to define an
optimization problem which searches for the sparsest possible image conforming
with the available Fourier samples. We solve the resulting problem using the
well-known Alternating Direction Method of Multipliers (ADMM). Unlike most
existing methods that work with small over-lapping image patches, the proposed
algorithm considers the whole image without dividing it into small blocks.
Experimental results prominently confirm its promising performance and
advantages over the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04388</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04388</id><created>2015-08-18</created><updated>2016-01-07</updated><authors><author><keyname>Perotti</keyname><forenames>Juan Ignacio</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author></authors><title>Hierarchical mutual information for the comparison of hierarchical
  community structures in complex networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI math-ph math.MP physics.bio-ph</categories><comments>14 pages and 12 figures</comments><journal-ref>Phys. Rev. E 92, 062825 (2015)</journal-ref><doi>10.1103/PhysRevE.92.062825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quest for a quantitative characterization of community and modular
structure of complex networks produced a variety of methods and algorithms to
classify different networks. However, it is not clear if such methods provide
consistent, robust and meaningful results when considering hierarchies as a
whole. Part of the problem is the lack of a similarity measure for the
comparison of hierarchical community structures. In this work we give a
contribution by introducing the {\it hierarchical mutual information}, which is
a generalization of the traditional mutual information, and allows to compare
hierarchical partitions and hierarchical community structures. The {\it
normalized} version of the hierarchical mutual information should behave
analogously to the traditional normalized mutual information. Here, the correct
behavior of the hierarchical mutual information is corroborated on an extensive
battery of numerical experiments. The experiments are performed on artificial
hierarchies, and on the hierarchical community structure of artificial and
empirical networks. Furthermore, the experiments illustrate some of the
practical applications of the hierarchical mutual information. Namely, the
comparison of different community detection methods, and the study of the
consistency, robustness and temporal evolution of the hierarchical modular
structure of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04389</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04389</id><created>2015-08-18</created><authors><author><keyname>Ranjan</keyname><forenames>Rajeev</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>A Deep Pyramid Deformable Part Model for Face Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a face detection algorithm based on Deformable Part Models and
deep pyramidal features. The proposed method called DP2MFD is able to detect
faces of various sizes and poses in unconstrained conditions. It reduces the
gap in training and testing of DPM on deep features by adding a normalization
layer to the deep convolutional neural network (CNN). Extensive experiments on
four publicly available unconstrained face detection datasets show that our
method is able to capture the meaningful structure of faces and performs
significantly better than many competitive face detection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04390</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04390</id><created>2015-08-18</created><authors><author><keyname>Daum</keyname><forenames>Sebastian</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>Tight Bounds for MIS in Multichannel Radio Networks</title><categories>cs.DC</categories><comments>37 pages, to be published in DISC 2015</comments><acm-class>C.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Daum et al. [PODC'13] presented an algorithm that computes a maximal
independent set (MIS) within $O(\log^2 n/F+\log n \mathrm{polyloglog} n)$
rounds in an $n$-node multichannel radio network with $F$ communication
channels. The paper uses a multichannel variant of the standard graph-based
radio network model without collision detection and it assumes that the network
graph is a polynomially bounded independence graph (BIG), a natural
combinatorial generalization of well-known geographic families. The upper bound
of that paper is known to be optimal up to a polyloglog factor.
  In this paper, we adapt algorithm and analysis to improve the result in two
ways. Mainly, we get rid of the polyloglog factor in the runtime and we thus
obtain an asymptotically optimal multichannel radio network MIS algorithm. In
addition, our new analysis allows to generalize the class of graphs from those
with polynomially bounded local independence to graphs where the local
independence is bounded by an arbitrary function of the neighborhood radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04395</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04395</id><created>2015-08-18</created><authors><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Chorowski</keyname><forenames>Jan</forenames></author><author><keyname>Serdyuk</keyname><forenames>Dmitriy</forenames></author><author><keyname>Brakel</keyname><forenames>Philemon</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>End-to-End Attention-based Large Vocabulary Speech Recognition</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04403</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04403</id><created>2015-08-18</created><authors><author><keyname>Dalchau</keyname><forenames>Neil</forenames></author><author><keyname>Murphy</keyname><forenames>Niall</forenames></author><author><keyname>Petersen</keyname><forenames>Rasmus</forenames></author><author><keyname>Yordanov</keyname><forenames>Boyan</forenames></author></authors><title>Synthesizing and tuning chemical reaction networks with specified
  behaviours</title><categories>cs.ET</categories><comments>17 pages, 6 figures, appeared the proceedings of the 21st conference
  on DNA Computing and Molecular Programming, 2015</comments><msc-class>68U99</msc-class><acm-class>C.4; F.3.1; G.1.6; I.2.2</acm-class><journal-ref>LNCS 9211 pp 16-33 2015</journal-ref><doi>10.1007/978-3-319-21999-8_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how to generate chemical reaction networks (CRNs) from functional
specifications. We propose a two-stage approach that combines synthesis by
satisfiability modulo theories and Markov chain Monte Carlo based optimisation.
First, we identify candidate CRNs that have the possibility to produce correct
computations for a given finite set of inputs. We then optimise the reaction
rates of each CRN using a combination of stochastic search techniques applied
to the chemical master equation, simultaneously improving the of correct
behaviour and ruling out spurious solutions. In addition, we use techniques
from continuous time Markov chain theory to study the expected termination time
for each CRN. We illustrate our approach by identifying CRNs for majority
decision-making and division computation, which includes the identification of
both known and unknown networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04405</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04405</id><created>2015-08-18</created><updated>2015-09-03</updated><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Stabilization of discrete-time piecewise affine systems with quantized
  signals</title><categories>cs.SY</categories><comments>To be presented at IEEE CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies quantized control for discrete-time piecewise affine
systems. For given stabilizing feedback controllers, we propose an encoding
strategy for local stability. If the quantized state is near the boundaries of
quantization regions, then the controller can recompute a better quantization
value. For the design of quantized feedback controllers, we also consider the
stabilization of piecewise affine systems with bounded disturbances. In order
to derive a less conservative design method with low computational cost, we
investigate a region to which the state belong in the next step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04417</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04417</id><created>2015-08-18</created><authors><author><keyname>Wang</keyname><forenames>Shiyao</forenames></author><author><keyname>Yang</keyname><forenames>Tianbo</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Zhao</keyname><forenames>Kang</forenames></author></authors><title>Concurrent diffusion of information and behaviors in online social
  networks -- a case study of the Ice Bucket Challenge</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the spread of the Ice Bucket Challenge on Twitter as a case study, this
research compared the concurrent diffusion patterns of both information and
behaviors in online social networks. Individual behaviors in taking the
Challenge were detected by applying text mining techniques to millions of
tweets. After comparing diffusion dynamics of information and behaviors at the
network level, the individual level, and the dyadic level, our analysis
revealed interesting differences and interactions between the two diffusion
processes and laid foundations for future predictive and prescriptive analytics
for information and behavior diffusion in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04422</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04422</id><created>2015-08-18</created><authors><author><keyname>Jansen</keyname><forenames>Aren</forenames></author><author><keyname>Sell</keyname><forenames>Gregory</forenames></author><author><keyname>Lyzinski</keyname><forenames>Vince</forenames></author></authors><title>Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural
  Networks</title><categories>stat.ML cs.LG cs.NE stat.ME</categories><comments>6 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several popular graph embedding techniques for representation learning and
dimensionality reduction rely on performing computationally expensive
eigendecompositions to derive a nonlinear transformation of the input data
space. The resulting eigenvectors encode the embedding coordinates for the
training samples only, preventing the transformation of novel data samples
without recomputation. In this paper, we present a method for out-of sample
extension of graph embeddings that uses deep neural networks (DNN) to
parametrically approximate these nonlinear maps. Compared with traditional
nonparametric out-of-sample extension methods, we demonstrate that the DNNs can
generalize with equal or better fidelity and require orders of magnitude less
computation at test time. Moreover, we find that unsupervised pretraining of
the DNNs improves optimization for larger network sizes, thus removing
sensitivity to model selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04428</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04428</id><created>2015-08-18</created><authors><author><keyname>Brunner</keyname><forenames>Andreas B. M.</forenames></author><author><keyname>Lewitzka</keyname><forenames>Steffen</forenames></author></authors><title>Topological representation of intuitionistic and distributive abstract
  logics</title><categories>cs.LO math.LO</categories><comments>19 pages. The results of this article were presented in a session at
  the XVI. Brazilian Logic Conference EBL in Petr\'opolis, Brazil, in 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue work of our earlier paper (Lewitzka and Brunner: Minimally
generated abstract logics, Logica Universalis 3(2), 2009), where abstract
logics and particularly intuitionistic abstract logics are studied. Abstract
logics can be topologized in a direct and natural way. This facilitates a
topological study of classes of concrete logics whenever they are given in
abstract form. Moreover, such a direct topological approach avoids the often
complex algebraic and lattice-theoretic machinery usually applied to represent
logics. Motivated by that point of view, we define in this paper the category
of intuitionistic abstract logics with stable logic maps as morphisms, and the
category of implicative spectral spaces with spectral maps as morphisms. We
show the equivalence of these categories and conclude that the larger
categories of distributive abstract logics and distributive sober spaces are
equivalent, too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04447</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04447</id><created>2015-08-18</created><authors><author><keyname>Soltan</keyname><forenames>Saleh</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Generation of Synthetic Spatially Embedded Power Grid Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of algorithms for enhancing the resilience and efficiency of
the power grid requires performance evaluation with real topologies of power
transmission networks. However, due to security reasons, such topologies and
particularly the locations of the substations and the lines are usually not
publicly available. Therefore, we study the structural properties of the North
American grids and present an algorithm for generating synthetic spatially
embedded networks with similar properties to a given grid. The algorithm uses
the Gaussian Mixture Model (GMM) for density estimation of the node positions
and generates a set of nodes with similar spatial distribution to the nodes in
a given network. Then, it uses two procedures, which are inspired by the
historical evolution of the grids, to connect the nodes. The algorithm has
several tunable parameters that allow generating grids similar to any given
grid. Particularly, we apply it to the Western Interconnection (WI) and to
grids that operate under the SERC Reliability Corporation (SERC) and the
Florida Reliability Coordinating Council (FRCC), and show that it generates
grids with similar structural and spatial properties to these grids. To the
best of our knowledge, this is the first attempt to consider the spatial
distribution of the nodes and lines and its importance in generating synthetic
power grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04451</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04451</id><created>2015-08-16</created><authors><author><keyname>Morita</keyname><forenames>Satoru</forenames></author></authors><title>Six Susceptible-Infected-Susceptible Models on Scale-free Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO q-bio.PE</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreading phenomena are ubiquitous in nature and society. For example,
disease, rumor, and information spread over underlying social and information
networks. It is well known that there is no threshold for epidemic models on
scale-free networks; this suggests that disease can spread on such networks,
regardless of how low the contact rate may be. In this paper, I consider six
models with different contact and propagation mechanisms. Each model is
analyzed by degree-based mean-field theory. I show that the presence or absence
of an outbreak threshold depends on the contact and propagation mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04458</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04458</id><created>2015-06-24</created><authors><author><keyname>Degirmenci</keyname><forenames>S.</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Joseph A.</forenames></author><author><keyname>Politte</keyname><forenames>David G.</forenames></author></authors><title>Multiresolution Approach to Acceleration of Iterative Image
  Reconstruction for X-Ray Imaging for Security Applications</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-dimensional x-ray CT image reconstruction in baggage scanning in
security applications is an important research field. The variety of materials
to be reconstructed is broader than medical x-ray imaging. Presence of high
attenuating materials such as metal may cause artifacts if analytical
reconstruction methods are used. Statistical modeling and the resultant
iterative algorithms are known to reduce these artifacts and present good
quantitative accuracy in estimates of linear attenuation coefficients. However,
iterative algorithms may require computations in order to achieve
quantitatively accurate results. For the case of baggage scanning, in order to
provide fast accurate inspection throughput, they must be accelerated
drastically. There are many approaches proposed in the literature to increase
speed of convergence. This paper presents a new method that estimates the
wavelet coefficients of the images in the discrete wavelet transform domain
instead of the image space itself. Initially, surrogate functions are created
around approximation coefficients only. As the iterations proceed, the wavelet
tree on which the updates are made is expanded based on a criterion and detail
coefficients at each level are updated and the tree is expanded this way. For
example, in the smooth regions of the image the detail coefficients are not
updated while the coefficients that represent the high-frequency component
around edges are being updated, thus saving time by focusing computations where
they are needed. This approach is implemented on real data from a SureScan (TM)
x1000 Explosive Detection System and compared to straightforward implementation
of the unregularized alternating minimization of O'Sullivan and Benac [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04465</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04465</id><created>2015-08-18</created><authors><author><keyname>Valadares</keyname><forenames>Arthur</forenames></author><author><keyname>Gabrielova</keyname><forenames>Eugenia</forenames></author><author><keyname>Lopes</keyname><forenames>Cristina V.</forenames></author></authors><title>On Designing and Testing Distributed Virtual Environments</title><categories>cs.SE cs.DC cs.PF</categories><comments>Wiley Journal on Concurrency and Computation: Practice and
  Experience, to appear (preprint)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Real-Time (DRT) systems are among the most complex software
systems to design, test, maintain and evolve. The existence of components
distributed over a network often conflicts with real-time requirements, leading
to design strategies that depend on domain- and even application-specific
knowledge. Distributed Virtual Environment (DVE) systems are DRT systems that
connect multiple users instantly with each other and with a shared virtual
space over a network. DVE systems deviate from traditional DRT systems in the
importance of the quality of the end user experience. We present an analysis of
important, but challenging, issues in the design, testing and evaluation of DVE
systems through the lens of experiments with a concrete DVE, OpenSimulator. We
frame our observations within six dimensions of well-known design concerns:
correctness, fault tolerance/prevention, scalability, time sensitivity,
consistency, and overhead of distribution. Furthermore, we place our
experimental work in a broader historical context, showing that these
challenges are intrinsic to DVEs and suggesting lines of future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04467</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04467</id><created>2015-08-18</created><authors><author><keyname>Kang</keyname><forenames>Zhao</forenames></author><author><keyname>Peng</keyname><forenames>Chong</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author></authors><title>Robust Subspace Clustering via Smoothed Rank Approximation</title><categories>cs.CV cs.IT cs.LG cs.NA math.IT stat.ML</categories><comments>Journal, code is available</comments><journal-ref>IEEE Signal Processing Letters, 22(2015)2088-2092</journal-ref><doi>10.1109/LSP.2015.2460737</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix rank minimizing subject to affine constraints arises in many
application areas, ranging from signal processing to machine learning. Nuclear
norm is a convex relaxation for this problem which can recover the rank exactly
under some restricted and theoretically interesting conditions. However, for
many real-world applications, nuclear norm approximation to the rank function
can only produce a result far from the optimum. To seek a solution of higher
accuracy than the nuclear norm, in this paper, we propose a rank approximation
based on Logarithm-Determinant. We consider using this rank approximation for
subspace clustering application. Our framework can model different kinds of
errors and noise. Effective optimization strategy is developed with theoretical
guarantee to converge to a stationary point. The proposed method gives
promising results on face clustering and motion segmentation tasks compared to
the state-of-the-art subspace clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04485</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04485</id><created>2015-08-18</created><authors><author><keyname>Lee</keyname><forenames>Kangwook</forenames></author><author><keyname>Pedarsani</keyname><forenames>Ramtin</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>SAFFRON: A Fast, Efficient, and Robust Framework for Group Testing based
  on Sparse-Graph Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group testing tackles the problem of identifying a population of $K$
defective items from a set of $n$ items by pooling groups of items efficiently
in order to cut down the number of tests needed. The result of a test for a
group of items is positive if any of the items in the group is defective and
negative otherwise. The goal is to judiciously group subsets of items such that
defective items can be reliably recovered using the minimum number of tests,
while also having a low-complexity decoding procedure.
  We describe SAFFRON (Sparse-grAph codes Framework For gROup testiNg), a
non-adaptive group testing paradigm that recovers at least a
$(1-\epsilon)$-fraction (for any arbitrarily small $\epsilon &gt; 0$) of $K$
defective items with high probability with $m=6C(\epsilon)K\log_2{n}$ tests,
where $C(\epsilon)$ is a precisely characterized constant that depends only on
$\epsilon$. For instance, it can provably recover at least $(1-10^{-6})K$
defective items with $m \simeq 68 K \log_2{n}$ tests. The computational
complexity of the decoding algorithm of SAFFRON is $\mathcal{O}(K\log n)$,
which is order-optimal. Further, we robustify SAFFRON such that it can reliably
recover the set of $K$ defective items even in the presence of erroneous or
noisy test results. We also propose Singleton-Only-SAFFRON, a variant of
SAFFRON, that recovers all the $K$ defective items with $m=2e(1+\alpha)K\log K
\log_2 n$ tests with probability
$1-\mathcal{O}{\left(\frac{1}{K^\alpha}\right)}$, where $\alpha&gt;0$ is a
constant. By leveraging powerful design and analysis tools from modern
sparse-graph coding theory, SAFFRON is the first approach to reliable,
large-scale probabilistic group testing that offers both precisely
characterizable number of tests needed (down to the constants) together with
order-optimal decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04486</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04486</id><created>2015-08-18</created><authors><author><keyname>Subakan</keyname><forenames>Y. Cem</forenames></author><author><keyname>Traa</keyname><forenames>Johannes</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author><author><keyname>Stein</keyname><forenames>Noah</forenames></author></authors><title>A Dictionary Learning Approach for Factorial Gaussian Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a parameter estimation method for factorially
parametrized models such as Factorial Gaussian Mixture Model and Factorial
Hidden Markov Model. Our contributions are two-fold. First, we show that the
emission matrix of the standard Factorial Model is unidentifiable even if the
true assignment matrix is known. Secondly, we address the issue of
identifiability by making a one component sharing assumption and derive a
parameter learning algorithm for this case. Our approach is based on a
dictionary learning problem of the form $X = O R$, where the goal is to learn
the dictionary $O$ given the data matrix $X$. We argue that due to the specific
structure of the activation matrix $R$ in the shared component factorial
mixture model, and an incoherence assumption on the shared component, it is
possible to extract the columns of the $O$ matrix without the need for
alternating between the estimation of $O$ and $R$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04514</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04514</id><created>2015-08-18</created><authors><author><keyname>Grant</keyname><forenames>Alex</forenames></author><author><keyname>Torokhti</keyname><forenames>Anatoli</forenames></author><author><keyname>Soto-Quiros</keyname><forenames>Pablo</forenames></author></authors><title>Compression and Recovery of Distributed Random Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the case when a set of spatially distributed sensors make local
observations which are noisy versions of a signal of interest. Each sensor
transmits compressed information about its measurements to the fusion center
which should recover the original signal within a prescribed accuracy. Such an
information processing relates to a wireless sensor network (WSN) scenario.
  The key problem is to find models of the sensors and fusion center so that
they will be optimal in the sense of minimization of the associated error under
a certain criterion, such as the mean square error (MSE). We determine the
models from the technique which is a combination of the maximum block
improvement (MBI) method and the generic Karhunen-Lo\`{e}ve transform (KLT).
The resulting multi-compressor KLT-MBI algorithm is given in terms of
pseudo-inverse matrices and, therefore, it is numerically stable and always
exists. The proposed model provides compression, de-noising and reconstruction
of distributed signals for the cases when known methods either are not
applicable (because of singularity of associated matrices) or produce larger
associated errors. Error analysis is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04515</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04515</id><created>2015-08-18</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Gelernter</keyname><forenames>Judith</forenames></author></authors><title>Exploring Metaphorical Senses and Word Representations for Identifying
  Metonyms</title><categories>cs.CL</categories><comments>9 pages, 8 pages content</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A metonym is a word with a figurative meaning, similar to a metaphor. Because
metonyms are closely related to metaphors, we apply features that are used
successfully for metaphor recognition to the task of detecting metonyms. On the
ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system
achieved 86.45% accuracy on the location metonyms. Our code can be found on
GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04521</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04521</id><created>2015-08-19</created><authors><author><keyname>Bhatnagar</keyname><forenames>Nayantara</forenames></author><author><keyname>Randall</keyname><forenames>Dana</forenames></author></authors><title>Simulated Tempering and Swapping on Mean-Field Models</title><categories>cs.DM math-ph math.MP math.PR</categories><comments>35 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulated and parallel tempering are families of Markov Chain Monte Carlo
algorithms where a temperature parameter is varied during the simulation to
overcome bottlenecks to convergence due to multimodality.
  In this work we introduce and analyze the convergence for a set of new
tempering distributions which we call \textit{entropy dampening}. For
asymmetric exponential distributions and the mean field Ising model with and
external field simulated tempering is known to converge slowly. We show that
tempering with entropy dampening distributions mixes in polynomial time for
these models.
  Examining slow mixing times of tempering more closely, we show that for the
mean-field 3-state ferromagnetic Potts model, tempering converges slowly
regardless of the temperature schedule chosen. On the other hand, tempering
with entropy dampening distributions converges in polynomial time to
stationarity. Finally we show that the slow mixing can be very expensive
practically. In particular, the mixing time of simulated tempering is an
exponential factor longer than the mixing time at the fixed temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04522</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04522</id><created>2015-08-19</created><updated>2015-09-07</updated><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Dey</keyname><forenames>Palash</forenames></author></authors><title>Fishing out Winners from Vote Streams</title><categories>cs.CC cs.AI cs.DM cs.DS cs.MA</categories><comments>Adding Acknowledgement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of winner determination from computational social
choice theory in the data stream model. Specifically, we consider the task of
summarizing an arbitrarily ordered stream of $n$ votes on $m$ candidates into a
small space data structure so as to be able to obtain the winner determined by
popular voting rules. As we show, finding the exact winner requires storing
essentially all the votes. So, we focus on the problem of finding an {\em
$\eps$-winner}, a candidate who could win by a change of at most $\eps$
fraction of the votes. We show non-trivial upper and lower bounds on the space
complexity of $\eps$-winner determination for several voting rules, including
$k$-approval, $k$-veto, scoring rules, approval, maximin, Bucklin, Copeland,
and plurality with run off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04524</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04524</id><created>2015-08-19</created><updated>2015-08-19</updated><authors><author><keyname>Baidoo-Williams</keyname><forenames>Henry E.</forenames></author><author><keyname>Chipara</keyname><forenames>Octav</forenames></author><author><keyname>Mudumbai</keyname><forenames>Raghuraman</forenames></author><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author></authors><title>PHY-layer link quality indicators for wireless networks using
  matched-filters</title><categories>cs.NI</categories><comments>6 pages</comments><acm-class>C.2; C.3; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to accurate real-time estimation of wireless link
quality using simple matched-filtering techniques. Our approach is based on the
simple observation that there is a portion of each packet transmission from any
given node that does not change from one packet to another; this includes
preamble sequences used to synchronize the receiver and also address
information in the packet header used for medium access control and routing.
Our approach can be thought of as a generalized and simplified variant of
standard signal processing techniques that are commonly used for preamble
detection, automatic gain control, carrier sensing and other functions in many
packet wireless networks. By using a combination of energy detection and
correlation techniques, we show that we can effectively detect packet
transmissions in real-time with low complexity, without decoding the packets
themselves, and indeed, even without detailed knowledge of the packet format.
We present extensive experimental results from a software-defined radio testbed
to illustrate the effectiveness of this approach for 802.15.4 (Zigbee) networks
even in the presence of strong interference signals and low SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04525</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04525</id><created>2015-08-19</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Yang</forenames></author><author><keyname>Gupta</keyname><forenames>Osho</forenames></author><author><keyname>Gelernter</keyname><forenames>Judith</forenames></author></authors><title>Recognizing Extended Spatiotemporal Expressions by Actively Trained
  Average Perceptron Ensembles</title><categories>cs.CL cs.LG</categories><comments>10 pages</comments><acm-class>D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise geocoding and time normalization for text requires that location and
time phrases be identified. Many state-of-the-art geoparsers and temporal
parsers suffer from low recall. Categories commonly missed by parsers are:
nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases,
prepositional phrases, and numerical phrases. We collected and annotated data
set by querying commercial web searches API with such spatiotemporal
expressions as were missed by state-of-the- art parsers. Due to the high cost
of sentence annotation, active learning was used to label training data, and a
new strategy was designed to better select training examples to reduce labeling
cost. For the learning algorithm, we applied an average perceptron trained
Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create
an ensemble, with the output phrase selected by voting. Our ensemble model was
tested on a range of sequential labeling tasks, and has shown competitive
performance. Our contributions include (1) an new dataset annotated with named
entities and expanded spatiotemporal expressions; (2) a comparison of inference
algorithms for ensemble models showing the superior accuracy of Belief
Propagation over Viterbi Decoding; (3) a new example re-weighting method for
active ensemble learning that 'memorizes' the latest examples trained; (4) a
spatiotemporal parser that jointly recognizes expanded spatiotemporal
expressions as well as named entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04526</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04526</id><created>2015-08-19</created><authors><author><keyname>Motlagh</keyname><forenames>Meysam Shahrbaf</forenames></author><author><keyname>Khuzani</keyname><forenames>Masoud Badiei</forenames></author><author><keyname>Mitran</keyname><forenames>Patrick</forenames></author></authors><title>On Lossy Joint Source-Channel Coding In Energy Harvesting Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>15 pages, 7 figures. To be published in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of lossy joint source-channel coding in a single-user
energy harvesting communication system with causal energy arrivals and the
energy storage unit may have leakage. In particular, we investigate the
achievable distortion in the transmission of a single source via an energy
harvesting transmitter over a point-to-point channel. We consider an adaptive
joint source-channel coding system, where the length of channel codewords
varies based on the available battery charge. We first establish a lower bound
on the achievable distortion. Then, as necessary conditions for local
optimality, we obtain two coupled equations that determine the mismatch ratio
between channel symbols and source symbols as well as the transmission power,
both as functions of the battery charge. As examples of continuous and discrete
sources, we consider Gaussian and binary sources respectively. For the Gaussian
case, we obtain a closed-form expression for the mismatch factor in terms of
the $Lambert W$ function, and show that an increasing transmission power policy
results in a decreasing mismatch factor policy and vice versa. Finally, we
numerically compare the performance of the adaptive mismatch factor scheme to
the case of a constant mismatch factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04531</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04531</id><created>2015-08-19</created><authors><author><keyname>Lopez</keyname><forenames>Francisco</forenames></author></authors><title>Modeling emergence of norms in multi-agent systems by applying tipping
  points ideas</title><categories>cs.MA</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Norms are known to be a major factor determining humans behavior. It's also
shown that norms can be quite effective tool for building agent-based
societies. Various normative architectures have been proposed for designing
normative multi-agent systems (NorMAS). Due to human nature of the concept
norms, many of these architectures are built based on theories in social
sciences. Tipping point theory, as is briefly discussed in this paper, seems to
have a great potential to be used for designing normative architectures. This
theory deals with the factors that affect social epidemics that arise in human
societies. In this paper, we try to apply the main concepts of this theory to
agent-based normative architectures. We show several ways to implement these
concepts, and study their effects in an agent-based normative scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04535</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04535</id><created>2015-08-19</created><updated>2015-08-21</updated><authors><author><keyname>Zhang</keyname><forenames>Ruimao</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image
  Retrieval and Person Re-identification</title><categories>cs.CV</categories><comments>14 pages, 5 figures. IEEE Transactions on Image Processing 2015</comments><doi>10.1109/TIP.2015.2467315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting informative image features and learning effective approximate
hashing functions are two crucial steps in image retrieval . Conventional
methods often study these two steps separately, e.g., learning hash functions
from a predefined hand-crafted feature space. Meanwhile, the bit lengths of
output hashing codes are preset in most previous methods, neglecting the
significance level of different bits and restricting their practical
flexibility. To address these issues, we propose a supervised learning
framework to generate compact and bit-scalable hashing codes directly from raw
images. We pose hashing learning as a problem of regularized similarity
learning. Specifically, we organize the training images into a batch of triplet
samples, each sample containing two images with the same label and one with a
different label. With these triplet samples, we maximize the margin between
matched pairs and mismatched pairs in the Hamming space. In addition, a
regularization term is introduced to enforce the adjacency consistency, i.e.,
images of similar appearances should have similar codes. The deep convolutional
neural network is utilized to train the model in an end-to-end fashion, where
discriminative image features and hash functions are simultaneously optimized.
Furthermore, each bit of our hashing codes is unequally weighted so that we can
manipulate the code lengths by truncating the insignificant bits. Our framework
outperforms state-of-the-arts on public benchmarks of similar image search and
also achieves promising results in the application of person re-identification
in surveillance. It is also shown that the generated bit-scalable hashing codes
well preserve the discriminative powers with shorter code lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04537</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04537</id><created>2015-08-19</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Pei</keyname><forenames>Yijian</forenames></author></authors><title>Personalized QoS Prediction of Cloud Services via Learning
  Neighborhood-based Model</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosion of cloud services on the Internet brings new challenges in
service discovery and selection. Particularly, the demand for efficient
quality-of-service (QoS) evaluation is becoming urgently strong. To address
this issue, this paper proposes neighborhood-based approach for QoS prediction
of cloud services by taking advantages of collaborative intelligence. Different
from heuristic collaborative filtering and matrix factorization, we define a
formal neighborhood-based prediction framework which allows an efficient global
optimization scheme, and then exploit different baseline estimate component to
improve predictive performance. To validate the proposed methods, a large-scale
QoS-specific dataset which consists of invocation records from 339 service
users on 5,825 web services on a world-scale distributed network is used.
Experimental results demonstrate that the learned neighborhood-based models can
overcome existing difficulties of heuristic collaborative filtering methods and
achieve superior performance than state-of-the-art prediction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04544</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04544</id><created>2015-08-19</created><authors><author><keyname>Chasparis</keyname><forenames>Georgios C.</forenames></author><author><keyname>Maggio</keyname><forenames>Martina</forenames></author><author><keyname>Bini</keyname><forenames>Enrico</forenames></author><author><keyname>&#xc5;rz&#xe9;n</keyname><forenames>Karl-Eric</forenames></author></authors><title>Design and Implementation of Distributed Resource Management for Time
  Sensitive Applications</title><categories>math.OC cs.DC cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address distributed convergence to fair allocations of CPU
resources for time-sensitive applications. We propose a novel resource
management framework where a centralized objective for fair allocations is
decomposed into a pair of performance-driven recursive processes for updating:
(a) the allocation of computing bandwidth to the applications (resource
adaptation), executed by the resource manager, and (b) the service level of
each application (service-level adaptation), executed by each application
independently. We provide conditions under which the distributed recursive
scheme exhibits convergence to solutions of the centralized objective (i.e.,
fair allocations). Contrary to prior work on centralized optimization schemes,
the proposed framework exhibits adaptivity and robustness to changes both in
the number and nature of applications, while it assumes minimum information
available to both applications and the resource manager. We finally validate
our framework with simulations using the TrueTime toolbox in MATLAB/Simulink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04546</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04546</id><created>2015-08-19</created><authors><author><keyname>Krull</keyname><forenames>Alexander</forenames></author><author><keyname>Brachmann</keyname><forenames>Eric</forenames></author><author><keyname>Michel</keyname><forenames>Frank</forenames></author><author><keyname>Yang</keyname><forenames>Michael Ying</forenames></author><author><keyname>Gumhold</keyname><forenames>Stefan</forenames></author><author><keyname>Rother</keyname><forenames>Carsten</forenames></author></authors><title>Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images</title><categories>cs.CV</categories><comments>16 pages, 8 figures</comments><msc-class>65-XX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis-by-synthesis has been a successful approach for many tasks in
computer vision, such as 6D pose estimation of an object in an RGB-D image
which is the topic of this work. The idea is to compare the observation with
the output of a forward process, such as a rendered image of the object of
interest in a particular pose. Due to occlusion or complicated sensor noise, it
can be difficult to perform this comparison in a meaningful way. We propose an
approach that &quot;learns to compare&quot;, while taking these difficulties into
account. This is done by describing the posterior density of a particular
object pose with a convolutional neural network (CNN) that compares an observed
and rendered image. The network is trained with the maximum likelihood
paradigm. We observe empirically that the CNN does not specialize to the
geometry or appearance of specific objects, and it can be used with objects of
vastly different shapes and appearances, and in different backgrounds. Compared
to state-of-the-art, we demonstrate a significant improvement on two different
datasets which include a total of eleven objects, cluttered background, and
heavy occlusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04554</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04554</id><created>2015-08-19</created><authors><author><keyname>Cao</keyname><forenames>Bokai</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Zhang</keyname><forenames>Jingyuan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Ragin</keyname><forenames>Ann B.</forenames></author></authors><title>Mining Brain Networks using Multiple Side Views for Neurological
  Disorder Identification</title><categories>cs.LG cs.CV cs.CY stat.AP stat.ML</categories><comments>in Proceedings of IEEE International Conference on Data Mining (ICDM)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining discriminative subgraph patterns from graph data has attracted great
interest in recent years. It has a wide variety of applications in disease
diagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the
graph representation alone. However, in many real-world applications, the side
information is available along with the graph data. For example, for
neurological disorder identification, in addition to the brain networks derived
from neuroimaging data, hundreds of clinical, immunologic, serologic and
cognitive measures may also be documented for each subject. These measures
compose multiple side views encoding a tremendous amount of supplemental
information for diagnostic purposes, yet are often ignored. In this paper, we
study the problem of discriminative subgraph selection using multiple side
views and propose a novel solution to find an optimal set of subgraph features
for graph classification by exploring a plurality of side views. We derive a
feature evaluation criterion, named gSide, to estimate the usefulness of
subgraph patterns based upon side views. Then we develop a branch-and-bound
algorithm, called gMSV, to efficiently search for optimal subgraph features by
integrating the subgraph mining process and the procedure of discriminative
feature selection. Empirical studies on graph classification tasks for
neurological disorders using brain networks demonstrate that subgraph patterns
selected by the multi-side-view guided subgraph selection approach can
effectively boost graph classification performances and are relevant to disease
diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04559</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04559</id><created>2015-08-19</created><authors><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author><author><keyname>Spurek</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Kamieniecki</keyname><forenames>Konrad</forenames></author><author><keyname>&#x15a;mieja</keyname><forenames>Marek</forenames></author><author><keyname>Misztal</keyname><forenames>Krzysztof</forenames></author></authors><title>Introduction to Cross-Entropy Clustering The R Package CEC</title><categories>cs.LG stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The R Package CEC performs clustering based on the cross-entropy clustering
(CEC) method, which was recently developed with the use of information theory.
The main advantage of CEC is that it combines the speed and simplicity of
$k$-means with the ability to use various Gaussian mixture models and reduce
unnecessary clusters. In this work we present a practical tutorial to CEC based
on the R Package CEC. Functions are provided to encompass the whole process of
clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04562</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04562</id><created>2015-08-19</created><authors><author><keyname>Zhang</keyname><forenames>Jingwei</forenames></author><author><keyname>Gerow</keyname><forenames>Aaron</forenames></author><author><keyname>Altosaar</keyname><forenames>Jaan</forenames></author><author><keyname>Evans</keyname><forenames>James</forenames></author><author><keyname>So</keyname><forenames>Richard Jean</forenames></author></authors><title>Fast, Flexible Models for Discovering Topic Correlation across
  Weakly-Related Collections</title><categories>cs.CL cs.IR</categories><comments>EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak topic correlation across document collections with different numbers of
topics in individual collections presents challenges for existing
cross-collection topic models. This paper introduces two probabilistic topic
models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address
problems that can arise when analyzing large, asymmetric, and potentially
weakly-related collections. Topic correlations in weakly-related collections
typically lie in the tail of the topic distribution, where they would be
overlooked by models unable to fit large numbers of topics. To efficiently
model this long tail for large-scale analysis, our models implement a parallel
sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et
al., 2015). The models are first evaluated on synthetic data, generated to
simulate various collection-level asymmetries. We then present a case study of
modeling over 300k documents in collections of sciences and humanities research
from JSTOR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04570</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04570</id><created>2015-08-19</created><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author><author><keyname>Palade</keyname><forenames>Vasile</forenames></author></authors><title>Proposal for the creation of a research facility for the development of
  the SP machine</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1508.04087. substantial
  text overlap with arXiv:1409.8027</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a proposal to create a research facility for the development of a
high-parallel version of the &quot;SP machine&quot;, based on the &quot;SP theory of
intelligence&quot;. We envisage that the new version of the SP machine will be an
open-source software virtual machine, derived from the existing &quot;SP computer
model&quot;, and hosted on an existing high-performance computer. It will be a means
for researchers everywhere to explore what can be done with the system and to
create new versions of it. The SP system is a unique attempt to simplify and
integrate observations and concepts across artificial intelligence, mainstream
computing, mathematics, and human perception and cognition, with information
compression as a unifying theme. Potential benefits and applications include
helping to solve problems associated with big data; facilitating the
development of autonomous robots; unsupervised learning, natural language
processing, several kinds of reasoning, fuzzy pattern recognition at multiple
levels of abstraction, computer vision, best-match and semantic forms of
information retrieval, software engineering, medical diagnosis, simplification
of computing systems, and the seamless integration of diverse kinds of
knowledge and diverse aspects of intelligence. Additional motivations include
the potential of the SP system to help solve problems in defence, security, and
the detection and prevention of crime; potential in terms of economic, social,
environmental, and academic criteria, and in terms of publicity; and the
potential for international influence in research. The main elements of the
proposed facility are described, including support for the development of
&quot;SP-neural&quot;, a neural version of the SP machine. The facility should be
permanent in the sense that it should be available for the foreseeable future,
and it should be designed to facilitate its use by researchers anywhere in the
world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04571</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04571</id><created>2015-08-19</created><authors><author><keyname>Merca&#x15f;</keyname><forenames>Robert</forenames></author></authors><title>A note on the avoidability of binary patterns with variables and
  reversals</title><categories>cs.FL</categories><comments>This work is mostly obsolete as a week ago Currie and Lafrance fully
  characterised the patterns with reversals in
  [http://arxiv.org/abs/1508.02101]. The only novelty of this work is given by
  the aperiodicity restriction of the infinite words avoiding such patterns</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we present a characterisation of all unary and binary patterns
that do not only contain variables, but also reversals of their instances.
These types of variables were studied recently in either more general or
particular cases. We show that the results are not surprising at all in the
general case, and extend the avoidability of these patterns to enforce
aperiodic words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04582</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04582</id><created>2015-08-19</created><authors><author><keyname>van Hasselt</keyname><forenames>Hado</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>Learning to Predict Independent of Span</title><categories>cs.LG</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how to learn multi-step predictions efficiently. Conventional
algorithms wait until observing actual outcomes before performing the
computations to update their predictions. If predictions are made at a high
rate or span over a large amount of time, substantial computation can be
required to store all relevant observations and to update all predictions when
the outcome is finally observed. We show that the exact same predictions can be
learned in a much more computationally congenial way, with uniform per-step
computation that does not depend on the span of the predictions. We apply this
idea to various settings of increasing generality, repeatedly adding desired
properties and each time deriving an equivalent span-independent algorithm for
the conventional algorithm that satisfies these desiderata. Interestingly,
along the way several known algorithmic constructs emerge spontaneously from
our derivations, including dutch eligibility traces, temporal difference
errors, and averaging. This allows us to link these constructs one-to-one to
the corresponding desiderata, unambiguously connecting the `how' to the `why'.
Each step, we make sure that the derived algorithm subsumes the previous
algorithms, thereby retaining their properties. Ultimately we arrive at a
single general temporal-difference algorithm that is applicable to the full
setting of reinforcement learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04584</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04584</id><created>2015-08-19</created><authors><author><keyname>Shi</keyname><forenames>Zhan</forenames></author><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author></authors><title>Local optimality of a coherent feedback scheme for distributed
  entanglement generation: the idealized infinite bandwidth limit</title><categories>quant-ph cs.SY</categories><comments>17 pages, 5 figures. Condensed version of this paper to appear in
  Proceedings of the 54th IEEE Conference on Decision and Control (CDC), Osaka,
  Japan, Dec. 15-18, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to prove a local optimality property of a
recently proposed coherent feedback configuration for distributed generation of
EPR entanglement using two nondegenerate optical parametric amplifiers (NOPAs)
in the idealized infinite bandwidth limit. This local optimality is with
respect to a class of similar coherent feedback configurations but employing
different unitary scattering matrices, representing different scattering of
propagating signals within the network. The infinite bandwidth limit is
considered as it significantly simplifies the analysis, allowing local
optimality criteria to be explicitly verified. Nonetheless, this limit is
relevant for the finite bandwidth scenario as it provides an accurate
approximation to the EPR entanglement in the low frequency region where EPR
entanglement exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04586</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04586</id><created>2015-08-19</created><authors><author><keyname>Vilaplana</keyname><forenames>Ver&#xf3;nica</forenames></author></authors><title>Saliency maps on image hierarchies</title><categories>cs.CV</categories><comments>Accepted for publication in Signal Processing: Image Communications,
  2015</comments><doi>10.1016/j.image.2015.07.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose two saliency models for salient object segmentation
based on a hierarchical image segmentation, a tree-like structure that
represents regions at different scales from the details to the whole image
(e.g. gPb-UCM, BPT). The first model is based on a hierarchy of image
partitions. The saliency at each level is computed on a region basis, taking
into account the contrast between regions. The maps obtained for the different
partitions are then integrated into a final saliency map. The second model
directly works on the structure created by the segmentation algorithm,
computing saliency at each node and integrating these cues in a straightforward
manner into a single saliency map. We show that the proposed models produce
high quality saliency maps. Objective evaluation demonstrates that the two
methods achieve state-of-the-art performance in several benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04595</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04595</id><created>2015-08-19</created><authors><author><keyname>Knight</keyname><forenames>Sophia</forenames><affiliation>CNRS, LORIA, Universit&#xe9; de Lorraine, France</affiliation></author><author><keyname>Lanese</keyname><forenames>Ivan</forenames><affiliation>University of Bologna/INRIA, Italy</affiliation></author><author><keyname>Lafuente</keyname><forenames>Alberto Lluch</forenames><affiliation>Technical University of Denmark, Denmark</affiliation></author><author><keyname>Vieira</keyname><forenames>Hugo Torres</forenames><affiliation>IMT Institute for Advanced Studies Lucca, Italy</affiliation></author></authors><title>Proceedings 8th Interaction and Concurrency Experience</title><categories>cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 189, 2015</journal-ref><doi>10.4204/EPTCS.189</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of ICE 2015, the 8th Interaction and
Concurrency Experience, which was held in Grenoble, France on the 4th and 5th
of June 2015 as a satellite event of DisCoTec 2015. The ICE procedure for paper
selection allows PC members to interact, anonymously, with authors. During the
review phase, each submitted paper is published on a discussion forum with
access restricted to the authors and to all the PC members not declaring a
conflict of interest. The PC members post comments and questions to which the
authors reply. Each paper was reviewed by three PC members, and altogether 9
papers, including 1 short paper, were accepted for publication (the workshop
also featured 4 brief announcements which are not part of this volume). We were
proud to host three invited talks, by Leslie Lamport (shared with the FRIDA
workshop), Joseph Sifakis and Steve Ross-Talbot. The abstracts of the last two
talks are included in this volume together with the regular papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04596</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04596</id><created>2015-08-19</created><authors><author><keyname>Alexandersen</keyname><forenames>Joe</forenames></author><author><keyname>Sigmund</keyname><forenames>Ole</forenames></author><author><keyname>Aage</keyname><forenames>Niels</forenames></author></authors><title>Large scale three-dimensional topology optimisation of heat sinks cooled
  by natural convection</title><categories>physics.flu-dyn cs.CE</categories><comments>Submitted (18th of August 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents the application of density-based topology optimisation to
the design of three-dimensional heat sinks cooled by natural convection. The
governing equations are the steady-state incompressible Navier-Stokes equations
coupled to the thermal convection-diffusion equation through the Bousinessq
approximation. The fully coupled non-linear multiphysics system is solved using
stabilised trilinear equal-order finite elements in a parallel framework
allowing for the optimisation of large scale problems with order of 40-330
million state degrees of freedom. The flow is assumed to be laminar and several
optimised designs are presented for Grashof numbers between $10^3$ and $10^6$.
Interestingly, it is observed that the number of branches in the optimised
design increases with increasing Grashof numbers, which is opposite to
two-dimensional optimised designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04603</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04603</id><created>2015-08-19</created><authors><author><keyname>Ivaldi</keyname><forenames>Serena</forenames></author><author><keyname>Lefort</keyname><forenames>Sebastien</forenames></author><author><keyname>Peters</keyname><forenames>Jan</forenames></author><author><keyname>Chetouani</keyname><forenames>Mohamed</forenames></author><author><keyname>Provasi</keyname><forenames>Joelle</forenames></author><author><keyname>Zibetti</keyname><forenames>Elisabetta</forenames></author></authors><title>Towards engagement models that consider individual factors in HRI: on
  the relation of extroversion and negative attitude towards robots to gaze and
  speech during a human-robot assembly task</title><categories>cs.RO cs.HC</categories><comments>24 pages, submitted to IJSR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the engagement is critical for human - robot interaction.
Engagement measures typically rely on the dynamics of the social signals
exchanged by the partners, especially speech and gaze. However, the dynamics of
these signals is likely to be influenced by individual and social factors, such
as personality traits, as it is well documented that they critically influence
how two humans interact with each other. Here, we assess the influence of two
factors, namely extroversion and negative attitude toward robots, on speech and
gaze during a cooperative task, where a human must physically manipulate a
robot to assemble an object. We evaluate if the scores of extroversion and
negative attitude towards robots co-variate with the duration and frequency of
gaze and speech cues. The experiments were carried out with the humanoid robot
iCub and N=56 adult participants. We found that the more people are extrovert,
the more and longer they tend to talk with the robot; and the more people have
a negative attitude towards robots, the less they will look at the robot face
and the more they will look at the robot hands where the assembly and the
contacts occur. Our results confirm and provide evidence that the engagement
models classically used in human-robot interaction should take into account
attitudes and personality traits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04606</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04606</id><created>2015-08-19</created><authors><author><keyname>Liu</keyname><forenames>Tao</forenames></author><author><keyname>Cao</keyname><forenames>Ming</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author></authors><title>Distributed Event-Triggered Control for Asymptotic Synchronization of
  Dynamical Networks</title><categories>cs.SY cs.MA math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the synchronization problem of a dynamical network with
event-based communication, where each node communicates to its neighbours only
when an event-triggering condition is fulfilled. Firstly, two estimators are
introduced into each node, one to estimate its own state, and the other to
estimate the average state of its neighbours. Then, with the assistance of the
two estimators, a distributed event-triggering rule with a dwell-time is
designed such that the network achieves synchronization asymptotically, and
meanwhile no Zeno behaviour occurs. The designed event-triggering rule only
depends on the information that each node can obtain, and thus can be
implemented in a decentralized way. The quantization effects are also
considered, and the logarithmic quantizer is used to achieve asymptotic
synchronization. Finally, numerical examples are given to show the
effectiveness of the proposed results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04627</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04627</id><created>2015-08-19</created><authors><author><keyname>Shastry</keyname><forenames>Bhargava</forenames></author></authors><title>Towards Vulnerability Discovery Using Extended Compile-time Analysis</title><categories>cs.CR cs.PL</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploitable vulnerabilities, are often, an outcome of semantic bugs in a
program's software implementation. Since analyzing large codebases for
detecting semantic bugs is hard, there is a reliance on software testing for
uncovering defects. Compiler-driven program analyzers such as the Clang Static
Analyzer are promising, but a local outlook limits their potential. In this
paper, we propose an extended compile-time analysis framework for detecting
potentially security-critical defects in object-oriented code, and evaluate the
proposal against large codebases. Our framework, that we call Melange,
non-intrusively retrofits a two-stage analysis pipeline into a codebase's build
system. Melange complements software testing: It empowers developers to fix
defects during active software development. Our analyzer scales up to large
codebases and has, thus far, reported known vulnerabilities in Chromium source
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04633</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04633</id><created>2015-08-19</created><authors><author><keyname>Textor</keyname><forenames>Johannes</forenames></author></authors><title>Drawing and Analyzing Causal DAGs with DAGitty</title><categories>cs.AI</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DAGitty is a software for drawing and analyzing causal diagrams, also known
as directed acyclic graphs (DAGs). Functions include identification of minimal
sufficient adjustment sets for estimating causal effects, diagnosis of
insufficient or invalid adjustment via the identification of biasing paths,
identification of instrumental variables, and derivation of testable
implications. DAGitty is provided in the hope that it is useful for researchers
and students in Epidemiology, Sociology, Psychology, and other empirical
disciplines. The software should run in any web browser that supports modern
JavaScript, HTML, and SVG. This is the user manual for DAGitty version 2.3. The
manual is updated with every release of a new stable version. DAGitty is
available at dagitty.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04635</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04635</id><created>2015-08-19</created><authors><author><keyname>Sarma</keyname><forenames>Gopal P.</forenames></author><author><keyname>Jacobs</keyname><forenames>Travis W.</forenames></author><author><keyname>Watts</keyname><forenames>Mark D.</forenames></author><author><keyname>Ghayoomi</keyname><forenames>Vahid</forenames></author><author><keyname>Gerkin</keyname><forenames>Richard C.</forenames></author><author><keyname>Larson</keyname><forenames>Stephen D.</forenames></author></authors><title>Unit Testing, Model Validation, and Biological Simulation</title><categories>q-bio.QM cs.SE</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of the software industry has gone hand in hand with the
development of tools and cultural practices for ensuring the reliability of
complex pieces of software. These tools and practices are now acknowledged to
be essential to the management of modern software. As computational models and
methods have become increasingly common in the biological sciences, it is
important to examine how these practices can accelerate biological software
development and improve research quality. In this article, we give a focused
case study of our experience with the practices of unit testing and test-driven
development in OpenWorm, an open-science project aimed at modeling
Caenorhabditis elegans. We identify and discuss the challenges of incorporating
test-driven development into a heterogeneous, data-driven project, as well as
the role of model validation tests, a category of tests unique to software
which expresses scientific models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04644</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04644</id><created>2015-08-19</created><updated>2015-09-16</updated><authors><author><keyname>Cui</keyname><forenames>Shawn X.</forenames></author><author><keyname>Freedman</keyname><forenames>Michael H.</forenames></author><author><keyname>Sattath</keyname><forenames>Or</forenames></author><author><keyname>Stong</keyname><forenames>Richard</forenames></author><author><keyname>Minton</keyname><forenames>Greg</forenames></author></authors><title>Quantum Max-flow/Min-cut</title><categories>math.CO cs.IT math.IT quant-ph</categories><comments>24 pages</comments><msc-class>81P45, 94A15, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical max-flow min-cut theorem describes transport through certain
idealized classical networks. We consider the quantum analog for tensor
networks. By associating a tensor to each node in an integral flow network, we
can also interpret it as a tensor network, and more specifically, as a linear
map. The quantum max flow is defined to be the maximal rank of this linear map
over all choices for the tensors in the tensor network. We show that every cut
in the tensor network provides an upper-bound on this rank, and therefore the
quantum max flow is no more than the quantum min cut. Unlike the classical case
(where the converse is also true, i.e., max flow = min cut) we show examples
where the quantum inequality is strict.
  We also found connections of quantum max-flow/min-cut with entropy of
entanglement and the quantum satisfiability problem. We speculate that the
phenomena revealed may be of interest both in spin systems in condensed matter
and in quantum gravity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04716</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04716</id><created>2015-08-19</created><updated>2015-08-20</updated><authors><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Ives</keyname><forenames>David J.</forenames></author><author><keyname>Savory</keyname><forenames>Seb</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author></authors><title>On the Impact of Optimal Modulation and FEC Overhead on Future Optical
  Networks</title><categories>cs.IT math.IT physics.optics</categories><comments>Some minor typos were corrected</comments><doi>10.1109/JLT.2016.2517699</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential of optimum selection of modulation and forward error correction
(FEC) overhead (OH) in future transparent nonlinear optical mesh networks is
studied from an information theory perspective. Different network topologies
are studied as well as both ideal soft-decision (SD) and hard-decision (HD) FEC
based on demap-and-decode (bit-wise) receivers. When compared to the de-facto
QPSK with 7% OH, our results show large gains in network throughput. When
compared to SD-FEC, HD-FEC is shown to cause network throughput losses of 12%,
15%, and 20% for a country, continental, and global network topology,
respectively. Furthermore, it is shown that most of the theoretically possible
gains can be achieved by using one modulation format and only two OHs. This is
in contrast to the infinite number of OHs required in the ideal case. The
obtained optimal OHs are between 5% and 80%, which highlights the potential
advantage of using FEC with high OHs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04720</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04720</id><created>2015-08-19</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Firouzi</keyname><forenames>Hamed</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Quickest Detection for Changes in Maximal kNN Coherence of Random
  Matrices</title><categories>math.ST cs.IT math.IT math.PR stat.TH</categories><comments>Submitted to IEEE Transactions on Information Theory, Aug, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of quickest detection of a change in the distribution of a
$n\times p$ random matrix based on a sequence of observations having a single
unknown change point is considered. The forms of the pre- and post-change
distributions of the rows of the matrices are assumed to belong to the family
of elliptically contoured densities with sparse dispersion matrices but are
otherwise unknown. A non-parametric stopping rule is proposed that is based on
a novel scalar summary statistic related to the maximal k-nearest neighbor
correlation between columns of each observed random matrix, and is related to a
test of existence of a vertex in a sample correlation graph having degree at
least $k$. Performance bounds on the delay and false alarm performance of the
proposed stopping rule are obtained. When the pre-change dispersion matrix is
diagonal it is shown that, among all functions of the proposed summary
statistic, the proposed stopping rule is asymptotically optimal under a minimax
quickest change detection (QCD) model, in the purely high-dimensional regime of
$p\rightarrow \infty$ and $n$ fixed. The significance is that the purely high
dimensional asymptotic regime considered here is asymptotic in $p$ and not $n$
making it especially well suited to big data regimes. The theory developed also
applies to fixed sample size tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04725</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04725</id><created>2015-08-19</created><authors><author><keyname>Knop</keyname><forenames>Du&#x161;an</forenames></author></authors><title>A note on partition into triangles parametrized by tree-width</title><categories>cs.DM</categories><comments>6 pages</comments><msc-class>05C70, 05C85, 68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parametrized complexity of the Partition into Triangles problem.
For this problem a (simple) graph with 3n vertices is given and the question is
whether it is possible to cover its vertices with n triangles (complete graphs
on 3 vertices). We prove that there is an FPT algorithm that decides the
Partition into Triangles problem and that the existence of a polynomial size
kernel is unlikely (unless NP $\subseteq$ coNP/poly).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04726</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04726</id><created>2015-08-19</created><updated>2015-08-21</updated><authors><author><keyname>Shevchenko</keyname><forenames>Nikita A.</forenames></author><author><keyname>Prilepsky</keyname><forenames>Jaroslaw E.</forenames></author><author><keyname>Derevyanko</keyname><forenames>Stanislav A.</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author><author><keyname>Turitsyn</keyname><forenames>Sergei K.</forenames></author></authors><title>A Lower Bound on the per Soliton Capacity of the Nonlinear Optical Fibre
  Channel</title><categories>cs.IT math.IT physics.optics</categories><comments>To be presented at the 2015 IEEE Information Theory Workshop, Jeju
  Island, Korea, October 11-15, 2015</comments><doi>10.1109/ITWF.2015.7360743</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A closed-form expression for a lower bound on the per soliton capacity of the
nonlinear optical fibre channel in the presence of (optical) amplifier
spontaneous emission (ASE) noise is derived. This bound is based on a
non-Gaussian conditional probability density function for the soliton amplitude
jitter induced by the ASE noise and is proven to grow logarithmically as the
signal-to-noise ratio increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04731</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04731</id><created>2015-08-19</created><authors><author><keyname>Salloum</keyname><forenames>Maher</forenames></author><author><keyname>Bennett</keyname><forenames>Janine C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Bhagatwala</keyname><forenames>Ankit</forenames></author><author><keyname>Chen</keyname><forenames>Jacqueline H.</forenames></author></authors><title>Enabling adaptive scientific workflows via trigger detection</title><categories>cs.CE cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1506.08258</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation architectures necessitate a shift away from traditional
workflows in which the simulation state is saved at prescribed frequencies for
post-processing analysis. While the need to shift to in~situ workflows has been
acknowledged for some time, much of the current research is focused on static
workflows, where the analysis that would have been done as a post-process is
performed concurrently with the simulation at user-prescribed frequencies.
Recently, research efforts are striving to enable adaptive workflows, in which
the frequency, composition, and execution of computational and data
manipulation steps dynamically depend on the state of the simulation. Adapting
the workflow to the state of simulation in such a data-driven fashion puts
extremely strict efficiency requirements on the analysis capabilities that are
used to identify the transitions in the workflow. In this paper we build upon
earlier work on trigger detection using sublinear techniques to drive adaptive
workflows. Here we propose a methodology to detect the time when sudden heat
release occurs in simulations of turbulent combustion. Our proposed method
provides an alternative metric that can be used along with our former metric to
increase the robustness of trigger detection. We show the effectiveness of our
metric empirically for predicting heat release for two use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04733</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04733</id><created>2015-08-18</created><authors><author><keyname>Khan</keyname><forenames>Samiya</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author></authors><title>Cloud based Big Data Analytics: A Survey of Current Research and Future
  Directions</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of the digital age has led to a rise in different types of data
with every passing day. In fact, it is expected that half of the total data
will be on the cloud by 2016. This data is complex and needs to be stored,
processed and analyzed for information that can be used by organizations. Cloud
computing provides an apt platform for big data analytics in view of the
storage and computing requirements of the latter. This makes cloud-based
analytics a viable research field. However, several issues need to be addressed
and risks need to be mitigated before practical applications of this
synergistic model can be popularly used. This paper explores the existing
research, challenges, open issues and future research direction for this field
of study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04734</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04734</id><created>2015-08-19</created><authors><author><keyname>Amarnath</keyname><forenames>M.</forenames></author><author><keyname>Arunav</keyname><forenames>S.</forenames></author><author><keyname>Kumar</keyname><forenames>Hemantha</forenames></author><author><keyname>Sugumaran</keyname><forenames>V.</forenames></author><author><keyname>Raghvendra</keyname><forenames>G. S</forenames></author></authors><title>Fault Diagnosis of Helical Gear Box using Large Margin K-Nearest
  Neighbors Classifier using Sound Signals</title><categories>cs.LG</categories><comments>Accepted for publication</comments><journal-ref>JVET V4 n2 2016</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gear drives are one of the most widely used transmission system in many
machinery. Sound signals of a rotating machine contain the dynamic information
about its health conditions. Not much information available in the literature
reporting suitability of sound signals for fault diagnosis applications.
Maximum numbers of literature are based on FFT (Fast Fourier Transform)
analysis and have its own limitations with non-stationary signals like the ones
from gears. In this paper, attempt has been made in using sound signals
acquired from gears in good and simulated faulty conditions for the purpose of
fault diagnosis through a machine learning approach. The descriptive
statistical features were extracted from the acquired sound signals and the
predominant features were selected using J48 decision tree technique. The
selected features were then used for classification using Large Margin
K-nearest neighbor approach. The paper also discusses the effect of various
parameters on classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04740</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04740</id><created>2015-08-19</created><authors><author><keyname>Rechner</keyname><forenames>Steffen</forenames></author><author><keyname>Berger</keyname><forenames>Annabell</forenames></author></authors><title>Marathon: An open source software library for the analysis of
  Markov-Chain Monte Carlo algorithms</title><categories>cs.DM cs.MS</categories><doi>10.1371/journal.pone.0147935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Markov-Chain Monte Carlo (MCMC) approach for
random sampling of combinatorial objects. The running time of such an algorithm
depends on the total mixing time of the underlying Markov chain and is unknown
in general. For some Markov chains, upper bounds on this total mixing time
exist but are too large to be applicable in practice. We try to answer the
question, whether the total mixing time is close to its upper bounds, or if
there is a significant gap between them. In doing so, we present the software
library marathon which is designed to support the analysis of MCMC based
sampling algorithms. The main application of this library is to compute
properties of so-called state graphs which represent the structure of Markov
chains. We use marathon to investigate the quality of several bounding methods
on four well-known Markov chains for sampling perfect matchings and bipartite
graph realizations. In a set of experiments, we compute the total mixing time
and several of its bounds for a large number of input instances. We find that
the upper bound gained by the famous canonical path method is several
magnitudes larger than the total mixing time and deteriorates with growing
input size. In contrast, the spectral bound is found to be a precise
approximation of the total mixing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04741</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04741</id><created>2015-08-19</created><authors><author><keyname>Couceiro</keyname><forenames>Miguel</forenames></author><author><keyname>Foldes</keyname><forenames>Stephan</forenames></author><author><keyname>Meletiou</keyname><forenames>Gerasimos C.</forenames></author></authors><title>Arrow type impossibility theorems over median algebras</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize trees as median algebras and semilattices by relaxing
conservativeness. Moreover, we describe median homomorphisms between products
of median algebras and show that Arrow type impossibility theorems for mappings
from a product $\mathbf{A}_1\times \cdots \times \mathbf{A}_n$ of median
algebras to a median algebra $\mathbf{B}$ are possible if and only if
$\mathbf{B}$ is a tree, when thought of as an ordered structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04742</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04742</id><created>2015-08-19</created><updated>2015-08-25</updated><authors><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>A short note on estimation of WCRE and WCE</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>5 pages, 4 figures</comments><msc-class>62N05, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note the author uses order statistics to estimate WCRE and WCE in
terms of empirical and survival functions. An example in both cases normal and
exponential WFs is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04747</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04747</id><created>2015-08-19</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Karrenbauer</keyname><forenames>Andreas</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Patt-Shamir</keyname><forenames>Boaz</forenames></author></authors><title>Near-Optimal Distributed Maximum Flow</title><categories>cs.DS cs.DC</categories><comments>34 pages, 5 figures, conference version appeared in ACM Symp. on
  Principles of Distributed Computing (PODC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a near-optimal distributed algorithm for $(1+o(1))$-approximation
of single-commodity maximum flow in undirected weighted networks that runs in
$(D+ \sqrt{n})\cdot n^{o(1)}$ communication rounds in the \Congest model. Here,
$n$ and $D$ denote the number of nodes and the network diameter, respectively.
This is the first improvement over the trivial bound of $O(n^2)$, and it nearly
matches the $\tilde{\Omega}(D+ \sqrt{n})$ round complexity lower bound.
  The development of the algorithm contains two results of independent
interest:
  (i) A $(D+\sqrt{n})\cdot n^{o(1)}$-round distributed construction of a
spanning tree of average stretch $n^{o(1)}$.
  (ii) A $(D+\sqrt{n})\cdot n^{o(1)}$-round distributed construction of an
$n^{o(1)}$-congestion approximator consisting of the cuts induced by $O(\log
n)$ virtual trees. The distributed representation of the cut approximator
allows for evaluation in $(D+\sqrt{n})\cdot n^{o(1)}$ rounds.
  All our algorithms make use of randomization and succeed with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04752</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04752</id><created>2015-08-18</created><authors><author><keyname>Brunnert</keyname><forenames>Andreas</forenames></author><author><keyname>van Hoorn</keyname><forenames>Andre</forenames></author><author><keyname>Willnecker</keyname><forenames>Felix</forenames></author><author><keyname>Danciu</keyname><forenames>Alexandru</forenames></author><author><keyname>Hasselbring</keyname><forenames>Wilhelm</forenames></author><author><keyname>Heger</keyname><forenames>Christoph</forenames></author><author><keyname>Herbst</keyname><forenames>Nikolas</forenames></author><author><keyname>Jamshidi</keyname><forenames>Pooyan</forenames></author><author><keyname>Jung</keyname><forenames>Reiner</forenames></author><author><keyname>von Kistowski</keyname><forenames>Joakim</forenames></author><author><keyname>Koziolek</keyname><forenames>Anne</forenames></author><author><keyname>Kro&#xdf;</keyname><forenames>Johannes</forenames></author><author><keyname>Spinner</keyname><forenames>Simon</forenames></author><author><keyname>V&#xf6;gele</keyname><forenames>Christian</forenames></author><author><keyname>Walter</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Wert</keyname><forenames>Alexander</forenames></author></authors><title>Performance-oriented DevOps: A Research Agenda</title><categories>cs.SE cs.PF</categories><report-no>SPEC-RG-2015-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DevOps is a trend towards a tighter integration between development (Dev) and
operations (Ops) teams. The need for such an integration is driven by the
requirement to continuously adapt enterprise applications (EAs) to changes in
the business environment. As of today, DevOps concepts have been primarily
introduced to ensure a constant flow of features and bug fixes into new
releases from a functional perspective. In order to integrate a non-functional
perspective into these DevOps concepts this report focuses on tools,
activities, and processes to ensure one of the most important quality
attributes of a software system, namely performance.
  Performance describes system properties concerning its timeliness and use of
resources. Common metrics are response time, throughput, and resource
utilization. Performance goals for EAs are typically defined by setting upper
and/or lower bounds for these metrics and specific business transactions. In
order to ensure that such performance goals can be met, several activities are
required during development and operation of these systems as well as during
the transition from Dev to Ops. Activities during development are typically
summarized by the term Software Performance Engineering (SPE), whereas
activities during operations are called Application Performance Management
(APM). SPE and APM were historically tackled independently from each other, but
the newly emerging DevOps concepts require and enable a tighter integration
between both activity streams. This report presents existing solutions to
support this integration as well as open research challenges in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04753</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04753</id><created>2015-08-18</created><authors><author><keyname>Briggs</keyname><forenames>Kim T.</forenames></author><author><keyname>Zhou</keyname><forenames>Baoguo</forenames></author><author><keyname>Dueck</keyname><forenames>Gerhard W.</forenames></author></authors><title>Cold Object Identification in the Java Virtual Machine</title><categories>cs.PL cs.OS</categories><comments>For submission to `Software: Practice and Experience'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Java applications instantiate objects within the Java heap that are
persistent but seldom if ever referenced by the application. Examples include
strings, such as error messages, and collections of value objects that are
preloaded for fast access but they may include objects that are seldom
referenced. This paper describes a stack-based framework for detecting these
&quot;cold&quot; objects at runtime, with a view to marshaling and sequestering them in
designated regions of the heap where they may be preferentially paged out to a
backing store, thereby freeing physical memory pages for occupation by more
active objects. Furthermore, we evaluate the correctness and efficiency of
stack-based approach with an Access Barrier. The experimental results from a
series of SPECjvm2008 benchmarks are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04757</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04757</id><created>2015-08-19</created><authors><author><keyname>Ferreira</keyname><forenames>Leonardo N.</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>Time Series Clustering via Community Detection in Networks</title><categories>stat.ML cs.LG cs.SI</categories><doi>10.1016/j.ins.2015.07.046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a technique for time series clustering using
community detection in complex networks. Firstly, we present a method to
transform a set of time series into a network using different distance
functions, where each time series is represented by a vertex and the most
similar ones are connected. Then, we apply community detection algorithms to
identify groups of strongly connected vertices (called a community) and,
consequently, identify time series clusters. Still in this paper, we make a
comprehensive analysis on the influence of various combinations of time series
distance functions, network generation methods and community detection
techniques on clustering results. Experimental study shows that the proposed
network-based approach achieves better results than various classic or
up-to-date clustering techniques under consideration. Statistical tests confirm
that the proposed method outperforms some classic clustering algorithms, such
as $k$-medoids, diana, median-linkage and centroid-linkage in various data
sets. Interestingly, the proposed method can effectively detect shape patterns
presented in time series due to the topological structure of the underlying
network constructed in the clustering process. At the same time, other
techniques fail to identify such patterns. Moreover, the proposed method is
robust enough to group time series presenting similar pattern but with time
shifts and/or amplitude variations. In summary, the main point of the proposed
method is the transformation of time series from time-space domain to
topological domain. Therefore, we hope that our approach contributes not only
for time series clustering, but also for general time series analysis tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04758</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04758</id><created>2015-08-19</created><authors><author><keyname>Xianfeng</keyname><affiliation>Janice</affiliation></author><author><keyname>Hu</keyname></author><author><keyname>Iwen</keyname><forenames>Mark</forenames></author><author><keyname>Kim</keyname><forenames>Hyejin</forenames></author></authors><title>Rapidly Computing Sparse Legendre Expansions via Sparse Fourier
  Transforms</title><categories>math.NA cs.NA</categories><msc-class>65D05, 42A10, 68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a general strategy for rapidly computing sparse
Legendre expansions. The resulting methods yield a new class of fast algorithms
capable of approximating a given function $f:[-1,1] \rightarrow \mathbb{R}$
with a near-optimal linear combination of $s$ Legendre polynomials of degree
$\leq N$ in just $(s \log N)^{\mathcal{O}(1)}$-time. When $s \ll N$ these
algorithms exhibit sublinear runtime complexities in $N$, as opposed to
traditional $\Omega(N \log N)$-time methods for computing all of the first $N$
Legendre coefficients of $f$. Theoretical as well as numerical results
demonstrate the promise of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04783</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04783</id><created>2015-08-19</created><authors><author><keyname>B&#xe9;langer</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Ouangraoua</keyname><forenames>A&#xef;da</forenames></author></authors><title>Alignment of protein-coding sequences with frameshift extension
  penalties</title><categories>cs.DS cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an algorithm for the alignment of protein- coding sequences
accounting for frameshifts. The main specificity of this algorithm as compared
to previously published protein-coding sequence alignment methods is the
introduction of a penalty cost for frameshift ex- tensions. Previous algorithms
have only used constant frameshift penal- ties. This is similar to the use of
scoring schemes with affine gap penalties in classical sequence alignment
algorithms. However, the overall penalty of a frameshift portion in an
alignment cannot be formulated as an affine function, because it should also
incorporate varying codon substitution scores. The second specificity of the
algorithm is its search space being the set of all possible alignments between
two coding sequences, under the classical definition of an alignment between
two DNA sequences. Previous algorithms have introduced constraints on the
length of the alignments, and additional symbols for the representation of
frameshift openings in an alignment. The algorithm has the same asymptotic
space and time complexity as the classical Needleman-Wunsch algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04785</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04785</id><created>2015-08-19</created><authors><author><keyname>Chen</keyname><forenames>KuanTing</forenames></author><author><keyname>Chen</keyname><forenames>Kezhen</forenames></author><author><keyname>Cong</keyname><forenames>Peizhong</forenames></author><author><keyname>Hsu</keyname><forenames>Winston H.</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>Who are the Devils Wearing Prada in New York City?</title><categories>cs.CV cs.CY</categories><doi>10.1145/2733373.2809930</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fashion is a perpetual topic in human social life, and the mass has the
penchant to emulate what large city residents and celebrities wear. Undeniably,
New York City is such a bellwether large city with all kinds of fashion
leadership. Consequently, to study what the fashion trends are during this
year, it is very helpful to learn the fashion trends of New York City.
Discovering fashion trends in New York City could boost many applications such
as clothing recommendation and advertising. Does the fashion trend in the New
York Fashion Show actually influence the clothing styles on the public? To
answer this question, we design a novel system that consists of three major
components: (1) constructing a large dataset from the New York Fashion Shows
and New York street chic in order to understand the likely clothing fashion
trends in New York, (2) utilizing a learning-based approach to discover fashion
attributes as the representative characteristics of fashion trends, and (3)
comparing the analysis results from the New York Fashion Shows and street-chic
images to verify whether the fashion shows have actual influence on the people
in New York City. Through the preliminary experiments over a large clothing
dataset, we demonstrate the effectiveness of our proposed system, and obtain
useful insights on fashion trends and fashion influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04789</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04789</id><created>2015-08-19</created><authors><author><keyname>Rello</keyname><forenames>Luz</forenames></author><author><keyname>Bayarri</keyname><forenames>Clara</forenames></author><author><keyname>Otal</keyname><forenames>Yolanda</forenames></author><author><keyname>Pielot</keyname><forenames>Martin</forenames></author></authors><title>A Computer-Based Method to Improve the Spelling of Children with
  Dyslexia</title><categories>cs.HC</categories><comments>8 pages, ASSETS'14, October 20-22, 2014, Rochester, NY, USA</comments><acm-class>K.4.2; K.3</acm-class><doi>10.1145/2661334.2661373</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method which aims to improve the spelling of
children with dyslexia through playful and targeted exercises. In contrast to
previous approaches, our method does not use correct words or positive examples
to follow, but presents the child a misspelled word as an exercise to solve. We
created these training exercises on the basis of the linguistic knowledge
extracted from the errors found in texts written by children with dyslexia. To
test the effectiveness of this method in Spanish, we integrated the exercises
in a game for iPad, DysEggxia (Piruletras in Spanish), and carried out a
within-subject experiment. During eight weeks, 48 children played either
DysEggxia or Word Search, which is another word game. We conducted tests and
questionnaires at the beginning of the study, after four weeks when the games
were switched, and at the end of the study. The children who played DysEggxia
for four weeks in a row had significantly less writing errors in the tests that
after playing Word Search for the same time. This provides evidence that
error-based exercises presented in a tablet help children with dyslexia improve
their spelling skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04804</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04804</id><created>2015-08-19</created><authors><author><keyname>Rajan</keyname><forenames>Adithya</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author><author><keyname>Zeng</keyname><forenames>Ruochen</forenames></author></authors><title>A Unified Fading Model Using Infinitely Divisible Distributions</title><categories>cs.IT math.IT</categories><comments>28 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to unify fading distributions by modeling the
magnitude-squared of the instantaneous channel gain as an infinitely divisible
random variable. A random variable is said to be infinitely divisible, if it
can be written as a sum of $n \geq 1$ independent and identically distributed
random variables, for each $n$. Infinitely divisible random variables have many
interesting mathematical properties, which can be applied in the performance
analysis of wireless systems. It is shown that the proposed unification
subsumes several unifications of fading distributions previously proposed in
the wireless communications literature. In fact, almost every distribution used
to model multipath, shadowing and composite multipath/shadowing is shown to be
included in the class of infinitely divisible random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04816</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04816</id><created>2015-08-19</created><updated>2015-10-01</updated><authors><author><keyname>Tanburn</keyname><forenames>Richard</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Okada</keyname><forenames>Emile</forenames><affiliation>Cambridge University</affiliation></author><author><keyname>Dattani</keyname><forenames>Nike</forenames><affiliation>Kyoto University</affiliation></author></authors><title>Reducing multi-qubit interactions in adiabatic quantum computation
  without adding auxiliary qubits. Part 1: The &quot;deduc-reduc&quot; method and its
  application to quantum factorization of numbers</title><categories>quant-ph cs.DM cs.DS math.NT</categories><msc-class>05C50, 11A41, 11A51, 11N35, 11N36, 11N80, 11Y05, 65K10, 65P10,
  65Y20, 68Q12, 81P68, 81P94, 94A60, 81-08</msc-class><acm-class>B.2.4; B.8.2; C.1.3; C.1.m; F.2.1; F.2.3; F.4.1; G.1.0; G.1.3;
  G.1.5; G.1.6; G.2.0; G.2.1; I.1.2; I.6.4; C.4; E.3; G.0; J.2; K.2</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Adiabatic quantum computing has recently been used to factor 56153 [Dattani &amp;
Bryans, arXiv:1411.6758] at room temperature, which is orders of magnitude
larger than any number attempted yet using Shor's algorithm (circuit-based
quantum computation). However, this number is still vastly smaller than RSA-768
which is the largest number factored thus far on a classical computer. We
address a major issue arising in the scaling of adiabatic quantum factorization
to much larger numbers. Namely, the existence of many 4-qubit, 3-qubit and
2-qubit interactions in the Hamiltonians. We showcase our method on various
examples, one of which shows that we can remove 94% of the 4-qubit interactions
and 83% of the 3-qubit interactions in the factorization of a 25-digit number
with almost no effort, without adding any auxiliary qubits. Our method is not
limited to quantum factoring. Its importance extends to the wider field of
discrete optimization. Any CSP (constraint-satisfiability problem),
psuedo-boolean optimization problem, or QUBO (quadratic unconstrained Boolean
optimization) problem can in principle benefit from the &quot;deduction-reduction&quot;
method which we introduce in this paper. We provide an open source code which
takes in a Hamiltonian (or a discrete discrete function which needs to be
optimized), and returns a Hamiltonian that has the same unique ground state(s),
no new auxiliary variables, and as few multi-qubit (multi-variable) terms as
possible with deduc-reduc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04819</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04819</id><created>2015-08-19</created><updated>2015-08-28</updated><authors><author><keyname>Keegan</keyname><forenames>Brian C.</forenames></author><author><keyname>Lev</keyname><forenames>Shakked</forenames></author><author><keyname>Arazy</keyname><forenames>Ofer</forenames></author></authors><title>Analyzing Organizational Routines in Online Knowledge Collaborations: A
  Case for Sequence Analysis in CSCW</title><categories>cs.SI cs.HC cs.IR physics.data-an stat.AP</categories><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research into socio-technical systems like Wikipedia has overlooked important
structural patterns in the coordination of distributed work. This paper argues
for a conceptual reorientation towards sequences as a fundamental unit of
analysis for understanding work routines in online knowledge collaboration. We
outline a research agenda for researchers in computer-supported cooperative
work (CSCW) to understand the relationships, patterns, antecedents, and
consequences of sequential behavior using methods already developed in fields
like bio-informatics. Using a data set of 37,515 revisions from 16,616 unique
editors to 96 Wikipedia articles as a case study, we analyze the prevalence and
significance of different sequences of editing patterns. We illustrate the
mixed method potential of sequence approaches by interpreting the frequent
patterns as general classes of behavioral motifs. We conclude by discussing the
methodological opportunities for using sequence analysis for expanding existing
approaches to analyzing and theorizing about co-production routines in online
knowledge collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04826</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04826</id><created>2015-08-19</created><updated>2015-08-26</updated><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Dither is Better than Dropout for Regularising Deep Neural Networks</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularisation of deep neural networks (DNN) during training is critical to
performance. By far the most popular method is known as dropout. Here, cast
through the prism of signal processing theory, we compare and contrast the
regularisation effects of dropout with those of dither. We illustrate some
serious inherent limitations of dropout and demonstrate that dither provides a
more effective regulariser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04839</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04839</id><created>2015-08-19</created><authors><author><keyname>Nikoue</keyname><forenames>Harold</forenames></author><author><keyname>Marzuoli</keyname><forenames>Aude</forenames></author><author><keyname>Clarke</keyname><forenames>John-Paul</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author><author><keyname>Peters</keyname><forenames>Jim</forenames></author></authors><title>Passenger Flow Predictions at Sydney International Airport: A
  Data-Driven Queuing Approach</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time spent in processing zones at an airport are an important part of the
passenger's airport experience. It undercuts the time spent in the rest of the
airport, and therefore the revenue that could be generated from shopping and
dining. It can also result in passengers missing flights and connections, which
has significant operational repercussions. Inadequate staffing levels are often
to blame for large congestion at an airport. In this paper, we present a
stochastic simulation that estimates the operational uncertainty in passenger
processing at immigration. Congestion and delays are estimated on arrivals and
departures based on scheduled flight departures and arrivals. We demonstrate
the use of cellular tracking data in refining the model, and an approach to
controlling congestion by adjusting staffing levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04843</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04843</id><created>2015-08-19</created><authors><author><keyname>Lee</keyname><forenames>Kisuk</forenames></author><author><keyname>Zlateski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Vishwanathan</keyname><forenames>Ashwin</forenames></author><author><keyname>Seung</keyname><forenames>H. Sebastian</forenames></author></authors><title>Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary
  Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efforts to automate the reconstruction of neural circuits from 3D electron
microscopic (EM) brain images are critical for the field of connectomics. An
important computation for reconstruction is the detection of neuronal
boundaries. Images acquired by serial section EM, a leading 3D EM technique,
are highly anisotropic, with inferior quality along the third dimension. For
such images, the 2D max-pooling convolutional network has set the standard for
performance at boundary detection. Here we achieve a substantial gain in
accuracy through three innovations. Following the trend towards deeper networks
for object recognition, we use a much deeper network than previously employed
for boundary detection. Second, we incorporate 3D as well as 2D filters, to
enable computations that use 3D context. Finally, we adopt a recursively
trained architecture in which a first network generates a preliminary boundary
map that is provided as input along with the original image to a second network
that generates a final boundary map. Backpropagation training is accelerated by
ZNN, a new implementation of 3D convolutional networks that uses multicore CPU
parallelism for speed. Our hybrid 2D-3D architecture could be more generally
applicable to other types of anisotropic 3D images, including video, and our
recursive framework for any image labeling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04848</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04848</id><created>2015-08-19</created><authors><author><keyname>Dokter</keyname><forenames>Kasper</forenames><affiliation>CWI</affiliation></author><author><keyname>Jongmans</keyname><forenames>Sung-Shik</forenames><affiliation>CWI</affiliation></author><author><keyname>Arbab</keyname><forenames>Farhad</forenames><affiliation>CWI</affiliation></author><author><keyname>Bliudze</keyname><forenames>Simon</forenames><affiliation>EPFL</affiliation></author></authors><title>Relating BIP and Reo</title><categories>cs.PL</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>D.3.1</acm-class><journal-ref>EPTCS 189, 2015, pp. 3-20</journal-ref><doi>10.4204/EPTCS.189.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordination languages simplify design and development of concurrent systems.
Particularly, exogenous coordination languages, like BIP and Reo, enable system
designers to express the interactions among components in a system explicitly.
In this paper we establish a formal relation between BI(P) (i.e., BIP without
the priority layer) and Reo, by defining transformations between their semantic
models. We show that these transformations preserve all properties expressible
in a common semantics. This formal relation comprises the basis for a solid
comparison and consolidation of the fundamental coordination concepts behind
these two languages. Moreover, this basis offers translations that enable users
of either language to benefit from the toolchains of the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04849</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04849</id><created>2015-08-19</created><authors><author><keyname>Barbanera</keyname><forenames>Franco</forenames><affiliation>Dipartimento di Matematica e Informatica, University of Catania</affiliation></author><author><keyname>van Bakel</keyname><forenames>Steffen</forenames><affiliation>Department of Computing, Imperial College London</affiliation></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames><affiliation>Dipartimento di Informatica, University of Torino</affiliation></author></authors><title>Orchestrated Session Compliance</title><categories>cs.LO</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>F.1.2; F.3.1; F.3.3</acm-class><journal-ref>EPTCS 189, 2015, pp. 21-36</journal-ref><doi>10.4204/EPTCS.189.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the notion of orchestrated compliance for client/server
interactions in the context of session contracts. Devising the notion of
orchestrator in such a context makes it possible to have orchestrators with
unbounded buffering capabilities and at the same time to guarantee any message
from the client to be eventually delivered by the orchestrator to the server,
while preventing the server from sending messages which are kept indefinitely
inside the orchestrator. The compliance relation is shown to be decidable by
means of 1) a procedure synthesising the orchestrators, if any, making a client
compliant with a server, and 2) a procedure for deciding whether an
orchestrator behaves in a proper way as mentioned before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04850</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04850</id><created>2015-08-19</created><authors><author><keyname>Luttik</keyname><forenames>Bas</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Yang</keyname><forenames>Fei</forenames><affiliation>Eindhoven University of Technology</affiliation></author></authors><title>Executable Behaviour and the \pi-Calculus (extended abstract)</title><categories>cs.LO</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595. arXiv admin note:
  substantial text overlap with arXiv:1410.4512</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.2; F.3.2</acm-class><journal-ref>EPTCS 189, 2015, pp. 37-52</journal-ref><doi>10.4204/EPTCS.189.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive Turing machines extend classical Turing machines with a facility to
model observable interactive behaviour. We call a behaviour executable if, and
only if, it is behaviourally equivalent to the behaviour of a reactive Turing
machine. In this paper, we study the relationship between executable behaviour
and behaviour that can be specified in the pi-calculus. We establish that all
executable behaviour can be specified in the pi-calculus up to
divergence-preserving branching bisimilarity. The converse, however, is not
true due to (intended) limitations of the model of reactive Turing machines.
That is, the pi-calculus allows the specification of behaviour that is not
executable up to divergence-preserving branching bisimilarity. Motivated by an
intuitive understanding of executability, we then consider a restriction on the
operational semantics of the pi-calculus that does associate with every pi-term
executable behaviour, at least up to the version of branching bisimilarity that
does not require the preservation of divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04851</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04851</id><created>2015-08-19</created><authors><author><keyname>Best</keyname><forenames>Eike</forenames><affiliation>Oldenburg</affiliation></author><author><keyname>Schlachter</keyname><forenames>Uli</forenames><affiliation>Oldenburg</affiliation></author></authors><title>Analysis of Petri Nets and Transition Systems</title><categories>cs.LO</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>F.3.2; D.2.2</acm-class><journal-ref>EPTCS 189, 2015, pp. 53-67</journal-ref><doi>10.4204/EPTCS.189.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a stand-alone, no-frills tool supporting the analysis of
(labelled) place/transition Petri nets and the synthesis of labelled transition
systems into Petri nets. It is implemented as a collection of independent,
dedicated algorithms which have been designed to operate modularly, portably,
extensibly, and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04852</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04852</id><created>2015-08-19</created><authors><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA -- Univ. Paris-Est, LACL</affiliation></author><author><keyname>Cristescu</keyname><forenames>Ioana</forenames><affiliation>Univ. Paris Diderot, P.P.S.</affiliation></author></authors><title>Reversible Barbed Congruence on Configuration Structures</title><categories>cs.LO</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>F.1.2; F.3.2 ; D.3.1</acm-class><journal-ref>EPTCS 189, 2015, pp. 68-85</journal-ref><doi>10.4204/EPTCS.189.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard contextual equivalence for process algebras is strong barbed
congruence. Configuration structures are a denotational semantics for processes
in which one can define equivalences that are more discriminating, i.e. that
distinguish the denotation of terms equated by barbed congruence. Hereditary
history preserving bisimulation (HHPB) is such a relation. We define a strong
back and forth barbed congruence using a reversible process algebra and show
that the relation induced by the back and forth congruence is equivalent to
HHPB, providing a contextual characterization of HHPB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04853</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04853</id><created>2015-08-19</created><authors><author><keyname>Scalas</keyname><forenames>Alceste</forenames><affiliation>Universit&#xe0; di Cagliari, Italy and Imperial College London, UK</affiliation></author><author><keyname>Bartoletti</keyname><forenames>Massimo</forenames><affiliation>Universit&#xe0; di Cagliari, Italy</affiliation></author></authors><title>The LTS WorkBench</title><categories>cs.LO cs.PL cs.SE</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>D.2.2; D.3.1; F.3.2</acm-class><journal-ref>EPTCS 189, 2015, pp. 86-98</journal-ref><doi>10.4204/EPTCS.189.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labelled Transition Systems (LTSs) are a fundamental semantic model in many
areas of informatics, especially concurrency theory. Yet, reasoning on LTSs and
relations between their states can be difficult and elusive: very simple
process algebra terms can give rise to a large (possibly infinite) number of
intricate transitions and interactions. To ease this kind of study, we present
LTSwb, a flexible and extensible LTS toolbox: this tutorial paper discusses its
design and functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04854</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04854</id><created>2015-08-19</created><authors><author><keyname>Given-Wilson</keyname><forenames>Thomas</forenames><affiliation>Inria</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>Inria</affiliation></author></authors><title>On the Expressiveness of Joining</title><categories>cs.LO</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595. arXiv admin note:
  substantial text overlap with arXiv:1408.1455</comments><proxy>EPTCS</proxy><acm-class>F.1.2; D.3.3</acm-class><journal-ref>EPTCS 189, 2015, pp. 99-113</journal-ref><doi>10.4204/EPTCS.189.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expressiveness of communication primitives has been explored in a common
framework based on the pi-calculus by considering four features: synchronism
(asynchronous vs synchronous), arity (monadic vs polyadic data), communication
medium (shared dataspaces vs channel-based), and pattern-matching (binding to a
name vs testing name equality vs intensionality). Here another dimension
coordination is considered that accounts for the number of processes required
for an interaction to occur. Coordination generalises binary languages such as
pi-calculus to joining languages that combine inputs such as the Join Calculus
and general rendezvous calculus. By means of possibility/impossibility of
encodings, this paper shows coordination is unrelated to the other features.
That is, joining languages are more expressive than binary languages, and no
combination of the other features can encode a joining language into a binary
language. Further, joining is not able to encode any of the other features
unless they could be encoded otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04855</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04855</id><created>2015-08-19</created><authors><author><keyname>Xu</keyname><forenames>Xian</forenames><affiliation>East China University of Science and Technology</affiliation></author><author><keyname>Yin</keyname><forenames>Qiang</forenames><affiliation>Shanghai Jiao Tong University</affiliation></author><author><keyname>Long</keyname><forenames>Huan</forenames><affiliation>Shanghai Jiao Tong University</affiliation></author></authors><title>On the Computation Power of Name Parameterization in Higher-order
  Processes</title><categories>cs.LO</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>F.1.2</acm-class><journal-ref>EPTCS 189, 2015, pp. 114-127</journal-ref><doi>10.4204/EPTCS.189.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameterization extends higher-order processes with the capability of
abstraction (akin to that in lambda-calculus), and is known to be able to
enhance the expressiveness. This paper focuses on the parameterization of
names, i.e. a construct that maps a name to a process, in the higher-order
setting. We provide two results concerning its computation capacity. First,
name parameterization brings up a complete model, in the sense that it can
express an elementary interactive model with built-in recursive functions.
Second, we compare name parameterization with the well-known pi-calculus, and
provide two encodings between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04856</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04856</id><created>2015-08-19</created><authors><author><keyname>Santos</keyname><forenames>C&#xe9;sar</forenames><affiliation>Lasige, Faculty of Sciences, University of Lisbon, Portugal</affiliation></author><author><keyname>Martins</keyname><forenames>Francisco</forenames><affiliation>Lasige, Faculty of Sciences, University of Lisbon, Portugal</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco Thudichum</forenames><affiliation>Lasige, Faculty of Sciences, University of Lisbon, Portugal</affiliation></author></authors><title>Deductive Verification of Parallel Programs Using Why3</title><categories>cs.PL cs.DC</categories><comments>In Proceedings ICE 2015, arXiv:1508.04595</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1</acm-class><journal-ref>EPTCS 189, 2015, pp. 128-142</journal-ref><doi>10.4204/EPTCS.189.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Message Passing Interface specification (MPI) defines a portable
message-passing API used to program parallel computers. MPI programs manifest a
number of challenges on what concerns correctness: sent and expected values in
communications may not match, resulting in incorrect computations possibly
leading to crashes; and programs may deadlock resulting in wasted resources.
Existing tools are not completely satisfactory: model-checking does not scale
with the number of processes; testing techniques wastes resources and are
highly dependent on the quality of the test set.
  As an alternative, we present a prototype for a type-based approach to
programming and verifying MPI like programs against protocols. Protocols are
written in a dependent type language designed so as to capture the most common
primitives in MPI, incorporating, in addition, a form of primitive recursion
and collective choice. Protocols are then translated into Why3, a deductive
software verification tool. Source code, in turn, is written in WhyML, the
language of the Why3 platform, and checked against the protocol. Programs that
pass verification are guaranteed to be communication safe and free from
deadlocks.
  We verified several parallel programs from textbooks using our approach, and
report on the outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04863</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04863</id><created>2015-08-19</created><authors><author><keyname>Soelistio</keyname><forenames>Yustinus Eko</forenames></author></authors><title>Application Distribution Model In Volunteer Computing Environment Using
  Peer-to-Peer Torrent Like Approach</title><categories>cs.DC</categories><comments>6 pages, Published in proceeding of 2013 International Conference on
  Computer, Control, Informatics and Its Applications (IC3INA)</comments><doi>10.1109/IC3INA.2013.6819184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volunteer computing has been known as an alternative solution to solve
complex problems. It is acknowledged for its simplicity and its ability to work
on multiple operating systems. Nonetheless, setting up a server for volunteer
computing can be time consuming and relatively complex to be implemented. This
paper offer a model which can ease the effort of setting up a server by making
the agent works two ways, as seeder and leecher, like P2P torrent approaches.
The model consists of measurement units to manage applications to be
distributed, system hierarchy, and basic procedures for the server and the
agent. The model has been tested in four scenarios using 2,000,000 to 3,000,000
integer data employing up to six nodes. The tests demonstrate speedup in three
of the scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04868</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04868</id><created>2015-08-19</created><updated>2015-08-20</updated><authors><author><keyname>Wilson</keyname><forenames>Duane</forenames></author><author><keyname>Ateniese</keyname><forenames>Giuseppe</forenames></author></authors><title>From Pretty Good To Great: Enhancing PGP using Bitcoin and the
  Blockchain</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PGP is built upon a Distributed Web of Trust in which the trustworthiness of
a user is established by others who can vouch through a digital signature for
that particular identity. Preventing its wholesale adoption are a number of
inherent weaknesses to include (but not limited to) the following: 1) Trust
Relationships are built on a subjective honor system, 2) Only first degree
relationships can be fully trusted, 3) Levels of trust are difficult to
quantify with actual values, and 4) Issues with the Web of Trust itself
(Certification and Endorsement). Although the security that PGP provides is
proven to be reliable, it has largely failed to garner large scale adoption. In
this paper, we propose several novel contributions to address the
aforementioned issues with PGP and associated Web of Trust. To address the
subjectivity of the Web of Trust, we provide a new certificate format based on
Bitcoin which allows a user to verify a PGP certificate using Bitcoin
identity-verification transactions - forming first degree trust relationships
that are tied to actual values (i.e., number of Bitcoins transferred during
transaction). Secondly, we present the design of a novel Distributed PGP key
server that leverages the Bitcoin transaction blockchain to store and retrieve
Bitcoin-Based PGP certificates. Lastly, we provide a web prototype application
that demonstrates several of these capabilities in an actual environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04872</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04872</id><created>2015-08-20</created><authors><author><keyname>Haryanto</keyname><forenames>Ardy Wibowo</forenames></author><author><keyname>Kusnadi</keyname><forenames>Adhi</forenames></author><author><keyname>Soelistio</keyname><forenames>Yustinus Eko</forenames></author></authors><title>Warehouse Layout Method Based on Ant Colony and Backtracking Algorithm</title><categories>cs.AI</categories><comments>5 pages, published in proceeding of the 14th IAPR International
  Conference on Quality in Research (QIR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Warehouse is one of the important aspects of a company. Therefore, it is
necessary to improve Warehouse Management System (WMS) to have a simple
function that can determine the layout of the storage goods. In this paper we
propose an improved warehouse layout method based on ant colony algorithm and
backtracking algorithm. The method works on two steps. First, it generates a
solutions parameter tree from backtracking algorithm. Then second, it deducts
the solutions parameter by using a combination of ant colony algorithm and
backtracking algorithm. This method was tested by measuring the time needed to
build the tree and to fill up the space using two scenarios. The method needs
0.294 to 33.15 seconds to construct the tree and 3.23 seconds (best case) to
61.41 minutes (worst case) to fill up the warehouse. This method is proved to
be an attractive alternative solution for warehouse layout system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04874</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04874</id><created>2015-08-20</created><updated>2015-11-05</updated><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author><author><keyname>Wong</keyname><forenames>Sam Chiu-wai</forenames></author></authors><title>A Faster Cutting Plane Method and its Implications for Combinatorial and
  Convex Optimization</title><categories>cs.DS cs.DM math.NA math.OC</categories><comments>111 pages, FOCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve upon the running time for finding a point in a convex set given a
separation oracle. In particular, given a separation oracle for a convex set
$K\subset \mathbb{R}^n$ contained in a box of radius $R$, we show how to either
find a point in $K$ or prove that $K$ does not contain a ball of radius
$\epsilon$ using an expected $O(n\log(nR/\epsilon))$ oracle evaluations and
additional time $O(n^3\log^{O(1)}(nR/\epsilon))$. This matches the oracle
complexity and improves upon the $O(n^{\omega+1}\log(nR/\epsilon))$ additional
time of the previous fastest algorithm achieved over 25 years ago by Vaidya for
the current matrix multiplication constant $\omega&lt;2.373$ when
$R/\epsilon=n^{O(1)}$.
  Using a mix of standard reductions and new techniques, our algorithm yields
improved runtimes for solving classic problems in continuous and combinatorial
optimization:
  Submodular Minimization: Our weakly and strongly polynomial time algorithms
have runtimes of $O(n^2\log nM\cdot\text{EO}+n^3\log^{O(1)}nM)$ and
$O(n^3\log^2 n\cdot\text{EO}+n^4\log^{O(1)}n)$, improving upon the previous
best of $O((n^4\text{EO}+n^5)\log M)$ and $O(n^5\text{EO}+n^6)$.
  Matroid Intersection: Our runtimes are $O(nrT_{\text{rank}}\log n\log (nM)
+n^3\log^{O(1)}(nM))$ and $O(n^2\log (nM) T_{\text{ind}}+n^3 \log^{O(1)}
(nM))$, achieving the first quadratic bound on the query complexity for the
independence and rank oracles. In the unweighted case, this is the first
improvement since 1986 for independence oracle.
  Submodular Flow: Our runtime is $O(n^2\log
nCU\cdot\text{EO}+n^3\log^{O(1)}nCU)$, improving upon the previous bests from
15 years ago roughly by a factor of $O(n^4)$.
  Semidefinite Programming: Our runtime is $\tilde{O}(n(n^2+m^{\omega}+S))$,
improving upon the previous best of $\tilde{O}(n(n^{\omega}+m^{\omega}+S))$ for
the regime where the number of nonzeros $S$ is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04885</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04885</id><created>2015-08-20</created><authors><author><keyname>Blom</keyname><forenames>Michelle</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author><author><keyname>Teague</keyname><forenames>Vanessa J.</forenames></author><author><keyname>Tidhar</keyname><forenames>Ron</forenames></author></authors><title>Efficient Computation of Exact IRV Margins</title><categories>cs.AI</categories><comments>20 pages, 6 tables, 6 figures</comments><msc-class>68T20, 90C05</msc-class><acm-class>G.1.6; G.2.1; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The margin of victory is easy to compute for many election schemes but
difficult for Instant Runoff Voting (IRV). This is important because arguments
about the correctness of an election outcome usually rely on the size of the
electoral margin. For example, risk-limiting audits require a knowledge of the
margin of victory in order to determine how much auditing is necessary. This
paper presents a practical branch-and-bound algorithm for exact IRV margin
computation that substantially improves on the current best-known approach.
Although exponential in the worst case, our algorithm runs efficiently in
practice on all the real examples we could find. We can efficiently discover
exact margins on election instances that cannot be solved by the current
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04886</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04886</id><created>2015-08-20</created><authors><author><keyname>Ji</keyname><forenames>Ankyda</forenames></author><author><keyname>Turkoglu</keyname><forenames>Kamran</forenames></author></authors><title>Development of a Low-Cost Experimental Quadcopter Testbed Using an
  Arduino Controller and Software</title><categories>math.OC cs.RO</categories><comments>(under review in SAGE Transactions of the Institute of Measurement
  and Control)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explains the integration process of an autonomous quadcopter
platform and the design of Arduino based novel software architecture that
enables the execution of advanced control laws on low-cost off-the-shelf
products based frameworks. Here, quadcopter dynamics are explored through the
classical nonlinear equations of motion. Next, quadcopter is designed, built
and assembled using off-the-shelf, low-cost products to carry a camera payload
which is mainly utilized for any type of surveillance missions. System
identification of the quadcopter dynamics is accomplished through the use of
sweep data and $CIFER^{\circledR}$ to obtain the dynamic model. The unstable,
non-linear quadcopter dynamics are stabilized using a generic control algorithm
through the novel Arduino based software architecture. Experimental results
demonstrate the validation of the integration and the novel software package
running on an Arduino board to control autonomous quadcopter flights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04887</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04887</id><created>2015-08-20</created><authors><author><keyname>Hsiao</keyname><forenames>Ko-Jen</forenames></author><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author><author><keyname>Calder</keyname><forenames>Jeff</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Multi-criteria Similarity-based Anomaly Detection using Pareto Depth
  Analysis</title><categories>cs.CV cs.LG stat.ML</categories><comments>The work is submitted to IEEE TNNLS Special Issue on Learning in
  Non-(geo)metric Spaces for review on October 28, 2013, revised on July 26,
  2015 and accepted on July 30, 2015. A preliminary version of this work is
  reported in the conference Advances in Neural Information Processing Systems
  (NIPS) 2012</comments><doi>10.1109/TNNLS.2015.2466686</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying patterns in a data set that exhibit
anomalous behavior, often referred to as anomaly detection. Similarity-based
anomaly detection algorithms detect abnormally large amounts of similarity or
dissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between
a test sample and the training samples. In many application domains there may
not exist a single dissimilarity measure that captures all possible anomalous
patterns. In such cases, multiple dissimilarity measures can be defined,
including non-metric measures, and one can test for anomalies by scalarizing
using a non-negative linear combination of them. If the relative importance of
the different dissimilarity measures are not known in advance, as in many
anomaly detection applications, the anomaly detection algorithm may need to be
executed multiple times with different choices of weights in the linear
combination. In this paper, we propose a method for similarity-based anomaly
detection using a novel multi-criteria dissimilarity measure, the Pareto depth.
The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the
concept of Pareto optimality to detect anomalies under multiple criteria
without having to run an algorithm multiple times with different choices of
weights. The proposed PDA approach is provably better than using linear
combinations of the criteria and shows superior performance on experiments with
synthetic and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04893</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04893</id><created>2015-08-20</created><authors><author><keyname>Burshtein</keyname><forenames>Amir</forenames></author><author><keyname>Birk</keyname><forenames>Michael</forenames></author><author><keyname>Chernyakova</keyname><forenames>Tanya</forenames></author><author><keyname>Eilam</keyname><forenames>Alon</forenames></author><author><keyname>Kempinskiand</keyname><forenames>Arcady</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Sub-Nyquist Sampling and Fourier Domain Beamforming in Volumetric
  Ultrasound Imaging</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key steps in ultrasound image formation is digital beamforming of
signals sampled by several transducer elements placed upon an array.
High-resolution digital beamforming introduces the demand for sampling rates
significantly higher than the signals' Nyquist rate, which greatly increases
the volume of data that must be transmitted from the system's front end. In 3D
ultrasound imaging, 2D transducer arrays rather than 1D arrays are used, and
more scan-lines are needed. This implies that the amount of sampled data is
vastly increased with respect to 2D imaging. In this work we show that a
considerable reduction in data rate can be achieved by applying the ideas of
Xampling and frequency domain beamforming, leading to a sub-Nyquist sampling
rate, which uses only a portion of the bandwidth of the ultrasound signals to
reconstruct the image. We extend previous work on frequency domain beamforming
for 2D ultrasound imaging to accommodate the geometry imposed by volumetric
scanning and a 2D grid of transducer elements. We demonstrate high image
quality from low-rate samples by simulation of a phantom image comprised of
several small reflectors. We also apply our technique on raw data of a heart
ventricle phantom obtained by a commercial 3D ultrasound system. We show that
by performing 3D beamforming in the frequency domain, sub-Nyquist sampling and
low processing rate are achievable, while maintaining adequate image quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04904</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04904</id><created>2015-08-20</created><authors><author><keyname>Besse</keyname><forenames>Philippe</forenames><affiliation>INSA Toulouse, IMT</affiliation></author><author><keyname>Guillouet</keyname><forenames>Brendan</forenames><affiliation>IMT</affiliation></author><author><keyname>Loubes</keyname><forenames>Jean-Michel</forenames></author><author><keyname>Fran&#xe7;ois</keyname><forenames>Royer</forenames></author></authors><title>Review and Perspective for Distance Based Trajectory Clustering</title><categories>stat.ML cs.LG stat.AP</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we tackle the issue of clustering trajectories of geolocalized
observations. Using clustering technics based on the choice of a distance
between the observations, we first provide a comprehensive review of the
different distances used in the literature to compare trajectories. Then based
on the limitations of these methods, we introduce a new distance : Symmetrized
Segment-Path Distance (SSPD). We finally compare this new distance to the
others according to their corresponding clustering results obtained using both
hierarchical clustering and affinity propagation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04906</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04906</id><created>2015-08-20</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author><author><keyname>Mishenin</keyname><forenames>Alexey</forenames></author></authors><title>Semi-supervised Learning with Regularized Laplacian</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a semi-supervised learning method based on the similarity graph and
RegularizedLaplacian. We give convenient optimization formulation of the
Regularized Laplacian method and establishits various properties. In
particular, we show that the kernel of the methodcan be interpreted in terms of
discrete and continuous time random walks and possesses several
importantproperties of proximity measures. Both optimization and linear algebra
methods can be used for efficientcomputation of the classification functions.
We demonstrate on numerical examples that theRegularized Laplacian method is
competitive with respect to the other state of the art semi-supervisedlearning
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04907</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04907</id><created>2015-08-20</created><updated>2016-02-04</updated><authors><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Zhou</keyname><forenames>Yongluan</forenames></author></authors><title>Tolerating Correlated Failures in Massively Parallel Stream Processing
  Engines</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault-tolerance techniques for stream processing engines can be categorized
into passive and active approaches. A typical passive approach periodically
checkpoints a processing task's runtime states and can recover a failed task by
restoring its runtime state using its latest checkpoint. On the other hand, an
active approach usually employs backup nodes to run replicated tasks. Upon
failure, the active replica can take over the processing of the failed task
with minimal latency. However, both approaches have their own inadequacies in
Massively Parallel Stream Processing Engines (MPSPE). The passive approach
incurs a long recovery latency especially when a number of correlated nodes
fail simultaneously, while the active approach requires extra replication
resources. In this paper, we propose a new fault-tolerance framework, which is
Passive and Partially Active (PPA). In a PPA scheme, the passive approach is
applied to all tasks while only a selected set of tasks will be actively
replicated. The number of actively replicated tasks depends on the available
resources. If tasks without active replicas fail, tentative outputs will be
generated before the completion of the recovery process. We also propose
effective and efficient algorithms to optimize a partially active replication
plan to maximize the quality of tentative outputs. We implemented PPA on top of
Storm, an open-source MPSPE and conducted extensive experiments using both real
and synthetic datasets to verify the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04909</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04909</id><created>2015-08-20</created><authors><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author><author><keyname>Gasso</keyname><forenames>Gilles</forenames><affiliation>LITIS</affiliation></author></authors><title>Histogram of gradients of Time-Frequency Representations for Audio scene
  detection</title><categories>cs.SD cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of audio scenes classification and
contributes to the state of the art by proposing a novel feature. We build this
feature by considering histogram of gradients (HOG) of time-frequency
representation of an audio scene. Contrarily to classical audio features like
MFCC, we make the hypothesis that histogram of gradients are able to encode
some relevant informations in a time-frequency {representation:} namely, the
local direction of variation (in time and frequency) of the signal spectral
power. In addition, in order to gain more invariance and robustness, histogram
of gradients are locally pooled. We have evaluated the relevance of {the novel
feature} by comparing its performances with state-of-the-art competitors, on
several datasets, including a novel one that we provide, as part of our
contribution. This dataset, that we make publicly available, involves $19$
classes and contains about $900$ minutes of audio scene recording. We thus
believe that it may be the next standard dataset for evaluating audio scene
classification algorithms. Our comparison results clearly show that our
HOG-based features outperform its competitors
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04912</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04912</id><created>2015-08-20</created><authors><author><keyname>De Rosa</keyname><forenames>Rocco</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author></authors><title>The ABACOC Algorithm: a Novel Approach for Nonparametric Classification
  of Data Streams</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stream mining poses unique challenges to machine learning: predictive models
are required to be scalable, incrementally trainable, must remain bounded in
size (even when the data stream is arbitrarily long), and be nonparametric in
order to achieve high accuracy even in complex and dynamic environments.
Moreover, the learning system must be parameterless ---traditional tuning
methods are problematic in streaming settings--- and avoid requiring prior
knowledge of the number of distinct class labels occurring in the stream. In
this paper, we introduce a new algorithmic approach for nonparametric learning
in data streams. Our approach addresses all above mentioned challenges by
learning a model that covers the input space using simple local classifiers.
The distribution of these classifiers dynamically adapts to the local (unknown)
complexity of the classification problem, thus achieving a good balance between
model complexity and predictive accuracy. We design four variants of our
approach of increasing adaptivity. By means of an extensive empirical
evaluation against standard nonparametric baselines, we show state-of-the-art
results in terms of accuracy versus model size. For the variant that imposes a
strict bound on the model size, we show better performance against all other
methods measured at the same model size value. Our empirical analysis is
complemented by a theoretical performance guarantee which does not rely on any
stochastic assumption on the source generating the stream.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04921</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04921</id><created>2015-08-20</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Aly</keyname><forenames>Salah A.</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Robust Node Estimation and Topology Discovery Algorithm in Large-Scale
  Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel algorithm for cardinality, i.e., the number of
nodes, estimation in large scale anonymous graphs using statistical inference
methods. Applications of this work include estimating the number of sensor
devices, online social users, active protein cells, etc. In anonymous graphs,
each node possesses little or non-existing information on the network topology.
In particular, this paper assumes that each node only knows its unique
identifier. The aim is to estimate the cardinality of the graph and the
neighbours of each node by querying a small portion of them. While the former
allows the design of more efficient coding schemes for the network, the second
provides a reliable way for routing packets. As a reference for comparison,
this work considers the Best Linear Unbiased Estimators (BLUE). For dense
graphs and specific running times, the proposed algorithm produces a
cardinality estimate proportional to the BLUE. Furthermore, for an arbitrary
number of iterations, the estimate converges to the BLUE as the number of
queried nodes tends to the total number of nodes in the network. Simulation
results confirm the theoretical results by revealing that, for a moderate
running time, asking a small group of nodes is sufficient to perform an
estimation of 95% of the whole network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04924</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04924</id><created>2015-08-20</created><updated>2015-09-06</updated><authors><author><keyname>Palangi</keyname><forenames>Hamid</forenames></author><author><keyname>Ward</keyname><forenames>Rabab</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>Distributed Compressive Sensing: A Deep Learning Approach</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various studies that address the compressed sensing problem with Multiple
Measurement Vectors (MMVs) have been recently carried. These studies assume the
vectors of the different channels to be jointly sparse. In this paper, we relax
this condition. Instead we assume that these sparse vectors depend on each
other but that this dependency is unknown. We capture this dependency by
computing the conditional probability of each entry in each vector being
non-zero, given the &quot;residuals&quot; of all previous vectors. To estimate these
probabilities, we propose the use of the Long Short-Term Memory (LSTM) [1], a
data driven model for sequence modelling that is deep in time. To calculate the
model parameters, we minimize a cross entropy cost function. To reconstruct the
sparse vectors at the decoder, we propose a greedy solver that uses the above
model to estimate the conditional probabilities. By performing extensive
experiments on two real world datasets, we show that the proposed method
significantly outperforms the general MMV solver (the Simultaneous Orthogonal
Matching Pursuit (SOMP)) and the model-based Bayesian methods including
Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally
Correlated Sources [3]. The proposed method does not add any complexity to the
general compressive sensing encoder. The trained model is used just at the
decoder. As the proposed method is a data driven method, it is only applicable
when training data is available. In many applications however, training data is
indeed available, e.g. in recorded images and videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04928</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04928</id><created>2015-08-20</created><authors><author><keyname>Narimatsu</keyname><forenames>Hiromi</forenames></author><author><keyname>Kasai</keyname><forenames>Hiroyuki</forenames></author></authors><title>Duration and Interval Hidden Markov Model for Sequential Data Analysis</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of sequential event data has been recognized as one of the essential
tools in data modeling and analysis field. In this paper, after the examination
of its technical requirements and issues to model complex but practical
situation, we propose a new sequential data model, dubbed Duration and Interval
Hidden Markov Model (DI-HMM), that efficiently represents &quot;state duration&quot; and
&quot;state interval&quot; of data events. This has significant implications to play an
important role in representing practical time-series sequential data. This
eventually provides an efficient and flexible sequential data retrieval.
Numerical experiments on synthetic and real data demonstrate the efficiency and
accuracy of the proposed DI-HMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04934</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04934</id><created>2015-08-20</created><authors><author><keyname>Painsky</keyname><forenames>Amichai</forenames></author><author><keyname>Rosset</keyname><forenames>Saharon</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>Generalized Independent Component Analysis Over Finite Alphabets</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1007.0528 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent component analysis (ICA) is a statistical method for transforming
an observable multidimensional random vector into components that are as
statistically independent as possible from each other.Usually the ICA framework
assumes a model according to which the observations are generated (such as a
linear transformation with additive noise). ICA over finite fields is a special
case of ICA in which both the observations and the independent components are
over a finite alphabet. In this work we consider a generalization of this
framework in which an observation vector is decomposed to its independent
components (as much as possible) with no prior assumption on the way it was
generated. This generalization is also known as Barlow's minimal redundancy
representation problem and is considered an open problem. We propose several
theorems and show that this NP hard problem can be accurately solved with a
branch and bound search tree algorithm, or tightly approximated with a series
of linear problems. Our contribution provides the first efficient and
constructive set of solutions to Barlow's problem.The minimal redundancy
representation (also known as factorial code) has many applications, mainly in
the fields of Neural Networks and Deep Learning. The Binary ICA (BICA) is also
shown to have applications in several domains including medical diagnosis,
multi-cluster assignment, network tomography and internet resource management.
In this work we show this formulation further applies to multiple disciplines
in source coding such as predictive coding, distributed source coding and
coding of large alphabet sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04936</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04936</id><created>2015-08-20</created><authors><author><keyname>Roehner</keyname><forenames>Bertrand</forenames></author></authors><title>Translation into any natural language of the error messages generated by
  any computer program</title><categories>cs.CY</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the introduction of the Fortran programming language some 60 years ago,
there has been little progress in making error messages more user-friendly. A
first step in this direction is to translate them into the natural language of
the students. In this paper we propose a simple script for Linux systems which
gives word by word translations of error messages. It works for most
programming languages and for all natural languages. Understanding the error
messages generated by compilers is a major hurdle for students who are learning
programming, particularly for non-native English speakers. Not only may they
never become &quot;fluent&quot; in programming but many give up programming altogether.
Whereas programming is a tool which can be useful in many human activities,
e.g. history, genealogy, astronomy, entomology, in many countries the skill of
programming remains confined to a narrow fringe of professional programmers. In
all societies, besides professional violinists there are also amateurs. It
should be the same for programming. It is our hope that once translated and
explained the error messages will be seen by the students as an aid rather than
as an obstacle and that in this way more students will enjoy learning and
practising programming. They should see it as a funny game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04940</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04940</id><created>2015-08-20</created><updated>2016-02-02</updated><authors><author><keyname>Dahlqvist</keyname><forenames>Fredrik</forenames></author><author><keyname>Pym</keyname><forenames>David</forenames></author></authors><title>Coalgebraic completeness-via-canonicity for distributive substructural
  logics</title><categories>cs.LO</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove strong completeness of a range of substructural logics with respect
to a natural poset-based relational semantics using a coalgebraic version of
completeness-via-canonicity. By formalizing the problem in the language of
coalgebraic logics, we develop a modular theory which covers a wide variety of
different logics under a single framework, and lends itself to further
extensions. Moreover, we believe that the coalgebraic framework provides a
systematic and principled way to study the relationship between resource models
on the semantics side, and substructural logics on the syntactic side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04945</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04945</id><created>2015-08-20</created><updated>2015-12-22</updated><authors><author><keyname>Yang</keyname><forenames>Weixin</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Liu</keyname><forenames>Manfei</forenames></author></authors><title>DeepWriterID: An End-to-end Online Text-independent Writer
  Identification System</title><categories>cs.CV cs.LG stat.ML</categories><comments>7 pages5 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to the rapid growth of touchscreen mobile terminals and pen-based
interfaces, handwriting-based writer identification systems are attracting
increasing attention for personal authentication, digital forensics, and other
applications. However, most studies on writer identification have not been
satisfying because of the insufficiency of data and difficulty of designing
good features under various conditions of handwritings. Hence, we introduce an
end-to-end system, namely DeepWriterID, employed a deep convolutional neural
network (CNN) to address these problems. A key feature of DeepWriterID is a new
method we are proposing, called DropSegment. It designs to achieve data
augmentation and improve the generalized applicability of CNN. For sufficient
feature representation, we further introduce path signature feature maps to
improve performance. Experiments were conducted on the NLPR handwriting
database. Even though we only use pen-position information in the pen-down
state of the given handwriting samples, we achieved new state-of-the-art
identification rates of 95.72% for Chinese text and 98.51% for English text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04955</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04955</id><created>2015-08-20</created><authors><author><keyname>Konyushkova</keyname><forenames>Ksenia</forenames></author><author><keyname>Sznitman</keyname><forenames>Raphael</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Introducing Geometry in Active Learning for Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an Active Learning approach to training a segmentation classifier
that exploits geometric priors to streamline the annotation process in 3D image
volumes. To this end, we use these priors not only to select voxels most in
need of annotation but to guarantee that they lie on 2D planar patch, which
makes it much easier to annotate than if they were randomly distributed in the
volume. A simplified version of this approach is effective in natural 2D
images. We evaluated our approach on Electron Microscopy and Magnetic Resonance
image volumes, as well as on natural images. Comparing our approach against
several accepted baselines demonstrates a marked performance increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04957</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04957</id><created>2015-08-20</created><authors><author><keyname>Dimitriou</keyname><forenames>Aggeliki</forenames></author><author><keyname>Dass</keyname><forenames>Ananya</forenames></author><author><keyname>Theodoratos</keyname><forenames>Dimitri</forenames></author></authors><title>Cohesiveness Relationships to Empower Keyword Search on Tree Data on the
  Web</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword search is the most popular querying technique on semistructured data.
Keyword queries are simple and con- venient. However, as a consequence of their
imprecision, the quality of their answers is poor and the existing algorithms
do not scale satisfactorily.
  In this paper, we introduce the novel concept of cohesive keyword queries for
tree data. Intuitively, a cohesiveness relationship on keywords indicates that
they should form a cohesive whole in a query result. Cohesive keyword queries
allow term nesting and keyword repetition. Although more expressive, they are
as simple as flat keyword queries. We provide formal semantics for cohesive
keyword queries rank- ing query results on the proximity of the keyword
instances. We design a stack based algorithm which efficiently evaluates
cohesive keyword queries. Our experiments demonstrate that our approach
outperforms in quality previous filtering semantics and our algorithm scales
smoothly on queries of even 20 keywords on large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04958</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04958</id><created>2015-08-20</created><authors><author><keyname>Sinn</keyname><forenames>Moritz</forenames></author><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author></authors><title>Difference Constraints: An adequate Abstraction for Complexity Analysis
  of Imperative Programs</title><categories>cs.PL</categories><comments>This is the extended version of our paper published at FMCAD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Difference constraints have been used for termination analysis in the
literature, where they denote relational inequalities of the form x' &lt;= y + c,
and describe that the value of x in the current state is at most the value of y
in the previous state plus some integer constant c. In this paper, we argue
that the complexity of imperative programs typically arises from counter
increments and resets, which can be modeled naturally by difference
constraints. We present the first practical algorithm for the analysis of
difference constraint programs and describe how C programs can be abstracted to
difference constraint programs. Our approach contributes to the field of
automated complexity and (resource) bound analysis by enabling automated
amortized complexity analysis for a new class of programs and providing a
conceptually simple program model that relates invariant- and bound analysis.
We demonstrate the effectiveness of our approach through a thorough
experimental comparison on real world C code: our tool Loopus computes the
complexity for considerably more functions in less time than related tools from
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04973</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04973</id><created>2015-08-20</created><authors><author><keyname>Gibilisco</keyname><forenames>Giovanni Paolo</forenames></author><author><keyname>Krstic</keyname><forenames>Srdan</forenames></author></authors><title>InstaCluster: Building A Big Data Cluster in Minutes</title><categories>cs.DC</categories><comments>5 pages, 1 figure</comments><msc-class>68M14</msc-class><acm-class>D.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deploying, configuring, and managing large clusters is very a demanding and
cumbersome task due to the complexity of such systems and the variety of skills
needed. One needs to perform low-level configuration of the cluster nodes to
ensure their interoperability and connectivity, as well as install, configure
and provision the needed services.
  In this paper we address this problem and demonstrate how to build a Big Data
analytic platform on Amazon EC2 in a matter of minutes. Moreover, to use our
tool, embedded into a public Amazon Machine Image, the user does not need to be
an expert in system administration or Big Data service configuration. Our tool
dramatically reduces the time needed to provision clusters, as well as the cost
of the infrastructure. Researchers enjoy an additional benefit of having a
simple way to specify the experimental environments they use, so that their
experiments can be easily reproduced by anyone using our tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04977</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04977</id><created>2015-08-20</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author></authors><title>nanopub-java: A Java Library for Nanopublications</title><categories>cs.DL</categories><comments>Proceedings of 5th Workshop on Linked Science 2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The concept of nanopublications was first proposed about six years ago, but
it lacked openly available implementations. The library presented here is the
first one that has become an official implementation of the nanopublication
community. Its core features are stable, but it also contains unofficial and
experimental extensions: for publishing to a decentralized server network, for
defining sets of nanopublications with indexes, for informal assertions, and
for digitally signing nanopublications. Most of the features of the library can
also be accessed via an online validator interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04978</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04978</id><created>2015-08-20</created><updated>2015-09-09</updated><authors><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Janicki</keyname><forenames>Artur</forenames></author><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author></authors><title>&quot;The Good, The Bad And The Ugly&quot;: Evaluation of Wi-Fi Steganography</title><categories>cs.MM cs.CR</categories><comments>6 pages, 6 figures, to appear in Proc. of: ICNIT 2015 - 6th
  International Conference on Networking and Information Technology, Tokyo,
  Japan, November 5-6, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new method for the evaluation of network
steganography algorithms based on the new concept of &quot;the moving observer&quot;. We
considered three levels of undetectability named: &quot;good&quot;, &quot;bad&quot;, and &quot;ugly&quot;. To
illustrate this method we chose Wi-Fi steganography as a solid family of
information hiding protocols. We present the state of the art in this area
covering well-known hiding techniques for 802.11 networks. &quot;The moving
observer&quot; approach could help not only in the evaluation of steganographic
algorithms, but also might be a starting point for a new detection system of
network steganography. The concept of a new detection system, called MoveSteg,
is explained in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04981</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04981</id><created>2015-08-20</created><authors><author><keyname>Je</keyname><forenames>Changsoo</forenames></author><author><keyname>Lee</keyname><forenames>Sang Wook</forenames></author><author><keyname>Park</keyname><forenames>Rae-Hong</forenames></author></authors><title>High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range
  Imaging</title><categories>cs.CV cs.GR physics.optics</categories><comments>13 pages, 12 figures, 8th European Conference on Computer Vision
  (ECCV), Prague, Czech Republic, May 2004, Proceedings, Part I</comments><acm-class>I.2.10; I.4.8</acm-class><journal-ref>Computer Vision - ECCV 2004, LNCS 3021, pp. 95-107,
  Springer-Verlag Berlin Heidelberg, May 10, 2004</journal-ref><doi>10.1007/978-3-540-24670-1_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For structured-light range imaging, color stripes can be used for increasing
the number of distinguishable light patterns compared to binary BW stripes.
Therefore, an appropriate use of color patterns can reduce the number of light
projections and range imaging is achievable in single video frame or in &quot;one
shot&quot;. On the other hand, the reliability and range resolution attainable from
color stripes is generally lower than those from multiply projected binary BW
patterns since color contrast is affected by object color reflectance and
ambient light. This paper presents new methods for selecting stripe colors and
designing multiple-stripe patterns for &quot;one-shot&quot; and &quot;two-shot&quot; imaging. We
show that maximizing color contrast between the stripes in one-shot imaging
reduces the ambiguities resulting from colored object surfaces and limitations
in sensor/projector resolution. Two-shot imaging adds an extra video frame and
maximizes the color contrast between the first and second video frames to
diminish the ambiguities even further. Experimental results demonstrate the
effectiveness of the presented one-shot and two-shot color-stripe imaging
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.04999</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.04999</id><created>2015-08-20</created><authors><author><keyname>Nam</keyname><forenames>Juhan</forenames></author><author><keyname>Herrera</keyname><forenames>Jorge</forenames></author><author><keyname>Lee</keyname><forenames>Kyogu</forenames></author></authors><title>A Deep Bag-of-Features Model for Music Auto-Tagging</title><categories>cs.LG cs.SD stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature learning and deep learning have drawn great attention in recent years
as a way of transforming input data into more effective representations using
learning algorithms. Such interest has grown up in the area of music
information retrieval (MIR) as well, particularly in music classification tasks
such as auto-tagging. While a number of promising results have been shown, it
is not well understood what acoustic sense the learned feature representations
have and how they are associated with semantic meaning of music. In this paper,
we attempt to demystify the learned audio features using a bag-of-features
model with two learning stages. The first stage learns to project local
acoustic patterns of musical signals onto a high-dimensional sparse space in an
unsupervised manner and summarizes an audio track as a bag-of-features. The
second stage maps the bag-of-features to semantic tags using deep neural
networks in a supervised manner. For the first stage, we focus on analyzing the
learned local audio features by quantitatively measuring the acoustic
properties and interpreting the statistics in semantic context. For the second
stage, we examine training choices and tuning parameters for the neural
networks and show how the deep representations of bag-of-features become more
discriminative. Through this analysis, we not only provide better understanding
of learned local audio features but also show the effectiveness of the deep
bag-of-features model in the music auto-tagging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05002</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05002</id><created>2015-08-20</created><authors><author><keyname>Deshpande</keyname><forenames>Hrishikesh Arun</forenames></author></authors><title>HoneyMesh: Preventing Distributed Denial of Service Attacks using
  Virtualized Honeypots</title><categories>cs.CR</categories><comments>5 Pages with 4 figures and 1 table</comments><journal-ref>IJERT ISSN: 2278-0181 Vol. 4 Issue 08, August-2015 pp. 263-267</journal-ref><doi>10.17577/IJERTV4IS080325</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Today, internet and web services have become an inseparable part of our
lives. Hence, ensuring continuous availability of service has become imperative
to the success of any organization. But these services are often hampered by
constant threats from myriad types of attacks. One such attack is called
distributed denial of service attack that results in issues ranging from
temporary slowdown of servers to complete non-availability of service.
Honeypot, which is a sort of a trap, can be used to interact with potential
attackers to deflect, detect or prevent such attacks and ensure continuous
availability of service. This paper gives insights into the problems posed by
distributed denial of service attacks, existing solutions that use honeypots
and how a mesh of virtualized honeypots can be used to prevent distributed
denial of service attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05003</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05003</id><created>2015-08-20</created><authors><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author><author><keyname>Yu</keyname><forenames>Adams Wei</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Smola</keyname><forenames>Alexander J.</forenames></author></authors><title>AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization</title><categories>stat.ML cs.LG math.OC</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed stochastic convex optimization under the delayed
gradient model where the server nodes perform parameter updates, while the
worker nodes compute stochastic gradients. We discuss, analyze, and experiment
with a setup motivated by the behavior of real-world distributed computation
networks, where the machines are differently slow at different time. Therefore,
we allow the parameter updates to be sensitive to the actual delays
experienced, rather than to worst-case bounds on the maximum delay. This
sensitivity leads to larger stepsizes, that can help gain rapid initial
convergence without having to wait too long for slower machines, while
maintaining the same asymptotic complexity. We obtain encouraging improvements
to overall convergence for distributed experiments on real datasets with up to
billions of examples and features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05013</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05013</id><created>2015-08-20</created><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author></authors><title>Message Passing and Combinatorial Optimization</title><categories>cs.AI cs.CC cs.DS math.AC math.PR</categories><comments>Ravanbakhsh, S. (2015), Message Passing and Combinatorial
  Optimization, PhD thesis, University of Alberta</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models use the intuitive and well-studied methods of graph theory
to implicitly represent dependencies between variables in large systems. They
can model the global behaviour of a complex system by specifying only local
factors. This thesis studies inference in discrete graphical models from an
algebraic perspective and the ways inference can be used to express and
approximate NP-hard combinatorial problems.
  We investigate the complexity and reducibility of various inference problems,
in part by organizing them in an inference hierarchy. We then investigate
tractable approximations for a subset of these problems using distributive law
in the form of message passing. The quality of the resulting message passing
procedure, called Belief Propagation (BP), depends on the influence of loops in
the graphical model. We contribute to three classes of approximations that
improve BP for loopy graphs A) loop correction techniques; B) survey
propagation, another message passing technique that surpasses BP in some
settings; and C) hybrid methods that interpolate between deterministic message
passing and Markov Chain Monte Carlo inference.
  We then review the existing message passing solutions and provide novel
graphical models and inference techniques for combinatorial problems under
three broad classes: A) constraint satisfaction problems such as
satisfiability, coloring, packing, set / clique-cover and dominating /
independent set and their optimization counterparts; B) clustering problems
such as hierarchical clustering, K-median, K-clustering, K-center and
modularity optimization; C) problems over permutations including assignment,
graph morphisms and alignment, finding symmetries and traveling salesman
problem. In many cases we show that message passing is able to find solutions
that are either near optimal or favourably compare with today's
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05017</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05017</id><created>2015-08-20</created><updated>2015-08-20</updated><authors><author><keyname>Goyal</keyname><forenames>Sanjay</forenames></author><author><keyname>Le</keyname><forenames>Tan</forenames></author><author><keyname>Chincholi</keyname><forenames>Amith</forenames></author><author><keyname>Elkourdi</keyname><forenames>Tariq</forenames></author><author><keyname>Demir</keyname><forenames>Alpaslan</forenames></author></authors><title>On the Packet Allocation of Multi-Band Aggregation Wireless Networks</title><categories>cs.NI</categories><comments>13 pages, 14 figures, submitted to IEEE Transactions on Vehicular
  Technology for review process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of heterogeneous networks with multiple radio access technologies
(RATs) is a system concept that both academia and industry are studying and
exploring for next generation wireless networks. A multi-RAT cross-layer
technique to minimize average packet end-to-end delay for a system with a
single user terminal and a single QoS class is proposed in [1]. In this paper,
we present a generic theoretical framework to obtain the optimal packet
distribution over multiple RATs when multiple user terminals are present in the
system and also when the system supports different QoS classes simultaneously.
We also propose a packet scheduling algorithm, called OMMA Leaky Bucket, to
achieve the optimal packet distribution scheme. A description of the
Opportunistic Multi-MAC Aggregation (OMMA) system architecture is presented
which includes functional description, discovery and association processes
between multi-RAT devices and dynamic RAT update management. We finally present
simulation results which show performance gains with the proposed OMMA Leaky
Bucket scheme in comparison to other existing mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05021</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05021</id><created>2015-08-20</created><authors><author><keyname>Venkat</keyname><forenames>Ramgopal</forenames></author><author><keyname>Divagar</keyname><forenames>Thirumoorthy</forenames></author><author><keyname>Luo</keyname><forenames>Tie</forenames></author><author><keyname>Tan</keyname><forenames>Hwee Pink</forenames></author></authors><title>Participatory Sensing for Government-Centric Applications: A Singapore
  Case Study</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singapore, an urbanized and populated country with high penetration of
smartphones, provides an excellent base for citizen-centric participatory
sensing applications. Mobile participatory sensing applications offer an
efficient means of directing feedback to government agencies for timely
identifying and solving problems of citizens' concern. While real deployments
of such applications are on an uprising trend in Singapore, there is no
concerted effort that studies the {\em user experience} of these applications.
To fill this gap, we conduct a market study by analyzing the user reviews on
the Google Play and Apple App Store for six major mobile crowdsourcing
applications created by Singapore government agencies. This study was carried
out for a period of 4 months during which we collected and analyzed 592
customer reviews. This was also supplemented by our personal use of the
applications during the same period.
  This paper presents the methodology and findings of this study, as well as
our recommendations of what improvements that these applications could
incorporate. We classify user reviews into 8 major concerns, and recommend 9
features to enhance the applications' utility. The recommendations are
presented in terms of user interface, incentive, and publicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05023</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05023</id><created>2015-08-20</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Jagadeesan</keyname><forenames>Radha</forenames></author><author><keyname>V&#xe1;k&#xe1;r</keyname><forenames>Matthijs</forenames></author></authors><title>Games for Dependent Types</title><categories>cs.LO</categories><comments>revised version of ICALP 2015 publication</comments><journal-ref>ICALP 2015, Part II, LNCS 9135</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model of dependent type theory (DTT) with Pi-, 1-, Sigma- and
intensional Id-types, which is based on a slight variation of the category of
AJM-games and history-free winning strategies. The model satisfies Streicher's
criteria of intensionality and refutes function extensionality. The principle
of uniqueness of identity proofs is satisfied.
  We show it contains a submodel as a full subcategory which gives a faithful
model of DTT with Pi-, 1-, Sigma- and intensional Id-types and, additionally,
finite inductive type families. This smaller model is fully (and faithfully)
complete with respect to the syntax at the type hierarchy built without
Id-types, as well as at the class of types where we allow for one strictly
positive occurrence of an Id-type. Definability for the full type hierarchy
with Id-types remains to be investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05028</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05028</id><created>2015-08-20</created><authors><author><keyname>Li</keyname><forenames>Yuncheng</forenames></author><author><keyname>Huang</keyname><forenames>Jifei</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>Using User Generated Online Photos to Estimate and Monitor Air Pollution
  in Major Cities</title><categories>cs.CV</categories><comments>ICIMCS '15</comments><doi>10.1145/2808492.2808564</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of economy in China over the past decade, air
pollution has become an increasingly serious problem in major cities and caused
grave public health concerns in China. Recently, a number of studies have dealt
with air quality and air pollution. Among them, some attempt to predict and
monitor the air quality from different sources of information, ranging from
deployed physical sensors to social media. These methods are either too
expensive or unreliable, prompting us to search for a novel and effective way
to sense the air quality. In this study, we propose to employ the state of the
art in computer vision techniques to analyze photos that can be easily acquired
from online social media. Next, we establish the correlation between the haze
level computed directly from photos with the official PM 2.5 record of the
taken city at the taken time. Our experiments based on both synthetic and real
photos have shown the promise of this image-based approach to estimating and
monitoring air pollution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05034</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05034</id><created>2015-08-20</created><authors><author><keyname>Proskurnikov</keyname><forenames>Anton V.</forenames></author><author><keyname>Matveev</keyname><forenames>Alexey</forenames></author><author><keyname>Cao</keyname><forenames>Ming</forenames></author></authors><title>Opinion Dynamics in Social Networks with Hostile Camps: Consensus vs.
  Polarization</title><categories>cs.SY math.OC</categories><comments>scheduled for publication in IEEE Transactions on Automatic Control,
  2016, vol. 61, no. 7 (accepted in August 2015)</comments><doi>10.1109/TAC.2015.2471655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the distributed protocols for multi-agent consensus assume that the
agents are mutually cooperative and &quot;trustful,&quot; and so the couplings among the
agents bring the values of their states closer. Opinion dynamics in social
groups, however, require beyond these conventional models due to ubiquitous
competition and distrust between some pairs of agents, which are usually
characterized by repulsive couplings and may lead to clustering of the
opinions. A simple yet insightful model of opinion dynamics with both
attractive and repulsive couplings was proposed recently by C. Altafini, who
examined first-order consensus algorithms over static signed graphs. This
protocol establishes modulus consensus, where the opinions become the same in
modulus but may differ in signs. In this paper, we extend the modulus consensus
model to the case where the network topology is an arbitrary time-varying
signed graph and prove reaching modulus consensus under mild sufficient
conditions of uniform connectivity of the graph. For cut-balanced graphs, not
only sufficient, but also necessary conditions for modulus consensus are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05038</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05038</id><created>2015-08-20</created><updated>2015-11-11</updated><authors><author><keyname>Thomas</keyname><forenames>Christopher</forenames></author><author><keyname>Kovashka</keyname><forenames>Adriana</forenames></author></authors><title>Seeing Behind the Camera: Identifying the Authorship of a Photograph</title><categories>cs.CV</categories><comments>Downloadable dataset and trained CNN to appear upon publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the novel problem of identifying the photographer behind the
photograph. To explore the feasibility of current computer vision techniques to
address this problem, we created a new dataset of over 180,000 images taken by
41 well-known photographers. Using this dataset, we examined the effectiveness
of a variety of features (low and high-level, including CNN features) at
identifying the photographer. We also trained a new deep convolutional neural
network for this task. Our results show that high-level features greatly
outperform low-level features at this task. We provide qualitative results
using these learned models that give insight into our method's ability to
distinguish between photographers, allow us to draw interesting conclusions
about what specific photographers shoot, and demonstrate two applications of
our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05039</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05039</id><created>2015-08-20</created><updated>2016-01-10</updated><authors><author><keyname>Proskurnikov</keyname><forenames>Anton V.</forenames></author><author><keyname>Cao</keyname><forenames>Ming</forenames></author></authors><title>Synchronization of Goodwin's oscillators under boundedness and
  nonnegativeness constraints for solutions</title><categories>cs.SY math.OC physics.bio-ph</categories><comments>under review in IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent paper by Hamadeh et al. (2012) an elegant analytic criterion
for incremental output feedback passivity (iOFP) of cyclic feedback systems
(CFS) has been reported, assuming that the constituent subsystems are
incrementally output strictly passive (iOSP). This criterion was used to prove
that a network of identical CFS can be synchronized under sufficiently strong
linear diffusive coupling. A very important class of CFS consists of biological
oscillators, named after Brian Goodwin and describing self-regulated chains of
enzymatic reactions, where the product of each reaction catalyzes the next
reaction, while the last product inhibits the first reaction in the chain.
Goodwin's oscillators are used, in particular, to model the dynamics of genetic
circadian pacemakers, hormonal cycles and some metabolic pathways. In this
paper we point out that for Goodwin's oscillators, where the individual
reactions have nonlinear (e.g. Mikhaelis-Menten) kinetics, the synchronization
criterion, obtained by Hamadeh et al., cannot be directly applied. This
criterion relies on the implicit assumption of the solution boundedness,
dictated also by the chemical feasibility (the state variables stand for the
concentrations of chemicals). Furthermore, to test the synchronization
condition one needs to know an explicit bound for a solution, which generally
cannot be guaranteed under linear coupling. At the same time, we show that
these restrictions can be avoided for a nonlinear synchronization protocol,
where the control inputs are &quot;saturated&quot; by a special nonlinear function
(belonging to a wide class), which guarantees nonnegativity of the solutions
and allows to get explicit ultimate bounds for them. We prove that oscillators
synchronize under such a protocol, provided that the couplings are sufficiently
strong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05044</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05044</id><created>2015-08-20</created><authors><author><keyname>Kayes</keyname><forenames>Imrul</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author></authors><title>Cultures in Community Question Answering</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Published in the proceedings of the 26th ACM Conference on Hypertext
  and Social Media (HT'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CQA services are collaborative platforms where users ask and answer
questions. We investigate the influence of national culture on people's online
questioning and answering behavior. For this, we analyzed a sample of 200
thousand users in Yahoo Answers from 67 countries. We measure empirically a set
of cultural metrics defined in Geert Hofstede's cultural dimensions and Robert
Levine's Pace of Life and show that behavioral cultural differences exist in
community question answering platforms. We find that national cultures differ
in Yahoo Answers along a number of dimensions such as temporal predictability
of activities, contribution-related behavioral patterns, privacy concerns, and
power inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05046</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05046</id><created>2015-08-20</created><authors><author><keyname>Mei</keyname><forenames>Xing</forenames></author><author><keyname>Qi</keyname><forenames>Honggang</forenames></author><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author><author><keyname>Lyu</keyname><forenames>Siwei</forenames></author></authors><title>Improving Image Restoration with Soft-Rounding</title><categories>cs.CV</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several important classes of images such as text, barcode and pattern images
have the property that pixels can only take a distinct subset of values. This
knowledge can benefit the restoration of such images, but it has not been
widely considered in current restoration methods. In this work, we describe an
effective and efficient approach to incorporate the knowledge of distinct pixel
values of the pristine images into the general regularized least squares
restoration framework. We introduce a new regularizer that attains zero at the
designated pixel values and becomes a quadratic penalty function in the
intervals between them. When incorporated into the regularized least squares
restoration framework, this regularizer leads to a simple and efficient step
that resembles and extends the rounding operation, which we term as
soft-rounding. We apply the soft-rounding enhanced solution to the restoration
of binary text/barcode images and pattern images with multiple distinct pixel
values. Experimental results show that soft-rounding enhanced restoration
methods achieve significant improvement in both visual quality and quantitative
measures (PSNR and SSIM). Furthermore, we show that this regularizer can also
benefit the restoration of general natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05051</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05051</id><created>2015-08-20</created><authors><author><keyname>Murray</keyname><forenames>Kenton</forenames></author><author><keyname>Chiang</keyname><forenames>David</forenames></author></authors><title>Auto-Sizing Neural Networks: With Applications to n-gram Language Models</title><categories>cs.CL</categories><comments>EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks have been shown to improve performance across a range of
natural-language tasks. However, designing and training them can be
complicated. Frequently, researchers resort to repeated experimentation to pick
optimal settings. In this paper, we address the issue of choosing the correct
number of units in hidden layers. We introduce a method for automatically
adjusting network size by pruning out hidden units through $\ell_{\infty,1}$
and $\ell_{2,1}$ regularization. We apply this method to language modeling and
demonstrate its ability to correctly choose the number of hidden units while
maintaining perplexity. We also include these models in a machine translation
decoder and show that these smaller neural models maintain the significant
improvements of their unpruned versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05052</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05052</id><created>2015-08-20</created><authors><author><keyname>Diaco</keyname><forenames>Nicholas</forenames></author><author><keyname>Khovanova</keyname><forenames>Tanya</forenames></author></authors><title>Weighing Coins and Keeping Secrets</title><categories>math.HO cs.CR</categories><comments>14 pages</comments><msc-class>00A08, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this expository paper we discuss a relatively new counterfeit coin problem
with an unusual goal: maintaining the privacy of, rather than revealing,
counterfeit coins in a set of both fake and real coins. We introduce two
classes of solutions to this problem --- one that respects the privacy of all
the coins and one that respects the privacy of only the fake coins --- and give
several results regarding each. We describe and generalize 6 unique strategies
that fall into these two categories. Furthermore, we explain conditions for the
existence of a solution, as well as showing proof of a solution's optimality in
select cases. In order to quantify exactly how much information is revealed by
a given solution, we also define the revealing factor and revealing
coefficient; these two values additionally act as a means of comparing the
relative effectiveness of different solutions. Most importantly, by introducing
an array of new concepts, we lay the foundation for future analysis of this
very interesting problem, as well as many other problems related to privacy and
the transfer of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05056</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05056</id><created>2015-08-20</created><updated>2015-08-24</updated><authors><author><keyname>Campos</keyname><forenames>Victor</forenames></author><author><keyname>Salvador</keyname><forenames>Amaia</forenames></author><author><keyname>Jou</keyname><forenames>Brendan</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author></authors><title>Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual
  Sentiment Prediction</title><categories>cs.MM cs.CV</categories><comments>Preprint of the paper accepted at the 1st Workshop on Affect and
  Sentiment in Multimedia (ASM), in ACM MultiMedia 2015. Brisbane, Australia</comments><acm-class>I.2.10; H.1.2</acm-class><doi>10.1145/2813524.2813530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual media are powerful means of expressing emotions and sentiments. The
constant generation of new content in social networks highlights the need of
automated visual sentiment analysis tools. While Convolutional Neural Networks
(CNNs) have established a new state-of-the-art in several vision problems,
their application to the task of sentiment analysis is mostly unexplored and
there are few studies regarding how to design CNNs for this purpose. In this
work, we study the suitability of fine-tuning a CNN for visual sentiment
prediction as well as explore performance boosting techniques within this deep
learning setting. Finally, we provide a deep-dive analysis into a benchmark,
state-of-the-art network architecture to gain insight about how to design
patterns for CNNs on the task of visual sentiment prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05072</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05072</id><created>2015-08-18</created><authors><author><keyname>Kammar</keyname><forenames>Ohad</forenames></author></authors><title>An absolute characterisation of locally determined omega-colimits</title><categories>math.CT cs.LO</categories><comments>Talk proposal for the Domains XI 2014 Workshop, uploaded for
  archiving purposes</comments><acm-class>F.3.2; D.3.3; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterising colimiting omega-cocones of projection pairs in terms of least
upper bounds of their embeddings and projections is important to the solution
of recursive domain equations. We present a universal characterisation of this
local property as omega-cocontinuity of locally continuous functors. We present
a straightforward proof using the enriched Yoneda embedding. The proof can be
generalised to Cattani and Fiore's notion of locality for adjoint pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05075</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05075</id><created>2015-08-18</created><authors><author><keyname>Allalen</keyname><forenames>Momme</forenames><affiliation>LRZ</affiliation></author><author><keyname>Brayford</keyname><forenames>David</forenames><affiliation>LRZ</affiliation></author><author><keyname>Tafani</keyname><forenames>Daniele</forenames><affiliation>LRZ</affiliation></author><author><keyname>Weinberg</keyname><forenames>Volker</forenames><affiliation>LRZ</affiliation></author><author><keyname>Mohr</keyname><forenames>Bernd</forenames><affiliation>JSC</affiliation></author><author><keyname>Br&#xf6;mmel</keyname><forenames>Dirk</forenames><affiliation>JSC</affiliation></author><author><keyname>Halver</keyname><forenames>Rene</forenames><affiliation>JSC</affiliation></author><author><keyname>Meinke</keyname><forenames>Jan</forenames><affiliation>JSC</affiliation></author><author><keyname>Mohanty</keyname><forenames>Sandipan</forenames><affiliation>JSC</affiliation></author></authors><title>The Mont-Blanc Project: First Phase Successfully Finished</title><categories>cs.DC</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Running from October 2011 to June 2015, the aim of the European project
Mont-Blanc has been to develop an approach to Exascale computing based on
embedded power-efficient technology. The main goals of the project were to i)
build an HPC prototype using currently available energy-efficient embedded
technology, ii) design a Next Generation system to overcome the limitations of
the built prototype and iii) port a set of representative Exascale applications
to the system. This article summarises the contributions from the Leibniz
Supercomputing Centre (LRZ) and the Juelich Supercomputing Centre (JSC),
Germany, to the Mont-Blanc project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05077</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05077</id><created>2015-08-20</created><authors><author><keyname>Vega</keyname><forenames>Gerardo</forenames></author></authors><title>A characterization of a class of optimal three-weight cyclic codes of
  dimension 3 over any finite field</title><categories>cs.IT math.IT</categories><comments>Preprint submitted to Finite Fields and Their Applications August 20,
  2015</comments><msc-class>11T71, 11T55, 12E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the problem of determining the weight distributions of
families of cyclic codes is, in general, notoriously difficult. An even harder
problem is to find characterizations of families of cyclic codes in terms of
their weight distributions. On the other hand, it is also well known that
cyclic codes with few weights have a great practical importance in coding
theory and cryptography. In particular, cyclic codes having three nonzero
weights have been studied by several authors, however, most of these efforts
focused on cyclic codes over a prime field. In this work we present a
characterization of a class of optimal three-weight cyclic codes of dimension 3
over any finite field. The codes under this characterization are, indeed,
optimal in the sense that their lengths reach the Griesmer lower bound for
linear codes. Consequently, these codes reach, simultaneously, the best
possible coding capacity, and also the best possible capabilities of error
detection and correction for linear codes. But because they are cyclic in
nature, they also possess a rich algebraic structure that can be utilized in a
variety of ways, particularly, in the design of very efficient coding and
decoding algorithms. What is also worth pointing out, is the simplicity of the
necessary and sufficient numerical conditions that characterize our class of
optimal three-weight cyclic codes. As we already pointed out, it is a hard
problem to find this kind of characterizations. However, for this particular
case the fundamental tool that allowed us to find our characterization was the
characterization for all two-weight irreducible cyclic codes that was
introduced by B. Schmidt and C. White (2002). Lastly, another feature about the
codes in this class, is that their duals seem to have always the same
parameters as the best known linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05084</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05084</id><created>2015-08-20</created><authors><author><keyname>Tutuncuoglu</keyname><forenames>Kaya</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Energy Harvesting Networks with Energy Cooperation: Procrastinating
  Policies</title><categories>cs.IT math.IT</categories><comments>Accepted for publication, IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers multiterminal networks with energy harvesting
transmitter nodes that are also capable of wirelessly transferring energy to or
receiving energy from other nodes in the network. In particular, the jointly
optimal transmit power and energy transfer policies that maximize
sum-throughput for the two-way, two-hop, and multiple access channels are
identified. It is shown for nodes with infinite-sized batteries that delaying
energy transfers until energy is needed immediately at the receiving node is
sum-throughput optimal. Focusing on such procrastinating policies without loss
of optimality, the stated joint optimization problem can be decomposed into
energy transfer and consumed energy allocation problems which are solved in
tandem. This decomposition is shown to hold for the finite-sized battery case
as well, using partially procrastinating policies that avoid battery overflows.
It is observed that for the two-hop channel, the proposed algorithm has a two
fluid water-filling interpretation, and for the multiple access channel, it
reduces to a single transmitter problem with aggregate energy arrivals.
Numerical results demonstrate the throughput improvement with bi-directional
energy cooperation over no cooperation and uni-directional cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05116</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05116</id><created>2015-08-20</created><authors><author><keyname>Riahi</keyname><forenames>Nima</forenames></author><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author></authors><title>Resolving Weak Sources within a Dense Array using a Network Approach</title><categories>physics.data-an cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-parametric technique to identify weak sources within dense sensor
arrays is developed using a network approach. No knowledge about the
propagation medium is needed except that signal strengths decay to
insignificant levels within a scale that is shorter than the aperture. We then
reinterpret the spatial covariance matrix of a wave field as a matrix whose
support is a connectivity matrix of a network of vertices (sensors) connected
into communities. These communities correspond to sensor clusters associated
with individual sources. We estimate the support of the covariance matrix from
limited-time data using a robust hypothesis test combined with a physical
distance criterion. The latter ensures sufficient network sparsity to prevent
vertex communities from forming by chance. We verify the approach on simulated
data and quantify its reliability. The method is then applied to data from a
dense 5200 element geophone array that blanketed 7$\times$10 km of the city of
Long Beach (CA). The analysis exposes a helicopter traversing the array, oil
production facilities, and reveals that low-frequency events tend to occur near
roads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05117</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05117</id><created>2015-08-20</created><updated>2015-10-13</updated><authors><author><keyname>Marino</keyname><forenames>Raffaele</forenames></author><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>Federico</forenames></author></authors><title>The Backtracking Survey Propagation Algorithm for Solving Random K-SAT
  Problems</title><categories>cs.CC cs.AI cs.DS</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satisfiability of random Boolean expressions built from many clauses with K
variables per clause (random K-satisfiability) is a fundamental problem in
combinatorial discrete optimization. Here we study random K-satisfiability for
K = 3 and K = 4 by the Backtracking Survey Propagation algorithm. This
algorithm is able to find, in a time linear in the problem size, solutions
within a region never reached before, very close to SAT-UNSAT threshold, and
even beyond the freezing threshold. For K = 3 the algorithmic threshold
practically coincides with the SAT-UNSAT threshold. We also study the whitening
process on all the solutions found by the Backtracking Survey Propagation
algorithm: none contains frozen variables and the whitening procedure is able
to remove all variables, following a two-steps process, in a time that diverges
approaching the algorithmic threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05128</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05128</id><created>2015-08-20</created><updated>2015-10-13</updated><authors><author><keyname>Sourek</keyname><forenames>Gustav</forenames></author><author><keyname>Aschenbrenner</keyname><forenames>Vojtech</forenames></author><author><keyname>Zelezny</keyname><forenames>Filip</forenames></author><author><keyname>Kuzelka</keyname><forenames>Ondrej</forenames></author></authors><title>Lifted Relational Neural Networks</title><categories>cs.AI cs.LG cs.NE</categories><comments>Expanded section on weight learning, added explanation of
  relationship to convolutional neural networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method combining relational-logic representations with neural
network learning. A general lifted architecture, possibly reflecting some
background domain knowledge, is described through relational rules which may be
handcrafted or learned. The relational rule-set serves as a template for
unfolding possibly deep neural networks whose structures also reflect the
structures of given training or testing relational examples. Different networks
corresponding to different examples share their weights, which co-evolve during
training by stochastic gradient descent algorithm. The framework allows for
hierarchical relational modeling constructs and learning of latent relational
concepts through shared hidden layers weights corresponding to the rules.
Discovery of notable relational concepts and experiments on 78 relational
learning benchmarks demonstrate favorable performance of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05133</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05133</id><created>2015-08-20</created><updated>2015-09-02</updated><authors><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>Steps Toward Deep Kernel Methods from Infinite Neural Networks</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contemporary deep neural networks exhibit impressive results on practical
problems. These networks generalize well although their inherent capacity may
extend significantly beyond the number of training examples. We analyze this
behavior in the context of deep, infinite neural networks. We show that deep
infinite layers are naturally aligned with Gaussian processes and kernel
methods, and devise stochastic kernels that encode the information of these
networks. We show that stability results apply despite the size, offering an
explanation for their empirical success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05135</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05135</id><created>2015-08-20</created><authors><author><keyname>Zhang</keyname><forenames>Shan</forenames></author><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>How Many Small Cells Can Be Turned off via Vertical Offloading under a
  Separation Architecture?</title><categories>cs.NI cs.IT math.IT</categories><comments>To be published in IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2015.2438301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To further improve the energy efficiency of heterogeneous networks, a
separation architecture called hyper-cellular network (HCN) has been proposed,
which decouples the control signaling and data transmission functions.
Specifically, the control coverage is guaranteed by macro base stations (MBSs),
whereas small cells (SCs) are only utilized for data transmission. Under HCN,
SCs can be dynamically turned off when traffic load decreases for energy
saving. A fundamental problem then arises: how many SCs can be turned off as
traffic varies? In this paper, we address this problem in a theoretical way,
where two sleeping schemes (i.e., random and repulsive schemes) with vertical
inter-layer offloading are considered. Analytical results indicate the
following facts: (1) Under the random scheme where SCs are turned off with
certain probability, the expected ratio of sleeping SCs is inversely
proportional to the traffic load of SC-layer and decreases linearly with the
traffic load of MBS-layer; (2) The repulsive scheme, which only turns off the
SCs close to MBSs, is less sensitive to the traffic variations; (3) deploying
denser MBSs enables turning off more SCs, which may help to improve network
energy-efficiency. Numerical results show that about 50% SCs can be turned off
on average under the predefined daily traffic profiles, and 10% more SCs can be
further turned off with inter-layer channel borrowing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05143</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05143</id><created>2015-08-20</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author></authors><title>A Discrete and Bounded Envy-free Cake Cutting Protocol for Four Agents</title><categories>cs.DS cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the well-studied cake cutting problem in which the goal is to
identify a fair allocation based on a minimal number of queries from the
agents. The problem has attracted considerable attention within various
branches of computer science, mathematics, and economics. Although, the elegant
Selfridge-Conway envy-free protocol for three agents has been known since 1960,
it has been a major open problem for the last fifty years to obtain a bounded
envy-free protocol for more than three agents. We propose a discrete and
bounded envy-free protocol for four agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05146</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05146</id><created>2015-08-20</created><authors><author><keyname>Zhang</keyname><forenames>Shan</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames><affiliation>Sherman</affiliation></author><author><keyname>Gong</keyname><forenames>Jie</forenames><affiliation>Sherman</affiliation></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Ning</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xueming</keyname><affiliation>Sherman</affiliation></author><author><keyname>Shen</keyname></author></authors><title>Spatial Traffic Shaping in Heterogeneous Cellular Networks with Energy
  Harvesting</title><categories>cs.NI cs.IT math.IT</categories><comments>To be presented at the IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH), which explores renewable energy as a supplementary
power source, is a promising 5G technology to support the huge energy demand of
heterogeneous cellular networks (HCN). However, the random arrival of renewable
energy brings great challenges to network management. By adjusting the
distribution of traffic load in spatial domain, traffic shaping helps to
balance the cell-level power demand and supply, and thus improves the
utilization of renewable energy. In this paper, we investigate the power saving
performance of traffic shaping in an analytical way, based on the statistic
information of energy arrival and traffic load. Specifically, an energy-optimal
traffic shaping scheme (EOTS) is devised for HCNs with EH, whereby the on-off
state of the off-grid small cell and the amount of offloading traffic are
adjusted dynamically with the energy variation, to minimize the on-grid power
consumption. Numerical results are given to demonstrate that for the daily
traffic and solar energy profiles, EOTS scheme can significantly reduce the
energy consumption, compared with the greedy method where users are always
offloaded to the off-grid small cell with priority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05149</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05149</id><created>2015-08-20</created><updated>2015-09-25</updated><authors><author><keyname>Kolte</keyname><forenames>Ritesh</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author><author><keyname>Permuter</keyname><forenames>Haim</forenames></author></authors><title>Cooperative Binning for Semideterministic Channels</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory, presented in
  part at ISIT 2015; version-2 contains an illustrative example</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity regions of semideterministic multiuser channels, such as the
semideterministic relay channel and the multiple access channel with partially
cribbing encoders, have been characterized using the idea of
partial-decode-forward. However, the requirement to explicitly decode part of
the message at intermediate nodes can be restrictive in some settings; for
example, when nodes have different side information regarding the state of the
channel. In this paper, we generalize this scheme to
$\textit{cooperative-bin-forward}$ by building on the observation that explicit
recovering of part of the message is not needed to induce cooperation. Instead,
encoders can bin their received signals and cooperatively forward the bin index
to the decoder. The main advantage of this new scheme is illustrated by
considering state-dependent extensions of the aforementioned semideterministic
setups. While partial-decode-forward is not applicable in these new setups,
cooperative-bin-forward continues to achieve capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05151</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05151</id><created>2015-08-20</created><updated>2015-10-20</updated><authors><author><keyname>Bailer</keyname><forenames>Christian</forenames></author><author><keyname>Taetz</keyname><forenames>Bertram</forenames></author><author><keyname>Stricker</keyname><forenames>Didier</forenames></author></authors><title>Flow Fields: Dense Correspondence Fields for Highly Accurate Large
  Displacement Optical Flow Estimation</title><categories>cs.CV</categories><acm-class>I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern large displacement optical flow algorithms usually use an
initialization by either sparse descriptor matching techniques or dense
approximate nearest neighbor fields. While the latter have the advantage of
being dense, they have the major disadvantage of being very outlier prone as
they are not designed to find the optical flow, but the visually most similar
correspondence. In this paper we present a dense correspondence field approach
that is much less outlier prone and thus much better suited for optical flow
estimation than approximate nearest neighbor fields. Our approach is
conceptually novel as it does not require explicit regularization, smoothing
(like median filtering) or a new data term, but solely our novel purely data
based search strategy that finds most inliers (even for small objects), while
it effectively avoids finding outliers. Moreover, we present novel enhancements
for outlier filtering. We show that our approach is better suited for large
displacement optical flow estimation than state-of-the-art descriptor matching
techniques. We do so by initializing EpicFlow (so far the best method on
MPI-Sintel) with our Flow Fields instead of their originally used
state-of-the-art descriptor matching technique. We significantly outperform the
original EpicFlow on MPI-Sintel, KITTI and Middlebury.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05154</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05154</id><created>2015-08-20</created><updated>2015-09-02</updated><authors><author><keyname>Nguyen</keyname><forenames>Khanh</forenames></author><author><keyname>O'Connor</keyname><forenames>Brendan</forenames></author></authors><title>Posterior calibration and exploratory analysis for natural language
  processing models</title><categories>cs.CL</categories><comments>15 pages (including supplementary information), proceedings of EMNLP
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many models in natural language processing define probabilistic distributions
over linguistic structures. We argue that (1) the quality of a model' s
posterior distribution can and should be directly evaluated, as to whether
probabilities correspond to empirical frequencies, and (2) NLP uncertainty can
be projected not only to pipeline components, but also to exploratory data
analysis, telling a user when to trust and not trust the NLP analysis. We
present a method to analyze calibration, and apply it to compare the
miscalibration of several commonly used models. We also contribute a
coreference sampling algorithm that can create confidence intervals for a
political event extraction task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05163</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05163</id><created>2015-08-20</created><authors><author><keyname>Soelistio</keyname><forenames>Yustinus Eko</forenames></author><author><keyname>Surendra</keyname><forenames>Martinus Raditia Sigit</forenames></author></authors><title>Simple Text Mining for Sentiment Analysis of Political Figure Using
  Naive Bayes Classifier Method</title><categories>cs.CL cs.IR</categories><comments>5 pages, published in the Proceedings of the 7th ICTS</comments><doi>10.12962/p9772338185001.a18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text mining can be applied to many fields. One of the application is using
text mining in digital newspaper to do politic sentiment analysis. In this
paper sentiment analysis is applied to get information from digital news
articles about its positive or negative sentiment regarding particular
politician. This paper suggests a simple model to analyze digital newspaper
sentiment polarity using naive Bayes classifier method. The model uses a set of
initial data to begin with which will be updated when new information appears.
The model showed promising result when tested and can be implemented to some
other sentiment analysis problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05170</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05170</id><created>2015-08-20</created><authors><author><keyname>Foster</keyname><forenames>Dylan J.</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Adaptive Online Learning</title><categories>cs.LG stat.ML</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for studying adaptive regret bounds in the
online learning framework, including model selection bounds and data-dependent
bounds. Given a data- or model-dependent bound we ask, &quot;Does there exist some
algorithm achieving this bound?&quot; We show that modifications to recently
introduced sequential complexity measures can be used to answer this question
by providing sufficient conditions under which adaptive rates can be achieved.
In particular each adaptive rate induces a set of so-called offset complexity
measures, and obtaining small upper bounds on these quantities is sufficient to
demonstrate achievability. A cornerstone of our analysis technique is the use
of one-sided tail inequalities to bound suprema of offset random processes.
  Our framework recovers and improves a wide variety of adaptive bounds
including quantile bounds, second-order data-dependent bounds, and small loss
bounds. In addition we derive a new type of adaptive bound for online linear
optimization based on the spectral norm, as well as a new online PAC-Bayes
theorem that holds for countably infinite sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05175</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05175</id><created>2015-08-21</created><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Tulino</keyname><forenames>Antonia M.</forenames></author><author><keyname>Llorca</keyname><forenames>Jaime</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Finite Length Analysis of Caching-Aided Coded Multicasting</title><categories>cs.IT math.IT</categories><comments>A shorter version appeared in the 52nd Annual Allerton Conference on
  Communication, Control, and Computing (Allerton), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study a noiseless broadcast link serving $K$ users whose
requests arise from a library of $N$ files. Every user is equipped with a cache
of size $M$ files each. It has been shown that by splitting all the files into
packets and placing individual packets in a random independent manner across
all the caches, it requires at most $N/M$ file transmissions for any set of
demands from the library. The achievable delivery scheme involves linearly
combining packets of different files following a greedy clique cover solution
to the underlying index coding problem. This remarkable multiplicative gain of
random placement and coded delivery has been established in the asymptotic
regime when the number of packets per file $F$ scales to infinity.
  In this work, we initiate the finite-length analysis of random caching
schemes when the number of packets $F$ is a function of the system parameters
$M,N,K$. Specifically, we show that existing random placement and clique cover
delivery schemes that achieve optimality in the asymptotic regime can have at
most a multiplicative gain of $2$ if the number of packets is sub-exponential.
Further, for any clique cover based coded delivery and a large class of random
caching schemes, that includes the existing ones, we show that the number of
packets required to get a multiplicative gain of $\frac{4}{3}g$ is at least
$O((N/M)^g)$. We exhibit a random placement and an efficient clique cover based
coded delivery scheme that approximately achieves this lower bound. We also
provide tight concentration results that show that the average (over the random
caching involved) number of transmissions concentrates very well requiring only
polynomial number of packets in the rest of the parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05176</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05176</id><created>2015-08-21</created><authors><author><keyname>Safta</keyname><forenames>Cosmin</forenames></author><author><keyname>Chen</keyname><forenames>Richard L. -Y.</forenames></author><author><keyname>Najm</keyname><forenames>Habib N.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Watson</keyname><forenames>Jean-Paul</forenames></author></authors><title>Efficient Representation of Uncertainty for Stochastic Economic Dispatch</title><categories>cs.CE math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1407.2232</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic economic dispatch models address uncertainties in forecasts of
renewable generation output by considering a finite number of realizations
drawn from a stochastic process model, typically via Monte Carlo sampling.
Accurate evaluations of expectations or higher-order moments for quantities of
interest, e.g., generating cost, can require a prohibitively large number of
samples. We propose an alternative to Monte Carlo sampling based on Polynomial
Chaos expansions. These representations are based on sparse quadrature methods,
and enable accurate propagation of uncertainties in model parameters. We also
investigate a method based on Karhunen-Loeve expansions that enables us to
efficiently represent uncertainties in renewable energy generation. Considering
expected production cost, we demonstrate that the proposed approach can yield
several orders of magnitude reduction in computational cost for solving
stochastic economic dispatch relative to Monte Carlo sampling, for a given
target error threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05181</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05181</id><created>2015-08-21</created><updated>2016-01-15</updated><authors><author><keyname>Biason</keyname><forenames>Alessandro</forenames></author><author><keyname>Laurenti</keyname><forenames>Nicola</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Achievable Secrecy Rates of an Energy Harvesting Device</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures, submitted to IEEE Journal on Selected Areas in
  Communications (major revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secrecy rate represents the amount of information per unit time that can
be securely sent on a communication link. In this work, we investigate the
achievable secrecy rates in an Energy Harvesting communication system composed
of one transmitter and one receiver. In particular, because of the energy
constraints and the channel conditions, it is important to understand when a
device should transmit and how much power should be used. We introduce and
discuss the structure of the Optimal Secrecy Policy, i.e., the technique that
maximizes the secrecy rate in several scenarios. Full knowledge and partial
knowledge of the channel are considered under a Nakagami fading scenario. We
show that high secrecy rates can be obtained only with power and coding rate
adaptation. Moreover, we highlight the importance of optimally dividing the
transmission power in the frequency domain, and note that the optimal scheme
provides high rewards with respect to the uniform power splitting case.
Analytically, we explain how to find the optimal policy and prove some of its
properties. In our numerical evaluation, we discuss how the maximum achievable
secrecy rate changes according to the various system parameters. Furthermore,
we discuss the effects of a finite battery on the system performance and we
note that, in order to achieve high secrecy rates, it is not necessary to use
very large batteries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05189</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05189</id><created>2015-08-21</created><authors><author><keyname>Bottesch</keyname><forenames>Ralph C.</forenames></author><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author></authors><title>Correlation in Hard Distributions in Communication Complexity</title><categories>cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect that the amount of correlation in a bipartite
distribution has on the communication complexity of a problem under that
distribution. We introduce a new family of complexity measures that
interpolates between the two previously studied extreme cases: the (standard)
randomised communication complexity and the case of distributional complexity
under product distributions.
  We give a tight characterisation of the randomised complexity of Disjointness
under distributions with mutual information $k$, showing that it is
$\Theta(\sqrt{n(k+1)})$ for all $0\leq k\leq n$. This smoothly interpolates
between the lower bounds of Babai, Frankl and Simon for the product
distribution case ($k=0$), and the bound of Razborov for the randomised case.
The upper bounds improve and generalise what was known for product
distributions, and imply that any tight bound for Disjointness needs
$\Omega(n)$ bits of mutual information in the corresponding distribution.
  We study the same question in the distributional quantum setting, and show a
lower bound of $\Omega((n(k+1))^{1/4})$, and an upper bound, matching up to a
logarithmic factor.
  We show that there are total Boolean functions $f_d$ on $2n$ inputs that have
distributional communication complexity $O(\log n)$ under all distributions of
information up to $o(n)$, while the (interactive) distributional complexity
maximised over all distributions is $\Theta(\log d)$ for $6n\leq d\leq
2^{n/100}$.
  We show that in the setting of one-way communication under product
distributions, the dependence of communication cost on the allowed error
$\epsilon$ is multiplicative in $\log(1/\epsilon)$ -- the previous upper bounds
had the dependence of more than $1/\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05228</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05228</id><created>2015-08-21</created><authors><author><keyname>Schmidt</keyname><forenames>Wolfgang</forenames></author><author><keyname>Hanspach</keyname><forenames>Michael</forenames></author><author><keyname>Keller</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>A Case Study on Covert Channel Establishment via Software Caches in
  High-Assurance Computing Systems</title><categories>cs.CR cs.OS</categories><comments>12 pages, based upon the master's thesis of Schmidt</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covert channels can be utilized to secretly deliver information from high
privileged processes to low privileged processes in the context of a
high-assurance computing system. In this case study, we investigate the
possibility of covert channel establishment via software caches in the context
of a framework for component-based operating systems. While component-based
operating systems offer security through the encapsulation of system service
processes, complete isolation of these processes is not reasonably feasible.
This limitation is practically demonstrated with our concept of a specific
covert timing channel based on file system caching. The stability of the covert
channel is evaluated and a methodology to disrupt the covert channel
transmission is presented. While these kinds of attacks are not limited to
high-assurance computing systems, our study practically demonstrates that even
security-focused computing systems with a minimal trusted computing base are
vulnerable for such kinds of attacks and careful design decisions are necessary
for secure operating system architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05232</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05232</id><created>2015-08-21</created><authors><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author></authors><title>Variable-mixing parameter quantized kernel robust mixed-norm algorithms
  for combating impulsive interference</title><categories>cs.SY</categories><comments>22 pages, 8 figures, submitted to IET signal processing on Jan 21,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To overcome the performance degradation in impulsive noise environments, a
kernel robust mixed-norm (KRMN) algorithm is presented which no longer requires
a Gaussian environment. It incorporates the robust mixed-norm (RMN) algorithm
and kernel method to obtain robustness against impulsive noise. However, it has
two major problems as follows: (1) The choice of the mixing parameter in the
KRMN algorithm is crucial to obtain satisfactory performance. (2) The structure
of KRMN grows linearly as the iteration goes on, thus it has high computational
burden and memory requirement. To solve the parameter selection problem, two
variable-mixing parameter KRMN (VPKRMN) algorithms are developed in this paper.
Furthermore, a sparsification algorithm, quantized VPKRMN (QVPKRMN) algorithm
is introduced for nonlinear system identification under impulsive interference
environment. The convergence property in the mean square sense has been carried
out, and the energy conservation relation (ECR) for QVPKRMN algorithm is
established. Simulations in the context of nonlinear system identification
under impulsive interference have shown that the proposed VPKRMN and QVPKRMN
algorithms provide superior performance than existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05243</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05243</id><created>2015-08-21</created><authors><author><keyname>Lucic</keyname><forenames>Mario</forenames></author><author><keyname>Bachem</keyname><forenames>Olivier</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Strong Coresets for Hard and Soft Bregman Clustering with Applications
  to Exponential Family Mixtures</title><categories>stat.ML cs.LG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coresets are efficient representations of datasets such that models trained
on a coreset are provably competitive with models trained on the original
dataset. As such, they have been successfully used to scale up clustering
models such as K-Means and Gaussian mixture models to massive datasets.
However, until now, the algorithms and corresponding theory were usually
specific to each clustering problem. We propose a single, practical algorithm
to construct strong coresets for a large class of hard and soft clustering
problems based on Bregman divergences. This class includes hard clustering with
popular distortion measures such as the Squared Euclidean distance, the
Mahalanobis distance, KL-divergence, Itakura-Saito distance and relative
entropy. The corresponding soft clustering problems are directly related to
popular mixture models due to a dual relationship between Bregman divergences
and Exponential family distributions. Our results recover existing coreset
constructions for K-Means and Gaussian mixture models and imply polynomial time
approximations schemes for various hard clustering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05253</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05253</id><created>2015-08-21</created><updated>2015-12-02</updated><authors><author><keyname>Nicosia</keyname><forenames>Gaia</forenames></author><author><keyname>Pacifici</keyname><forenames>Andrea</forenames></author><author><keyname>Pferschy</keyname><forenames>Ulrich</forenames></author></authors><title>Price of Fairness for Allocating a Bounded Resource</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of allocating a scarce resource among
several players (or agents). A central decision maker wants to maximize the
total utility of all agents. However, such a solution may be unfair for one or
more agents in the sense that it can be achieved through a very unbalanced
allocation of the resource. On the other hand fair/balanced allocations may be
far from being optimal from a central point of view. So, in this paper we are
interested in assessing the quality of fair solutions, i.e. in measuring the
system efficiency loss under a fair allocation compared to the one that
maximizes the sum of agents utilities. This indicator is usually called the
Price of Fairness and we study it under three different definitions of
fairness, namely maximin, Kalai-Smorodinski and proportional fairness.
  Our results are of two different types. We first formalize a number of
properties holding for any general multi-agent problem without any special
assumption on the agents utility sets. Then we introduce an allocation problem,
where each agent can consume the resource in given discrete quantities (items),
such that the maximization of the total utility is given by a Subset Sum
Problem. For the resulting Fair Subset Sum Problem, in the case of two agents,
we provide upper and lower bounds on the Price of Fairness as functions of an
upper bound on the items size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05269</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05269</id><created>2015-08-21</created><authors><author><keyname>Galam</keyname><forenames>Serge</forenames></author><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>Modeling Radicalization Phenomena in Heterogeneous Populations</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phenomenon of radicalization is investigated within an heterogeneous
population composed of a core subpopulation, sharing a way of life locally
rooted, and a recently immigrated subpopulation of different origins with ways
of life which can be partly in conflict with the local one. While core agents
are embedded in the country prominent culture and identity, they are not likely
to modify their way of life, which make them naturally inflexible about it. On
the opposite, the new comers can either decide to live peacefully with the core
people adapting their way of life, or to keep strictly on their way and oppose
the core population, leading eventually to criminal activities. To study the
corresponding dynamics of radicalization we introduce a 3-state agent model
with a proportion of inflexible agents and a proportion of flexible ones, which
can be either peaceful or opponent. Assuming agents interact via weighted pairs
within a Lotka-Volterra like Ordinary Differential Equation framework, the
problem is analytically solved exactly. Results shed a new light on the
instrumental role core agents can play through individual activeness towards
peaceful agents to either curb or inflate radicalization. Some hints are
outlined at new possible public policies towards social integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05273</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05273</id><created>2015-08-21</created><authors><author><keyname>da Silva</keyname><forenames>Alex Pereira</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author><author><keyname>de Almeida</keyname><forenames>Andre Lima Ferrer</forenames></author></authors><title>Rank-1 Tensor Approximation Methods and Application to Deflation</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of the attractiveness of the canonical polyadic (CP) tensor
decomposition in various applications, several algorithms have been designed to
compute it, but efficient ones are still lacking. Iterative deflation
algorithms based on successive rank-1 approximations can be used to perform
this task, since the latter are rather easy to compute. We first present an
algebraic rank-1 approximation method that performs better than the standard
higher-order singular value decomposition (HOSVD) for three-way tensors.
Second, we propose a new iterative rank-1 approximation algorithm that improves
any other rank-1 approximation method. Third, we describe a probabilistic
framework allowing to study the convergence of deflation CP decomposition
(DCPD) algorithms based on successive rank-1 approximations. A set of computer
experiments then validates theoretical results and demonstrates the efficiency
of DCPD algorithms compared to other ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05282</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05282</id><created>2015-08-21</created><updated>2015-10-15</updated><authors><author><keyname>Bliznets</keyname><forenames>Ivan</forenames></author><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Komosa</keyname><forenames>Pawel</forenames></author><author><keyname>Mach</keyname><forenames>Lukas</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Michal</forenames></author></authors><title>Lower bounds for the parameterized complexity of Minimum Fill-in and
  other completion problems</title><categories>cs.CC cs.DS</categories><comments>Accepted to SODA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we focus on several completion problems for subclasses of
chordal graphs: Minimum Fill-In, Interval Completion, Proper Interval
Completion, Threshold Completion, and Trivially Perfect Completion. In these
problems, the task is to add at most k edges to a given graph in order to
obtain a chordal, interval, proper interval, threshold, or trivially perfect
graph, respectively. We prove the following lower bounds for all these
problems, as well as for the related Chain Completion problem: Assuming the
Exponential Time Hypothesis, none of these problems can be solved in time
2^O(n^(1/2) / log^c n) or 2^O(k^(1/4) / log^c k) n^O(1), for some integer c.
Assuming the non-existence of a subexponential-time approximation scheme for
Min Bisection on d-regular graphs, for some constant d, none of these problems
can be solved in time 2^o(n) or 2^o(sqrt(k)) n^O(1).
  For all the aforementioned completion problems, apart from Proper Interval
Completion, FPT algorithms with running time of the form 2^O(sqrt(k) log k)
n^O(1) are known. Thus, the second result proves that a significant improvement
of any of these algorithms would lead to a surprising breakthrough in the
design of approximation algorithms for Min Bisection.
  To prove our results, we use a reduction methodology based on combining the
classic approach of starting with a sparse instance of 3-Sat, prepared using
the Sparsification Lemma, with the existence of almost linear-size
Probabilistically Checkable Proofs (PCPs). Apart from our main results, we also
obtain lower bounds excluding the existence of subexponential algorithms for
the Optimum Linear Arrangement problem, as well as improved, yet still not
tight, lower bounds for Feedback Arc Set in Tournaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05288</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05288</id><created>2015-08-21</created><updated>2015-11-10</updated><authors><author><keyname>Spanknebel</keyname><forenames>Martin</forenames></author><author><keyname>Pawelzik</keyname><forenames>Klaus</forenames></author></authors><title>Dynamics of Human Cooperation in Economic Games</title><categories>physics.soc-ph cs.GT cs.LG math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human decision behaviour is quite diverse. In many games humans on average do
not achieve maximal payoff and the behaviour of individual players remains
inhomogeneous even after playing many rounds. For instance, in repeated
prisoner dilemma games humans do not always optimize their mean reward and
frequently exhibit broad distributions of cooperativity. The reasons for these
failures of maximization are not known. Here we show that the dynamics
resulting from the tendency to shift choice probabilities towards previously
rewarding choices in closed loop interaction with the strategy of the opponent
can not only explain systematic deviations from 'rationality', but also
reproduce the diversity of choice behaviours. As a representative example we
investigate the dynamics of choice probabilities in prisoner dilemma games with
opponents using strategies with different degrees of extortion and generosity.
We find that already a simple model for human learning can account for a
surprisingly wide range of human decision behaviours. It reproduces suppression
of cooperation against extortionists and increasing cooperation when playing
with generous opponents, explains the broad distributions of individual choices
in ensembles of players, and predicts the evolution of individual subjects'
cooperation rates over the course of the games. We conclude that important
aspects of human decision behaviours are rooted in elementary learning
mechanisms realised in the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05299</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05299</id><created>2015-08-21</created><updated>2016-02-12</updated><authors><author><keyname>Betz</keyname><forenames>Volker</forenames></author><author><keyname>Roux</keyname><forenames>Stephane Le</forenames></author></authors><title>Stable states of perturbed Markov chains</title><categories>cs.DM math.PR</categories><msc-class>60J10, 60J22, 68R10</msc-class><acm-class>G.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an infinitesimal perturbation of a discrete-time finite Markov chain,
we seek the states that are stable despite the perturbation, \textit{i.e.} the
states whose weights in the stationary distributions can be bounded away from
$0$ as the noise fades away. Chemists, economists, and computer scientists have
been studying irreducible perturbations built with exponential maps. Under
these assumptions, Young proved the existence of and computed the stable states
in cubic time. We fully drop these assumptions, generalize Young's technique,
and show that stability is decidable as long as $f\in O(g)$ is. Furthermore, if
the perturbation maps (and their multiplications) satisfy $f\in O(g)$ or $g\in
O(f)$, we prove the existence of and compute the stable states and the
metastable dynamics at all time scales where some states vanish. Conversely, if
the big-$O$ assumption does not hold, we build a perturbation with these maps
and no stable state. Our algorithm also runs in cubic time despite the general
assumptions and the additional work. Proving the correctness of the algorithm
relies on new or rephrased results in Markov chain theory, and on algebraic
abstractions thereof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05306</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05306</id><created>2015-08-21</created><authors><author><keyname>Zuo</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Shuai</keyname><forenames>Bing</forenames></author><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Yang</keyname><forenames>Qingxiong</forenames></author></authors><title>Exemplar Based Deep Discriminative and Shareable Feature Learning for
  Scene Image Classification</title><categories>cs.CV</categories><comments>Pattern Recognition, Elsevier, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to encode the class correlation and class specific information in
image representation, we propose a new local feature learning approach named
Deep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to
hierarchically learn feature transformation filter banks to transform raw pixel
image patches to features. The learned filter banks are expected to: (1) encode
common visual patterns of a flexible number of categories; (2) encode
discriminative information; and (3) hierarchically extract patterns at
different visual levels. Particularly, in each single layer of DDSFL, shareable
filters are jointly learned for classes which share the similar patterns.
Discriminative power of the filters is achieved by enforcing the features from
the same category to be close, while features from different categories to be
far away from each other. Furthermore, we also propose two exemplar selection
methods to iteratively select training data for more efficient and effective
learning. Based on the experimental results, DDSFL can achieve very promising
performance, and it also shows great complementary effect to the
state-of-the-art Caffe features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05312</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05312</id><created>2015-08-21</created><authors><author><keyname>Cheong</keyname><forenames>Se-Hang</forenames></author><author><keyname>Si</keyname><forenames>Yain-Whar</forenames></author></authors><title>Accelerating the Kamada-Kawai algorithm for boundary detection in a
  mobile ad hoc network</title><categories>cs.NI</categories><comments>29 pages, 15 figures, 6 algorithms</comments><msc-class>68W40</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Force-directed algorithms such as the Kamada-Kawai algorithm have shown
promising results for solving the boundary detection problem in a mobile ad hoc
network. However, the classical Kamada-Kawai algorithm does not scale well when
it is used in networks with large numbers of nodes. It also produces poor
results in non-convex networks. To address these problems, this paper proposes
an improved version of the Kamada-Kawai algorithm. The proposed extension
includes novel heuristics and algorithms that achieve a faster energy level
reduction. Our experimental results show that the improved algorithm can
significantly shorten the processing time and detect boundary nodes with an
acceptable level of accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05326</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05326</id><created>2015-08-21</created><authors><author><keyname>Bowman</keyname><forenames>Samuel R.</forenames></author><author><keyname>Angeli</keyname><forenames>Gabor</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>A large annotated corpus for learning natural language inference</title><categories>cs.CL</categories><comments>To appear at EMNLP 2015. The data will be posted shortly before the
  conference (the week of 14 Sep) at http://nlp.stanford.edu/projects/snli/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding entailment and contradiction is fundamental to understanding
natural language, and inference about entailment and contradiction is a
valuable testing ground for the development of semantic representations.
However, machine learning research in this area has been dramatically limited
by the lack of large-scale resources. To address this, we introduce the
Stanford Natural Language Inference corpus, a new, freely available collection
of labeled sentence pairs, written by humans doing a novel grounded task based
on image captioning. At 570K pairs, it is two orders of magnitude larger than
all other resources of its type. This increase in scale allows lexicalized
classifiers to outperform some sophisticated existing entailment models, and it
allows a neural network-based model to perform competitively on natural
language inference benchmarks for the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05328</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05328</id><created>2015-08-21</created><authors><author><keyname>Zhou</keyname><forenames>Luowei</forenames></author><author><keyname>Yang</keyname><forenames>Pei</forenames></author><author><keyname>Chen</keyname><forenames>Chunlin</forenames></author><author><keyname>Gao</keyname><forenames>Yang</forenames></author></authors><title>Multi-agent Reinforcement Learning with Sparse Interactions by
  Negotiation and Knowledge Transfer</title><categories>cs.MA cs.AI</categories><comments>12 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning has significant applications for multi-agent systems,
especially in unknown dynamic environments. However, most multi-agent
reinforcement learning (MARL) algorithms suffer from such problems as
exponential computation complexity in the joint state-action space, which makes
it difficult to scale up to realistic multi-agent problems. In this paper, a
novel algorithm named negotiation-based MARL with sparse interactions (NegoSI)
is presented. In contrast to traditional sparse-interaction based MARL
algorithms, NegoSI adopts the equilibrium concept and makes it possible for
agents to select the non-strict Equilibrium Dominating Strategy Profile
(non-strict EDSP) or Meta equilibrium for their joint actions. The presented
NegoSI algorithm consists of four parts: the equilibrium-based framework for
sparse interactions, the negotiation for the equilibrium set, the minimum
variance method for selecting one joint action and the knowledge transfer of
local Q-values. In this integrated algorithm, three techniques, i.e., unshared
value functions, equilibrium solutions and sparse interactions are adopted to
achieve privacy protection, better coordination and lower computational
complexity, respectively. To evaluate the performance of the presented NegoSI
algorithm, two groups of experiments are carried out regarding three criteria:
steps of each episode (SEE), rewards of each episode (REE) and average runtime
(AR). The first group of experiments is conducted using six grid world games
and shows fast convergence and high scalability of the presented algorithm.
Then in the second group of experiments NegoSI is applied to an intelligent
warehouse problem and simulated results demonstrate the effectiveness of the
presented NegoSI algorithm compared with other state-of-the-art MARL
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05342</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05342</id><created>2015-06-10</created><authors><author><keyname>Casas</keyname><forenames>Noe</forenames></author></authors><title>Genetic Algorithms for multimodal optimization: a review</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we provide a comprehensive review of the different
evolutionary algorithm techniques used to address multimodal optimization
problems, classifying them according to the nature of their approach. On the
one hand there are algorithms that address the issue of the early convergence
to a local optimum by differentiating the individuals of the population into
groups and limiting their interaction, hence having each group evolve with a
high degree of independence. On the other hand other approaches are based on
directly addressing the lack of genetic diversity of the population by
introducing elements into the evolutionary dynamics that promote new niches of
the genotypical space to be explored. Finally, we study multi-objective
optimization genetic algorithms, that handle the situations where multiple
criteria have to be satisfied with no penalty for any of them. Very rich
literature has arised over the years on these topics, and we aim at offering an
overview of the most important techniques of each branch of the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05344</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05344</id><created>2015-08-21</created><authors><author><keyname>Johri</keyname><forenames>Rajit</forenames></author><author><keyname>Rao</keyname><forenames>Jayanthi</forenames></author><author><keyname>Yu</keyname><forenames>Hai</forenames></author><author><keyname>Zhang</keyname><forenames>Hongwei</forenames></author></authors><title>A Multi-Scale Spatiotemporal Perspective of Connected and Automated
  Vehicles: Applications and Wireless Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication is a basis of the vision of connected and automated
vehicles (CAVs). Given the heterogeneity of both wireless communication
technologies and CAV applications, one question that is critical to technology
road-mapping and policy making is which communication technology is more
suitable for a specific CAV application. Focusing on the technical aspect of
this question, we present a multi-scale spatiotemporal perspective of wireless
communication technologies as well as canonical CAV applications in active
safety, fuel economy and emission control, vehicle automation, and vehicular
infotainment. Our analysis shows that CAV applications in the regime of small
spatiotemporal scale communication requirements are best supported by V2V
communications, applications in the regime of large spatiotemporal scale
communication requirements are better supported by cellular communications, and
applications in the regime of small spatial scale but medium-to-large temporal
scale can be supported by both V2V and cellular communications and provide the
opportunity of leveraging heterogeneous communication resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05347</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05347</id><created>2015-08-21</created><updated>2015-08-25</updated><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author></authors><title>Pricing Queries Approximately Optimally</title><categories>cs.GT cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data as a commodity has always been purchased and sold. Recently, web
services that are data marketplaces have emerged that match data buyers with
data sellers. So far there are no guidelines how to price queries against a
database. We consider the recently proposed query-based pricing framework of
Koutris et al and ask the question of computing optimal input prices in this
framework by formulating a buyer utility model.
  We establish the interesting and deep equivalence between arbitrage-freeness
in the query-pricing framework and envy-freeness in pricing theory for
appropriately chosen buyer valuations. Given the approximation hardness results
from envy-free pricing we then develop logarithmic approximation pricing
algorithms exploiting the max flow interpretation of the arbitrage-free pricing
for the restricted query language proposed by Koutris et al. We propose a novel
polynomial-time logarithmic approximation pricing scheme and show that our new
scheme performs better than the existing envy-free pricing algorithms
instance-by-instance. We also present a faster pricing algorithm that is always
greater than the existing solutions, but worse than our previous scheme. We
experimentally show how our pricing algorithms perform with respect to the
existing envy-free pricing algorithms and to the optimal exponentially
computable solution, and our experiments show that our approximation algorithms
consistently arrive at about 99% of the optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05348</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05348</id><created>2015-05-23</created><authors><author><keyname>Ding</keyname><forenames>Guoru</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Wu</keyname><forenames>Qihui</forenames></author><author><keyname>Yao</keyname><forenames>Yu-Dong</forenames></author><author><keyname>Li</keyname><forenames>Rongpeng</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author><author><keyname>Zou</keyname><forenames>Yulong</forenames></author></authors><title>On the Limits of Predictability in Real-World Radio Spectrum State
  Dynamics: From Entropy Theory to 5G Spectrum Sharing</title><categories>cs.NI cs.IT math.IT</categories><comments>To appear, IEEE Communications Magazine (Accepted from open call),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A range of applications in cognitive radio networks, from adaptive spectrum
sensing to predictive spectrum mobility and dynamic spectrum access, depend on
our ability to foresee the state evolution of radio spectrum, raising a
fundamental question: To what degree is radio spectrum state (RSS) predictable?
In this paper, we explore the fundamental limits of predictability in RSS
dynamics by studying the RSS evolution patterns in spectrum bands of several
popular services, including TV bands, ISM bands, and Cellular bands, etc. From
an information theory perspective, we introduce a methodology of using
statistical entropy measures and Fano inequality to quantify the degree of
predictability underlying real-world spectrum measurements. Despite the
apparent randomness, we find a remarkable predictability, as large as 90%, in
the real-world RSS dynamics over a number of spectrum bands for all popular
services. Furthermore, we discuss the potential applications of
prediction-based spectrum sharing in 5G wireless communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05352</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05352</id><created>2015-08-19</created><authors><author><keyname>Situngkir</keyname><forenames>Hokky</forenames></author><author><keyname>Prasetyo</keyname><forenames>Yanu Endar</forenames></author></authors><title>On Social and Economic Spheres: An Observation of the 'gantangan'
  Indonesian tradition</title><categories>cs.CY</categories><comments>6 pages, 2 figures. Paper presented in Seminar on Enhancing
  Grassroots Innovation Competitiveness for Poverty Alleviation, Yogyakarta,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indonesian traditional villagers have a tradition for the sake of their own
social and economic security named 'nyumbang'. There are wide variations of the
traditions across the archipelago, and we revisit an observation to one in
Subang, West Java, Indonesia. The paper discusses and employs the evolutionary
game theoretic insights to see the process of 'gantangan', of the intertwining
social cohesion and economic expectation of the participation within the
traditional activities. The current development of the gantangan tradition is
approached and generalized to propose a view between the economic and social
sphere surrounding modern people. While some explanations due to the current
development of gantangan is drawn, some aspects related to traditional views
complying the modern life with social and economic expectations is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05367</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05367</id><created>2015-07-31</created><updated>2015-10-21</updated><authors><author><keyname>Mesa</keyname><forenames>Andrea</forenames></author><author><keyname>Basterrech</keyname><forenames>Sebasti&#xe1;n</forenames></author><author><keyname>Guerberoff</keyname><forenames>Gustavo</forenames></author><author><keyname>Alvarez-Valin</keyname><forenames>Fernando</forenames></author></authors><title>Hidden Markov Models for Gene Sequence Classification: Classifying the
  VSG genes in the Trypanosoma brucei Genome</title><categories>q-bio.GN cs.CE cs.LG</categories><comments>Accepted article in July, 2015 in Pattern Analysis and Applications,
  Springer. The article contains 23 pages, 4 figures, 8 tables and 51
  references</comments><acm-class>G.3; I.5.1; I.5.2; J.3</acm-class><doi>10.1007/s10044-015-0508-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents an application of Hidden Markov Models (HMMs) for
pattern recognition on genome sequences. We apply HMM for identifying genes
encoding the Variant Surface Glycoprotein (VSG) in the genomes of Trypanosoma
brucei (T. brucei) and other African trypanosomes. These are parasitic protozoa
causative agents of sleeping sickness and several diseases in domestic and wild
animals. These parasites have a peculiar strategy to evade the host's immune
system that consists in periodically changing their predominant cellular
surface protein (VSG). The motivation for using patterns recognition methods to
identify these genes, instead of traditional homology based ones, is that the
levels of sequence identity (amino acid and DNA sequence) amongst these genes
is often below of what is considered reliable in these methods. Among pattern
recognition approaches, HMM are particularly suitable to tackle this problem
because they can handle more naturally the determination of gene edges. We
evaluate the performance of the model using different number of states in the
Markov model, as well as several performance metrics. The model is applied
using public genomic data. Our empirical results show that the VSG genes on T.
brucei can be safely identified (high sensitivity and low rate of false
positives) using HMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05372</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05372</id><created>2015-08-21</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Rojas</keyname><forenames>Cristobal</forenames></author><author><keyname>Schneider</keyname><forenames>Jon</forenames></author></authors><title>Tight space-noise tradeoffs in computing the ergodic measure</title><categories>cs.CC math.DS</categories><comments>25 pages</comments><msc-class>68Q05, 37C40</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we obtain tight bounds on the space-complexity of computing the
ergodic measure of a low-dimensional discrete-time dynamical system affected by
Gaussian noise. If the scale of the noise is $\varepsilon$, and the function
describing the evolution of the system is not by itself a source of
computational complexity, then the density function of the ergodic measure can
be approximated within precision $\delta$ in space polynomial in $\log
1/\varepsilon+\log\log 1/\delta$. We also show that this bound is tight up to
polynomial factors.
  In the course of showing the above, we prove a result of independent interest
in space-bounded computation: that it is possible to exponentiate an $n$ by $n$
matrix to an exponentially large power in space polylogarithmic in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05373</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05373</id><created>2015-08-21</created><authors><author><keyname>Liu</keyname><forenames>Yun-Fu</forenames></author><author><keyname>Guo</keyname><forenames>Jing-Ming</forenames></author></authors><title>Dot-Diffused Halftoning with Improved Homogeneity</title><categories>cs.MM cs.IT math.IT</categories><comments>Accepted to IEEE Trans. on Image Processing</comments><doi>10.1109/TIP.2015.2470599</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to the error diffusion, dot diffusion provides an additional
pixel-level parallelism for digital halftoning. However, even though its
periodic and blocking artifacts had been eased by previous works, it was still
far from satisfactory in terms of the blue noise spectrum perspective. In this
work, we strengthen the relationship among the pixel locations of the same
processing order by an iterative halftoning method, and the results demonstrate
a significant improvement. Moreover, a new approach of deriving the averaged
power spectrum density (APSD) is proposed to avoid the regular sampling of the
well-known Bartlett's procedure which inaccurately presents the halftone
periodicity of certain halftoning techniques with parallelism. As a result, the
proposed dot diffusion is substantially superior to the state-of-the-art
parallel halftoning methods in terms of visual quality and artifact-free
property, and competitive runtime to the theoretical fastest ordered dithering
is offered simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05374</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05374</id><created>2015-05-11</created><authors><author><keyname>Dimitroulis</keyname><forenames>Christos</forenames></author><author><keyname>Raptis</keyname><forenames>Theophanes</forenames></author><author><keyname>Raptis</keyname><forenames>Vasilios</forenames></author></authors><title>POLYANA - A tool for the calculation of molecular radial distribution
  functions based on Molecular Dynamics trajectories</title><categories>cs.MS</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an application for the calculation of radial distribution
functions for molecular centres of mass, based on trajectories generated by
molecular simulation methods (Molecular Dynamics, Monte Carlo). When designing
this application, the emphasis was placed on ease of use as well as ease of
further development. In its current version, the program can read trajectories
generated by the well-known DL_POLY package, but it can be easily extended to
treat other formats. It is also very easy to 'hack' the program so it can
compute intermolecular radial distribution functions for groups of interaction
sites rather than whole molecules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05383</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05383</id><created>2015-08-20</created><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>On Monotonicity of the Optimal Transmission Policy in Cross-layer
  Adaptive m-QAM Modulation</title><categories>stat.ML cs.IT math.IT</categories><comments>27 pages (single column), 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a cross-layer adaptive modulation system that is modeled
as a Markov decision process (MDP). We study how to utilize the monotonicity of
the optimal transmission policy to relieve the computational complexity of
dynamic programming (DP). In this system, a scheduler controls the bit rate of
the m-quadrature amplitude modulation (m-QAM) in order to minimize the
long-term losses incurred by the queue overflow in the data link layer and the
transmission power consumption in the physical layer. The work is done in two
steps. Firstly, we observe the L-natural-convexity and submodularity of DP to
prove that the optimal policy is always nondecreasing in queue occupancy/state
and derive the sufficient condition for it to be nondecreasing in both queue
and channel states. We also show that, due to the L-natural-convexity of DP,
the variation of the optimal policy in queue state is restricted by a bounded
marginal effect: The increment of the optimal policy between adjacent queue
states is no greater than one. Secondly, we use the monotonicity results to
present two low complexity algorithms: monotonic policy iteration (MPI) based
on L-natural-convexity and discrete simultaneous perturbation stochastic
approximation (DSPSA). We run experiments to show that the time complexity of
MPI based on L-natural-convexity is much lower than that of DP and the
conventional MPI that is based on submodularity and DSPSA is able to adaptively
track the optimal policy when the system parameters change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05384</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05384</id><created>2015-08-21</created><authors><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-Laszl&#xf3;</forenames></author></authors><title>Control Principles of Complex Networks</title><categories>cs.SY math.OC physics.soc-ph</categories><comments>55 pages, 41 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reflection of our ultimate understanding of a complex system is our ability
to control its behavior. Typically, control has multiple prerequisites: It
requires an accurate map of the network that governs the interactions between
the system's components, a quantitative description of the dynamical laws that
govern the temporal behavior of each component, and an ability to influence the
state and temporal behavior of a selected subset of the components. With deep
roots in nonlinear dynamics and control theory, notions of control and
controllability have taken a new life recently in the study of complex
networks, inspiring several fundamental questions: What are the control
principles of complex systems? How do networks organize themselves to balance
control with functionality? To address these here we review recent advances on
the controllability and the control of complex networks, exploring the
intricate interplay between a system's structure, captured by its network
topology, and the dynamical laws that govern the interactions between the
components. We match the pertinent mathematical results with empirical findings
and applications. We show that uncovering the control principles of complex
systems can help us explore and ultimately understand the fundamental laws that
govern their behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05400</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05400</id><created>2015-08-21</created><updated>2015-08-27</updated><authors><author><keyname>Zhang</keyname><forenames>Liang</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Renewable Energy-Aware Inter-datacenter Virtual Machine Migration over
  Elastic Optical Networks</title><categories>cs.NI</categories><comments>23 pages, 8 figures, Cloudcom 2015</comments><report-no>TR-ANL-2015-005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datacenters (DCs) are deployed in a large scale to support the ever
increasing demand for data processing to support various applications. The
energy consumption of DCs becomes a critical issue. Powering DCs with renewable
energy can effectively reduce the brown energy consumption and thus alleviates
the energy consumption problem. Owing to geographical deployments of DCs, the
renewable energy generation and the data processing demands usually vary in
different DCs. Migrating virtual machines (VMs) among DCs according to the
availability of renewable energy helps match the energy demands and the
renewable energy generation in DCs, and thus maximizes the utilization of
renewable energy. Since migrating VMs incurs additional traffic in the network,
the VM migration is constrained by the network capacity. The inter-datacenter
(inter-DC) VM migration with network capacity constraints is an NP-hard
problem. In this paper, we propose two heuristic algorithms that approximate
the optimal VM migration solution. Through extensive simulations, we show that
the proposed algorithms, by migrating VM among DCs, can reduce up to 31% of
brown energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05411</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05411</id><created>2015-08-21</created><authors><author><keyname>Gahi</keyname><forenames>Youssef</forenames></author><author><keyname>Guennoun</keyname><forenames>Mouhcine</forenames></author><author><keyname>Guennoun</keyname><forenames>Zouhair</forenames></author><author><keyname>El-khatib</keyname><forenames>Khalil</forenames></author></authors><title>On the use of homomorphic encryption to secure cloud computing,
  services, and routing protocols</title><categories>cs.CR cs.NI</categories><comments>Youssef Gahi, PhD dissertation, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The trend towards delegating data processing to a remote party raises major
concerns related to privacy violations for both end-users and service
providers. These concerns have attracted the attention of the research
community, and several techniques have been proposed to protect against
malicious parties by providing secure communication protocols. Most of the
proposed techniques, however, require the involvement of a third party, and
this by itself can be viewed as another security concern. These security
breaches can be avoided by following a new approach that depends on data
sorted, managed, and stored in encrypted form at the remote servers. To realize
such an approach, the encryption cryptosystem must support algebraic operations
over encrypted data. This cryptosystem can be effective in protecting data and
supporting the construction of programs that can process encrypted input and
produce encrypted output. In fact, the latter programs do not decrypt the
input, and therefore, they can be run by an un-trusted party without revealing
their data and internal states. Furthermore, such programs prove to be
practical in situations where we need to outsource private computations,
especially in the context of cloud computing. Homomorphic cryptosystems are
perfectly aligned with these objectives as they are a strong foundation for
schemes that allow a blind processing of encrypted data without the need to
decrypt them. In this dissertation we rely on homomorphic encryption schemes to
secure cloud computing, services and routing protocols. We design several
circuits that allow for the blind processing and management of data such that
malicious parties are denied access to sensitive information. We select five
areas to apply our models to. These models are easily customized for many other
areas. We also provide prototypes that we use to study the performance and
robustness of our models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05417</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05417</id><created>2015-08-21</created><authors><author><keyname>Kuscu</keyname><forenames>Murat</forenames></author><author><keyname>Akan</keyname><forenames>Ozgur B.</forenames></author></authors><title>On the Physical Design of Receiver for Molecular Communications</title><categories>cs.ET</categories><comments>submitted to IEEE Transactions on Nanobioscience</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communications, where molecules are used to encode, transmit, and
receive information, is a promising means of enabling the coordination of
nanoscale devices. The paradigm has been extensively studied from various
aspects, including channel modeling and noise analysis. Comparatively little
attention has been given to the physical design of molecular receiver and
transmitter, envisioning biological synthetic cells with intrinsic molecular
reception and transmission abilities as the future nanomachines. However, this
assumption leads to a discrepancy between the envisaged applications requiring
complex communication interfaces and protocols, and the very limited
computational capacities of the envisioned biological nanomachines. In this
paper, we examine the feasibility of designing a molecular receiver, in a
physical domain other than synthetic biology, meeting the basic requirements of
nanonetwork applications. We first review the state-of-the-art biosensing
approaches to determine whether they can inspire a receiver design. We reveal
that nanoscale field effect transistor (FET)-based electrical biosensor is a
particularly useful starting point towards designing a molecular receiver.
Focusing on FET-based molecular receivers with a conceptual approach, we
provide a guideline elaborating on their operation principles, performance
metrics, and design parameters. We then develop a deterministic model for
signal flow in silicon nanowire (SiNW) FET-based molecular receiver, and
investigate its sensitivity for varying system parameters. Lastly, we discuss
the practical challenges of implementing the receiver, and present the future
research avenues from a communication theoretical perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05424</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05424</id><created>2015-08-21</created><authors><author><keyname>Bordewich</keyname><forenames>Magnus</forenames></author><author><keyname>Semple</keyname><forenames>Charles</forenames></author></authors><title>Reticulation-Visible Networks</title><categories>math.CO cs.DS</categories><msc-class>05C85, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ be a finite set, $\mathcal N$ be a reticulation-visible network on
$X$, and $\mathcal T$ be a rooted binary phylogenetic tree. We show that there
is a polynomial-time algorithm for deciding whether or not $\mathcal N$
displays $\mathcal T$. Furthermore, for all $|X|\ge 1$, we show that $\mathcal
N$ has at most $8|X|-7$ vertices in total and at most $3|X|-3$ reticulation
vertices, and that these upper bounds are sharp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05430</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05430</id><created>2015-08-21</created><authors><author><keyname>Rahman</keyname><forenames>Md. Mazder</forenames></author><author><keyname>Dueck</keyname><forenames>Gerhard W.</forenames></author></authors><title>Synthesis of Linear Nearest Neighbor Quantum Circuits</title><categories>cs.ET</categories><comments>Presented at the 10th International Workshop on Boolean Problems
  (2012), Freiberg, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents models for transforming standard reversible circuits into
Linear Nearest Neighbor (LNN) architecture without inserting SWAP gates.
Templates to optimize the transformed LNN circuits are proposed. All minimal
LNN circuits for all 3-qubit functions have been generated to serve as
benchmarks to evaluate heuristic optimization algorithms. The minimal results
generated are compared with optimized LNN circuits obtained from the post
synthesis algorithm --- template matching with LNN templates. Experiments show
that the suggested synthesis flow significantly improves the quantum cost of
circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05457</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05457</id><created>2015-08-21</created><authors><author><keyname>Pranata</keyname><forenames>Heru</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Ependi</keyname><forenames>Usman</forenames></author></authors><title>Analisis Keamanan Protokol Secure Socket Layer (SSL) Terhadap Proses
  Sniffing di Jaringan</title><categories>cs.CR</categories><comments>6 pages, Student Colloquium Sistem Informasi &amp; Teknik Informatika
  (SC-SITI) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of information technology, especially in the field of computer
network allows the exchange of information faster and more complex and the data
that is exchanged can vary. Security of data on communication in the network is
a major thing. Secure socket layer (SSL) is the solution to the problem, but
further research on the security of the SSL protocol transactions should be
done to determine the extent of SSL can secure the data on the network. When
the computer sends data across the network, the data is transmitted in packets.
Sniffing is a technique of monitoring of every packet traversing the network.
Security threat presented by sniffers is their ability to capture all incoming
and outgoing packets through the network, which includes the passwords,
usernames and other sensitive issues. Packet sniffer captures the data
addressed to other devices, which will then be stored for later analysis later.
Sniffing can also be used by system administrators to monitor the network and
solve problems in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05463</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05463</id><created>2015-08-21</created><updated>2015-11-10</updated><authors><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Siva</keyname><forenames>Parthipan</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity</title><categories>cs.CV cs.LG cs.NE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks is a branch in machine learning that has seen a meteoric
rise in popularity due to its powerful abilities to represent and model
high-level abstractions in highly complex data. One area in deep neural
networks that is ripe for exploration is neural connectivity formation. A
pivotal study on the brain tissue of rats found that synaptic formation for
specific functional connectivity in neocortical neural microcircuits can be
surprisingly well modeled and predicted as a random formation. Motivated by
this intriguing finding, we introduce the concept of StochasticNet, where deep
neural networks are formed via stochastic connectivity between neurons. As a
result, any type of deep neural networks can be formed as a StochasticNet by
allowing the neuron connectivity to be stochastic. Stochastic synaptic
formations, in a deep neural network architecture, can allow for efficient
utilization of neurons for performing specific tasks. To evaluate the
feasibility of such a deep neural network architecture, we train a
StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and
STL-10). Experimental results show that a StochasticNet, using less than half
the number of neural connections as a conventional deep neural network,
achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and
SVHN dataset. Interestingly, StochasticNet with less than half the number of
neural connections, achieved a higher accuracy (relative improvement in test
error rate of ~6% compared to ConvNet) on the STL-10 dataset than a
conventional deep neural network. Finally, StochasticNets have faster
operational speeds while achieving better or similar accuracy performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05465</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05465</id><created>2015-08-21</created><updated>2016-01-25</updated><authors><author><keyname>Yoshikawa</keyname><forenames>Hiyori</forenames></author><author><keyname>Hirai</keyname><forenames>Hiroshi</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author></authors><title>A representation of antimatroids by Horn rules and its application to
  educational systems</title><categories>math.CO cs.LO</categories><comments>Major revision; including references/connections on implicational
  systems and updating experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a representation of an antimatroid by Horn rules, motivated by its
recent application to computer-aided educational systems. We associate any set
$\mathcal{R}$ of Horn rules with the unique maximal antimatroid
$\mathcal{A}(\mathcal{R})$ that is contained in the union-closed family
$\mathcal{K}(\mathcal{R})$ naturally determined by ${\cal R}$. We address
algorithmic and Boolean function theoretic aspects on the association ${\cal R}
\mapsto \mathcal{A}(\mathcal{R})$, where ${\cal R}$ is viewed as the input. We
present linear time algorithms to solve the membership problem and the
inference problem for ${\cal A}({\cal R})$. We also provide efficient
algorithms for generating all members and all implicates of ${\cal A}({\cal
R})$. We show that this representation is essentially equivalent to the
Korte-Lov\'{a}sz representation of antimatroids by rooted sets. Based on the
equivalence, we provide a quadratic time algorithm to construct the
uniquely-determined minimal representation. % These results have potential
applications to computer-aided educational systems, where an antimatroid is
used as a model of the space of possible knowledge states of learners, and is
constructed by giving Horn queries to a human expert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05470</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05470</id><created>2015-08-22</created><authors><author><keyname>Naidan</keyname><forenames>Bilegsaikhan</forenames></author><author><keyname>Boytsov</keyname><forenames>Leonid</forenames></author></authors><title>Non-Metric Space Library Manual</title><categories>cs.MS cs.IR</categories><comments>Methodology paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes a library for similarity searching. Even though the
library contains a variety of metric-space access methods, our main focus is on
search methods for non-metric spaces. Because there are fewer exact solutions
for non-metric spaces, many of our methods give only approximate answers. Thus,
the methods are evaluated in terms of efficiency-effectiveness trade-offs
rather than merely in terms of their efficiency. Our goal is, therefore, to
provide not only state-of-the-art approximate search methods for both
non-metric and metric spaces, but also the tools to measure search quality. We
concentrate on technical details, i.e., how to compile the code, run the
benchmarks, evaluate results, and use our code in other applications.
Additionally, we explain how to extend the code by adding new search methods
and spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05477</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05477</id><created>2015-08-22</created><authors><author><keyname>Huang</keyname><forenames>Wenchao</forenames></author><author><keyname>Xiong</keyname><forenames>Yan</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Hu</keyname><forenames>Yiqing</forenames></author><author><keyname>Mao</keyname><forenames>Xufei</forenames></author><author><keyname>Yang</keyname><forenames>Panlong</forenames></author></authors><title>WalkieLokie: Relative Positioning for Augmented Reality Using a Dummy
  Acoustic Speaker</title><categories>cs.CY</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and implement a novel relative positioning system, WalkieLokie, to
enable more kinds of Augmented Reality applications, e.g., virtual shopping
guide, virtual business card sharing. WalkieLokie calculates the distance and
direction between an inquiring user and the corresponding target. It only
requires a dummy speaker binding to the target and broadcasting inaudible
acoustic signals. Then the user walking around can obtain the position using a
smart device. The key insight is that when a user walks, the distance between
the smart device and the speaker changes; and the pattern of displacement
(variance of distance) corresponds to the relative position. We use a
second-order phase locked loop to track the displacement and further estimate
the position. To enhance the accuracy and robustness of our strategy, we
propose a synchronization mechanism to synthesize all estimation results from
different timeslots. We show that the mean error of ranging and direction
estimation is 0.63m and 2.46 degrees respectively, which is accurate even in
case of virtual business card sharing. Furthermore, in the shopping mall where
the environment is quite severe, we still achieve high accuracy of positioning
one dummy speaker, and the mean position error is 1.28m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05488</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05488</id><created>2015-08-22</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author></authors><title>CudaChain: A Practical GPU-accelerated 2D Convex Hull Algorithm</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a practical GPU-accelerated convex hull algorithm and a
novel Sorting-based Preprocessing Approach (SPA) for planar point sets. The
proposed algorithm consists of two stages: (1) two rounds of preprocessing
performed on the GPU and (2) the finalization of calculating the expected
convex hull on the CPU. We first discard the interior points that locate inside
a quadrilateral formed by four extreme points, and then distribute the
remaining points into several (typically four) sub regions. For each subset of
points, we first sort them in parallel, then perform the second round of
discarding using SPA, and finally form a simple chain for the current remaining
points. A simple polygon can be easily generated by directly connecting all the
chains in sub regions. We at last obtain the expected convex hull of the input
points by calculating the convex hull of the simple polygon. We use the library
Thrust to realize the parallel sorting, reduction, and partitioning for better
efficiency and simplicity. Experimental results show that our algorithm
achieves 5x ~ 6x speedups over the Qhull implementation for 20M points. Thus,
this algorithm is competitive in practical applications for its simplicity and
satisfied efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05494</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05494</id><created>2015-08-22</created><authors><author><keyname>Erhard</keyname><forenames>Michael</forenames></author><author><keyname>Horn</keyname><forenames>Greg</forenames></author><author><keyname>Diehl</keyname><forenames>Moritz</forenames></author></authors><title>A quaternion-based model for optimal control of the SkySails airborne
  wind energy system</title><categories>math.OC cs.SY</categories><comments>18 pages, 14 figures, submitted to Journal of Applied Mathematics and
  Mechanics (ZAMM)</comments><msc-class>49N90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Airborne wind energy systems are capable of extracting energy from higher
wind speeds at higher altitudes. The configuration considered in this paper is
based on a tethered kite flown in a pumping orbit. This pumping cycle generates
energy by winching out at high tether forces and driving a generator while
flying figures-of-eight, or lemniscates, as crosswind pattern. Then, the tether
is reeled in while keeping the kite at a neutral position, thus leaving a net
amount of generated energy. In order to achieve an economic operation,
optimization of pumping cycles is of great interest.
  In this paper, first the principles of airborne wind energy will be briefly
revisited. The first contribution is a singularity-free model for the tethered
kite dynamics in quaternion representation, where the model is derived from
first principles. The second contribution is an optimal control formulation and
numerical results for complete pumping cycles. Based on the developed model,
the setup of the optimal control problem (OCP) is described in detail along
with its numerical solution based on the direct multiple shooting method in the
CasADi optimization environment. Optimization results for a pumping cycle
consisting of six lemniscates show that the approach is capable to find an
optimal orbit in a few minutes of computation time. For this optimal orbit, the
power output is increased by a factor of two compared to a sophisticated
initial guess for the considered test scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05495</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05495</id><created>2015-08-22</created><authors><author><keyname>Korki</keyname><forenames>Mehdi</forenames></author><author><keyname>Zayyani</keyname><forenames>Hadi</forenames></author><author><keyname>Zhang</keyname><forenames>Jingxin</forenames></author></authors><title>Bayesian Hypothesis Testing for Block Sparse Signal Recovery</title><categories>stat.ML cs.IT math.IT</categories><comments>5 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1412.2316</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents a novel Block Bayesian Hypothesis Testing Algorithm
(Block-BHTA) for reconstructing block sparse signals with unknown block
structures. The Block-BHTA comprises the detection and recovery of the
supports, and the estimation of the amplitudes of the block sparse signal. The
support detection and recovery is performed using a Bayesian hypothesis
testing. Then, based on the detected and reconstructed supports, the nonzero
amplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA is
demonstrated by numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05497</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05497</id><created>2015-08-22</created><updated>2015-09-23</updated><authors><author><keyname>John</keyname><forenames>Ajith K.</forenames></author><author><keyname>Shah</keyname><forenames>Shetal</forenames></author><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author><author><keyname>Akshay</keyname><forenames>S.</forenames></author></authors><title>Skolem Functions for Factored Formulas</title><categories>cs.LO</categories><comments>Full version of FMCAD 2015 conference publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a propositional formula F(x,y), a Skolem function for x is a function
\Psi(y), such that substituting \Psi(y) for x in F gives a formula semantically
equivalent to \exists F. Automatically generating Skolem functions is of
significant interest in several applications including certified QBF solving,
finding strategies of players in games, synthesising circuits and bit-vector
programs from specifications, disjunctive decomposition of sequential circuits
etc. In many such applications, F is given as a conjunction of factors, each of
which depends on a small subset of variables. Existing algorithms for Skolem
function generation ignore any such factored form and treat F as a monolithic
function. This presents scalability hurdles in medium to large problem
instances. In this paper, we argue that exploiting the factored form of F can
give significant performance improvements in practice when computing Skolem
functions. We present a new CEGAR style algorithm for generating Skolem
functions from factored propositional formulas. In contrast to earlier work,
our algorithm neither requires a proof of QBF satisfiability nor uses
composition of monolithic conjunctions of factors. We show experimentally that
our algorithm generates smaller Skolem functions and outperforms
state-of-the-art approaches on several large benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05506</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05506</id><created>2015-08-22</created><authors><author><keyname>Fuda</keyname><forenames>Toru</forenames></author><author><keyname>Tonozaki</keyname><forenames>Miho</forenames></author></authors><title>Brudno's theorem for Z^d (or Z^d_+) subshifts</title><categories>math.DS cs.IT math.IT</categories><comments>19 pages</comments><msc-class>28Dxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Brudno's theorem of $1$-dimensional shift dynamical system to
$\mathbb{Z}^d$ (or $\mathbb{Z}_+^d$) subshifts. That is to say, in
$\mathbb{Z}^d$ (or $\mathbb{Z}^d_+$) subshift, the Kolmogorov-Sinai entropy is
equivalent to the Kolmogorov complexity density almost everywhere for an
ergodic shift-invariant measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05508</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05508</id><created>2015-08-22</created><authors><author><keyname>Peng</keyname><forenames>Baolin</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Wong</keyname><forenames>Kam-Fai</forenames></author></authors><title>Towards Neural Network-based Reasoning</title><categories>cs.AI cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05514</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05514</id><created>2015-08-22</created><authors><author><keyname>Ardeshiri</keyname><forenames>Tohid</forenames></author><author><keyname>Orguner</keyname><forenames>Umut</forenames></author><author><keyname>&#xd6;zkan</keyname><forenames>Emre</forenames></author></authors><title>Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence</title><categories>stat.ML cs.CV cs.LG cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a greedy mixture reduction algorithm which is capable of pruning
mixture components as well as merging them based on the Kullback-Leibler
divergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD
based method since it is not restricted to merging operations. The capability
of pruning (in addition to merging) gives the algorithm the ability of
preserving the peaks of the original mixture during the reduction. Analytical
approximations are derived to circumvent the computational intractability of
the KLD which results in a computationally efficient method. The proposed
algorithm is compared with Runnalls' and Williams' methods in two numerical
examples, using both simulated and real world data. The results indicate that
the performance and computational complexity of the proposed approach make it
an efficient alternative to existing mixture reduction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05515</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05515</id><created>2015-08-22</created><authors><author><keyname>Shi</keyname><forenames>Yishuo</forenames></author><author><keyname>Zhang</keyname><forenames>Zhao</forenames></author><author><keyname>Du</keyname><forenames>Ding-Zhu</forenames></author></authors><title>Approximation Algorithm for Minimum Weight (k,m)-CDS Problem in Unit
  Disk Graph</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wireless sensor network, the virtual backbone plays an important role.
Due to accidental damage or energy depletion, it is desirable that the virtual
backbone is fault-tolerant. A fault-tolerant virtual backbone can be modeled as
a k-connected m-fold dominating set ((k,m)-CDS for short). In this paper, we
present a constant approximation algorithm for the minimum weight (k,m)-CDS
problem in unit disk graph under the assumption that m&gt;=k. Prior to this work,
constant approximation algorithms are known for k=1 with weight and 2&lt;=k&lt;=3
without weight. Our result is the first constant approximation algorithm for
the (k,m)-CDS problem with general k,m and with weight. The performance ratio
is (\alpha+5\rho) for k&gt;=3 and (\alpha+2.5\rho) for k=2, where \alpha is the
performance ratio for the minimum weight m-fold dominating set problem and \rho
is the performance ratio for the subset k-connected subgraph problem. Using
currently best known ratios for \alpha and \rho, the performance ratio is
(k^2\log k) for k&gt;=3 and (8+\epsilon) for k=2, where \epsilon is an arbitrary
positive real number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05525</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05525</id><created>2015-08-22</created><authors><author><keyname>Gong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Exploiting Social Trust Assisted Reciprocity (STAR) towards
  Utility-Optimal Socially-aware Crowdsensing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile crowdsensing takes advantage of pervasive mobile devices to collect
and process data for a variety of applications (e.g., traffic monitoring,
spectrum sensing). In this study, a socially-aware crowdsensing system is
advocated, in which a cloud-based platform incentivizes mobile users to
participate in sensing tasks} by leveraging social trust among users, upon
receiving sensing requests. For this system, social trust assisted reciprocity
(STAR) - a synergistic marriage of social trust and reciprocity, is exploited
to design an incentive mechanism that stimulates users' participation.
  Given the social trust structure among users, the efficacy of STAR for
satisfying users' sensing requests is thoroughly investigated. Specifically, it
is first shown that all requests can be satisfied if and only if sufficient
social credit can be &quot;transferred&quot; from users who request more sensing service
than they can provide to users who can provide more than they request. Then
utility maximization for sensing services under STAR is investigated, and it is
shown that it boils down to maximizing the utility of a circulation flow in the
combined social graph and request graph. Accordingly, an algorithm that
iteratively cancels a cycle of positive weight in the residual graph is
developed, which computes the optimal solution efficiently, for both cases of
divisible and indivisible sensing service. Extensive simulation results
corroborate that STAR can significantly outperform the mechanisms using social
trust only or reciprocity only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05537</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05537</id><created>2015-08-22</created><authors><author><keyname>Gui</keyname><forenames>Ning</forenames></author><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Blondia</keyname><forenames>Chris</forenames></author></authors><title>A framework for adaptive real-time applications: the declarative
  real-time OSGi component model</title><categories>cs.SE</categories><comments>Published in Proc. of the 7th workshop on Reflective and adaptive
  middleware (ARM-08). Authors' version</comments><doi>10.1145/1462716.1462722</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, more and more applications require OSGi to have some form of
real-time support, which is currently very limited. The resulting closed-system
solutions lack of a standard management scheme which forbids standard,
system-wide policies for real-time system's deployment, adaptation, and
reconfiguration. In order to tackle this problem, this paper proposes a
declarative real-time component model. In this model, the distinguishing
real-time contract of each component is declaratively described, and a general
component real-time management interface is designed. They are used to maintain
an accurate view of existing real-time components' promised contracts. A
real-time component runtime service is designed to control the whole lifecycle
of the components. By using global information and general control interface,
it can adjust the system continue to operate without impairing the deployed
components' real-time contracts in the face of run-time changes. This system
allows itself to be easily extended with other constraint resolving policies to
fit different context. The prototype has been tested into a simulated control
system. The result shows this framework can provide good real time performance
while still provides real-time component dynamicity support as well. To the
best of our knowledge, this is the first comprehensive solution providing
explicit real-time support from design to execution in OSGi framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05538</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05538</id><created>2015-08-22</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author><author><keyname>Nikishkin</keyname><forenames>Vladimir</forenames></author></authors><title>Optimal Algorithms and Lower Bounds for Testing Closeness of Structured
  Distributions</title><categories>cs.DS cs.IT math.IT math.ST stat.TH</categories><comments>27 pages, to appear in FOCS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a general unified method that can be used for $L_1$ {\em closeness
testing} of a wide range of univariate structured distribution families. More
specifically, we design a sample optimal and computationally efficient
algorithm for testing the equivalence of two unknown (potentially arbitrary)
univariate distributions under the $\mathcal{A}_k$-distance metric: Given
sample access to distributions with density functions $p, q: I \to \mathbb{R}$,
we want to distinguish between the cases that $p=q$ and
$\|p-q\|_{\mathcal{A}_k} \ge \epsilon$ with probability at least $2/3$. We show
that for any $k \ge 2, \epsilon&gt;0$, the {\em optimal} sample complexity of the
$\mathcal{A}_k$-closeness testing problem is $\Theta(\max\{
k^{4/5}/\epsilon^{6/5}, k^{1/2}/\epsilon^2 \})$. This is the first $o(k)$
sample algorithm for this problem, and yields new, simple $L_1$ closeness
testers, in most cases with optimal sample complexity, for broad classes of
structured distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05542</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05542</id><created>2015-08-22</created><authors><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Geraseminko</keyname><forenames>Mikhail</forenames></author><author><keyname>Yeh</keyname><forenames>Shu-ping</forenames></author><author><keyname>Himayat</keyname><forenames>Nageen</forenames></author><author><keyname>Talwar</keyname><forenames>Shilpa</forenames></author></authors><title>Proportional Fair Traffic Splitting and Aggregation in Heterogeneous
  Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic load balancing and resource allocation is set to play a crucial role
in leveraging the dense and increasingly heterogeneous deployment of wireless
networks. Although the problem of optimal user to access point (AP) association
has received considerable traction, optimal algorithms for splitting user
traffic across multiple radio access technologies (RATs) and/or across APs
available to a user are not well investigated. In this paper, we consider a
scenario where traffic for each user may be split across macrocell and small
cell, and develop a closed form solution is developed for optimal splitting and
aggregation. The optimal solution lends itself to a &quot;water-filling&quot; based
interpretation, where the fraction of user's traffic sent over macrocell is
proportional to ratio of user's peak capacity on that macrocell and its
throughput on the small cell. Moreover, this fraction is shown to increase with
increasing latency on the small cell backhaul. Using comprehensive system level
simulations, the developed optimal solution is shown to provide substantial
edge and median throughput gain over representative association algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05545</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05545</id><created>2015-08-22</created><updated>2016-01-14</updated><authors><author><keyname>Weilbach</keyname><forenames>Christian</forenames></author><author><keyname>K&#xfc;hne</keyname><forenames>Konrad</forenames></author><author><keyname>Bieniusa</keyname><forenames>Annette</forenames></author></authors><title>Decoupling conflicts for configurable resolution in an open replication
  system</title><categories>cs.DC</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replikativ is a replication middleware supporting a new kind of confluent
replicated datatype resembling a distributed version control system. It retains
the order of write operations at the trade-off of reduced availability with
after-the- fact conflict resolution. The system allows to develop applications
with distributed state in a similar fashion as native applications with
exclusive local state, while transparently exposing the necessary compromises
in terms of the CAP theorem. In this paper, we give a specification of the
replicated datatype and discuss its usage in the replikativ middleware.
Experiments with the implementation show the feasibility of the concept as a
foundation for replication as a service (RaaS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05550</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05550</id><created>2015-08-22</created><updated>2016-02-22</updated><authors><author><keyname>Lindenbaum</keyname><forenames>Ofir</forenames></author><author><keyname>Yeredor</keyname><forenames>Arie</forenames></author><author><keyname>Salhov</keyname><forenames>Moshe</forenames></author><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author></authors><title>MultiView Diffusion Maps</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we consider learning a reduced dimensionality representation
from datasets obtained under multiple views. Such multiple views of datasets
can be obtained, for example, when the same underlying process is observed
using several different modalities, or measured with different instrumentation.
Our goal is to effectively exploit the availability of such multiple views for
various purposes, such as non-linear embedding, manifold learning, spectral
clustering, anomaly detection and non-linear system identification. Our
proposed method exploits the intrinsic relation within each view, as well as
the mutual relations between views. We do this by defining a cross-view model,
in which an implied Random Walk process between objects is restrained to hop
between the different views. Our method is robust to scaling of each dataset,
and is insensitive to small structural changes in the data. Within this
framework, we define new diffusion distances and analyze the spectra of the
implied kernels. We demonstrate the applicability of the proposed approach on
both artificial and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05553</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05553</id><created>2015-08-22</created><authors><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>A Practical O(R\log\log n+n) time Algorithm for Computing the Longest
  Common Subsequence</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the much studied LCS problem for two given
sequences. Based on the algorithm of Iliopoulos and Rahman for solving the LCS
problem, we have suggested 3 new improved algorithms. We first reformulate the
problem in a very succinct form. The problem LCS is abstracted to an abstract
data type DS on an ordered positive integer set with a special operation
Update(S,x). For the two input sequences X and Y of equal length n, the first
improved algorithm uses a van Emde Boas tree for DS and its time and space
complexities are O(R\log\log n+n) and O(R), where R is the number of matched
pairs of the two input sequences. The second algorithm uses a balanced binary
search tree for DS and its time and space complexities are O(R\log L+n) and
O(R), where L is the length of the longest common subsequence of X and Y. The
third algorithm uses an ordered vector for DS and its time and space
complexities are O(nL) and O(R).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05559</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05559</id><created>2015-08-22</created><updated>2015-10-11</updated><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author></authors><title>Structured Interactive Music Scores</title><categories>cs.LO</categories><acm-class>D.1.6; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Interactive Scores is a formalism for the design and performance of
interactive scenarios that provides temporal relations (TRs) among the objects
of the scenario. We can model TRs among objects in Time Stream Petri nets, but
it is difficult to represent global constraints. This can be done explicitly in
the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We want to
formalize a heterogeneous system that controls in one subsystem the concurrent
execution of the objects using ntcc, and audio and video processing in the
other. We also plan to develop an automatic verifier for ntcc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05565</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05565</id><created>2015-08-22</created><updated>2015-12-04</updated><authors><author><keyname>Ding</keyname><forenames>Weicong</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Necessary and Sufficient Conditions and a Provably Efficient Algorithm
  for Separable Topic Discovery</title><categories>cs.LG cs.CL cs.IR stat.ML</categories><comments>Typo corrected; Revised argument in Lemma 3 and 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop necessary and sufficient conditions and a novel provably
consistent and efficient algorithm for discovering topics (latent factors) from
observations (documents) that are realized from a probabilistic mixture of
shared latent factors that have certain properties. Our focus is on the class
of topic models in which each shared latent factor contains a novel word that
is unique to that factor, a property that has come to be known as separability.
Our algorithm is based on the key insight that the novel words correspond to
the extreme points of the convex hull formed by the row-vectors of a suitably
normalized word co-occurrence matrix. We leverage this geometric insight to
establish polynomial computation and sample complexity bounds based on a few
isotropic random projections of the rows of the normalized word co-occurrence
matrix. Our proposed random-projections-based algorithm is naturally amenable
to an efficient distributed implementation and is attractive for modern
web-scale distributed data mining applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05567</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05567</id><created>2015-08-23</created><authors><author><keyname>Grimmer</keyname><forenames>Benjamin</forenames></author></authors><title>Dual-Fitting Approximation Algorithms for Network Connectivity Problems</title><categories>cs.DS cs.DM math.OC</categories><acm-class>G.1.6; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the NP-complete network connectivity problem of Dual Power
Assignment (DPA). This models an ad hoc networks where each node can either
operate at high or low power. The goal is to produce a minimum power strongly
connected network. We give a Dual-Fitting algorithm to DPA with a
3/2-approximation ratio, improving the previous best know approximation of
$11/7\approx 1.57$. Another standard network design problem is Minimum Strongly
Connected Spanning Subgraph (MSCS). We propose a new problem generalizing MSCS
and DPA called Star Strong Connectivity (SSC). Then we show that our
Dual-Fitting approach achieves a 1.6-approximation ratio to SSC. This
Dual-Fitting approach may have applications to other connectivity programs with
cut-based linear programming relaxations. As a result of our approximations, we
prove new upper bounds on the integrality gaps of these problems. For
completeness, we present a family of instances of MSCS (and thus SSC) with
integrality gap approaching 4/3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05572</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05572</id><created>2015-08-23</created><updated>2015-09-23</updated><authors><author><keyname>Vaidhiyan</keyname><forenames>Nidhin Koshy</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Learning to detect an oddball target</title><categories>cs.IT math.IT</categories><comments>24 pages, 4 figures. Submitted to IEEE Transactions on Information
  Theory. A new analytical proof replaces the previous proof of Proposition 3,
  which was based on numerical computations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting an odd process among a group of Poisson
point processes, all having the same rate except the odd process. The actual
rates of the odd and non-odd processes are unknown to the decision maker. We
consider a time-slotted sequential detection scenario where, at the beginning
of each slot, the decision maker can choose which process to observe during
that time slot. We are interested in policies that satisfy a given constraint
on the probability of false detection. We propose a generalised likelihood
ratio based sequential policy which, via suitable thresholding, can be made to
satisfy the given constraint on the probability of false detection. Further, we
show that the proposed policy is asymptotically optimal in terms of the
conditional expected stopping time among all policies that satisfy the
constraint on the probability of false detection. The asymptotic is as the
probability of false detection is driven to zero.
  We apply our results to a particular visual search experiment studied
recently by neuroscientists. Our model suggests a neuronal dissimilarity index
for the visual search task. The neuronal dissimilarity index, when applied to
visual search data from the particular experiment, correlates strongly with the
behavioural data. However, the new dissimilarity index performs worse than some
previously proposed neuronal dissimilarity indices. We explain why this may be
attributed to the experiment conditons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05573</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05573</id><created>2015-08-23</created><authors><author><keyname>Brewster</keyname><forenames>Richard C.</forenames></author><author><keyname>McGuinness</keyname><forenames>Sean</forenames></author><author><keyname>Moore</keyname><forenames>Benjamin</forenames></author><author><keyname>Noel</keyname><forenames>Jonathan A.</forenames></author></authors><title>A Dichotomy Theorem for Circular Colouring Reconfiguration</title><categories>math.CO cs.CC cs.DM</categories><comments>20 pages, 4 figures</comments><msc-class>05C15, 05C85, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;reconfiguration problem&quot; for circular colourings asks, given two
$(p,q)$-colourings $f$ and $g$ of a graph $G$, is it possible to transform $f$
into $g$ by changing the colour of one vertex at a time such that every
intermediate mapping is a $(p,q)$-colouring? We show that this problem can be
solved in polynomial time for $2\leq p/q &lt;4$ and is PSPACE-complete for
$p/q\geq 4$. This generalizes a known dichotomy theorem for reconfiguring
classical graph colourings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05581</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05581</id><created>2015-08-23</created><updated>2015-11-02</updated><authors><author><keyname>Pang</keyname><forenames>Yanwei</forenames></author><author><keyname>Cao</keyname><forenames>Jiale</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Learning Sampling Distributions for Efficient Object Detection</title><categories>cs.CV cs.LG</categories><comments>14 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is an important task in computer vision and learning
systems. Multistage particle windows (MPW), proposed by Gualdi et al., is an
algorithm of fast and accurate object detection. By sampling particle windows
from a proposal distribution (PD), MPW avoids exhaustively scanning the image.
Despite its success, it is unknown how to determine the number of stages and
the number of particle windows in each stage. Moreover, it has to generate too
many particle windows in the initialization step and it redraws unnecessary too
many particle windows around object-like regions. In this paper, we attempt to
solve the problems of MPW. An important fact we used is that there is large
probability for a randomly generated particle window not to contain the object
because the object is a sparse event relevant to the huge number of candidate
windows. Therefore, we design the proposal distribution so as to efficiently
reject the huge number of non-object windows. Specifically, we propose the
concepts of rejection, acceptance, and ambiguity windows and regions. This
contrasts to MPW which utilizes only on region of support. The PD of MPW is
acceptance-oriented whereas the PD of our method (called iPW) is
rejection-oriented. Experimental results on human and face detection
demonstrate the efficiency and effectiveness of the iPW algorithm. The source
code is publicly accessible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05591</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05591</id><created>2015-08-23</created><updated>2015-09-23</updated><authors><author><keyname>Nasir</keyname><forenames>Muhammad Anis Uddin</forenames></author><author><keyname>Girdzijauskas</keyname><forenames>Sarunas</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author></authors><title>Socially-Aware Distributed Hash Tables for Decentralized Online Social
  Networks</title><categories>cs.DC cs.SI</categories><comments>10 pages, p2p 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many decentralized online social networks (DOSNs) have been proposed due to
an increase in awareness related to privacy and scalability issues in
centralized social networks. Such decentralized networks transfer processing
and storage functionalities from the service providers towards the end users.
DOSNs require individualistic implementation for services, (i.e., search,
information dissemination, storage, and publish/subscribe). However, many of
these services mostly perform social queries, where OSN users are interested in
accessing information of their friends. In our work, we design a socially-aware
distributed hash table (DHTs) for efficient implementation of DOSNs. In
particular, we propose a gossip-based algorithm to place users in a DHT, while
maximizing the social awareness among them. Through a set of experiments, we
show that our approach reduces the lookup latency by almost 30% and improves
the reliability of the communication by nearly 10% via trusted contacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05608</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05608</id><created>2015-08-23</created><authors><author><keyname>David</keyname><forenames>Yahel</forenames></author><author><keyname>Shimkin</keyname><forenames>Nahum</forenames></author></authors><title>The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced
with several sources (arms) of items (rewards), and interested in finding the
best item overall. At each time step the agent chooses an arm, and obtains a
random real valued reward. The rewards of each arm are assumed to be i.i.d.,
with an unknown probability distribution that generally differs among the arms.
Under the PAC framework, we provide lower bounds on the sample complexity of
any $(\epsilon,\delta)$-correct algorithm, and propose algorithms that attain
this bound up to logarithmic factors. We compare the performance of this
multi-arm algorithms to the variant in which the arms are not distinguishable
by the agent and are chosen randomly at each stage. Interestingly, when the
maximal rewards of the arms happen to be similar, the latter approach may
provide better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05617</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05617</id><created>2015-08-23</created><authors><author><keyname>de Franca</keyname><forenames>Fabricio Olivetti</forenames></author><author><keyname>Goya</keyname><forenames>Denise Hideko</forenames></author><author><keyname>Penteado</keyname><forenames>Claudio Luis de Camargo</forenames></author></authors><title>The message does not matter: the influence of the network on information
  diffusion</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How an information spreads throughout a social network is a valuable
knowledge sought by many groups such as marketing enterprises and political
parties. If they can somehow predict the impact of a given message or
manipulate it in order to amplify how long it will spread, it would give them a
huge advantage over their competitors. Intuitively, it is expected that two
factors contribute to make an information becoming viral: how influential the
person who spreads is inside its network and the content of the message. The
former should have a more important role, since people will not just blindly
share any content, or will they? In this work it is found that the degree of a
node alone is capable of accurately predicting how many followers of the seed
user will spread the information through a simple linear regression. The
analysis was performed with five different messages from Twitter network that
was shared with different degrees along the users. The results show evidences
that no matter the content, the number of affected neighbors is predictable.
The role of the content of the messages of a user is likely to influence the
network formation and the path the message will follow through the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05624</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05624</id><created>2015-08-23</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author><author><keyname>Nedic</keyname><forenames>Olgica</forenames></author><author><keyname>Fronczak</keyname><forenames>Agata</forenames></author><author><keyname>Fronczak</keyname><forenames>Piotr</forenames></author></authors><title>Quantifying the quality of peer reviewers through Zipf's law</title><categories>physics.soc-ph cs.DL</categories><comments>28 pages; 8 Tables; 9 Figures; 39 references; prepared for and to be
  published in Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a statistical and other analysis of peer reviewers in
order to approach their &quot;quality&quot; through some quantification measure, thereby
leading to some quality metrics. Peer reviewer reports for the Journal of the
Serbian Chemical Society are examined. The text of each report has first to be
adapted to word counting software in order to avoid jargon inducing confusion
when searching for the word frequency: e.g. C must be distinguished, depending
if it means Carbon or Celsius, etc. Thus, every report has to be carefully
&quot;rewritten&quot;. Thereafter, the quantity, variety and distribution of words are
examined in each report and compared to the whole set. Two separate months,
according when reports came in, are distinguished to observe any possible
hidden spurious effects. Coherence is found. An empirical distribution is
searched for through a Zipf-Pareto rank-size law. It is observed that peer
review reports are very far from usual texts in this respect. Deviations from
the usual (first) Zipf's law are discussed. A theoretical suggestion for the
&quot;best (or worst) report&quot; and by extension &quot;good (or bad) reviewer&quot;, within this
context, is provided from an entropy argument, through the concept of &quot;distance
to average&quot; behavior. Another entropy-based measure also allows to measure the
journal reviews (whence reviewers) for further comparison with other journals
through their own reviewer reports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05626</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05626</id><created>2015-08-23</created><authors><author><keyname>Maguire</keyname><forenames>Joseph</forenames></author><author><keyname>Renaud</keyname><forenames>Karen</forenames></author></authors><title>You Only Live Twice or &quot;The Years We Wasted Caring about
  Shoulder-Surfing&quot;</title><categories>cs.CR cs.HC</categories><comments>Proceedings of the BCS HCI 2012</comments><doi>10.1145/2377916.2377975</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passwords are a good idea, in theory. They have the potential to act as a
fairly strong gateway. In practice though, passwords are plagued with problems.
They are (1) easily shared, (2) trivial to observe and (3) maddeningly elusive
when forgotten. While alternatives to passwords have been proposed, none, as
yet, have been adopted widely. There seems to be a reluctance to switch from
tried and tested passwords to novel alternatives, even if the most glaring
flaws of passwords can be mitigated. One argument is that there is not enough
investigation into the feasibility of many password alternatives. Graphical
authentication mechanisms are a case in point. Therefore, in this paper, we
detail the design of two prototype applications that utilise graphical
authentication mechanisms. However, when forced to consider the design of such
prototypes, we find that pertinent password problems eg. observation of entry,
are just that: password problems. We conclude that effective, alternative
authentication mechanisms should target authentication scenarios rather than
the well-known problems of passwords. This is the only route to wide-spread
adoption of alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05631</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05631</id><created>2015-08-23</created><updated>2015-08-27</updated><authors><author><keyname>Reem</keyname><forenames>Daniel</forenames></author><author><keyname>De Pierro</keyname><forenames>Alvaro</forenames></author></authors><title>A new convergence analysis and perturbation resilience of some
  accelerated proximal forward-backward algorithms with errors</title><categories>math.OC cs.NA physics.med-ph</categories><comments>27 pages, very slight non-mathematical modifications (mainly
  additional references and thanks)</comments><msc-class>90C25, 90C31, 49K40, 49M27, 90C59</msc-class><acm-class>G.1.6; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in science and engineering involve, as part of their solution
process, the consideration of a separable function which is the sum of two
convex functions, one of them possibly non-smooth. Recently a few works have
discussed inexact versions of several accelerated proximal methods aiming at
solving this minimization problem. This paper shows that inexact versions of a
method of Beck and Teboulle (FISTA) preserve, in a Hilbert space setting, the
same (non-asymptotic) rate of convergence under some assumptions on the decay
rate of the error terms. The notion of inexactness discussed here seems to be
rather simple, but, interestingly, when comparing to related works, similar
decay rates of the errors terms yield similar convergence rates. The derivation
sheds some light on the somewhat mysterious origin of some parameters which
appear in various accelerated methods. A consequence of the analysis is that
the accelerated method is perturbation resilient, making it suitable, in
principle, for the superiorization methodology. Taking this into account, we
also re-examine the superiorization methodology and significantly extend its
scope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05648</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05648</id><created>2015-08-23</created><authors><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author></authors><title>FreeNet: Spectrum and Energy Harvesting Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dramatic mobile data traffic growth is not only resulting in the spectrum
crunch but is also leading to exorbitant energy consumption. It is thus
desirable to liberate mobile and wireless networks from the constraint of the
spectrum scarcity and to rein in the growing energy consumption. This article
introduces FreeNet, figuratively synonymous to &quot;Free Network&quot;, which engineers
the spectrum and energy harvesting techniques to alleviate the spectrum and
energy constraints by sensing and harvesting spare spectrum for data
communications and utilizing renewable energy as power supplies, respectively.
Hence, FreeNet increases the spectrum and energy efficiency of wireless
networks and enhances the network availability. As a result, FreeNet can be
deployed to alleviate network congestion in urban areas, provision broadband
services in rural areas, and upgrade emergency communication capacity. This
article provides a brief analysis of the design of FreeNet that accommodates
the dynamics of the spare spectrum and employs renewable energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05657</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05657</id><created>2015-08-23</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>A Sard theorem for graph theory</title><categories>cs.DM cs.CG math.GT</categories><comments>30 pages, 8 figures</comments><msc-class>05C50, 57M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The zero locus of a function f on a graph G is defined as the graph with
vertex set consisting of all complete subgraphs of G, on which f changes sign
and where x,y are connected if one is contained in the other. For d-graphs,
finite simple graphs for which every unit sphere is a d-sphere, the zero locus
of (f-c) is a (d-1)-graph for all c different from the range of f. If this Sard
lemma is inductively applied to an ordered list functions f_1,...,f_k in which
the functions are extended on the level surfaces, the set of critical values
(c_1,...,c_k) for which F-c=0 is not a (d-k)-graph is a finite set. This
discrete Sard result allows to construct explicit graphs triangulating a given
algebraic set. We also look at a second setup: for a function F from the vertex
set to R^k, we give conditions for which the simultaneous discrete algebraic
set { F=c } defined as the set of simplices of dimension in {k, k+1,...,n} on
which all f_i change sign, is a (d-k)-graph in the barycentric refinement of G.
This maximal rank condition is adapted from the continuum and the graph { F=c }
is a (n-k)-graph. While now, the critical values can have positive measure, we
are closer to calculus: for k=2 for example, extrema of functions f under a
constraint {g=c} happen at points, where the gradients of f and g are parallel
D f = L D g, the Lagrange equations on the discrete network. As for an
application, we illustrate eigenfunctions of geometric graphs and especially
the second eigenvector of 3-spheres, which by Courant-Fiedler has exactly two
nodal regions. The separating nodal surface of the second eigenfunction f_2
belonging to the smallest nonzero eigenvalue always appears to be a 2-sphere in
experiments if G is a 3-sphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05672</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05672</id><created>2015-08-23</created><authors><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Fan</keyname><forenames>Cuiling</forenames></author></authors><title>New infinite families of p-ary weakly regular bent functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characterization and construction of bent functions are challenging
problems. The paper generalizes the constructions of Boolean bent functions by
Mesnager \cite{M2014}, Xu et al. \cite{XCX2015} and $p$-ary bent functions by
Xu et al. \cite{XC2015} to the construction of $p$-ary weakly regular bent
functions and presents new infinite families of $p$-ary weakly regular bent
functions from some known weakly regular bent functions (square functions,
Kasami functions, and the Maiorana-McFarland class of bent functions). Further,
new infinite families of $p$-ary bent idempotents are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05673</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05673</id><created>2015-08-23</created><authors><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Fan</keyname><forenames>Cuiling</forenames></author></authors><title>Constructing bent functions and bent idempotents of any possible
  algebraic degrees</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bent functions as optimal combinatorial objects are difficult to characterize
and construct. In the literature, bent idempotents are a special class of bent
functions and few constructions have been presented, which are restricted by
the degree of finite fields and have algebraic degree no more than 4. In this
paper, several new infinite families of bent functions are obtained by adding
the the algebraic combination of linear functions to some known bent functions
and their duals are calculated. These bent functions contain some previous work
on infinite families of bent functions by Mesnager \cite{M2014} and Xu et al.
\cite{XCX2015}. Further, infinite families of bent idempotents of any possible
algebraic degree are constructed from any quadratic bent idempotent. To our
knowledge, it is the first univariate representation construction of infinite
families of bent idempotents over $\mathbb{F}_{2^{2m}}$ of algebraic degree
between 2 and $m$, which solves the open problem on bent idempotents proposed
by Carlet \cite{C2014}. And an infinite family of anti-self-dual bent functions
are obtained. The sum of three anti-self-dual bent functions in such a family
is also anti-self-dual bent and belongs to this family. This solves the open
problem proposed by Mesnager \cite{M2014}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05674</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05674</id><created>2015-08-23</created><updated>2015-09-01</updated><authors><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Fan</keyname><forenames>Cuiling</forenames></author></authors><title>Two infinite classes of rotation symmetric bent functions with simple
  representation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature, few $n$-variable rotation symmetric bent functions have
been constructed. In this paper, we present two infinite classes of rotation
symmetric bent functions on $\mathbb{F}_2^{n}$ of the two forms:
  {\rm (i)} $f(x)=\sum_{i=0}^{m-1}x_ix_{i+m} + \gamma(x_0+x_m,\cdots,
x_{m-1}+x_{2m-1})$,
  {\rm (ii)} $f_t(x)= \sum_{i=0}^{n-1}(x_ix_{i+t}x_{i+m} +x_{i}x_{i+t})+
\sum_{i=0}^{m-1}x_ix_{i+m}+ \gamma(x_0+x_m,\cdots, x_{m-1}+x_{2m-1})$,
  \noindent where $n=2m$, $\gamma(X_0,X_1,\cdots, X_{m-1})$ is any rotation
symmetric polynomial, and $m/gcd(m,t)$ is odd. The class (i) of rotation
symmetric bent functions has algebraic degree ranging from 2 to $m$ and the
other class (ii) has algebraic degree ranging from 3 to $m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05683</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05683</id><created>2015-08-23</created><authors><author><keyname>Liu</keyname><forenames>Siqi</forenames></author><author><keyname>Liu</keyname><forenames>Sidong</forenames></author><author><keyname>Pujol</keyname><forenames>Sonia</forenames></author><author><keyname>Kikinis</keyname><forenames>Ron</forenames></author><author><keyname>Feng</keyname><forenames>Dagan</forenames></author><author><keyname>Fulham</keyname><forenames>Michael</forenames></author><author><keyname>Cai</keyname><forenames>Weidong</forenames></author></authors><title>Morphometry-Based Longitudinal Neurodegeneration Simulation with MR
  Imaging</title><categories>cs.CV</categories><comments>6 pages, 3 figures, preprint for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a longitudinal MR simulation framework which simulates the future
neurodegenerative progression by outputting the predicted follow-up MR image
and the voxel-based morphometry (VBM) map. This framework expects the patients
to have at least 2 historical MR images available. The longitudinal and
cross-sectional VBM maps are extracted to measure the affinity between the
target subject and the template subjects collected for simulation. Then the
follow-up simulation is performed by resampling the latest available target MR
image with a weighted sum of non-linear transformations derived from the
best-matched templates. The leave-one-out strategy was used to compare
different simulation methods. Compared to the state-of-the-art voxel-based
method, our proposed morphometry-based simulation achieves better accuracy in
most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05694</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05694</id><created>2015-08-24</created><authors><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Torrieri</keyname><forenames>Don</forenames></author></authors><title>Optimization of an Adaptive Frequency-Hopping Network</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 4 figures, 1 table, IEEE Military Commun. Conf. (MILCOM),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a methodology for optimizing a frequency-hopping network
that uses continuous-phase frequency-shift keying and adaptive
capacity-approaching channel coding. The optimization takes into account the
spatial distribution of the interfering mobiles, Nakagami fading, and lognormal
shadowing. It includes the effects of both co-channel interference and
adjacent-channel interference, which arises due to spectral-splatter effects.
The average network performance depends on the choice of the modulation index,
the number of frequency-hopping channels, and the fractional in-band power,
which are assumed to be fixed network parameters. The performance of a given
transmission depends on the code rate, which is adapted in response to the
interference to meet a constraint on outage probability. The optimization
proceeds by choosing a set of fixed network parameters, drawing the interferers
from the spatial distribution, and determining the maximum rate that satisfies
the outage constraint. The process is repeated for a large number of network
realizations, and the fixed network parameters that maximize the area spectral
efficiency are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05699</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05699</id><created>2015-08-24</created><updated>2015-09-08</updated><authors><author><keyname>Northcutt</keyname><forenames>Curtis G.</forenames></author><author><keyname>Ho</keyname><forenames>Andrew D.</forenames></author><author><keyname>Chuang</keyname><forenames>Isaac L.</forenames></author></authors><title>Detecting and Preventing &quot;Multiple-Account&quot; Cheating in Massive Open
  Online Courses</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a cheating strategy enabled by the features of massive open
online courses (MOOCs) and detectable by virtue of the sophisticated data
systems that MOOCs provide. The strategy, Copying Answers using Multiple
Existences Online (CAMEO), involves a user who gathers solutions to assessment
questions using a &quot;harvester&quot; account and then submits correct answers using a
separate &quot;master&quot; account. We use &quot;clickstream&quot; learner data to detect CAMEO
use among 1.9 million course participants in 115 MOOCs from two universities.
Using conservative thresholds, we estimate CAMEO prevalence at 1,237
certificates, accounting for 1.3% of the certificates in the 69 MOOCs with
CAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO
strategy. CAMEO users are more likely to be young, male, and international than
other MOOC certificate earners. We identify preventive strategies that can
decrease CAMEO rates and show evidence of their effectiveness in science
courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05703</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05703</id><created>2015-08-24</created><updated>2015-12-02</updated><authors><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author><author><keyname>Bhushan</keyname><forenames>Indra</forenames></author></authors><title>Impact of CFO Estimation on the Performance of ZF Receiver in Massive
  MU-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Vehicular Technology (TVT) as a
  correspondence paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the impact of carrier frequency offset (CFO)
estimation/compensation on the information rate performance of the zero-forcing
(ZF) receiver in the uplink of a multi-user massive multiple-input
multiple-output (MIMO) system. Analysis of the derived closed-form expression
of the per-user information rate reveals that with increasing number of BS
antennas $M$, an $\mathcal{O}(\sqrt{M})$ array gain is achievable, which is
same as that achieved in the ideal zero CFO scenario. Also it is observed that
compared to the ideal zero CFO case, the performance degradation in the
presence of residual CFO (after CFO compensation) is the same for both ZF and
MRC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05704</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05704</id><created>2015-08-24</created><authors><author><keyname>Qadar</keyname><forenames>Muhammad Ali</forenames></author><author><keyname>Zhaowen</keyname><forenames>Yan</forenames></author><author><keyname>Hua</keyname><forenames>Li</forenames></author></authors><title>Iterative Thresholded Bi-Histogram Equalization for Medical Image
  Enhancement</title><categories>cs.CV</categories><comments>8 Pages, 8 Figures, International Journal of Computer Applications
  (IJCA)</comments><journal-ref>International Journal of Computer Applications (0975 8887) Volume
  114 No. 8, March 2015</journal-ref><doi>10.5120/19999-1753</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enhancement of human vision to get an insight to information content is of
vital importance. The traditional histogram equalization methods have been
suffering from amplified contrast with the addition of artifacts and a
surprising unnatural visibility of the processed images. In order to overcome
these drawbacks, this paper proposes interative, mean, and multi-threshold
selection criterion with plateau limits, which consist of histogram
segmentation, clipping and transformation modules. The histogram partition
consists of multiple thresholding processes that divide the histogram into two
parts, whereas the clipping process nicely enhances the contrast by having a
check on the rate of enhancement that could be tuned. Histogram equalization to
each segmented sub-histogram provides the output image with preserved
brightness and enhanced contrast. Results of the present study showed that the
proposed method efficiently handles the noise amplification. Further, it also
preserves the brightness by retaining natural look of targeted image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05710</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05710</id><created>2015-08-24</created><authors><author><keyname>Zhuang</keyname><forenames>Zixuan</forenames></author></authors><title>An Experimental Study of Distributed Quantile Estimation</title><categories>cs.DB</categories><comments>M.S. Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantiles are very important statistics information used to describe the
distribution of datasets. Given the quantiles of a dataset, we can easily know
the distribution of the dataset, which is a fundamental problem in data
analysis. However, quite often, computing quantiles directly is inappropriate
due to the memory limitations. Further, in many settings such as data streaming
and sensor network model, even the data size is unpredictable. Although the
quantiles computation has been widely studied, it was mostly in the sequential
setting. In this paper, we study several quantile computation algorithms in the
distributed setting and compare them in terms of space usage, running time, and
accuracy. Moreover, we provide detailed experimental comparisons between
several popular algorithms. Our work focuses on the approximate quantile
algorithms which provide error bounds. Approximate quantiles have received more
attentions than exact ones since they are often faster, can be more easily
adapted to the distributed setting while giving sufficiently good statistical
information on the data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05711</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05711</id><created>2015-08-24</created><authors><author><keyname>Zhao</keyname><forenames>Shen-Yi</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author></authors><title>Fast Asynchronous Parallel Stochastic Gradient Decent</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent~(SGD) and its variants have become more and more
popular in machine learning due to their efficiency and effectiveness. To
handle large-scale problems, researchers have recently proposed several
parallel SGD methods for multicore systems. However, existing parallel SGD
methods cannot achieve satisfactory performance in real applications. In this
paper, we propose a fast asynchronous parallel SGD method, called AsySVRG, by
designing an asynchronous strategy to parallelize the recently proposed SGD
variant called stochastic variance reduced gradient~(SVRG). Both theoretical
and empirical results show that AsySVRG can outperform existing
state-of-the-art parallel SGD methods like Hogwild! in terms of convergence
rate and computation cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05723</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05723</id><created>2015-08-24</created><updated>2015-08-26</updated><authors><author><keyname>Huang</keyname><forenames>Shan</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Haijian</forenames></author></authors><title>Joint Frequency Estimation with Two Sub-Nyquist Sampling Sequences</title><categories>cs.IT math.IT</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications of frequency estimation, the frequencies of the signals
are so high that the data sampled at Nyquist rate are hard to acquire due to
hardware limitation. In this paper, we propose a novel method based on subspace
techniques to estimate the frequencies by using two sub-Nyquist sample
sequences, provided that the two under-sampled ratios are relatively prime
integers. We analyze the impact of under-sampling and expand the estimated
frequencies which suffer from aliasing. Through jointing the results estimated
from these two sequences, the frequencies approximate to the frequency
components really contained in the signals are screened. The method requires a
small quantity of hardware and calculation. Numerical results show that this
method is valid and accurate at quite low sampling rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05726</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05726</id><created>2015-08-24</created><authors><author><keyname>Huleihel</keyname><forenames>Wasim</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Codewords With Memory Improve Achievable Rate Regions of the Memoryless
  Gaussian Interference Channel</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-user Gaussian interference channel (GIC) has been extensively studied
in the literature during the last four decades. The full characterization of
the capacity region of the GIC is a long-standing open problem, except the case
of strong or very strong interference. For general GIC's, many inner bounds
have been provided over the years, among of them, the Han-Kobayashi (HK)
region, is the most celebrated one. Unfortunately, the calculation of the HK
region is prohibitively complex, due to the appearance of some auxiliary random
variables, whose optimal choice is an open problem. As in other multi-user
communication systems, these achievable regions are based on ensembles of
i.i.d. (memoryless) codewords, in the sense that the symbols within each
codeword are drawn independently. In this paper, we show that for the GIC, it
is worthwhile to employ random coding ensembles of codewords with memory.
Specifically, we take known achievable regions for the GIC, and
generalize/improve them by allowing dependency between the code symbols. For
example, we improve the state-of-the-art HK region by drawing the codewords (of
each codeword and for each user) from a first-order autoregressive moving
average (ARMA) Gaussian process. In this way, we suggest several new achievable
rate regions, which are easily calculable, and which are strictly better than
state-of-the-art known achievable regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05736</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05736</id><created>2015-08-24</created><authors><author><keyname>Afriansyah</keyname><forenames>Aidil</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Andryani</keyname><forenames>Ria</forenames></author></authors><title>E-Monitoring Program Pembangunan Infrastruktur Perdesaan (PPIP) pada
  Dinas PU Cipta Karya dan Pengairan Kabupaten Muba</title><categories>cs.CY</categories><comments>6 pages. Student Colloquium Sistem Informasi &amp; Teknik Informatika
  (SC-SITI) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information technology is widely used as a media monitor various activities.
In this study, the authors will utilize IT to monitor the Rural Infrastructure
Development Program (PPIP). PPIP is a program of community development at the
village level in the framework of the provision of basic infrastructure in
rural settlements carried out by the Directorate General of Human Settlements
Ministry of Public Works to support the policy of the Indonesian government.
PPIP Kab. Muba through the Department of Public Works and Human Settlement
Irrigation District Muba as actors in the process of distribution of program
development implementation, disbursement, monitoring (monitoring), and
reporting. In the implementation process of the realization of all the
monitoring data is processed in a conventional manner or format of diverse
reports from the field so often goes wrong, the late submission of reports and
inaccuracies among reports received by the condition of the field. The system
can manage employment targets, reporting a physical realization and financial
absorption, process data reporting information on a regular basis, timely,
complete and factual as the data obtained directly from implementing
supervisory officers in the field. This system is built with web-based
information technology using PHP and MySQL database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05737</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05737</id><created>2015-08-24</created><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Frank</keyname><forenames>Michael</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>When Six Gates are Not Enough</title><categories>cs.CC</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the pigeonhole principle to show that there must exist Boolean
functions on 7 inputs with a multiplicative complexity of at least 7, i.e.,
that cannot be computed with only 6 multiplications in the Galois field with
two elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05752</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05752</id><created>2015-08-24</created><authors><author><keyname>Bo&#x142;t</keyname><forenames>Witold</forenames></author><author><keyname>Baetens</keyname><forenames>Jan M.</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author></authors><title>An evolutionary approach to the identification of Cellular Automata
  based on partial observations</title><categories>cs.NE nlin.CG</categories><comments>IEEE CEC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the identification problem of Cellular Automata
(CAs). The problem is defined and solved in the context of partial observations
with time gaps of unknown length, i.e. pre-recorded, partial configurations of
the system at certain, unknown time steps. A solution method based on a
modified variant of a Genetic Algorithm (GA) is proposed and illustrated with
brief experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05753</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05753</id><created>2015-08-24</created><updated>2015-09-18</updated><authors><author><keyname>Bioglio</keyname><forenames>Valerio</forenames></author><author><keyname>Gabry</keyname><forenames>Frederic</forenames></author><author><keyname>Land</keyname><forenames>Ingmar</forenames></author></authors><title>Optimizing MDS Codes for Caching at the Edge</title><categories>cs.IT math.IT</categories><comments>to appear in Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of optimal MDS-encoded cache
placement at the wireless edge to minimize the backhaul rate in heterogeneous
networks. We derive the backhaul rate performance of any caching scheme based
on file splitting and MDS encoding and we formulate the optimal caching scheme
as a convex optimization problem. We then thoroughly investigate the
performance of this optimal scheme for an important heterogeneous network
scenario. We compare it to several other caching strategies and we analyze the
influence of the system parameters, such as the popularity and size of the
library files and the capabilities of the small-cell base stations, on the
overall performance of our optimal caching strategy. Our results show that the
careful placement of MDS-encoded content in caches at the wireless edge leads
to a significant decrease of the load of the network backhaul and hence to a
considerable performance enhancement of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05764</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05764</id><created>2015-08-24</created><updated>2015-10-23</updated><authors><author><keyname>Beguerisse-D&#xed;az</keyname><forenames>Mariano</forenames></author><author><keyname>McLennan</keyname><forenames>Amy K.</forenames></author><author><keyname>Gardu&#xf1;o-Hern&#xe1;ndez</keyname><forenames>Guillermo</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author><author><keyname>Ulijaszek</keyname><forenames>Stanley J.</forenames></author></authors><title>The 'who' and 'what' of #diabetes on Twitter</title><categories>physics.soc-ph cs.CY cs.IR cs.SI</categories><comments>23 pages, 10 figures, 7 tables. Supplemental spreadsheet available
  from http://wwwf.imperial.ac.uk/~mbegueri/Docs/Diabetes.xls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media are being increasingly used for health promotion. Yet the
landscape of users and messages in such public fora is not well understood. So
far, studies have typically focused either on people suffering from a disease,
or on agencies that address it, but have not looked more broadly at all the
participants in the debate and discussions. We study the conversation about
diabetes on Twitter through the systematic analysis of a large collection of
tweets containing the term 'diabetes', as well as the interactions between
their authors. We address three questions: (1) what themes arise in these
messages?; (2) who talks about diabetes and in what capacity?; and (3) which
type of users contribute to which themes? To answer these questions, we employ
a mixed-methods approach, using techniques from anthropology, network science
and information retrieval. We find that diabetes-related tweets fall within
broad thematic groups: health information, news, social interaction, and
commercial. Humorous messages and messages with references to popular culture
appear constantly over time, more than any other type of tweet in this corpus.
Top 'authorities' are found consistently across time and comprise bloggers,
advocacy groups and NGOs related to diabetes, as well as stockmarket-listed
companies with no specific diabetes expertise. These authorities fall into
seven interest communities in their Twitter follower network. In contrast, the
landscape of 'hubs' is diffuse and fluid over time. We discuss the implications
of our findings for public health professionals and policy makers. Our methods
are generally applicable to investigations where similar data are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05766</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05766</id><created>2015-08-24</created><authors><author><keyname>Kazda</keyname><forenames>Alexandr</forenames></author></authors><title>$n$-permutability and linear Datalog implies symmetric Datalog</title><categories>cs.CC</categories><msc-class>68Q17, 68R05, 03C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if $\mathbb A$ is a core relational structure such that
$CSP(\mathbb{A})$ can be solved by a linear Datalog program, and $\mathbb A$ is
$n$-permutable for some $n$, then $CSP(\mathbb A)$ can be solved by a symmetric
Datalog program (and thus $CSP(\mathbb{A})$ lies in deterministic logspace). At
the moment, it is not known for which structures $\mathbb A$ will
$CSP(\mathbb{A})$ be solvable by a linear Datalog program. However, once
somebody obtains a characterization of linear Datalog, our result immediately
gives a characterization of symmetric Datalog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05769</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05769</id><created>2015-08-24</created><authors><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author><author><keyname>Pareja</keyname><forenames>Daniel</forenames></author></authors><title>Multi-round Master-Worker Computing: a Repeated Game Approach</title><categories>cs.DC cs.GT</categories><comments>21 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a computing system where a master processor assigns tasks for
execution to worker processors through the Internet. We model the workers
decision of whether to comply (compute the task) or not (return a bogus result
to save the computation cost) as a mixed extension of a strategic game among
workers. That is, we assume that workers are rational in a game-theoretic
sense, and that they randomize their strategic choice. Workers are assigned
multiple tasks in subsequent rounds. We model the system as an infinitely
repeated game of the mixed extension of the strategic game. In each round, the
master decides stochastically whether to accept the answer of the majority or
verify the answers received, at some cost. Incentives and/or penalties are
applied to workers accordingly. Under the above framework, we study the
conditions in which the master can reliably obtain tasks results, exploiting
that the repeated games model captures the effect of long-term interaction.
That is, workers take into account that their behavior in one computation will
have an effect on the behavior of other workers in the future. Indeed, should a
worker be found to deviate from some agreed strategic choice, the remaining
workers would change their own strategy to penalize the deviator. Hence, being
rational, workers do not deviate. We identify analytically the parameter
conditions to induce a desired worker behavior, and we evaluate experi-
mentally the mechanisms derived from such conditions. We also compare the
performance of our mechanisms with a previously known multi-round mechanism
based on reinforcement learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05774</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05774</id><created>2015-08-24</created><authors><author><keyname>Terekhov</keyname><forenames>I. S.</forenames></author><author><keyname>Reznichenko</keyname><forenames>A. V.</forenames></author><author><keyname>Kharkov</keyname><forenames>Ya. A.</forenames></author><author><keyname>Turitsyn</keyname><forenames>S. K.</forenames></author></authors><title>Optimal input signal distribution and per-sample mutual information for
  nondispersive nonlinear optical fiber channel in large SNR limit</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a model nondispersive nonlinear optical fiber channel the analytical
expressions for the conditional probability, conditional entropy, output signal
entropy, and per-sample mutual information in the large $\mathrm{SNR}$
(signal-to-noise ratio) limit were derived. The conditional probability was
calculated using Feynman path-integral technique in the leading and
next-to-leading order in $1/\mathrm{SNR}$. In the leading order in
$1/\mathrm{SNR}$ the input signal distribution maximizing the mutual
information in the intermediate power region was found. In this region the
mutual information for optimal distribution was calculated in large
$\mathrm{SNR}$ limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05776</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05776</id><created>2015-08-24</created><authors><author><keyname>Sahin</keyname><forenames>Alphan</forenames></author><author><keyname>Eroglu</keyname><forenames>Yusuf Said</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Pala</keyname><forenames>Nezih</forenames></author><author><keyname>Yuksel</keyname><forenames>Murat</forenames></author></authors><title>Hybrid 3D Localization for Visible Light Communication Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE/OSA Journal of Lightwave Technology (10 pages, 14
  figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we investigate hybrid utilization of angle-of-arrival (AOA)
and received signal strength (RSS) information in visible light communication
(VLC) systems for 3D localization. We show that AOA-based localization method
allows the receiver to locate itself via a least squares estimator by
exploiting the directionality of light-emitting diodes (LEDs). We then prove
that when the RSS information is taken into account, the positioning accuracy
of AOA-based localization can be improved further using a weighted least
squares solution. On the other hand, when the radiation patterns of LEDs are
explicitly considered in the estimation, RSS-based localization yields highly
accurate results. In order to deal with the system of nonlinear equations for
RSS-based localization, we develop an analytical learning rule based on the
Newton-Raphson method. The non-convex structure is addressed by initializing
the learning rule based on 1) location estimates, and 2) a newly developed
method, which we refer as random report and cluster algorithm. As a benchmark,
we also derive analytical expression of the Cramer-Rao lower bound (CRLB) for
RSS-based localization, which captures any deployment scenario positioning in
3D geometry. Finally, we demonstrate the effectiveness of the proposed
solutions for a wide range of LED characteristics and orientations through
extensive computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05782</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05782</id><created>2015-08-24</created><authors><author><keyname>Vasser</keyname><forenames>Madis</forenames></author><author><keyname>K&#xe4;ngsepp</keyname><forenames>Markus</forenames></author><author><keyname>Aru</keyname><forenames>Jaan</forenames></author></authors><title>Change Blindness in 3D Virtual Reality</title><categories>q-bio.NC cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present change blindness study subjects explored stereoscopic three
dimensional (3D) environments through a virtual reality (VR) headset. A novel
method that tracked the subjects' head movements was used for inducing changes
in the scene whenever the changing object was out of the field of view. The
effect of change location (foreground or background in 3D depth) on change
blindness was investigated. Two experiments were conducted, one in the lab (n =
50) and the other online (n = 25). Up to 25% of the changes were undetected and
the mean overall search time was 27 seconds in the lab study. Results indicated
significantly lower change detection success and more change cycles if the
changes occurred in the background, with no differences in overall search
times. The results confirm findings from previous studies and extend them to 3D
environments. The study also demonstrates the feasibility of online VR
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05784</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05784</id><created>2015-08-24</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Bruckdorfer</keyname><forenames>Till</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author></authors><title>A Universal Point Set for 2-Outerplanar Graphs</title><categories>cs.CG</categories><comments>23 pages, 11 figures, conference version at GD 2015</comments><msc-class>05C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A point set $S \subseteq \mathbb{R}^2$ is universal for a class $\cal G$ if
every graph of ${\cal G}$ has a planar straight-line embedding on $S$. It is
well-known that the integer grid is a quadratic-size universal point set for
planar graphs, while the existence of a sub-quadratic universal point set for
them is one of the most fascinating open problems in Graph Drawing. Motivated
by the fact that outerplanarity is a key property for the existence of small
universal point sets, we study 2-outerplanar graphs and provide for them a
universal point set of size $O(n \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05786</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05786</id><created>2015-08-24</created><authors><author><keyname>Kapelko</keyname><forenames>Rafal</forenames></author><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames></author></authors><title>On the Displacement for Covering a $d-$dimensional Cube with Randomly
  Placed Sensors</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $n$ sensors placed randomly and independently with the uniform
distribution in a $d-$dimensional unit cube ($d\ge 2$). The sensors have
identical sensing range equal to $r$, for some $r &gt;0$. We are interested in
moving the sensors from their initial positions to new positions so as to
ensure that the $d-$dimensional unit cube is completely covered, i.e., every
point in the $d-$dimensional cube is within the range of a sensor. If the
$i$-th sensor is displaced a distance $d_i$, what is a displacement of minimum
cost? As cost measure for the displacement of the team of sensors we consider
the $a$-total movement defined as the sum $M_a:= \sum_{i=1}^n d_i^a$, for some
constant $a&gt;0$. We assume that $r$ and $n$ are chosen so as to allow full
coverage of the $d-$dimensional unit cube and $a &gt; 0$.
  The main contribution of the paper is to show the existence of a tradeoff
between the $d-$dimensional cube, sensing radius and $a$-total movement. The
main results can be summarized as follows for the case of the $d-$dimensional
cube.
  If the $d-$dimensional cube sensing radius is $\frac{1}{2n^{1/d}}$ and
$n=m^d$, for some $m\in N$, then we present an algorithm that uses
$O\left(n^{1-\frac{a}{2d}}\right)$ total expected movement (see Algorithm 2 and
Theorem 5).
  If the $d-$dimensional cube sensing radius is greater than
$\frac{3^{3/d}}{(3^{1/d}-1)(3^{1/d}-1)}\frac{1}{2n^{1/d}}$ and $n$ is a natural
number then the total expected movement is
$O\left(n^{1-\frac{a}{2d}}\left(\frac{\ln n}{n}\right)^{\frac{a}{2d}}\right)$
(see Algorithm 3 and Theorem 7).
  In addition, we simulate Algorithm 2 and discuss the results of our
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05788</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05788</id><created>2015-08-24</created><updated>2015-08-25</updated><authors><author><keyname>Landsberg</keyname><forenames>Joseph M.</forenames><affiliation>TAMU</affiliation></author><author><keyname>Ressayre</keyname><forenames>Nicolas</forenames><affiliation>ICJ</affiliation></author></authors><title>Permanent v. determinant: an exponential lower bound assumingsymmetry
  and a potential path towards Valiant's conjecture</title><categories>math.AG cs.CC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a study of determinantal representations with symmetry. We show
that Grenet's determinantal representation for the permanent is optimal among
determinantal representations respecting left multiplication by permutation and
diagonal matrices (roughly half the symmetry group of the permanent). In
particular, if any optimal determinantal representation of the permanent must
be polynomially related to one with such symmetry, then Valiant's conjecture on
permanent v. determinant is true.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05789</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05789</id><created>2015-08-24</created><authors><author><keyname>Fatemi</keyname><forenames>Mitra</forenames></author><author><keyname>Amini</keyname><forenames>Arash</forenames></author><author><keyname>Baboulaz</keyname><forenames>Loic</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Shapes From Pixels</title><categories>cs.IT math.IT</categories><comments>13 pages, 14 figures</comments><doi>10.1109/TIP.2016.2514507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous-domain visual signals are usually captured as discrete (digital)
images. This operation is not invertible in general, in the sense that the
continuous-domain signal cannot be exactly reconstructed based on the discrete
image, unless it satisfies certain constraints (\emph{e.g.}, bandlimitedness).
In this paper, we study the problem of recovering shape images with smooth
boundaries from a set of samples. Thus, the reconstructed image is constrained
to regenerate the same samples (consistency), as well as forming a shape
(bilevel) image. We initially formulate the reconstruction technique by
minimizing the shape perimeter over the set of consistent binary shapes. Next,
we relax the non-convex shape constraint to transform the problem into
minimizing the total variation over consistent non-negative-valued images. We
also introduce a requirement (called reducibility) that guarantees equivalence
between the two problems. We illustrate that the reducibility property
effectively sets a requirement on the minimum sampling density. One can draw
analogy between the reducibility property and the so-called restricted isometry
property (RIP) in compressed sensing which establishes the equivalence of the
$\ell_0$ minimization with the relaxed $\ell_1$ minimization. We also evaluate
the performance of the relaxed alternative in various numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05803</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05803</id><created>2015-08-24</created><authors><author><keyname>Llinares-Lopez</keyname><forenames>Felipe</forenames></author><author><keyname>Papaxanthos</keyname><forenames>Laetitia</forenames></author><author><keyname>Bodenham</keyname><forenames>Dean</forenames></author><author><keyname>Borgwardt</keyname><forenames>Karsten</forenames></author></authors><title>Searching for significant patterns in stratified data</title><categories>stat.ML cs.LG</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant pattern mining, the problem of finding itemsets that are
significantly enriched in one class of objects, is statistically challenging,
as the large space of candidate patterns leads to an enormous multiple testing
problem. Recently, the concept of testability was proposed as one approach to
correct for multiple testing in pattern mining while retaining statistical
power. Still, these strategies based on testability do not allow one to
condition the test of significance on the observed covariates, which severely
limits its utility in biomedical applications. Here we propose a strategy and
an efficient algorithm to perform significant pattern mining in the presence of
categorical covariates with K states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05804</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05804</id><created>2015-08-24</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bernardo</forenames></author><author><keyname>Porto</keyname><forenames>Fabio</forenames></author></authors><title>A note on the complexity of the causal ordering problem</title><categories>cs.AI</categories><comments>17 pages, 4 figures</comments><acm-class>F.2.0; I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a concise report on the complexity of the causal
ordering problem, originally introduced by Simon to reason about causal
relations implicit in mathematical models. We show that Simon's classical
algorithm to infer causal ordering is NP-Hard---an intractability previously
guessed by Nayak but not yet proven. We present then a detailed account based
on Nayak's suggested algorithmic solution (the best available), which is
dominated by computing transitive closure---bounded in time by $O(|\mathcal
S|^3)$, where $\mathcal S(\mathcal E, \mathcal V)$ is the input system
structure composed of a set $\mathcal E$ of equations over a set $\mathcal V$
of variables with density $|\mathcal S|$. We also comment on the potential of
causal ordering for emerging applications in large-scale hypothesis management
and predictive analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05808</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05808</id><created>2015-04-24</created><authors><author><keyname>Loukas</keyname><forenames>Andreas</forenames></author><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Distributed Autoregressive Moving Average Graph Filters</title><categories>cs.SI cs.DC cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><doi>10.1109/LSP.2015.2448655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of autoregressive moving average (ARMA) filters on a
graph and show how they can be implemented in a distributed fashion. Our graph
filter design philosophy is independent of the particular graph, meaning that
the filter coefficients are derived irrespective of the graph. In contrast to
finite-impulse response (FIR) graph filters, ARMA graph filters are robust
against changes in the signal and/or graph. In addition, when time-varying
signals are considered, we prove that the proposed graph filters behave as ARMA
filters in the graph domain and, depending on the implementation, as first or
higher ARMA filters in the time domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05812</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05812</id><created>2015-07-11</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author></authors><title>Euskahaldun: Euskararen Aldeko Martxa Baten Sare Sozialetako Islaren
  Bilketa eta Analisia</title><categories>cs.SI</categories><comments>in Basque</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is motivated by the dearth of research that deals with social media
content created from the Basque Country or written in Basque language. While
social fingerprints during events have been analysed in numerous other
locations and languages, this article aims to fill this gap so as to initiate a
much-needed research area within the Basque scientific community. To this end,
we describe the methodology we followed to collect tweets posted during the
quintessential exhibition race in support of the Basque language. We also
present the results of the analysis of these tweets. Our analysis shows that
the most eventful moments lead to spikes in tweeting activity, producing more
tweets. Furthermore, we emphasize the importance of having an official account
for the event in question, which helps improve the visibility of the event in
the social network as well as the dissemination of information to the Basque
community. Along with the official account, journalists and news organisations
play a crucial role in the diffusion of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05814</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05814</id><created>2015-08-24</created><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Structural Complexity of Multi-Valued Partial Functions Computed by
  Nondeterministic Pushdown Automata</title><categories>cs.FL cs.CC</categories><comments>(Extended Abstract, A4, 10pt, 8 pages) This extended abstract has
  already appeared in the Proceedings of the 15th Italian Conference of
  Theoretical Computer Science (ICTCS 2014), September 17-19, Perugia, Italy,
  CEUR Workshop Proceedings, vol.1231, pp.225-236, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues a systematic and comprehensive study on the structural
properties of CFL functions, which are in general multi-valued partial
functions computed by one-way one-head nondeterministic pushdown automata
equipped with write-only output tapes (or pushdown transducers), where CFL
refers to a relevance to context-free languages. The CFL functions tend to
behave quite differently from their corresponding context-free languages. We
extensively discuss containments, separations, and refinements among various
classes of functions obtained from the CFL functions by applying Boolean
operations, functional composition, many-one relativization, and Turing
relativization. In particular, Turing relativization helps construct a
hierarchy over the class of CFL functions. We also analyze the computational
complexity of optimization functions, which are to find optimal values of CFL
functions, and discuss their relationships to the associated languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05817</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05817</id><created>2015-08-24</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>&#xd6;zbal</keyname><forenames>G&#xf6;zde</forenames></author><author><keyname>Strapparava</keyname><forenames>Carlo</forenames></author></authors><title>Echoes of Persuasion: The Effect of Euphony in Persuasive Communication</title><categories>cs.CL cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the effect of various lexical, syntactic, semantic and stylistic
features have been addressed in persuasive language from a computational point
of view, the persuasive effect of phonetics has received little attention. By
modeling a notion of euphony and analyzing four datasets comprising persuasive
and non-persuasive sentences in different domains (political speeches, movie
quotes, slogans and tweets), we explore the impact of sounds on different forms
of persuasiveness. We conduct a series of analyses and prediction experiments
within and across datasets. Our results highlight the positive role of phonetic
devices on persuasion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05821</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05821</id><created>2015-06-04</created><authors><author><keyname>van der Steen</keyname><forenames>J. M.</forenames></author><author><keyname>van Schijndel</keyname><forenames>A. W. M.</forenames></author></authors><title>The development of a mapping tool for the evaluation of building systems
  for future climate scenarios on European scale</title><categories>cs.CY</categories><comments>21 pages, 24 figures, pre-conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a tool for the mapping of the performance of building
systems on European scale for different (future) time periods. The tool is to
use for users and be applicable for different building systems. Users should
also be able to use a broad range of climate parameters to assess the influence
of climate change on these climatic parameters. Also should the calculation
time be reasonable short. The mapping tool is developed in MATLAB, which can be
used by other users for their own studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05822</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05822</id><created>2015-06-27</created><authors><author><keyname>Hardeep</keyname></author><author><keyname>Singh</keyname><forenames>Parminder</forenames></author></authors><title>Andriod Based Punjabi TTS System</title><categories>cs.CY</categories><comments>5 pages,3 Figures and 1 Table Published with International Journal of
  Computer Science Trends and Technology (IJCST)</comments><journal-ref>International Journal of Computer Science Trends and Technology
  (IJCST) V3(3): Page(233-237) May-Jun 2015. ISSN: 2347-8578</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usage of mobile phones is nearly 3.5 times more than that of personal
computers. Android has the largest share among its counter parts like IOS,
Windows and Symbian Android applications have a very few restrictions on them.
TTS systems on Android are available for many languages but a very few systems
of this type are available for Punjabi language. Our research work had the aim
to develop an application that will be able to produce synthetic Punjabi
speech. The paper examines the methodology used to develop speech synthesis TTS
system for the Punjabi content, which is written in Gurmukhi script. For the
development of this system, we use concatenative speech synthesis method with
phonemes as the basic units of concatenation. Some challenges like application
size, processing time, must be considered, while porting this TTS system to
resource-limited devices like mobile phones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05830</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05830</id><created>2015-08-24</created><authors><author><keyname>Marsden</keyname><forenames>Stuart</forenames></author><author><keyname>Vankka</keyname><forenames>Jouko</forenames></author></authors><title>Tactical Network Modeller Simulation Tool Combined discrete event and
  network back-ends</title><categories>cs.NI</categories><comments>6 pages, 4 Figures, Accepted for MILCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the implementation of a tactical network simulation
tool. The tool is called Tactical Network Modeller (TNM). TNM uses some novel
techniques to simplify the building of the network model using graph theory
constrained by a hierarchical tree which reflects the organisation structure.
TNM allows models to be constructed using an Application Programming Interface
(API) or a node based User Interface (UI). When the model is constructed,
different simulation back-ends can be applied to it. A discrete event
simulation and a network emulation back-end are implemented building on top of
open source tools. TNM is simple to create models for non technical users. The
model can be used to analyse information flows. The same model can be used for
a full network emulation. This allows real software and protocols to be tested
in a realistic simulated environment. The flexibility of the software allows
its use from engineering up to campaign planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05856</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05856</id><created>2015-08-24</created><updated>2015-10-13</updated><authors><author><keyname>Challacombe</keyname><forenames>Matt</forenames></author><author><keyname>Haut</keyname><forenames>Terry</forenames></author><author><keyname>Bock</keyname><forenames>Nicolas</forenames></author></authors><title>A $N$-Body Solver for Square Root Iteration</title><categories>cs.NA math.NA</categories><report-no>LA-UR-15-26304</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the Sparse Approximate Matrix Multiply ($\tt SpAMM$) $n$-body
solver for first order Newton Schulz iteration of the matrix square root and
inverse square root. The solver performs recursive two-sided metric queries on
a modified Cauchy-Schwarz criterion, culling negligible sub-volumes of the
product-tensor for problems with structured decay in the sub-space metric.
These sub-structures are shown to bound the relative error in the matrix-matrix
product, and in favorable cases, to enjoy a reduced computational complexity
governed by dimensionality reduction of the product volume. A main contribution
is demonstration of a new, algebraic locality that develops under contractive
identity iteration, with collapse of the metric-subspace onto the identity's
plane diagonal, resulting in a stronger $\tt SpAMM$ bound. Also, we carry out a
first order {Fr\'{e}chet} analyses for single and dual channel instances of the
square root iteration, and look at bifurcations due to ill-conditioning and a
too aggressive $\tt SpAMM$ approximation. Then, we show that extreme $\tt
SpAMM$ approximation and contractive identity iteration can be achieved for
ill-conditioned systems through regularization, and we demonstrate the
potential for acceleration with a scoping, product representation of the
inverse factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05873</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05873</id><created>2015-08-24</created><authors><author><keyname>Ni</keyname><forenames>Jingen</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Bermudez</keyname><forenames>Jos&#xe9; Carlos M.</forenames></author></authors><title>Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for
  Stationary Gaussian Inputs and Slow Learning</title><categories>cs.NA cs.LG</categories><comments>11 pages, 8 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some system identification problems impose nonnegativity constraints on the
parameters to estimate due to inherent physical characteristics of the unknown
system. The nonnegative least-mean-square (NNLMS) algorithm and its variants
allow to address this problem in an online manner. A nonnegative least mean
fourth (NNLMF) algorithm has been recently proposed to improve the performance
of these algorithms in cases where the measurement noise is not Gaussian. This
paper provides a first theoretical analysis of the stochastic behavior of the
NNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation
results illustrate the accuracy of the proposed analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05879</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05879</id><created>2015-08-24</created><authors><author><keyname>Junior</keyname><forenames>Gilberto P. Silva</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author><author><keyname>Sandri</keyname><forenames>Sandra</forenames></author><author><keyname>Bustince</keyname><forenames>Humberto</forenames></author><author><keyname>Barrenechea</keyname><forenames>Edurne</forenames></author><author><keyname>Marco-Detchart</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Optical images-based edge detection in Synthetic Aperture Radar images</title><categories>cs.CV</categories><comments>Accepted for publication in Knowledge-Based Systems</comments><doi>10.1016/j.knosys.2015.07.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of adapting optical images-based edge detection
techniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.
We modify the gravitational edge detection technique (inspired by the Law of
Universal Gravity) proposed by Lopez-Molina et al, using the non-standard
neighbourhood configuration proposed by Fu et al, to reduce the speckle noise
in polarimetric SAR imagery. We compare the modified and unmodified versions of
the gravitational edge detection technique with the well-established one
proposed by Canny, as well as with a recent multiscale fuzzy-based technique
proposed by Lopez-Molina et Alejandro We also address the issues of aggregation
of gray level images before and after edge detection and of filtering. All
techniques addressed here are applied to a mosaic built using class
distributions obtained from a real scene, as well as to the true PolSAR image;
the mosaic results are assessed using Baddeley's Delta Metric. Our experiments
show that modifying the gravitational edge detection technique with a
non-standard neighbourhood configuration produces better results than the
original technique, as well as the other techniques used for comparison. The
experiments show that adapting edge detection methods from Computational
Intelligence for use in PolSAR imagery is a new field worthy of exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05891</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05891</id><created>2015-08-24</created><authors><author><keyname>Crisman</keyname><forenames>Karl-Dieter</forenames></author><author><keyname>Orrison</keyname><forenames>Michael E.</forenames></author></authors><title>Representation Theory of the Symmetric Group in Voting Theory and Game
  Theory</title><categories>math.RT cs.GT</categories><comments>20 pages</comments><msc-class>91B12, 91A12, 20C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey of some of the ways in which the representation theory
of the symmetric group has been used in voting theory and game theory. In
particular, we use permutation representations that arise from the action of
the symmetric group on tabloids to describe, for example, a surprising
relationship between the Borda count and Kemeny rule in voting. We also explain
a powerful representation-theoretic approach to working with linear symmetric
solution concepts in cooperative game theory. Along the way, we discuss new
research questions that arise within and because of the
representation-theoretic framework we are using.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05896</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05896</id><created>2015-08-24</created><authors><author><keyname>Rashtchi</keyname><forenames>Rozita</forenames></author><author><keyname>Gohary</keyname><forenames>Ramy H.</forenames></author><author><keyname>Yanikomeroglu</keyname><forenames>Halim</forenames></author></authors><title>Generalized Cross-Layer Designs for Generic Half-Duplex Multicarrier
  Wireless Networks with Frequency-Reuse</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transaction on Wireless
  Communications (TWC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, joint designs of data routes and resource allocations are
developed for generic half-duplex multicarrier wireless networks in which each
subcarrier can be reused by multiple links. Two instances are considered. The
first instance pertains to the general case in which each subcarrier can be
timeshared by multiple links, whereas the second instance pertains to a special
case in which time-sharing is not allowed and a subcarrier, once assigned to a
set of links, is used by those links throughout the signalling interval. Novel
frameworks are developed to optimize the joint design of data routes,
subcarrier schedules and power allocations. These design problems are nonconvex
and hence difficult to solve. To circumvent this difficulty, efficient
techniques based on geometric programming are developed to obtain locally
optimal solutions. Numerical results show that the designs developed in both
instances yield performance that is superior to that of their counterparts in
which frequency-reuse is not allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05902</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05902</id><created>2015-08-24</created><authors><author><keyname>Maiya</keyname><forenames>Arun S.</forenames></author></authors><title>A Framework for Comparing Groups of Documents</title><categories>cs.CL cs.SI</categories><comments>6 pages; 2015 Conference on Empirical Methods in Natural Language
  Processing (EMNLP '15)</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework for comparing multiple groups of documents. A
bipartite graph model is proposed where document groups are represented as one
node set and the comparison criteria are represented as the other node set.
Using this model, we present basic algorithms to extract insights into
similarities and differences among the document groups. Finally, we demonstrate
the versatility of our framework through an analysis of NSF funding programs
for basic research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05910</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05910</id><created>2015-08-24</created><authors><author><keyname>Nath</keyname><forenames>P.</forenames></author><author><keyname>Singh</keyname><forenames>D. K.</forenames></author></authors><title>A sum form functional equation on a closed domain and its role in
  information theory</title><categories>cs.IT math.IT</categories><comments>18 pages. Already submitted for review</comments><msc-class>39B52, 39B82</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to finding the general solutions of the functional
equation
  $\sumin \sumjm h(p_iq_j)=\sumin h(p_i)+\sumjm k_j(q_j)+\lambda\sumin
h(p_i)\sumjm k_j(q_j)$
  valid for all complete probability distributions $(p_1,\ldots,p_n)$,
$(q_1,\ldots,q_m)$, $0\le p_i\le 1$, $0\le q_j\le 1$, $i=1,\ldots,n$;
$j=1,\ldots,m$, $\sumin p_i=1$, $\sumjm q_j=1$; $n\ge 3$, $m\ge 3$ fixed
integers; $\lambda\in\RR$, $\lambda\neq 0$ and the mappings $h:I\to\RR$,
$k_j:I\to\RR$, $j=1,\ldots,m$; $I=[0,1]$, $\RR$ denoting the set of all real
numbers. A special case of the above functional equation was treated earlier by
L. Losonczi and Gy. Maksa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05931</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05931</id><created>2015-08-22</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author></authors><title>gScan: Accelerating Graham Scan on the GPU</title><categories>cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1508.05488</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fast implementation of the Graham scan on the GPU. The
proposed algorithm is composed of two stages: (1) two rounds of preprocessing
performed on the GPU and (2) the finalization of finding the convex hull on the
CPU. We first discard the interior points that locate inside a quadrilateral
formed by four extreme points, sort the remaining points according to the
angles, and then divide them into the left and the right regions. For each
region, we perform a second round of filtering using the proposed preprocessing
approach to discard the interior points in further. We finally obtain the
expected convex hull by calculating the convex hull of the remaining points on
the CPU. We directly employ the parallel sorting, reduction, and partitioning
provided by the library Thrust for better efficiency and simplicity.
Experimental results show that our implementation achieves 6x ~ 7x speedups
over the Qhull implementation for 20M points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05935</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05935</id><created>2015-08-23</created><authors><author><keyname>Zhang</keyname><forenames>Shengli</forenames></author><author><keyname>Wu</keyname><forenames>Xiugang</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author></authors><title>STAC: Simultaneous Transmitting and Air Computing in Wireless Data
  Center Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data center network (DCN), wired or wireless, features large amounts of
Many-to-One (M2O) sessions. Each M2O session is currently operated based on
Point-to-Point (P2P) communications and Store-and-Forward (SAF) relays, and is
generally followed by certain further computation at the destination.
%typically a weighted summation of the received digits. Different from this
separate P2P/SAF-based-transmission and computation strategy, this paper
proposes STAC, a novel physical layer scheme that achieves Simultaneous
Transmission and Air Computation in wireless DCNs. In particular, STAC takes
advantage of the superposition nature of electromagnetic (EM) waves, and allows
multiple transmitters to transmit in the same time slot with appropriately
chosen parameters, such that the received superimposed signal can be directly
transformed to the needed summation at the receiver. Exploiting the static
channel environment and compact space in DCN, we propose an enhanced Software
Defined Network (SDN) architecture to enable STAC, where wired connections are
established to provide the wireless transceivers external reference signals.
Theoretical analysis and simulation show that with STAC used, both the
bandwidth and energy efficiencies can be improved severalfold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05968</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05968</id><created>2015-08-24</created><authors><author><keyname>Sundar</keyname><forenames>Kaarthik</forenames></author><author><keyname>Venkatachalam</keyname><forenames>Saravanan</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author></authors><title>Formulations and algorithms for the multiple depot, fuel-constrained,
  multiple vehicle routing problem</title><categories>cs.DS</categories><comments>6 pages, 2 figures, submitted to American Control Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiple depot, multiple vehicle routing problem with fuel
constraints. We are given a set of targets, a set of depots and a set of
homogeneous vehicles, one for each depot. The depots are also allowed to act as
refueling stations. The vehicles are allowed to refuel at any depot, and our
objective is to determine a route for each vehicle with a minimum total cost
such that each target is visited at least once by some vehicle, and the
vehicles never run out fuel as it traverses its route. We refer this problem as
Multiple Depot, Fuel-Constrained, Multiple Vehicle Routing Problem (FCMVRP).
This paper presents four new mixed integer linear programming formulations to
compute an optimal solution for the problem. Extensive computational results
for a large set of instances are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05977</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05977</id><created>2015-08-24</created><authors><author><keyname>Guenda</keyname><forenames>K.</forenames></author><author><keyname>Gulliver</keyname><forenames>T. A.</forenames></author></authors><title>Algebraic Quantum Synchronizable Codes</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this work we construct an infinite family of quantum synchronizable codes
from duadic codes. Further, we show that the quantum synchronizable codes
obtained from repeated roots may be a solution to design good codes since it
gives more flexibility in constructing the codes and keep the errors rate
closely to the same as when using simple roots cyclic codes. We also gives some
useful constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.05995</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.05995</id><created>2015-08-24</created><authors><author><keyname>Ding</keyname><forenames>Jianrui</forenames></author><author><keyname>Xian</keyname><forenames>Min</forenames></author><author><keyname>Cheng</keyname><forenames>H. D.</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Xu</keyname><forenames>Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Yingtao</forenames></author></authors><title>An algorithm for Left Atrial Thrombi detection using Transesophageal
  Echocardiography</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transesophageal echocardiography (TEE) is widely used to detect left atrium
(LA)/left atrial appendage (LAA) thrombi. In this paper, the local binary
pattern variance (LBPV) features are extracted from region of interest (ROI).
And the dynamic features are formed by using the information of its neighbor
frames in the sequence. The sequence is viewed as a bag, and the images in the
sequence are considered as the instances. Multiple-instance learning (MIL)
method is employed to solve the LAA thrombi detection. The experimental results
show that the proposed method can achieve better performance than that by using
other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06010</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06010</id><created>2015-08-24</created><authors><author><keyname>Ahmad</keyname><forenames>Muhammad Zubair</forenames></author><author><keyname>Khan</keyname><forenames>Amir Ali</forenames></author><author><keyname>Mezghani</keyname><forenames>Sihem</forenames></author><author><keyname>Perrin</keyname><forenames>Eric</forenames></author><author><keyname>Mouhoubi</keyname><forenames>Kamel</forenames></author><author><keyname>Bodnar</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Vrabie</keyname><forenames>Valeriu</forenames></author></authors><title>Wavelet subspace decomposition of thermal infrared images for defect
  detection in artworks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monitoring the health of ancient artworks requires adequate prudence because
of the sensitive nature of these materials. Classical techniques for
identifying the development of faults rely on acoustic testing. These
techniques, being invasive, may result in causing permanent damage to the
material, especially if the material is inspected periodically. Non destructive
testing has been carried out for different materials since long. In this
regard, non-invasive systems were developed based on infrared thermometry
principle to identify the faults in artworks. The test artwork is heated and
the thermal response of the different layers is captured with the help of a
thermal infrared camera. However, prolonged heating risks overheating and thus
causing damage to artworks and an alternate approach is to use pseudo-random
binary sequence excitations. The faults in the artwork, though, cannot be
detected on the captured images, especially if their strength is weak. The
weaker faults are either masked by the stronger ones, by the pictorial layer of
the artwork or by the non-uniform heating. This work addresses the detection
and localization of the faults through a wavelet based subspace decomposition
scheme. The proposed scheme, on one hand, allows to remove the background
while, on the other hand, removes the undesired high frequency noise. It is
shown that the detection parameter is proportional to the diameter and the
depth of the fault. A criterion is proposed to select the optimal wavelet basis
along with suitable level selection for wavelet decomposition and
reconstruction. The proposed approach is tested on a laboratory developed test
sample with known fault locations and dimensions as well as real artworks. A
comparison with a previously reported method demonstrates the efficacy of the
proposed approach for fault detection in artworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06011</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06011</id><created>2015-08-24</created><authors><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Performance Analysis of Fifth-Generation Cellular Uplink</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 5 figures, IEEE Military Commun. Conf. (MILCOM), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fifth-generation cellular networks are expected to exhibit at least three
primary physical-layer differences relative to fourth-generation ones:
millimeter-wave propagation, antenna-array directionality, and densification of
base stations. In this paper, the effects of these differences on the
performance of single-carrier frequency-domain multiple-access uplink systems
with frequency hopping are assessed. A new analysis, which is much more
detailed than any other in the existing literature and accommodates actual
base-station topologies, captures the primary features of uplink
communications. Distance-dependent power-law, shadowing, and fading models
based on millimeter-wave measurements are introduced. The beneficial effects of
base-station densification, highly directional sectorization, and frequency
hopping are illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06013</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06013</id><created>2015-08-24</created><authors><author><keyname>Bahmani</keyname><forenames>Zeinab</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Vasiloglou</keyname><forenames>Nikolaos</forenames></author></authors><title>ERBlox: Combining Matching Dependencies with Machine Learning for Entity
  Resolution</title><categories>cs.DB cs.AI cs.LG</categories><comments>To appear in Proc. SUM, 2015</comments><journal-ref>Proc. SUM'15, 2015, Springer LNAI 9310, pp. 399-414</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity resolution (ER), an important and common data cleaning problem, is
about detecting data duplicate representations for the same external entities,
and merging them into single representations. Relatively recently, declarative
rules called matching dependencies (MDs) have been proposed for specifying
similarity conditions under which attribute values in database records are
merged. In this work we show the process and the benefits of integrating three
components of ER: (a) Classifiers for duplicate/non-duplicate record pairs
built using machine learning (ML) techniques, (b) MDs for supporting both the
blocking phase of ML and the merge itself; and (c) The use of the declarative
language LogiQL -an extended form of Datalog supported by the LogicBlox
platform- for data processing, and the specification and enforcement of MDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06018</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06018</id><created>2015-08-24</created><authors><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Feketa</keyname><forenames>Petro</forenames></author><author><keyname>Dashkovskiy</keyname><forenames>Sergey</forenames></author></authors><title>A combination of small-gain and density propagation inequalities for
  stability analysis of networked systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of stability analysis of a large-scale
interconnection of nonlinear systems for which the small-gain condition does
not hold globally is considered. A combination of the small-gain and density
propagation inequalities is employed to prove almost input-to-state stability
of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06019</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06019</id><created>2015-08-24</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>Koivisto</keyname><forenames>Mikko</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author></authors><title>Dense Subset Sum may be the hardest</title><categories>cs.DS cs.CC cs.DM cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Subset Sum problem asks whether a given set of $n$ positive integers
contains a subset of elements that sum up to a given target $t$. It is an
outstanding open question whether the $O^*(2^{n/2})$-time algorithm for Subset
Sum by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting
by a &quot;truly faster&quot;, $O^*(2^{(0.5-\delta)n})$-time algorithm, with some
constant $\delta &gt; 0$. Continuing an earlier work [STACS 2015], we study Subset
Sum parameterized by the maximum bin size $\beta$, defined as the largest
number of subsets of the $n$ input integers that yield the same sum. For every
$\epsilon &gt; 0$ we give a truly faster algorithm for instances with $\beta \leq
2^{(0.5-\epsilon)n}$, as well as instances with $\beta \geq 2^{0.661n}$.
Consequently, we also obtain a characterization in terms of the popular density
parameter $n/\log_2 t$: if all instances of density at least $1.003$ admit a
truly faster algorithm, then so does every instance. This goes against the
current intuition that instances of density 1 are the hardest, and therefore is
a step toward answering the open question in the affirmative. Our results stem
from novel combinations of earlier algorithms for Subset Sum and a study of an
extremal question in additive combinatorics connected to the problem of
Uniquely Decodable Code Pairs in information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06021</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06021</id><created>2015-08-24</created><authors><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Symbol Detection for Frame-Based Faster-than-Nyquist Signaling via
  Sum-of-Absolute-Values Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose a new symbol detection method for
faster-than-Nyquist signaling (FTNS) systems. Based on frame theory, we
formulate a symbol detection problem as a under-determined linear equation on a
finite set. The problem is reformulated as a sum-of-absolute-values (SOAV)
optimization that can be efficiently solved by the fast iterative shrinkage
thresholding algorithm (FISTA). The proximity operator for the convex
optimization is derived analytically. Simulation results are given to show that
the proposed method can successfully detect symbols in faster-than-Nyquist
signaling systems and has lower complexity in terms of computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06025</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06025</id><created>2015-08-25</created><updated>2016-02-01</updated><authors><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Strong data-processing inequalities for channels and Bayesian networks</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data-processing inequality, that is, $I(U;Y) \le I(U;X)$ for a Markov
chain $U \to X \to Y$, has been the method of choice for proving impossibility
(converse) results in information theory and many other disciplines. Various
channel-dependent improvements (called strong data-processing inequalities, or
SDPIs) of this inequality have been proposed both classically and more
recently. In this note we first survey known results relating various notions
of contraction for a single channel. Then we consider the basic extension:
given SDPI for each constituent channel in a Bayesian network, how does one
produce an end-to-end SDPI?
  Our approach is based on the (extract of the) Evans-Schulman method, which is
demonstrated for three different kinds of SDPIs, namely, the usual
Ahslwede-G\'acs type contraction coefficients (mutual information), Dobrushin's
contraction coefficients (total variation), and finally the $F_I$-curve (the
best possible SDPI for a given channel). As an example, we demonstrate how to
obtain SDPI for an $n$-letter memoryless channel with feedback given an SDPI
for $n=1$.
  Finally, we discuss a simple observation on the equivalence of SDPI and
comparison to erasure channel (in the sense of &quot;less noisy&quot; order). This leads
to a simple proof of a curious inequality of Samorodnitsky (2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06033</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06033</id><created>2015-08-25</created><authors><author><keyname>Cui</keyname><forenames>Zhiyong</forenames></author><author><keyname>Long</keyname><forenames>Ying</forenames></author></authors><title>Perspectives on Stability and Mobility of Passenger's Travel Behavior
  through Smart Card Data</title><categories>cs.CY</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing studies have extensively used temporal-spatial data to mining the
mobility patterns of different kinds of travelers. Smart Card Data (SCD)
collected by the Automated Fare Collection (AFC) systems can reflect a general
view of the mobility pattern of the whole bus and metro riders in urban area.
Since the mobility and stability are temporally and spatially dynamic and
therefore difficult to measure, few work focuses on the transition of their
travel pattern between a long time interval. In this paper, an overview of the
relation between stability and regularity of public transit riders based on SCD
of Beijing is presented first. To analyze the temporal travel pattern of urban
residents, travelers are classified into two categories, extreme and
non-extreme travelers. We have two lines for profiling all cardholders,
rule-based approach for extreme and improved density-based clustering method
for non-extreme. Similar clusters are aggregated according their features of
regularity and occasionality. By combining transition matrix of passenger's
temporal travel pattern and socioeconomic data of Beijing in the year of 2010
and 2014, several analyses about resident's temporal mobility and stability are
presented to shed lights on the interdependence between stability and mobility
in the time dimension. The results indicate that passengers's regularity is
hard to predict, extreme travel patterns are more vulnerable and overall
non-extreme travel patterns nearly stay the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06034</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06034</id><created>2015-08-25</created><authors><author><keyname>Ng</keyname><forenames>Jun-Ping</forenames></author><author><keyname>Abrecht</keyname><forenames>Viktoria</forenames></author></authors><title>Better Summarization Evaluation with Word Embeddings for ROUGE</title><categories>cs.CL cs.IR</categories><comments>Pre-print - To appear in proceedings of the Conference on Empirical
  Methods in Natural Language Processing (EMNLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ROUGE is a widely adopted, automatic evaluation measure for text
summarization. While it has been shown to correlate well with human judgements,
it is biased towards surface lexical similarities. This makes it unsuitable for
the evaluation of abstractive summarization, or summaries with substantial
paraphrasing. We study the effectiveness of word embeddings to overcome this
disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps,
word embeddings are used to compute the semantic similarity of the words used
in summaries instead. Our experimental results show that our proposal is able
to achieve better correlations with human judgements when measured with the
Spearman and Kendall rank coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06038</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06038</id><created>2015-08-25</created><authors><author><keyname>Zarai</keyname><forenames>Yoram</forenames></author><author><keyname>Mendel</keyname><forenames>Oz</forenames></author><author><keyname>Margaliot</keyname><forenames>Michael</forenames></author></authors><title>Analyzing Linear Communication Networks using the Ribosome Flow Model</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ribosome Flow Model (RFM) describes the unidirectional movement of
interacting particles along a one-dimensional chain of sites. As a site becomes
fuller, the effective entry rate into this site decreases. The RFM has been
used to model and analyze mRNA translation, a biological process in which
ribosomes (the particles) move along the mRNA molecule (the chain), and decode
the genetic information into proteins.
  Here we propose the RFM as an analytical framework for modeling and analyzing
linear communication networks. In this context, the moving particles are
data-packets, the chain of sites is a one dimensional set of ordered buffers,
and the decreasing entry rate to a fuller buffer represents a kind of
decentralized backpressure flow control. For an RFM with homogeneous link
capacities, we provide closed-form expressions for important network metrics
including the throughput and end-to-end delay. We use these results to analyze
the hop length and the transmission probability (in a contention access mode)
that minimize the end-to-end delay in a multihop linear network, and provide
closed-form expressions for the optimal parameter values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06044</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06044</id><created>2015-08-25</created><authors><author><keyname>Li</keyname><forenames>Hanchuan</forenames></author><author><keyname>Shen</keyname><forenames>Haichen</forenames></author><author><keyname>Xu</keyname><forenames>Shengliang</forenames></author><author><keyname>Zhang</keyname><forenames>Congle</forenames></author></authors><title>Visualizing NLP annotations for Crowdsourcing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualizing NLP annotation is useful for the collection of training data for
the statistical NLP approaches. Existing toolkits either provide limited visual
aid, or introduce comprehensive operators to realize sophisticated linguistic
rules. Workers must be well trained to use them. Their audience thus can hardly
be scaled to large amounts of non-expert crowdsourced workers. In this paper,
we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to
annotate two general categories of NLP problems: clustering and parsing.
Workers can finish the tasks with simplified operators in an interactive
interface, and fix errors conveniently. User studies show our toolkit is very
friendly to NLP non-experts, and allow them to produce high quality labels for
several sophisticated problems. We release our source code and toolkit to spur
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06056</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06056</id><created>2015-08-25</created><authors><author><keyname>Biswas</keyname><forenames>Tanmay</forenames></author><author><keyname>Mandal</keyname><forenames>Sudhindu Bikash</forenames></author><author><keyname>Saha</keyname><forenames>Debasree</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>A Novel Reconfigurable Hardware Design for Speech Enhancement Based on
  Multi-Band Spectral Subtraction Involving Magnitude and Phase Components</title><categories>cs.SD cs.AR</categories><comments>Yet to be published (manuscript)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient reconfigurable hardware design for speech
enhancement based on multi band spectral subtraction algorithm and involving
both magnitude and phase components. Our proposed design is novel as it
estimates environmental noise from speech adaptively utilizing both magnitude
and phase components of the speech spectrum. We performed multi-band spectrum
subtraction by dividing the noisy speech spectrum into different non-uniform
frequency bands having varying signal to noise ratio (SNR) and subtracting the
estimated noise from each of these frequency bands. This results to the
elimination of noise from both high SNR and low SNR signal components for all
the frequency bands. We have coined our proposed speech enhancement technique
as Multi Band Magnitude Phase Spectral Subtraction (MBMPSS). The magnitude and
phase operations are executed concurrently exploiting the parallel logic blocks
of Field Programmable Gate Array (FPGA), thus increasing the throughput of the
system to a great extent. We have implemented our design on Spartan6 Lx45 FPGA
and presented the implementation result in terms of resource utilization and
delay information for the different blocks of our design. To the best of our
best knowledge, this is a new type of hardware design for speech enhancement
application and also a first of its kind implementation on reconfigurable
hardware. We have used benchmark audio data for the evaluation of the proposed
hardware and the experimental results show that our hardware shows a better SNR
value compared to the existing state of the art research works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06069</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06069</id><created>2015-08-25</created><authors><author><keyname>Daniel</keyname><forenames>Muhammad Ilham</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Wardani</keyname><forenames>Kiky Rizky Nova</forenames></author></authors><title>Evaluasi Celah Keamanan Web Server pada LPSE Kota Palembang</title><categories>cs.CR</categories><comments>6 pages, presented at the Student Colloquium Sistem Informasi &amp;
  Teknik Informatika (SC-SITI) 2015, Palembang, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Along the development of information technology systems among the public at
large, also develops information systems to facilitate the public to access and
search for information in the form of a website. Electronic Procurement Service
(LPSE) Palembang is a business unit set up to organize the service system of
government procurement of goods or services electronically. And to allow
companies or providers that want to follow the procurement of goods or
services, LPSE providing a website that can be accessed from anywhere so the
company or provider to follow the procurement of goods or services without
having to come to the office LPSE. In the management of its website, LPSE
Palembang has its own web server so that the need to consider the existing
security system on the web server. Web servers often become the target of
attacks by an attacker. This study is set to test the security system of the
web server to find out if a web server is secure or not of the crime committed
by an attacker. This research involves penetration testing with multiple
applications. The results show some holes and suggestions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06071</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06071</id><created>2015-08-25</created><authors><author><keyname>Hacohen</keyname><forenames>Shlomi</forenames></author><author><keyname>Shoval</keyname><forenames>Shraga</forenames></author><author><keyname>Shvalb</keyname><forenames>Nir</forenames></author></authors><title>A Navigation Function For Uncertain Environment</title><categories>cs.RO</categories><comments>13 pages, 5 figures</comments><msc-class>68T40, 70Q05</msc-class><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel motion planning algorithm for stochastic
scenarios. We extend the concept of a navigation function to such scenarios.
Our main idea is to consider both the Gaussian distribution probabilities of
the players' locations and disc (or star sets) geometry of the objects
operating in the work space. We do so by formulating a probability density
function that encloses both. We use the PDF to de?ne a metric between the
robot, the obstacles and the con?guration space boundary. In order to de?ne the
navigation function we formulate a safe probability value for collision. By
analytically investigating the PDF we ?nd a convenient approximation for a safe
distance in the sense of that metric. We prove that the resulting map is a
navigation function and demonstrate our algorithm for various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06073</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06073</id><created>2015-08-25</created><authors><author><keyname>Kuehne</keyname><forenames>Hilde</forenames></author><author><keyname>Serre</keyname><forenames>Thomas</forenames></author></authors><title>Cooking in the kitchen: A generative approach to the recognition,
  parsing and segmentation of human daily activities</title><categories>cs.CV</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As research on action recognition matures, the focus is shifting away from
categorizing basic task-oriented actions using hand-segmented video datasets to
understanding complex goal-oriented daily human activities in real-world
settings. Temporally structured models would seem obvious to tackle this set of
problems, but so far, cases where these models have outperformed simpler
unstructured bag-of-word types of models are scarce. With the increasing
availability of large human activity datasets, combined with the development of
novel feature coding techniques that yield more compact representations, it is
time to revisit structured generative approaches.
  Here, we describe an end-to-end generative approach from the encoding of
features to the structural modeling of complex human activities by applying
Fisher vectors and temporal models for the analysis of video sequences.
  We systematically evaluate the proposed approach on several available
datasets (ADL, MPIICooking, and Breakfast datasets) using a variety of
performance metrics. Through extensive system evaluations, we demonstrate that
combining compact video representations based on Fisher Vectors with HMM-based
modeling yields very significant gains in accuracy and when properly trained
with sufficient training samples, structured temporal models outperform
unstructured bag-of-word types of models by a large margin on the tested
performance metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06091</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06091</id><created>2015-08-25</created><authors><author><keyname>Dhanjal</keyname><forenames>Charanpal</forenames><affiliation>LTCI</affiliation></author><author><keyname>Gaudel</keyname><forenames>Romaric</forenames><affiliation>SEQUEL</affiliation></author><author><keyname>Clemencon</keyname><forenames>Stephan</forenames><affiliation>LTCI</affiliation></author></authors><title>AUC Optimisation and Collaborative Filtering</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recommendation systems, one is interested in the ranking of the predicted
items as opposed to other losses such as the mean squared error. Although a
variety of ways to evaluate rankings exist in the literature, here we focus on
the Area Under the ROC Curve (AUC) as it widely used and has a strong
theoretical underpinning. In practical recommendation, only items at the top of
the ranked list are presented to the users. With this in mind, we propose a
class of objective functions over matrix factorisations which primarily
represent a smooth surrogate for the real AUC, and in a special case we show
how to prioritise the top of the list. The objectives are differentiable and
optimised through a carefully designed stochastic gradient-descent-based
algorithm which scales linearly with the size of the data. In the special case
of square loss we show how to improve computational complexity by leveraging
previously computed measures. To understand theoretically the underlying matrix
factorisation approaches we study both the consistency of the loss functions
with respect to AUC, and generalisation using Rademacher theory. The resulting
generalisation analysis gives strong motivation for the optimisation under
study. Finally, we provide computation results as to the efficacy of the
proposed method using synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06092</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06092</id><created>2015-08-25</created><authors><author><keyname>Cancelliere</keyname><forenames>R.</forenames></author><author><keyname>Deluca</keyname><forenames>R.</forenames></author><author><keyname>Gai</keyname><forenames>M.</forenames></author><author><keyname>Gallinari</keyname><forenames>P.</forenames></author><author><keyname>Rubini</keyname><forenames>L.</forenames></author></authors><title>An analysis of numerical issues in neural training by pseudoinversion</title><categories>cs.LG cs.NE</categories><comments>11 pages, submitted to: Comp. Appl. Math</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some novel strategies have recently been proposed for single hidden layer
neural network training that set randomly the weights from input to hidden
layer, while weights from hidden to output layer are analytically determined by
pseudoinversion. These techniques are gaining popularity in spite of their
known numerical issues when singular and/or almost singular matrices are
involved. In this paper we discuss a critical use of Singular Value Analysis
for identification of these drawbacks and we propose an original use of
regularisation to determine the output weights, based on the concept of
critical hidden layer size. This approach also allows to limit the training
computational effort. Besides, we introduce a novel technique which relies an
effective determination of input weights to the hidden layer dimension. This
approach is tested for both regression and classification tasks, resulting in a
significant performance improvement with respect to alternative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06093</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06093</id><created>2015-08-25</created><updated>2015-12-17</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Energy Group-Buying with Loading Sharing for Green Cellular Networks</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>This is a longer version of a paper to be appear in IEEE Journal on
  Selected Areas in Communications Special Issue on Energy-Efficient Techniques
  for 5G Wireless Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the emerging hybrid electricity market, mobile network operators (MNOs) of
cellular networks can make day-ahead energy purchase commitments at low prices
and real-time flexible energy purchase at high prices. To minimize electricity
bills, it is essential for MNOs to jointly optimize the day-ahead and real-time
energy purchase based on their time-varying wireless traffic load. In this
paper, we consider two different MNOs coexisting in the same area, and exploit
their collaboration in both energy purchase and wireless load sharing for
energy cost saving. Specifically, we propose a new approach named energy group
buying with load sharing, in which the two MNOs are aggregated as a single
group to make the day-ahead and real-time energy purchase, and their base
stations (BSs) share the wireless traffic to maximally turn lightly-loaded BSs
into sleep mode. When the two MNOs belong to the same entity and aim to
minimize their total energy cost, we use the two-stage stochastic programming
to obtain the optimal day-ahead and real-time energy group buying jointly with
wireless load sharing. When the two MNOs belong to different entities and are
self-interested in minimizing their individual energy costs, we propose a novel
repeated Nash bargaining scheme for them to negotiate and share their energy
costs under energy group buying and load sharing. Our proposed repeated Nash
bargaining scheme is shown to achieve Pareto-optimal and fair energy cost
reductions for both MNOs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06095</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06095</id><created>2015-08-25</created><authors><author><keyname>Cancelliere</keyname><forenames>Rossella</forenames></author><author><keyname>Gai</keyname><forenames>Mario</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author><author><keyname>Rubini</keyname><forenames>Luca</forenames></author></authors><title>OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based
  Neural Training</title><categories>cs.NE cs.LG stat.ML</categories><comments>Published on Neural Networks</comments><doi>10.1016/j.neunet.2015.07.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the training of single hidden layer neural networks
by pseudoinversion, which, in spite of its popularity, is sometimes affected by
numerical instability issues. Regularization is known to be effective in such
cases, so that we introduce, in the framework of Tikhonov regularization, a
matricial reformulation of the problem which allows us to use the condition
number as a diagnostic tool for identification of instability. By imposing
well-conditioning requirements on the relevant matrices, our theoretical
analysis allows the identification of an optimal value for the regularization
parameter from the standpoint of stability. We compare with the value derived
by cross-validation for overfitting control and optimisation of the
generalization performance. We test our method for both regression and
classification tasks. The proposed method is quite effective in terms of
predictivity, often with some improvement on performance with respect to the
reference cases considered. This approach, due to analytical determination of
the regularization parameter, dramatically reduces the computational load
required by many other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06096</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06096</id><created>2015-08-25</created><authors><author><keyname>Downing</keyname><forenames>Nicholas</forenames></author><author><keyname>Feydy</keyname><forenames>Thibaut</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Unsatisfiable Cores and Lower Bounding for Constraint Programming</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint Programming (CP) solvers typically tackle optimization problems by
repeatedly finding solutions to a problem while placing tighter and tighter
bounds on the solution cost. This approach is somewhat naive, especially for
soft-constraint optimization problems in which the soft constraints are mostly
satisfied. Unsatisfiable-core approaches to solving soft constraint problems in
SAT (e.g. MAXSAT) force all soft constraints to be hard initially. When solving
fails they return an unsatisfiable core, as a set of soft constraints that
cannot hold simultaneously. These are reverted to soft and solving continues.
Since lazy clause generation solvers can also return unsatisfiable cores we can
adapt this approach to constraint programming. We adapt the original MAXSAT
unsatisfiable core solving approach to be usable for constraint programming and
define a number of extensions. Experimental results show that our methods are
beneficial on a broad class of CP-optimization benchmarks involving soft
constraints, cardinality or preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06098</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06098</id><created>2015-08-25</created><authors><author><keyname>Nassopoulos</keyname><forenames>Georges</forenames></author><author><keyname>Serrano-Alvarado</keyname><forenames>Patricia</forenames></author><author><keyname>Molli</keyname><forenames>Pascal</forenames></author><author><keyname>Desmontils</keyname><forenames>Emmanuel</forenames></author></authors><title>Tracking Federated Queries in the Linked Data</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Federated query engines allow data consumers to execute queries over the
federation of Linked Data (LD). However, as federated queries are decomposed
into potentially thousands of subqueries distributed among SPARQL endpoints,
data providers do not know federated queries, they only know subqueries they
process. Consequently, unlike warehousing approaches, LD data providers have no
access to secondary data. In this paper, we propose FETA (FEderated query
TrAcking), a query tracking algorithm that infers Basic Graph Patterns (BGPs)
processed by a federation from a shared log maintained by data providers.
Concurrent execution of thousand subqueries generated by multiple federated
query engines makes the query tracking process challenging and uncertain.
Experiments with Anapsid show that FETA is able to extract BGPs which, even in
a worst case scenario, contain BGPs of original queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06103</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06103</id><created>2015-08-25</created><authors><author><keyname>Deng</keyname><forenames>Xiaomao</forenames></author><author><keyname>Cai</keyname><forenames>Xiao-chuan</forenames></author><author><keyname>Zou</keyname><forenames>Jun</forenames></author></authors><title>Two-level space-time domain decomposition methods for unsteady inverse
  problems</title><categories>cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of processor cores on supercomputers becomes larger and larger,
algorithms with high degree of parallelism attract more attention. In this
work, we propose a novel space-time coupled algorithm for solving an inverse
problem associated with the time-dependent convection-diffusion equation in
three dimensions. We introduce a mixed finite element/finite difference method
and a one-level and a two-level space-time parallel domain decomposition
preconditioner for the Karush-Kuhn-Tucker (KKT) system induced from
reformulating the inverse problem as an output least-squares optimization
problem in the space-time domain. The new full space approach eliminates the
sequential steps of the optimization outer loop and the inner forward and
backward time marching processes, thus achieves high degree of parallelism.
Numerical experiments validate that this approach is effective and robust for
recovering unsteady moving sources. We report strong scalability results
obtained on a supercomputer with more than 1,000 processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06106</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06106</id><created>2015-08-25</created><authors><author><keyname>Starosolski</keyname><forenames>Roman</forenames></author></authors><title>Reversible Denoising and Lifting Based Color Component Transformation
  for Lossless Image Compression</title><categories>cs.MM</categories><comments>Reversible color space transformation; Denoising; Lifting technique;
  Lossless image compression</comments><msc-class>94A08 (Primary) 68P30, 94A15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unwanted side effect of reversible color space transformation, usually
being a sequence of lifting steps, is that while removing correlation it
contaminates transformed components with noise from other components. To remove
correlation without increasing noise, we integrate denoising into the lifting
steps and obtain a reversible image component transformation. For JPEG-LS,
JPEG2000, and JPEG XR algorithms in a lossless mode, RDgDb color space
transformation and a simple denoising filter, we find that the proposed method
is especially effective for images in optical resolutions of acquisition
devices, while for typical images it may lead to increased bitrates. We also
present an efficient estimator of image component transformation effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06110</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06110</id><created>2015-08-25</created><updated>2016-01-06</updated><authors><author><keyname>Melis</keyname><forenames>Luca</forenames></author><author><keyname>Danezis</keyname><forenames>George</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author></authors><title>Efficient Private Statistics with Succinct Sketches</title><categories>cs.CR</categories><comments>To appear in NDSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale collection of contextual information is often essential in order
to gather statistics, train machine learning models, and extract knowledge from
data. The ability to do so in a {\em privacy-preserving} way -- i.e., without
collecting fine-grained user data -- enables a number of additional
computational scenarios that would be hard, or outright impossible, to realize
without strong privacy guarantees. In this paper, we present the design and
implementation of practical techniques for privately gathering statistics from
large data streams. We build on efficient cryptographic protocols for private
aggregation and on data structures for succinct data representation, namely,
Count-Min Sketch and Count Sketch. These allow us to reduce the communication
and computation complexity incurred by each data source (e.g., end-users) from
linear to logarithmic in the size of their input, while introducing a
parametrized upper-bounded error that does not compromise the quality of the
statistics. We then show how to use our techniques, efficiently, to instantiate
real-world privacy-friendly systems, supporting recommendations for media
streaming services, prediction of user locations, and computation of median
statistics for Tor hidden services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06119</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06119</id><created>2015-08-25</created><authors><author><keyname>Slawik</keyname><forenames>Mathias</forenames></author><author><keyname>Zilci</keyname><forenames>Beg&#xfc;m &#x130;lke</forenames></author><author><keyname>Knaack</keyname><forenames>Fabian</forenames></author><author><keyname>K&#xfc;pper</keyname><forenames>Axel</forenames></author></authors><title>The Open Service Compendium. Business-pertinent Cloud Service Discovery,
  Assessment, and Selection</title><categories>cs.SE cs.CY cs.DC</categories><comments>14 pages, to be presented at GECON 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When trying to discover, assess, and select cloud services, companies face
many challenges, such as fast-moving markets, vast numbers of offerings, and
highly ambiguous selection criteria. This publication presents the Open Service
Compendium (OSC), an information system which supports businesses in their
discovery, assessment and cloud service selection by offering a simple dynamic
service description language, business-pertinent vocabularies, as well as
matchmaking functionality. It contributes to the state of the art by offering a
more practical, mature, simple, and usable approach than related works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06121</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06121</id><created>2015-08-25</created><authors><author><keyname>Perevoshchikov</keyname><forenames>Vitaly</forenames></author></authors><title>Weight Assignment Logic</title><categories>cs.FL cs.LO</categories><comments>This is the full version of the paper published at DLT 2015</comments><journal-ref>DLT 2015. LNCS, vol. 9168, pp. 413-425. Springer (2015)</journal-ref><doi>10.1007/978-3-319-21500-6_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a weight assignment logic for reasoning about quantitative
languages of infinite words. This logic is an extension of the classical MSO
logic and permits to describe quantitative properties of systems with multiple
weight parameters, e.g., the ratio between rewards and costs. We show that this
logic is expressively equivalent to unambiguous weighted B\&quot;uchi automata. We
also consider an extension of weight assignment logic which is expressively
equivalent to nondeterministic weighted B\&quot;uchi automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06124</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06124</id><created>2015-08-25</created><authors><author><keyname>Zhou</keyname><forenames>Kuang</forenames><affiliation>DRUID</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>DRUID</affiliation></author><author><keyname>Pan</keyname><forenames>Quan</forenames></author></authors><title>A similarity-based community detection method with multiple prototype
  representation</title><categories>cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>Physica A: Statistical Mechanics and its Applications, 2015,
  pp.519-531</journal-ref><doi>10.1016/j.physa.2015.07.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communities are of great importance for understanding graph structures in
social networks. Some existing community detection algorithms use a single
prototype to represent each group. In real applications, this may not
adequately model the different types of communities and hence limits the
clustering performance on social networks. To address this problem, a
Similarity-based Multi-Prototype (SMP) community detection approach is proposed
in this paper. In SMP, vertices in each community carry various weights to
describe their degree of representativeness. This mechanism enables each
community to be represented by more than one node. The centrality of nodes is
used to calculate prototype weights, while similarity is utilized to guide us
to partitioning the graph. Experimental results on computer generated and
real-world networks clearly show that SMP performs well for detecting
communities. Moreover, the method could provide richer information for the
inner structure of the detected communities with the help of prototype weights
compared with the existing community detection models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06143</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06143</id><created>2015-08-25</created><authors><author><keyname>Teknomo</keyname><forenames>Kardi</forenames></author><author><keyname>Gerilla</keyname><forenames>Gloria P.</forenames></author></authors><title>Pedestrian Static Trajectory Analysis of a Hypermarket</title><categories>cs.SI physics.soc-ph</categories><comments>Proceedings of the Eastern Asia Society for Transportation Studies,
  Vol.7, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a combination of pedestrian data collection and
analysis and modeling that may yield higher competitive advantage in the
business environment. The data collection is only based on simple inventory and
questionnaire surveys on a hypermarket to obtain trajectory path of pedestrian
movement. Though the data has limitation by using static trajectories, our
techniques showed that it is possible to obtain aggregation of flow pattern and
alley attractiveness similar to the result of aggregation using dynamic
trajectory. A case study of a real hypermarket demonstrates that daily
necessity products are closely related to higher flow pattern. Using the
proposed method, we are also able to quantify pedestrian behavior that shoppers
tend to walk about 7 times higher than the ideal shortest path
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06158</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06158</id><created>2015-08-25</created><authors><author><keyname>D&#x119;bowski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Regular Hilberg Processes: An Example of Processes with a Vanishing
  Entropy Rate</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><msc-class>94A17, 60G10, 94A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A regular Hilberg process is a stationary process that satisfies both a
power-law growth of topological entropy and a hyperlogarithmic growth of
maximal repetition. Hypothetically, such processes may arise in statistical
modeling of natural language. A striking property of ergodic regular Hilberg
processes is that the length of the Lempel-Ziv code is orders of magnitude
larger than the block entropy. This is possible since regular Hilberg processes
have a vanishing entropy rate. In this paper, we provide a constructive example
of regular Hilberg processes, which we call random hierarchical association
(RHA) processes. We demonstrate that for those RHA processes, the expected
length of any uniquely decodable code is orders of magnitude larger than the
block entropy of the ergodic component of the RHA process. Our proposition
complements the classical result by Shields concerning nonexistence of
universal redundancy rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06161</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06161</id><created>2015-08-25</created><authors><author><keyname>Barrett</keyname><forenames>Daniel Paul</forenames></author><author><keyname>Bronikowski</keyname><forenames>Scott Alan</forenames></author><author><keyname>Yu</keyname><forenames>Haonan</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Robot Language Learning, Generation, and Comprehension</title><categories>cs.RO cs.AI cs.CL cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unified framework which supports grounding natural-language
semantics in robotic driving. This framework supports acquisition (learning
grounded meanings of nouns and prepositions from human annotation of robotic
driving paths), generation (using such acquired meanings to generate sentential
description of new robotic driving paths), and comprehension (using such
acquired meanings to support automated driving to accomplish navigational goals
specified in natural language). We evaluate the performance of these three
tasks by having independent human judges rate the semantic fidelity of the
sentences associated with paths, achieving overall average correctness of 94.6%
and overall average completeness of 85.6%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06163</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06163</id><created>2015-08-25</created><updated>2016-02-25</updated><authors><author><keyname>Cheng</keyname><forenames>Guangliang</forenames></author><author><keyname>Zhu</keyname><forenames>Feiyun</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>Accurate Urban Road Centerline Extraction from VHR Imagery via
  Multiscale Segmentation and Tensor Voting</title><categories>cs.CV</categories><comments>25 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  It is very useful and increasingly popular to extract accurate road
centerlines from very-high-resolution (VHR) re- mote sensing imagery for
various applications, such as road map generation and updating etc. There are
three shortcomings of current methods: (a) Due to the noise and occlusions
(owing to vehicles and trees), most road extraction methods bring in
heterogeneous classification results; (b) Morphological thinning algorithm is
widely used to extract road centerlines, while it pro- duces small spurs around
the centerlines; (c) Many methods are ineffective to extract centerlines around
the road intersections. To address the above three issues, we propose a novel
method to ex- tract smooth and complete road centerlines via three techniques:
the multiscale joint collaborative representation (MJCR) &amp; graph cuts (GC),
tensor voting (TV) &amp; non-maximum suppression (NMS) and fitting based connection
algorithm. Specifically, a MJCR-GC based road area segmentation method is
proposed by incorporating mutiscale features and spatial information. In this
way, a homogenous road segmentation result is achieved. Then, to obtain a
smooth and correct road centerline network, a TV-NMS based centerline
extraction method is introduced. This method not only extracts smooth road
centerlines, but also connects the discontinuous road centerlines. Finally, to
overcome the ineffectiveness of current methods in the road intersection, a
fitting based road centerline connection algorithm is proposed. As a result, we
can get a complete road centerline network. Extensive experiments on two
datasets demonstrate that our method achieves higher quantitative results, as
well as more satisfactory visual performances by comparing with state-of-the-
art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06171</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06171</id><created>2015-08-25</created><authors><author><keyname>Je</keyname><forenames>Changsoo</forenames></author><author><keyname>Park</keyname><forenames>Hyung-Min</forenames></author></authors><title>BREN: Body Reflection Essence-Neuter Model for Separation of Reflection
  Components</title><categories>cs.CV cs.GR physics.optics</categories><comments>4 pages, 4 figures</comments><acm-class>I.2.10; I.3.7; I.4.1; I.4.8</acm-class><journal-ref>Optics Letters, Volume 40, Issue 9, pp. 1940-1943, May 1, 2015</journal-ref><doi>10.1364/OL.40.001940</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel reflection color model consisting of body essence and
(mixed) neuter, and present an effective method for separating dichromatic
reflection components using a single image. Body essence is an entity invariant
to interface reflection, and has two degrees of freedom unlike hue and maximum
chromaticity. As a result, the proposed method is insensitive to noise and
proper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red,
green, and blue), contrary to the maximum chromaticity-based methods. Interface
reflection is separated by using a Gaussian function, which removes a critical
thresholding problem. Furthermore, the method does not require any region
segmentation. Experimental results show the efficacy of the proposed model and
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06173</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06173</id><created>2015-08-25</created><updated>2016-01-19</updated><authors><author><keyname>Zhou</keyname><forenames>Shanyu</forenames></author><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author></authors><title>Blocking Avoidance in Transportation Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The blocking problem naturally arises in transportation systems as multiple
vehicles with different itineraries share available resources. In this paper,
we investigate the impact of the blocking problem to the waiting time at the
intersections of transportation systems. We assume that different vehicles,
depending on their Internet connection capabilities, may communicate their
intentions (e.g., whether they will turn left or right or continue straight) to
intersections (specifically to devices attached to traffic lights). We consider
that information collected by these devices are transmitted to and processed in
a cloud-based traffic control system. Thus, a cloud-based system, based on the
intention information, can calculate average waiting times at intersections. We
consider this problem as a queuing model, and we characterize average waiting
times by taking into account (i) blocking probability, and (ii) vehicles'
ability to communicate their intentions. Then, by using average waiting times
at intersection, we develop a shortest delay algorithm that calculates the
routes with shortest delays between two points in a transportation network. Our
simulation results confirm our analysis, and demonstrate that our shortest
delay algorithm significantly improves over baselines that are unaware of the
blocking problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06181</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06181</id><created>2015-08-25</created><authors><author><keyname>Je</keyname><forenames>Changsoo</forenames></author><author><keyname>Tang</keyname><forenames>Min</forenames></author><author><keyname>Lee</keyname><forenames>Youngeun</forenames></author><author><keyname>Lee</keyname><forenames>Minkyoung</forenames></author><author><keyname>Kim</keyname><forenames>Young J.</forenames></author></authors><title>PolyDepth: Real-time Penetration Depth Computation using Iterative
  Contact-Space Projection</title><categories>cs.GR cs.CG cs.RO</categories><comments>Presented in ACM SIGGRAPH 2012. 15 pages, 23 figures</comments><acm-class>I.2.9; I.3.5; I.3.7; I.6.8</acm-class><journal-ref>ACM Transactions on Graphics (ToG 2012), Volume 31, Issue 1,
  Article 5, pp. 1-14, January 1, 2012</journal-ref><doi>10.1145/2077341.2077346</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a real-time algorithm that finds the Penetration Depth (PD)
between general polygonal models based on iterative and local optimization
techniques. Given an in-collision configuration of an object in configuration
space, we find an initial collision-free configuration using several methods
such as centroid difference, maximally clear configuration, motion coherence,
random configuration, and sampling-based search. We project this configuration
on to a local contact space using a variant of continuous collision detection
algorithm and construct a linear convex cone around the projected
configuration. We then formulate a new projection of the in-collision
configuration onto the convex cone as a Linear Complementarity Problem (LCP),
which we solve using a type of Gauss-Seidel iterative algorithm. We repeat this
procedure until a locally optimal PD is obtained. Our algorithm can process
complicated models consisting of tens of thousands triangles at interactive
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06182</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06182</id><created>2015-08-22</created><updated>2015-08-28</updated><authors><author><keyname>Rosenberg</keyname><forenames>Gili</forenames></author><author><keyname>Haghnegahdar</keyname><forenames>Poya</forenames></author><author><keyname>Goddard</keyname><forenames>Phil</forenames></author><author><keyname>Carr</keyname><forenames>Peter</forenames></author><author><keyname>Wu</keyname><forenames>Kesheng</forenames></author><author><keyname>de Prado</keyname><forenames>Marcos L&#xf3;pez</forenames></author></authors><title>Solving the Optimal Trading Trajectory Problem Using a Quantum Annealer</title><categories>q-fin.CP cs.DS math.OC q-fin.PM quant-ph</categories><comments>7 pages; minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve a multi-period portfolio optimization problem using D-Wave Systems'
quantum annealer. We derive a formulation of the problem, discuss several
possible integer encoding schemes, and present numerical examples that show
high success rates. The formulation incorporates transaction costs (including
permanent and temporary market impact), and, significantly, the solution does
not require the inversion of a covariance matrix. The discrete multi-period
portfolio optimization problem we solve is significantly harder than the
continuous variable problem. We present insight into how results may be
improved using suitable software enhancements, and why current quantum
annealing technology limits the size of problem that can be successfully solved
today. The formulation presented is specifically designed to be scalable, with
the expectation that as quantum annealing technology improves, larger problems
will be solvable using the same techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06183</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06183</id><created>2015-08-25</created><authors><author><keyname>Kong</keyname><forenames>Lei</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author><author><keyname>Zhang</keyname><forenames>Hua</forenames></author><author><keyname>Zhao</keyname><forenames>Chunming</forenames></author></authors><title>Performance of a Free Space Optical Relay-Assisted Hybrid RF/FSO System
  in Generalized M-Distributed Channels</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Photonics Journal</comments><doi>10.1109/JPHOT.2015.2470106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the average symbol error rate (ASER) performance of a
dual-hop hybrid relaying system relying on both radio frequency (RF) and free
space optical (FSO) links. Specifically, the RF link is used for supporting
mobile communication, while the FSO link is adopted as the backhaul of the
cellular infrastructure. Considering non-line-of-sight (NLoS) RF transmissions
and a generalized atmospheric turbulence (AT) channel, the associated
statistical features constituted of both the exact and the asymptotic moment
generating functions (MGF) are derived in closed form. They are then used for
calculating the ASER of M-ary phase shift keying (PSK), differentially encoded
non-coherent PSK (DPSK) and non-coherent frequency-shift keying (FSK). A range
of additional asymptotic expressions are also derived for all the modulation
schemes under high signal-to-noise ratios (SNR). It is observed from the
asymptotic analysis that the ASERs of all the modulation schemes are dominated
by the average SNR of the RF link in the hybrid relaying system using a fixed
relay gain, while in the relaying system using a dynamic channel dependent
relay gain, the ASERs of all the modulation schemes depend both on the average
SNR and on the AT condition of the FSO path. We also find that the fixed-gain
relaying strategy achieves twice the diversity order of the channel-dependent
relaying strategy albeit at the cost of requiring a high power amplifier (PA)
dynamic range at the relay node. Furthermore, by comparing the asymptotic
ASERs, we calculate the SNR differences between the different modulation
schemes in both the fixed-gain and the channel-dependent relaying system.
Finally, simulation results are presented for confirming the accuracy of our
expressions and observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06184</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06184</id><created>2015-08-25</created><authors><author><keyname>Kayes</keyname><forenames>Imrul</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author></authors><title>Privacy Concerns vs. User Behavior in Community Question Answering</title><categories>cs.SI cs.CY</categories><comments>Published in the proceedings of the international conference on
  Advances in Social Network Analysis and Mining (ASONAM'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community-based question answering (CQA) platforms are crowd-sourced services
for sharing user expertise on various topics, from mechanical repairs to
parenting. While they naturally build-in an online social network
infrastructure, they carry a very different purpose from Facebook-like social
networks, where users &quot;hang-out&quot; with their friends and tend to share more
personal information. It is unclear, thus, how the privacy concerns and their
correlation with user behavior in an online social network translate into a CQA
platform. This study analyzes one year of recorded traces from a mature CQA
platform to understand the association between users' privacy concerns as
manifested by their account settings and their activity in the platform. The
results show that privacy preference is correlated with behavior in the
community in terms of engagement, retention, accomplishments and deviance from
the norm. We find privacy-concerned users have higher qualitative and
quantitative contributions, show higher retention, report more abuses, have
higher perception on answer quality and have larger social circles. However, at
the same time, these users also exhibit more deviant behavior than the users
with public profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06191</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06191</id><created>2015-08-25</created><authors><author><keyname>Wong</keyname><forenames>Justin</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Neuro-Fuzzy Method to Improving Backfiring Conversion Ratios</title><categories>cs.SE cs.AI</categories><comments>International Conference on Soft Computing, Intelligent System and
  Information Technology, Bali, Indonesia, pp. 12-17, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software project estimation is crucial aspect in delivering software on time
and on budget. Software size is an important metric in determining the effort,
cost, and productivity. Today, source lines of code and function point are the
most used sizing metrics. Backfiring is a well-known technique for converting
between function points and source lines of code. However when backfiring is
used, there is a high margin of error. This study introduces a method to
improve the accuracy of backfiring. Intelligent systems have been used in
software prediction models to improve performance over traditional techniques.
For this reason, a hybrid Neuro-Fuzzy is used because it takes advantages of
the neural networks learning and fuzzy logic human-like reasoning. This paper
describes an improved backfiring technique which uses Neuro-Fuzzy and compares
the new method against the default conversion ratios currently used by software
practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06193</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06193</id><created>2015-08-25</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>F2- Rules for Qualification of Developing and Managing Software Product
  Line</title><categories>cs.SE</categories><comments>International Conference on Software Engineering Research and
  Practice, Las Vegas, pp. 827-833, 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software product line has emerged as a valuable concept of developing
software based on reusable software assets. The concept aims on effective
utilization of software assets, reduced time to delivery, improved quality and
better benefits to cost ratio of products. In this paper we have defined
certain rules for the qualification of developing and managing a software
product line. An organization must follow these rules in order to establish and
manage software product line effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06195</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06195</id><created>2015-08-25</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Usability as a Dominant Quality Attribute</title><categories>cs.SE cs.HC</categories><comments>International Conference on Software Engineering Research and
  Practice, Las Vegas, pp. 571-575, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whenever an architect or a team of architects begins an architectural design,
there are certain goals set to achieve. There are many factors involved in
setting up goals for the architecture design such as type of the project, end
user perspective, functional and non-functional requirements and so on. This
paper reviews and further elaborates strategy for the usability characteristics
of software architecture. Although user centered designs are tremendously
gaining popularity, still in many design scenarios, usability is barely even
considered as one of the primary goals. This work provides an opportunity to
compare different strategies and evaluate their pros and cons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06203</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06203</id><created>2015-08-25</created><authors><author><keyname>Gao</keyname><forenames>Qimin</forenames></author><author><keyname>Brown</keyname><forenames>Lyndon J</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Integrating Schedulability Analysis with UML-RT</title><categories>cs.SY</categories><journal-ref>Journal of Control and Intelligent Systems, 34(2): 125-135, ACTA
  Press, 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of object oriented techniques and methodologies for the design of
real-time control systems appear to be necessary in order to deal with the
increasing complexity of such systems. Recently many object-oriented methods
have been used for the modeling and design of real-time control systems. We
believe that an approach that integrates the advancements in both object
modeling and design methods, and real-time scheduling theory is the key to
successful use of object oriented technology for real-time software. However,
past approaches to integrate the two either restrict the object models, or do
not allow sophisticated schedulability analysis techniques. In this paper we
show how schedulability analysis can be integrated with object-oriented design;
we develop the schedulability and feasibility analysis method for the external
messages that may suffer release jitter due to being dispatched by a tick
driven scheduler in real-time control system, and we also develop the
scheduliability method for sporadic activities, where message arrive
sporadically then execute periodically for some bounded time. This method can
be used to cope with timing constraints in complex real-time control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06206</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06206</id><created>2015-08-25</created><authors><author><keyname>Di Iorio</keyname><forenames>Angelo</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Dimou</keyname><forenames>Anastasia</forenames></author><author><keyname>Vahdati</keyname><forenames>Sahar</forenames></author></authors><title>Semantic Publishing Challenge - Assessing the Quality of Scientific
  Output by Information Extraction and Interlinking</title><categories>cs.DL</categories><comments>To appear in: E. Cabrio and M. Stankovic and M. Dragoni and A.
  Gangemi and R. Navigli and V. Presutti and D. Garigliotti and A. L. Gentile
  and A. Nuzzolese and A. Di Iorio and A. Dimou and C. Lange and S. Vahdati and
  A. Freitas and C. Unger and D. Reforgiato Recupero (eds.). Semantic Web
  Evaluation Challenges 2015. Communications in Computer and Information
  Science, Springer, 2015. arXiv admin note: text overlap with arXiv:1408.3863</comments><acm-class>H.3.7; I.7.4; H.3.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Semantic Publishing Challenge series aims at investigating novel
approaches for improving scholarly publishing using Linked Data technology. In
2014 we had bootstrapped this effort with a focus on extracting information
from non-semantic publications - computer science workshop proceedings volumes
and their papers - to assess their quality. The objective of this second
edition was to improve information extraction but also to interlink the 2014
dataset with related ones in the LOD Cloud, thus paving the way for
sophisticated end-user services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06208</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06208</id><created>2015-08-25</created><authors><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>CBSE CASE Environment</title><categories>cs.SE</categories><comments>4th International Multi-Conference on Information Society, Ljubljana,
  Slovenia, pp. 379-382, 2001</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the need to produce ever larger and more complex software systems, the
use of reusable components has become increasingly imperative. Of the many
existing and proposed techniques for software development, it seems clear that
components-based software engineering (CBSE) will be at the forefront of new
approaches to the production of software systems, and holds the promise of
substantially enhancing the software development and maintenance process. The
required features of a CASE environment suitable for component reuse will be
put forward in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06216</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06216</id><created>2015-08-25</created><authors><author><keyname>Cohen</keyname><forenames>Reuven</forenames></author><author><keyname>Katzir</keyname><forenames>Liran</forenames></author><author><keyname>Yehezkel</keyname><forenames>Aviv</forenames></author></authors><title>Cardinality Estimation Meets Good-Turing</title><categories>cs.DS</categories><comments>17 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardinality estimation algorithms receive a stream of elements whose order
might be arbitrary, with possible repetitions, and return the number of
distinct elements. Such algorithms usually seek to minimize the required
storage and processing at the price of inaccuracy in their output. Real-world
applications of these algorithms are required to process large volumes of
monitored data, making it impractical to collect and analyze the entire input
stream. In such cases, it is common practice to sample and process only a small
part of the stream elements. This paper presents and analyzes a generic
algorithm for combining every cardinality estimation algorithm with a sampling
process. We show that the proposed sampling algorithm does not affect the
estimator's asymptotic unbiasedness, and we analyze the sampling effect on the
estimator's variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06218</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06218</id><created>2015-08-24</created><updated>2015-09-14</updated><authors><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Taheri</keyname><forenames>Mina</forenames></author></authors><title>GATE: Greening At The Edge</title><categories>cs.NI</categories><comments>7 Pages, 12 Figures, Submitted to IEEE Wireless Communications</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Dramatic data traffic growth, especially wireless data, is driving a
significant surge in energy consumption in the last mile access of the
telecommunications infrastructure. The growing energy consumption not only
escalates the operators' operational expenditures (OPEX) but also leads to a
significant rise of carbon footprints. Therefore, enhancing the energy
efficiency of broadband access networks is becoming a necessity to bolster
social, environmental, and economic sustainability. This article provides an
overview on the design and optimization of energy efficient broadband access
networks, analyzes the energy efficient design of passive optical networks,
discusses the enabling technologies for next generation broadband wireless
access networks, and elicits the emerging technologies for enhancing the energy
efficiency of the last mile access of the network infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06226</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06226</id><created>2014-12-18</created><authors><author><keyname>Bonzi</keyname><forenames>E. V.</forenames></author><author><keyname>Grad</keyname><forenames>G. B.</forenames></author><author><keyname>Maggi</keyname><forenames>A. M.</forenames></author><author><keyname>Mu&#xf1;&#xf3;z</keyname><forenames>M. R.</forenames></author></authors><title>Study of the characteristic parameters of the normal voices of
  Argentinian speakers</title><categories>q-bio.NC cs.SD</categories><comments>5 pages, 6 figures</comments><proxy>Luis Pugnaloni</proxy><journal-ref>Papers in Physics 6, 060002 (2014)</journal-ref><doi>10.4279/PIP.060002</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The voice laboratory permits to study the human voices using a method that is
objective and noninvasive. In this work, we have studied the parameters of the
human voice such as pitch, formant, jitter, shimmer and harmonic-noise ratio of
a group of young people. This statistical information of parameters is obtained
from Argentinian speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06235</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06235</id><created>2015-08-25</created><updated>2015-10-31</updated><authors><author><keyname>Khashabi</keyname><forenames>Daniel</forenames></author><author><keyname>Wieting</keyname><forenames>John</forenames></author><author><keyname>Liu</keyname><forenames>Jeffrey Yufei</forenames></author><author><keyname>Liang</keyname><forenames>Feng</forenames></author></authors><title>Clustering With Side Information: From a Probabilistic Model to a
  Deterministic Algorithm</title><categories>stat.ML cs.AI cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a model-based clustering method (TVClust) that
robustly incorporates noisy side information as soft-constraints and aims to
seek a consensus between side information and the observed data. Our method is
based on a nonparametric Bayesian hierarchical model that combines the
probabilistic model for the data instance and the one for the side-information.
An efficient Gibbs sampling algorithm is proposed for posterior inference.
Using the small-variance asymptotics of our probabilistic model, we then derive
a new deterministic clustering algorithm (RDP-means). It can be viewed as an
extension of K-means that allows for the inclusion of side information and has
the additional property that the number of clusters does not need to be
specified a priori. Empirical studies have been carried out to compare our work
with many constrained clustering algorithms from the literature on both a
variety of data sets and under a variety of conditions such as using noisy side
information and erroneous k values. The results of our experiments show strong
results for our probabilistic and deterministic approaches under these
conditions when compared to other algorithms in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06257</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06257</id><created>2015-08-25</created><authors><author><keyname>Hosseinmardi</keyname><forenames>Homa</forenames></author><author><keyname>Mattson</keyname><forenames>Sabrina Arredondo</forenames></author><author><keyname>Rafiq</keyname><forenames>Rahat Ibn</forenames></author><author><keyname>Han</keyname><forenames>Richard</forenames></author><author><keyname>Lv</keyname><forenames>Qin</forenames></author><author><keyname>Mishr</keyname><forenames>Shivakant</forenames></author></authors><title>Prediction of Cyberbullying Incidents on the Instagram Social Network</title><categories>cs.IR cs.CY cs.SI</categories><comments>arXiv admin note: text overlap with arXiv:1503.03909</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyberbullying is a growing problem affecting more than half of all American
teens. The main goal of this paper is to investigate fundamentally new
approaches to understand and automatically detect and predict incidents of
cyberbullying in Instagram, a media-based mobile social network. In this work,
we have collected a sample data set consisting of Instagram images and their
associated comments. We then designed a labeling study and employed human
contributors at the crowd-sourced CrowdFlower website to label these media
sessions for cyberbullying. A detailed analysis of the labeled data is then
presented, including a study of relationships between cyberbullying and a host
of features such as cyberaggression, profanity, social graph features, temporal
commenting behavior, linguistic content, and image content. Using the labeled
data, we further design and evaluate the performance of classifiers to
automatically detect and pre- dict incidents of cyberbullying and
cyberaggression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06262</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06262</id><created>2015-08-25</created><updated>2015-09-27</updated><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Recovery of Sparse Positive Signals on the Sphere from Low Resolution
  Measurements</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2015.2485281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter considers the problem of recovering a positive stream of Diracs
on a sphere from its projection onto the space of low-degree spherical
harmonics, namely, from its low-resolution version. We suggest recovering the
Diracs via a tractable convex optimization problem. The resulting recovery
error is proportional to the noise level and depends on the density of the
Diracs. We validate the theory by numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06264</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06264</id><created>2015-08-25</created><authors><author><keyname>Wang</keyname><forenames>Jingbin</forenames></author><author><keyname>Wang</keyname><forenames>Haoxiang</forenames></author><author><keyname>Zhou</keyname><forenames>Yihua</forenames></author><author><keyname>McDonald</keyname><forenames>Nancy</forenames></author></authors><title>Multiple kernel multivariate performance learning using cutting plane
  algorithm</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multi-kernel classifier learning algorithm to
optimize a given nonlinear and nonsmoonth multivariate classifier performance
measure. Moreover, to solve the problem of kernel function selection and kernel
parameter tuning, we proposed to construct an optimal kernel by weighted linear
combination of some candidate kernels. The learning of the classifier parameter
and the kernel weight are unified in a single objective function considering to
minimize the upper boundary of the given multivariate performance measure. The
objective function is optimized with regard to classifier parameter and kernel
weight alternately in an iterative algorithm by using cutting plane algorithm.
The developed algorithm is evaluated on two different pattern classification
methods with regard to various multivariate performance measure optimization
problems. The experiment results show the proposed algorithm outperforms the
competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06268</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06268</id><created>2015-08-25</created><updated>2015-09-22</updated><authors><author><keyname>Navarro</keyname><forenames>C. A.</forenames></author><author><keyname>Huang</keyname><forenames>Wei</forenames></author><author><keyname>Deng</keyname><forenames>Youjin</forenames></author></authors><title>Adaptive Multi-GPU Exchange Monte Carlo for the 3D Random Field Ising
  Model</title><categories>physics.comp-ph cond-mat.stat-mech cs.DC</categories><comments>15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an adaptive multi-GPU Exchange Monte Carlo method designed for the
simulation of the 3D Random Field Model. The algorithm design is based on a
two-level parallelization scheme that allows the method to scale its
performance in the presence of faster and GPUs as well as multiple GPUs. The
set of temperatures is adapted according to the exchange rate observed from
short trial runs, leading to an increased exchange rate at zones where the
exchange process is sporadic. Performance results show that parallel tempering
is an ideal strategy for being implemented on the GPU, and runs between one to
two orders of magnitude with respect to a single-core CPU version, with
multi-GPU scaling being approximately $99\%$ efficient. The results obtained
extend the possibilities of simulation to sizes of $L = 32, 64$ for a
workstation with two GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06269</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06269</id><created>2015-08-25</created><authors><author><keyname>Vasal</keyname><forenames>Deepanshu</forenames></author><author><keyname>Subramanian</keyname><forenames>Vijay</forenames></author><author><keyname>Anastasopoulos</keyname><forenames>Achilleas</forenames></author></authors><title>A systematic process for evaluating structured perfect Bayesian
  equilibria in dynamic games with asymmetric information</title><categories>math.OC cs.GT cs.SY</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a finite horizon dynamic game with $N$ players who observe their
types privately and take actions, which are publicly observed. Their actions
and types jointly determine their instantaneous rewards. Since each player has
a different information set, this is a dynamic game with asymmetric information
and there is no known methodology to find perfect Bayesian equilibria (PBE) for
such games in general. In this paper, we develop a methodology to obtain a
class of PBE using a belief state based on common information of the players.
We show a structural result that the common information can be summarized in
this belief state such that any expected reward profile that can be achieved by
any general strategy profile can also be achieved by a policy based on players'
private information and this belief state. With this as our motivation, we
state our main result that provides a two-step backward-forward inductive
algorithm to find the class of PBE of this game that are based on this belief
state. We refer to such equilibria as \textit{structured Bayesian perfect
equilibria} (SPBE). The backward inductive part of this algorithm defines an
equilibrium generating function. Each period in the backward induction involves
solving a fixed point equation on the space of probability simplexes for every
possible belief on types. Then using this function, equilibrium strategies and
beliefs are defined through a forward recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06270</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06270</id><created>2015-08-25</created><authors><author><keyname>Gao</keyname><forenames>Qimin</forenames></author><author><keyname>Brown</keyname><forenames>Lyndon J.</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Extending UML-RT for Control System Modeling</title><categories>cs.SE cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1508.06203</comments><journal-ref>American Journal of Applied Sciences, 1(4): 338-347, Science
  Publications,2004</journal-ref><doi>10.3844/ajassp.2004.338.347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing interest in adopting object technologies for the
development of real-time control systems. Several commercial tools, currently
available, provide object-oriented modeling and design support for real-time
control systems. While these products provide many useful facilities, such as
visualization tools and automatic code generation, they are all weak in
addressing the central characteristic of real-time control systems design,
i.e., providing support for a designer to reason about timeliness properties.
We believe an approach that integrates the advancements in both object modeling
and design methods and real-time scheduling theory is the key to successful use
of object technology for real-time software. Surprisingly several past
approaches to integrate the two either restrict the object models, or do not
allow sophisticated schedulability analysis techniques. This study shows how
schedulability analysis can be integrated with UML for Real-Time (UML-RT) to
deal with timing properties in real time control systems. More specifically, we
develop the schedulability and feasibility analysis modeling for the external
messages that may suffer release jitter due to being dispatched by a tick
driven scheduler in real-time control system and we also develop the
scheduliablity modeling for sporadic activities, where messages arrive
sporadically then execute periodically for some bounded time. This method can
be used to cope with timing constraints in realistic and complex real-time
control systems. Using this method, a designer can quickly evaluate the impact
of various implementation decisions on schedulability. In conjunction with
automatic code-generation, we believe that this will greatly streamline the
design and development of real-time control systems software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06307</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06307</id><created>2015-08-25</created><authors><author><keyname>Parsaeefard</keyname><forenames>Saeedeh</forenames></author><author><keyname>Dawadi</keyname><forenames>Rajesh</forenames></author><author><keyname>Derakhshani</keyname><forenames>Mahsa</forenames></author><author><keyname>Le-Ngoc</keyname><forenames>Tho</forenames></author></authors><title>Joint User-Association and Resource-Allocation in Virtualized Wireless
  Networks</title><categories>cs.IT cs.NI math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a down-link transmission of multicell virtualized
wireless networks (VWNs) where users of different service providers (slices)
within a specific region are served by a set of base stations (BSs) through
orthogonal frequency division multiple access (OFDMA). In particular, we
develop a joint BS assignment, sub-carrier and power allocation algorithm to
maximize the network throughput, while satisfying the minimum required rate of
each slice. Under the assumption that each user at each transmission instance
can connect to no more than one BS, we introduce the user-association factor
(UAF) to represent the joint sub-carrier and BS assignment as the optimization
variable vector in the mathematical problem formulation. Sub-carrier reuse is
allowed in different cells, but not within one cell. As the proposed
optimization problem is inherently non-convex and NP-hard, by applying the
successive convex approximation (SCA) and complementary geometric programming
(CGP), we develop an efficient two-step iterative approach with low
computational complexity to solve the proposed problem. For a given
power-allocation, Step 1 derives the optimum userassociation and subsequently,
for an obtained user-association, Step 2 find the optimum power-allocation.
Simulation results demonstrate that the proposed iterative algorithm
outperforms the traditional approach in which each user is assigned to the BS
with the largest average value of signal strength, and then, joint sub-carrier
and power allocation is obtained for the assigned users of each cell.
Especially, for the cell-edge users, simulation results reveal a coverage
improvement up to 57% and 71% for uniform and non-uniform users distribution,
respectively leading to more reliable transmission and higher spectrum
efficiency for VWN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06314</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06314</id><created>2015-08-25</created><authors><author><keyname>Salloum</keyname><forenames>Maher</forenames></author><author><keyname>Fabian</keyname><forenames>Nathan</forenames></author><author><keyname>Hensinger</keyname><forenames>David M.</forenames></author><author><keyname>Templeton</keyname><forenames>Jeremy A.</forenames></author></authors><title>Compressed Sensing and Reconstruction of Unstructured Mesh Datasets</title><categories>cs.IT cs.DC cs.SY math.IT math.OC</categories><comments>18 pages, 7 figures</comments><report-no>SAND2015-4995C</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exascale computing promises quantities of data too large to efficiently store
and transfer across networks in order to be able to analyze and visualize the
results. We investigate Compressive Sensing (CS) as a way to reduce the size of
the data as it is being stored. CS works by sampling the data on the
computational cluster within an alternative function space such as wavelet
bases, and then reconstructing back to the original space on visualization
platforms. While much work has gone into exploring CS on structured data sets,
such as image data, we investigate its usefulness for point clouds such as
unstructured mesh datasets found in many finite element simulations. We sample
using second generation wavelets (SGW) and reconstruct using the Stagewise
Orthogonal Matching Pursuit (StOMP) algorithm. We analyze the compression
ratios achievable and quality of reconstructed results at each compression
rate. We are able to achieve compression ratios between 10 and 30 on moderate
size datasets with minimal visual deterioration as a result of the lossy
compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06316</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06316</id><created>2015-08-25</created><authors><author><keyname>Khong</keyname><forenames>Sei Zhen</forenames></author><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author></authors><title>Positive Systems Analysis Via Integral Linear Constraints</title><categories>math.OC cs.SY</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Closed-loop positivity of feedback interconnections of positive monotone
nonlinear systems is investigated. It is shown that an instantaneous gain
condition on the open-loop systems which implies feedback well-posedness also
guarantees feedback positivity. Furthermore, the notion of integral linear
constraints (ILC) is utilised as a tool to characterise uncertainty in positive
feedback systems. Robustness analysis of positive linear time-varying and
nonlinear feedback systems is studied using ILC, paralleling the well-known
results based on integral quadratic constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06320</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06320</id><created>2015-08-25</created><authors><author><keyname>Hannig</keyname><forenames>Frank</forenames></author><author><keyname>Koch</keyname><forenames>Dirk</forenames></author><author><keyname>Ziener</keyname><forenames>Daniel</forenames></author></authors><title>Proceedings of the Second International Workshop on FPGAs for Software
  Programmers (FSP 2015)</title><categories>cs.AR cs.DC cs.PL</categories><comments>Website of the workshop: https://www12.cs.fau.de/ws/fsp2015/</comments><proxy>Frank Hannig</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers accepted at the Second International Workshop
on FPGAs for Software Programmers (FSP 2015), held in London, United Kingdom,
September 1st, 2015. FSP 2015 was co-located with the International Conference
on Field Programmable Logic and Applications (FPL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06329</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06329</id><created>2015-08-25</created><authors><author><keyname>Lupinska</keyname><forenames>Agnieszka</forenames></author></authors><title>A Parallel Algorithm to Test Chordality of Graphs</title><categories>cs.DC cs.DS</categories><comments>MSc thesis, promoter: dr Maciej \'Slusarek</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple parallel algorithm to test chordality of graphs which is
based on the parallel Lexicographical Breadth-First Search algorithm. In total,
the algorithm takes time O(N ) on N-threads machine and it performs work O(N 2
) , where N is the number of vertices in a graph. Our implementation of the
algorithm uses a GPU environment Nvidia CUDA C. The algorithm is implemented in
CUDA 4.2 and it has been tested on Nvidia GeForce GTX 560 Ti of compute
capability 2.1. At the end of the thesis we present the results achieved by our
implementation and compare them with the results achieved by the sequential
algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06333</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06333</id><created>2015-08-25</created><updated>2016-02-05</updated><authors><author><keyname>Ruiz</keyname><forenames>Ubaldo</forenames></author><author><keyname>Isler</keyname><forenames>Volkan</forenames></author></authors><title>Capturing an Omnidirectional Evader in Convex Environments using a
  Differential Drive Robot</title><categories>cs.RO</categories><comments>6 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of capturing an Omnidirectional Evader in convex
environments using a Differential Drive Robot (DDR). The DDR wins the game if
at any time instant it captures (collides with) the evader. The evader wins if
it can avoid capture forever. Both players are unit disks with the same maximum
(bounded) speed, but the DDR can only change its motion direction at a bounded
rate. We show that despite this limitation, the DDR can capture the evader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06336</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06336</id><created>2015-08-25</created><authors><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Bradley</keyname><forenames>Joseph K.</forenames></author><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>SPRIGHT: A Fast and Robust Framework for Sparse Walsh-Hadamard Transform</title><categories>cs.IT cs.LG math.IT</categories><comments>Part of our results was reported in ISIT 2014, titled &quot;The SPRIGHT
  algorithm for robust sparse Hadamard Transforms.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the Walsh-Hadamard Transform (WHT) of
some $N$-length input vector in the presence of noise, where the $N$-point
Walsh spectrum is $K$-sparse with $K = {O}(N^{\delta})$ scaling sub-linearly in
the input dimension $N$ for some $0&lt;\delta&lt;1$. Over the past decade, there has
been a resurgence in research related to the computation of Discrete Fourier
Transform (DFT) for some length-$N$ input signal that has a $K$-sparse Fourier
spectrum. In particular, through a sparse-graph code design, our earlier work
on the Fast Fourier Aliasing-based Sparse Transform (FFAST) algorithm computes
the $K$-sparse DFT in time ${O}(K\log K)$ by taking ${O}(K)$ noiseless samples.
Inspired by the coding-theoretic design framework, Scheibler et al. proposed
the Sparse Fast Hadamard Transform (SparseFHT) algorithm that elegantly
computes the $K$-sparse WHT in the absence of noise using ${O}(K\log N)$
samples in time ${O}(K\log^2 N)$. However, the SparseFHT algorithm explicitly
exploits the noiseless nature of the problem, and is not equipped to deal with
scenarios where the observations are corrupted by noise. Therefore, a question
of critical interest is whether this coding-theoretic framework can be made
robust to noise. Further, if the answer is yes, what is the extra price that
needs to be paid for being robust to noise? In this paper, we show, quite
interestingly, that there is {\it no extra price} that needs to be paid for
being robust to noise other than a constant factor. In other words, we can
maintain the same sample complexity ${O}(K\log N)$ and the computational
complexity ${O}(K\log^2 N)$ as those of the noiseless case, using our SParse
Robust Iterative Graph-based Hadamard Transform (SPRIGHT) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06340</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06340</id><created>2015-08-25</created><authors><author><keyname>Arad</keyname><forenames>Itai</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author><author><keyname>Sundaram</keyname><forenames>Aarthi</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Linear time algorithm for quantum 2SAT</title><categories>quant-ph cs.CC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A canonical result about satisfiability theory is that the 2-SAT problem can
be solved in linear time, despite the NP-hardness of the 3-SAT problem. In the
quantum 2-SAT problem, we are given a family of 2-qubit projectors $\Pi_{ij}$
on a system of $n$ qubits, and the task is to decide whether the Hamiltonian
$H=\sum \Pi_{ij}$ has a 0-eigenvalue, or it is larger than $1/n^\alpha$ for
some $\alpha=O(1)$. The problem is not only a natural extension of the
classical 2-SAT problem to the quantum case, but is also equivalent to the
problem of finding the ground state of 2-local frustration-free Hamiltonians of
spin $\frac{1}{2}$, a well-studied model believed to capture certain key
properties in modern condensed matter physics. While Bravyi has shown that the
quantum 2-SAT problem has a classical polynomial-time algorithm, the running
time of his algorithm is $O(n^4)$. In this paper we give a classical algorithm
with linear running time in the number of local projectors, therefore achieving
the best possible complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06346</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06346</id><created>2015-08-25</created><updated>2016-02-16</updated><authors><author><keyname>Nguyen</keyname><forenames>Dinh Hoa</forenames></author></authors><title>Reduced-order Distributed Consensus Controller Design via Edge Dynamics</title><categories>cs.SY math.OC</categories><comments>submitted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach to design reduced-order distributed
consensus controllers for multi-agent systems (MASs) with identical linear
dynamics of agents. A new model namely edge dynamics representing the
differences on agents' states is first presented. Then the distributed
consensus controller design is shown to be equivalent to the synthesis of a
distributed stabilizing controller for this edge dynamics. Consequently, based
on LQR approach, the globally optimal and locally optimal distributed
stabilizing controller designs are proposed, of which the locally distributed
stabilizing controller for the edge dynamics results in a distributed consensus
controller for the MAS with no conservative bound on the coupling strength.
This approach is next further developed to obtain reduced-order distributed
consensus controllers for linear MASs. Several numerical examples are
introduced to illustrate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06347</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06347</id><created>2015-08-25</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author><author><keyname>Gebler</keyname><forenames>Daniel E.</forenames></author></authors><title>Proceedings of the Combined 22th International Workshop on
  Expressiveness in Concurrency and 12th Workshop on Structural Operational
  Semantics</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 190, 2015</journal-ref><doi>10.4204/EPTCS.190</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Combined 22nd International
Workshop on Expressiveness in Concurrency and the 12th Workshop on Structural
Operational Semantics (EXPRESS/SOS 2015) which was held on 31 August 2015 in
Madrid, Spain, as an affiliated workshop of CONCUR 2015, the 26th International
Conference on Concurrency Theory. The EXPRESS workshops aim at bringing
together researchers interested in the expressiveness of various formal systems
and semantic notions, particularly in the field of concurrency. Their focus has
traditionally been on the comparison between programming concepts (such as
concurrent, functional, imperative, logic and object-oriented programming) and
between mathematical models of computation (such as process algebras, Petri
nets, event structures, modal logics, and rewrite systems) on the basis of
their relative expressive power. The EXPRESS workshop series has run
successfully since 1994 and over the years this focus has become broadly
construed. The SOS workshops aim at being a forum for researchers, students and
practitioners interested in new developments, and directions for future
investigation, in the field of structural operational semantics. One of the
specific goals of the SOS workshop series is to establish synergies between the
concurrency and programming language communities working on the theory and
practice of SOS. Since 2012, the EXPRESS and SOS communities have organized an
annual combined EXPRESS/SOS workshop on the expressiveness of mathematical
models of computation and the formal semantics of systems and programming
concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06350</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06350</id><created>2015-08-25</created><authors><author><keyname>Xu</keyname><forenames>Minxian</forenames></author><author><keyname>Luo</keyname><forenames>Guangchun</forenames></author><author><keyname>Tian</keyname><forenames>Ling</forenames></author><author><keyname>Chen</keyname><forenames>Aiguo</forenames></author><author><keyname>Jiang</keyname><forenames>Yaqiu</forenames></author><author><keyname>Li</keyname><forenames>Guozhong</forenames></author><author><keyname>Tian</keyname><forenames>Wenhong</forenames></author></authors><title>Prepartition: Paradigm for the Load Balance of Virtual Machine
  Allocation in Data Centers</title><categories>cs.DC</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is significant to apply load-balancing strategy to improve the performance
and reliability of resource in data centers. One of the challenging scheduling
problems in Cloud data centers is to take the allocation and migration of
reconfigurable virtual machines (VMs) as well as the integrated features of
hosting physical machines (PMs) into consideration. In the reservation model,
the workload of data centers has fixed process interval characteristics. In
general, load-balance scheduling is NP-hard problem as proved in many open
literatures. Traditionally, for offline load balance without migration, one of
the best approaches is LPT (Longest Process Time first), which is well known to
have approximation ratio 4/3. With virtualization, reactive (post) migration of
VMs after allocation is one popular way for load balance and traffic
consolidation. However, reactive migration has difficulty to reach predefined
load balance objectives, and may cause interruption and instability of service
and other associated costs. In view of this, we propose a new paradigm, called
Prepartition, it proactively sets process-time bound for each request on each
PM and prepares in advance to migrate VMs to achieve the predefined balance
goal. Prepartition can reduce process time by preparing VM migration in advance
and therefore reduce instability and achieve better load balance as desired. We
also apply the Prepartition to online (PrepartitionOn) load balance and compare
it with existing online scheduling algorithms. Both theoretical and
experimental results are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06356</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06356</id><created>2015-08-25</created><updated>2015-10-18</updated><authors><author><keyname>Cui</keyname><forenames>Yan</forenames></author><author><keyname>Chen</keyname><forenames>Quan</forenames></author><author><keyname>Yang</keyname><forenames>Junfeng</forenames></author></authors><title>EOS: Automatic In-vivo Evolution of Kernel Policies for Better
  Performance</title><categories>cs.OS</categories><comments>14 pages, technique report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's monolithic kernels often implement a small, fixed set of policies
such as disk I/O scheduling policies, while exposing many parameters to let
users select a policy or adjust the specific setting of the policy. Ideally,
the parameters exposed should be flexible enough for users to tune for good
performance, but in practice, users lack domain knowledge of the parameters and
are often stuck with bad, default parameter settings.
  We present EOS, a system that bridges the knowledge gap between kernel
developers and users by automatically evolving the policies and parameters in
vivo on users' real, production workloads. It provides a simple policy
specification API for kernel developers to programmatically describe how the
policies and parameters should be tuned, a policy cache to make in-vivo tuning
easy and fast by memorizing good parameter settings for past workloads, and a
hierarchical search engine to effectively search the parameter space.
Evaluation of EOS on four main Linux subsystems shows that it is easy to use
and effectively improves each subsystem's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06366</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06366</id><created>2015-08-26</created><updated>2015-12-08</updated><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Wireless Powered Communication Networks: An Overview</title><categories>cs.NI</categories><comments>The paper has been accepted for publication by IEEE Wireless
  Communications Mgazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless powered communication network (WPCN) is a new networking paradigm
where the battery of wireless communication devices can be remotely replenished
by means of microwave wireless power transfer (WPT) technology. WPCN eliminates
the need of frequent manual battery replacement/recharging, and thus
significantly improves the performance over conventional battery-powered
communication networks in many aspects, such as higher throughput, longer
device lifetime, and lower network operating cost. However, the design and
future application of WPCN is essentially challenged by the low WPT efficiency
over long distance and the complex nature of joint wireless information and
power transfer within the same network. In this article, we provide an overview
of the key networking structures and performance enhancing techniques to build
an efficient WPCN. Besides, we point out new and challenging future research
directions for WPCN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06367</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06367</id><created>2015-08-26</created><authors><author><keyname>Kedia</keyname><forenames>Piyus</forenames></author><author><keyname>Bansal</keyname><forenames>Sorav</forenames></author></authors><title>A Software-only Mechanism for Device Passthrough and Sharing</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network processing elements in virtual machines, also known as Network
Function Virtualization (NFV) often face CPU bottlenecks at the virtualization
interface. Even highly optimized paravirtual device interfaces fall short of
the throughput requirements of modern devices. Passthrough devices, together
with SR-IOV support for multiple device virtual functions (VF) and IOMMU
support, mitigate this problem somewhat, by allowing a VM to directly control a
device partition bypassing the virtualization stack. However, device
passthrough requires high-end (expensive and power-hungry) hardware, places
scalability limits on consolidation ratios, and does not support efficient
switching between multiple VMs on the same host.
  We present a paravirtual interface that securely exposes an I/O device
directly to the guest OS running inside the VM, and yet allows that device to
be securely shared among multiple VMs and the host. Compared to the best-known
paravirtualization interfaces, our paravirtual interface supports up to 2x
higher throughput, and is closer in performance to device passthrough. Unlike
device passthrough however, we do not require SR-IOV or IOMMU support, and
allow fine-grained dynamic resource allocation, significantly higher
consolidation ratios, and seamless VM migration. Our security mechanism is
based on a novel approach called dynamic binary opcode subtraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06369</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06369</id><created>2015-08-26</created><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Ding</keyname><forenames>Zhi</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Wireless Communications in the Era of Big Data</title><categories>cs.NI</categories><comments>This article is accepted and to appear in IEEE Communications
  Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapidly growing wave of wireless data service is pushing against the
boundary of our communication network's processing power. The pervasive and
exponentially increasing data traffic present imminent challenges to all the
aspects of the wireless system design, such as spectrum efficiency, computing
capabilities and fronthaul/backhaul link capacity. In this article, we discuss
the challenges and opportunities in the design of scalable wireless systems to
embrace such a &quot;bigdata&quot; era. On one hand, we review the state-of-the-art
networking architectures and signal processing techniques adaptable for
managing the bigdata traffic in wireless networks. On the other hand, instead
of viewing mobile bigdata as a unwanted burden, we introduce methods to
capitalize from the vast data traffic, for building a bigdata-aware wireless
network with better wireless service quality and new mobile applications. We
highlight several promising future research directions for wireless
communications in the mobile bigdata era.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06370</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06370</id><created>2015-08-26</created><authors><author><keyname>Nag</keyname><forenames>Akash</forenames></author><author><keyname>Karforma</keyname><forenames>Sunil</forenames></author></authors><title>DSA Security Enhancement through Efficient Nonce Generation</title><categories>cs.CR</categories><journal-ref>Journal of Global Research in Computer Science (JGRCS). Vol.5(10).
  pp:14-19. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Digital Signature Algorithm (DSA) has become the de facto standard for
authentication of transacting entities since its inception as a standard by
NIST. An integral part of the signing process in DSA is the generation of a
random number called a nonce or an ephemeral key. If sufficient caution is not
taken while generating the nonce, it can lead to the discovery of the
private-key paving the way for critical security violations further on. The
standard algorithms for generation of the nonce as specified by NIST, as well
as the widely implemented random number generators, fail to serve as true
random sources, thus leaving the DSA algorithm open to attack, resulting in
possible signature forgery in electronic transactions, by potential attackers.
Furthermore, the user can select the nonce arbitrarily, which leads to a
subliminal channel being present to exchange messages through each signature,
which may be intolerable for security reasons. In this paper, we have improved
the security of the DSA algorithm by proposing an efficient nonce-generation
process, which ensures that the generated nonce is sufficiently random as well
as unique for each generated signature, thereby securing the signing process.
Furthermore, our algorithm also ensures that there are no subliminal channels
present in DSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06374</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06374</id><created>2015-08-26</created><updated>2015-08-27</updated><authors><author><keyname>Koplenig</keyname><forenames>Alexander</forenames></author></authors><title>A fully data-driven method to identify (correlated) changes in
  diachronic corpora</title><categories>cs.CL cs.IR stat.AP</categories><comments>typological changes only: reference-source-not-found-errors removed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method for measuring synchronic corpus (dis-)similarity put
forward by Kilgarriff (2001) is adapted and extended to identify trends and
correlated changes in diachronic text data, using the Corpus of Historical
American English (Davies 2010a) and the Google Ngram Corpora (Michel et al.
2010a). This paper shows that this fully data-driven method, which extracts
word types that have undergone the most pronounced change in frequency in a
given period of time, is computationally very cheap and that it allows
interpretations of diachronic trends that are both intuitively plausible and
motivated from the perspective of information theory. Furthermore, it
demonstrates that the method is able to identify correlated linguistic changes
and diachronic shifts that can be linked to historical events. Finally, it can
help to improve diachronic POS tagging and complement existing NLP approaches.
This indicates that the approach can facilitate an improved understanding of
diachronic processes in language change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06377</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06377</id><created>2015-08-26</created><authors><author><keyname>Xiang</keyname><forenames>Chengdi</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author></authors><title>Performance Analysis and Coherent Guaranteed Cost Control for Uncertain
  Quantum Systems Using Small Gain and Popov Methods</title><categories>cs.SY</categories><comments>9 pages, 3 figures. This paper has extended the results in the
  previous paper arXiv:1404.3884</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends applications of the quantum small gain and Popov methods
from existing results on robust stability to performance analysis results for a
class of uncertain quantum systems. This class of systems involves a nominal
linear quantum system and is subject to quadratic perturbations in the system
Hamiltonian. Based on these two methods, coherent guaranteed cost controllers
are designed for a given quantum system to achieve improved control
performance. An illustrative example also shows that the quantum Popov approach
can obtain less conservative results than the quantum small gain approach for
the same uncertain quantum system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06380</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06380</id><created>2015-08-26</created><authors><author><keyname>Saha</keyname><forenames>Suman</forenames></author><author><keyname>Ghrera</keyname><forenames>Satya P.</forenames></author></authors><title>Network Community Detection on Metric Space</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Algorithms 2015, 8(3), 680-696</journal-ref><doi>10.3390/a8030680</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Community detection in a complex network is an important problem of much
interest in recent years. In general, a community detection algorithm chooses
an objective function and captures the communities of the network by optimizing
the objective function, and then, one uses various heuristics to solve the
optimization problem to extract the interesting communities for the user. In
this article, we demonstrate the procedure to transform a graph into points of
a metric space and develop the methods of community detection with the help of
a metric defined for a pair of points. We have also studied and analyzed the
community structure of the network therein. The results obtained with our
approach are very competitive with most of the well-known algorithms in the
literature, and this is justified over the large collection of datasets. On the
other hand, it can be observed that time taken by our algorithm is quite less
compared to other methods and justifies the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06381</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06381</id><created>2015-08-26</created><authors><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>Zhao</keyname><forenames>Ming-Min</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author><author><keyname>Zhao</keyname><forenames>Min-Jian</forenames></author></authors><title>Joint Transceiver Design Algorithms for Multiuser MISO Relay Systems
  with Energy Harvesting</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a multiuser relay system with simultaneous
wireless information and power transfer. Assuming that both base station (BS)
and relay station (RS) are equipped with multiple antennas, this work studies
the joint transceiver design problem for the BS beamforming vectors, the RS
amplify-and-forward transformation matrix and the power splitting (PS) ratios
at the single-antenna receivers. Firstly, an iterative algorithm based on
alternating optimization (AO) and with guaranteed convergence is proposed to
successively optimize the transceiver coefficients. Secondly, a novel design
scheme based on switched relaying (SR) is proposed that can significantly
reduce the computational complexity and overhead of the AO based designs while
maintaining a similar performance. In the proposed SR scheme, the RS is
equipped with a codebook of permutation matrices. For each permutation matrix,
a latent transceiver is designed which consists of BS beamforming vectors,
optimally scaled RS permutation matrix and receiver PS ratios. For the given
CSI, the optimal transceiver with the lowest total power consumption is
selected for transmission. We propose a concave-convex procedure based and
subgradient-type iterative algorithms for the non-robust and robust latent
transceiver designs. Simulation results are presented to validate the
effectiveness of all the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06388</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06388</id><created>2015-08-26</created><authors><author><keyname>Qiao</keyname><forenames>Mu</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author></authors><title>Gaussian Mixture Models with Component Means Constrained in Pre-selected
  Subspaces</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a Gaussian mixture model (GMM) with component means
constrained in a pre-selected subspace. Applications to classification and
clustering are explored. An EM-type estimation algorithm is derived. We prove
that the subspace containing the component means of a GMM with a common
covariance matrix also contains the modes of the density and the class means.
This motivates us to find a subspace by applying weighted principal component
analysis to the modes of a kernel density and the class means. To circumvent
the difficulty of deciding the kernel bandwidth, we acquire multiple subspaces
from the kernel densities based on a sequence of bandwidths. The GMM
constrained by each subspace is estimated; and the model yielding the maximum
likelihood is chosen. A dimension reduction property is proved in the sense of
being informative for classification or clustering. Experiments on real and
simulated data sets are conducted to examine several ways of determining the
subspace and to compare with the reduced rank mixture discriminant analysis
(MDA). Our new method with the simple technique of spanning the subspace only
by class means often outperforms the reduced rank MDA when the subspace
dimension is very low, making it particularly appealing for visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06395</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06395</id><created>2015-08-26</created><authors><author><keyname>Bavarian</keyname><forenames>Mohammad</forenames></author><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Ito</keyname><forenames>Tsuyoshi</forenames></author></authors><title>On the Role of Shared Randomness in Simultaneous Communication</title><categories>cs.CC cs.IT math.IT</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two parties wish to carry out certain distributed computational tasks, and
they are given access to a source of correlated random bits. It allows the
parties to act in a correlated manner, which can be quite useful. But what
happens if the shared randomness is not perfect? In this work, we initiate the
study of the power of different sources of shared randomness in communication
complexity. This is done in the setting of simultaneous message passing (SMP)
model of communication complexity, which is one of the most suitable models for
studying the resource of shared randomness. Toward characterising the power of
various sources of shared randomness, we introduce a measure for the quality of
a source - we call it collision complexity. Our results show that the collision
complexity tightly characterises the power of a (shared) randomness resource in
the SMP model.
  Of independent interest is our demonstration that even the weakest sources of
shared randomness can in some cases increase the power of SMP substantially:
the equality function can be solved very efficiently with virtually any
nontrivial shared randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06410</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06410</id><created>2015-08-26</created><authors><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>A Self-Tuning Receiver-Initiated MAC Protocol for Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>4 pages, 6 figures</comments><journal-ref>IEEE Wireless Communications Letters, Vol. 4, No. 6, December
  2015. Pages 601--604</journal-ref><doi>10.1109/LWC.2015.2472398</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Receiver-initiated medium access control protocols for wireless sensor
networks are theoretically able to adapt to changing network conditions in a
distributed manner. However, existing algorithms rely on fixed beacon rates at
each receiver. We present a new received initiated MAC protocol that adapts the
beacon rate at each receiver to its actual traffic load. Our proposal uses a
computationally inexpensive formula for calculating the optimum beacon rate
that minimizes network energy consumption and, so, it can be easily adopted by
receivers. Simulation results show that our proposal reduces collisions and
diminishes delivery time maintaining a low duty cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06419</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06419</id><created>2015-08-26</created><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>CARTE</affiliation></author></authors><title>Translation-like Actions and Aperiodic Subshifts on Groups</title><categories>cs.FL math.DS math.GR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that if $G$ admits a f.g. subgroup $H$ with a
weaklyaperiodic SFT (resp. an undecidable domino problem), then $G$itself has a
weakly aperiodic SFT (resp. an undecidable domino problem).We prove that we can
replace the property &quot;$H$ is a subgroup of $G$&quot;by &quot;$H$ acts translation-like on
$G$&quot;, provided $H$ is finitely presented.In particular:* If $G\_1$ and $G\_2$
are f.g. infinite groups, then $G\_1 \times G\_2$ has a weakly aperiodic SFT
(and actually a undecidable domino problem). In particular the Grigorchuk group
has an undecidable domino problem. * Every infinite f.g. $p$-group admits a
weakly aperiodic SFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06420</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06420</id><created>2015-08-26</created><authors><author><keyname>Bir&#xf3;</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Kern</keyname><forenames>Walter</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author><author><keyname>Wojuteczky</keyname><forenames>P&#xe9;ter</forenames></author></authors><title>The Stable Fixtures Problem with Payments</title><categories>cs.GT cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize two well-known game-theoretic models by introducing multiple
partners matching games, defined by a graph $G=(N,E)$, with an integer vertex
capacity function $b$ and an edge weighting $w$. The set $N$ consists of a
number of players that are to form a set $M\subseteq E$ of 2-player coalitions
$ij$ with value $w(ij)$, such that each player $i$ is in at most $b(i)$
coalitions. A payoff is a mapping $p: N \times N \rightarrow {\mathbb R}$ with
$p(i,j)+p(j,i)=w(ij)$ if $ij\in M$ and $p(i,j)=p(j,i)=0$ if $ij\notin M$. The
pair $(M,p)$ is called a solution. A pair of players $i,j$ with $ij\in
E\setminus M$ blocks a solution $(M,p)$ if $i, j$ can form, possibly only after
withdrawing from one of their existing 2-player coalitions, a new 2-player
coalition in which they are mutually better off. A solution is stable if it has
no blocking pairs. We give a polynomial-time algorithm that either finds that
no stable solution exists, or obtains a stable solution. Previously this result
was only known for multiple partners assignment games, which correspond to the
case where $G$ is bipartite (Sotomayor, 1992) and for the case where $b\equiv
1$ (Bir\'o et al., 2012). We also characterize the set of stable solutions of a
multiple partners matching game in two different ways and perform a study on
the core of the corresponding cooperative game, where coalitions of any size
may be formed. In particular we show that the standard relation between the
existence of a stable solution and the non-emptiness of the core, which holds
in the other models with payments, is no longer valid for our (most general)
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06429</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06429</id><created>2015-08-26</created><updated>2015-12-18</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Improved Analyses of the Randomized Power Method and Block Lanczos
  Method</title><categories>cs.NA math.NA</categories><msc-class>15A18, 65F30, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power method and block Lanczos method are popular numerical algorithms
for computing the truncated singular value decomposition (SVD) and eigenvalue
decomposition problems. Especially in the literature of randomized numerical
linear algebra, the power method is widely applied to improve the quality of
randomized sketching, and relative-error bounds have been well established.
Recently, Musco &amp; Musco (2015) proposed a block Krylov subspace method that
fully exploits the intermediate results of the power iteration to accelerate
convergence. They showed spectral gap-independent bounds which are stronger
than the power method by order-of-magnitude. This paper offers novel error
analysis techniques and significantly improves the bounds of both the
randomized power method and the block Lanczos method. This paper also
establishes the first gap-independent bound for the warm-start block Lanczos
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06434</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06434</id><created>2015-08-26</created><authors><author><keyname>Benammar</keyname><forenames>Meryem</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author></authors><title>Rate-Distortion Function for a Heegard-Berger Problem with Two Sources
  and Degraded Reconstruction sets</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate an instance of the Heegard-Berger problem with
two sources and arbitrarily correlated side information sequences at two
decoders, in which the reconstruction sets at the decoders are degraded.
Specifically, two sources are to be encoded in a manner that one of the two is
reproduced losslessly by both decoders, and the other is reproduced to within
some prescribed distortion level at one of the two decoders. We establish a
single-letter characterization of the rate-distortion function for this model.
The investigation of this result in some special cases also sheds light on the
utility of joint compression of the two sources. Furthermore, we also
generalize our result to the setting in which the source component that is to
be recovered by both users is reconstructed in a lossy fashion, under the
requirement that all terminals (i.e., the encoder and both decoders) can share
an exact copy of the compressed version of this source component, i.e., a
common encoder-decoders reconstruction constraint. For this model as well, we
establish a single-letter characterization of the associated rate-distortion
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06435</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06435</id><created>2015-08-26</created><authors><author><keyname>Vlad</keyname><forenames>Valentin</forenames></author><author><keyname>Popa</keyname><forenames>Cezar D.</forenames></author><author><keyname>Turcu</keyname><forenames>Corneliu O.</forenames></author><author><keyname>Buzduga</keyname><forenames>Corneliu</forenames></author></authors><title>A solution for applying IEC 61499 function blocks in the development of
  substation automation systems</title><categories>cs.SE</categories><comments>International Journal of Computers and Communications, Volume 9, 2015
  http://www.naun.org/main/UPress/cc/2015/a082012-141.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a solution for applying IEC 61499 function blocks along
with IEC 61850 specifications in modeling and implementing control applications
for substations automation. The IEC 61499 artifacts are used for structuring
the control logic, while the IEC 61850 concepts for communication and
information exchange between the automation devices. The proposed control
architecture was implemented and validated in a simple fault protection
scenario with simulated power equipment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06440</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06440</id><created>2015-08-26</created><updated>2015-08-27</updated><authors><author><keyname>Alexandropoulos</keyname><forenames>George C.</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Maximal Ratio Transmission in Wireless Poisson Networks under Spatially
  Correlated Fading Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The downlink of a wireless network where multi-antenna base stations (BSs)
communicate with single-antenna mobile stations (MSs) using maximal ratio
transmission (MRT) is considered here. The locations of BSs are modeled by a
homogeneous Poisson point process (PPP) and the channel gains between the
multiple antennas of each BS and the single antenna of each MS are modeled as
spatially arbitrarily correlated Rayleigh random variables. We first present
novel closed-form expressions for the distribution of the power of the
interference resulting from the coexistence of one intended and one unintended
MRT over the considered correlated fading channels. The derived expressions are
then used to obtain closed-form expressions for the success probability and
area spectral efficiency of the wireless communication network under
investigation. Simulation results corroborate the validity of the presented
expressions. A key result of this work is that the effect of spatial
correlation on the network throughput may be contrasting depending on the
density of BSs, the signal-to-interference-plus-noise ratio (SINR) level, and
the background noise power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06446</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06446</id><created>2015-08-26</created><updated>2015-08-27</updated><authors><author><keyname>Tekumalla</keyname><forenames>Lavanya Sita</forenames></author><author><keyname>Agrawal</keyname><forenames>Priyanka</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Indrajit</forenames></author></authors><title>Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric
  Admixture Modeling</title><categories>stat.ML cs.LG</categories><comments>Proceedings of European Conference of Machine Learning (ECML) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixture
modeling, where the number of mixture components grows with the number of data
items. The Hierarchical Dirichlet Process (HDP), is an extension of DP for
grouped data, often used for non-parametric topic modeling, where each group is
a mixture over shared mixture densities. The Nested Dirichlet Process (nDP), on
the other hand, is an extension of the DP for learning group level
distributions from data, simultaneously clustering the groups. It allows group
level distributions to be shared across groups in a non-parametric setting,
leading to a non-parametric mixture of mixtures. The nCRF extends the nDP for
multilevel non-parametric mixture modeling, enabling modeling topic
hierarchies. However, the nDP and nCRF do not allow sharing of distributions as
required in many applications, motivating the need for multi-level
non-parametric admixture modeling. We address this gap by proposing multi-level
nested HDPs (nHDP) where the base distribution of the HDP is itself a HDP at
each level thereby leading to admixtures of admixtures at each level. Because
of couplings between various HDP levels, scaling up is naturally a challenge
during inference. We propose a multi-level nested Chinese Restaurant Franchise
(nCRF) representation for the nested HDP, with which we outline an inference
algorithm based on Gibbs Sampling. We evaluate our model with the two level
nHDP for non-parametric entity topic modeling where an inner HDP creates a
countably infinite set of topic mixtures and associates them with author
entities, while an outer HDP associates documents with these author entities.
In our experiments on two real world research corpora, the nHDP is able to
generalize significantly better than existing models and detect missing author
entities with a reasonable level of accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06451</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06451</id><created>2015-08-26</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author><author><keyname>G&#xf3;mez-Rodr&#xed;guez</keyname><forenames>Carlos</forenames></author></authors><title>Crossings as a side effect of dependency lengths</title><categories>cs.CL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic structure of sentences exhibits a striking regularity:
dependencies tend to not cross when drawn above the sentence. Here we
investigate two competing hypotheses for the origins of non-crossing
dependencies. The traditional hypothesis is that the low frequency of
dependency crossings arises from an independent principle of syntax that
reduces crossings practically to zero. An alternative to this view is the
hypothesis that crossings are a side effect of dependency lengths. According to
this view, sentences with shorter dependency lengths should tend to have fewer
crossings. We recast the traditional view as a null hypothesis where one of the
variables, i.e. the number of crossings, is mean independent of the other, i.e.
the sum of dependency lengths. The alternative view is then a positive
correlation between these two variables. In spite of the rough estimation of
dependency crossings that this sum provides, we are able to reject the
traditional view in the majority of languages considered. The alternative
hypothesis can lead to a more parsimonious theory of syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06460</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06460</id><created>2015-08-26</created><updated>2015-10-21</updated><authors><author><keyname>Hounkanli</keyname><forenames>Kokouvi</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Deterministic Broadcasting and Gossiping with Beeps</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcasting and gossiping are fundamental communication tasks in networks.
In broadcasting,one node of a network has a message that must be learned by all
other nodes. In gossiping, every node has a (possibly different) message, and
all messages must be learned by all nodes. We study these well-researched tasks
in a very weak communication model, called the {\em beeping model}.
Communication proceeds in synchronous rounds. In each round, a node can either
listen, i.e., stay silent, or beep, i.e., emit a signal. A node hears a beep in
a round, if it listens in this round and if one or more adjacent nodes beep in
this round. All nodes have different labels from the set $\{0,\dots , L-1\}$.
  Our aim is to provide fast deterministic algorithms for broadcasting and
gossiping in the beeping model. Let $N$ be an upper bound on the size of the
network and $D$ its diameter. Let $m$ be the size of the message in
broadcasting, and $M$ an upper bound on the size of all input messages in
gossiping. For the task of broadcasting we give an algorithm working in time
$O(D+m)$ for arbitrary networks, which is optimal. For the task of gossiping we
give an algorithm working in time $O(N(M+D\log L))$ for arbitrary networks.
  At the time of writing this paper we were unaware of the paper: A. Czumaj, P.
Davis, Communicating with Beeps, arxiv:1505.06107 [cs.DC] which contains the
same results for broadcasting and a stronger upper bound for gossiping in a
slightly different model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06464</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06464</id><created>2015-08-26</created><authors><author><keyname>Hirose</keyname><forenames>Osamu</forenames></author><author><keyname>Kawaguchi</keyname><forenames>Shotaro</forenames></author><author><keyname>Tokunaga</keyname><forenames>Terumasa</forenames></author><author><keyname>Toyoshima</keyname><forenames>Yu</forenames></author><author><keyname>Teramoto</keyname><forenames>Takayuki</forenames></author><author><keyname>Kuge</keyname><forenames>Sayuri</forenames></author><author><keyname>Ishihara</keyname><forenames>Takeshi</forenames></author><author><keyname>Iino</keyname><forenames>Yuichi</forenames></author><author><keyname>Yoshida</keyname><forenames>Ryo</forenames></author></authors><title>SPF-CellTracker: Tracking multiple cells with strongly-correlated moves
  using a spatial particle filter</title><categories>cs.CV</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking many cells in time-lapse 3D image sequences is an important
challenging task of bioimage informatics. Motivated by a study of brain-wide 4D
imaging of neural activity in C. elegans, we present a new method of multi-cell
tracking. Data types to which the method is applicable are characterized as
follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to
distinguish cells based only on shape and size, (iii) the number of imaged
cells ranges in several hundreds, (iv) moves of nearly-located cells are
strongly correlated and (v) cells do not divide. We developed a tracking
software suite which we call SPF-CellTracker. Incorporating dependency on
cells' moves into prediction model is the key to reduce the tracking errors:
cell-switching and coalescence of tracked positions. We model target cells'
correlated moves as a Markov random field and we also derive a fast computation
algorithm, which we call spatial particle filter. With the live-imaging data of
nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are
imaged, we demonstrate an advantage of the proposed method over the standard
particle filter and a method developed by Tokunaga et al. (2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06466</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06466</id><created>2015-08-26</created><authors><author><keyname>Zhang</keyname><forenames>Jiemeng</forenames></author><author><keyname>Guo</keyname><forenames>Yingjun</forenames></author><author><keyname>Wen</keyname><forenames>Zhixiong</forenames></author></authors><title>On the regularity of $\{\lfloor\log_b(\alpha n+\beta)\rfloor\}_{n\geq0}$</title><categories>cs.FL</categories><comments>10 pages</comments><msc-class>11B85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\alpha,\beta$ be real numbers and $b\geq2$ be an integer. Allouche and
Shallit showed that the sequence $\{\lfloor\alpha n+\beta\rfloor\}_{n\geq0}$ is
$b$-regular if and only if $\alpha$ is rational. In this paper, using a
base-independent regular language, we prove a similar result that the sequence
$\{\lfloor\log_b(\alpha n+\beta)\rfloor\}_{n\geq0}$ is $b$-regular if and only
if $\alpha$ is rational. In particular, when $\alpha=\sqrt{2},\beta=0$ and
$b=2$, we answer the question of Allouche and Shallit that the sequence
$\{\lfloor\frac{1}{2}+\log_2n\rfloor\}_{n\geq0}$ is not $2$-regular, which has
been proved by Bell, Moshe and Rowland respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06467</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06467</id><created>2015-08-26</created><authors><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Wider</keyname><forenames>Nicolas</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author></authors><title>Higher-Order Aggregate Networks in the Analysis of Temporal Networks:
  Path structures and centralities</title><categories>physics.soc-ph cs.SI</categories><comments>27 pages, 13 figures, 3 tables</comments><msc-class>05C82</msc-class><acm-class>C.2.1; G.2.2; H.1.2; H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on temporal networks has highlighted the limitations of a
static network perspective for our understanding of complex systems with
dynamic topologies. In particular, recent works have shown that i) the specific
order in which links occur in real-world temporal networks affects causality
structures and thus the evolution of dynamical processes, and ii) higher-order
aggregate representations of temporal networks can be used to analytically
study the effect of these order correlations on dynamical processes. In this
article we analyze the effect of order correlations on path-based centrality
measures in real-world temporal networks. Analyzing temporal equivalents of
betweenness, closeness and reach centrality in six empirical temporal networks,
we first show that an analysis of the commonly used static, time-aggregated
representation can give misleading results about the actual importance of
nodes. We further study higher-order time-aggregated networks, a recently
proposed generalization of the commonly applied static, time-aggregated
representation of temporal networks. Here, we particularly define path-based
centrality measures based on second-order aggregate networks, empirically
validating that node centralities calculated in this way better capture the
true temporal centralities of nodes than node centralities calculated based on
the commonly used static (first-order) representation. Apart from providing a
simple and practical method for the approximation of path-based centralities in
temporal networks, our results highlight interesting perspectives for the use
of higher-order aggregate networks in the analysis of time-stamped network
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06477</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06477</id><created>2015-08-26</created><authors><author><keyname>Rakotomamonjy</keyname><forenames>A.</forenames><affiliation>LITIS</affiliation></author><author><keyname>Ko&#xe7;o</keyname><forenames>S.</forenames></author><author><keyname>Ralaivola</keyname><forenames>L.</forenames></author></authors><title>Greedy methods, randomization approaches and multi-arm bandit algorithms
  for efficient sparsity-constrained optimization</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several sparsity-constrained algorithms such as Orthogonal Matching Pursuit
or the Frank-Wolfe algorithm with sparsity constraints work by iteratively
selecting a novel atom to add to the current non-zero set of variables. This
selection step is usually performed by computing the gradient and then by
looking for the gradient component with maximal absolute entry. This step can
be computationally expensive especially for large-scale and high-dimensional
data. In this work, we aim at accelerating these sparsity-constrained
optimization algorithms by exploiting the key observation that, for these
algorithms to work, one only needs the coordinate of the gradient's top entry.
Hence, we introduce algorithms based on greedy methods and randomization
approaches that aim at cheaply estimating the gradient and its top entry.
Another of our contribution is to cast the problem of finding the best gradient
entry as a best arm identification in a multi-armed bandit problem. Owing to
this novel insight, we are able to provide a bandit-based algorithm that
directly estimates the top entry in a very efficient way. Theoretical results
stating that the resulting inexact Frank-Wolfe or Orthogonal Matching Pursuit
algorithms act, with high probability, similarly to their exact versions are
also given. We have carried out several experiments showing that the greedy
deterministic and the bandit approaches we propose can achieve an acceleration
of an order of magnitude while being as efficient as the exact gradient when
used in algorithms such as
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06479</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06479</id><created>2015-08-26</created><updated>2015-09-10</updated><authors><author><keyname>Zhao</keyname><forenames>Yongwang</forenames></author><author><keyname>Yang</keyname><forenames>Zhibin</forenames></author><author><keyname>Sanan</keyname><forenames>David</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author></authors><title>Event-based Formalization of Safety-critical Operating System Standards:
  An Experience Report on ARINC 653 using Event-B</title><categories>cs.SE</categories><comments>12 pages</comments><acm-class>D.2.1; D.2.4; D.4.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standards play the key role in safety-critical systems. Errors in standards
could mislead system developer's understanding and introduce bugs into system
implementations. In this paper, we present an Event-B formalization and
verification for the ARINC 653 standard, which provides a standardized
interface between safety-critical real-time operating systems and application
software, as well as a set of functionalities aimed to improve the safety and
certification process of such safety-critical systems. The formalization is a
complete model of ARINC 653, and provides a necessary foundation for the formal
development and verification of ARINC 653 compliant operating systems and
applications. Six hidden errors were discovered from the verification using the
Event-B formal reasoning approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06483</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06483</id><created>2015-08-26</created><authors><author><keyname>Hamada</keyname><forenames>Naoki</forenames></author><author><keyname>Homma</keyname><forenames>Katsumi</forenames></author><author><keyname>Higuchi</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Kikuchi</keyname><forenames>Hideyuki</forenames></author></authors><title>Population Synthesis via k-Nearest Neighbor Crossover Kernel</title><categories>cs.NE</categories><comments>10 pages, 4 figures, IEEE International Conference on Data Mining
  (ICDM) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent development of multi-agent simulations brings about a need for
population synthesis. It is a task of reconstructing the entire population from
a sampling survey of limited size (1% or so), supplying the initial conditions
from which simulations begin. This paper presents a new kernel density
estimator for this task. Our method is an analogue of the classical
Breiman-Meisel-Purcell estimator, but employs novel techniques that harness the
huge degree of freedom which is required to model high-dimensional nonlinearly
correlated datasets: the crossover kernel, the k-nearest neighbor restriction
of the kernel construction set and the bagging of kernels. The performance as a
statistical estimator is examined through real and synthetic datasets. We
provide an &quot;optimization-free&quot; parameter selection rule for our method, a
theory of how our method works and a computational cost analysis. To
demonstrate the usefulness as a population synthesizer, our method is applied
to a household synthesis task for an urban micro-simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06491</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06491</id><created>2015-08-26</created><authors><author><keyname>Andreas</keyname><forenames>Jacob</forenames></author><author><keyname>Klein</keyname><forenames>Dan</forenames></author></authors><title>Alignment-based compositional semantics for instruction following</title><categories>cs.CL</categories><comments>in proceedings of EMNLP 2015</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper describes an alignment-based model for interpreting natural
language instructions in context. We approach instruction following as a search
over plans, scoring sequences of actions conditioned on structured observations
of text and the environment. By explicitly modeling both the low-level
compositional structure of individual actions and the high-level structure of
full plans, we are able to learn both grounded representations of sentence
meaning and pragmatic constraints on interpretation. To demonstrate the model's
flexibility, we apply it to a diverse set of benchmark tasks. On every task, we
outperform strong task-specific baselines, and achieve several new
state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06496</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06496</id><created>2015-08-26</created><authors><author><keyname>Zamani</keyname><forenames>Majid</forenames></author><author><keyname>Rungger</keyname><forenames>Matthias</forenames></author><author><keyname>Esfahani</keyname><forenames>Peyman Mohajerin</forenames></author></authors><title>Approximations of Stochastic Hybrid Systems: A Compositional Approach</title><categories>math.OC cs.SY</categories><comments>24 pages, 7 figures</comments><msc-class>93E03, 93A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a compositional framework for the construction of
approximations of the interconnection of a class of stochastic hybrid systems.
As special cases, this class of systems includes both jump linear stochastic
systems and linear stochastic hybrid automata. In the proposed framework, an
approximation is itself a stochastic hybrid system, which can be used as a
replacement of the original stochastic hybrid system in a controller design
process. We employ a notion of so-called stochastic simulation function to
quantify the error between the approximation and the original system. In the
first part of the paper, we derive sufficient conditions which facilitate the
compositional quantification of the error between the interconnection of
stochastic hybrid subsystems and that of their approximations using the
quantified error between the stochastic hybrid subsystems and their
corresponding approximations. In particular, we show how to construct
stochastic simulation functions for approximations of interconnected stochastic
hybrid systems using the stochastic simulation function for the approximation
of each component. In the second part of the paper, we focus on a specific
class of stochastic hybrid systems, namely, jump linear stochastic systems, and
propose a constructive scheme to determine approximations together with their
stochastic simulation functions for this class of systems. Finally, we
illustrate the effectiveness of the proposed results by constructing an
approximation of the interconnection of four jump linear stochastic subsystems
in a compositional way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06511</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06511</id><created>2015-08-26</created><authors><author><keyname>Aravind</keyname><forenames>N. R.</forenames></author><author><keyname>Joglekar</keyname><forenames>Pushkar S.</forenames></author></authors><title>On the expressive power of read-once determinants</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study the notion of read-$k$ projections of the determinant:
a polynomial $f \in \mathbb{F}[x_1, \ldots, x_n]$ is called a {\it read-$k$
projection of determinant} if $f=det(M)$, where entries of matrix $M$ are
either field elements or variables such that each variable appears at most $k$
times in $M$. A monomial set $S$ is said to be expressible as read-$k$
projection of determinant if there is a read-$k$ projection of determinant $f$
such that the monomial set of $f$ is equal to $S$. We obtain basic results
relating read-$k$ determinantal projections to the well-studied notion of
determinantal complexity. We show that for sufficiently large $n$, the $n
\times n$ permanent polynomial $Perm_n$ and the elementary symmetric
polynomials of degree $d$ on $n$ variables $S_n^d$ for $2 \leq d \leq n-2$ are
not expressible as read-once projection of determinant, whereas $mon(Perm_n)$
and $mon(S_n^d)$ are expressible as read-once projections of determinant. We
also give examples of monomial sets which are not expressible as read-once
projections of determinant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06525</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06525</id><created>2015-08-26</created><authors><author><keyname>Khoury</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Hall&#xe9;</keyname><forenames>Sylvain</forenames></author></authors><title>Runtime Enforcement With Partial Control</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study carries forward the line of enquiry that seeks to characterize
precisely which security policies are enforceable by runtime monitors. In this
regard, Basin et al.\ recently refined the structure that helps distinguish
between those actions that the monitor can potentially suppress or insert in
the execution, from those that the monitor can only observe. In this paper, we
generalize this model by organizing the universe of possible actions in a
lattice that naturally corresponds to the levels of monitor control. We then
delineate the set of properties that are enforceable under this paradigm and
relate our results to previous work in the field. Finally, we explore the set
of security policies that are enforceable if the monitor is given greater
latitude to alter the execution of its target, which allows us to reflect on
the capabilities of different types of monitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06526</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06526</id><created>2015-08-26</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>A Logical Approach to Event Handling in Imperative Languages</title><categories>cs.PL</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While event handling is a key element in modern interactive programming, it
is unfortunate that its theoretical foundation is rather weak. To solve this
problem, we propose to adopt a game-logical approach of computability logic
\cite{Jap08} to event handling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06535</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06535</id><created>2015-08-26</created><authors><author><keyname>Glauner</keyname><forenames>Patrick O.</forenames></author></authors><title>Deep Convolutional Neural Networks for Smile Recognition</title><categories>cs.CV cs.LG cs.NE</categories><comments>MSc thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis describes the design and implementation of a smile detector based
on deep convolutional neural networks. It starts with a summary of neural
networks, the difficulties of training them and new training methods, such as
Restricted Boltzmann Machines or autoencoders. It then provides a literature
review of convolutional neural networks and recurrent neural networks. In order
to select databases for smile recognition, comprehensive statistics of
databases popular in the field of facial expression recognition were generated
and are summarized in this thesis. It then proposes a model for smile
detection, of which the main part is implemented. The experimental results are
discussed in this thesis and justified based on a comprehensive model selection
performed. All experiments were run on a Tesla K40c GPU benefiting from a
speedup of up to factor 10 over the computations on a CPU. A smile detection
test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous
Facial Action (DISFA) database, significantly outperforming existing approaches
with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under
various variations, such as retaining less neutral images or only the low or
high intensities, of which the results are extensively compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06538</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06538</id><created>2015-08-24</created><updated>2016-01-19</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Schmidt</keyname><forenames>Angelika</forenames></author><author><keyname>Tegn&#xe9;r</keyname><forenames>Jesper</forenames></author></authors><title>Causality, Information and Biological Computation: An algorithmic
  software approach to life, disease and the immune system</title><categories>cs.NE cs.AI</categories><comments>30 pages, 8 figures. Invited chapter contribution to Information and
  Causality: From Matter to Life. Sara I. Walker, Paul C.W. Davies and George
  Ellis (eds.), Cambridge University Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biology has taken strong steps towards becoming a computer science aiming at
reprogramming nature after the realisation that nature herself has reprogrammed
organisms by harnessing the power of natural selection and the digital
prescriptive nature of replicating DNA. Here we further unpack ideas related to
computability, algorithmic information theory and software engineering, in the
context of the extent to which biology can be (re)programmed, and with how we
may go about doing so in a more systematic way with all the tools and concepts
offered by theoretical computer science in a translation exercise from
computing to molecular biology and back. These concepts provide a means to a
hierarchical organization thereby blurring previously clear-cut lines between
concepts like matter and life, or between tumour types that are otherwise taken
as different and may not have however a different cause. This does not diminish
the properties of life or make its components and functions less interesting.
On the contrary, this approach makes for a more encompassing and integrated
view of nature, one that subsumes observer and observed within the same system,
and can generate new perspectives and tools with which to view complex diseases
like cancer, approaching them afresh from a software-engineering viewpoint that
casts evolution in the role of programmer, cells as computing machines, DNA and
genes as instructions and computer programs, viruses as hacking devices, the
immune system as a software debugging tool, and diseases as an
information-theoretic battlefield where all these forces deploy. We show how
information theory and algorithmic programming may explain fundamental
mechanisms of life and death.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06556</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06556</id><created>2015-08-26</created><authors><author><keyname>Mahmoud</keyname><forenames>Amena</forenames></author></authors><title>The Power of the Depth of Iteration in Defining Relations by Induction</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis we study inductive definitions over finite structures,
particularly, the depth of inductive definitions. We also study infinitary
finite variable logic which contains fixed-point logic and we introduce a new
complexity measure $\textrm{FO}_{\bigvee}[f(n),g(n)]$ which counts the number,
$f(n)$, of $\vee$-symbols, and the number, $g(n)$, of variables, in first-order
formulas needed to express a given property. We prove that for $f(n)\geq
\log{n}$, $\textrm{NSPACE}[f(n)] \subseteq
\textrm{FO}_{\bigvee}[f(n)+\left(\frac{f(n)}{\log{n}}\right)^2,\frac{f(n)}{\log{n}}]$,
and that for any $f(n),g(n)$, $\textrm{FO}_{\bigvee}[f(n),g(n)]\subseteq
\textrm{DSPACE}[f(n)g(n)\log{n}]$. Also we study the expressive power of
quantifier rank and number of variables and we prove that there is a property
of words expressible with two variables and quantifier rank $2^n+2$ but not
expressible with quantifier rank $n$ with any number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06561</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06561</id><created>2015-08-25</created><authors><author><keyname>Nag</keyname><forenames>Akash</forenames></author><author><keyname>Karforma</keyname><forenames>Sunil</forenames></author></authors><title>A Space-Efficient Approach towards Distantly Homologous Protein
  Similarity Searches</title><categories>cs.CE q-bio.QM</categories><journal-ref>International Journal of Advanced Research in Computer Science
  (IJARCS). Vol.6(2). pp:19-22. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein similarity searches are a routine job for molecular biologists where
a query sequence of amino acids needs to be compared and ranked against an
ever-growing database of proteins. All available algorithms in this field can
be grouped into two categories, either solving the problem using sequence
alignment through dynamic programming, or, employing certain heuristic measures
to perform an initial screening followed by applying an optimal sequence
alignment algorithm to the closest matching candidates. While the first
approach suffers from huge time and space demands, the latter approach might
miss some protein sequences which are distantly related to the query sequence.
In this paper, we propose a heuristic pair-wise sequence alignment algorithm
that can be efficiently employed for protein database searches for moderately
sized databases. The proposed algorithm is sufficiently fast to be applicable
to database searches for short query sequences, has constant auxiliary space
requirements, produces good alignments, and is sensitive enough to return even
distantly related protein chains that might be of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06570</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06570</id><created>2015-08-26</created><updated>2015-12-15</updated><authors><author><keyname>Khalili</keyname><forenames>Shahrouz</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Uplink HARQ for Distributed and Cloud RAN via Separation of Control and
  Data Planes</title><categories>cs.IT cs.NI math.IT</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of uplink HARQ in a Distributed-Radio Access Network
(D-RAN) or Cloud-RAN (C-RAN) architecture is constrained by the two-way latency
on the fronthaul links connecting the Remote Radio Heads (RRHs) with the
Baseband Units (BBUs) that perform decoding. To overcome this limitation, this
work considers an architecture based on the separation of control and data
planes, in which retransmission decisions are made at the edge of the network,
that is, by the RRHs or User Equipments (UEs), while data decoding is carried
out remotely at the BBUs. This architecture enables low-latency local
retransmission decisions to be made at the RRHs or UEs, which are not subject
to the fronthaul latency constraints, while at the same time leveraging the
decoding capability of the BBUs. A D-RAN system is first considered in which
low-latency local feedback from the RRH assigned to a given UE is used to drive
the UE's HARQ process. Throughput and probability of error of this solution are
analyzed for the three standard HARQ modes of Type-I, Chase Combining and
Incremental Redundancy over a general fading MIMO link. Then, novel
user-centric low-latency feedback strategies are proposed and analyzed for the
C-RAN architecture based on limited &quot;hard&quot; or &quot;soft&quot; local feedback from the
RRHs to the UE and on retransmission decisions taken at the UE. The analysis
presented in this work allows the optimization of the considered schemes, as
well as the investigation of the impact of system parameters such as HARQ
protocol type, blocklength and number of antennas on the performance of
low-latency local HARQ decisions in D-RAN and C-RAN architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06574</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06574</id><created>2015-08-26</created><authors><author><keyname>Aslett</keyname><forenames>Louis J. M.</forenames></author><author><keyname>Esperan&#xe7;a</keyname><forenames>Pedro M.</forenames></author><author><keyname>Holmes</keyname><forenames>Chris C.</forenames></author></authors><title>A review of homomorphic encryption and software tools for encrypted
  statistical machine learning</title><categories>stat.ML cs.CR cs.LG</categories><comments>21 pages, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in cryptography promise to enable secure statistical
computation on encrypted data, whereby a limited set of operations can be
carried out without the need to first decrypt. We review these homomorphic
encryption schemes in a manner accessible to statisticians and machine
learners, focusing on pertinent limitations inherent in the current state of
the art. These limitations restrict the kind of statistics and machine learning
algorithms which can be implemented and we review those which have been
successfully applied in the literature. Finally, we document a high performance
R package implementing a recent homomorphic scheme in a general framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06576</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06576</id><created>2015-08-26</created><updated>2015-09-02</updated><authors><author><keyname>Gatys</keyname><forenames>Leon A.</forenames></author><author><keyname>Ecker</keyname><forenames>Alexander S.</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>A Neural Algorithm of Artistic Style</title><categories>cs.CV cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In fine art, especially painting, humans have mastered the skill to create
unique visual experiences through composing a complex interplay between the
content and style of an image. Thus far the algorithmic basis of this process
is unknown and there exists no artificial system with similar capabilities.
However, in other key areas of visual perception such as object and face
recognition near-human performance was recently demonstrated by a class of
biologically inspired vision models called Deep Neural Networks. Here we
introduce an artificial system based on a Deep Neural Network that creates
artistic images of high perceptual quality. The system uses neural
representations to separate and recombine content and style of arbitrary
images, providing a neural algorithm for the creation of artistic images.
Moreover, in light of the striking similarities between performance-optimised
artificial neural networks and biological vision, our work offers a path
forward to an algorithmic understanding of how humans create and perceive
artistic imagery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06583</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06583</id><created>2015-08-26</created><authors><author><keyname>Hounkanli</keyname><forenames>Kokouvi</forenames></author><author><keyname>Miller</keyname><forenames>Avery</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Global Synchronization and Consensus Using Beeps in a Fault-Prone MAC</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consensus is one of the fundamental tasks studied in distributed computing.
Processors have input values from some set $V$ and they have to decide the same
value from this set. If all processors have the same input value, then they
must all decide this value. We study the task of consensus in a Multiple Access
Channel (MAC) prone to faults, under a very weak communication model called the
$\mathit{beeping\ model}$. Communication proceeds in synchronous rounds. Some
processors wake up spontaneously, in possibly different rounds decided by an
adversary. In each round, an awake processor can either listen, i.e., stay
silent, or beep, i.e., emit a signal. In each round, a fault can occur in the
channel independently with constant probability $0&lt;p&lt;1$. In a fault-free round,
an awake processor hears a beep if it listens in this round and if one or more
other processors beep in this round. A processor still dormant in a fault-free
round in which some other processor beeps is woken up by this beep and hears
it. In a faulty round nothing is heard, regardless of the behaviour of the
processors.
  An algorithm working with error probability at most $\epsilon$, for a given
$\epsilon&gt;0$, is called $\epsilon$-$\mathit{safe}$. Our main result is the
design and analysis, for any constant $\epsilon&gt;0$, of a deterministic
$\epsilon$-safe consensus algorithm that works in time $O(\log w)$ in a
fault-prone MAC, where $w$ is the smallest input value of all participating
processors. We show that this time cannot be improved, even when the MAC is
fault-free. The main algorithmic tool that we develop to achieve our goal, and
that might be of independent interest, is a deterministic algorithm that, with
arbitrarily small constant error probability, establishes a global clock in a
fault-prone MAC in constant time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06585</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06585</id><created>2015-08-26</created><updated>2015-11-09</updated><authors><author><keyname>Georgiev</keyname><forenames>Galin</forenames></author></authors><title>Towards universal neural nets: Gibbs machines and ACE</title><categories>cs.CV cs.LG cs.NE</categories><acm-class>I.2.6; I.4; I.5; I.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of neural nets - \emph{Gibbs machines} - which are a type of
variational auto-encoders, designed for gradual learning. They offer an
universal platform for incrementally adding newly learned features, including
physical symmetries, and are directly connected to information geometry and
thermodynamics. Combining them with classifiers, gives rise to a brand of
universal generative neural nets - stochastic auto-classifier-encoders (ACE).
ACE have state-of-the-art performance in their class, both for classification
and density estimation for the MNIST data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06586</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06586</id><created>2015-08-26</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Carlos Pedro</forenames></author></authors><title>Financial Market Modeling with Quantum Neural Networks</title><categories>q-fin.CP cs.NE physics.soc-ph q-fin.GN</categories><msc-class>91B80, 68T05, 92B20, 81P68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Econophysics has developed as a research field that applies the formalism of
Statistical Mechanics and Quantum Mechanics to address Economics and Finance
problems. The branch of Econophysics that applies of Quantum Theory to
Economics and Finance is called Quantum Econophysics. In Finance, Quantum
Econophysics' contributions have ranged from option pricing to market dynamics
modeling, behavioral finance and applications of Game Theory, integrating the
empirical finding, from human decision analysis, that shows that nonlinear
update rules in probabilities, leading to non-additive decision weights, can be
computationally approached from quantum computation, with resulting quantum
interference terms explaining the non-additive probabilities. The current work
draws on these results to introduce new tools from Quantum Artificial
Intelligence, namely Quantum Artificial Neural Networks as a way to build and
simulate financial market models with adaptive selection of trading rules,
leading to turbulence and excess kurtosis in the returns distributions for a
wide range of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06589</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06589</id><created>2015-08-26</created><authors><author><keyname>Kalluri</keyname><forenames>Tarun</forenames></author><author><keyname>Bohara</keyname><forenames>Vivek Ashok</forenames></author></authors><title>Cooperative Spectrum Sharing Relaying Protocols With Energy Harvesting
  Cognitive User</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper has been submitted to IEEE transactions on Wireless
  communication for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of wireless information and power transfer in energy constrained
wireless networks has caught the interest of researchers due to its potential
in increasing the lifetime of sensor nodes and mitigate the environment hazards
caused by conventional cell batteries. Similarly, the advancements in areas of
cooperative spectrum sharing protocols has enabled efficient use of frequency
spectrum between a licensed primary user and a secondary user. In this paper,
we consider an energy constrained secondary user which harvests energy from the
primary signal and relays the primary signal in exchange for the spectrum
access. We consider Nakagami-m fading model and propose two key protocols,
namely time-splitting cooperative spectrum sharing (TS-CSS) and power-sharing
cooperative spectrum sharing (PS-CSS), and derive expressions for the outage
probabilities of the primary and secondary user in decode-forward and
amplify-forward relaying modes. From the obtained results, it has been shown
that the secondary user can carry its own transmission without adversely
affecting the performance of the primary user and that PS-CSS protocol
outperforms the TS-PSS protocol in terms of outage probability over a wide
range of Signal to noise ratio(SNRs). The effect of various system parameters
on the outage performance of these protocols have also been studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06594</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06594</id><created>2015-08-26</created><authors><author><keyname>Kekatos</keyname><forenames>Vassilis</forenames></author><author><keyname>Zhang</keyname><forenames>Liang</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author></authors><title>Voltage Regulation Algorithms for Multiphase Power Distribution Grids</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-varying renewable energy generation can result in serious
under-/over-voltage conditions in future distribution grids. Augmenting
conventional utility-owned voltage regulating equipment with the reactive power
capabilities of distributed generation units is a viable solution. Local
control options attaining global voltage regulation optimality at fast
convergence rates is the goal here. In this context, novel reactive power
control rules are analyzed under a unifying linearized grid model. For
single-phase grids, our proximal gradient scheme has computational complexity
comparable to that of the rule suggested by the IEEE 1547.8 standard, but it
enjoys well-characterized convergence guarantees. Adding memory to the scheme
results in accelerated convergence. For three-phase grids, it is shown that
reactive injections have a counter-intuitive effect on bus voltage magnitudes
across phases. Nevertheless, when our control scheme is applied to unbalanced
conditions, it is shown to reach an equilibrium point. Yet this point may not
correspond to the minimizer of a voltage regulation problem. Numerical tests
using the IEEE 13-bus, the IEEE 123-bus, and a Southern California Edison
47-bus feeder with increased renewable penetration verify the convergence
properties of the schemes and their resiliency to grid topology
reconfigurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06610</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06610</id><created>2015-08-26</created><authors><author><keyname>Cis&#x142;ak</keyname><forenames>Aleksander</forenames></author></authors><title>Full-text and Keyword Indexes for String Searching</title><categories>cs.DS</categories><comments>Master's thesis, 107 pages</comments><msc-class>68W32</msc-class><acm-class>H.3.3; I.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a literature review for full-text and keyword
indexes as well as our contributions (which are mostly practice-oriented).
  The first contribution is the FM-bloated index, which is a modification of
the well-known FM-index (a compressed, full-text index) that trades space for
speed. In our approach, the count table and the occurrence lists store
information about selected $q$-grams in addition to the individual characters.
Two variants are described, namely one using $O(n \log^2 n)$ bits of space with
$O(m + \log m \log \log n)$ average query time, and one with linear space and
$O(m \log \log n)$ average query time, where $n$ is the input text length and
$m$ is the pattern length. We experimentally show that a significant speedup
can be achieved by operating on $q$-grams (albeit at the cost of very high
space requirements, hence the name &quot;bloated&quot;).
  In the category of keyword indexes we present the so-called split index,
which can efficiently solve the $k$-mismatches problem, especially for 1 error.
Our implementation in the C++ language is focused mostly on data compaction,
which is beneficial for the search speed (by being cache friendly). We compare
our solution with other algorithms and we show that it is faster when the
Hamming distance is used. Query times in the order of 1 microsecond were
reported for one mismatch for a few-megabyte natural language dictionary on a
medium-end PC.
  A minor contribution includes string sketches which aim to speed up
approximate string comparison at the cost of additional space ($O(1)$ per
string). They can be used in the context of keyword indexes in order to deduce
that two strings differ by at least $k$ mismatches with the use of fast bitwise
operations rather than an explicit verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06613</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06613</id><created>2015-08-26</created><authors><author><keyname>Bersani</keyname><forenames>Marcello M.</forenames></author><author><keyname>Bianculli</keyname><forenames>Domenico</forenames></author><author><keyname>Ghezzi</keyname><forenames>Carlo</forenames></author><author><keyname>Krstic</keyname><forenames>Srdan</forenames></author><author><keyname>Pietro</keyname><forenames>Pierluigi San</forenames></author></authors><title>Efficient Large-scale Trace Checking Using MapReduce</title><categories>cs.SE cs.LO</categories><comments>13 pages, 8 figures</comments><msc-class>68N30</msc-class><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of checking a logged event trace against a temporal logic
specification arises in many practical cases. Unfortunately, known algorithms
for an expressive logic like MTL (Metric Temporal Logic) do not scale with
respect to two crucial dimensions: the length of the trace and the size of the
time interval for which logged events must be buffered to check satisfaction of
the specification. The former issue can be addressed by distributed and
parallel trace checking algorithms that can take advantage of modern cloud
computing and programming frameworks like MapReduce. Still, the latter issue
remains open with current state-of-the-art approaches.
  In this paper we address this memory scalability issue by proposing a new
semantics for MTL, called lazy semantics. This semantics can evaluate temporal
formulae and boolean combinations of temporal-only formulae at any arbitrary
time instant. We prove that lazy semantics is more expressive than standard
point-based semantics and that it can be used as a basis for a correct
parametric decomposition of any MTL formula into an equivalent one with
smaller, bounded time intervals. We use lazy semantics to extend our previous
distributed trace checking algorithm for MTL. We evaluate the proposed
algorithm in terms of memory scalability and time/memory tradeoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06614</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06614</id><created>2015-08-26</created><authors><author><keyname>Sharaf-Dabbagh</keyname><forenames>Yaman</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author></authors><title>Transfer Learning for Device Fingerprinting with Application to
  Cognitive Radio Networks</title><categories>cs.CR</categories><comments>6 pages, 3 figures, in Proceedings of IEEE 26th International
  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), Hong
  Kong, P.R. China, Aug. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Primary user emulation (PUE) attacks are an emerging threat to cognitive
radio (CR) networks in which malicious users imitate the primary users (PUs)
signals to limit the access of secondary users (SUs). Ascertaining the identity
of the devices is a key technical challenge that must be overcome to thwart the
threat of PUE attacks. Typically, detection of PUE attacks is done by
inspecting the signals coming from all the devices in the system, and then
using these signals to form unique fingerprints for each device. Current
detection and fingerprinting approaches require certain conditions to hold in
order to effectively detect attackers. Such conditions include the need for a
sufficient amount of fingerprint data for users or the existence of both the
attacker and the victim PU within the same time frame. These conditions are
necessary because current methods lack the ability to learn the behavior of
both SUs and PUs with time. In this paper, a novel transfer learning (TL)
approach is proposed, in which abstract knowledge about PUs and SUs is
transferred from past time frames to improve the detection process at future
time frames. The proposed approach extracts a high level representation for the
environment at every time frame. This high level information is accumulated to
form an abstract knowledge database. The CR system then utilizes this database
to accurately detect PUE attacks even if an insufficient amount of fingerprint
data is available at the current time frame. The dynamic structure of the
proposed approach uses the final detection decisions to update the abstract
knowledge database for future runs. Simulation results show that the proposed
method can improve the performance with an average of 3.5% for only 10%
relevant information between the past knowledge and the current environment
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06615</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06615</id><created>2015-08-26</created><updated>2015-12-01</updated><authors><author><keyname>Kim</keyname><forenames>Yoon</forenames></author><author><keyname>Jernite</keyname><forenames>Yacine</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author><author><keyname>Rush</keyname><forenames>Alexander M.</forenames></author></authors><title>Character-Aware Neural Language Models</title><categories>cs.CL cs.NE stat.ML</categories><comments>AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a simple neural language model that relies only on
character-level inputs. Predictions are still made at the word-level. Our model
employs a convolutional neural network (CNN) and a highway network over
characters, whose output is given to a long short-term memory (LSTM) recurrent
neural network language model (RNN-LM). On the English Penn Treebank the model
is on par with the existing state-of-the-art despite having 60% fewer
parameters. On languages with rich morphology (Arabic, Czech, French, German,
Spanish, Russian), the model outperforms word-level/morpheme-level LSTM
baselines, again with fewer parameters. The results suggest that on many
languages, character inputs are sufficient for language modeling. Analysis of
word representations obtained from the character composition part of the model
reveals that the model is able to encode, from characters only, both semantic
and orthographic information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06624</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06624</id><created>2015-08-26</created><authors><author><keyname>Li</keyname><forenames>Ke</forenames></author></authors><title>Discriminating quantum states: the multiple Chernoff distance</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of testing multiple quantum hypotheses
$\{\rho_1^{\otimes n},\ldots,\rho_r^{\otimes n}\}$, where an arbitrary prior
distribution is given and each of the $r$ hypotheses is $n$ copies of a quantum
state. It is known that the average error probability $P_e$ decays
exponentially to zero, that is, $P_e\thicksim\exp(-\xi n)$. However, this error
exponent $\xi$ is generally unknown, except for the case that $r=2$.
  In this paper, we solve the long-standing open problem of identifying the
above error exponent, by proving Nussbaum and Szko\l a's conjecture that
$\xi=\min_{i\neq j}C(\rho_i,\rho_j)$. The right-hand side of this equality is
called the multiple quantum Chernoff distance, and
$C(\rho_i,\rho_j):=\max_{0\leq s\leq
1}\{-\log\operatorname{Tr}\rho_i^s\rho_j^{1-s}\}$ has been previously
identified as the optimal error exponent for testing two hypotheses,
$\rho_i^{\otimes n}$ versus $\rho_j^{\otimes n}$.
  The main ingredient of our proof is a new upper bound for the average error
probability, for testing an ensemble of finite-dimensional, but otherwise
general, quantum states. This upper bound, up to a states-dependent factor,
matches the multiple-state generalization of Nussbaum and Szko\l a's lower
bound. Specialized to the case $r=2$, we give an alternative proof to the
achievability of the binary-hypothesis Chernoff distance, which was originally
proved by Audenaert et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06655</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06655</id><created>2015-08-26</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Pajaziti</keyname><forenames>Arianit</forenames></author></authors><title>Tapping Into the Wells of Social Energy: A Case Study Based on Falls
  Identification</title><categories>cs.CY</categories><comments>Submitted to the 17th Int.l Conf. on Information Integration and
  Web-based Applications &amp; Services (iiWAS2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are purely technological solutions the best answer we can get to the
shortcomings our organizations are often experiencing today? The results we
gathered in this work lead us to giving a negative answer to such question.
Science and technology are powerful boosters, though when they are applied to
the &quot;local, static organization of an obsolete yesterday&quot; they fail to
translate in the solutions we need to our problems. Our stance here is that
those boosters should be applied to novel, distributed, and dynamic models able
to allow us to escape from the local minima our societies are currently locked
in. One such model is simulated in this paper to demonstrate how it may be
possible to tap into the vast basins of social energy of our human societies to
realize ubiquitous computing sociotechnical services for the identification and
timely response to falls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06656</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06656</id><created>2015-08-26</created><authors><author><keyname>Dai</keyname><forenames>Yongyu</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author></authors><title>Power Allocation for Multi-Pair Massive MIMO Two-Way AF Relaying with
  Robust Linear Processing</title><categories>cs.IT math.IT</categories><comments>14 pages, double column, 9 figure. Submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multi-pair two-way amplify-and-forward (AF)
relaying system where multiple sources exchange information via a relay node
equipped with large-scale arrays. Supposing that channel estimation is
non-ideal, and that the relay employs either maximum-ratio
combining/maximum-ratio transmission (MRC/MRT) or zero-forcing
reception/zero-forcing transmission (ZFR/ZFT) beamforming to process the
signals, we derive two corresponding closed-form lower bound expressions for
the ergodic achievable rate of each pair. The closed-form expressions enable us
to design optimal power allocation (OPA) schemes by formulating different
optimization problems to improve system performance, such as minimizing the
total user power consumption to prolong the lifetime of battery-powered
devices, considering fairness among different users by adopting the max-min
achievable rate criterion, or maximizing the sum spectral efficiency, under
certain practical constraints. Our proposed OPA schemes are based on either
linear or geometric programming, which can be solved by optimization tools. The
derived closed-form expressions for the achievable rate are verified to be
accurate predictors of the system performance by Monte-Carlo simulations.
Furthermore, numerical results demonstrate the effectiveness of the proposed
OPAs in comparison to the equal power allocation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06669</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06669</id><created>2015-08-26</created><authors><author><keyname>Li</keyname><forenames>Yanran</forenames></author><author><keyname>Li</keyname><forenames>Wenjie</forenames></author><author><keyname>Sun</keyname><forenames>Fei</forenames></author><author><keyname>Li</keyname><forenames>Sujian</forenames></author></authors><title>Component-Enhanced Chinese Character Embeddings</title><categories>cs.CL</categories><comments>6 pages, 2 figures, conference, EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed word representations are very useful for capturing semantic
information and have been successfully applied in a variety of NLP tasks,
especially on English. In this work, we innovatively develop two
component-enhanced Chinese character embedding models and their bigram
extensions. Distinguished from English word embeddings, our models explore the
compositions of Chinese characters, which often serve as semantic indictors
inherently. The evaluations on both word similarity and text classification
demonstrate the effectiveness of our models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06686</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06686</id><created>2015-08-26</created><updated>2016-01-29</updated><authors><author><keyname>Mankad</keyname><forenames>Shawn</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>Analysis of multiview legislative networks with structured matrix
  factorization: Does Twitter influence translate to the real world?</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>Published at http://dx.doi.org/10.1214/15-AOAS858 in the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS858</report-no><journal-ref>Annals of Applied Statistics 2015, Vol. 9, No. 4, 1950-1972</journal-ref><doi>10.1214/15-AOAS858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of social media platforms has fundamentally altered the public
discourse by providing easy to use and ubiquitous forums for the exchange of
ideas and opinions. Elected officials often use such platforms for
communication with the broader public to disseminate information and engage
with their constituencies and other public officials. In this work, we
investigate whether Twitter conversations between legislators reveal their
real-world position and influence by analyzing multiple Twitter networks that
feature different types of link relations between the Members of Parliament
(MPs) in the United Kingdom and an identical data set for politicians within
Ireland. We develop and apply a matrix factorization technique that allows the
analyst to emphasize nodes with contextual local network structures by
specifying network statistics that guide the factorization solution. Leveraging
only link relation data, we find that important politicians in Twitter networks
are associated with real-world leadership positions, and that rankings from the
proposed method are correlated with the number of future media headlines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06705</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06705</id><created>2015-08-26</created><authors><author><keyname>Kinnison</keyname><forenames>Jeff</forenames></author><author><keyname>Remy</keyname><forenames>Sekou L.</forenames></author></authors><title>Using Genetic Algorithms to Benchmark the Cloud</title><categories>cs.DC cs.NE cs.PF</categories><report-no>Sp2015M03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel application of Genetic Algorithms(GAs) to
quantify the performance of Platform as a Service (PaaS), a cloud service model
that plays a critical role in both industry and academia. While Cloud
benchmarks are not new, in this novel concept, the authors use a GA to take
advantage of the elasticity in Cloud services in a graceful manner that was not
previously possible. Using Google App Engine, Heroku, and Python Anywhere with
three distinct classes of client computers running our GA codebase, we
quantified the completion time for application of the GA to search for the
parameters of controllers for dynamical systems. Our results show statistically
significant differences in PaaS performance by vendor, and also that the
performance of the PaaS performance is dependent upon the client that uses it.
Results also show the effectiveness of our GA in determining the level of
service of PaaS providers, and for determining if the level of service of one
PaaS vendor is repeatable with another. Such a concept could then increase the
appeal of PaaS Cloud services by making them more financially appealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06707</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06707</id><created>2015-08-26</created><authors><author><keyname>Dardha</keyname><forenames>Ornela</forenames><affiliation>University of Glasgow, United Kingdom</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames><affiliation>University of Groningen, The Netherlands</affiliation></author></authors><title>Comparing Deadlock-Free Session Typed Processes</title><categories>cs.LO cs.PL</categories><comments>In Proceedings EXPRESS/SOS 2015, arXiv:1508.06347</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 190, 2015, pp. 1-15</journal-ref><doi>10.4204/EPTCS.190.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides respecting prescribed protocols, communication-centric systems should
never &quot;get stuck&quot;. This requirement has been expressed by liveness properties
such as progress or (dead)lock freedom. Several typing disciplines that ensure
these properties for mobile processes have been proposed. Unfortunately, very
little is known about the precise relationship between these disciplines--and
the classes of typed processes they induce.
  In this paper, we compare L and K, two classes of deadlock-free, session
typed concurrent processes. The class L stands out for its canonicity: it
results naturally from interpretations of linear logic propositions as session
types. The class K, obtained by encoding session types into Kobayashi's usage
types, includes processes not typable in other type systems.
  We show that L is strictly included in K. We also identify the precise
condition under which L and K coincide. One key observation is that the degree
of sharing between parallel processes determines a new expressiveness hierarchy
for typed processes. We also provide a type-preserving rewriting procedure of
processes in K into processes in L. This procedure suggests that, while
effective, the degree of sharing is a rather subtle criteria for distinguishing
typed processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06708</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06708</id><created>2015-08-26</created><authors><author><keyname>Li</keyname><forenames>Sijin</forenames></author><author><keyname>Zhang</keyname><forenames>Weichen</forenames></author><author><keyname>Chan</keyname><forenames>Antoni B.</forenames></author></authors><title>Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose
  Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on structured-output learning using deep neural networks
for 3D human pose estimation from monocular images. Our network takes an image
and 3D pose as inputs and outputs a score value, which is high when the
image-pose pair matches and low otherwise. The network structure consists of a
convolutional neural network for image feature extraction, followed by two
sub-networks for transforming the image features and pose into a joint
embedding. The score function is then the dot-product between the image and
pose embeddings. The image-pose embedding and score function are jointly
trained using a maximum-margin cost function. Our proposed framework can be
interpreted as a special form of structured support vector machines where the
joint feature space is discriminatively learned using deep neural networks. We
test our framework on the Human3.6m dataset and obtain state-of-the-art results
compared to other recent methods. Finally, we present visualizations of the
image-pose embedding space, demonstrating the network has learned a high-level
embedding of body-orientation and pose-configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06709</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06709</id><created>2015-08-26</created><authors><author><keyname>Dedei&#x107;</keyname><forenames>Jovana</forenames><affiliation>University of Novi Sad, Serbia</affiliation></author><author><keyname>Pantovi&#x107;</keyname><forenames>Jovanka</forenames><affiliation>University of Novi Sad, Serbia</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames><affiliation>University of Groningen, The Netherlands</affiliation></author></authors><title>On Compensation Primitives as Adaptable Processes</title><categories>cs.LO cs.PL</categories><comments>In Proceedings EXPRESS/SOS 2015, arXiv:1508.06347</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 190, 2015, pp. 16-30</journal-ref><doi>10.4204/EPTCS.190.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare mechanisms for compensation handling and dynamic update in calculi
for concurrency. These mechanisms are increasingly relevant in the
specification of reliable communicating systems. Compensations and updates are
intuitively similar: both specify how the behavior of a concurrent system
changes at runtime in response to an exceptional event. However, calculi with
compensations and updates are technically quite different. We investigate the
relative expressiveness of these calculi: we develop encodings of core process
languages with compensations into a calculus of adaptable processes developed
in prior work. Our encodings shed light on the (intricate) semantics of
compensation handling and its key constructs. They also enable the transference
of existing verification and reasoning techniques for adaptable processes to
core languages with compensation handling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06710</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06710</id><created>2015-08-26</created><authors><author><keyname>D'Argenio</keyname><forenames>Pedro R.</forenames><affiliation>FaMAF, Universidad Nacional de C&#xf3;rdoba - CONICET</affiliation></author><author><keyname>Lee</keyname><forenames>Matias David</forenames><affiliation>FaMAF, Universidad Nacional de C&#xf3;rdoba - CONICET</affiliation></author><author><keyname>Gebler</keyname><forenames>Daniel</forenames><affiliation>Department of Computer Science, VU University Amsterdam</affiliation></author></authors><title>SOS rule formats for convex and abstract probabilistic bisimulations</title><categories>cs.LO cs.PL</categories><comments>In Proceedings EXPRESS/SOS 2015, arXiv:1508.06347</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 190, 2015, pp. 31-45</journal-ref><doi>10.4204/EPTCS.190.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic transition system specifications (PTSSs) in the $nt \mu f\theta
/ nt\mu x\theta$ format provide structural operational semantics for
Segala-type systems that exhibit both probabilistic and nondeterministic
behavior and guarantee that bisimilarity is a congruence for all operator
defined in such format. Starting from the $nt \mu f\theta / nt\mu x\theta$
format, we obtain restricted formats that guarantee that three coarser
bisimulation equivalences are congruences. We focus on (i) Segala's variant of
bisimulation that considers combined transitions, which we call here &quot;convex
bisimulation&quot;; (ii) the bisimulation equivalence resulting from considering
Park &amp; Milner's bisimulation on the usual stripped probabilistic transition
system (translated into a labelled transition system), which we call here
&quot;probability obliterated bisimulation&quot;; and (iii) a &quot;probability abstracted
bisimulation&quot;, which, like bisimulation, preserves the structure of the
distributions but instead, it ignores the probability values. In addition, we
compare these bisimulation equivalences and provide a logic characterization
for each of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06711</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06711</id><created>2015-08-26</created><authors><author><keyname>Peters</keyname><forenames>Kirstin</forenames></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author></authors><title>Analysing and Comparing Encodability Criteria</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2015, arXiv:1508.06347. The Isabelle/HOL
  source files, and a full proof document, are available in the Archive of
  Formal Proofs, at
  http://afp.sourceforge.net/entries/Encodability_Process_Calculi.shtml</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 190, 2015, pp. 46-60</journal-ref><doi>10.4204/EPTCS.190.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encodings or the proof of their absence are the main way to compare process
calculi. To analyse the quality of encodings and to rule out trivial or
meaningless encodings, they are augmented with quality criteria. There exists a
bunch of different criteria and different variants of criteria in order to
reason in different settings. This leads to incomparable results. Moreover it
is not always clear whether the criteria used to obtain a result in a
particular setting do indeed fit to this setting. We show how to formally
reason about and compare encodability criteria by mapping them on requirements
on a relation between source and target terms that is induced by the encoding
function. In particular we analyse the common criteria full abstraction,
operational correspondence, divergence reflection, success sensitiveness, and
respect of barbs; e.g. we analyse the exact nature of the simulation relation
(coupled simulation versus bisimulation) that is induced by different variants
of operational correspondence. This way we reduce the problem of analysing or
comparing encodability criteria to the better understood problem of comparing
relations on processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06712</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06712</id><created>2015-08-26</created><authors><author><keyname>Hatzel</keyname><forenames>Meike</forenames></author><author><keyname>Wagner</keyname><forenames>Christoph</forenames></author><author><keyname>Peters</keyname><forenames>Kirstin</forenames></author><author><keyname>Nestmann</keyname><forenames>Uwe</forenames></author></authors><title>Encoding CSP into CCS</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2015, arXiv:1508.06347</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 190, 2015, pp. 61-75</journal-ref><doi>10.4204/EPTCS.190.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study encodings from CSP into asynchronous CCS with name passing and
matching, so in fact, the asynchronous pi-calculus. By doing so, we discuss two
different ways to map the multi-way synchronisation mechanism of CSP into the
two-way synchronisation mechanism of CCS. Both encodings satisfy the criteria
of Gorla except for compositionality, as both use an additional top-level
context. Following the work of Parrow and Sj\&quot;odin, the first encoding uses a
centralised coordinator and establishes a variant of weak bisimilarity between
source terms and their translations. The second encoding is decentralised, and
thus more efficient, but ensures only a form of coupled similarity between
source terms and their translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06713</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06713</id><created>2015-08-26</created><authors><author><keyname>Rowe</keyname><forenames>Reuben N. S.</forenames><affiliation>UCL</affiliation></author></authors><title>Encoding the Factorisation Calculus</title><categories>cs.LO</categories><comments>In Proceedings EXPRESS/SOS 2015, arXiv:1508.06347</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 190, 2015, pp. 76-90</journal-ref><doi>10.4204/EPTCS.190.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jay and Given-Wilson have recently introduced the Factorisation (or SF-)
calculus as a minimal fundamental model of intensional computation. It is a
combinatory calculus containing a special combinator, F, which is able to
examine the internal structure of its first argument. The calculus is
significant in that as well as being combinatorially complete it also exhibits
the property of structural completeness, i.e. it is able to represent any
function on terms definable using pattern matching on arbitrary normal forms.
In particular, it admits a term that can decide the structural equality of any
two arbitrary normal forms.
  Since SF-calculus is combinatorially complete, it is clearly at least as
powerful as the more familiar and paradigmatic Turing-powerful computational
models of Lambda Calculus and Combinatory Logic. Its relationship to these
models in the converse direction is less obvious, however. Jay and Given-Wilson
have suggested that SF-calculus is strictly more powerful than the
aforementioned models, but a detailed study of the connections between these
models is yet to be undertaken.
  This paper begins to bridge that gap by presenting a faithful encoding of the
Factorisation Calculus into the Lambda Calculus preserving both reduction and
strong normalisation. The existence of such an encoding is a new result. It
also suggests that there is, in some sense, an equivalence between the former
model and the latter. We discuss to what extent our result constitutes an
equivalence by considering it in the context of some previously defined
frameworks for comparing computational power and expressiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06717</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06717</id><created>2015-08-26</created><authors><author><keyname>Maurya</keyname><forenames>Chandresh Kumar</forenames></author><author><keyname>Toshniwal</keyname><forenames>Durga</forenames></author><author><keyname>Venkoparao</keyname><forenames>Gopalan Vijendran</forenames></author></authors><title>Online Anomaly Detection via Class-Imbalance Learning</title><categories>cs.LG</categories><comments>This paper is accepted for publication in IC3 2015, Jaypee Noida</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection is an important task in many real world applications such
as fraud detection, suspicious activity detection, health care monitoring etc.
In this paper, we tackle this problem from supervised learning perspective in
online learning setting. We maximize well known \emph{Gmean} metric for
class-imbalance learning in online learning framework. Specifically, we show
that maximizing \emph{Gmean} is equivalent to minimizing a convex surrogate
loss function and based on that we propose novel online learning algorithm for
anomaly detection. We then show, by extensive experiments, that the performance
of the proposed algorithm with respect to $sum$ metric is as good as a recently
proposed Cost-Sensitive Online Classification(CSOC) algorithm for
class-imbalance learning over various benchmarked data sets while keeping
running time close to the perception algorithm. Our another conclusion is that
other competitive online algorithms do not perform consistently over data sets
of varying size. This shows the potential applicability of our proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06721</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06721</id><created>2015-08-27</created><authors><author><keyname>Karim</keyname><forenames>Mohammad S.</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Network Coding for Video Distortion Reduction in Device-to-Device
  Communications</title><categories>cs.NI</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of distributing a real-time video
sequence to a group of partially connected cooperative wireless devices using
instantly decodable network coding (IDNC). In such a scenario, the coding
conflicts occur to service multiple devices with an immediately decodable
packet and the transmission conflicts occur from simultaneous transmissions of
multiple devices. To avoid these conflicts, we introduce a novel IDNC graph
that represents all feasible coding and transmission conflict-free decisions in
one unified framework. Moreover, a real-time video sequence has a hard deadline
and unequal importance of video packets. Using these video characteristics and
the new IDNC graph, we formulate the problem of minimizing the mean video
distortion before the deadline as a finite horizon Markov decision process
(MDP) problem. However, the backward induction algorithm that finds the optimal
policy of the MDP formulation has high modelling and computational
complexities. To reduce these complexities, we further design a two-stage
maximal independent set selection algorithm, which can efficiently reduce the
mean video distortion before the deadline. Simulation results over a real video
sequence show that our proposed IDNC algorithms improve the received video
quality compared to the existing IDNC algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06724</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06724</id><created>2015-08-27</created><authors><author><keyname>Nabeshima</keyname><forenames>Katsusuke</forenames></author><author><keyname>Tajima</keyname><forenames>Shinichi</forenames></author></authors><title>Algebraic Local Cohomology with Parameters and Parametric Standard Bases
  for Zero-Dimensional Ideals</title><categories>cs.SC</categories><comments>31 pages</comments><msc-class>13D45, 32C37, 13J05, 32A27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computation method of algebraic local cohomology with parameters,
associated with zero-dimensional ideal with parameter, is introduced. This
computation method gives us in particular a decomposition of the parameter
space depending on the structure of algebraic local cohomology classes. This
decomposition informs us several properties of input ideals and the output of
our algorithm completely describes the multiplicity structure of input ideals.
An efficient algorithm for computing a parametric standard basis of a given
zero-dimensional ideal, with respect to an arbitrary local term order, is also
described as an application of the computation method. The algorithm can always
output &quot;reduced&quot; standard basis of a given zero-dimensional ideal, even if the
zero-dimensional ideal has parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06725</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06725</id><created>2015-08-27</created><authors><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Han</keyname><forenames>Yan-bin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-lin</forenames></author></authors><title>Image Type Water Meter Character Recognition Based on Embedded DSP</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, we combined DSP processor with image processing algorithm and
studied the method of water meter character recognition. We collected water
meter image through camera at a fixed angle, and the projection method is used
to recognize those digital images. The experiment results show that the method
can recognize the meter characters accurately and artificial meter reading is
replaced by automatic digital recognition, which improves working efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06728</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06728</id><created>2015-08-27</created><authors><author><keyname>Sardey</keyname><forenames>Mohini P.</forenames></author><author><keyname>Kharate</keyname><forenames>G. K.</forenames></author></authors><title>A Comparative Analysis of Retrieval Techniques In Content Based Image
  Retrieval</title><categories>cs.CV</categories><comments>9 pages, 4 figures, 2 tables</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Basic group of visual techniques such as color, shape, texture are used in
Content Based Image Retrievals (CBIR) to retrieve query image or subregion of
image to find similar images in image database. To improve query result,
relevance feedback is used many times in CBIR to help user to express their
preference and improve query results.In this paper, a new approach for image
retrieval is proposed which is based on the features such as Color Histogram,
Eigen Values and Match Point. Images from various types of database are first
identified by using edge detection techniques.Once the image is identified,
then the image is searched in the particular database, then all related images
are displayed. This will save the retrieval time. Further to retrieve the
precise query image, any of the three techniques are used and comparison is
done w.r.t. average retrieval time. Eigen value technique found to be the best
as compared with other two techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06731</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06731</id><created>2015-08-27</created><authors><author><keyname>Amaxilatis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Logaras</keyname><forenames>Marios</forenames></author><author><keyname>Michail</keyname><forenames>Othon</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>NETCS: A New Simulator of Population Protocols and Network Constructors</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Constructors are an extension of the standard population protocol
model in which finite-state agents interact in pairs under the control of an
adversary scheduler. In this work we present NETCS, a simulator designed to
evaluate the performance of various network constructors and population
protocols under different schedulers and network configurations. Our simulator
provides researchers with an intuitive user interface and a quick
experimentation environment to evaluate their work. It also harnesses the power
of the cloud, as experiments are executed remotely and scheduled through the
web interface provided. To prove the validity and quality of our simulator we
provide an extensive evaluation of multiple protocols with more than 100000
experiments for different network sizes and configurations that validate the
correctness of the theoretical analysis of existing protocols and estimate the
real values of the hidden asymptotic coefficients. We also show experimentally
(with more than 40000 experiments) that a probabilistic algorithm is capable of
counting the actual size of the network in bounded time given a unique leader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06738</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06738</id><created>2015-08-27</created><authors><author><keyname>Chan</keyname><forenames>Wai Hong Ronald</forenames></author><author><keyname>Wildemeersch</keyname><forenames>Matthias</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Characterization and Control of Diffusion Processes in Multi-Agent
  Networks</title><categories>cs.SI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion processes are instrumental to describe the movement of a continuous
quantity in a generic network of interacting agents. Here, we present a
probabilistic framework for diffusion in networks and propose to classify agent
interactions according to two protocols where the total network quantity is
conserved or variable. For both protocols, our focus is on asymmetric
interactions between agents involving directed graphs. Specifically, we define
how the dynamics of conservative and non-conservative networks relate to the
weighted in-degree Laplacian and the weighted out-degree Laplacian. Our
framework allows the addition and subtraction of the considered quantity to and
from a set of nodes. This enables the modeling of stubborn agents with
time-invariant quantities, and the process of dynamic learning. We highlight
several stability and convergence characteristics of our framework, and define
the conditions under which asymptotic convergence is guaranteed when the
network topology is variable. In addition, we indicate how our framework
accommodates external network control and targeted network design. We show how
network diffusion can be externally manipulated by applying time-varying input
functions at individual nodes. Desirable network structures can also be
constructed by adjusting the dominant diffusion modes. To this purpose, we
propose a Markov decision process that learns these network adjustments through
a reinforcement learning algorithm, suitable for large networks. The presented
network control and design schemes enable flow modifications that allow the
alteration of the dynamic and stationary behavior of the network in
conservative and non-conservative networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06746</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06746</id><created>2015-08-27</created><authors><author><keyname>Lee</keyname><forenames>Sang-Rim</forenames></author><author><keyname>Jung</keyname><forenames>Jaehoon</forenames></author><author><keyname>Park</keyname><forenames>Haewook</forenames></author><author><keyname>Lee</keyname><forenames>Inkyu</forenames></author></authors><title>A New Energy Efficient Beamforming Strategy for MISO Interfering
  Broadcast Channels based on Large Systems Analysis</title><categories>cs.IT math.IT</categories><comments>25pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new beamforming design to maximize energy
efficiency (EE) for multiple input single output interfering broadcast channels
(IFBC). Under this model, the EE problem is non-convex in general due to the
coupled interference and its fractional form, and thus it is difficult to solve
the problem. Conventional algorithms which address this problem have adopted an
iterative method for each channel realization, which requires high
computational complexity. In order to reduce the computational complexity, we
parameterize the beamforming vector by scalar parameters related to beam
direction and power. Then, by employing asymptotic results of random matrix
theory with this parametrization, we identify the optimal parameters to
maximize the EE in the large system limit assuming that the number of transmit
antennas and users are large with a fixed ratio. In the asymptotic regime, our
solutions depend only on the second order channel statistics, which yields
significantly reduced computational complexity and system overhead compared to
the conventional approaches. Hence, the beamforming vector to maximize the EE
performance can be determined with local channel state information and the
optimized parameters. Based on the asymptotic results, the proposed scheme can
provide insights on the average EE performance, and a simple yet efficient
beamforming strategy is introduced for the finite system case. Numerical
results confirm that the proposed scheme shows a negligible performance loss
compared to the best result achieved by the conventional approaches even with
small system dimensions, with much reduced system complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06754</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06754</id><created>2015-08-27</created><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author></authors><title>Factorizations of the Fibonacci Infinite Word</title><categories>cs.FL cs.DM math.CO</categories><msc-class>68R15</msc-class><journal-ref>Published on Journal of Integer Sequences, Vol. 18 (2015), Article
  15.9.3</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this note is to survey the factorizations of the Fibonacci
infinite word that make use of the Fibonacci words and other related words, and
to show that all these factorizations can be easily derived in sequence
starting from elementary properties of the Fibonacci numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06760</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06760</id><created>2015-08-27</created><authors><author><keyname>Bruckdorfer</keyname><forenames>Till</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author></authors><title>On Embeddability of Buses in Point Sets</title><categories>cs.CG</categories><comments>19 pages, 9 figures, conference version at GD 2015</comments><msc-class>05C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set membership of points in the plane can be visualized by connecting
corresponding points via graphical features, like paths, trees, polygons,
ellipses. In this paper we study the \emph{bus embeddability problem} (BEP):
given a set of colored points we ask whether there exists a planar realization
with one horizontal straight-line segment per color, called bus, such that all
points with the same color are connected with vertical line segments to their
bus. We present an ILP and an FPT algorithm for the general problem. For
restricted versions of this problem, such as when the relative order of buses
is predefined, or when a bus must be placed above all its points, we provide
efficient algorithms. We show that another restricted version of the problem
can be solved using 2-stack pushall sorting. On the negative side we prove the
NP-completeness of a special case of BEP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06775</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06775</id><created>2015-08-27</created><authors><author><keyname>Aziz</keyname><forenames>Fauzan</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Hadinata</keyname><forenames>Novri</forenames></author></authors><title>Sistem Informasi Eksekutif Berbasis Web pada Fakultas Pertanian
  Universitas Muhammadiyah Palembang</title><categories>cs.CY</categories><comments>6 pages, Student Colloquium Sistem Informasi &amp; Teknik Informatika
  (SC-SITI) 2015. F. Aziz, et al., &quot;Sistem Informasi Eksekutif Berbasis Web
  Pada Fakultas Pertanian Universitas Muhammadiyah Palembang,&quot; presented at the
  Student Colloquium Sistem Informasi &amp; Teknik Informatika (SC-SITI2015),
  Palembang, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information technology (IT) is able to fulfill one of the main needs of an
organization, such as how the executive know and manage the performance of the
organization he leads, including the human resources (HR). Faculty of
Agriculture, University of Muhammadiyah Palembang (UMP) has had personnel
information system which is used to manage HR data both employees and
lecturers. However, the information system is to support the operational
activities only. Therefore it is necessary to build an executive information
system (SIE) in the faculty is the dean. By using the SIE, the dean can easily
access summary data visualization, namely the appearance of information in the
form of graphs making it easier for executives to make decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06778</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06778</id><created>2015-08-27</created><authors><author><keyname>Csat&#xf3;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>A graph interpretation of the least squares ranking method</title><categories>cs.GT</categories><msc-class>15A06, 91B14</msc-class><journal-ref>Social Choice and Welfare (2015). 44(1): 51-69</journal-ref><doi>10.1007/s00355-014-0820-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper aims at analyzing the least squares ranking method for generalized
tournaments with possible missing and multiple paired comparisons. The
bilateral relationships may reflect the outcomes of a sport competition,
product comparisons, or evaluation of political candidates and policies. It is
shown that the rating vector can be obtained as a limit point of an iterative
process based on the scores in almost all cases. The calculation is interpreted
on an undirected graph with loops attached to some nodes, revealing that the
procedure takes into account not only the given object's results but also the
strength of objects compared with it. We explore the connection between this
method and another procedure defined for ranking the nodes in a digraph, the
positional power measure. The decomposition of the least squares solution
offers a number of ways to modify the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06779</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06779</id><created>2015-08-27</created><updated>2015-09-10</updated><authors><author><keyname>Basold</keyname><forenames>Henning</forenames><affiliation>Radboud University</affiliation></author></authors><title>Dependent Inductive and Coinductive Types are Fibrational Dialgebras</title><categories>cs.LO</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015, pp. 3-17</journal-ref><doi>10.4204/EPTCS.191.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I establish the categorical structure necessary to interpret
dependent inductive and coinductive types. It is well-known that dependent type
theories \`a la Martin-L\&quot;of can be interpreted using fibrations. Modern
theorem provers, however, are based on more sophisticated type systems that
allow the definition of powerful inductive dependent types (known as inductive
families) and, somewhat limited, coinductive dependent types. I define a class
of functors on fibrations and show how data type definitions correspond to
initial and final dialgebras for these functors. This description is also a
proposal of how coinductive types should be treated in type theories, as they
appear here simply as dual of inductive types. Finally, I show how dependent
data types correspond to algebras and coalgebras, and give the correspondence
to dependent polynomial functors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06780</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06780</id><created>2015-08-27</created><authors><author><keyname>Ko&#x142;odziejczyk</keyname><forenames>Leszek Aleksander</forenames></author><author><keyname>Michalewski</keyname><forenames>Henryk</forenames></author></authors><title>How unprovable is Rabin's decidability theorem?</title><categories>math.LO cs.FL cs.LO</categories><comments>21 pages</comments><msc-class>03B30, 03F35, 03B25, 03D05, 68Q45, 03E60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the strength of set-theoretic axioms needed to prove Rabin's theorem
on the decidability of the MSO theory of the infinite binary tree. We first
show that the complementation theorem for tree automata, which forms the
technical core of typical proofs of Rabin's theorem, is equivalent over the
moderately strong second-order arithmetic theory $\mathsf{ACA}_0$ to a
determinacy principle implied by the positional determinacy of all parity games
and implying the determinacy of all Gale-Stewart games given by boolean
combinations of ${\bf \Sigma^0_2}$ sets. It follows that complementation for
tree automata is provable from $\Pi^1_3$- but not $\Delta^1_3$-comprehension.
  We then use results due to MedSalem-Tanaka, M\&quot;ollerfeld and
Heinatsch-M\&quot;ollerfeld to prove that over $\Pi^1_2$-comprehension, the
complementation theorem for tree automata, decidability of the MSO theory of
the infinite binary tree, positional determinacy of parity games and
determinacy of $\mathrm{Bool}({\bf \Sigma^0_2})$ Gale-Stewart games are all
equivalent. Moreover, these statements are equivalent to the
$\Pi^1_3$-reflection principle for $\Pi^1_2$-comprehension. It follows in
particular that Rabin's decidability theorem is not provable in
$\Delta^1_3$-comprehension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06781</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06781</id><created>2015-08-27</created><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Sekar</keyname><forenames>Shreyas</forenames></author></authors><title>Computing Stable Coalitions: Approximation Algorithms for Reward Sharing</title><categories>cs.GT cs.AI</categories><comments>Under Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a setting where selfish agents are to be assigned to coalitions or
projects from a fixed set P. Each project k is characterized by a valuation
function; v_k(S) is the value generated by a set S of agents working on project
k. We study the following classic problem in this setting: &quot;how should the
agents divide the value that they collectively create?&quot;. One traditional
approach in cooperative game theory is to study core stability with the
implicit assumption that there are infinite copies of one project, and agents
can partition themselves into any number of coalitions. In contrast, we
consider a model with a finite number of non-identical projects; this makes
computing both high-welfare solutions and core payments highly non-trivial.
  The main contribution of this paper is a black-box mechanism that reduces the
problem of computing a near-optimal core stable solution to the purely
algorithmic problem of welfare maximization; we apply this to compute an
approximately core stable solution that extracts one-fourth of the optimal
social welfare for the class of subadditive valuations. We also show much
stronger results for several popular sub-classes: anonymous, fractionally
subadditive, and submodular valuations, as well as provide new approximation
algorithms for welfare maximization with anonymous functions. Finally, we
establish a connection between our setting and the well-studied simultaneous
auctions with item bidding; we adapt our results to compute approximate pure
Nash equilibria for these auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06782</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06782</id><created>2015-08-27</created><authors><author><keyname>Becchetti</keyname><forenames>Luca</forenames></author><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Stabilizing Consensus with Many Opinions</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following distributed consensus problem: Each node in a
complete communication network of size $n$ initially holds an \emph{opinion},
which is chosen arbitrarily from a finite set $\Sigma$. The system must
converge toward a consensus state in which all, or almost all nodes, hold the
same opinion. Moreover, this opinion should be \emph{valid}, i.e., it should be
one among those initially present in the system. This condition should be met
even in the presence of an adaptive, malicious adversary who can modify the
opinions of a bounded number of nodes in every round.
  We consider the \emph{3-majority dynamics}: At every round, every node pulls
the opinion from three random neighbors and sets his new opinion to the
majority one (ties are broken arbitrarily). Let $k$ be the number of valid
opinions. We show that, if $k \leqslant n^{\alpha}$, where $\alpha$ is a
suitable positive constant, the 3-majority dynamics converges in time
polynomial in $k$ and $\log n$ with high probability even in the presence of an
adversary who can affect up to $o(\sqrt{n})$ nodes at each round.
  Previously, the convergence of the 3-majority protocol was known for
$|\Sigma| = 2$ only, with an argument that is robust to adversarial errors. On
the other hand, no anonymous, uniform-gossip protocol that is robust to
adversarial errors was known for $|\Sigma| &gt; 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06791</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06791</id><created>2015-08-27</created><authors><author><keyname>Clarkson</keyname><forenames>James</forenames></author><author><keyname>Kotselidis</keyname><forenames>Christos</forenames></author><author><keyname>Brown</keyname><forenames>Gavin</forenames></author><author><keyname>Luj&#xe1;n</keyname><forenames>Mikel</forenames></author></authors><title>Boosting Java Performance using GPGPUs</title><categories>cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous programming has started becoming the norm in order to achieve
better performance by running portions of code on the most appropriate hardware
resource. Currently, significant engineering efforts are undertaken in order to
enable existing programming languages to perform heterogeneous execution mainly
on GPUs. In this paper we describe Jacc, an experimental framework which allows
developers to program GPGPUs directly from Java. By using the Jacc framework,
developers have the ability to add GPGPU support into their applications with
minimal code refactoring.
  To simplify the development of GPGPU applications we allow developers to
model heterogeneous code using two key abstractions: \textit{tasks}, which
encapsulate all the information needed to execute code on a GPGPU; and
\textit{task graphs}, which capture the inter-task control-flow of the
application. Using this information the Jacc runtime is able to automatically
handle data movement and synchronization between the host and the GPGPU;
eliminating the need for explicitly managing disparate memory spaces.
  In order to generate highly parallel GPGPU code, Jacc provides developers
with the ability to decorate key aspects of their code using annotations. The
compiler, in turn, exploits this information in order to automatically generate
code without requiring additional code refactoring.
  Finally, we demonstrate the advantages of Jacc, both in terms of
programmability and performance, by evaluating it against existing Java
frameworks. Experimental results show an average performance speedup of 32x and
a 4.4x code decrease across eight evaluated benchmarks on a NVIDIA Tesla K20m
GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06792</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06792</id><created>2015-08-27</created><authors><author><keyname>Ma&#xdf;berg</keyname><forenames>Jens</forenames></author></authors><title>The Depth-Restricted Rectilinear Steiner Arborescence Problem is
  NP-complete</title><categories>cs.CC math.CO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the rectilinear Steiner arborescence problem the task is to build a
shortest rectilinear Steiner tree connecting a given root and a set of
terminals which are placed in the plane such that all root-terminal-paths are
shortest paths. This problem is known to be NP-hard.
  In this paper we consider a more restricted version of this problem. In our
case we have a depth restrictions $d(t)\in\mathbb{N}$ for every terminal $t$.
We are looking for a shortest binary rectilinear Steiner arborescence such that
each terminal $t$ is at depth $d(t)$, that is, there are exactly $d(t)$ Steiner
points on the unique root-$t$-path is exactly $d(t)$. We prove that even this
restricted version is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06802</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06802</id><created>2015-08-27</created><authors><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>Lengler</keyname><forenames>Johannes</forenames></author></authors><title>Introducing Elitist Black-Box Models: When Does Elitist Selection Weaken
  the Performance of Evolutionary Algorithms?</title><categories>cs.NE cs.DS</categories><comments>A short version of this work has been presented at the GECCO
  conference 2015 in Madrid, Spain. Available at
  http://dl.acm.org/citation.cfm?doid=2739480.2754654</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Black-box complexity theory provides lower bounds for the runtime of
black-box optimizers like evolutionary algorithms and serves as an inspiration
for the design of new genetic algorithms. Several black-box models covering
different classes of algorithms exist, each highlighting a different aspect of
the algorithms under considerations. In this work we add to the existing
black-box notions a new \emph{elitist black-box model}, in which algorithms are
required to base all decisions solely on (a fixed number of) the best search
points sampled so far. Our model combines features of the ranking-based and the
memory-restricted black-box models with elitist selection.
  We provide several examples for which the elitist black-box complexity is
exponentially larger than that the respective complexities in all previous
black-box models, thus showing that the elitist black-box complexity can be
much closer to the runtime of typical evolutionary algorithms.
  We also introduce the concept of $p$-Monte Carlo black-box complexity, which
measures the time it takes to optimize a problem with failure probability at
most $p$. Even for small~$p$, the $p$-Monte Carlo black-box complexity of a
function class $\mathcal F$ can be smaller by an exponential factor than its
typically regarded Las Vegas complexity (which measures the \emph{expected}
time it takes to optimize $\mathcal F$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06805</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06805</id><created>2015-08-27</created><authors><author><keyname>Goeders</keyname><forenames>Jeffrey</forenames></author><author><keyname>Wilton</keyname><forenames>Steven J. E.</forenames></author></authors><title>Allowing Software Developers to Debug HLS Hardware</title><categories>cs.SE cs.AR</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-Level Synthesis (HLS) is emerging as a mainstream design methodology,
allowing software designers to enjoy the benefits of a hardware implementation.
Significant work has led to effective compilers that produce high-quality
hardware designs from software specifications. However, in order to fully
benefit from the promise of HLS, a complete ecosystem that provides the ability
to analyze, debug, and optimize designs is essential. This ecosystem has to be
accessible to software designers. This is challenging, since software
developers view their designs very differently than how they are physically
implemented on-chip. Rather than individual sequential lines of code, the
implementation consists of gates operating in parallel across multiple clock
cycles. In this paper, we report on our efforts to create an ecosystem that
allows software designers to debug HLS-generated circuits in a familiar manner.
We have implemented our ideas in a debug framework that will be included in the
next release of the popular LegUp high-level synthesis tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06811</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06811</id><created>2015-08-27</created><authors><author><keyname>M&#xf6;ller</keyname><forenames>Konrad</forenames></author><author><keyname>Kumm</keyname><forenames>Martin</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Charles-Frederic</forenames></author><author><keyname>Zipf</keyname><forenames>Peter</forenames></author></authors><title>Model-based Hardware Design for FPGAs using Folding Transformations
  based on Subcircuits</title><categories>cs.AR</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a tool flow and results for a model-based hardware design for
FPGAs from Simulink descriptions which nicely integrates into existing
environments. While current commercial tools do not exploit some high-level
optimizations, we investigate the promising approach of using reusable
subcircuits for folding transformations to control embedded multiplier usage
and to optimize logic block usage. We show that resource improvements of up to
70% compared to the original model are possible, but it is also shown that
subcircuit selection is a critical task. While our tool flow provides good
results already, the investigation and optimization of subcircuit selection is
clearly identified as an additional keypoint to extend high-level control on
low-level FPGA mapping properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06821</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06821</id><created>2015-08-27</created><authors><author><keyname>Korinth</keyname><forenames>Jens</forenames></author><author><keyname>de la Chevallerie</keyname><forenames>David</forenames></author><author><keyname>Koch</keyname><forenames>Andreas</forenames></author></authors><title>ThreadPoolComposer - An Open-Source FPGA Toolchain for Software
  Developers</title><categories>cs.DC cs.AR</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This extended abstract presents ThreadPoolComposer, a high-level
synthesis-based development framework and meta-toolchain that provides a
uniform programming interface for FPGAs portable across multiple platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06823</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06823</id><created>2015-08-27</created><authors><author><keyname>Kumar</keyname><forenames>Vinay B. Y.</forenames></author><author><keyname>Engineer</keyname><forenames>Pinalkumar</forenames></author><author><keyname>Datar</keyname><forenames>Mandar</forenames></author><author><keyname>Turakhia</keyname><forenames>Yatish</forenames></author><author><keyname>Agarwal</keyname><forenames>Saurabh</forenames></author><author><keyname>Diwale</keyname><forenames>Sanket</forenames></author><author><keyname>Patkar</keyname><forenames>Sachin B.</forenames></author></authors><title>Framework for Application Mapping over Packet-Switched Network of FPGAs:
  Case Studies</title><categories>cs.DC</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algorithm-to-hardware High-level synthesis (HLS) tools today are
purported to produce hardware comparable in quality to handcrafted designs,
particularly with user directive driven or domains specific HLS. However, HLS
tools are not readily equipped for when an application/algorithm needs to
scale. We present a (work-in-progress) semi-automated framework to map
applications over a packet-switched network of modules (single FPGA) and then
to seamlessly partition such a network over multiple FPGAs over quasi-serial
links. We illustrate the framework through three application case studies: LDPC
Decoding, Particle Filter based Object Tracking, and Matrix Vector
Multiplication over GF(2). Starting with high-level representations of each
case application, we first express them in an intermediate message passing
formulation, a model of communicating processing elements. Once the processing
elements are identified, these are either handcrafted or realized using HLS.
The rest of the flow is automated where the processing elements are plugged on
to a configurable network-on-chip (CONNECT) topology of choice, followed by
partitioning the 'on-chip' links to work seamlessly across chips/FPGAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06827</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06827</id><created>2015-08-27</created><authors><author><keyname>Bansal</keyname><forenames>Kshitij</forenames></author><author><keyname>Reynolds</keyname><forenames>Andrew</forenames></author><author><keyname>King</keyname><forenames>Tim</forenames></author><author><keyname>Barrett</keyname><forenames>Clark</forenames></author><author><keyname>Wies</keyname><forenames>Thomas</forenames></author></authors><title>On Deciding Local Theory Extensions via E-matching</title><categories>cs.LO</categories><doi>10.1007/978-3-319-21668-3_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satisfiability Modulo Theories (SMT) solvers incorporate decision procedures
for theories of data types that commonly occur in software. This makes them
important tools for automating verification problems. A limitation frequently
encountered is that verification problems are often not fully expressible in
the theories supported natively by the solvers. Many solvers allow the
specification of application-specific theories as quantified axioms, but their
handling is incomplete outside of narrow special cases.
  In this work, we show how SMT solvers can be used to obtain complete decision
procedures for local theory extensions, an important class of theories that are
decidable using finite instantiation of axioms. We present an algorithm that
uses E-matching to generate instances incrementally during the search,
significantly reducing the number of generated instances compared to eager
instantiation strategies. We have used two SMT solvers to implement this
algorithm and conducted an extensive experimental evaluation on benchmarks
derived from verification conditions for heap-manipulating programs. We believe
that our results are of interest to both the users of SMT solvers as well as
their developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06829</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06829</id><created>2015-08-27</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Wahlstrom</keyname><forenames>Magnus</forenames></author></authors><title>Tight Lower Bounds for the Workflow Satisfiability Problem Based on the
  Strong Exponential Time Hypothesis</title><categories>cs.DS cs.CC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Workflow Satisfiability Problem (WSP) asks whether there exists an
assignment of authorized users to the steps in a workflow specification,
subject to certain constraints on the assignment. The problem is NP-hard even
when restricted to just not equals constraints. Since the number of steps $k$
is relatively small in practice, Wang and Li (2010) introduced a
parametrisation of WSP by $k$. Wang and Li (2010) showed that, in general, the
WSP is W[1]-hard, i.e., it is unlikely that there exists a fixed-parameter
tractable (FPT) algorithm for solving the WSP. Crampton et al. (2013) and Cohen
et al. (2014) designed FPT algorithms of running time $O^*(2^{k})$ and
$O^*(2^{k\log_2 k})$ for the WSP with so-called regular and user-independent
constraints, respectively. In this note, we show that there are no algorithms
of running time $O^*(2^{ck})$ and $O^*(2^{ck\log_2 k})$ for the two
restrictions of WSP, respectively, with any $c&lt;1$, unless the Strong
Exponential Time Hypothesis fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06830</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06830</id><created>2015-08-27</created><authors><author><keyname>Jim&#xe9;nez-Gonz&#xe1;lez</keyname><forenames>Daniel</forenames></author><author><keyname>&#xc1;lvarez</keyname><forenames>Carlos</forenames></author><author><keyname>Filgueras</keyname><forenames>Antonio</forenames></author><author><keyname>Martorell</keyname><forenames>Xavier</forenames></author><author><keyname>Langer</keyname><forenames>Jan</forenames></author><author><keyname>Noguera</keyname><forenames>Juanjo</forenames></author><author><keyname>Vissers</keyname><forenames>Kees</forenames></author></authors><title>Coarse-Grain Performance Estimator for Heterogeneous Parallel Computing
  Architectures like Zynq All-Programmable SoC</title><categories>cs.DC cs.PF</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous computing is emerging as a mandatory requirement for
power-efficient system design. With this aim, modern heterogeneous platforms
like Zynq All-Programmable SoC, that integrates ARM-based SMP and programmable
logic, have been designed. However, those platforms introduce large design
cycles consisting on hardware/software partitioning, decisions on granularity
and number of hardware accelerators, hardware/software integration, bitstream
generation, etc.
  This paper presents a performance parallel heterogeneous estimation for
systems where hardware/software co-design and run-time heterogeneous task
scheduling are key. The results show that the programmer can quickly decide,
based only on her/his OmpSs (OpenMP + extensions) application, which is the
co-design that achieves nearly optimal heterogeneous parallel performance,
based on the methodology presented and considering only synthesis estimation
results. The methodology presented reduces the programmer co-design decision
from hours to minutes and shows high potential on hardware/software
heterogeneous parallel performance estimation on the Zynq All-Programmable SoC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06832</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06832</id><created>2015-08-27</created><authors><author><keyname>V&#xe9;stias</keyname><forenames>M&#xe1;rio P.</forenames></author><author><keyname>Duarte</keyname><forenames>Rui Policarpo</forenames></author><author><keyname>Neto</keyname><forenames>Hor&#xe1;cio C.</forenames></author></authors><title>Designing Hardware/Software Systems for Embedded High-Performance
  Computing</title><categories>cs.AR</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose an architecture and methodology to design
hardware/software systems for high-performance embedded computing on FPGA. The
hardware side is based on a many-core architecture whose design is generated
automatically given a set of architectural parameters. Both the architecture
and the methodology were evaluated running dense matrix multiplication and
sparse matrix-vector multiplication on a ZYNQ-7020 FPGA platform. The results
show that using a system-level design of the system avoids complex hardware
design and still provides good performance results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06836</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06836</id><created>2015-08-27</created><authors><author><keyname>Pavlinovic</keyname><forenames>Zvonimir</forenames></author><author><keyname>King</keyname><forenames>Tim</forenames></author><author><keyname>Wies</keyname><forenames>Thomas</forenames></author></authors><title>On Practical SMT-Based Type Error Localization</title><categories>cs.PL</categories><acm-class>D.2.5; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compilers for statically typed functional programming languages are notorious
for generating confusing type error messages. When the compiler detects a type
error, it typically reports the program location where the type checking failed
as the source of the error. Since other error sources are not even considered,
the actual root cause is often missed. A more adequate approach is to consider
all possible error sources and report the most useful one subject to some
usefulness criterion. In our previous work, we showed that this approach can be
formulated as an optimization problem related to satisfiability modulo theories
(SMT). This formulation cleanly separates the heuristic nature of usefulness
criteria from the underlying search problem. Unfortunately, algorithms that
search for an optimal error source cannot directly use principal types which
are crucial for dealing with the exponential-time complexity of the decision
problem of polymorphic type checking. In this paper, we present a new algorithm
that efficiently finds an optimal error source in a given ill-typed program.
Our algorithm uses an improved SMT encoding to cope with the high complexity of
polymorphic typing by iteratively expanding the typing constraints from which
principal types are derived. The algorithm preserves the clean separation
between the heuristics and the actual search. We have implemented our algorithm
for OCaml. In our experimental evaluation, we found that the algorithm reduces
the running times for optimal type error localization from minutes to seconds
and scales better than previous localization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06843</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06843</id><created>2015-08-27</created><authors><author><keyname>Knodel</keyname><forenames>Oliver</forenames></author><author><keyname>Spallek</keyname><forenames>Rainer G.</forenames></author></authors><title>RC3E: Provision and Management of Reconfigurable Hardware Accelerators
  in a Cloud Environment</title><categories>cs.DC</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous systems consisting of general-purpose processors and different
types of hardware accelerators are becoming more and more common in HPC
systems. Especially FPGAs provide a promising opportunity to improve both
performance and energy efficiency of such systems. Adding FPGAs to clouds or
data centers allows easy access to such reconfigurable resources. In this paper
we present our cloud service models and cloud hypervisor called RC3E, which
integrates virtualized FPGA-based hardware accelerators into a cloud
environment. With our hardware and software framework, multiple (virtual) user
designs can be executed on a single physical FPGA device. We demonstrate the
performance of our approach by implementing up to four virtual user cores on a
single device and present future perspectives for FPGAs in cloud-based data
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06845</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06845</id><created>2015-08-27</created><authors><author><keyname>Aslett</keyname><forenames>Louis J. M.</forenames></author><author><keyname>Esperan&#xe7;a</keyname><forenames>Pedro M.</forenames></author><author><keyname>Holmes</keyname><forenames>Chris C.</forenames></author></authors><title>Encrypted statistical machine learning: new privacy preserving methods</title><categories>stat.ML cs.CR cs.LG stat.ME</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two new statistical machine learning methods designed to learn on
fully homomorphic encrypted (FHE) data. The introduction of FHE schemes
following Gentry (2009) opens up the prospect of privacy preserving statistical
machine learning analysis and modelling of encrypted data without compromising
security constraints. We propose tailored algorithms for applying extremely
random forests, involving a new cryptographic stochastic fraction estimator,
and na\&quot;{i}ve Bayes, involving a semi-parametric model for the class decision
boundary, and show how they can be used to learn and predict from encrypted
data. We demonstrate that these techniques perform competitively on a variety
of classification data sets and provide detailed information about the
computational practicalities of these and other FHE methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06847</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06847</id><created>2015-08-27</created><authors><author><keyname>Marcilon</keyname><forenames>Thiago Braga</forenames></author><author><keyname>Sampaio</keyname><forenames>Rudini Menezes</forenames></author></authors><title>The maximum time of 2-neighbour bootstrap percolation in grid graphs and
  some parameterized results</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2-neighborhood bootstrap percolation on a graph $G$, an infection spreads
according to the following deterministic rule: infected vertices of $G$ remain
infected forever and in consecutive rounds healthy vertices with at least two
already infected neighbors become infected. Percolation occurs if eventually
every vertex is infected. The maximum time $t(G)$ is the maximum number of
rounds needed to eventually infect the entire vertex set. In 2013, it was
proved by Benevides et al \cite{eurocomb13} that $t(G)$ is NP-hard for planar
graphs and that deciding whether $t(G)\geq k$ is polynomial time solvable for
$k\leq 2$, but is NP-complete for $k\geq 4$. They left two open problems about
the complexity for $k=3$ and for planar bipartite graphs. In 2014, we solved
the first problem\cite{wg2014}. In this paper, we solve the second one by
proving that $t(G)$ is NP-complete even in grid graphs with maximum degree 3.
We also prove that $t(G)$ is polynomial time solvable for solid grid graphs
with maximum degree 3. Moreover, we prove that the percolation time problem is
W[1]-hard on the treewidth of the graph, but it is fixed parameter tractable
with parameters treewidth$+k$ and maxdegree$+k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06853</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06853</id><created>2015-08-27</created><authors><author><keyname>Liciotti</keyname><forenames>Daniele</forenames></author><author><keyname>Contigiani</keyname><forenames>Marco</forenames></author><author><keyname>Frontoni</keyname><forenames>Emanuele</forenames></author><author><keyname>Mancini</keyname><forenames>Adriano</forenames></author><author><keyname>Zingaretti</keyname><forenames>Primo</forenames></author><author><keyname>Placidi</keyname><forenames>Valerio</forenames></author></authors><title>Shopper Analytics: a customer activity recognition system using a
  distributed RGB-D camera network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present an integrated system consisted of a RGB-D
camera and a software able to monitor shoppers in intelligent retail
environments. We propose an innovative low cost smart system that can
understand the shoppers' behavior and, in particular, their interactions with
the products in the shelves, with the aim to develop an automatic RGB-D
technique for video analysis. The system of cameras detects the presence of
people and univocally identifies them. Through the depth frames, the system
detects the interactions of the shoppers with the products on the shelf and
determines if a product is picked up or if the product is taken and then put
back and finally, if there is not contact with the products. The system is low
cost and easy to install, and experimental results demonstrated that its
performances are satisfactory also in real environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06874</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06874</id><created>2015-08-27</created><authors><author><keyname>Marcilon</keyname><forenames>Thiago Braga</forenames></author><author><keyname>Sampaio</keyname><forenames>Rudini Menezes</forenames></author></authors><title>The maximum time of 2-neighbor bootstrap percolation: complexity results</title><categories>cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1209.4339 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2-neighborhood bootstrap percolation on a graph G, an infection spreads
according to the following deterministic rule: infected vertices of G remain
infected forever and in consecutive rounds healthy vertices with at least 2
already infected neighbors become infected. Percolation occurs if eventually
every vertex is infected. The maximum time t(G) is the maximum number of rounds
needed to eventually infect the entire vertex set. In 2013, it was proved
\cite{eurocomb13} that deciding whether $t(G)\geq k$ is polynomial time
solvable for k=2, but is NP-Complete for k=4 and, if the problem is restricted
to bipartite graphs, it is NP-Complete for k=7. In this paper, we solve the
open questions. We obtain an $O(mn^5)$-time algorithm to decide whether
$t(G)\geq 3$. For bipartite graphs, we obtain an $O(mn^3)$-time algorithm to
decide whether $t(G)\geq 3$, an $O(m^2n^9)$-time algorithm to decide whether
$t(G)\geq 4$ and we prove that $t(G)\geq 5$ is NP-Complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06878</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06878</id><created>2015-08-27</created><authors><author><keyname>Bhattacharya</keyname><forenames>Kunal</forenames></author><author><keyname>Ghosh</keyname><forenames>Asim</forenames></author><author><keyname>Monsivais</keyname><forenames>Daniel</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin I. M.</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Sex differences in social focus across the lifecycle in humans</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Age and gender are two important factors that play crucial roles in the way
organisms allocate their social effort. In this study, we analyse a large
mobile phone dataset to explore the way lifehistory influences human sociality
and the way social networks are structured. Our results indicate that these
aspects of human behaviour are strongly related to the age and gender such that
younger individuals have more contacts and, among them, males more than
females. However, the rate of decrease in the number of contacts with age
differs between males and females, such that there is a reversal in the number
of contacts around the late 30s. We suggest that this pattern can be attributed
to the difference in reproductive investments that are made by the two sexes.
We analyse the inequality in social investment patterns and suggest that the
age and gender-related differences that we find reflect the constraints imposed
by reproduction in a context where time (a form of social capital) is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06889</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06889</id><created>2015-08-27</created><authors><author><keyname>B&#xe9;rczi-Kov&#xe1;cs</keyname><forenames>Erika R.</forenames></author><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author></authors><title>The complexity of the Clar number problem and an FPT algorithm</title><categories>cs.DM cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Clar number of a (hydro)carbon molecule, introduced by Clar [E. Clar,
\emph{The aromatic sextet}, (1972).], is the maximum number of mutually
disjoint resonant hexagons in the molecule. Calculating the Clar number can be
formulated as an optimization problem on 2-connected planar graphs. Namely, it
is the maximum number of mutually disjoint even faces a perfect matching can
simultaneously alternate on. It was proved by Abeledo and Atkinson [H. G.
Abeledo and G. W. Atkinson, \emph{Unimodularity of the clar number problem},
Linear algebra and its applications \textbf{420} (2007), no. 2, 441--448] that
the Clar number can be computed in polynomial time if the plane graph has even
faces only. We prove that calculating the Clar number in general 2-connected
plane graphs is NP-hard. We also prove NP-hardness of the maximum independent
set problem for 2-connected plane graphs with odd faces only, which may be of
independent interest. Finally, we give an FPT algorithm that determines the
Clar number of a given 2-connected plane graph. The parameter of the algorithm
is the length of the shortest odd join in the planar dual graph. For fullerenes
this is not yet a polynomial algorithm, but for certain carbon nanotubes it
gives an efficient algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06899</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06899</id><created>2015-08-27</created><updated>2016-03-03</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Contradiction-tolerant process algebra with propositional signals</title><categories>cs.LO</categories><comments>25 pages; 26 pages, occurrences of wrong symbol for bisimulation
  equivalence replaced and links to electronic versions of cited papers added
  in reference list</comments><acm-class>D.2.1; D.2.4; F.3.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper, an ACP-style process algebra was proposed in which
propositions are used as the visible part of the state of processes and as
state conditions under which processes may proceed. This process algebra,
called ACPps, is built on classical propositional logic. In this paper, we
present a version of ACPps built on a paraconsistent propositional logic which
is essentially the same as CLuNs. There are many systems that would have to
deal with self-contradictory states if no special measures were taken. For a
number of these systems, it is conceivable that accepting self-contradictory
states and dealing with them in a way based on a paraconsistent logic is an
alternative to taking special measures. The presented version of ACPps can be
suited for the description and analysis of systems that deal with
self-contradictory states in a way based on the above-mentioned paraconsistent
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06901</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06901</id><created>2015-08-27</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Huang</keyname><forenames>Gang</forenames></author><author><keyname>Wilford</keyname><forenames>Paul A.</forenames></author></authors><title>Compressive Sensing via Low-Rank Gaussian Mixture Models</title><categories>stat.ML cs.LG stat.AP</categories><comments>12 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new compressive sensing (CS) inversion algorithm by utilizing
the Gaussian mixture model (GMM). While the compressive sensing is performed
globally on the entire image as implemented in our lensless camera, a low-rank
GMM is imposed on the local image patches. This low-rank GMM is derived via
eigenvalue thresholding of the GMM trained on the projection of the measurement
data, thus learned {\em in situ}. The GMM and the projection of the measurement
data are updated iteratively during the reconstruction. Our GMM algorithm
degrades to the piecewise linear estimator (PLE) if each patch is represented
by a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also
developed for CS inversion, constituting an additional contribution of this
paper. Extensive results on both simulation data and real data captured by the
lensless camera demonstrate the efficacy of the proposed algorithm.
Furthermore, we compare the CS reconstruction results using our algorithm with
the JPEG compression. Simulation results demonstrate that when limited
bandwidth is available (a small number of measurements), our algorithm can
achieve comparable results as JPEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06904</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06904</id><created>2015-08-27</created><authors><author><keyname>Thom</keyname><forenames>Markus</forenames></author><author><keyname>Gritschneder</keyname><forenames>Franz</forenames></author></authors><title>A Theory for Rapid Exact Signal Scanning with Deep Multi-Scale
  Convolutional Neural Networks</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze a rigorous formulation of the dynamics of a signal
processing scheme that aims at dense scanning of large input signals. Recently
proposed methodologies lack a satisfactory discussion of whether they actually
produce the correct results according to their definition, especially in the
context of Convolutional Neural Networks. We improve on this through an exact
characterization of the requirements for a sound sliding window approach. The
tools developed in this paper are especially beneficial if Convolutional Neural
Networks are employed, but can also be used as a more general framework to
validate related approaches to signal scanning. The contributed theory helps to
eliminate redundant computations and renders special case treatment
unnecessary, resulting in a dramatic boost in efficiency particularly on
massively parallel processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06911</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06911</id><created>2015-08-27</created><authors><author><keyname>Ramachandran</keyname><forenames>Arthi</forenames></author><author><keyname>Chaintreau</keyname><forenames>Augustin</forenames></author></authors><title>Who Contributes to the Knowledge Sharing Economy?</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages in ACM Conference on Online Social Networks 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information sharing dynamics of social networks rely on a small set of
influencers to effectively reach a large audience. Our recent results and
observations demonstrate that the shape and identity of this elite, especially
those contributing \emph{original} content, is difficult to predict.
Information acquisition is often cited as an example of a public good. However,
this emerging and powerful theory has yet to provably offer qualitative
insights on how specialization of users into active and passive participants
occurs.
  This paper bridges, for the first time, the theory of public goods and the
analysis of diffusion in social media. We introduce a non-linear model of
\emph{perishable} public goods, leveraging new observations about sharing of
media sources. The primary contribution of this work is to show that
\emph{shelf time}, which characterizes the rate at which content get renewed,
is a critical factor in audience participation. Our model proves a fundamental
\emph{dichotomy} in information diffusion: While short-lived content has simple
and predictable diffusion, long-lived content has complex specialization. This
occurs even when all information seekers are \emph{ex ante} identical and could
be a contributing factor to the difficulty of predicting social network
participation and evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06918</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06918</id><created>2015-08-26</created><authors><author><keyname>Brodic</keyname><forenames>Darko</forenames></author><author><keyname>Amelio</keyname><forenames>Alessia</forenames></author></authors><title>Classification of the Extremely Low Frequency Magnetic Field Radiation
  Measurement from the Laptop Computers</title><categories>cs.CY</categories><comments>14 pages, 9 figures</comments><acm-class>I.2.6; D.4.8</acm-class><journal-ref>Measurement Science Review, Volume 15, No. 4, 2015</journal-ref><doi>10.1515/msr-2015-0028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the level of the extremely low-frequency magnetic field,
which is produced by laptop computers. The magnetic field, which is
characterized by extremely low frequencies up to 300 Hz is measured due to its
hazardous effects to the laptop user's health. The experiment consists of
testing 13 different laptop computers in normal operation conditions. The
measuring of the magnetic field is performed in the adjacent neighborhood of
the laptop computers. The measured data are presented and then classified. The
classification is performed by the K-Medians method in order to determine the
critical positions of the laptop. At the end, the measured magnetic field
values are compared with the critical values suggested by different safety
standards. It is shown that some of the laptop computers emit a very strong
magnetic field. Hence, they must be used with extreme caution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06924</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06924</id><created>2015-08-27</created><updated>2015-09-11</updated><authors><author><keyname>Mueller</keyname><forenames>Erik T.</forenames></author><author><keyname>Minsky</keyname><forenames>Henry</forenames></author></authors><title>Using Thought-Provoking Children's Questions to Drive Artificial
  Intelligence Research</title><categories>cs.AI</categories><comments>update Institute name and URL; fix date of Myers and Myers interview;
  add reference to Turing++ questions; clarify that we developed the set of
  annotation tags</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to use thought-provoking children's questions (TPCQs), namely
Highlights BrainPlay questions, to drive artificial intelligence research.
These questions are designed to stimulate thought and learning in children, and
they can be used to do the same thing in AI systems. We introduce the TPCQ
task, which consists of taking a TPCQ question as input and producing as output
both (1) answers to the question and (2) learned generalizations. We discuss
how BrainPlay questions stimulate learning. We analyze 244 BrainPlay questions,
and we report statistics on question type, question class, answer cardinality,
answer class, types of knowledge needed, and types of reasoning needed. We find
that BrainPlay questions span many aspects of intelligence. We envision an AI
system based on the society of mind (Minsky 1986; 2006) consisting of a
multilevel architecture with diverse resources that run in parallel to jointly
answer and learn from questions. Because the answers to BrainPlay questions and
the generalizations learned from them are often highly open-ended, we suggest
using human judges for evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06927</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06927</id><created>2015-08-27</created><authors><author><keyname>Cheng</keyname><forenames>Long</forenames></author><author><keyname>Wang</keyname><forenames>Yunpeng</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author><author><keyname>Hou</keyname><forenames>Zeng-Guang</forenames></author><author><keyname>Tan</keyname><forenames>Min</forenames></author></authors><title>On Convergence Rate of Leader-Following Consensus of Linear Multi-Agent
  Systems with Communication Noises</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note further studies the previously proposed consensus protocol for
linear multi-agent systems with communication noises in [15], [16]. Each agent
is allowed to have its own time-varying gain to attenuate the effect of
communication noises. Therefore, the common assumption in most references that
all agents have the same noise-attenuation gain is not necessary. It has been
proved that if all noise-attenuation gains are infinitesimal of the same order,
then the mean square leader-following consensus can be reached. Furthermore,
the convergence rate of the multi-agent system has been investigated. If the
noise-attenuation gains belong to a class of functions which are bounded above
and below by $t^{-\beta}$ $(\beta\in(0,1))$ asymptotically, then the states of
all follower agents are convergent in mean square to the leader's state with
the rate characterized by a function bounded above by $t^{-\beta}$
asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06936</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06936</id><created>2015-08-27</created><authors><author><keyname>Barnett</keyname><forenames>Alex H.</forenames></author><author><keyname>Magland</keyname><forenames>Jeremy F.</forenames></author><author><keyname>Greengard</keyname><forenames>Leslie F.</forenames></author></authors><title>Validation of neural spike sorting algorithms without ground-truth
  information</title><categories>q-bio.NC cs.CV</categories><comments>22 pages, 7 figures; submitted to J. Neurosci. Meth</comments><msc-class>94A12, 92C55, 62M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a suite of validation metrics that assess the credibility of a
given automatic spike sorting algorithm applied to a given electrophysiological
recording, when ground-truth is unavailable. By rerunning the spike sorter two
or more times, the metrics measure stability under various perturbations
consistent with variations in the data itself, making no assumptions about the
noise model, nor about the internal workings of the sorting algorithm. Such
stability is a prerequisite for reproducibility of results. We illustrate the
metrics on standard sorting algorithms for both in vivo and ex vivo recordings.
We believe that such metrics could reduce the significant human labor currently
spent on validation, and should form an essential part of large-scale automated
spike sorting and systematic benchmarking of algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06944</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06944</id><created>2015-08-27</created><updated>2016-01-07</updated><authors><author><keyname>Shaham</keyname><forenames>Nimrod</forenames></author><author><keyname>Burak</keyname><forenames>Yoram</forenames></author></authors><title>Continuous parameter working memory in a balanced chaotic neural network</title><categories>cond-mat.dis-nn cs.NE q-bio.NC</categories><comments>5 pages, 4 figures. Supplemental Material available on request.
  Version 2 includes very minor corrections in the text, and a higher
  resolution rendering of Fig. 4. Version 3 is a revised version of the
  manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Working memory, the ability to maintain information over time scales greater
than those characterizing single neurons, is essential to many brain functions.
It remains unclear whether neural networks in the balanced state, an important
model for activity in the cortex, can support a continuum of stable states that
would make it possible to store a continuous variable in working memory while
also accounting for the stochastic behavior of single neurons. Here we propose
a simple neural architecture that achieves this goal. We show analytically that
in the limit of an infinite network a continuous parameter can be stored
indefinitely on a continuum of balanced states. For finite networks we
calculate the diffusivity along the attractor driven by the chaotic noise in
the network, and show that it is inversely proportional to the system size.
Thus, for large enough (but realistic) neural population sizes, and with
suitable tuning of the network connections, it is possible to maintain
continuous parameter values over time scales larger by several orders of
magnitude than the single neuron time scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06950</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06950</id><created>2015-08-27</created><authors><author><keyname>Sreenivasan</keyname><forenames>Sameet</forenames></author><author><keyname>Chan</keyname><forenames>Kevin S.</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author><author><keyname>Korniss</keyname><forenames>Gyorgy</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw</forenames></author></authors><title>Information Cascades in Feed-based Networks of Users with Limited
  Attention</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 5 figures, For IEEE Transactions on Network Science and
  Engineering (submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build a model of information cascades on feed-based networks, taking into
account the finite attention span of users, message generation rates and
message forwarding rates. Using this model, we study through simulations, the
effect of the extent of user attention on the probability that the cascade
becomes viral. In analogy with a branching process, we estimate the branching
factor associated with the cascade process for different attention spans and
different forwarding probabilities, and demonstrate that beyond a certain
attention span, critical forwarding probabilities exist that constitute a
threshold after which cascades can become viral. The critical forwarding
probabilities have an inverse relationship with the attention span. Next, we
develop a semi-analytical approach for our model, that allows us determine the
branching factor for given values of message generation rates, message
forwarding rates and attention spans. The branching factors obtained using this
analytical approach show good agreement with those obtained through
simulations. Finally, we analyze an event specific dataset obtained from
Twitter, and show that estimated branching factors correlate well with the
cascade size distributions associated with distinct hashtags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06961</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06961</id><created>2015-08-27</created><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author></authors><title>Bearing-Based Formation Stabilization with Directed Interaction
  Topologies</title><categories>cs.SY</categories><comments>To appear in the 2015 CDC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of stabilizing target formations specified by
inter-neighbor bearings with relative position measurements. While the
undirected case has been studied in the existing works, this paper focuses on
the case where the interaction topology is directed. It is shown that a linear
distributed control law, which was proposed previously for undirected cases,
can still be applied to the directed case. The formation stability in the
directed case, however, relies on a new notion termed bearing persistence,
which describes whether or not the directed underlying graph is persistent with
the bearing rigidity of a formation. If a target formation is not bearing
persistent, undesired equilibriums will appear and global formation stability
cannot be guaranteed. The notion of bearing persistence is defined by the
bearing Laplacian matrix and illustrated by simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06967</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06967</id><created>2015-08-27</created><authors><author><keyname>Hamdaoui</keyname><forenames>Bechir</forenames></author></authors><title>A Simple Algorithm for Coloring m-Clique Holes</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An m-clique hole is a sequence $\phi=(\Phi_1,\Phi_2,\dots,\Phi_m)$ of $m$
distinct cliques such that $|\Phi_i| \leq m$ for all $i=1,2,\ldots,m$, and
whose clique graph is a hole on $m$ vertices. That is, $\phi$ is an m-clique
hole if for all $i\neq j$, $i,j=1,2,\ldots,m$, $\Phi_i \cap \Phi_{j} \neq
\emptyset$ if and only if $(j-1)~\mbox{mod}~m = (j+1)~\mbox{mod}~m =
i~\mbox{mod}~m$. This paper derives a sufficient and necessary condition on
m-colorability of m-clique holes, and proposes a coloring algorithm that colors
m-clique holes with exactly m colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06973</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06973</id><created>2015-08-26</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author></authors><title>The Relation Between Acausality and Interference in Quantum-Like
  Bayesian Networks</title><categories>cs.AI</categories><comments>In proceedings of the 9th International Conference on Quantum
  Interactions, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse a quantum-like Bayesian Network that puts together cause/effect
relationships and semantic similarities between events. These semantic
similarities constitute acausal connections according to the Synchronicity
principle and provide new relationships to quantum like probabilistic graphical
models. As a consequence, beliefs (or any other event) can be represented in
vector spaces, in which quantum parameters are determined by the similarities
that these vectors share between them. Events attached by a semantic meaning do
not need to have an explanation in terms of cause and effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.06976</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.06976</id><created>2015-08-26</created><authors><author><keyname>Acharya</keyname><forenames>Saurav</forenames></author><author><keyname>Lee</keyname><forenames>Byung Suk</forenames></author><author><keyname>Hines</keyname><forenames>Paul</forenames></author></authors><title>Real-time Top-K Predictive Query Processing over Event Streams</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of predicting the k events that are most
likely to occur next, over historical real-time event streams. Existing
approaches to causal prediction queries have a number of limitations. First,
they exhaustively search over an acyclic causal network to find the most likely
k effect events; however, data from real event streams frequently reflect
cyclic causality. Second, they contain conservative assumptions intended to
exclude all possible non-causal links in the causal network; it leads to the
omission of many less-frequent but important causal links. We overcome these
limitations by proposing a novel event precedence model and a run-time causal
inference mechanism. The event precedence model constructs a first order
absorbing Markov chain incrementally over event streams, where an edge between
two events signifies a temporal precedence relationship between them, which is
a necessary condition for causality. Then, the run-time causal inference
mechanism learns causal relationships dynamically during query processing. This
is done by removing some of the temporal precedence relationships that do not
exhibit causality in the presence of other events in the event precedence
model. This paper presents two query processing algorithms -- one performs
exhaustive search on the model and the other performs a more efficient reduced
search with early termination. Experiments using two real datasets (cascading
blackouts in power systems and web page views) verify the effectiveness of the
probabilistic top-k prediction queries and the efficiency of the algorithms.
Specifically, the reduced search algorithm reduced runtime, relative to
exhaustive search, by 25-80% (depending on the application) with only a small
reduction in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07034</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07034</id><created>2015-08-27</created><authors><author><keyname>Ghosh</keyname><forenames>Bappaditya</forenames></author><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author></authors><title>Cyclic codes over the ring $\mathbb{F}_p[u,v] / \langle
  u^k,v^2,uv-vu\rangle$</title><categories>cs.IT math.IT</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be a prime number. In this paper, we discuss the structures of cyclic
codes over the ring $ \mathbb{F}_p[u, v] / \langle u^k, v^2, uv-vu\rangle$. We
find a unique set of generators for these codes. We also study the rank and the
Hamming distance of these codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07053</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07053</id><created>2015-08-27</created><authors><author><keyname>Hata</keyname><forenames>Kenji</forenames></author><author><keyname>Leung</keyname><forenames>Sherman</forenames></author><author><keyname>Krishna</keyname><forenames>Ranjay</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>SentenceRacer: A Game with a Purpose for Image Sentence Annotation</title><categories>cs.HC</categories><comments>2 pages, 2 figures, 2 tables, potential CSCW poster submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently datasets that contain sentence descriptions of images have enabled
models that can automatically generate image captions. However, collecting
these datasets are still very expensive. Here, we present SentenceRacer, an
online game that gathers and verifies descriptions of images at no cost.
Similar to the game hangman, players compete to uncover words in a sentence
that ultimately describes an image. SentenceRacer both generates and verifies
that the sentences are accurate descriptions. We show that SentenceRacer
generates annotations of higher quality than those generated on Amazon
Mechanical Turk (AMT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07065</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07065</id><created>2015-08-27</created><updated>2015-10-02</updated><authors><author><keyname>Hirai</keyname><forenames>Hiroshi</forenames></author></authors><title>A dual descent algorithm for node-capacitated multiflow problems and its
  applications</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop an $O((m \log k) {\rm MSF} (n,m,1))$-time algorithm
to find a half-integral node-capacitated multiflow of the maximum total
flow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where
${\rm MSF} (n',m',\gamma)$ denotes the time complexity of solving the maximum
submodular flow problem in a network with $n'$ edges, $m'$ edges, and the
complexity $\gamma$ of computing the exchange capacity of the submodular
function describing the problem. By using Fujishige-Zhang algorithm for
submodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \log
k)$ time. This is the first combinatorial strongly polynomial time algorithm
for this problem. Our algorithm is designed on the basis of a developing theory
of discrete convex functions on certain graph structures. Applications include
&quot;ellipsoid-free&quot; combinatorial implementations of a 2-approximation algorithm
for the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07066</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07066</id><created>2015-08-27</created><updated>2015-10-28</updated><authors><author><keyname>Zhao</keyname><forenames>Yongwang</forenames></author></authors><title>A survey on formal specification and verification of separation kernels</title><categories>cs.SE</categories><acm-class>D.2.1; D.2.4; D.4.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation kernels are fundamental software of safety and security critical
systems, which provide to their hosted software applications high-assurance
partitioning and information flow control properties. The application of
separation kernels in critical domain demands the correctness of the kernel by
formal verification. To the best of our knowledge, there is no survey paper on
this topic. This paper presents an overview of formal specification and
verification of separation kernels. We first overview the concepts of
separation kernels and formal verification. Then, we survey the state of the
art on this topic after the year 2000. Finally, we summarize research works by
comparing them and discussing some issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07075</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07075</id><created>2015-08-27</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Sun</keyname><forenames>Mengwei</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Zhao</keyname><forenames>Chenglin</forenames></author></authors><title>Low-complexity Non-coherent Signal Detection for Nano-Scale Molecular
  Communications</title><categories>cs.ET</categories><comments>14 pages, 7 figures, Submitted to IEEE Transactions on NanoBioscience</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nano-scale molecular communication is a viable way of exchanging information
between nano-machines. In this letter, a low-complexity and non-coherent signal
detection technique is proposed to mitigate the inter-symbol-interference (ISI)
and additive noise. In contrast to existing coherent detection methods of high
complexity, the proposed non-coherent signal detector is more practical when
the channel conditions are hard to acquire accurately or hidden from the
receiver. The proposed scheme employs the concentration difference to detect
the ISI corrupted signals and we demonstrate that it can suppress the ISI
effectively. The concentration difference is a stable characteristic,
irrespective of the diffusion channel conditions. In terms of complexity, by
excluding matrix operations or likelihood calculations, the new detection
scheme is particularly suitable for nano-scale molecular communication systems
with a small energy budget or limited computation resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07076</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07076</id><created>2015-08-27</created><authors><author><keyname>Gallehdari</keyname><forenames>Zahra</forenames></author><author><keyname>Meskin</keyname><forenames>Nader</forenames></author><author><keyname>Khorasani</keyname><forenames>Khashayar</forenames></author></authors><title>An $H_{\infty}$ Cooperative Fault Recovery Control of Multi-Agent
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, an $H_{\infty}$ performance fault recovery control problem for
a team of multi-agent systems that is subject to actuator faults is studied.
Our main objective is to design a distributed control reconfiguration strategy
such that a) in absence of disturbances the state consensus errors either
remain bounded or converge to zero asymptotically, b) in presence of actuator
fault the output of the faulty system behaves exactly the same as that of the
healthy system, and c) the specified $H_{\infty}$ performance bound is
guaranteed to be minimized in presence of bounded energy disturbances. The
gains of the reconfigured control laws are selected first by employing a
geometric approach where a set of controllers guarantees that the output of the
faulty agent imitates that of the healthy agent and the consensus achievement
objectives are satisfied. Next, the remaining degrees of freedom in the
selection of the control law gains are used to minimize the bound on a
specified $H_{\infty}$ performance index. The effects of uncertainties and
imperfections in the FDI module decision in correctly estimating the fault
severity as well as delays in invoking the reconfigured control laws are
investigated and a bound on the maximum tolerable estimation uncertainties and
time delays are obtained. Our proposed distributed and cooperative control
recovery approach is applied to a team of five autonomous underwater vehicles
to demonstrate its capabilities and effectiveness in accomplishing the overall
team requirements subject to various actuator faults, delays in invoking the
recovery control, fault estimation and isolation imperfections and
unreliabilities under variuos control recovery scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07081</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07081</id><created>2015-08-27</created><authors><author><keyname>Sofyan</keyname><forenames>Muhammad</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Syahputra</keyname><forenames>Hadi</forenames></author></authors><title>Analisis dan Perancangan Wireless Roaming (Studi Kasus Universitas
  Baturaja)</title><categories>cs.CY</categories><comments>6 pages, Student Colloquium Sistem Informasi &amp; Teknik Informatika
  (SC-SITI) 2015. M. Sofyan, et al., &quot;Analisis dan Perancangan Wireless Roaming
  (Studi Kasus Universitas Baturaja),&quot; presented at the Student Colloquium
  Sistem Informasi &amp; Teknik Informatika (SC-SITI) 2015, Palembang, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless roaming is one way to improve the reliability of a network of
hotspots that are still using the topology Basic Service Set (BSS). When the
user walks away from one access point (AP) or one AP die then begins to lose
the signal, the mobile station (MS) is automatically connected with the AP to
another without reconfiguring. Devices that support wireless roaming is the AP
TP-Link TL-WR740N using DD-WRT firmware that supports DHCP forwarder. Wireless
roaming makes it easy for the user if there is more than one AP in an area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07091</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07091</id><created>2015-08-28</created><updated>2016-03-02</updated><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author><author><keyname>Feraud</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>Multi-armed Bandit Problem with Known Trend</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in Theorem 1</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variant of the multi-armed bandit model, which we call
multi-armed bandit problem with known trend, where the gambler knows the shape
of the reward function of each arm but not its distribution. This new problem
is motivated by different online problems like active learning, music and
interface recommendation applications, where when an arm is sampled by the
model the received reward change according to a known trend. By adapting the
standard multi-armed bandit algorithm UCB1 to take advantage of this setting,
we propose the new algorithm named A-UCB that assumes a stochastic model. We
provide upper bounds of the regret which compare favourably with the ones of
UCB1. We also confirm that experimentally with different simulations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07092</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07092</id><created>2015-08-28</created><updated>2015-10-15</updated><authors><author><keyname>Ma</keyname><forenames>Saisai</forenames></author><author><keyname>Li</keyname><forenames>Jiuyong</forenames></author><author><keyname>Liu</keyname><forenames>Lin</forenames></author><author><keyname>Le</keyname><forenames>Thuc Duy</forenames></author></authors><title>Mining Combined Causes in Large Data Sets</title><categories>cs.AI</categories><comments>This paper has been accepted by Knowledge-Based Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, many methods have been developed for detecting causal
relationships in observational data. Some of them have the potential to tackle
large data sets. However, these methods fail to discover a combined cause, i.e.
a multi-factor cause consisting of two or more component variables which
individually are not causes. A straightforward approach to uncovering a
combined cause is to include both individual and combined variables in the
causal discovery using existing methods, but this scheme is computationally
infeasible due to the huge number of combined variables. In this paper, we
propose a novel approach to address this practical causal discovery problem,
i.e. mining combined causes in large data sets. The experiments with both
synthetic and real world data sets show that the proposed method can obtain
high-quality causal discoveries with a high computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07096</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07096</id><created>2015-08-28</created><authors><author><keyname>Huang</keyname><forenames>Yanping</forenames></author><author><keyname>Zhang</keyname><forenames>Sai</forenames></author></authors><title>Partitioning Large Scale Deep Belief Networks Using Dropout</title><categories>stat.ML cs.LG cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1207.0580 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning methods have shown great promise in many practical
applications, ranging from speech recognition, visual object recognition, to
text processing. However, most of the current deep learning methods suffer from
scalability problems for large-scale applications, forcing researchers or users
to focus on small-scale problems with fewer parameters.
  In this paper, we consider a well-known machine learning model, deep belief
networks (DBNs) that have yielded impressive classification performance on a
large number of benchmark machine learning tasks. To scale up DBN, we propose
an approach that can use the computing clusters in a distributed environment to
train large models, while the dense matrix computations within a single machine
are sped up using graphics processors (GPU). When training a DBN, each machine
randomly drops out a portion of neurons in each hidden layer, for each training
case, making the remaining neurons only learn to detect features that are
generally helpful for producing the correct answer. Within our approach, we
have developed four methods to combine outcomes from each machine to form a
unified model. Our preliminary experiment on the mnst handwritten digit
database demonstrates that our approach outperforms the state of the art test
error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07097</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07097</id><created>2015-08-28</created><authors><author><keyname>Huynh</keyname><forenames>Hoai Nguyen</forenames></author><author><keyname>Legara</keyname><forenames>Erika Fille</forenames></author><author><keyname>Monterola</keyname><forenames>Christopher</forenames></author></authors><title>A Dynamical Model of Twitter Activity Profiles</title><categories>cs.SI cs.CY cs.HC physics.soc-ph</categories><comments>10 pages, 5 figures</comments><acm-class>J.2; J.4; I.6</acm-class><journal-ref>Hypertext '15, 49-57 (2015)</journal-ref><doi>10.1145/2700171.2791029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of the era of Big Data has allowed many researchers to dig into
various socio-technical systems, including social media platforms. In
particular, these systems have provided them with certain verifiable means to
look into certain aspects of human behavior. In this work, we are specifically
interested in the behavior of individuals on social media platforms---how they
handle the information they get, and how they share it. We look into Twitter to
understand the dynamics behind the users' posting activities---tweets and
retweets---zooming in on topics that peaked in popularity. Three mechanisms are
considered: endogenous stimuli, exogenous stimuli, and a mechanism that
dictates the decay of interest of the population in a topic. We propose a model
involving two parameters $\eta^\star$ and $\lambda$ describing the tweeting
behaviour of users, which allow us to reconstruct the findings of Lehmann et
al. (2012) on the temporal profiles of popular Twitter hashtags. With this
model, we are able to accurately reproduce the temporal profile of user
engagements on Twitter. Furthermore, we introduce an alternative in classifying
the collective activities on the socio-technical system based on the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07099</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07099</id><created>2015-08-28</created><authors><author><keyname>Iannacci</keyname><forenames>Francis</forenames></author><author><keyname>Huang</keyname><forenames>Yanping</forenames></author></authors><title>ChirpCast: Data Transmission via Audio</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present ChirpCast, a system for broadcasting network access
keys to laptops ultrasonically. This work explores several modulation
techniques for sending and receiving data using sound waves through commodity
speakers and built-in laptop microphones. Requiring only that laptop users run
a small application, the system successfully provides robust room-specific
broadcasting at data rates of 200 bits/second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07103</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07103</id><created>2015-08-28</created><authors><author><keyname>Zhao</keyname><forenames>Songlin</forenames></author></authors><title>Regularized Kernel Recursive Least Square Algoirthm</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most adaptive signal processing applications, system linearity is assumed
and adaptive linear filters are thus used. The traditional class of supervised
adaptive filters rely on error-correction learning for their adaptive
capability. The kernel method is a powerful nonparametric modeling tool for
pattern analysis and statistical signal processing. Through a nonlinear
mapping, kernel methods transform the data into a set of points in a
Reproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast
convergence rate in stationary scenario. However the good performance is
obtained at a cost of high computation complexity. Sparsification in kernel
methods is know to related to less computational complexity and memory
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07113</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07113</id><created>2015-08-28</created><authors><author><keyname>Zhu</keyname><forenames>Shixin</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojing</forenames></author></authors><title>Cyclic DNA codes over F2+uF2+vF2+uvF2</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the structure of cyclic DNA codes of arbitrary lengths
over the ring R=F2+uF2+vF2+uvF2 and establish relations to codes over R1=F2+uF2
by defining a Gray map between R and R1^2 where R1 is the ring with 4 elements.
Cyclic codes of arbitrary lengths over R satisfied the reverse constraint and
the reverse-complement constraint are studied in this paper. The GC content
constraint is considered in the last.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07123</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07123</id><created>2015-08-28</created><authors><author><keyname>Yamashina</keyname><forenames>Kazushi</forenames></author><author><keyname>Ohkawa</keyname><forenames>Takeshi</forenames></author><author><keyname>Ootsu</keyname><forenames>Kanemitsu</forenames></author><author><keyname>Yokota</keyname><forenames>Takashi</forenames></author></authors><title>Proposal of ROS-compliant FPGA Component for Low-Power Robotic Systems</title><categories>cs.AR cs.DC cs.RO</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, robots are required to be autonomous and their robotic
software are sophisticated. Robots have a problem of insufficient performance,
since it cannot equip with a high-performance microprocessor due to
battery-power operation. On the other hand, FPGA devices can accelerate
specific functions in a robot system without increasing power consumption by
implementing customized circuits. But it is difficult to introduce FPGA devices
into a robot due to large development cost of an FPGA circuit compared to
software. Therefore, in this study, we propose an FPGA component technology for
an easy integration of an FPGA into robots, which is compliant with ROS (Robot
Operating System). As a case study, we designed ROS-compliant FPGA component of
image labeling using Xilinx Zynq platform. The developed ROS-component FPGA
component performs 1.7 times faster compared to the ordinary ROS software
component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07126</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07126</id><created>2015-08-28</created><authors><author><keyname>Shannon</keyname><forenames>Lesley</forenames></author><author><keyname>Matthews</keyname><forenames>Eric</forenames></author><author><keyname>Doyle</keyname><forenames>Nicholas</forenames></author><author><keyname>Fedorova</keyname><forenames>Alexandra</forenames></author></authors><title>Performance monitoring for multicore embedded computing systems on FPGAs</title><categories>cs.AR cs.PF</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When designing modern embedded computing systems, most software programmers
choose to use multicore processors, possibly in combination with
general-purpose graphics processing units (GPGPUs) and/or hardware
accelerators. They also often use an embedded Linux O/S and run
multi-application workloads that may even be multi-threaded. Modern FPGAs are
large enough to combine multicore hard/soft processors with multiple hardware
accelerators as custom compute units, enabling entire embedded compute systems
to be implemented on a single FPGA. Furthermore, the large FPGA vendors also
support embedded Linux kernels for both their soft and embedded processors.
When combined with high-level synthesis to generate hardware accelerators using
a C-to-gates flows, the necessary primitives for a framework that can enable
software designers to use FPGAs as their custom compute platform now exist.
However, in order to ensure that computing resources are integrated and shared
effectively, software developers need to be able to monitor and debug the
runtime performance of the applications in their workload. This paper describes
ABACUS, a performance-monitoring framework that can be used to debug the
execution behaviours and interactions of multi-application workloads on
multicore systems. We also discuss how this framework is extensible for use
with hardware accelerators in heterogeneous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07127</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07127</id><created>2015-08-28</created><authors><author><keyname>Huang</keyname><forenames>Chun-Hsian</forenames></author><author><keyname>Tseng</keyname><forenames>Kwuan-Wei</forenames></author><author><keyname>Lin</keyname><forenames>Chih-Cheng</forenames></author><author><keyname>Lin</keyname><forenames>Fang-Yu</forenames></author><author><keyname>Hsiung</keyname><forenames>Pao-Ann</forenames></author></authors><title>Virtualization Architecture for NoC-based Reconfigurable Systems</title><categories>cs.AR cs.OS</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a virtualization architecture for NoC-based reconfigurable
systems. The motivation of this work is to develop a service-oriented
architecture that includes Partial Reconfigurable Region as a Service (PRRaaS)
and Processing Element as a Service (PEaaS) for software applications.
According to the requirements of software applications, new PEs can be created
on-demand by (re)configuring the logic resource of the PRRs in the FPGA, while
the configured PEs can also be virtualized to support multiple application
tasks at the same time. As a result, such a two-level virtualization mechanism,
including the gate-level virtualization and the PE-level virtualization,
enables an SoC to be dynamically adapted to changing application requirements.
Therefore, more software applications can be performed, and system performance
can be further enhanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07130</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07130</id><created>2015-08-28</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Parallel Dither and Dropout for Regularising Deep Neural Networks</title><categories>cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective regularisation during training can mean the difference between
success and failure for deep neural networks. Recently, dither has been
suggested as alternative to dropout for regularisation during batch-averaged
stochastic gradient descent (SGD). In this article, we show that these methods
fail without batch averaging and we introduce a new, parallel regularisation
method that may be used without batch averaging. Our results for
parallel-regularised non-batch-SGD are substantially better than what is
possible with batch-SGD. Furthermore, our results demonstrate that dither and
dropout are complimentary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07136</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07136</id><created>2015-08-28</created><authors><author><keyname>Stewart</keyname><forenames>Robert</forenames></author><author><keyname>Bhowmik</keyname><forenames>Deepayan</forenames></author><author><keyname>Michaelson</keyname><forenames>Greg</forenames></author><author><keyname>Wallace</keyname><forenames>Andrew</forenames></author></authors><title>RIPL: An Efficient Image Processing DSL for FPGAs</title><categories>cs.DC cs.PL</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Field programmable gate arrays (FPGAs) can accelerate image processing by
exploiting fine-grained parallelism opportunities in image operations. FPGA
language designs are often subsets or extensions of existing languages, though
these typically lack suitable hardware computation models so compiling them to
FPGAs leads to inefficient designs. Moreover, these languages lack image
processing domain specificity. Our solution is RIPL, an image processing domain
specific language (DSL) for FPGAs. It has algorithmic skeletons to express
image processing, and these are exploited to generate deep pipelines of highly
concurrent and memory-efficient image processing components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07139</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07139</id><created>2015-08-28</created><authors><author><keyname>Strauch</keyname><forenames>Tobias</forenames></author></authors><title>Using System Hyper Pipelining (SHP) to Improve the Performance of a
  Coarse-Grained Reconfigurable Architecture (CGRA) Mapped on an FPGA</title><categories>cs.AR</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well known method C-Slow Retiming (CSR) can be used to automatically
convert a given CPU into a multithreaded CPU with independent threads. These
CPUs are then called streaming or barrel processors. System Hyper Pipelining
(SHP) adds a new flexibility on top of CSR by allowing a dynamic number of
threads to be executed and by enabling the threads to be stalled, bypassed and
reordered. SHP is now applied on the programming elements (PE) of a
coarse-grained reconfigurable architecture (CGRA). By using SHP, more
performance can be achieved per PE. Fork-Join operations can be implemented on
a PE using the flexibility provided by SHP to dynamically adjust the number of
threads per PE. Multiple threads can share the same data locally, which greatly
reduces the data traffic load on the CGRA's routing structure. The paper shows
the results of a CGRA using SHP-ed RISC-V cores as PEs implemented on a FPGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07142</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07142</id><created>2015-08-28</created><authors><author><keyname>Gray</keyname><forenames>Ian</forenames></author><author><keyname>Chan</keyname><forenames>Yu</forenames></author><author><keyname>Garside</keyname><forenames>Jamie</forenames></author><author><keyname>Audsley</keyname><forenames>Neil</forenames></author><author><keyname>Wellings</keyname><forenames>Andy</forenames></author></authors><title>Transparent hardware synthesis of Java for predictable large-scale
  distributed systems</title><categories>cs.DC</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The JUNIPER project is developing a framework for the construction of
large-scale distributed systems in which execution time bounds can be
guaranteed. Part of this work involves the automatic implementation of input
Java code on FPGAs, both for speed and predictability. An important focus of
this work is to make the use of FPGAs transparent though runtime co-design and
partial reconfiguration. Initial results show that the use of Java does not
hamper hardware generation, and provides tight execution time estimates. This
paper describes an overview the approach taken, and presents some preliminary
results that demonstrate the promise in the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07148</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07148</id><created>2015-08-28</created><authors><author><keyname>Do</keyname><forenames>Thanh-Toan</forenames></author><author><keyname>Doan</keyname><forenames>Anh-Zung</forenames></author><author><keyname>Cheung</keyname><forenames>Ngai-Man</forenames></author></authors><title>Discrete Hashing with Deep Neural Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of learning binary hash codes for large
scale image search by proposing a novel hashing method based on deep neural
network. The advantage of our deep model over previous deep model used in
hashing is that our model contains necessary criteria for producing good codes
such as similarity preserving, balance and independence. Another advantage of
our method is that instead of relaxing the binary constraint of codes during
the learning process as most previous works, in this paper, by introducing the
auxiliary variable, we reformulate the optimization into two sub-optimization
steps allowing us to efficiently solve binary constraints without any
relaxation.
  The proposed method is also extended to the supervised hashing by leveraging
the label information such that the learned binary codes preserve the pairwise
label of inputs.
  The experimental results on three benchmark datasets show the proposed
methods outperform state-of-the-art hashing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07174</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07174</id><created>2015-08-28</created><authors><author><keyname>Borodin</keyname><forenames>Mikhail</forenames></author><author><keyname>De</keyname><forenames>Kaushik</forenames></author><author><keyname>Navarro</keyname><forenames>Jose Garcia</forenames></author><author><keyname>Golubkov</keyname><forenames>Dmitry</forenames></author><author><keyname>Klimentov</keyname><forenames>Alexei</forenames></author><author><keyname>Maeno</keyname><forenames>Tadashi</forenames></author><author><keyname>South</keyname><forenames>David</forenames></author><author><keyname>Collaboration</keyname><forenames>Alexandre Vaniachine on behalf of the ATLAS</forenames></author></authors><title>Unified System for Processing Real and Simulated Data in the ATLAS
  Experiment</title><categories>cs.DC hep-ex physics.ins-det</categories><comments>XVII International Conference Data Analytics and Management in Data
  Intensive Domains (DAMDID/RCDL), Obninsk, Russia, October 13 - 16, 2015</comments><msc-class>68T42</msc-class><acm-class>C.1.4; H.3.4; I.6.7; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The physics goals of the next Large Hadron Collider run include high
precision tests of the Standard Model and searches for new physics. These goals
require detailed comparison of data with computational models simulating the
expected data behavior. To highlight the role which modeling and simulation
plays in future scientific discovery, we report on use cases and experience
with a unified system built to process both real and simulated data of growing
volume and variety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07175</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07175</id><created>2015-08-28</created><updated>2015-09-12</updated><authors><author><keyname>Wang</keyname><forenames>Zihao</forenames></author><author><keyname>Cheung</keyname><forenames>Yiuming</forenames></author></authors><title>Competitive and Penalized Clustering Auto-encoder</title><categories>cs.LG</categories><comments>The paper has been withdrawn due to completing more experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auto-encoders (AE) has been widely applied in different fields of machine
learning. However, as a deep model, there are a large amount of learnable
parameters in the AE, which would cause over-fitting and slow learning speed in
practice. Many researchers have been study the intrinsic structure of AE and
showed different useful methods to regularize those parameters. In this paper,
we present a novel regularization method based on a clustering algorithm which
is able to classify the parameters into different groups. With this
regularization, parameters in a given group have approximate equivalent values
and over-fitting problem could be alleviated. Moreover, due to the competitive
behavior of clustering algorithm, this model also overcomes some intrinsic
problems of clustering algorithms like the determination of number of clusters.
Experiments on handwritten digits recognition verify the effectiveness of our
novel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07181</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07181</id><created>2015-08-28</created><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Lehner</keyname><forenames>Florian</forenames></author></authors><title>Fast Factorization of Cartesian products of Hypergraphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cartesian products of graphs and hypergraphs have been studied since the
1960s. For (un)directed hypergraphs, unique \emph{prime factor decomposition
(PFD)} results with respect to the Cartesian product are known. However, there
is still a lack of algorithms, that compute the PFD of directed hypergraphs
with respect to the Cartesian product.
  In this contribution, we focus on the algorithmic aspects for determining the
Cartesian prime factors of a finite, connected, directed hypergraph and present
a first polynomial time algorithm to compute its PFD. In particular, the
algorithm has time complexity $O(|E||V|r^2)$ for hypergraphs $H=(V,E)$, where
the rank $r$ is the maximum number of vertices contained in an hyperedge of
$H$. If $r$ is bounded, then this algorithm performs even in
$O(|E|\log^2(|V|))$ time. Thus, our method additionally improves also the time
complexity of PFD-algorithms designed for undirected hypergraphs that have time
complexity $O(|E||V|r^6\Delta^6)$, where $\Delta$ is the maximum number of
hyperedges a vertex is contained in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07192</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07192</id><created>2015-08-28</created><updated>2015-10-14</updated><authors><author><keyname>Bussas</keyname><forenames>Matthias</forenames></author><author><keyname>Sawade</keyname><forenames>Christoph</forenames></author><author><keyname>Scheffer</keyname><forenames>Tobias</forenames></author><author><keyname>Landwehr</keyname><forenames>Niels</forenames></author></authors><title>Varying-coefficient models with isotropic Gaussian process priors</title><categories>cs.LG stat.ML</categories><comments>17 pages, 4 Figures, minor revision</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study learning problems in which the conditional distribution of the
output given the input varies as a function of additional task variables. In
varying-coefficient models with Gaussian process priors, a Gaussian process
generates the functional relationship between the task variables and the
parameters of this conditional. Varying-coefficient models subsume hierarchical
Bayesian multitask models, but also generalizations in which the conditional
varies continuously, for instance, in time or space. However, Bayesian
inference in varying-coefficient models is generally intractable. We show that
inference for varying-coefficient models with isotropic Gaussian process priors
resolves to standard inference for a Gaussian process that can be solved
efficiently. MAP inference in this model resolves to multitask learning using
task and instance kernels, and inference for hierarchical Bayesian multitask
models can be carried out efficiently using graph-Laplacian kernels. We report
on experiments for geospatial prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07217</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07217</id><created>2015-08-28</created><authors><author><keyname>Sen</keyname><forenames>Sagnik</forenames></author></authors><title>On homomorphism of oriented graphs with respect to push operation</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An oriented graph is a directed graph without any cycle of length at most 2.
To push a vertex of a directed graph is to reverse the orientation of the arcs
incident to that vertex. Klostermeyer and MacGillivray defined push graphs
which are equivalence class of oriented graphs with respect to vertex pushing
operation. They studied the homomorphism of the equivalence classes of oriented
graphs with respect to push operation. In this article, we further study the
same topic and answer some of the questions asked in the above mentioned work.
The anti-twinned graph of an oriented graph is obtained by adding and pushing a
copy of each of its vertices. In particular, we show that two oriented graphs
are in a push relation if and only if they have isomorphic anti-twinned graphs.
Moreover, we study oriented homomorphisms of outerplanar graphs with girth at
least five, planar graphs and planar graphs with girth at least eight with
respect to the push operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07219</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07219</id><created>2015-08-28</created><updated>2015-11-27</updated><authors><author><keyname>B&#xfc;rgisser</keyname><forenames>Peter</forenames></author><author><keyname>Kohn</keyname><forenames>Kathl&#xe9;n</forenames></author><author><keyname>Lairez</keyname><forenames>Pierre</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author></authors><title>Computing the Chow variety of quadratic space curves</title><categories>math.AG cs.SC</categories><msc-class>14C05, 14-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadrics in the Grassmannian of lines in 3-space form a 19-dimensional
projective space. We study the subvariety of coisotropic hypersurfaces.
Following Gel'fand, Kapranov and Zelevinsky, it decomposes into Chow forms of
plane conics, Chow forms of pairs of lines, and Hurwitz forms of quadric
surfaces. We compute the ideals of these loci.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07222</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07222</id><created>2015-08-28</created><authors><author><keyname>Das</keyname><forenames>Sandip</forenames></author><author><keyname>Nandi</keyname><forenames>Soumen</forenames></author><author><keyname>Sen</keyname><forenames>Sagnik</forenames></author></authors><title>On chromatic number of colored mixed graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(m,n)$-colored mixed graph $G$ is a graph with its arcs having one of the
$m$ different colors and edges having one of the $n$ different colors. A
homomorphism $f$ of an $(m,n)$-colored mixed graph $G$ to an $(m,n)$-colored
mixed graph $H$ is a vertex mapping such that if $uv$ is an arc (edge) of color
$c$ in $G$, then $f(u)f(v)$ is an arc (edge) of color $c$ in $H$. The
\textit{$(m,n)$-colored mixed chromatic number} $\chi_{(m,n)}(G)$ of an
$(m,n)$-colored mixed graph $G$ is the order (number of vertices) of the
smallest homomorphic image of $G$. This notion was introduced by
Ne\v{s}et\v{r}il and Raspaud (2000, J. Combin. Theory, Ser. B 80, 147--155).
They showed that $\chi_{(m,n)}(G) \leq k(2m+n)^{k-1}$ where $G$ is a
$k$-acyclic colorable graph. We proved the tightness of this bound. We also
showed that the acyclic chromatic number of a graph is bounded by $k^2 + k^{2 +
\lceil log_{(2m+n)} log_{(2m+n)} k \rceil}$ if its $(m,n)$-colored mixed
chromatic number is at most $k$.
  Furthermore, using probabilistic method, we showed that for graphs with
maximum degree $\Delta$ its $(m,n)$-colored mixed chromatic number is at most
$2(\Delta-1)^{2m+n} (2m+n)^{\Delta-1}$. In particular, the last result directly
improves the upper bound $2\Delta^2 2^{\Delta}$ of oriented chromatic number of
graphs with maximum degree $\Delta$, obtained by Kostochka, Sopena and Zhu
(1997, J. Graph Theory 24, 331--340) to $2(\Delta-1)^2 2^{\Delta -1}$. We also
show that there exists a graph with maximum degree $\Delta$ and $(m,n)$-colored
mixed chromatic number at least $(2m+n)^{\Delta / 2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07230</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07230</id><created>2015-08-28</created><updated>2015-09-26</updated><authors><author><keyname>Janson</keyname><forenames>Svante</forenames></author></authors><title>On the tails of the limiting Quicksort distribution</title><categories>math.PR cs.DS</categories><comments>8 pages. v2: Typos corrected and some formulations improved</comments><msc-class>60C05, 68P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give asymptotics for the left and right tails of the limiting Quicksort
distribution. The results agree with, but are less precise than, earlier
non-rigorous results by Knessl and Spankowski.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07231</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07231</id><created>2015-08-28</created><authors><author><keyname>Turcksin</keyname><forenames>Bruno</forenames></author><author><keyname>Heister</keyname><forenames>Timo</forenames></author><author><keyname>Bangerth</keyname><forenames>Wolfgang</forenames></author></authors><title>Clone and graft: Testing scientific applications as they are built</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes our experience developing and maintaining automated
tests for scientific applications. The main idea evolves around building on
already existing tests by cloning and grafting. The idea is demonstrated on a
minimal model problem written in Python.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07240</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07240</id><created>2015-08-28</created><authors><author><keyname>Parand</keyname><forenames>Kourosh</forenames></author><author><keyname>Khaleqi</keyname><forenames>Sajjad</forenames></author></authors><title>Rational Chebyshev of Second Kind Collocation Method for Solving a Class
  of Astrophysics Problems</title><categories>cs.NA math.NA</categories><comments>arXiv admin note: text overlap with arXiv:1008.2063</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lane-Emden equation has been used to model several phenomenas in
theoretical physics, mathematical physics and astrophysics such as the theory
of stellar structure. This study is an attempt to utilize the collocation
method with the Rational Chebyshev of Second Kind function (RSC) to solve the
Lane-Emden equation over the semi-infinit interval [0; +infinity). According to
well-known results and comparing with previous methods, it can be said that
this method is efficient and applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07243</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07243</id><created>2015-08-28</created><authors><author><keyname>Reyes</keyname><forenames>J. C. De los</forenames></author><author><keyname>Sch&#xf6;nlieb</keyname><forenames>C. -B.</forenames></author><author><keyname>Valkonen</keyname><forenames>T.</forenames></author></authors><title>Bilevel parameter learning for higher-order total variation
  regularisation models</title><categories>math.OC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bilevel optimisation approach for parameter learning in
higher-order total variation image reconstruction models. Apart from the least
squares cost functional, naturally used in bilevel learning, we propose and
analyse an alternative cost, based on a Huber regularised TV-seminorm.
Differentiability properties of the solution operator are verified and a
first-order optimality system is derived. Based on the adjoint information, a
quasi-Newton algorithm is proposed for the numerical solution of the bilevel
problems. Numerical experiments are carried out to show the suitability of our
approach and the improved performance of the new cost functional. Thanks to the
bilevel optimisation framework, also a detailed comparison between TGV$^2$ and
ICTV is carried out, showing the advantages and shortcomings of both
regularisers, depending on the structure of the processed images and their
noise level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07246</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07246</id><created>2015-08-28</created><updated>2016-02-04</updated><authors><author><keyname>Taha</keyname><forenames>Ahmad F.</forenames></author><author><keyname>Qi</keyname><forenames>Junjian</forenames></author><author><keyname>Wang</keyname><forenames>Jianhui</forenames></author><author><keyname>Panchal</keyname><forenames>Jitesh H.</forenames></author></authors><title>Risk Mitigation for Dynamic State Estimation Against Cyber Attacks and
  Unknown Inputs</title><categories>cs.SY cs.CR math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phasor measurement units (PMUs) can be effectively utilized for the
monitoring and control of the power grid. As the cyber-world becomes
increasingly embedded into power grids, the risks of this inevitable evolution
become serious. In this paper, we present a risk mitigation strategy, based on
dynamic state estimation, to eliminate threat levels from the grid's unknown
inputs and potential attack vectors. The strategy requires (a) the potentially
incomplete knowledge of power system models and parameters and (b) real-time
PMU measurements.
  First, we utilize state-of-the-art dynamic state estimators, representing the
higher order depictions of linearized, small-signal model or nonlinear
representations of the power system dynamics for state- and unknown inputs
estimation. Second, estimates of potential attack vectors are obtained through
an attack detection algorithm. Third, the estimation and detection components
are seamlessly utilized in an optimization framework to determine the PMU
measurements under cyber-attacks. Finally, a risk mitigation strategy is
employed to guarantee the elimination of threats from attacks, ensuring the
observability of the power system through available safe measurements.
Numerical results on a 16-machine 68-bus system are included to illustrate the
effectiveness of the proposed approach. Insightful suggestions, extensions, and
open research problems are also posed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07252</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07252</id><created>2015-08-28</created><authors><author><keyname>Taha</keyname><forenames>Ahmad F.</forenames></author><author><keyname>Qi</keyname><forenames>Junjian</forenames></author><author><keyname>Wang</keyname><forenames>Jianhui</forenames></author><author><keyname>Panchal</keyname><forenames>Jitesh H.</forenames></author></authors><title>Dynamic State Estimation under Cyber Attacks: A Comparative Study of
  Kalman Filters and Observers</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilizing highly synchronized measurements from synchrophasors, dynamic state
estimation (DSE) can be applied for real-time monitoring of smart grids.
Concurrent DSE studies for power systems are intolerant to unknown inputs and
potential attack vectors --- a research gap we will fill in this work.
Particularly, we (a) present an overview of concurrent estimation techniques,
highlighting key deficiencies, (b) develop DSE methods based on cubature Kalman
filter and dynamic observers, (c) rigorously examine the strengths and
weaknesses of the proposed methods under attack-vectors and unknown inputs, and
(d) provide comprehensive recommendations for DSE. Numerical results and
in-depth remarks are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07256</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07256</id><created>2015-08-28</created><authors><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Toru&#x144;czyk</keyname><forenames>Szymon</forenames></author></authors><title>On ultralimits of sparse graph classes</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of nowhere denseness is one of the central concepts of the
recently developed theory of sparse graphs. We study the properties of nowhere
dense graph classes by investigating appropriate limit objects defined using
the ultraproduct construction. It appears that different equivalent definitions
of nowhere denseness, for example via quasi-wideness or the splitter game,
correspond to natural notions for the limit objects that are conceptually
simpler and allow for less technically involved reasonings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07257</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07257</id><created>2015-08-27</created><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Structure of the automorphism group of the augmented cube graph</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \noindent The augmented cube graph $AQ_n$ is the Cayley graph of
$\mathbb{Z}_2^n$ with respect to the set of $2n-1$ generators $\{e_1,e_2,
\ldots,e_n, 00\ldots0011, 00\ldots0111, 11\ldots1111 \}$. It is known that the
order of the automorphism group of the graph $AQ_n$ is $2^{n+3}$, for all $n
\ge 4$. In the present paper, we obtain the structure of the automorphism group
of $AQ_n$ to be \[ \Aut(AQ_n) \cong \mathbb{Z}_2^n \rtimes D_8~~(n \ge 4),\]
where $D_8$ is the dihedral group of order 8. It is shown that the Cayley graph
$AQ_3$ is non-normal and that $AQ_n$ is normal for all $n \ge 4$. We also
analyze the clique structure of $AQ_4$ and show that the automorphism group of
$AQ_4$ is isomorphic to that of $AQ_3$: \[ \Aut(AQ_4) \cong \Aut(AQ_3) \cong
(D_8 \times D_8) \rtimes C_2.\] All the nontrivial blocks of $AQ_4$ are also
determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07265</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07265</id><created>2015-08-28</created><updated>2015-09-29</updated><authors><author><keyname>Strano</keyname><forenames>Emanuele</forenames></author><author><keyname>Shai</keyname><forenames>Saray</forenames></author><author><keyname>Dobson</keyname><forenames>Simon</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author></authors><title>Multiplex networks in metropolitan areas: generic features and local
  effects</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 8 figures. Final version with an additional discussion on
  the total congestion</comments><journal-ref>Journal Royal Society Interface 12:20150651 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most large cities are spanned by more than one transportation system. These
different modes of transport have usually been studied separately: it is
however important to understand the impact on urban systems of the coupling
between them and we report in this paper an empirical analysis of the coupling
between the street network and the subway for the two large metropolitan areas
of London and New York. We observe a similar behaviour for network quantities
related to quickest paths suggesting the existence of generic mechanisms
operating beyond the local peculiarities of the specific cities studied. An
analysis of the betweenness centrality distribution shows that the introduction
of underground networks operate as a decentralising force creating congestions
in places located at the end of underground lines. Also, we find that
increasing the speed of subways is not always beneficial and may lead to
unwanted uneven spatial distributions of accessibility. In fact, for London --
but not for New York -- there is an optimal subway speed in terms of global
congestion. These results show that it is crucial to consider the full,
multimodal, multi-layer network aspects of transportation systems in order to
understand the behaviour of cities and to avoid possible negative side-effects
of urban planning decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07266</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07266</id><created>2015-08-28</created><authors><author><keyname>Kim</keyname><forenames>Suin</forenames></author><author><keyname>Park</keyname><forenames>Sungjoon</forenames></author><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author><author><keyname>Kim</keyname><forenames>Sooyoung</forenames></author><author><keyname>Byun</keyname><forenames>Jeongmin</forenames></author><author><keyname>Oh</keyname><forenames>Alice</forenames></author></authors><title>Understanding Editing Behaviors in Multilingual Wikipedia</title><categories>cs.SI cs.CL cs.CY</categories><comments>34 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilingualism is common offline, but we have a more limited understanding
of the ways multilingualism is displayed online and the roles that
multilinguals play in the spread of content between speakers of different
languages. We take a computational approach to studying multilingualism using
one of the largest user-generated content platforms, Wikipedia. We study
multilingualism by collecting and analyzing a large dataset of the content
written by multilingual editors of the English, German, and Spanish editions of
Wikipedia. This dataset contains over two million paragraphs edited by over
15,000 multilingual users from July 8 to August 9, 2013. We analyze these
multilingual editors in terms of their engagement, interests, and language
proficiency in their primary and non-primary (secondary) languages and find
that the English edition of Wikipedia displays different dynamics from the
Spanish and German editions. Users primarily editing the Spanish and German
editions make more complex edits than users who edit these editions as a second
language. In contrast, users editing the English edition as a second language
make edits that are just as complex as the edits by users who primarily edit
the English edition. In this way, English serves a special role bringing
together content written by multilinguals from many language editions.
Nonetheless, language remains a formidable hurdle to the spread of content: we
find evidence for a complexity barrier whereby editors are less likely to edit
complex content in a second language. In addition, we find that multilinguals
are less engaged and show lower levels of language proficiency in their second
languages. We also examine the topical interests of multilingual editors and
find that there is no significant difference between primary and non-primary
editors in each language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07269</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07269</id><created>2015-08-28</created><authors><author><keyname>Zaeemzadeh</keyname><forenames>Alireza</forenames></author><author><keyname>Joneidi</keyname><forenames>Mohsen</forenames></author><author><keyname>Shahrasbi</keyname><forenames>Behzad</forenames></author><author><keyname>Rahnavard</keyname><forenames>Nazanin</forenames></author></authors><title>Missing Spectrum-Data Recovery in Cognitive Radio Networks Using
  Piecewise Constant Nonnegative Matrix Factorization</title><categories>cs.OH cs.IT math.IT</categories><comments>6 pages, 6 figures, Accepted for presentation in MILCOM'15 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a missing spectrum data recovery technique for
cognitive radio (CR) networks using Nonnegative Matrix Factorization (NMF). It
is shown that the spectrum measurements collected from secondary users (SUs)
can be factorized as product of a channel gain matrix times an activation
matrix. Then, an NMF method with piecewise constant activation coefficients is
introduced to analyze the measurements and estimate the missing spectrum data.
The proposed optimization problem is solved by a Majorization-Minimization
technique. The numerical simulation verifies that the proposed technique is
able to accurately estimate the missing spectrum data in the presence of noise
and fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07272</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07272</id><created>2015-08-28</created><authors><author><keyname>Shore</keyname><forenames>Jesse</forenames></author></authors><title>Market Formation as Transitive Closure: the Evolving Pattern of Trade in
  Music</title><categories>cs.SI physics.soc-ph</categories><comments>accepted for publication at Network Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Where do new markets come from? I construct a network model in which national
markets are nodes and flows of recorded music between them are links and
conduct a longitudinal analysis of the global pattern of trade in the period
1976 to 2010. I hypothesize that new export markets are developed through a
process of transitive closure in the network of international trade. When two
countries' markets experience the same social influences, it brings them close
enough together for new homophilous ties to be formed. The implication is that
consumption of foreign products helps, not hurts, home-market producers develop
overseas markets, but only in those countries that have a history of consuming
the same foreign products that were consumed in the home market. Selling in a
market changes what is valued in that market, and new market formation is a
consequence of having social influences in common.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="82000" completeListSize="102538">1122234|83001</resumptionToken>
</ListRecords>
</OAI-PMH>
