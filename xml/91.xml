<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:01:50Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|90001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01951</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01951</id><created>2016-01-07</created><authors><author><keyname>Klan</keyname><forenames>Petr</forenames></author></authors><title>Optimized Integral Controller Searching Prime Number Orders</title><categories>cs.SY</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of integration is generally applicable to automatic control of
processes. As shown in this paper, integral controller performs efficient
searches in the extensive prime sets, too. An inspiration by the simple
analytic rules for PID controller tuning results in integral controller that
ensures predictable work regardless of the cardinality of primes. It gives an
innovative application of the feedback control which relates to a gradient
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01952</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01952</id><created>2016-01-07</created><updated>2016-01-11</updated><authors><author><keyname>Devasia</keyname><forenames>Santosh</forenames></author><author><keyname>Lee</keyname><forenames>Alexander</forenames></author></authors><title>A Scalable Low-Cost-UAV Traffic Network (uNet)</title><categories>cs.SY</categories><comments>To be submitted to journal, 21 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a new Unmanned Aerial Vehicle (UAV) operation paradigm
to enable a large number of relatively low-cost UAVs to fly
beyond-line-of-sight without costly sensing and communication systems or
substantial human intervention in individual UAV control. Under current
free-flight-like paradigm, wherein a UAV can travel along any route as long as
it avoids restricted airspace and altitudes. However, this requires expensive
on-board sensing and communication as well as substantial human effort in order
to ensure avoidance of obstacles and collisions. The increased cost serves as
an impediment to the emergence and development of broader UAV applications. The
main contribution of this work is to propose the use of pre-established route
network for UAV traffic management, which allows: (i) pre- mapping of obstacles
along the route network to reduce the onboard sensing requirements and the
associated costs for avoiding such obstacles; and (ii) use of well-developed
routing algorithms to select UAV schedules that avoid conflicts. Available
GPS-based navigation can be used to fly the UAV along the selected route and
time schedule with relatively low added cost, which therefore, reduces the
barrier to entry into new UAV-applications market. Finally, this article
proposes a new decoupling scheme for conflict-free transitions between edges of
the route network at each node of the route network to reduce potential
conflicts between UAVs and ensuing delays. A simulation example is used to
illustrate the proposed uNet approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01958</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01958</id><created>2016-01-08</created><authors><author><keyname>Ducoffe</keyname><forenames>Guillaume</forenames></author><author><keyname>Legay</keyname><forenames>Sylvain</forenames></author><author><keyname>Nisse</keyname><forenames>Nicolas</forenames></author></authors><title>On computing tree and path decompositions with metric constraints on the
  bags</title><categories>cs.CC cs.DS</categories><comments>50 pages, 39 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We here investigate on the complexity of computing the \emph{tree-length} and
the \emph{tree-breadth} of any graph $G$, that are respectively the best
possible upper-bounds on the diameter and the radius of the bags in a tree
decomposition of $G$. \emph{Path-length} and \emph{path-breadth} are similarly
defined and studied for path decompositions. So far, it was already known that
tree-length is NP-hard to compute. We here prove it is also the case for
tree-breadth, path-length and path-breadth. Furthermore, we provide a more
detailed analysis on the complexity of computing the tree-breadth. In
particular, we show that graphs with tree-breadth one are in some sense the
hardest instances for the problem of computing the tree-breadth. We give new
properties of graphs with tree-breadth one. Then we use these properties in
order to recognize in polynomial-time all graphs with tree-breadth one that are
planar or bipartite graphs. On the way, we relate tree-breadth with the notion
of \emph{$k$-good} tree decompositions (for $k=1$), that have been introduced
in former work for routing. As a byproduct of the above relation, we prove that
deciding on the existence of a $k$-good tree decomposition is NP-complete (even
if $k=1$). All this answers open questions from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01963</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01963</id><created>2015-12-13</created><authors><author><keyname>Morrison</keyname><forenames>Greg</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author><author><keyname>Pammolli</keyname><forenames>Fabio</forenames></author></authors><title>Disambiguation of Patent Inventors and Assignees Using High-Resolution
  Geolocation Data</title><categories>cs.DL physics.data-an physics.soc-ph</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patent data represent a significant source of information on innovation and
the evolution of technology through networks of citations, co-invention and
co-assignment of new patents. A major obstacle to extracting useful information
from this data is the problem of name disambiguation: linking alternate
spellings of individuals or institutions to a single identifier to uniquely
determine the parties involved in the creation of a technology. In this paper,
we describe a new algorithm that uses high-resolution geolocation to
disambiguate both inventor and assignees on more than 3.6 million patents found
in the European Patent Office (EPO), under the Patent Cooperation treaty (PCT),
and in the US Patent and Trademark Office (USPTO). We show that our algorithm
has both high precision and recall in comparison to a manual disambiguation of
EPO assignee names in Boston and Paris, and show it performs well for a
benchmark of USPTO inventor names that can be linked to a high-resolution
address (but poorly for inventors that never provided a high quality address).
The most significant benefit of this work is the high quality assignee
disambiguation with worldwide coverage coupled with an inventor disambiguation
that is competitive with other state of the art approaches. To our knowledge
this is the broadest and most accurate simultaneous disambiguation and
cross-linking of the inventor and assignee names for a significant fraction of
patents in these three major patent collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01974</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01974</id><created>2016-01-08</created><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>P&#xe1;l</keyname><forenames>D&#xe1;vid</forenames></author></authors><title>Scale-Free Online Learning</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1502.05744</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design algorithms for online linear optimization that have optimal regret
and at the same time do not need to know any upper or lower bounds on the norm
of the loss vectors. We achieve adaptiveness to the norms of the loss vectors
by scale invariance, i.e., our algorithms make exactly the same decisions if
the sequence of loss vectors is multiplied by any positive constant. One of our
algorithms works for any decision set, bounded or unbounded. For unbounded
decisions sets, this is the first adaptive algorithm for online linear
optimization with a non-vacuous regret bound.
  We also study a popular scale-free variant of online mirror descent
algorithm, and we show that in two natural settings it has linear or worse
regret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01975</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01975</id><created>2016-01-08</created><authors><author><keyname>Fefferman</keyname><forenames>Bill</forenames></author><author><keyname>Lin</keyname><forenames>Cedric</forenames></author></authors><title>Quantum Merlin Arthur with Exponentially Small Gap</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of QMA proof systems with inverse exponentially small
promise gap. We show that this class can be exactly characterized by PSPACE,
the class of problems solvable with a polynomial amount of memory. As
applications we show that a &quot;precise&quot; version of the Local Hamiltonian problem
is PSPACE-complete, and give a provable setting in which the ability to prepare
PEPS states is not as powerful as the ability to prepare the ground state of
general Local Hamiltonians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01983</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01983</id><created>2016-01-08</created><authors><author><keyname>Bursalioglu</keyname><forenames>Ozgun Y.</forenames></author><author><keyname>Wang</keyname><forenames>Chenwei</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>RRH based Massive MIMO with &quot;on the Fly&quot; Pilot Contamination Control</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures, extension of ICC 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense large-scale antenna deployments are one of the most promising
technologies for delivering very large throughputs per unit area in the
downlink (DL) of cellular networks. We consider such a dense deployment
involving a distributed system formed by multi-antenna remote radio head (RRH)
units connected to the same fronthaul serving a geographical area. Knowledge of
the DL channel between each active user and its nearby RRH antennas is most
efficiently obtained at the RRHs via reciprocity based training, that is, by
estimating a user's channel using uplink (UL) pilots transmitted by the user,
and exploiting the UL/DL channel reciprocity.
  We consider aggressive pilot reuse across an RRH system, whereby a single
pilot dimension is simultaneously assigned to multiple active users. We
introduce a novel coded pilot approach, which allows each RRH unit to detect
pilot collisions, i.e., when more than a single user in its proximity uses the
same pilot dimensions. Thanks to the proposed coded pilot approach, pilot
contamination can be substantially avoided. As shown, such strategy can yield
densification benefits in the form of increased multiplexing gain per UL pilot
dimension with respect to conventional reuse schemes and some recent approaches
assigning pseudorandom pilot vectors to the active users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01988</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01988</id><created>2016-01-08</created><authors><author><keyname>Li</keyname><forenames>Chen</forenames></author><author><keyname>Adcock</keyname><forenames>Ben</forenames></author></authors><title>Compressed sensing with local structure: uniform recovery guarantees for
  the sparsity in levels class</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing, it is often desirable to consider signals possessing
additional structure beyond sparsity. One such structured signal model - which
forms the focus of this paper - is the local sparsity in levels class. This
class has recently found applications in problems such as compressive imaging,
multi-sensor acquisition systems and sparse regularization in inverse problems.
In this paper we present uniform recovery guarantees for this class when the
measurement matrix corresponds to a subsampled isometry. We do this by
establishing a variant of the standard restricted isometry property for the
sparsity in levels class, known as the restricted isometry property in levels.
Interestingly, besides the usual log factors, our uniform recovery guarantees
are simpler and less stringent than existing nonuniform recovery guarantees.
For the particular case of discrete Fourier sampling with Haar wavelet
sparsity, a corollary of our main theorem yields a new recovery guarantee which
improves over the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02014</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02014</id><created>2016-01-10</created><authors><author><keyname>Martin</keyname><forenames>Carlos</forenames></author></authors><title>Predicting the large-scale evolution of tag systems</title><categories>cs.FL cs.DM math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for predicting the large-scale evolution of a tag system
from its production rules. The tag system's evolution is divided into stages
called 'epochs' in which the tag system evolves monotonously. The distribution
of strings of symbols in the queue at the beginning of an epoch determines the
large-scale behavior of the tag system during that epoch, including its growth
rate. To predict the tag system's large-scale properties over multiple epochs,
we show how to predict the next epoch's initial queue contents from the current
epoch's initial queue contents. We compare the values predicted by this method
to simulations and find that great prediction accuracy is retained over several
epochs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02034</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02034</id><created>2016-01-08</created><authors><author><keyname>Jain</keyname><forenames>Ayush</forenames></author><author><keyname>Seo</keyname><forenames>Joon Young</forenames></author><author><keyname>Goel</keyname><forenames>Karan</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Andrew</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author><author><keyname>Sundaram</keyname><forenames>Hari</forenames></author></authors><title>It's just a matter of perspective(s): Crowd-Powered Consensus
  Organization of Corpora</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of organizing a collection of objects - images, videos -
into clusters, using crowdsourcing. This problem is notoriously hard for
computers to do automatically, and even with crowd workers, is challenging to
orchestrate: (a) workers may cluster based on different latent hierarchies or
perspectives; (b) workers may cluster at different granularities even when
clustering using the same perspective; and (c) workers may only see a small
portion of the objects when deciding how to cluster them (and therefore have
limited understanding of the &quot;big picture&quot;). We develop cost-efficient,
accurate algorithms for identifying the consensus organization (i.e., the
organizing perspective most workers prefer to employ), and incorporate these
algorithms into a cost-effective workflow for organizing a collection of
objects, termed ORCHESTRA. We compare our algorithms with other algorithms for
clustering, on a variety of real-world datasets, and demonstrate that ORCHESTRA
organizes items better and at significantly lower costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02039</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02039</id><created>2016-01-08</created><authors><author><keyname>Acemoglu</keyname><forenames>Daron</forenames></author><author><keyname>Makhdoumi</keyname><forenames>Ali</forenames></author><author><keyname>Malekian</keyname><forenames>Azarakhsh</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author></authors><title>Informational Braess' Paradox: The Effect of Information on Traffic
  Congestion</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To systematically study the implications of additional information about
routes provided to certain users (e.g., via GPS-based route guidance systems),
we introduce a new class of congestion games in which users have differing
information sets about the available edges and can only use routes consisting
of edges in their information set. After defining the notion of Information
Constrained Wardrop Equilibrium (ICWE) for this class of congestion games and
studying its basic properties, we turn to our main focus: whether additional
information can be harmful (in the sense of generating greater equilibrium
costs/delays). We formulate this question in the form of Informational Braess'
Paradox (IBP), which extends the classic Braess' Paradox in traffic equilibria,
and asks whether users receiving additional information can become worse off.
We provide a comprehensive answer to this question showing that in any network
in the series of linearly independent (SLI) class, which is a strict subset of
series-parallel network, IBP cannot occur, and in any network that is not in
the SLI class, there exists a configuration of edge-specific cost functions for
which IBP will occur. In the process, we establish several properties of the
SLI class of networks, which are comprised of linearly independent networks
joined together. These properties include the characterization of the
complement of the SLI class in terms of embedding a specific set of subgraphs,
and also show that whether a graph is SLI can be determined in linear time. We
further prove that the worst-case inefficiency performance of ICWE is no worse
than the standard Wardrop Equilibrium with one type of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02049</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02049</id><created>2016-01-08</created><updated>2016-01-12</updated><authors><author><keyname>Adamczak</keyname><forenames>Rados&#x142;aw</forenames></author></authors><title>A note on the sample complexity of the Er-SpUD algorithm by Spielman,
  Wang and Wright for exact recovery of sparsely used dictionaries</title><categories>math.PR cs.LG math.ST stat.TH</categories><comments>Minor typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering an invertible $n \times n$ matrix $A$
and a sparse $n \times p$ random matrix $X$ based on the observation of $Y =
AX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Using
only elementary tools from the theory of empirical processes we show that a
version of the Er-SpUD algorithm by Spielman, Wang and Wright with high
probability recovers $A$ and $X$ exactly, provided that $p \ge Cn\log n$, which
is optimal up to the constant $C$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02059</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02059</id><created>2016-01-08</created><authors><author><keyname>Black</keyname><forenames>Andrew P.</forenames></author><author><keyname>Bruce</keyname><forenames>Kim B.</forenames></author><author><keyname>Noble</keyname><forenames>James</forenames></author></authors><title>The Essence of Inheritance</title><categories>cs.PL</categories><comments>This paper was submitted for inclusion in a Festschrift entitled &quot;A
  list of successes that can change the world&quot;, to be published by Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming languages serve a dual purpose: to communicate programs to
computers, and to communicate programs to humans. Indeed, it is this dual
purpose that makes programming language design a constrained and challenging
problem. Inheritance is an essential aspect of that second purpose: it is a
tool to improve communication. Humans understand new concepts most readily by
first looking at a number of concrete examples, and later abstracting over
those examples. The essence of inheritance is that it mirrors this process: it
provides a formal mechanism for moving from the concrete to the abstract.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02068</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02068</id><created>2016-01-08</created><updated>2016-01-27</updated><authors><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Minimax Subsampling for Estimation and Prediction in Low-Dimensional
  Linear Regression</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>35 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subsampling strategies are derived to sample a small portion of design (data)
points in a low-dimensional linear regression model $y=X\beta+\varepsilon$ with
near-optimal statistical rates. Our results apply to both problems of
estimation of the underlying linear model $\beta$ and predicting the
real-valued response $y$ of a new data point $x$. The derived subsampling
strategies are minimax optimal under the fixed design setting, up to a small
$(1+\epsilon)$ relative factor. We also give interpretable subsampling
probabilities for the random design setting and demonstrate explicit gaps in
statistial rates between optimal and baseline (e.g., uniform) subsampling
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02069</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02069</id><created>2016-01-08</created><authors><author><keyname>Smirnov</keyname><forenames>Andrei V</forenames></author></authors><title>Dynamic Transposition of Melodic Sequences on Digital Devices</title><categories>cs.SD</categories><comments>13 pages, 5 figures, 3 music samples</comments><msc-class>11N25, 70J40, 11K70, 11J70, 42A45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is proposed which enables one to produce musical compositions by
using transposition in place of harmonic progression. A transposition scale is
introduced to provide a set of intervals commensurate with the musical scale,
such as chromatic or just intonation scales. A sequence of intervals selected
from the transposition scale is used to shift instrument frequency at
predefined times during the composition which serves as a harmonic sequence of
a composition. A transposition sequence constructed in such a way can be
extended to a hierarchy of sequences. The fundamental sound frequency of an
instrument is obtained as a product of the base frequency, instrument key
factor, and a cumulative product of respective factors from all the harmonic
sequences. The multiplication factors are selected from subsets of rational
numbers, which form instrument scales and transposition scales of different
levels. Each harmonic sequence can be related to its own transposition scale,
or a single scale can be used for all levels. When composing for an orchestra
of instruments, harmonic sequences and instrument scales can be assigned
independently to each musical instrument. The method solves the problem of
using just intonation scale across multiple octaves as well as simplifies
writing of instrument scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02071</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02071</id><created>2016-01-08</created><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author><author><keyname>Baeza-Yates</keyname><forenames>Ricardo</forenames></author></authors><title>Sentiment Visualisation Widgets for Exploratory Search</title><categories>cs.HC</categories><comments>Presented at the Social Personalization Workshop held jointly with
  ACM Hypertext 2014. 6 pages</comments><acm-class>H.3.3; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the usage of \emph{visualisation widgets} for exploratory
search with \emph{sentiment} as a facet. Starting from specific design goals
for depiction of ambivalence in sentiment, two visualization widgets were
implemented: \emph{scatter plot} and \emph{parallel coordinates}. Those widgets
were evaluated against a text baseline in a small-scale usability study with
exploratory tasks using Wikipedia as dataset. The study results indicate that
users spend more time browsing with scatter plots in a positive way. A post-hoc
analysis of individual differences in behavior revealed that when considering
two types of users, \emph{explorers} and \emph{achievers}, engagement with
scatter plots is positive and significantly greater \textit{when users are
explorers}. We discuss the implications of these findings for sentiment-based
exploratory search and personalised user interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02075</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02075</id><created>2016-01-08</created><authors><author><keyname>Shim</keyname><forenames>Hyungbo</forenames></author><author><keyname>Park</keyname><forenames>Gyunghoon</forenames></author><author><keyname>Joo</keyname><forenames>Youngjun</forenames></author><author><keyname>Back</keyname><forenames>Juhoon</forenames></author><author><keyname>Jo</keyname><forenames>Nam Hoon</forenames></author></authors><title>Yet Another Tutorial of Disturbance Observer: Robust Stabilization and
  Recovery of Nominal Performance</title><categories>cs.SY math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a tutorial-style review on the recent results about the
disturbance observer (DOB) in view of robust stabilization and recovery of the
nominal performance. The analysis is based on the case when the bandwidth of
Q-filter is large, and it is explained in a pedagogical manner that, even in
the presence of plant uncertainties and disturbances, the behavior of real
uncertain plant can be made almost similar to that of disturbance-free nominal
system both in the transient and in the steady-state. The conventional DOB is
interpreted in a new perspective, and its restrictions and extensions are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02076</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02076</id><created>2016-01-08</created><authors><author><keyname>Mungmode</keyname><forenames>Sachin</forenames></author><author><keyname>Sedamkar</keyname><forenames>R. R.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Niranjan</forenames></author></authors><title>An Enhanced Edge Adaptive Steganography Approach Using Threshold Value
  for Region Selection</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to improve the quality and the modification rate of a
Stego Image. The input image provided for estimating the quality of an image
and the modified rate is a bitmap image. The threshold value is used as a
parameter for selecting the high frequency pixels from the Cover Image. The
data embedding process are performed on the pixels that are found with the help
of Threshold value by using LSBMR. The quality of an image is estimated by the
value of PSNR and the modification rate of an image is estimated by the value
of MSE. The proposed approach achieves about 0.2 to 0.6 % of improvement in the
quality of an image and about 4 to 10 % of improvement in the modification rate
of an image compared to the edge detection techniques such as Sobel and Canny.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02082</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02082</id><created>2016-01-09</created><authors><author><keyname>Liang</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Mixed-ADC Massive MIMO Uplink in Frequency-Selective Channels</title><categories>cs.IT math.IT</categories><comments>30 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to investigate the recently developed mixed-ADC
architecture for frequency-selective channels. Multi-carrier techniques such as
orthogonal frequency division multiplexing (OFDM) are employed to handle
inter-symbol interference (ISI). A frequency-domain equalizer is designed for
mitigating the inter-carrier interference (ICI) introduced by the strong
nonlinearity of coarse quantization. For static single-input-multiple-output
(SIMO) channels, a closed-form expression of the generalized mutual information
(GMI) is derived, and based on which the linear frequency-domain equalizer is
optimized. The analysis is then extended to ergodic time-varying SIMO channels,
where numerically tight lower and upper bounds of the GMI are derived. The
analytical results are naturally applicable to the multi-user scenario, for
both static and time-varying fading channels. Extensive numerical studies
reveal that the mixed-ADC architecture with a small proportion of
high-resolution ADCs is sufficient to achieve a large portion of the capacity
of ideal conventional architecture, and that it is effective in eliminating the
error floor encountered by receivers with coarse quantization only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02088</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02088</id><created>2016-01-09</created><authors><author><keyname>Kappes</keyname><forenames>J&#xf6;rg Hendrik</forenames></author><author><keyname>Swoboda</keyname><forenames>Paul</forenames></author><author><keyname>Savchynskyy</keyname><forenames>Bogdan</forenames></author><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Multicuts and Perturb &amp; MAP for Probabilistic Graph Clustering</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a probabilistic graphical model formulation for the graph
clustering problem. This enables to locally represent uncertainty of image
partitions by approximate marginal distributions in a mathematically
substantiated way, and to rectify local data term cues so as to close contours
and to obtain valid partitions.
  We exploit recent progress on globally optimal MAP inference by integer
programming and on perturbation-based approximations of the log-partition
function, in order to sample clusterings and to estimate marginal distributions
of node-pairs both more accurately and more efficiently than state-of-the-art
methods. Our approach works for any graphically represented problem instance.
This is demonstrated for image segmentation and social network cluster
analysis. Our mathematical ansatz should be relevant also for other
combinatorial problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02093</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02093</id><created>2016-01-09</created><updated>2016-01-13</updated><authors><author><keyname>Mor&#xe8;re</keyname><forenames>Olivier</forenames></author><author><keyname>Veillard</keyname><forenames>Antoine</forenames></author><author><keyname>Lin</keyname><forenames>Jie</forenames></author><author><keyname>Petta</keyname><forenames>Julie</forenames></author><author><keyname>Chandrasekhar</keyname><forenames>Vijay</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Group Invariant Deep Representations for Image Instance Retrieval</title><categories>cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image instance retrieval pipelines are based on comparison of vectors
known as global image descriptors between a query image and the database
images. Due to their success in large scale image classification,
representations extracted from Convolutional Neural Networks (CNN) are quickly
gaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors
for image instance retrieval. While CNN-based descriptors are generally
remarked for good retrieval performance at lower bitrates, they nevertheless
present a number of drawbacks including the lack of robustness to common object
transformations such as rotations compared with their interest point based FV
counterparts.
  In this paper, we propose a method for computing invariant global descriptors
from CNNs. Our method implements a recently proposed mathematical theory for
invariance in a sensory cortex modeled as a feedforward neural network. The
resulting global descriptors can be made invariant to multiple arbitrary
transformation groups while retaining good discriminativeness.
  Based on a thorough empirical evaluation using several publicly available
datasets, we show that our method is able to significantly and consistently
improve retrieval results every time a new type of invariance is incorporated.
We also show that our method which has few parameters is not prone to
overfitting: improvements generalize well across datasets with different
properties with regard to invariances. Finally, we show that our descriptors
are able to compare favourably to other state-of-the-art compact descriptors in
similar bitranges, exceeding the highest retrieval results reported in the
literature on some datasets. A dedicated dimensionality reduction step
--quantization or hashing-- may be able to further improve the competitiveness
of the descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02098</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02098</id><created>2016-01-09</created><authors><author><keyname>Wang</keyname><forenames>Qingjun</forenames></author><author><keyname>Lv</keyname><forenames>Haiyan</forenames></author><author><keyname>Yue</keyname><forenames>Jun</forenames></author><author><keyname>Mitchell</keyname><forenames>Eugene</forenames></author></authors><title>Supervised multiview learning based on simultaneous learning of
  multiview intact and single view classifier</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiview learning problem refers to the problem of learning a classifier
from multiple view data. In this data set, each data points is presented by
multiple different views. In this paper, we propose a novel method for this
problem. This method is based on two assumptions. The first assumption is that
each data point has an intact feature vector, and each view is obtained by a
linear transformation from the intact vector. The second assumption is that the
intact vectors are discriminative, and in the intact space, we have a linear
classifier to separate the positive class from the negative class. We define an
intact vector for each data point, and a view-conditional transformation matrix
for each view, and propose to reconstruct the multiple view feature vectors by
the product of the corresponding intact vectors and transformation matrices.
Moreover, we also propose a linear classifier in the intact space, and learn it
jointly with the intact vectors. The learning problem is modeled by a
minimization problem, and the objective function is composed of a Cauchy error
estimator-based view-conditional reconstruction term over all data points and
views, and a classification error term measured by hinge loss over all the
intact vectors of all the data points. Some regularization terms are also
imposed to different variables in the objective function. The minimization
problem is solve by an iterative algorithm using alternate optimization
strategy and gradient descent algorithm. The proposed algorithm shows it
advantage in the compression to other multiview learning algorithms on
benchmark data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02109</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02109</id><created>2016-01-09</created><authors><author><keyname>Zhang</keyname><forenames>Yiwei</forenames></author><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Invertible binary matrix with maximum number of $2$-by-$2$ invertible
  submatrices</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem is related to all-or-nothing transforms (AONT) suggested by
Rivest as a preprocessing for encrypting data with a block cipher. Since then
there have been various applications of AONTs in cryptography and security.
D'Arco, Esfahani and Stinson posed the problem on the constructions of binary
matrices for which the desired properties of an AONT hold with the maximum
probability. That is, for given integers $t\le s$, what is the maximum number
of $t$-by-$t$ invertible submatrices in a binary matrix of order $s$? For the
case $t=2$, let $R_2(s)$ denote the maximal proportion of 2-by-2 invertible
submatrices. D'Arco, Esfahani and Stinson conjectured that the limit is between
0.492 and 0.625. We completely solve the case $t=2$ by showing that
$\lim_{s\rightarrow\infty}R_2(s)=0.5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02115</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02115</id><created>2016-01-09</created><authors><author><keyname>Lorenzo</keyname><forenames>B.</forenames></author><author><keyname>Kovacevic</keyname><forenames>I.</forenames></author><author><keyname>Gonzalez-Castano</keyname><forenames>F. J.</forenames></author><author><keyname>Burguillo</keyname><forenames>J. C.</forenames></author></authors><title>Exploiting Context-Awareness for Secure Spectrum Trading in Multi-hop
  Cognitive Cellular Networks</title><categories>cs.NI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider context-awareness to enhance route reliability and
robustness in multi-hop cognitive networks. A novel context-aware route
discovery protocol is presented to enable secondary users to select the route
according to their QoS requirements. The protocol facilitates adjacent relay
selection under different criteria, such as shortest available path, route
reliability and relay reputation. New routing and security-based metrics are
defined to measure route robustness in spatial, frequency and temporal domains.
Secure throughput, defined as the percentage of traffic not being intercepted
in the network, is provided. The resources needed for trading are then obtained
by jointly optimizing secure throughput and trading price. Simulation results
show that when there is a traffic imbalance of factor 4 between the primary and
secondary networks, 4 channels are needed to achieve 90% link reliability and
99% secure throughput in the secondary network. Besides, when relay reputation
varies from 0.5 to 0.9, a 20% variation in the required resources is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02117</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02117</id><created>2016-01-09</created><authors><author><keyname>Magurawalage</keyname><forenames>Chathura Sarathchandra</forenames></author><author><keyname>Yang</keyname><forenames>Kun</forenames></author></authors><title>LAPPS: Location Aware Password Protection System</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location Aware Password Protection System (LAPPS) is designed to strengthen
the security of traditional password protection systems. This is achieved by
adding several layers of protection to the passwords that most traditional
password protection systems generate. The current implementation looks at the
Password/Pin numbers of Credit/Debit cards that are used on Automated Teller
Machine (ATM),though the underlying design of the system can be used in many
other scenarios. A password that is generated will be allocated to a particular
user and to the ATM that is nearest to the user. LAPPS ensures the following
qualities of the passwords that it generates. Location Awareness: The passwords
are generated according to the users' geographical area, that they request
their passwords from. So a password will only be active in just one location.
Time Awareness: A password will only be valid for five minutes. The unused
passwords will be discarded. Dynamic: The user has to have a new password each
time he/she logs in. A password is generated to be used only once. User
Oriented/Specific: The received password can only be used by the requester, and
can only be used on its allocated ATM. Two Factor Authenticity: The
confidential information will be secured using two-factor authentication. For
extra security, a Pin generating device has been introduced. This will produce
an eight digit number that the user has to supply to the mobile application,
before requesting for a password. The user can obtain a pin number by inserting
his/her Debit/Credit card and the fixed password that has been allocated when
the user registers with the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02124</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02124</id><created>2016-01-09</created><authors><author><keyname>Wang</keyname><forenames>Boyue</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author></authors><title>Kernelized LRR on Grassmann Manifolds for Subspace Clustering</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank representation (LRR) has recently attracted great interest due to
its pleasing efficacy in exploring low-dimensional sub- space structures
embedded in data. One of its successful applications is subspace clustering, by
which data are clustered according to the subspaces they belong to. In this
paper, at a higher level, we intend to cluster subspaces into classes of
subspaces. This is naturally described as a clustering problem on Grassmann
manifold. The novelty of this paper is to generalize LRR on Euclidean space
onto an LRR model on Grassmann manifold in a uniform kernelized LRR framework.
The new method has many applications in data analysis in computer vision tasks.
The proposed models have been evaluated on a number of practical data analysis
applications. The experimental results show that the proposed models outperform
a number of state-of-the-art subspace clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02129</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02129</id><created>2016-01-09</created><authors><author><keyname>Shou</keyname><forenames>Zheng</forenames></author><author><keyname>Wang</keyname><forenames>Dongang</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Action Temporal Localization in Untrimmed Videos via Multi-stage CNNs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address action temporal localization in untrimmed long videos. This is
important because videos in real applications are usually unconstrained and
contain multiple action instances plus video content of background scenes or
other activities. To address this challenging issue, we exploit the
effectiveness of deep networks in action temporal localization via multi-stage
segment-based 3D ConvNets: (1) a proposal stage identifies candidate segments
in a long video that may contain actions; (2) a classification stage learns
one-vs-all action classification model to serve as initialization for the
localization stage; and (3) a localization stage fine-tunes on the model learnt
in the classification stage to localize each action instance. We propose a
novel loss function for the localization stage to explicitly consider temporal
overlap and therefore achieve high temporal localization accuracy. On two
large-scale benchmarks, our approach achieves significantly superior
performances compared with other state-of-the-art systems: mAP increases from
1.7% to 7.4% on MEXaction2 and increased from 15.0% to 19.0% on THUMOS 2014,
when the overlap threshold for evaluation is set to 0.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02130</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02130</id><created>2016-01-09</created><authors><author><keyname>Kathiravelu</keyname><forenames>Pradeeban</forenames></author><author><keyname>Veiga</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>SENDIM for Incremental Development of Cloud Networks</title><categories>cs.NI cs.DC</categories><report-no>INESC-ID Tec. Rep. 23/2015, October 2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the limited and varying availability of cheap infrastructure and
resources, cloud network systems and applications are tested in simulation and
emulation environments prior to physical deployments, at different stages of
development. Configuration management tools manage deployments and migrations
across different cloud platforms, mitigating tedious system administration
efforts. However, currently a cloud networking simulation cannot be migrated as
an emulation, or vice versa, without rewriting and manually re-deploying the
simulated application. This paper presents SENDIM (Sendim is a northeastern
Portuguese town close to the Spanish border, where the rare Mirandese language
is spoken), a Simulation, Emulation, aNd Deployment Integration Middleware for
cloud networks. As an orchestration platform for incrementally building
Software-Defined Cloud Networks (SDCN), SENDIM manages the development and
deployment of algorithms and architectures the entire length from
visualization, simulation, emulation, to physical deployments. Hence, SENDIM
optimizes the evaluation of cloud networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02131</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02131</id><created>2016-01-09</created><authors><author><keyname>Kathiravelu</keyname><forenames>Pradeeban</forenames></author><author><keyname>Grbac</keyname><forenames>Tihana Galinac</forenames></author><author><keyname>Veiga</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>A FIRM Approach to Software-Defined Service Composition</title><categories>cs.DC cs.NI cs.SE</categories><comments>INESC-ID Tec. Rep. 22/2015, October 2015</comments><report-no>INESC-ID Tec. Rep. 22/2015, October 2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service composition is an aggregate of services often leveraged to automate
the enterprise business processes. While Service Oriented Architecture (SOA)
has been a forefront of service composition, services can be realized as
efficient distributed and parallel constructs such as MapReduce, which are not
typically exploited in service composition. With the advent of
Software\-Defined Networking (SDN), global view and control of the entire
network is made available to the networking controller, which can further be
leveraged in application level. This paper presents FIRM, an approach for
Software-Defined Service Composition by leveraging SDN and MapReduce. FIRM
comprises Find, Invoke, Return, and Manage, as the core procedures in achieving
a QoS-Aware Service Composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02132</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02132</id><created>2016-01-09</created><authors><author><keyname>Jones</keyname><forenames>Cliff B.</forenames></author><author><keyname>Hayes</keyname><forenames>Ian J.</forenames></author></authors><title>Possible values: exploring a concept for concurrency</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  An important issue in concurrency is interference. This issue manifests
itself in both shared-variable and communication-based concurrency --- this
paper focusses on the former case where interference is caused by the
environment of a process changing the values of shared variables.
Rely/guarantee approaches have been shown to be useful in specifying and
reasoning compositionally about concurrent programs. This paper explores the
use of a &quot;possible values&quot; notation for reasoning about variables whose values
can be changed multiple times by interference. Apart from the value of this
concept in providing clear specifications, it offers a principled way of
avoiding the need for some auxiliary (or ghost) variables whose unwise use can
destroy compositionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02137</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02137</id><created>2016-01-09</created><authors><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>A&#xef;ssa</keyname><forenames>Sonia</forenames></author></authors><title>Modeling and Analysis of Cooperative Relaying in Spectrum-Sharing
  Cellular Systems</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Vehicular Technology, accepted for publication,
  11 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, spectrum-sharing technology is integrated into cellular
systems to improve spectrum efficiency. Macrocell users are primary users (PUs)
while those within local cells, e.g., femtocell users, or desiring
cost-effective services, e.g., roamers, are identified as secondary users
(SUs). The SUs share the spectrum resources of the PUs in a underlay way, thus
the transmit power of a secondary is strictly limited by the primary's
tolerable interference power. Given such constraints, a cooperative relaying
transmission between a SU and the macrocell base station (BS) is necessary. In
order to guarantee the success of dual-hop relaying and avoid multi-hop
relaying, a new cooperative paradigm is proposed, where an idle PU (instead of
a secondary as assumed in general) in the vicinity of a target SU is chosen to
serve as a relaying node, thanks to the fact that any PU can always transmit to
the macrocell BS directly. Moreover, two-way relaying strategy is applied at
the chosen relaying node so as to further improve the spectral efficiency. Our
results demonstrate that the proposed system is particularly suitable for
delay-tolerant wireless services with asymmetric downlink/uplink traffics, such
as e-mail checking, web browsing, social networking and data streaming, which
are the most popular applications for SUs in spectrum-sharing cellular
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02150</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02150</id><created>2016-01-09</created><authors><author><keyname>Elmesalawy</keyname><forenames>Mahmoud M.</forenames></author><author><keyname>Ali</keyname><forenames>A. S.</forenames></author></authors><title>A Grouped System Architecture for Smart Grids Based AMI Communications
  Over LTE</title><categories>cs.NI</categories><comments>16 pages, 11 figures and 3 tables</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  7, No. 6, Pp55-70, December 2015</journal-ref><doi>10.5121/ijwmn.2015.7606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A smart grid based Advanced Metering Infrastructure (AMI), is a technology
that enables the utilities to monitor and control the electricity consumption
through a set of various smart meters (SMs) connected via a two way
communication infrastructure. One of the key challenges for smart grids is how
to connect a large number of devices. On the other hand, 4G Long Term Evolution
(LTE), the latest standard for mobile communications, was developed to provide
stable service performance and higher data rates for a large number of mobile
users. Therefore, LTE is considered a promising solution for wide area
connectivity for SMs. In this paper, a grouped hierarchal architecture for SMs
communications over LTE is introduced. Then, an efficient grouped scheduling
technique is proposed for SMs transmissions over LTE. The proposed architecture
efficiently solves the overload problem due to AMI traffic and guarantees a
full monitoring and control for energy consumption. The results of our
suggested solution showed that LTE can serve better for smart grids based AMI
with particular grouping and scheduling scheme. In addition, the presented
technique can able to be used in urban areas having high density of SMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02155</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02155</id><created>2016-01-09</created><authors><author><keyname>Nazempour</keyname><forenames>Rezvan</forenames></author><author><keyname>Monfared</keyname><forenames>Mohammad Ali Saniee</forenames></author><author><keyname>Zio</keyname><forenames>Enrico</forenames></author></authors><title>A complex network theory approach for optimizing contamination warning
  sensor location in water distribution networks</title><categories>cs.SI</categories><comments>23 pages, 8 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drinking water for human health and well-being is crucial. Accidental and
intentional water contamination can pose great danger to consumers. Optimal
design of a system that can quickly detect the presence of contamination in a
water distribution network is very challenging for technical and operational
reasons. However, on the one hand improvement in chemical and biological sensor
technology has created the possibility of designing efficient contamination
detection systems. On the other hand, methods and tools from complex network
theory, which was primarily the domain of mathematicians and physicists,
provide analytical output for engineers to design, optimize, operate, and
maintain complex network systems such as power grids, water distribution
networks, telecommunication systems, internet, roads, supply chains, traffic
and transportation systems. In this work, we develop a new modeling approach
for the optimal placement of sensors for contamination detection in a water
distribution network. The approach originally combines classical optimization
and complex systems theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02166</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02166</id><created>2016-01-09</created><authors><author><keyname>S&#xf8;gaard</keyname><forenames>Anders</forenames></author></authors><title>Empirical Gaussian priors for cross-lingual transfer learning</title><categories>cs.CL</categories><comments>Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence model learning algorithms typically maximize log-likelihood minus
the norm of the model (or minimize Hamming loss + norm). In cross-lingual
part-of-speech (POS) tagging, our target language training data consists of
sequences of sentences with word-by-word labels projected from translations in
$k$ languages for which we have labeled data, via word alignments. Our training
data is therefore very noisy, and if Rademacher complexity is high, learning
algorithms are prone to overfit. Norm-based regularization assumes a constant
width and zero mean prior. We instead propose to use the $k$ source language
models to estimate the parameters of a Gaussian prior for learning new POS
taggers. This leads to significantly better performance in multi-source
transfer set-ups. We also present a drop-out version that injects (empirical)
Gaussian noise during online learning. Finally, we note that using empirical
Gaussian priors leads to much lower Rademacher complexity, and is superior to
optimally weighted model interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02190</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02190</id><created>2016-01-10</created><authors><author><keyname>Sanjari</keyname><forenames>Sina</forenames></author><author><keyname>Ozgoli</keyname><forenames>Sadjaad</forenames></author></authors><title>Sliding Mode Control Design: a Sum of Squares Approach</title><categories>cs.SY</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to systematically design sliding mode control
and manifold to stabilize nonlinear uncertain systems. The objective is also
accomplished to enlarge the inner bound of region of attraction for closed-loop
dynamics. The method is proposed to design a control that guarantees both
asymptotic and finite time stability given helped by (bilinear) sum of squares
programming. The approach introduces an iterative algorithm to search over
sliding mode manifold and Lyapunov function simultaneity. In the case of local
stability it concludes also the subset of estimated region of attraction for
reduced order sliding mode dynamics. The sliding mode manifold and the
corresponding Lyapunov function are obtained if the iterative SOS optimization
program has a solution. Results are demonstrated employing the method for
several examples to show potential of the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02191</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02191</id><created>2016-01-10</created><authors><author><keyname>Ji</keyname><forenames>Yuting</forenames></author><author><keyname>Zheng</keyname><forenames>Tongxin</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author></authors><title>Stochastic Interchange Scheduling in the Real-Time Electricity Market</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of multi-area interchange scheduling in the presence of
stochastic generation and load is considered. A new interchange scheduling
technique based on a two-stage stochastic minimization of overall expected
operating cost is proposed. Because directly solving the stochastic
optimization is intractable, an equivalent problem that maximizes the expected
social welfare is formulated. The proposed technique leverages the operator's
capability of forecasting locational marginal prices (LMPs) and obtains the
optimal interchange schedule without iterations among operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02197</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02197</id><created>2016-01-10</created><authors><author><keyname>Zheng</keyname><forenames>Wei-Long</forenames></author><author><keyname>Zhu</keyname><forenames>Jia-Yi</forenames></author><author><keyname>Lu</keyname><forenames>Bao-Liang</forenames></author></authors><title>Identifying Stable Patterns over Time for Emotion Recognition from EEG</title><categories>cs.HC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate stable patterns of electroencephalogram (EEG)
over time for emotion recognition using a machine learning approach. Up to now,
various findings of activated patterns associated with different emotions have
been reported. However, their stability over time has not been fully
investigated yet. In this paper, we focus on identifying EEG stability in
emotion recognition. To validate the efficiency of the machine learning
algorithms used in this study, we systematically evaluate the performance of
various popular feature extraction, feature selection, feature smoothing and
pattern classification methods with the DEAP dataset and a newly developed
dataset for this study. The experimental results indicate that stable patterns
exhibit consistency across sessions; the lateral temporal areas activate more
for positive emotion than negative one in beta and gamma bands; the neural
patterns of neutral emotion have higher alpha responses at parietal and
occipital sites; and for negative emotion, the neural patterns have significant
higher delta responses at parietal and occipital sites and higher gamma
responses at prefrontal sites. The performance of our emotion recognition
system shows that the neural patterns are relatively stable within and between
sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02200</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02200</id><created>2016-01-10</created><authors><author><keyname>Kannampillil</keyname><forenames>Harikumar</forenames></author><author><keyname>Nambisan</keyname><forenames>Anand Krishnadas</forenames></author><author><keyname>Kizhakkekundil</keyname><forenames>Sandra</forenames></author><author><keyname>Sugathan</keyname><forenames>Shreeja</forenames></author><author><keyname>Nagaraj</keyname><forenames>Nithin</forenames></author></authors><title>Compressed Shattering</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The central idea of compressed sensing is to exploit the fact that most
signals of interest are sparse in some domain and use this to reduce the number
of measurements to encode. However, if the sparsity of the input signal is not
precisely known, but known to lie within a specified range, compressed sensing
as such cannot exploit this fact and would need to use the same number of
measurements even for a very sparse signal. In this paper, we propose a novel
method called Compressed Shattering to adapt compressed sensing to the
specified sparsity range, without changing the sensing matrix by creating
shattered signals which have fixed sparsity. This is accomplished by first
suitably permuting the input spectrum and then using a filter bank to create
fixed sparsity shattered signals. By ensuring that all the shattered signals
are utmost 1-sparse, we make use of a simple but efficient deterministic
sensing matrix to yield very low number of measurements. For a discrete-time
signal of length 1000, with a sparsity range of $5 - 25$, traditional
compressed sensing requires $175$ measurements, whereas Compressed Shattering
would only need $20 - 100$ measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02204</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02204</id><created>2016-01-10</created><authors><author><keyname>Lee</keyname><forenames>Hyeonbeom</forenames></author><author><keyname>Kim</keyname><forenames>Suseong</forenames></author><author><keyname>Kim</keyname><forenames>H. Jin</forenames></author></authors><title>Control of an Aerial Manipulator using On-line Parameter Estimator for
  an Unknown Payload</title><categories>cs.RO</categories><comments>2015 IEEE International Conference on Automation Science and
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an estimation and control algorithm for an aerial
manipulator using a hexacopter with a 2-DOF robotic arm. The unknown parameters
of a payload are estimated by an on-line estimator based on parametrization of
the aerial manipulator dynamics. With the estimated mass information and the
augmented passivity-based controller, the aerial manipulator can fly with the
unknown object. Simulation for an aerial manipulator is performed to compare
estimation performance between the proposed control algorithm and conventional
adaptive sliding mode controller. Experimental results show a successful flight
of a custom-made aerial manipulator while the unknown parameters related to an
additional payload were estimated satisfactorily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02213</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02213</id><created>2016-01-10</created><authors><author><keyname>Berthold</keyname><forenames>Michael R.</forenames></author><author><keyname>H&#xf6;ppner</keyname><forenames>Frank</forenames></author></authors><title>On Clustering Time Series Using Euclidean Distance and Pearson
  Correlation</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For time series comparisons, it has often been observed that z-score
normalized Euclidean distances far outperform the unnormalized variant. In this
paper we show that a z-score normalized, squared Euclidean Distance is, in
fact, equal to a distance based on Pearson Correlation. This has profound
impact on many distance-based classification or clustering methods. In addition
to this theoretically sound result we also show that the often used k-Means
algorithm formally needs a mod ification to keep the interpretation as Pearson
correlation strictly valid. Experimental results demonstrate that in many cases
the standard k-Means algorithm generally produces the same results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02216</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02216</id><created>2016-01-10</created><authors><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Wang</keyname><forenames>Lifeng</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Mallik</keyname><forenames>Ranjan K.</forenames></author></authors><title>Physical Layer Security in Three-Tier Wireless Sensor Networks: A
  Stochastic Geometry Approach</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a tractable framework for exploiting the potential
benefits of physical layer security in three-tier wireless sensor networks
using stochastic geometry. In such networks, the sensing data from the remote
sensors are collected by sinks with the help of access points, and the external
eavesdroppers intercept the data transmissions.We focus on the secure
transmission in two scenarios: i) the active sensors transmit their sensing
data to the access points, and ii) the active access points forward the data to
the sinks. We derive new compact expressions for the average secrecy rate in
these two scenarios. We also derive a new compact expression for the overall
average secrecy rate. Numerical results corroborate our analysis and show that
multiple antennas at the access points can enhance the security of three-tier
wireless sensor networks. Our results show that increasing the number of access
points decreases the average secrecy rate between the access point and its
associated sink. However, we find that increasing the number of access points
first increases the overall average secrecy rate, with a critical value beyond
which the overall average secrecy rate then decreases. When increasing the
number of active sensors, both the average secrecy rate between the sensor and
its associated access point and the overall average secrecy rate decrease. In
contrast, increasing the number of sinks improves both the average secrecy rate
between the access point and its associated sink, as well as the overall
average secrecy rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02220</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02220</id><created>2016-01-10</created><authors><author><keyname>Arnab</keyname><forenames>Anurag</forenames></author><author><keyname>Sapienza</keyname><forenames>Michael</forenames></author><author><keyname>Golodetz</keyname><forenames>Stuart</forenames></author><author><keyname>Valentin</keyname><forenames>Julien</forenames></author><author><keyname>Miksik</keyname><forenames>Ondrej</forenames></author><author><keyname>Izadi</keyname><forenames>Shahram</forenames></author><author><keyname>Torr</keyname><forenames>Philip</forenames></author></authors><title>Joint Object-Material Category Segmentation from Audio-Visual Cues</title><categories>cs.CV cs.SD</categories><comments>Published in British Machine Vision Conference (BMVC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is not always possible to recognise objects and infer material properties
for a scene from visual cues alone, since objects can look visually similar
whilst being made of very different materials. In this paper, we therefore
present an approach that augments the available dense visual cues with sparse
auditory cues in order to estimate dense object and material labels. Since
estimates of object class and material properties are mutually informative, we
optimise our multi-output labelling jointly using a random-field framework. We
evaluate our system on a new dataset with paired visual and auditory data that
we make publicly available. We demonstrate that this joint estimation of object
and material labels significantly outperforms the estimation of either category
in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02223</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02223</id><created>2016-01-10</created><authors><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Mousavifar</keyname><forenames>S. Ali</forenames></author><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Leung</keyname><forenames>Cyril</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author></authors><title>Wireless Energy Harvesting in a Cognitive Relay Network</title><categories>cs.IT math.IT</categories><comments>11 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless energy harvesting is regarded as a promising energy supply
alternative for energy-constrained wireless networks. In this paper, a new
wireless energy harvesting protocol is proposed for an underlay cognitive relay
network with multiple primary user (PU) transceivers. In this protocol, the
secondary nodes can harvest energy from the primary network (PN) while sharing
the licensed spectrum of the PN. In order to assess the impact of different
system parameters on the proposed network, we first derive an exact expression
for the outage probability for the secondary network (SN) subject to three
important power constraints: 1) the maximum transmit power at the secondary
source (SS) and at the secondary relay (SR), 2) the peak interference power
permitted at each PU receiver, and 3) the interference power from each PU
transmitter to the SR and to the secondary destination (SD). To obtain
practical design insights into the impact of different parameters on successful
data transmission of the SN, we derive throughput expressions for both the
delay-sensitive and the delay-tolerant transmission modes. We also derive
asymptotic closed-form expressions for the outage probability and the
delay-sensitive throughput and an asymptotic analytical expression for the
delay-tolerant throughput as the number of PU transceivers goes to infinity.
The results show that the outage probability improves when PU transmitters are
located near SS and sufficiently far from SR and SD. Our results also show that
when the number of PU transmitters is large, the detrimental effect of
interference from PU transmitters outweighs the benefits of energy harvested
from the PU transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02225</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02225</id><created>2016-01-10</created><authors><author><keyname>Mansouri</keyname><forenames>Hamid</forenames><affiliation>Machine Vision Lab., Computer Engineering Department, Ferdowsi University of Mashhad, Mashhad, Iran</affiliation></author><author><keyname>Pourreza</keyname><forenames>Hamid-Reza</forenames><affiliation>Machine Vision Lab., Computer Engineering Department, Ferdowsi University of Mashhad, Mashhad, Iran</affiliation></author></authors><title>Parallel Stroked Multi Line: a model-based method for compressing large
  fingerprint databases</title><categories>cs.CV cs.DS</categories><comments>26 pages, 10 figures, submitted to Computer Vision and Image
  Understanding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing usage of fingerprints as an important biometric data, the
need to compress the large fingerprint databases has become essential. The most
recommended compression algorithm, even by standards, is JPEG2K. But at high
compression rates, this algorithm is ineffective. In this paper, a model is
proposed which is based on parallel lines with same orientations, arbitrary
widths and same gray level values located on rectangle with constant gray level
value as background. We refer to this algorithm as Parallel Stroked Multi Line
(PSML). By using Adaptive Geometrical Wavelet and employing PSML, a compression
algorithm is developed. This compression algorithm can preserve fingerprint
structure and minutiae. The exact algorithm of computing the PSML model take
exponential time. However, we have proposed an alternative approximation
algorithm, which reduces the time complexity to $O(n^3)$. The proposed PSML
alg. has significant advantage over Wedgelets Transform in PSNR value and
visual quality in compressed images. The proposed method, despite the lower
PSNR values than JPEG2K algorithm in common range of compression rates, in all
compression rates have nearly equal or greater advantage over JPEG2K when used
by Automatic Fingerprint Identification Systems (AFIS). At high compression
rates, according to PSNR values, mean EER rate and visual quality, the encoded
images with JPEG2K can not be identified from each other after compression.
But, images encoded by the PSML alg. retained the sufficient information to
maintain fingerprint identification performances similar to the ones obtained
by raw images without compression. One the U.are.U 400 database, the mean EER
rate for uncompressed images is 4.54%, while at 267:1 compression ratio, this
value becomes 49.41% and 6.22% for JPEG2K and PSML, respectively. This result
shows a significant improvement over the standard JPEG2K algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02241</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02241</id><created>2016-01-10</created><authors><author><keyname>Meng</keyname><forenames>Xuesong</forenames></author><author><keyname>Vukovic</keyname><forenames>Ana</forenames></author><author><keyname>Benson</keyname><forenames>Trevor M.</forenames></author><author><keyname>Sewell</keyname><forenames>Phillip</forenames></author></authors><title>Extended Capability Models for Carbon Fiber Composite (CFC) Panels in
  the Unstructured Transmission Line Modelling (UTLM) Method</title><categories>cs.CE</categories><comments>8 pages; submitted to IEEE Transactions on Electromagnetic
  Compatibility</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An effective model of single and multilayered thin panels, including those
formed using carbon fiber composite (CFC) materials, is incorporated into the
Transmission Line Modeling (TLM) method. The thin panel model is a
one-dimensional (1D) one based on analytical expansions of cotangent and
cosecant functions that are used to describe the admittance matrix in the
frequency domain; these are then converted into the time domain by using
digital filter theory and an inverse Z transform. The model, which is extended
to allow for material anisotropy, is executed within 1D TLM codes. And, for the
first time, the two-dimensional (2D) thin surface model is embedded in
unstructured three-dimensional (3D) TLM codes. The approach is validated by
using it to study some canonical structures with analytic solutions, and
against results taken from the literature. It is then used to investigate
shielding effectiveness of carbon fiber composite materials in a practical
curved aerospace-related structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02245</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02245</id><created>2016-01-10</created><authors><author><keyname>Montejo</keyname><forenames>Alejandra Gait&#xe1;n</forenames></author><author><keyname>Michel-Manzo</keyname><forenames>Octavio A.</forenames></author><author><keyname>Terrero-Escalante</keyname><forenames>C&#xe9;sar A.</forenames></author></authors><title>On parallel solution of ordinary differential equations</title><categories>math.NA cs.DC</categories><comments>30 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the performance of a parallel iterated Runge-Kutta method is
compared versus those of the serial fouth order Runge-Kutta and Dormand-Prince
methods. It was found that, typically, the runtime for the parallel method is
comparable to that of the serial versions, thought it uses considerably more
computational resources. A new algorithm is proposed where full parallelization
is used to estimate the best stepsize for integration. It is shown that this
new method outperforms the others, notably, in the integration of very large
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02250</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02250</id><created>2016-01-10</created><authors><author><keyname>Asghari</keyname><forenames>Seyed Mohammad</forenames></author><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author></authors><title>Decentralized Control Problems with Substitutable Actions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a decentralized system with multiple controllers and define
substitutability of one controller by another in open-loop strategies. We
explore the implications of this property on the optimization of closed-loop
strategies. In particular, we focus on the decentralized LQG problem with
substitutable actions. Even though the problem we formulate does not belong to
the known classes of &quot;simpler&quot; decentralized problems such as partially nested
or quadratically invariant problems, our results show that, under the
substitutability assumption, linear strategies are optimal and we provide a
complete state space characterization of optimal strategies. We also identify a
family of information structures that all give the same optimal cost as the
centralized information structure under the substitutability assumption. Our
results suggest that open-loop substitutability can work as a counterpart of
the information structure requirements that enable simplification of
decentralized control problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02257</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02257</id><created>2016-01-10</created><authors><author><keyname>Finn</keyname><forenames>Robert</forenames></author><author><keyname>Kulis</keyname><forenames>Brian</forenames></author></authors><title>A Sufficient Statistics Construction of Bayesian Nonparametric
  Exponential Family Conjugate Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conjugate pairs of distributions over infinite dimensional spaces are
prominent in statistical learning theory, particularly due to the widespread
adoption of Bayesian nonparametric methodologies for a host of models and
applications. Much of the existing literature in the learning community focuses
on processes possessing some form of computationally tractable conjugacy as is
the case for the beta and gamma processes (and, via normalization, the
Dirichlet process). For these processes, proofs of conjugacy and requisite
derivation of explicit computational formulae for posterior density parameters
are idiosyncratic to the stochastic process in question. As such, Bayesian
Nonparametric models are currently available for a limited number of conjugate
pairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In each
of these above cases the likelihood process belongs to the class of discrete
exponential family distributions. The exclusion of continuous likelihood
distributions from the known cases of Bayesian Nonparametric Conjugate models
stands as a disparity in the researcher's toolbox.
  In this paper we first address the problem of obtaining a general
construction of prior distributions over infinite dimensional spaces possessing
distributional properties amenable to conjugacy. Second, we bridge the divide
between the discrete and continuous likelihoods by illustrating a canonical
construction for stochastic processes whose Levy measure densities are from
positive exponential families, and then demonstrate that these processes in
fact form the prior, likelihood, and posterior in a conjugate family. Our
canonical construction subsumes known computational formulae for posterior
density parameters in the cases where the likelihood is from a discrete
distribution belonging to an exponential family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02258</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02258</id><created>2016-01-10</created><authors><author><keyname>de Haan</keyname><forenames>Ronald</forenames></author><author><keyname>Szymanik</keyname><forenames>Jakub</forenames></author></authors><title>Characterizing Polynomial Ramsey Quantifiers</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ramsey quantifiers are a natural object of study not only for logic and
computer science, but also for the formal semantics of natural language.
Restricting attention to finite models leads to the natural question whether
all Ramsey quantifiers are either polynomial-time computable or NP-hard, and
whether we can give a natural characterization of the polynomial-time
computable quantifiers. In this paper, we first show that there exist
intermediate Ramsey quantifiers and then we prove a dichotomy result for a
large and natural class of Ramsey quantifiers, based on a reasonable and
widely-believed complexity assumption. We show that the polynomial-time
computable quantifiers in this class are exactly the constant-log-bounded
Ramsey quantifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02267</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02267</id><created>2016-01-10</created><authors><author><keyname>Abedin</keyname><forenames>Paniz</forenames></author><author><keyname>Akbari</keyname><forenames>Saieed</forenames></author><author><keyname>Daneshmand</keyname><forenames>Mahsa</forenames></author><author><keyname>Demange</keyname><forenames>Marc</forenames></author><author><keyname>Ekim</keyname><forenames>Tinaz</forenames></author></authors><title>Improper Twin Edge Coloring of Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph whose each component has order at least 3. Let $s : E(G)
\rightarrow \mathbb{Z}_k$ for some integer $k\geq 2$ be an improper edge
coloring of $G$ (where adjacent edges may be assigned the same color). If the
induced vertex coloring $c : V (G) \rightarrow \mathbb{Z}_k$ defined by $c(v) =
\sum_{e\in E_v} s(e) \mbox{ in } \mathbb{Z}_k,$ (where the indicated sum is
computed in $\mathbb{Z}_k$ and $E_v$ denotes the set of all edges incident to
$v$) results in a proper vertex coloring of $G$, then we refer to such a
coloring as an improper twin $k$-edge coloring. The minimum $k$ for which $G$
has an improper twin $k$-edge coloring is called the improper twin chromatic
index of $G$ and is denoted by $\chi'_{it}(G)$.
  In this paper, we show that if $G$ is a graph with vertex chromatic number
$\chi(G)$, then $\chi'_{it}(G)=\chi(G)$, unless $\chi(G)=2 \pmod 4$ and in this
case $\chi'_{it}(G)\in \{\chi(G), \chi(G)+1\}$. Moreover, we show that it is
NP-hard to decide whether $\chi'_{it}(G)=\chi(G)$ or $\chi'_{it}(G)=\chi(G)+1$
and give some examples of perfect graph classes for which the problem is
polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02280</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02280</id><created>2016-01-10</created><authors><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author></authors><title>On the geometric properties of the semi-Lagrangian discontinuous
  Galerkin scheme for the Vlasov-Poisson equation</title><categories>math.NA cs.NA physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semi-Lagrangian discontinuous Galerkin method, coupled with a splitting
approach in time, has recently been introduced for the Vlasov--Poisson
equation. Since these methods are conservative, local in space, and able to
limit numerical diffusion, they are considered a promising alternative to more
traditional semi-Lagrangian schemes. In this paper we study the conservation of
important invariants and the long time behavior of the semi-Lagrangian
discontinuous Galerkin method. To that end we conduct a theoretical analysis
and perform a number of numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02281</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02281</id><created>2016-01-10</created><authors><author><keyname>Wu</keyname><forenames>David J.</forenames></author><author><keyname>Zimmerman</keyname><forenames>Joe</forenames></author><author><keyname>Planul</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author><author><keyname>Mitchell</keyname><forenames>John C.</forenames></author></authors><title>Privacy-Preserving Shortest Path Computation</title><categories>cs.CR</categories><comments>Extended version of NDSS 2016 paper</comments><doi>10.14722/ndss.2016.23052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigation is one of the most popular cloud computing services. But in
virtually all cloud-based navigation systems, the client must reveal her
location and destination to the cloud service provider in order to learn the
fastest route. In this work, we present a cryptographic protocol for navigation
on city streets that provides privacy for both the client's location and the
service provider's routing data. Our key ingredient is a novel method for
compressing the next-hop routing matrices in networks such as city street maps.
Applying our compression method to the map of Los Angeles, for example, we
achieve over tenfold reduction in the representation size. In conjunction with
other cryptographic techniques, this compressed representation results in an
efficient protocol suitable for fully-private real-time navigation on city
streets. We demonstrate the practicality of our protocol by benchmarking it on
real street map data for major cities such as San Francisco and Washington,
D.C.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02284</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02284</id><created>2016-01-10</created><updated>2016-03-04</updated><authors><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author><author><keyname>Yates</keyname><forenames>Roy</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Update or Wait: How to Keep Your Data Fresh</title><categories>cs.IT math.IT</categories><comments>14 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study how to manage the freshness of status updates sent from
a source to a remote monitor via a network server. A proper metric of data
freshness at the monitor is the age-of-information, which is defined as how old
the latest update is since the moment this update was generated at the source.
A logical policy is the zero-wait policy, i.e., the source submits a fresh
update once the server is free, which achieves the maximum throughput and the
minimum average delay. Surprisingly, this zero-wait policy does not always
minimize the average age. This motivates us to study how to optimally control
the status updates to keep data fresh and to understand when the zero-wait
policy is optimal. We introduce a penalty function to characterize the level of
&quot;dissatisfaction&quot; on data staleness, and formulate the average age penalty
minimization problem as a constrained semi-Markov decision process (SMDP) with
an uncountable state space. Despite of the difficulty of this problem, we
develop efficient algorithms to find the optimal status update policy. We show
that, in many scenarios, the optimal policy is to wait for a certain amount of
time before submitting a new update. In particular, the zero-wait policy can be
far from optimality if (i) the penalty function grows quickly with respect to
the age, and (ii) the update service times are highly random and positive
correlated. To the best of our knowledge, this is the first optimal control
policy which is proven to minimize the age-of-information in status update
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02298</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02298</id><created>2016-01-10</created><authors><author><keyname>Azar</keyname><forenames>Pablo</forenames></author><author><keyname>Goldwasser</keyname><forenames>Shafi</forenames></author><author><keyname>Park</keyname><forenames>Sunoo</forenames></author></authors><title>How to Incentivize Data-Driven Collaboration Among Competing Parties</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of vast amounts of data is changing how we can make medical
discoveries, predict global market trends, save energy, and develop educational
strategies. In some settings such as Genome Wide Association Studies or deep
learning, sheer size of data seems critical. When data is held distributedly by
many parties, they must share it to reap its full benefits.
  One obstacle to this revolution is the lack of willingness of different
parties to share data, due to reasons such as loss of privacy or competitive
edge. Cryptographic works address privacy aspects, but shed no light on
individual parties' losses/gains when access to data carries tangible rewards.
Even if it is clear that better overall conclusions can be drawn from
collaboration, are individual collaborators better off by collaborating?
Addressing this question is the topic of this paper.
  * We formalize a model of n-party collaboration for computing functions over
private inputs in which participants receive their outputs in sequence, and the
order depends on their private inputs. Each output &quot;improves&quot; on preceding
outputs according to a score function.
  * We say a mechanism for collaboration achieves collaborative equilibrium if
it ensures higher reward for all participants when collaborating (rather than
working alone). We show that in general, computing a collaborative equilibrium
is NP-complete, yet we design efficient algorithms to compute it in a range of
natural model settings.
  Our collaboration mechanisms are in the standard model, and thus require a
central trusted party; however, we show this assumption is unnecessary under
standard cryptographic assumptions. We show how to implement the mechanisms in
a decentralized way with new extensions of secure multiparty computation that
impose order/timing constraints on output delivery to different players, as
well as privacy and correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02300</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02300</id><created>2016-01-10</created><authors><author><keyname>Kim</keyname><forenames>Young-Min</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Bonnevay</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author></authors><title>Temporal Multinomial Mixture for Instance-Oriented Evolutionary
  Clustering</title><categories>cs.IR cs.LG stat.ML</categories><doi>10.1007/978-3-319-16354-3_66</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary clustering aims at capturing the temporal evolution of clusters.
This issue is particularly important in the context of social media data that
are naturally temporally driven. In this paper, we propose a new probabilistic
model-based evolutionary clustering technique. The Temporal Multinomial Mixture
(TMM) is an extension of classical mixture model that optimizes feature
co-occurrences in the trade-off with temporal smoothness. Our model is
evaluated for two recent case studies on opinion aggregation over time. We
compare four different probabilistic clustering models and we show the
superiority of our proposal in the task of instance-oriented clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02306</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02306</id><created>2016-01-10</created><authors><author><keyname>Bojic</keyname><forenames>Iva</forenames></author><author><keyname>Nizetic-Kosovic</keyname><forenames>Ivana</forenames></author><author><keyname>Belyi</keyname><forenames>Alexander</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Sublinear scaling of country attractiveness observed from Flickr dataset</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of people who decide to share their photographs publicly increases
every day, consequently making available new almost real-time insights of human
behavior while traveling. Rather than having this statistic once a month or
yearly, urban planners and touristic workers now can make decisions almost
simultaneously with the emergence of new events. Moreover, these datasets can
be used not only to compare how popular different touristic places are, but
also predict how popular they should be taking into an account their
characteristics. In this paper we investigate how country attractiveness scales
with its population and size using number of foreign users taking photographs,
which is observed from Flickr dataset, as a proxy for attractiveness. The
results showed two things: to a certain extent country attractiveness scales
with population, but does not with its size; and unlike in case of Spanish
cities, country attractiveness scales sublinearly with population, and not
superlinearly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02309</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02309</id><created>2016-01-10</created><authors><author><keyname>Wang</keyname><forenames>Syu-Siang</forenames></author><author><keyname>Chern</keyname><forenames>Alan</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Hung</keyname><forenames>Jeih-weih</forenames></author><author><keyname>Lu</keyname><forenames>Xugang</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Hui</forenames></author><author><keyname>Su</keyname><forenames>Borching</forenames></author></authors><title>Wavelet speech enhancement based on nonnegative matrix factorization</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For most of the state-of-the-art speech enhancement techniques, a spectrogram
is usually preferred than the respective time-domain raw data since it reveals
more compact presentation together with conspicuous temporal information over a
long time span. However, the short-time Fourier transform (STFT) that creates
the spectrogram in general distorts the original signal and thereby limits the
capability of the associated speech enhancement techniques. In this study, we
propose a novel speech enhancement method that adopts the algorithms of
discrete wavelet packet transform (DWPT) and nonnegative matrix factorization
(NMF) in order to conquer the aforementioned limitation. In brief, the DWPT is
first applied to split a time-domain speech signal into a series of subband
signals without introducing any distortion. Then we exploit NMF to highlight
the speech component for each subband. Finally, the enhanced subband signals
are joined together via the inverse DWPT to reconstruct a noise-reduced signal
in time domain. We evaluate the proposed DWPT-NMF based speech enhancement
method on the MHINT task. Experimental results show that this new method
behaves very well in prompting speech quality and intelligibility and it
outperforms the convnenitional STFT-NMF based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02311</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02311</id><created>2016-01-10</created><updated>2016-03-08</updated><authors><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Prakash</keyname><forenames>Anupam</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author><author><keyname>Yuen</keyname><forenames>Henry</forenames></author></authors><title>On the sum-of-squares degree of symmetric quadratic functions</title><categories>cs.CC quant-ph</categories><comments>33 pages. Second version fixes some typos and adds references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how well functions over the boolean hypercube of the form
$f_k(x)=(|x|-k)(|x|-k-1)$ can be approximated by sums of squares of low-degree
polynomials, obtaining good bounds for the case of approximation in
$\ell_{\infty}$-norm as well as in $\ell_1$-norm. We describe three
complexity-theoretic applications: (1) a proof that the recent breakthrough
lower bound of Lee, Raghavendra, and Steurer on the positive semidefinite
extension complexity of the correlation and TSP polytopes cannot be improved
further by showing better sum-of-squares degree lower bounds on
$\ell_1$-approximation of $f_k$; (2) a proof that Grigoriev's lower bound on
the degree of Positivstellensatz refutations for the knapsack problem is
optimal, answering an open question from his work; (3) bounds on the query
complexity of quantum algorithms whose expected output approximates such
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02320</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02320</id><created>2016-01-10</created><updated>2016-01-12</updated><authors><author><keyname>Yang</keyname><forenames>Shudi</forenames></author><author><keyname>Yao</keyname><forenames>Zheng-An</forenames></author><author><keyname>Zhao</keyname><forenames>Chang-An</forenames></author></authors><title>A Class of Three-Weight Linear Codes and Their Complete Weight
  Enumerators</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><msc-class>94B15, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, linear codes constructed from defining sets have been investigated
extensively and they have many applications. In this paper, for an odd prime
$p$, we propose a class of $p$-ary linear codes by choosing a proper defining
set. Their weight enumerators and complete weight enumerators are presented
explicitly. The results show that they are linear codes with three weights and
suitable for the constructions of authentication codes and secret sharing
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02323</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02323</id><created>2016-01-11</created><authors><author><keyname>Khonji</keyname><forenames>Majid</forenames></author><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author></authors><title>Optimal Power Flow with Inelastic Demands for Demand Response in Radial
  Distribution Networks</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical optimal power flow problem optimizes the power flow in a power
network considering the associated flow and operating constraints. In this
paper, we investigate optimal power flow in the context of utility-maximizing
demand response management in distribution networks, in which customers'
demands are satisfied subject to the operating constraints of voltage and
transmission power capacity. The prior results concern only elastic demands
that can be partially satisfied, whereas power demands in practice can be
inelastic with binary control decisions, which gives rise to a mixed integer
programming problem. We shed light on the hardness and approximability by
polynomial-time algorithms for optimal power flow problem with inelastic
demands. We show that this problem is inapproximable for general power network
topology with upper and lower bounds of nodal voltage. Then, we propose an
efficient algorithm for a relaxed problem in radial networks with bounded
transmission power loss and upper bound of nodal voltage. We derive an
approximation ratio between the proposed algorithm and the exact optimal
solution. Simulations show that the proposed algorithm can produce
close-to-optimal solutions in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02327</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02327</id><created>2016-01-11</created><authors><author><keyname>Hu</keyname><forenames>Guang-Neng</forenames></author><author><keyname>Dai</keyname><forenames>Xin-Yu</forenames></author><author><keyname>Song</keyname><forenames>Yunya</forenames></author><author><keyname>Huang</keyname><forenames>Shu-Jian</forenames></author><author><keyname>Chen</keyname><forenames>Jia-Jun</forenames></author></authors><title>A Synthetic Approach for Recommendation: Combining Ratings, Social
  Relations, and Reviews</title><categories>cs.IR cs.AI</categories><comments>7 pages, 8 figures</comments><journal-ref>24th IJCAI,2015,1756-1762</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recommender systems (RSs) provide an effective way of alleviating the
information overload problem by selecting personalized choices. Online social
networks and user-generated content provide diverse sources for recommendation
beyond ratings, which present opportunities as well as challenges for
traditional RSs. Although social matrix factorization (Social MF) can integrate
ratings with social relations and topic matrix factorization can integrate
ratings with item reviews, both of them ignore some useful information. In this
paper, we investigate the effective data fusion by combining the two
approaches, in two steps. First, we extend Social MF to exploit the graph
structure of neighbors. Second, we propose a novel framework MR3 to jointly
model these three types of information effectively for rating prediction by
aligning latent factors and hidden topics. We achieve more accurate rating
prediction on two real-life datasets. Furthermore, we measure the contribution
of each data source to the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02328</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02328</id><created>2016-01-11</created><updated>2016-01-12</updated><authors><author><keyname>Pattanayak</keyname><forenames>Sukhamoy</forenames></author><author><keyname>Singh</keyname><forenames>Abhay Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Pratyush</forenames></author></authors><title>On Quantum Codes Obtained From Cyclic Codes Over
  $\mathbb{F}_2+u\mathbb{F}_2+u^2\mathbb{F}_2$</title><categories>cs.IT math.IT</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1407.1232 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R=\mathbb{F}_2+u\mathbb{F}_2+u^2\mathbb{F}_2$ be a non-chain finite
commutative ring, where $u^3=u$. In this paper, we mainly study the
construction of quantum codes from cyclic codes over $R$. We obtained
self-orthogonal codes over $\mathbb{F}_2$ as gray images of linear and cyclic
codes over $R$. The parameters of quantum codes which are obtained from cyclic
code over $R$ are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02339</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02339</id><created>2016-01-11</created><authors><author><keyname>He</keyname><forenames>Wangpeng</forenames></author><author><keyname>Ding</keyname><forenames>Yin</forenames></author><author><keyname>Zi</keyname><forenames>Yanyang</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Periodic Transients Separation Algorithm for Detecting Bearing Faults</title><categories>cs.SD</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the problem of noise reduction with simultaneous
components separation in vibration signals for faults diagnosis of bearing. The
observed vibration signal is modeled as a summation of two components
contaminated by noise, and each component composes of periodic transients. To
estimate the two components simultaneously, an approach by solving an
optimization problem is proposed in this paper. The problem adopts convex
sparsity-based regularization scheme for decomposition, and non-convex
regularization is used to further promote the sparsity but preserving the
global convexity. A synthetic example is presented to illustrate the
performance of the proposed approach for periodic feature extraction. The
performance and effectiveness of the proposed method are further demonstrated
by applying to compound faults and single fault diagnosis of a locomotive
bearing. The results show the proposed approach can effectively detect and
separate the features of outer and inner race defects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02351</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02351</id><created>2016-01-11</created><authors><author><keyname>Laurent</keyname><forenames>Thomas</forenames><affiliation>CSC</affiliation></author><author><keyname>Ventresque</keyname><forenames>Anthony</forenames><affiliation>CSC</affiliation></author><author><keyname>Papadakis</keyname><forenames>Mike</forenames><affiliation>CSC</affiliation></author><author><keyname>Henard</keyname><forenames>Christopher</forenames><affiliation>CSC</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>CSC</affiliation></author></authors><title>Assessing and Improving the Mutation Testing Practice of PIT</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutation testing is used extensively to support the experimentation of
software engineering studies. Its application to real-world projects is
possible thanks to modern tools that automate the whole mutation analysis
process. However, popular mutation testing tools use a restrictive set of
mutants which do not conform to the community standards as supported by the
mutation testing literature. This can be problematic since the effectiveness of
mutation depends on its mutants. We therefore examine how effective are the
mutants of a popular mutation testing tool, named PIT, compared to
comprehensive ones, as drawn from the literature and personal experience. We
show that comprehensive mutants are harder to kill and encode faults not
captured by the mutants of PIT for a range of 11% to 62% of the Java classes of
the considered projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02371</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02371</id><created>2016-01-11</created><authors><author><keyname>Schwan</keyname><forenames>Christian</forenames></author><author><keyname>Strehler</keyname><forenames>Martin</forenames></author></authors><title>Energy-efficient Routing of Hybrid Vehicles</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a constrained shortest path problem with two resources. These two
resources can be converted into each other in a particular manner. Our
practical application is the energy optimal routing of hybrid vehicles. Due to
the possibility of converting fuel into electric energy this setting adds new
characteristics and new combinatorial possibilities to the common constrained
shortest path problem (CSP). We formulate the resulting problem as a
generalization of CSP. We show that optimal paths in this model may contain
cycles and we state conditions to prevent them. The main contribution is a
polynomial-time approximation scheme and a simpler approximation algorithm for
computing energy-optimal paths in graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02372</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02372</id><created>2016-01-11</created><authors><author><keyname>Kos</keyname><forenames>Jernej</forenames></author><author><keyname>Milutinovi&#x107;</keyname><forenames>Mitar</forenames></author><author><keyname>&#x10c;ehovin</keyname><forenames>Luka</forenames></author></authors><title>nodewatcher: A Substrate for Growing Your own Community Network</title><categories>cs.SI</categories><journal-ref>Computer Networks, Volume 93, Part 2, 24 December 2015</journal-ref><doi>10.1016/j.comnet.2015.09.021</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Community networks differ from regular networks by their organic growth
patterns -- there is no central planning body that would decide how the network
is built. Instead, the network grows in a bottom-up fashion as more people
express interest in participating in the community and connect with their
neighbours. People who participate in community networks are usually volunteers
with limited free time. Due to these factors, making the management of
community networks simpler and easier for all participants is the key component
in boosting their growth. Specifics of individual networks often force
communities to develop their own sets of tools and best practices which are
hard to share and do not interoperate well with others. We propose a new
general community network management platform nodewatcher that is built around
the core principle of modularity and extensibility, making it suitable for
reuse by different community networks. Devices are configured using
platform-independent configuration which nodewatcher can transform into
deployable firmware images, eliminating any manual device configuration,
reducing errors, and enabling participation of novice maintainers. An embedded
monitoring system enables live overview and validation of the whole community
network. We show how the system successfully operates in an actual community
wireless network, wlan Slovenija.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02376</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02376</id><created>2016-01-11</created><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Du</keyname><forenames>Tianming</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Deep Learning over Multi-field Categorical Data: A Case Study on User
  Response Prediction</title><categories>cs.LG cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Predicting user responses, such as click-through rate and conversion rate,
are critical in many web applications including web search, personalised
recommendation, and online advertising. Different from continuous raw features
that we usually found in the image and audio domains, the input features in web
space are always of multi-field and are mostly discrete and categorical while
their dependencies are little known. Major user response prediction models have
to either limit themselves to linear models or require manually building up
high-order combination features. The former loses the ability of exploring
feature interactions, while the latter results in a heavy computation in the
large feature space. To tackle the issue, we propose two novel models using
deep neural networks (DNNs) to automatically learn effective patterns from
categorical feature interactions and make predictions of users' ad clicks. To
get our DNNs efficiently work, we propose to leverage three feature
transformation methods, i.e., factorisation machines (FMs), restricted
Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper
presents the structure of our models and their efficient training algorithms.
The large-scale experiments with real-world data demonstrate that our methods
work better than major state-of-the-art models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02377</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02377</id><created>2016-01-11</created><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Chen</keyname><forenames>Lingxi</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Implicit Look-alike Modelling in Display Ads: Transfer Collaborative
  Filtering to CTR Estimation</title><categories>cs.LG cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  User behaviour targeting is essential in online advertising. Compared with
sponsored search keyword targeting and contextual advertising page content
targeting, user behaviour targeting builds users' interest profiles via
tracking their online behaviour and then delivers the relevant ads according to
each user's interest, which leads to higher targeting accuracy and thus more
improved advertising performance. The current user profiling methods include
building keywords and topic tags or mapping users onto a hierarchical taxonomy.
However, to our knowledge, there is no previous work that explicitly
investigates the user online visits similarity and incorporates such similarity
into their ad response prediction. In this work, we propose a general framework
which learns the user profiles based on their online browsing behaviour, and
transfers the learned knowledge onto prediction of their ad response.
Technically, we propose a transfer learning model based on the probabilistic
latent factor graphic models, where the users' ad response profiles are
generated from their online browsing profiles. The large-scale experiments
based on real-world data demonstrate significant improvement of our solution
over some strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02379</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02379</id><created>2016-01-11</created><authors><author><keyname>Lotz</keyname><forenames>Alex</forenames></author><author><keyname>Hamann</keyname><forenames>Arne</forenames></author><author><keyname>L&#xfc;tkebohle</keyname><forenames>Ingo</forenames></author><author><keyname>Stampfer</keyname><forenames>Dennis</forenames></author><author><keyname>Lutz</keyname><forenames>Matthias</forenames></author><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author></authors><title>Modeling Non-Functional Application Domain Constraints for
  Component-Based Robotics Software Systems</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2015 (arXiv:1601.00877)</comments><report-no>DSLRob/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service robots are complex, heterogeneous, software intensive systems built
from components. Recent robotics research trends mainly address isolated
capabilities on functional level. Non-functional properties, such as
responsiveness or deterministic behavior, are addressed only in isolation (if
at all). We argue that handling such non-functional properties on system level
is a crucial next step. We claim that precise control over
application-specific, dynamic execution and interaction behavior of functional
components -- i.e. clear computation and communication semantics on model level
without hidden code-defined parts -- is a key ingredient thereto.
  In this paper, we propose modeling concepts for these semantics, and present
a meta-model which (i) enables component developers to implement component
functionalities without presuming application-specific, system-level
attributes, and (ii) enables system integrators to reason about causal
dependencies between components as well as system-level data-flow
characteristics. This allows to control data-propagation semantics and system
properties such as end-to-end latencies during system integration without
breaking component encapsulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02380</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02380</id><created>2016-01-11</created><authors><author><keyname>Raghavan</keyname><forenames>Vasanthan</forenames></author><author><keyname>Subramanian</keyname><forenames>Sundar</forenames></author><author><keyname>Cezanne</keyname><forenames>Juergen</forenames></author><author><keyname>Sampath</keyname><forenames>Ashwin</forenames></author></authors><title>Directional Beamforming for Millimeter-Wave MIMO Systems</title><categories>cs.IT math.IT</categories><comments>20 pages, 11 figures, Extended version of the paper published at IEEE
  Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on beamforming in a millimeter-wave (mmW)
multi-input multi-output (MIMO) setup that has gained increasing traction in
meeting the high data-rate requirements of next-generation wireless systems.
For a given MIMO channel matrix, the optimality of beamforming with the
dominant right-singular vector (RSV) at the transmit end and with the matched
filter to the RSV at the receive end has been well-understood. When the channel
matrix can be accurately captured by a physical (geometric) scattering model
across multiple clusters/paths as is the case in mmW MIMO systems, we provide a
physical interpretation for this optimal structure: beam steering across the
different paths with appropriate power allocation and phase compensation. While
such an explicit physical interpretation has not been provided hitherto,
practical implementation of such a structure in a mmW system is fraught with
considerable difficulties (complexity as well as cost) as it requires the use
of per-antenna gain and phase control. This paper characterizes the loss in
received SNR with an alternate low-complexity beamforming solution that needs
only per-antenna phase control and corresponds to steering the beam to the
dominant path at the transmit and receive ends. While the loss in received SNR
can be arbitrarily large (theoretically), this loss is minimal in a large
fraction of the channel realizations reinforcing the utility of directional
beamforming as a good candidate solution for mmW MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02388</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02388</id><created>2016-01-11</created><updated>2016-02-25</updated><authors><author><keyname>Chalermsook</keyname><forenames>Parinya</forenames></author><author><keyname>Vaz</keyname><forenames>Daniel</forenames></author></authors><title>New Integrality Gap Results for the Firefighters Problem on Trees</title><categories>cs.DM cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The firefighter problem is NP-hard and admits a $(1-1/e)$ approximation based
on rounding the canonical LP. In this paper, we first show a matching
integrality gap of $(1-1/e+\epsilon)$ on the canonical LP. This result relies
on a powerful combinatorial gadget that can be used to prove integrality gap
results for many problem settings. We also consider the canonical LP augmented
with simple additional constraints (as suggested by Hartke). We provide several
evidences that these constraints improve the integrality gap of the canonical
LP: (i) Extreme points of the new LP are integral for some known tractable
instances and (ii) A natural family of instances that are bad for the canonical
LP admits an improved approximation algorithm via the new LP. We conclude by
presenting a $5/6$ integrality gap instance for the new LP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02391</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02391</id><created>2016-01-11</created><authors><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author></authors><title>Almost universal codes for fading wiretap channels</title><categories>cs.IT math.IT math.NT</categories><comments>5 pages, to be submitted to IEEE International Symposium on
  Information Theory (ISIT) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fading wiretap channel model where the transmitter has only
statistical channel state information, and the legitimate receiver and
eavesdropper have perfect channel state information. We propose a sequence of
non-random lattice codes which achieve strong secrecy and semantic security
over ergodic fading channels. The construction is almost universal in the sense
that it achieves the same constant gap to secrecy capacity over Gaussian and
ergodic fading models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02403</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02403</id><created>2016-01-11</created><authors><author><keyname>Habernal</keyname><forenames>Ivan</forenames></author><author><keyname>Gurevych</keyname><forenames>Iryna</forenames></author></authors><title>Argumentation Mining in User-Generated Web Discourse</title><categories>cs.CL</categories><comments>Under review in Computational Linguistics. First submission: 7 March
  2015. Revised submission: 10 January 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of argumentation mining, an evolving research field in computational
linguistics, is to design methods capable of analyzing people's argumentation.
In this article, we go beyond the state of the art in several ways. (i) We deal
with actual Web data and take up the challenges given by the variety of
registers, multiple domains, and unrestricted noisy user-generated Web
discourse. (ii) We bridge the gap between normative argumentation theories and
argumentation phenomena encountered in actual data by adapting an argumentation
model tested in an extensive annotation study. (iii) We create a new gold
standard corpus (90k tokens in 340 documents) and experiment with several
machine learning methods to identify argument components. We offer the data,
source codes, and annotation guidelines to the community under free licenses.
Our findings show that argumentation mining in user-generated Web discourse is
a feasible but challenging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02415</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02415</id><created>2016-01-11</created><authors><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author></authors><title>Subexponential time algorithms for finding small tree and path
  decompositions</title><categories>cs.DS</categories><comments>Extended abstract appeared in proceedings of ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Size Tree Decomposition (MSTD) and Minimum Size Path
Decomposition (MSPD) problems ask for a given n-vertex graph G and integer k,
what is the minimum number of bags of a tree decomposition (respectively, path
decomposition) of G of width at most k. The problems are known to be
NP-complete for each fixed $k\geq 4$. We present algorithms that solve both
problems for fixed k in $2^{O(n/ \log n)}$ time and show that they cannot be
solved in $2^{o(n / \log n)}$ time, assuming the Exponential Time Hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02420</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02420</id><created>2016-01-11</created><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author></authors><title>Fundamental Bounds and Approaches to Sequence Reconstruction from
  Nanopore Sequencers</title><categories>cs.IT math.IT</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanopore sequencers are emerging as promising new platforms for
high-throughput sequencing. As with other technologies, sequencer errors pose a
major challenge for their effective use. In this paper, we present a novel
information theoretic analysis of the impact of insertion-deletion (indel)
errors in nanopore sequencers. In particular, we consider the following
problems: (i) for given indel error characteristics and rate, what is the
probability of accurate reconstruction as a function of sequence length; (ii)
what is the number of `typical' sequences within the distortion bound induced
by indel errors; (iii) using replicated extrusion (the process of passing a DNA
strand through the nanopore), what is the number of replicas needed to reduce
the distortion bound so that only one typical sequence exists within the
distortion bound.
  Our results provide a number of important insights: (i) the maximum length of
a sequence that can be accurately reconstructed in the presence of indel and
substitution errors is relatively small; (ii) the number of typical sequences
within the distortion bound is large; and (iii) replicated extrusion is an
effective technique for unique reconstruction. In particular, we show that the
number of replicas is a slow function (logarithmic) of sequence length --
implying that through replicated extrusion, we can sequence large reads using
nanopore sequencers. Our model considers indel and substitution errors
separately. In this sense, it can be viewed as providing (tight) bounds on
reconstruction lengths and repetitions for accurate reconstruction when the two
error modes are considered in a single model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02431</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02431</id><created>2016-01-11</created><authors><author><keyname>Peersman</keyname><forenames>Claudia</forenames></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author><author><keyname>Vandekerckhove</keyname><forenames>Reinhild</forenames></author><author><keyname>Vandekerckhove</keyname><forenames>Bram</forenames></author><author><keyname>Van Vaerenbergh</keyname><forenames>Leona</forenames></author></authors><title>The Effects of Age, Gender and Region on Non-standard Linguistic
  Variation in Online Social Networks</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a corpus-based analysis of the effects of age, gender and region
of origin on the production of both &quot;netspeak&quot; or &quot;chatspeak&quot; features and
regional speech features in Flemish Dutch posts that were collected from a
Belgian online social network platform. The present study shows that combining
quantitative and qualitative approaches is essential for understanding
non-standard linguistic variation in a CMC corpus. It also presents a
methodology that enables the systematic study of this variation by including
all non-standard words in the corpus. The analyses resulted in a convincing
illustration of the Adolescent Peak Principle. In addition, our approach
revealed an intriguing correlation between the use of regional speech features
and chatspeak features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02433</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02433</id><created>2016-01-11</created><authors><author><keyname>Halilaj</keyname><forenames>Lavdim</forenames></author><author><keyname>Grangel-Gonz&#xe1;lez</keyname><forenames>Irl&#xe1;n</forenames></author><author><keyname>Coskun</keyname><forenames>G&#xf6;khan</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Git4Voc: Git-based Versioning for Collaborative Vocabulary Development</title><categories>cs.AI cs.DB cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Collaborative vocabulary development in the context of data integration is
the process of finding consensus between the experts of the different systems
and domains. The complexity of this process is increased with the number of
involved people, the variety of the systems to be integrated and the dynamics
of their domain. In this paper we advocate that the realization of a powerful
version control system is the heart of the problem. Driven by this idea and the
success of Git in the context of software development, we investigate the
applicability of Git for collaborative vocabulary development. Even though
vocabulary development and software development have much more similarities
than differences there are still important differences. These need to be
considered within the development of a successful versioning and collaboration
system for vocabulary development. Therefore, this paper starts by presenting
the challenges we were faced with during the creation of vocabularies
collaboratively and discusses its distinction to software development. Based on
these insights we propose Git4Voc which comprises guidelines how Git can be
adopted to vocabulary development. Finally, we demonstrate how Git hooks can be
implemented to go beyond the plain functionality of Git by realizing
vocabulary-specific features like syntactic validation and semantic diffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02436</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02436</id><created>2016-01-11</created><authors><author><keyname>Van Chien</keyname><forenames>Trinh</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Joint Power Allocation and User Association Optimization for Massive
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>30 pages, 11 figures, submitted to IEEE Trans. Wireless Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the joint power allocation and user association
problem in multi-cell Massive MIMO (multiple-input multiple-output) downlink
(DL) systems. The target is to minimize the total transmit power consumption
when each user is served by an optimized subset of the base stations (BSs),
using non-coherent joint transmission. We first derive a lower bound on the
ergodic spectral efficiency (SE), which is applicable for any channel
distribution and precoding scheme. Closed-form expressions are obtained for
Rayleigh fading channels with either maximum ratio transmission (MRT) or zero
forcing (ZF) precoding. From these bounds, we further formulate the DL power
minimization problems with fixed SE constraints for the users. These problems
are proved to be solvable as linear programs, giving the optimal power
allocation and BS-user association with low complexity. Furthermore, we
formulate a max-min fairness problem which maximizes the worst SE among the
users, and we show that it can be solved as a quasi-linear program. Simulations
manifest that the proposed methods provide good SE for the users using less
transmit power than in small-scale systems and the optimal user association can
effectively balance the load between BSs when needed. Even though our framework
allows the joint transmission from multiple BSs, there is an overwhelming
probability that only one BS is associated with each user at the optimal
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02437</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02437</id><created>2016-01-11</created><authors><author><keyname>Shi</keyname><forenames>MinJia</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Good Self-dual Generalized Quasi-Cyclic Codes Exist</title><categories>cs.IT math.IT</categories><comments>submitted to Information Processing Letters</comments><msc-class>94A15, 94B20, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there are good long binary generalized quasi-cyclic self-dual
(either Type I or Type II) codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02452</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02452</id><created>2016-01-11</created><authors><author><keyname>Butting</keyname><forenames>Arvid</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schulze</keyname><forenames>Christoph</forenames></author><author><keyname>Thomas</keyname><forenames>Ulrike</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Modeling Reusable, Platform-Independent Robot Assembly Processes</title><categories>cs.SE</categories><comments>Presented at DSLRob 2015 (arXiv:1601.00877)</comments><report-no>DSLRob/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart factories that allow flexible production of highly individualized goods
require flexible robots, usable in efficient assembly lines. Compliant robots
can work safely in shared environments with domain experts, who have to program
such robots easily for arbitrary tasks. We propose a new domain-specific
language and toolchain for robot assembly tasks for compliant manipulators.
With the LightRocks toolchain, assembly tasks are modeled on different levels
of abstraction, allowing a separation of concerns between domain experts and
robotics experts: externally provided, platform-independent assembly plans are
instantiated by the domain experts using models of processes and tasks. Tasks
are comprised of skills, which combine platform-specific action models provided
by robotics experts. Thereby it supports a flexible production and re-use of
modeling artifacts for various assembly processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02453</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02453</id><created>2016-01-11</created><authors><author><keyname>Merca&#x15f;</keyname><forenames>Robert</forenames></author><author><keyname>Nowotka</keyname><forenames>Dirk</forenames></author></authors><title>A note on Thue games</title><categories>cs.DM cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we improve on a result from~\cite{GryKosZma15}. In particular,
we investigate the situation where a word is constructed jointly by two players
who alternately append letters to the end of an existing word. One of the
players (Ann) tries to avoid (non-trivial) repetitions, while the other one
(Ben) tries to enforce them. We show a construction that is closer to the lower
bound showed in~\cite{GryKozMic13} using entropy compression, and building on
the probabilistic arguments based on a version of the Lov\'asz Local Lemma
from~\cite{Peg11}. We provide an explicit strategy for Ann to avoid
(non-trivial) repetitions over a $7$-letter alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02460</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02460</id><created>2016-01-11</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Joint Optimization of Cloud and Edge Processing for Fog Radio Access
  Networks</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the joint design of cloud and edge processing for the
downlink of a fog radio access network (F-RAN). In an F-RAN, as in cloud-RAN
(C-RAN), a baseband processing unit (BBU) can perform joint baseband processing
on behalf of the remote radio heads (RRHs) that are connected to the BBU by
means of the fronthaul links. In addition to the minimal functionalities of
conventional RRHs in C-RAN, the RRHs in an F-RAN may be equipped with local
caches, in which frequently requested contents can be stored, as well as with
baseband processing capabilities. They are hence referred to as enhanced RRH
(eRRH). This work focuses on the design of the delivery phase for an arbitrary
pre-fetching strategy used to populate the caches of the eRRHs. Two
fronthauling modes are considered, namely a hard-transfer mode, whereby
non-cached files are communicated over the fronthaul links to a subset of
eRRHs, and a soft-transfer mode, whereby the fronthaul links are used to convey
quantized baseband signals as in a C-RAN. Unlike the hard-transfer mode in
which baseband processing is traditionally carried out only at the eRRHs, the
soft-transfer mode enables both centralized precoding at the BBU and local
precoding at the eRRHs based on the cached contents, by means of a novel
superposition coding approach. To attain the advantages of both approaches, a
hybrid design of soft- and hard-transfer modes is also proposed. The problem of
maximizing the delivery rate is tackled under fronthaul capacity and per-eRRH
power constraints. Numerical results are provided to compare the performance of
hard- and soft-transfer fronthauling modes, as well as of the hybrid scheme,
for different baseline pre-fetching strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02465</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02465</id><created>2016-01-11</created><authors><author><keyname>Bachmann</keyname><forenames>Ivana</forenames></author><author><keyname>Reyes</keyname><forenames>Patricio</forenames></author><author><keyname>Silva</keyname><forenames>Alonso</forenames></author><author><keyname>Bustos-Jim&#xe9;nez</keyname><forenames>Javier</forenames></author></authors><title>The Hayastan Shakarian cut: measuring the impact of network
  disconnections</title><categories>cs.SI</categories><comments>4 pages, 8 figures, short paper submitted to DRCN 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present the Hayastan Shakarian (HS), a robustness index
for complex networks. HS measures the impact of a network disconnection (edge)
while comparing the sizes of the remaining connected components. Strictly
speaking, the Hayastan Shakarian index is defined as edge removal that produces
the maximal inverse of the size of the largest connected component divided by
the sum of the sizes of the remaining ones.
  We tested our index in attack strategies where the nodes are disconnected in
decreasing order of a specified metric. We considered using the Hayastan
Shakarian cut (disconnecting the edge with max HS) and other well-known
strategies as the higher betweenness centrality disconnection. All strategies
were compared regarding the behavior of the robustness (R-index) during the
attacks. In an attempt to simulate the internet backbone, the attacks were
performed in complex networks with power-law degree distributions (scale-free
networks).
  Preliminary results show that attacks based on disconnecting using the
Hayastan Shakarian cut are more dangerous (decreasing the robustness) than the
same attacks based on other centrality measures. We believe that the Hayastan
Shakarian cut, as well as other measures based on the size of the largest
connected component, provides a good addition to other robustness metrics for
complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02472</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02472</id><created>2016-01-11</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Deconinck</keyname><forenames>Geert</forenames></author><author><keyname>Lauwereins</keyname><forenames>Rudy</forenames></author></authors><title>An Application-Level Dependable Technique for Farmer-Worker Parallel
  Programs</title><categories>cs.DC</categories><comments>In LNCS 1225 (1997), Proc. of the High-Performance Computing and
  Networking Conference ISBN: 978-3-540-62898-9 (Print) 978-3-540-69041-2
  (Online)</comments><doi>10.1007/BFb0031636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An application-level technique is described for farmer-worker parallel
applications which allows a worker to be added or removed from the computing
farm at any moment of the run time without affecting the overall outcome of the
computation. The technique is based on uncoupling the farmer from the workers
by means of a separate module which asynchronously feeds these latter with new
&quot;units of work&quot; on an on-demand basis, and on a special feeding strategy based
on bookkeeping the status of each work-unit. An augmentation of the LINDA model
is finally proposed to exploit the bookkeeping algorithm for tuple management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02478</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02478</id><created>2016-01-11</created><updated>2016-01-12</updated><authors><author><keyname>Sim&#xf5;es</keyname><forenames>Jefferson Elbert</forenames></author><author><keyname>Figueiredo</keyname><forenames>Daniel R.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author></authors><title>Approximating the degree sequences of two random graphs</title><categories>math.PR cs.DM</categories><comments>(changes in v2: typo corrections)</comments><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a seminal paper, McKay and Wormald proposed a framework for approximating
the distribution of the degree sequence of an Erd\H{o}s-R\'enyi random graph by
a sequence of independent random variables. We extend their framework to the
case of two independent random graphs, giving similar bounds on the resulting
error for corresponding event probabilities. In particular, we show that, for
events with probabilities asymptotically smaller than any power law on the
approximation model, the same bounds also hold on the original model. Finally,
as an example application, we apply the developed framework to bound the
probability of having an isomorphism between two independent random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02480</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02480</id><created>2016-01-11</created><authors><author><keyname>Yukalov</keyname><forenames>V. I.</forenames></author><author><keyname>Sornette</keyname><forenames>D.</forenames></author></authors><title>Quantum probability and quantum decision making</title><categories>quant-ph cs.IT math.IT physics.soc-ph</categories><comments>Latex file, 18 pages</comments><journal-ref>Philos. Trans. Roy. Soc. A 374 (2016) 20150100</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rigorous general definition of quantum probability is given, which is valid
for elementary events and for composite events, for operationally testable
measurements as well as for inconclusive measurements, and also for
non-commuting observables in addition to commutative observables. Our proposed
definition of quantum probability makes it possible to describe quantum
measurements and quantum decision making on the same common mathematical
footing. Conditions are formulated for the case when quantum decision theory
reduces to its classical counterpart and for the situation where the use of
quantum decision theory is necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02481</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02481</id><created>2016-01-11</created><authors><author><keyname>Byrka</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Lewandowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Moldenhauer</keyname><forenames>Carsten</forenames></author></authors><title>Approximation algorithms for node-weighted prize-collecting Steiner tree
  problems on planar graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the prize-collecting version of the Node-weighted Steiner Tree
problem (NWPCST) restricted to planar graphs. We give a new primal-dual
Lagrangian-multiplier-preserving (LMP) 3-approximation algorithm for planar
NWPCST. We then show a ($2.88 + \epsilon$)-approximation which establishes a
new best approximation guarantee for planar NWPCST. This is done by combining
our LMP algorithm with a threshold rounding technique and utilizing the
2.4-approximation of Berman and Yaroslavtsev for the version without penalties.
We also give a primal-dual 4-approximation algorithm for the more general
forest version using techniques introduced by Hajiaghay and Jain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02484</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02484</id><created>2016-01-11</created><authors><author><keyname>Abou-Saleh</keyname><forenames>Faris</forenames></author><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Gibbons</keyname><forenames>Jeremy</forenames></author><author><keyname>McKinna</keyname><forenames>James</forenames></author><author><keyname>Stevens</keyname><forenames>Perdita</forenames></author></authors><title>Reflections on Monadic Lenses</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional transformations (bx) have primarily been modeled as pure
functions, and do not account for the possibility of the side-effects that are
available in most programming languages. Recently several formulations of bx
that use monads to account for effects have been proposed, both among
practitioners and in academic research. The combination of bx with effects
turns out to be surprisingly subtle, leading to problems with some of these
proposals and increasing the complexity of others. This paper reviews the
proposals for monadic lenses to date, and offers some improved definitions,
paying particular attention to the obstacles to naively adding monadic effects
to existing definitions of pure bx such as lenses and symmetric lenses, and the
subtleties of equivalence of symmetric bidirectional transformations in the
presence of effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02487</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02487</id><created>2016-01-11</created><authors><author><keyname>Karali</keyname><forenames>Abubakrelsedik</forenames></author><author><keyname>Bassiouny</keyname><forenames>Ahmad</forenames></author><author><keyname>El-Saban</keyname><forenames>Motaz</forenames></author></authors><title>Facial Expression Recognition in the Wild using Rich Deep Features</title><categories>cs.CV</categories><comments>in International Conference in Image Processing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial Expression Recognition is an active area of research in computer
vision with a wide range of applications. Several approaches have been
developed to solve this problem for different benchmark datasets. However,
Facial Expression Recognition in the wild remains an area where much work is
still needed to serve real-world applications. To this end, in this paper we
present a novel approach towards facial expression recognition. We fuse rich
deep features with domain knowledge through encoding discriminant facial
patches. We conduct experiments on two of the most popular benchmark datasets;
CK and TFE. Moreover, we present a novel dataset that, unlike its precedents,
consists of natural - not acted - expression images. Experimental results show
that our approach achieves state-of-the-art results over standard benchmarks
and our own dataset
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02489</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02489</id><created>2016-01-03</created><authors><author><keyname>Patranabis</keyname><forenames>Anirban</forenames></author><author><keyname>Banerjee</keyname><forenames>Kaushik</forenames></author><author><keyname>Midya</keyname><forenames>Vishal</forenames></author><author><keyname>Sanyal</keyname><forenames>Shankha</forenames></author><author><keyname>Banerjee</keyname><forenames>Archi</forenames></author><author><keyname>Sengupta</keyname><forenames>Ranjan</forenames></author><author><keyname>Ghosh</keyname><forenames>Dipak</forenames></author></authors><title>Categorization of Tablas by Wavelet Analysis</title><categories>cs.SD</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tabla, a percussion instrument, mainly used to accompany vocalists,
instrumentalists and dancers in every style of music from classical to light in
India, mainly used for keeping rhythm. This percussion instrument consists of
two drums played by two hands, structurally different and produces different
harmonic sounds. Earlier work has done labeling tabla strokes from real time
performances by testing neural networks and tree based classification methods.
The current work extends previous work by C. V. Raman and S. Kumar in 1920 on
spectrum modeling of tabla strokes. In this paper we have studied spectral
characteristics (by wavelet analysis by sub band coding method and using
torrence wavelet tool) of nine strokes from each of five tablas using Wavelet
transform. Wavelet analysis is now a common tool for analyzing localized
variations of power within a time series and to find the frequency distribution
in time frequency space. Statistically, we will look into the patterns depicted
by harmonics of different sub bands and the tablas. Distribution of dominant
frequencies at different sub-band of stroke signals, distribution of power and
behavior of harmonics are the important features, leads to categorization of
tabla.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02499</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02499</id><created>2016-01-11</created><authors><author><keyname>Klan</keyname><forenames>Petr</forenames></author></authors><title>Modeling the Dynamics of Discussions in Social Networks</title><categories>cs.SI</categories><comments>11 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method of modeling the dynamics of electronic discussions is
proposed based on the so called FOPDT model (First Order Plus Dead Time) known
from the process control. Knowledge of the model points to possibility of
estimating dynamic movements in discussions as well as understanding and
designing their maintaining and guidance. Real discussions are processed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02502</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02502</id><created>2016-01-11</created><authors><author><keyname>Coulmance</keyname><forenames>Jocelyn</forenames></author><author><keyname>Marty</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Wenzek</keyname><forenames>Guillaume</forenames></author><author><keyname>Benhalloum</keyname><forenames>Amine</forenames></author></authors><title>Trans-gram, Fast Cross-lingual Word-embeddings</title><categories>cs.CL</categories><comments>EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Trans-gram, a simple and computationally-efficient method to
simultaneously learn and align wordembeddings for a variety of languages, using
only monolingual data and a smaller set of sentence-aligned data. We use our
new method to compute aligned wordembeddings for twenty-one languages using
English as a pivot language. We show that some linguistic features are aligned
across languages for which we do not have aligned data, even though those
properties do not exist in the pivot language. We also achieve state of the art
results on standard cross-lingual text classification and word translation
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02513</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02513</id><created>2016-01-11</created><authors><author><keyname>Kalofolias</keyname><forenames>Vassilis</forenames></author></authors><title>How to learn a graph from smooth signals</title><categories>stat.ML cs.LG physics.data-an</categories><comments>8 pages + supplementary material. Accepted in AISTATS 2016, Cadiz,
  Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework that learns the graph structure underlying a set of
smooth signals. Given $X\in\mathbb{R}^{m\times n}$ whose rows reside on the
vertices of an unknown graph, we learn the edge weights
$w\in\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that
$\text{tr}{X^\top LX}$ is small. We show that the problem is a weighted
$\ell$-1 minimization that leads to naturally sparse solutions. We point out
how known graph learning or construction techniques fall within our framework
and propose a new model that performs better than the state of the art in many
settings. We present efficient, scalable primal-dual based algorithms for both
our model and the previous state of the art, and evaluate their performance on
artificial and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02522</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02522</id><created>2016-01-11</created><updated>2016-01-12</updated><authors><author><keyname>Perraudin</keyname><forenames>Nathana&#xeb;l</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Stationary signal processing on graphs</title><categories>cs.DS stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are a central tool in machine learning and information processing as
they allow to conveniently capture the structure of complex datasets. In this
context, it is of high importance to develop flexible models of signals defined
over graphs or networks. In this paper, we generalize the traditional concept
of wide sense stationarity to signals defined over the vertices of arbitrary
weighted undirected graphs. We show that stationarity is intimately linked to
statistical invariance under a localization operator reminiscent of
translation. We prove that stationary graph signals are characterized by a
well-defined Power Spectral Density that can be efficiently estimated even for
large graphs. We leverage this new concept to derive Wiener-type estimation
procedures of noisy and partially observed signals and illustrate the
performance of this new model for denoising and regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02536</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02536</id><created>2016-01-11</created><authors><author><keyname>Kathiravelu</keyname><forenames>Pradeeban</forenames></author><author><keyname>Chen</keyname><forenames>Xiao</forenames></author><author><keyname>Mitthalal</keyname><forenames>Dipesh Dugar</forenames></author><author><keyname>Veiga</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>JikesRVM: Internal Mechanisms Study and Garbage Collection with MMTk</title><categories>cs.PL</categories><comments>Technical Report Submitted at Instituto Superior T\'ecnico,
  Universidade de Lisboa as part of the AVExe Virtual Execution Environments
  Module</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Level Language Virtual Machines is a core topic of interest for the
researchers who are into virtual execution environments. As an open source
virtual machine released to 16 universities, as early as 2001, Jikes RVM has
been a major drive for many researches. While working on this project, we
studied the JIT compilation of Jikes RVM as well as the Garbage Collection (GC)
which is handled by the Memory Management Toolkit (MMTk), a part of the Jikes
RVM. We also studied the Compressor Mark-Compact Collector algorithm and
implemented it for MMTk. We have also implemented a micro-benchmark for the GC
algorithms in Java, named &quot;XPDBench&quot;, for benchmarking the implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02539</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02539</id><created>2016-01-11</created><authors><author><keyname>Wu</keyname><forenames>Zhizheng</forenames></author><author><keyname>King</keyname><forenames>Simon</forenames></author></authors><title>Investigating gated recurrent neural networks for speech synthesis</title><categories>cs.CL cs.NE</categories><comments>Accepted by ICASSP 2016</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, recurrent neural networks (RNNs) as powerful sequence models have
re-emerged as a potential acoustic model for statistical parametric speech
synthesis (SPSS). The long short-term memory (LSTM) architecture is
particularly attractive because it addresses the vanishing gradient problem in
standard RNNs, making them easier to train. Although recent studies have
demonstrated that LSTMs can achieve significantly better performance on SPSS
than deep feed-forward neural networks, little is known about why. Here we
attempt to answer two questions: a) why do LSTMs work well as a sequence model
for SPSS; b) which component (e.g., input gate, output gate, forget gate) is
most important. We present a visual analysis alongside a series of experiments,
resulting in a proposal for a simplified architecture. The simplified
architecture has significantly fewer parameters than an LSTM, thus reducing
generation complexity considerably without degrading quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02543</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02543</id><created>2016-01-11</created><authors><author><keyname>Pandey</keyname><forenames>Vinod Kumar</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Evaluating the Performance of a Speech Recognition based System</title><categories>cs.CL cs.AI cs.HC</categories><comments>7 pages, 2 figure, ACC 2011</comments><journal-ref>ACC (3) 2011: 230-238</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech based solutions have taken center stage with growth in the services
industry where there is a need to cater to a very large number of people from
all strata of the society. While natural language speech interfaces are the
talk in the research community, yet in practice, menu based speech solutions
thrive. Typically in a menu based speech solution the user is required to
respond by speaking from a closed set of words when prompted by the system. A
sequence of human speech response to the IVR prompts results in the completion
of a transaction. A transaction is deemed successful if the speech solution can
correctly recognize all the spoken utterances of the user whenever prompted by
the system. The usual mechanism to evaluate the performance of a speech
solution is to do an extensive test of the system by putting it to actual
people use and then evaluating the performance by analyzing the logs for
successful transactions. This kind of evaluation could lead to dissatisfied
test users especially if the performance of the system were to result in a poor
transaction completion rate. To negate this the Wizard of Oz approach is
adopted during evaluation of a speech system. Overall this kind of evaluations
is an expensive proposition both in terms of time and cost. In this paper, we
propose a method to evaluate the performance of a speech solution without
actually putting it to people use. We first describe the methodology and then
show experimentally that this can be used to identify the performance
bottlenecks of the speech solution even before the system is actually used thus
saving evaluation time and expenses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02546</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02546</id><created>2016-01-11</created><authors><author><keyname>Rupprechter</keyname><forenames>Samuel</forenames></author></authors><title>Automatic Determination of Chord Roots</title><categories>cs.SD</categories><comments>BSc Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though chord roots constitute a fundamental concept in music theory,
existing models do not explain and determine them to full satisfaction. We
present a new method which takes sequential context into account to resolve
ambiguities and detect nonharmonic tones. We extract features from chord pairs
and use a decision tree to determine chord roots. This leads to a quantitative
improvement in correctness of the predicted roots in comparison to other
models. All this raises the question how much harmonic and nonharmonic tones
actually contribute to the perception of chord roots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02553</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02553</id><created>2016-01-11</created><authors><author><keyname>Kim</keyname><forenames>Suyoun</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author><author><keyname>Lane</keyname><forenames>Ian</forenames></author></authors><title>Environmental Noise Embeddings for Robust Speech Recognition</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deep neural network architecture for speech recognition
that explicitly employs knowledge of the background environmental noise within
a deep neural network acoustic model. A deep neural network is used to predict
the acoustic environment in which the system in being used. The discriminative
embedding generated at the bottleneck layer of this network is then
concatenated with traditional acoustic features as input to a deep neural
network acoustic model. Using simulated acoustic environments we show that the
proposed approach significantly improves speech recognition accuracy in noisy
and highly reverberant environments, outperforming multi-condition training and
multi-task learning for this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02578</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02578</id><created>2016-01-11</created><authors><author><keyname>Cardelli</keyname><forenames>Luca</forenames></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames></author><author><keyname>Laurenti</keyname><forenames>Luca</forenames></author></authors><title>Programming Discrete Distributions with Chemical Reaction Networks</title><categories>cs.DC cs.DM q-bio.MN q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biochemical interactions are inherently stochastic: the time for the
reactions to fire and which reaction fires next are both random variables. It
has been argued that natural systems use stochasticity to perform functions
that would be impossible in a deterministic setting. However, the mechanisms
used by cells to compute in a noisy environment are not well understood. We
explore the range of probabilistic behaviours that can be engineered with
Chemical Reaction Networks (CRNs). We show that at steady state CRNs are able
to &quot;program&quot; any distribution with support in $\mathbb{N}^m$, with $m \geq 1$.
We present an algorithm to systematically program a CRN so that its stochastic
semantics at steady state approximates a particular distribution with
arbitrarily small error. We also give optimized schemes for special
distributions, including the uniform distribution. Finally, we formulate a
calculus that is complete for finite support distributions, and that can be
compiled to a restricted class of CRNs that at steady state realize those
distributions. We illustrate the approach on an example of drug resistance in
bacteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02601</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02601</id><created>2016-01-08</created><authors><author><keyname>Shan</keyname><forenames>Songling</forenames></author><author><keyname>Yao</keyname><forenames>Bing</forenames></author></authors><title>A Note On Vertex Distinguishing Edge colorings of Trees</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper edge coloring of a simple graph $G$ is called a vertex
distinguishing edge coloring (vdec) if for any two distinct vertices $u$ and
$v$ of $G$, the set of the colors assigned to the edges incident to $u$ differs
from the set of the colors assigned to the edges incident to $v$. The minimum
number of colors required for all vdecs of $G$ is denoted by $\chi\,'_s(G)$
called the vdec chromatic number of $G$. Let $n_d(G)$ denote the number of
vertices of degree $d$ in $G$. In this note, we show that a tree $T$ with
$n_2(T)\leq n_1(T)$ holds $\chi\,'_s(T)=n_1(T)+1$ if its diameter $D(T)=3$ or
one of two particular trees with $D(T) =4$, and $\chi\,'_s(T)=n_1(T)$
otherwise; furthermore $\chi\,'_{es}(T)=\chi\,'_s(T)$ when $|E(T)|\leq
2(n_1(T)+1)$, where $\chi\,'_{es}(T)$ is the equitable vdec chromatic number of
$T$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02603</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02603</id><created>2016-01-10</created><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Lallich</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>How to Use Temporal-Driven Constrained Clustering to Detect Typical
  Evolutions</title><categories>cs.LG cs.DS</categories><journal-ref>Int. J. Artif. Intell. Tools 23, 1460013 (2014) [26 pages]</journal-ref><doi>10.1142/S0218213014600136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new time-aware dissimilarity measure that takes
into account the temporal dimension. Observations that are close in the
description space, but distant in time are considered as dissimilar. We also
propose a method to enforce the segmentation contiguity, by introducing, in the
objective function, a penalty term inspired from the Normal Distribution
Function. We combine the two propositions into a novel time-driven constrained
clustering algorithm, called TDCK-Means, which creates a partition of coherent
clusters, both in the multidimensional space and in the temporal space. This
algorithm uses soft semi-supervised constraints, to encourage adjacent
observations belonging to the same entity to be assigned to the same cluster.
We apply our algorithm to a Political Studies dataset in order to detect
typical evolution phases. We adapt the Shannon entropy in order to measure the
entity contiguity, and we show that our proposition consistently improves
temporal cohesion of clusters, without any significant loss in the
multidimensional variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02605</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02605</id><created>2016-01-11</created><authors><author><keyname>Pandey</keyname><forenames>Vinod K.</forenames></author><author><keyname>Pande</keyname><forenames>Arun</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>A Mobile Phone based Speech Therapist</title><categories>cs.CY cs.HC</categories><comments>6 pages, 6 figures, SimPe. [2011] Remote Speech Therapist Vinod
  Pandey, Arun Pande, Sunil Kopparapu SiMPE 2011, Stockholm, Sweden, Aug 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patients with articulatory disorders often have difficulty in speaking. These
patients need several speech therapy sessions to enable them speak normally.
These therapy sessions are conducted by a specialized speech therapist. The
goal of speech therapy is to develop good speech habits as well as to teach how
to articulate sounds the right way. Speech therapy is critical for continuous
improvement to regain normal speech. Speech therapy sessions require a patient
to travel to a hospital or a speech therapy center for extended periods of time
regularly; this makes the process of speech therapy not only time consuming but
also very expensive. Additionally, there is a severe shortage of trained speech
therapists around the globe in general and in developing countries in
particular. In this paper, we propose a low cost mobile speech therapist, a
system that enables speech therapy using a mobile phone which eliminates the
need of the patient to frequently travel to a speech therapist in a far away
hospital. The proposed system, which is being built, enables both synchronous
and asynchronous interaction between the speech therapist and the patient
anytime anywhere
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02644</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02644</id><created>2016-01-11</created><updated>2016-01-19</updated><authors><author><keyname>Mansouryar</keyname><forenames>Mohsen</forenames></author><author><keyname>Steil</keyname><forenames>Julian</forenames></author><author><keyname>Sugano</keyname><forenames>Yusuke</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author></authors><title>3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye
  Trackers</title><categories>cs.HC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D gaze information is important for scene-centric attention analysis but
accurate estimation and analysis of 3D gaze in real-world environments remains
challenging. We present a novel 3D gaze estimation method for monocular
head-mounted eye trackers. In contrast to previous work, our method does not
aim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gaze
directions in scene camera coordinate space. We first provide a detailed
discussion of the 3D gaze estimation task and summarize different methods,
including our own. We then evaluate the performance of different 3D gaze
estimation approaches using both simulated and real data. Through experimental
validation, we demonstrate the effectiveness of our method in reducing parallax
error, and we identify research challenges for the design of 3D calibration
procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02650</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02650</id><created>2016-01-11</created><authors><author><keyname>Tomaszuk</keyname><forenames>Dominik</forenames></author></authors><title>Inference rules for RDF(S) and OWL in N3Logic</title><categories>cs.DB cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents inference rules for Resource Description Framework (RDF),
RDF Schema (RDFS) and Web Ontology Language (OWL). Our formalization is based
on Notation 3 Logic, which extended RDF by logical symbols and created Semantic
Web logic for deductive RDF graph stores. We also propose OWL-P that is a
lightweight formalism of OWL and supports soft inferences by omitting complex
language constructs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02658</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02658</id><created>2016-01-11</created><updated>2016-03-04</updated><authors><author><keyname>Banks</keyname><forenames>Jess</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Information-theoretic thresholds for community detection in sparse
  networks</title><categories>math.PR cond-mat.stat-mech cs.CC cs.IT cs.SI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give upper and lower bounds on the information-theoretic threshold for
community detection in the stochastic block model. Specifically, let $k$ be the
number of groups, $d$ be the average degree, the probability of edges between
vertices within and between groups be $c_\mathrm{in}/n$ and $c_\mathrm{out}/n$
respectively, and let $\lambda = (c_\mathrm{in}-c_\mathrm{out})/(kd)$. We show
that, when $k$ is large, and $\lambda = O(1/k)$, the critical value of $d$ at
which community detection becomes possible -- in physical terms, the
condensation threshold -- is \[ d_c = \Theta\!\left( \frac{\log k}{k \lambda^2}
\right) \, , \] with tighter results in certain regimes. Above this threshold,
we show that the only partitions of the nodes into $k$ groups are correlated
with the ground truth, giving an exponential-time algorithm that performs
better than chance -- in particular, detection is possible for $k \ge 5$ in the
disassortative case $\lambda &lt; 0$ and for $k \ge 11$ in the assortative case
$\lambda &gt; 0$. (Similar upper bounds were obtained independently by Abbe and
Sandon.) Below this threshold, we use recent results of Neeman and Netrapalli
(who generalized arguments of Mossel, Neeman, and Sly) to show that no
algorithm can label the vertices better than chance, or even distinguish the
block model from an Erd\H{o}s-R\'enyi random graph with high probability. We
also rely on bounds on certain functions of doubly stochastic matrices due to
Achlioptas and Naor; indeed, our lower bound on $d_c$ is the second moment
lower bound on the $k$-colorability threshold for random graphs with a certain
effective degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02680</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02680</id><created>2015-12-06</created><authors><author><keyname>Marzag&#xe3;o</keyname><forenames>Thiago</forenames></author></authors><title>Using SVM to pre-classify government purchases</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Brazilian government often misclassifies the goods it buys. That makes it
hard to audit government expenditures. We cannot know whether the price paid
for a ballpoint pen (code #7510) was reasonable if the pen was misclassified as
a technical drawing pen (code #6675) or as any other good. This paper shows how
we can use machine learning to reduce misclassification. I trained a support
vector machine (SVM) classifier that takes a product description as input and
returns the most likely category codes as output. I trained the classifier
using 20 million goods purchased by the Brazilian government between 1999-04-01
and 2015-04-02. In 83.3% of the cases the correct category code was one of the
three most likely category codes identified by the classifier. I used the
trained classifier to develop a web app that might help the government reduce
misclassification. I open sourced the code on GitHub; anyone can use and modify
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02683</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02683</id><created>2016-01-11</created><authors><author><keyname>MacFie</keyname><forenames>Andrew</forenames></author></authors><title>Software for enumerative and analytic combinatorics</title><categories>cs.MS math.CO</categories><msc-class>68R99</msc-class><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey some general-purpose symbolic software packages that implement
algorithms from enumerative and analytic combinatorics. Software for the
following areas is covered: basic combinatorial objects, symbolic
combinatorics, P\'olya theory, combinatorial species, and asymptotics. We
describe the capabilities that the packages offer as well as some of the
algorithms used, and provide links to original documentation. Most of the
packages are freely downloadable from the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02687</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02687</id><created>2016-01-08</created><authors><author><keyname>Huwald</keyname><forenames>Jan</forenames></author><author><keyname>Richter</keyname><forenames>Stephan</forenames></author><author><keyname>Dittrich</keyname><forenames>Peter</forenames></author></authors><title>Compressing molecular dynamics trajectories: breaking the
  one-bit-per-sample barrier</title><categories>cs.DC cs.IT math.IT physics.comp-ph q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular dynamics simulations yield large amounts of trajectory data. For
their durable storage and accessibility an efficient compression algorithm is
paramount. State of the art domain-specific algorithms combine quantization,
Huffman encoding and occasionally domain knowledge. We propose the high
resolution trajectory compression scheme (HRTC) that relies on piecewise linear
functions to approximate quantized trajectories. By splitting the error budget
between quantization and approximation, our approach beats the current state of
the art by several orders of magnitude given the same error tolerance. It
allows storing samples at far less than one bit per sample. It is simple and
fast enough to be integrated into the inner simulation loop, store every time
step, and become the primary representation of trajectory data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02697</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02697</id><created>2016-01-11</created><authors><author><keyname>Mirzaei</keyname><forenames>Saber</forenames></author></authors><title>Minimum Average Delay of Routing Trees</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general communication tree embedding problem is the problem of mapping a
set of communicating terminals, represented by a graph G, into the set of
vertices of some physical network represented by a tree T. In the case where
the vertices of G are mapped into the leaves of the host tree T the underlying
tree is called a routing tree and if the internal vertices of T are forced to
have degree 3, the host tree is known as layout tree. Different optimization
problems have been studied in the class of communication tree problems such as
well-known minimum edge dilation and minimum edge congestion problems. In this
report we study the less investigate measure i.e. tree length, which is a
representative for average edge dilation (communication delay) measure and also
for average edge congestion measure. We show that finding a routing tree T for
an arbitrary graph G with minimum tree length is an NP-Hard problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02699</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02699</id><created>2016-01-11</created><authors><author><keyname>Kim</keyname><forenames>Juyeop</forenames></author><author><keyname>Choi</keyname><forenames>Sang Won</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Song</keyname><forenames>Yong-Soo</forenames></author><author><keyname>Kim</keyname><forenames>Yong-Kyu</forenames></author></authors><title>Group Communication Over LTE : A Radio Access Perspective</title><categories>cs.NI cs.IT math.IT</categories><comments>will be published in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long Term Evolution (LTE), which has its root on commercial mobile
communications, recently becomes an influential solution to future public
safety communications. To verify the feasibility of LTE for public safety, it
is essential to investigate whether an LTE system optimized for one-to-one
communications is capable of providing group communication, which is one of the
most important service concepts in public safety. In general, a number of first
responders for public safety need to form a group for communicating with each
other or sharing the common data for collaboration on their mission. In this
article, we analyze how the current LTE system can support group communication
in a radio access aspect. Based on the requirements for group communication, we
validate whether each LTE-enabled radio access method can efficiently support
group communication. In addition, we propose a new multicast transmission
scheme, termed index-coded Hybrid Automatic Retransmission reQuest (HARQ). By
applying the index coding concept to HARQ operations, we show that the LTE
system can provide group communication more sophisticatedly in terms of radio
resource efficiency and scalability. We finally evaluate the performance of
LTE-enabled group communication using several radio access methods and show how
the proposed transmission scheme brings the performance enhancement via system
level simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02705</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02705</id><created>2016-01-11</created><authors><author><keyname>Sung</keyname><forenames>Jaeyong</forenames></author><author><keyname>Jin</keyname><forenames>Seok Hyun</forenames></author><author><keyname>Lenz</keyname><forenames>Ian</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal
  Embedding</title><categories>cs.RO cs.AI cs.LG</categories><comments>Journal Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a large variety of objects and appliances in human environments,
such as stoves, coffee dispensers, juice extractors, and so on. It is
challenging for a roboticist to program a robot for each of these object types
and for each of their instantiations. In this work, we present a novel approach
to manipulation planning based on the idea that many household objects share
similarly-operated object parts. We formulate the manipulation planning as a
structured prediction problem and learn to transfer manipulation strategy
across different objects by embedding point-cloud, natural language, and
manipulation trajectory data into a shared embedding space using a deep neural
network. In order to learn semantically meaningful spaces throughout our
network, we introduce a method for pre-training its lower layers for multimodal
feature embedding and a method for fine-tuning this embedding space using a
loss-based margin. In order to collect a large number of manipulation
demonstrations for different objects, we develop a new crowd-sourcing platform
called Robobarista. We test our model on our dataset consisting of 116 objects
and appliances with 249 parts along with 250 language instructions, for which
there are 1225 crowd-sourced manipulation demonstrations. We further show that
our robot with our model can even prepare a cup of a latte with appliances it
has never seen before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02708</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02708</id><created>2016-01-11</created><updated>2016-01-16</updated><authors><author><keyname>Karimi</keyname><forenames>S.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>A hybrid multi-time-step framework for pore-scale and continuum-scale
  modeling of solute transport in porous media</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a computational framework,which is based on a
domain decomposition technique, to employ both finite element method (which is
a popular continuum modeling approach) and lattice Boltzmann method (which is a
popular pore-scale modeling approach) in the same computational domain. To
bridge the gap across the disparate length and time-scales, we first propose a
new method to enforce continuum-scale boundary conditions (i.e., Dirichlet and
Neumann boundary conditions) onto the numerical solution from the lattice
Boltzmann method. This method are based on maximization of entropy and preserve
the non-negativity of discrete distributions under the lattice Boltzmann
method. The proposed computational framework allows different grid sizes,
orders of interpolation, and time-steps in different subdomains. This allows
for different desired resolutions in the numerical solution in different
subdomains. Through numerical experiments, the effect of grid and time-step
refinement, disparity of time-steps in different subdomains, domain
partitioning, and the number of iteration steps on the accuracy and rate of
convergence of the proposed methodology are studied. Finally, to showcase the
performance of this framework in porous media applications, we use it to
simulate the dissolution of calcium carbonate in a porous structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02712</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02712</id><created>2016-01-11</created><authors><author><keyname>Straszak</keyname><forenames>Damian</forenames></author><author><keyname>Vishnoi</keyname><forenames>Nisheeth K.</forenames></author></authors><title>IRLS and Slime Mold: Equivalence and Convergence</title><categories>cs.DS cs.ET cs.IT math.IT math.NA math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a connection between two dynamical systems arising
in entirely different contexts: one in signal processing and the other in
biology. The first is the famous Iteratively Reweighted Least Squares (IRLS)
algorithm used in compressed sensing and sparse recovery while the second is
the dynamics of a slime mold (Physarum polycephalum). Both of these dynamics
are geared towards finding a minimum l1-norm solution in an affine subspace.
Despite its simplicity the convergence of the IRLS method has been shown only
for a certain regularization of it and remains an important open problem. Our
first result shows that the two dynamics are projections of the same dynamical
system in higher dimensions. As a consequence, and building on the recent work
on Physarum dynamics, we are able to prove convergence and obtain complexity
bounds for a damped version of the IRLS algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02727</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02727</id><created>2016-01-11</created><authors><author><keyname>Hull</keyname><forenames>Thomas C.</forenames></author></authors><title>Coloring connections with counting mountain-valley assignments</title><categories>math.CO cs.CG</categories><journal-ref>Origami6: Proceedings of the 6th International Meeting of Origami
  in Science, Mathematics, and Education, The American Mathematical Society,
  2015, pp. 3-11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey more recent attempts at enumerating the number of mountain-valley
assignments that allow a given crease pattern to locally fold flat. In
particular, we solve this problem for square twist tessellations and generalize
the method used to a broader family of crease patterns. We also describe the
more difficult case of the Miura-ori and a recently-discovered bijection with
3-vertex colorings of grid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02732</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02732</id><created>2016-01-12</created><authors><author><keyname>Choudhary</keyname><forenames>Aruni</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Raghvendra</keyname><forenames>Sharath</forenames></author></authors><title>Polynomial-Sized Topological Approximations Using The Permutahedron</title><categories>cs.CG math.AT</categories><comments>24 pages, 1 figure</comments><msc-class>68W01, 68W25, 55U99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical methods to model topological properties of point clouds, such as
the Vietoris-Rips complex, suffer from the combinatorial explosion of complex
sizes. We propose a novel technique to approximate a multi-scale filtration of
the Rips complex with improved bounds for size: precisely, for $n$ points in
$\mathbb{R}^d$, we obtain a $O(d)$-approximation with at most $n2^{O(d \log
k)}$ simplices of dimension $k$ or lower. In conjunction with dimension
reduction techniques, our approach yields a $O(\mathrm{polylog}
(n))$-approximation of size $n^{O(1)}$ for Rips complexes on arbitrary metric
spaces. This result stems from high-dimensional lattice geometry and exploits
properties of the permutahedral lattice, a well-studied structure in discrete
geometry.
  Building on the same geometric concept, we also present a lower bound result
on the size of an approximate filtration: we construct a point set for which
every $(1+\epsilon)$-approximation of the \v{C}ech filtration has to contain
$n^{\Omega(\log\log n)}$ features, provided that $\epsilon &lt;\frac{1}{\log^{1+c}
n}$ for $c\in(0,1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02733</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02733</id><created>2016-01-12</created><authors><author><keyname>Hosseini-Asl</keyname><forenames>Ehsan</forenames></author><author><keyname>Zurada</keyname><forenames>Jacek M.</forenames></author><author><keyname>Nasraoui</keyname><forenames>Olfa</forenames></author></authors><title>Deep Learning of Part-based Representation of Data Using Sparse
  Autoencoders with Nonnegativity Constraints</title><categories>cs.LG stat.ML</categories><comments>Accepted for publication in IEEE Transactions of Neural Networks and
  Learning Systems</comments><doi>10.1109/TNNLS.2015.2479223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a new deep learning autoencoder network, trained by a
nonnegativity constraint algorithm (NCAE), that learns features which show
part-based representation of data. The learning algorithm is based on
constraining negative weights. The performance of the algorithm is assessed
based on decomposing data into parts and its prediction performance is tested
on three standard image data sets and one text dataset. The results indicate
that the nonnegativity constraint forces the autoencoder to learn features that
amount to a part-based representation of data, while improving sparsity and
reconstruction quality in comparison with the traditional sparse autoencoder
and Nonnegative Matrix Factorization. It is also shown that this newly acquired
representation improves the prediction performance of a deep neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02742</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02742</id><created>2016-01-12</created><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author></authors><title>Property Checking By Logic Relaxation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework for Property Checking (PC) of sequential
circuits. It is based on a method called Lo-gic Relaxation (LoR). Given a
safety property, the LoR method relaxes the transition system at hand, which
leads to expanding the set of reachable states. For j-th time frame, the LoR
method computes a superset A_j of the set of bad states reachable in j
transitions only by the relaxed system. Set A_j is constructed by a technique
called partial quantifier elimination. If A_j does not contain a bad state and
this state is reachable in j transitions in the relaxed system, it is also
reachable in the original system. Hence the property in question does not hold.
  The appeal of PC by LoR is as follows. An inductive invariant (or a
counterexample) generated by LoR is a result of computing the states reachable
only in the relaxed system. So, the complexity of PC can be drastically reduced
by finding a &quot;faulty&quot; relaxation that is close to the original system. This is
analogous to equivalence checking whose complexity strongly depends on how
similar the designs to be compared are.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02745</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02745</id><created>2016-01-12</created><authors><author><keyname>Smolensky</keyname><forenames>Paul</forenames></author><author><keyname>Lee</keyname><forenames>Moontae</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Yih</keyname><forenames>Wen-tau</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>Basic Reasoning with Tensor Product Representations</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the initial development of a general theory for
mapping inference in predicate logic to computation over Tensor Product
Representations (TPRs; Smolensky (1990), Smolensky &amp; Legendre (2006)). After an
initial brief synopsis of TPRs (Section 0), we begin with particular examples
of inference with TPRs in the 'bAbI' question-answering task of Weston et al.
(2015) (Section 1). We then present a simplification of the general analysis
that suffices for the bAbI task (Section 2). Finally, we lay out the general
treatment of inference over TPRs (Section 3). We also show the simplification
in Section 2 derives the inference methods described in Lee et al. (2016); this
shows how the simple methods of Lee et al. (2016) can be formally extended to
more general reasoning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02752</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02752</id><created>2016-01-12</created><updated>2016-01-27</updated><authors><author><keyname>Dastjerdi</keyname><forenames>Amir Vahid</forenames></author><author><keyname>Gupta</keyname><forenames>Harshit</forenames></author><author><keyname>Calheiros</keyname><forenames>Rodrigo N.</forenames></author><author><keyname>Ghosh</keyname><forenames>Soumya K.</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Fog Computing: Principles, Architectures, and Applications</title><categories>cs.DC</categories><comments>26 pages, 6 figures, a Book Chapter in Internet of Things: Principles
  and Paradigms, R. Buyya and A. Dastjerdi (eds), Morgan Kaufmann, Burlington,
  Massachusetts, USA, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Everything (IoE) solutions gradually bring every object
online, and processing data in centralized cloud does not scale to requirements
of such environment. This is because, there are applications such as health
monitoring and emergency response that require low latency and delay caused by
transferring data to the cloud and then back to the application can seriously
impact the performance. To this end, Fog computing has emerged, where cloud
computing is extended to the edge of the network to decrease the latency and
network congestion. Fog computing is a paradigm for managing a highly
distributed and possibly virtualized environment that provides compute and
network services between sensors and cloud data centers. This chapter provides
background and motivations on emergence of Fog computing and defines its key
characteristics. In addition, a reference architecture for Fog computing is
presented and recent related development and applications are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02756</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02756</id><created>2016-01-12</created><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Zeilberger</keyname><forenames>Doron</forenames></author></authors><title>Factorization of C-finite Sequences</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how to decide whether a given C-finite sequence can be written
nontrivially as a product of two other C-finite sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02763</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02763</id><created>2016-01-12</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Bounds and Constructions of Codes with Multiple Localities</title><categories>cs.IT cs.DM cs.NI math.CO math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies bounds and constructions of locally repairable codes
(LRCs) with multiple localities so-called multiple-locality LRCs (ML-LRCs). In
the simplest case of two localities some code symbols of an ML-LRC have a
certain locality while the remaining code symbols have another one. We extend
two bounds, the Singleton and the alphabet-dependent upper bound on the
dimension of Cadambe--Mazumdar for LRCs, to the case of ML-LRCs with more than
two localities. Furthermore, we construct Singleton-optimal ML-LRCs as well as
codes that achieve the extended alphabet-dependent bound. We give a family of
binary ML-LRCs based on generalized code concatenation that is optimal with
respect to the alphabet-dependent bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02768</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02768</id><created>2016-01-12</created><authors><author><keyname>Frey</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>Potioc, UB</affiliation></author><author><keyname>Daniel</keyname><forenames>Maxime</forenames><affiliation>INRIA, Potioc</affiliation></author><author><keyname>Castet</keyname><forenames>Julien</forenames><affiliation>INRIA, Potioc</affiliation></author><author><keyname>Hachet</keyname><forenames>Martin</forenames><affiliation>INRIA, Potioc</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>Potioc, INRIA</affiliation></author></authors><title>Framework for Electroencephalography-based Evaluation of User Experience</title><categories>cs.HC</categories><comments>in ACM. CHI '16 - SIGCHI Conference on Human Factors in Computing
  System, May 2016, San Jose, United States</comments><proxy>ccsd</proxy><doi>10.1145/2858036.2858525</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring brain activity with electroencephalography (EEG) is mature enough
to assess mental states. Combined with existing methods, such tool can be used
to strengthen the understanding of user experience. We contribute a set of
methods to estimate continuously the user's mental workload, attention and
recognition of interaction errors during different interaction tasks. We
validate these measures on a controlled virtual environment and show how they
can be used to compare different interaction techniques or devices, by
comparing here a keyboard and a touch-based interface. Thanks to such a
framework, EEG becomes a promising method to improve the overall usability of
complex computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02771</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02771</id><created>2016-01-12</created><authors><author><keyname>Adamczewski</keyname><forenames>Boris</forenames></author><author><keyname>Cassaigne</keyname><forenames>Julien</forenames></author><author><keyname>Gonidec</keyname><forenames>Marion Le</forenames></author></authors><title>On the computational complexity of algebraic numbers: the
  Hartmanis--Stearns problem revisited</title><categories>math.NT cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexity of integer base expansions of algebraic irrational
numbers from a computational point of view. We show that the Hartmanis--Stearns
problem can be solved in a satisfactory way for the class of multistack
machines. In this direction, our main result is that the base-$b$ expansion of
an algebraic irrational real number cannot be generated by a deterministic
pushdown automaton. We also confirm an old claim of Cobham proving that such
numbers cannot be generated by a tag machine with dilation factor larger than
one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02778</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02778</id><created>2016-01-12</created><authors><author><keyname>Ingibergsson</keyname><forenames>Johann Thor Mogensen</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author><author><keyname>Kraft</keyname><forenames>Dirk</forenames></author></authors><title>Towards Declarative Safety Rules for Perception Specification
  Architectures</title><categories>cs.RO cs.CY</categories><comments>Presented at DSLRob 2015 (arXiv:1601.00877)</comments><report-no>DSLRob/2015/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agriculture has a high number of fatalities compared to other blue collar
fields, additionally population decreasing in rural areas is resulting in
decreased work force. These issues have resulted in increased focus on
improving efficiency of and introducing autonomy in agriculture. Field robots
are an increasingly promising branch of robotics targeted at full automation in
agriculture. The safety aspect however is rely addressed in connection with
safety standards, which limits the real-world applicability. In this paper we
present an analysis of a vision pipeline in connection with functional-safety
standards, in order to propose solutions for how to ascertain that the system
operates as required. Based on the analysis we demonstrate a simple mechanism
for verifying that a vision pipeline is functioning correctly, thus improving
the safety in the overall system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02781</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02781</id><created>2016-01-12</created><authors><author><keyname>Zareen</keyname><forenames>Farhana Javed</forenames></author><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Jabin</keyname><forenames>Suraiya</forenames></author></authors><title>A Cloud based Mobile Biometric Authentication Framework</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exponential growth of enrollment in the biometric system produces massive
amount of high dimensionality data that leads to the degradation of performance
of the system. Also different kind of issues pops up in handling such huge data
sets. Therefore in order to overcome the performance issues arising due to this
data deluge we aim to implement this system in a distributed manner on a high
performance cluster cloud. High availability and scalability are some of the
added advantages of using this approach. In this paper we propose a mobile
biometric system that uses dynamic signatures and performs authentication on
the go. This framework includes steps involving data capture using any handheld
mobile device, then storage, preprocessing and training in a distributed manner
overcloud. Moreover, the methodology adopted by us is very novel as it provides
better time efficiency and scalability. We have systematically designed and
conducted experiments to evaluate the proposed framework on dynamic signature
datasets recorded in real time. The experiments demonstrate that better
performance was achieved by our proposed framework as compared to the other
methods used in recent literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02788</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02788</id><created>2016-01-12</created><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Symbol-Level Multiuser MISO Precoding for Multi-level Adaptive
  Modulation: A Multicast View</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbol-level precoding is a new paradigm for multiuser downlink systems which
aims at creating constructive interference among the transmitted data streams.
This can be enabled by designing the precoded signal of the multiantenna
transmitter on a symbol level, taking into account both channel state
information and data symbols. Previous literature has studied this paradigm for
MPSK modulations by addressing various performance metrics, such as power
minimization and maximization of the minimum rate. In this paper, we extend
this to generic multi-level modulations i.e. MQAM and APSK by establishing
connection to PHY layer multicasting with phase constraints. Furthermore, we
address adaptive modulation schemes which are crucial in enabling the
throughput scaling of symbol-level precoded systems. In this direction, we
design signal processing algorithms for minimizing the required power under
per-user SINR or goodput constraints. Extensive numerical results show that the
proposed algorithm provides considerable power and energy efficiency gains,
while adapting the employed modulation scheme to match the requested data rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02789</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02789</id><created>2016-01-12</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kor&#x17e;inek</keyname><forenames>Danijel</forenames></author></authors><title>Comparison and Adaptation of Automatic Evaluation Metrics for Quality
  Assessment of Re-Speaking</title><categories>cs.CL stat.AP stat.ML</categories><comments>Comparison and Adaptation of Automatic Evaluation Metrics for Quality
  Assessment of Re-Speaking. arXiv admin note: text overlap with
  arXiv:1509.09088</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Re-speaking is a mechanism for obtaining high quality subtitles for use in
live broadcast and other public events. Because it relies on humans performing
the actual re-speaking, the task of estimating the quality of the results is
non-trivial. Most organisations rely on humans to perform the actual quality
assessment, but purely automatic methods have been developed for other similar
problems, like Machine Translation. This paper will try to compare several of
these methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will
then be matched to the human-derived NER metric, commonly used in re-speaking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02800</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02800</id><created>2016-01-12</created><authors><author><keyname>Liqat</keyname><forenames>Umer</forenames></author><author><keyname>Bankovic</keyname><forenames>Zorana</forenames></author><author><keyname>Lopez-Garcia</keyname><forenames>Pedro</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Inferring Energy Bounds Statically by Evolutionary Analysis of Basic
  Blocks</title><categories>cs.DC cs.PL</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/6</report-no><acm-class>F.3.2; D.3.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are currently witnessing an increasing number of energy-bound devices,
including in some cases mission critical systems, for which there is a need to
optimize their energy consumption and verify that they will perform their
function within the available energy budget. In this work we propose a novel
parametric approach to estimating tight energy bounds (both upper and lower)
that are practical for energy verification and optimization applications in
embedded systems. Our approach consists in dividing a program into basic
(&quot;branchless&quot;) blocks, establishing the maximal (resp. minimal) energy
consumption for each block using an evolutionary algorithm, and combining the
obtained values according to the program control flow, using static analysis,
to produce energy bound functions. Such functions depend on input data sizes,
and return upper or lower bounds on the energy consumption of the program for
any given set of input values of those sizes, without running the program. The
approach has been tested on XMOS chips, but is general enough to be applied to
any microprocessor and programming language. Our experimental results show that
the bounds obtained by our prototype tool can be tight while remaining on the
safe side of budgets in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02828</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02828</id><created>2016-01-12</created><authors><author><keyname>Swietojanski</keyname><forenames>Pawel</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Learning Hidden Unit Contributions for Unsupervised Acoustic Model
  Adaptation</title><categories>cs.CL cs.LG cs.SD</categories><comments>13 pages, Submitted to IEEE/ACM Transactions on Audio, Speech and
  Language Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a broad study on the adaptation of neural network acoustic
models by means of learning hidden unit contributions (LHUC) -- a method that
linearly re-combines hidden units in a speaker- or environment-dependent manner
using small amounts of unsupervised adaptation data. We also extend LHUC to a
speaker adaptive training (SAT) framework that leads to more adaptable DNN
acoustic model, which can work in both a speaker-dependent and a
speaker-independent manner, without the requirement to maintain auxiliary
speaker-dependent feature extractors or to introduce significant
speaker-dependent changes to the DNN structure. Through a series of experiments
on four different speech recognition benchmarks (TED talks, Switchboard, AMI
meetings and Aurora4) and over 270 test speakers we show that LHUC in both its
test-only and SAT variants results in consistent word error rate reductions
ranging from 5% to 23% relative depending on the task and the degree of
mismatch between training and test data. In addition we have investigated the
effect of the amount of adaptation data per speaker, the quality of adaptation
targets when estimating transforms in an unsupervised manner, the
complementarity to other adaptation techniques, one-shot adaptation, and an
extension to adapting DNNs trained in a sequence discriminative manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02831</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02831</id><created>2016-01-12</created><authors><author><keyname>Faigle</keyname><forenames>Ulrich</forenames></author><author><keyname>Grabisch</keyname><forenames>Michel</forenames></author></authors><title>Least Square Approximations and Linear Values of Cooperative Games</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many important values for cooperative games are known to arise from least
square optimization problems. The present investigation develops an
optimization framework to explain and clarify this phenomenon in a general
setting. The main result shows that every linear value results from some least
square approximation problem and that, conversely, every least square
approximation problem with linear constraints yields a linear value.
  This approach includes and extends previous results on so-called least square
values and semivalues in the literature. In particular, is it demonstrated how
known explicit formulas for solutions under additional assumptions easily
follow from the general results presented here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02848</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02848</id><created>2016-01-12</created><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Invariance to ordinal transformations in rank-aware databases</title><categories>cs.DB</categories><msc-class>68P15, 03B52</msc-class><acm-class>H.2.3; H.2.4; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study influence of ordinal transformations on results of queries in
rank-aware databases which derive their operations with ranked relations from
totally ordered structures of scores with infima acting as aggregation
functions. We introduce notions of ordinal containment and equivalence of
ranked relations and prove that infima-based algebraic operations with ranked
relations are invariant to ordinal transformations: Queries applied to original
and transformed data yield results which are equivalent in terms of the order
given by scores, meaning that top-k results of queries remain the same. We show
this important property is preserved in alternative query systems based of
relational calculi developed in context of G\&quot;odel logic. We comment on
relationship to monotone query evaluation and show that the results can be
attained in alternative rank-aware approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02852</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02852</id><created>2016-01-12</created><authors><author><keyname>Choi</keyname><forenames>Jinsoo</forenames></author><author><keyname>Oh</keyname><forenames>Tae-Hyun</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>Human Attention Estimation for Natural Images: An Automatic Gaze
  Refinement Approach</title><categories>cs.CV cs.HC cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photo collections and its applications today attempt to reflect user
interactions in various forms. Moreover, photo collections aim to capture the
users' intention with minimum effort through applications capturing user
intentions. Human interest regions in an image carry powerful information about
the user's behavior and can be used in many photo applications. Research on
human visual attention has been conducted in the form of gaze tracking and
computational saliency models in the computer vision community, and has shown
considerable progress. This paper presents an integration between implicit gaze
estimation and computational saliency model to effectively estimate human
attention regions in images on the fly. Furthermore, our method estimates human
attention via implicit calibration and incremental model updating without any
active participation from the user. We also present extensive analysis and
possible applications for personal photo collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02864</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02864</id><created>2016-01-12</created><authors><author><keyname>Heinlein</keyname><forenames>Daniel</forenames></author><author><keyname>Kiermaier</keyname><forenames>Michael</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Wassermann</keyname><forenames>Alfred</forenames></author></authors><title>Tables of subspace codes</title><categories>math.CO cs.IT math.IT</categories><comments>13 pages, 6 tables, 2 screenshots</comments><msc-class>51E23, 05B40, 11T71, 94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main problem of subspace coding asks for the maximum possible cardinality
of a subspace code with minimum distance at least $d$ over $\mathbb{F}_q^n$,
where the dimensions of the codewords, which are vector spaces, are contained
in $K\subseteq\{0,1,\dots,n\}$. In the special case of $K=\{k\}$ one speaks of
constant dimension codes. Since this emerging field is very prosperous on the
one hand side and there are a lot of connections to classical objects from
Galois geometry it is a bit difficult to keep or to obtain an overview about
the current state of knowledge. To this end we have implemented an on-line
database of the (at least to us) known results at
\url{subspacecodes.uni-bayreuth.de}. The aim of this recurrently updated
technical report is to provide a user guide how this technical tool can be used
in research projects and to describe the so far implemented theoretic and
algorithmic knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02865</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02865</id><created>2016-01-12</created><authors><author><keyname>Nightingale</keyname><forenames>Peter</forenames></author><author><keyname>Rendl</keyname><forenames>Andrea</forenames></author></authors><title>Essence' Description</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A description of the Essence' language as used by the tool Savile Row.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02867</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02867</id><created>2016-01-12</created><authors><author><keyname>Kurz</keyname><forenames>Denis</forenames></author><author><keyname>Mutzel</keyname><forenames>Petra</forenames></author></authors><title>A Sidetrack-Based Algorithm for Finding the k Shortest Simple Paths in a
  Directed Graph</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for the k shortest simple path problem on weighted
directed graphs (kSSP) that is based on Eppstein's algorithm for a similar
problem in which paths are allowed to contain cycles. In contrast to most other
algorithms for kSSP, ours is not based on Yen's algorithm and does not solve
replacement path problems. Its worst-case running time is on par with
state-of-the-art algorithms for kSSP. Using our algorithm, one may find O(m)
simple paths with a single shortest path tree computation and O(n + m)
additional time per path in well-behaved cases, where n is the number of nodes
and m is the number of edges. Our computational results show that on random
graphs and large road networks, these well-behaved cases are quite common and
our algorithm is faster than existing algorithms by an order of magnitude.
Further, the running time is far better predictable due to very small
dispersion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02880</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02880</id><created>2015-12-30</created><updated>2016-01-18</updated><authors><author><keyname>Nittoor</keyname><forenames>Vivek S.</forenames></author></authors><title>About the catalog of (3, g) Hamiltonian bipartite graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a catalog of a catalog of a class of (3, g) graphs for even
girth g, satisfying 6 &lt;= g &lt;= 16. A (k, g) graph is a graph with regular degree
k and girth g. Our catalog has the following properties. Firstly, our catalog
contains the smallest known (3, g) graphs. We have identified an appropriate
class of trivalent graphs for our catalog, such that the (3, g) graph of
minimum order within the class is also the smallest known (3, g) graph.
Secondly, our catalog contains (3, g) graphs for more orders than other
listings. Thirdly, the class of graphs have been defined so that a practical
algorithm to generate graphs can be created, and have implemented such an
algorithm. Fourthly, our catalog is infinite, we extend the results into
knowledge about infinitely many graphs. Our findings are as follows. Firstly,
we have identified Hamiltonian bipartite graphs (HBGs), as a promising class of
trivalent graphs that can lead to a catalog of (3, g) graphs for even girth g
with graphs for more orders than other listings, that is also expected to
contain a (3, g) graph with minimum order. Secondly, our catalog of (3, g)
graphs has graphs for more orders compared with other lists, with many graphs
outside of the vertex-transitive class. Thirdly, in order to make the
computation more tractable, we have introduced the symmetry factor b, which
reflects the extent of rotational symmetry. We introduce the D3 chord index
notation, a concise notation for trivalent HBGs. Fourthly, results on the
minimum order for existence of a (3, g) HBG, and minimum symmetry factor for
existence of a (3, g) HBG are of wider interest from an extremal graph theory
perspective. Lastly from a cage problem perspective, an analysis on upper
bounds and lower bounds within subclasses has been done, where proofs of
emptiness of several subclasses partially supports the likeliness of the (3,
14) record graph to be a cage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02882</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02882</id><created>2015-12-29</created><authors><author><keyname>Faigle</keyname><forenames>Ulrich</forenames></author><author><keyname>Sch&#xf6;nhuth</keyname><forenames>Alexander</forenames></author></authors><title>On Hidden States in Quantum Random Walks</title><categories>quant-ph cs.IT math.IT</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently pointed out that identifiability of quantum random walks and
hidden Markov processes underlie the same principles. This analogy immediately
raises questions on the existence of hidden states also in quantum random walks
and their relationship with earlier debates on hidden states in quantum
mechanics. The overarching insight was that not only hidden Markov processes,
but also quantum random walks are finitary processes. Since finitary processes
enjoy nice asymptotic properties, this also encourages to further investigate
the asymptotic properties of quantum random walks. Here, answers to all these
questions are given. Quantum random walks, hidden Markov processes and finitary
processes are put into a unifying model context. In this context, quantum
random walks are seen to not only enjoy nice ergodic properties in general, but
also intuitive quantum-style asymptotic properties. It is also pointed out how
hidden states arising from our framework relate to hidden states in earlier,
prominent treatments on topics such as the EPR paradoxon or Bell's
inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02887</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02887</id><created>2015-12-30</created><updated>2016-01-18</updated><authors><author><keyname>Nittoor</keyname><forenames>Vivek S.</forenames></author></authors><title>Comparison of catalog of (3, g) Hamiltonian bipartite graphs with other
  known lists</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on discussion of a catalog of a class of (3, g)
graphs for even girth g. A (k, g) graph is a graph with regular degree k and
girth g. We first show that this catalog of graphs extends infintely. We
compare our catalog with other known lists of (3, g) graphs such as the
enumerations of trivalent symmetric graphs enumerations of trivalent
vertex-transitive graphs, and show that our catalog has graphs for more orders
than these lists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02904</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02904</id><created>2016-01-12</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author><author><keyname>Noah</keyname><forenames>Shahrul Azman Mohd.</forenames></author><author><keyname>Saad</keyname><forenames>Saidah</forenames></author></authors><title>Social Network Extraction: Superficial Method and Information Retrieval</title><categories>cs.IR cs.SI</categories><comments>11 pages, 1 figures</comments><acm-class>F.2.2</acm-class><journal-ref>Proceeding of International Conference on Informatics for
  Development (ICID'11), c2-110 - c2-115 (2011)</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Social network has become one of the themes of government issues, mainly
dealing with the chaos. The use of web is steadily gaining ground in these
issues. However, most of the web documents are unstructured and lack of
semantic. In this paper we proposed an Information Retrieval driven method for
dealing with heterogeneity of features in the web. The proposed solution is to
compare some approaches have shown the capacity to extract social relation:
strength relations and relations based on online academic database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02912</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02912</id><created>2016-01-12</created><authors><author><keyname>Burger</keyname><forenames>Martin</forenames></author><author><keyname>Gilboa</keyname><forenames>Guy</forenames></author><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Eckardt</keyname><forenames>Lina</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Spectral Decompositions using One-Homogeneous Functionals</title><categories>cs.NA math.OC math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the use of absolutely one-homogeneous regularization
functionals in a variational, scale space, and inverse scale space setting to
define a nonlinear spectral decomposition of input data. We present several
theoretical results that explain the relation between the different
definitions. Additionally, results on the orthogonality of the decomposition, a
Parseval-type identity and the notion of generalized (nonlinear) eigenvectors
closely link our nonlinear multiscale decompositions to the well-known linear
filtering theory. Numerical results are used to illustrate our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02913</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02913</id><created>2016-01-12</created><authors><author><keyname>Li</keyname><forenames>Xinchao</forenames></author><author><keyname>Xu</keyname><forenames>Peng</forenames></author><author><keyname>Shi</keyname><forenames>Yue</forenames></author><author><keyname>Larson</keyname><forenames>Martha</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>Learning Subclass Representations for Visually-varied Image
  Classification</title><categories>cs.MM cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present a subclass-representation approach that predicts
the probability of a social image belonging to one particular class. We explore
the co-occurrence of user-contributed tags to find subclasses with a strong
connection to the top level class. We then project each image on to the
resulting subclass space to generate a subclass representation for the image.
The novelty of the approach is that subclass representations make use of not
only the content of the photos themselves, but also information on the
co-occurrence of their tags, which determines membership in both subclasses and
top-level classes. The novelty is also that the images are classified into
smaller classes, which have a chance of being more visually stable and easier
to model. These subclasses are used as a latent space and images are
represented in this space by their probability of relatedness to all of the
subclasses. In contrast to approaches directly modeling each top-level class
based on the image content, the proposed method can exploit more information
for visually diverse classes. The approach is evaluated on a set of $2$ million
photos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale
Flickr-tag Image Classification Grand Challenge. Experiments show that the
proposed system delivers sound performance for visually diverse classes
compared with methods that directly model top classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02919</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02919</id><created>2016-01-12</created><updated>2016-01-28</updated><authors><author><keyname>Andrearczyk</keyname><forenames>Vincent</forenames></author><author><keyname>Whelan</keyname><forenames>Paul F.</forenames></author></authors><title>Using Filter Banks in Convolutional Neural Networks for Texture
  Classification</title><categories>cs.CV cs.NE</categories><comments>7 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has established many new state of the art solutions in the last
decade in areas such as object, scene and speech recognition. In particular
Convolutional Neural Network (CNN) is a category of deep learning which obtains
excellent results in object detection and recognition tasks. Its architecture
is indeed well suited to object analysis by learning and classifying complex
(deep) features that represent parts of an object or the object itself.
However, some of its features are very similar to texture analysis methods. CNN
layers can be thought of as filter banks of complexity increasing with the
depth. Filter banks are powerful tools to extract texture features and have
been widely used in texture analysis. In this paper we develop a simple network
architecture named Texture CNN (T-CNN) which explores this observation. It is
built on the idea that the overall shape information extracted by the fully
connected layers of a classic CNN is of minor importance in texture analysis.
Therefore, we pool an energy measure from the last convolution layer which we
connect to a fully connected layer. We show that our approach can improve the
performance of a network while greatly reducing the memory usage and
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02923</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02923</id><created>2016-01-12</created><authors><author><keyname>Zhang</keyname><forenames>June</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author></authors><title>Cascading Edge Failures: A Dynamic Network Process</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the dynamics of edges in a network. The Dynamic Bond
Percolation (DBP) process models, through stochastic local rules, the
dependence of an edge $(a,b)$ in a network on the states of its neighboring
edges. Unlike previous models, DBP does not assume statistical independence
between different edges. In applications, this means for example that failures
of transmission lines in a power grid are not statistically independent, or
alternatively, relationships between individuals (dyads) can lead to changes in
other dyads in a social network. We consider the time evolution of the
probability distribution of the network state, the collective states of all the
edges (bonds), and show that it converges to a stationary distribution. We use
this distribution to study the emergence of global behaviors like consensus
(i.e., catastrophic failure or full recovery of the entire grid) or coexistence
(i.e., some failed and some operating substructures in the grid). In
particular, we show that, depending on the local dynamical rule, different
network substructures, such as hub or triangle subgraphs, are more prone to
failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02927</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02927</id><created>2016-01-12</created><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Momeni</keyname><forenames>Fakhri</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author></authors><title>Opening Scholarly Communication in Social Sciences: Supporting Open Peer
  Review with Fidus Writer</title><categories>cs.DL</categories><comments>4 pages, 1 figure, poster paper accepted at the 2016 Annual EA
  Conference: &quot;Innovating the Gutenberg Galaxis. The role of peer review and
  open access in university knowledge dissemination and evaluation&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our system will initially provide readers, authors and reviewers with an
alternative, thus having the potential to gain wider acceptance and gradually
replace the old, incoherent publication process of our journals and of others
in related fields. It will make journals more &quot;open&quot; (in terms of reusability)
that are open access already, and it has the potential to serve as an incentive
for turning &quot;closed&quot; journals into open access ones. In this poster paper we
will present the framework of the OSCOSS system and highlight the reviewer use
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02932</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02932</id><created>2016-01-12</created><authors><author><keyname>Tomescu</keyname><forenames>Alexandru I.</forenames></author><author><keyname>Medvedev</keyname><forenames>Paul</forenames></author></authors><title>Safe and complete contig assembly via omnitigs</title><categories>q-bio.QM cs.DM cs.DS q-bio.GN</categories><comments>A shorter version of this paper will appear in the proceedings of
  RECOMB 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contig assembly is the first stage that most assemblers solve when
reconstructing a genome from a set of reads. Its output consists of contigs --
a set of strings that are promised to appear in any genome that could have
generated the reads. From the introduction of contigs 20 years ago, assemblers
have tried to obtain longer and longer contigs, but the following question was
never solved: given a genome graph $G$ (e.g. a de Bruijn, or a string graph),
what are all the strings that can be safely reported from $G$ as contigs? In
this paper we finally answer this question, and also give a polynomial time
algorithm to find them. Our experiments show that these strings, which we call
omnitigs, are 66% to 82% longer on average than the popular unitigs, and 29% of
dbSNP locations have more neighbors in omnitigs than in unitigs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02939</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02939</id><created>2016-01-05</created><authors><author><keyname>Gainer-Dewar</keyname><forenames>Andrew</forenames></author><author><keyname>Vera-Licona</keyname><forenames>Paola</forenames></author></authors><title>The minimal hitting set generation problem: algorithms and computation</title><categories>cs.DS cs.AI cs.CC</categories><msc-class>68W05, 68R05, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding inclusion-minimal &quot;hitting sets&quot; for a given collection of sets is a
fundamental combinatorial problem with applications in domains as diverse as
Boolean algebra, computational biology, and data mining. Much of the
algorithmic literature focuses on the problem of *recognizing* the collection
of minimal hitting sets; however, in many of the applications, it is more
important to *generate* these hitting sets. We survey twenty algorithms from
across a variety of domains, considering their history, classification, useful
features, and computational performance on a variety of synthetic and
real-world inputs. We also provide a suite of implementations of these
algorithms with a ready-to-use, platform-agnostic interface based on Docker
containers and the AlgoRun framework, so that interested computational
scientists can easily perform similar tests with inputs from their own research
areas on their own computers or through a convenient Web interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02947</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02947</id><created>2015-12-26</created><authors><author><keyname>Radecki</keyname><forenames>Peter</forenames></author><author><keyname>Hencey</keyname><forenames>Brandon</forenames></author></authors><title>Online Model Estimation for Predictive Thermal Control of Buildings</title><categories>cs.SY cs.LG</categories><comments>14 pages, 15 figures, 2 tables, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes a general, scalable method to learn control-oriented
thermal models of buildings that could enable wide-scale deployment of
cost-effective predictive controls. An Unscented Kalman Filter augmented for
parameter and disturbance estimation is shown to accurately learn and predict a
building's thermal response. Recent studies of heating, ventilating, and air
conditioning (HVAC) systems have shown significant energy savings with advanced
model predictive control (MPC). A scalable cost-effective method to readily
acquire accurate, robust models of individual buildings' unique thermal
envelopes has historically been elusive and hindered the widespread deployment
of prediction-based control systems. Continuous commissioning and lifetime
performance of these thermal models requires deployment of on-line data-driven
system identification and parameter estimation routines. We propose a novel
gray-box approach using an Unscented Kalman Filter based on a multi-zone
thermal network and validate it with EnergyPlus simulation data. The filter
quickly learns parameters of a thermal network during periods of known or
constrained loads and then characterizes unknown loads in order to provide
accurate 24+ hour energy predictions. This study extends our initial
investigation by formalizing parameter and disturbance estimation routines and
demonstrating results across a year-long study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02960</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02960</id><created>2016-01-12</created><authors><author><keyname>Almeida</keyname><forenames>P. J.</forenames></author><author><keyname>Napp</keyname><forenames>D.</forenames></author><author><keyname>Pinto</keyname><forenames>R.</forenames></author></authors><title>Superregular matrices and applications to convolutional codes</title><categories>cs.IT math.CO math.IT math.RA</categories><msc-class>94B10, 15B33</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main results of this paper are twofold: the first one is a matrix
theoretical result. We say that a matriz is superregular if all of its minors
that are not trivially zero are nonzero. Given a a times b, a larger than or
equal to b, superregular matrix over a field, we show that if all of its rows
are nonzero then any linear combination of its columns, with nonzero
coefficients, has at least a-b+1 nonzero entries. Secondly, we make use of this
result to construct convolutional codes that attain the maximum possible
distance for some fixed parameters of the code, namely, the rate and the Forney
indices. These results answer some open questions on distances and
constructions of convolutional codes posted in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02970</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02970</id><created>2016-01-12</created><authors><author><keyname>Cichy</keyname><forenames>Radoslaw M.</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Pantazis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author><author><keyname>Oliva</keyname><forenames>Aude</forenames></author></authors><title>Deep Neural Networks predict Hierarchical Spatio-temporal Cortical
  Dynamics of Human Visual Object Recognition</title><categories>cs.CV q-bio.NC</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complex multi-stage architecture of cortical visual pathways provides the
neural basis for efficient visual object recognition in humans. However, the
stage-wise computations therein remain poorly understood. Here, we compared
temporal (magnetoencephalography) and spatial (functional MRI) visual brain
representations with representations in an artificial deep neural network (DNN)
tuned to the statistics of real-world visual recognition. We showed that the
DNN captured the stages of human visual processing in both time and space from
early visual areas towards the dorsal and ventral streams. Further
investigation of crucial DNN parameters revealed that while model architecture
was important, training on real-world categorization was necessary to enforce
spatio-temporal hierarchical relationships with the brain. Together our results
provide an algorithmically informed view on the spatio-temporal dynamics of
visual object recognition in the human visual brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02975</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02975</id><created>2016-01-12</created><authors><author><keyname>M&#x142;ynarska</keyname><forenames>Ewa</forenames></author><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Indicators of Good Student Performance in Moodle Activity Data</title><categories>cs.CY cs.AI</categories><comments>Short version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we conduct an analysis of Moodle activity data focused on
identifying early predictors of good student performance. The analysis shows
that three relevant hypotheses are largely supported by the data. These
hypotheses are: early submission is a good sign, a high level of activity is
predictive of good results and evening activity is even better than daytime
activity. We highlight some pathological examples where high levels of activity
correlates with bad results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.02987</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.02987</id><created>2016-01-12</created><authors><author><keyname>Raghavan</keyname><forenames>Vasanthan</forenames></author><author><keyname>Cezanne</keyname><forenames>Juergen</forenames></author><author><keyname>Subramanian</keyname><forenames>Sundar</forenames></author><author><keyname>Sampath</keyname><forenames>Ashwin</forenames></author><author><keyname>Koymen</keyname><forenames>Ozge</forenames></author></authors><title>Beamforming Tradeoffs for Initial UE Discovery in Millimeter-Wave MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>36 pages, 19 figures, 1 table, Accepted for publication for the
  Special issue call on signal processing for millimeter wave systems, JSTSP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave MIMO systems have gained increasing traction towards the goal
of meeting the high data-rate requirements in next-generation wireless systems.
The focus of this work is on low-complexity beamforming approaches for initial
UE discovery in such systems. Towards this goal, we first note the structure of
the optimal beamformer with per-antenna gain and phase control and the
structure of good beamformers with per-antenna phase-only control. Learning
these beamforming structures in mmW systems is fraught with considerable
complexities such as the need for a non-broadcast system design, the
sensitivity of the beamformer approximants to small path length changes, etc.
To overcome these issues, we establish a physical interpretation between these
beamformer structures and the angles of departure/arrival of the dominant
path(s). This physical interpretation provides a theoretical underpinning to
the emerging interest on directional beamforming approaches that are less
sensitive to small path length changes. While classical approaches for
direction learning such as MUSIC have been well-understood, they suffer from
many practical difficulties in a mmW context such as a non-broadcast system
design and high computational complexity. A simpler broadcast solution for mmW
systems is the adaptation of directional codebooks for beamforming at the two
ends. We establish fundamental limits for the best beam broadening codebooks
and propose a construction motivated by a virtual subarray architecture that is
within a couple of dB of the best tradeoff curve at all useful beam broadening
factors. We finally provide the received SNR loss-UE discovery latency tradeoff
with the proposed constructions. Our results show that users with a reasonable
link margin can be quickly discovered by the proposed design with a smooth
roll-off in performance as the link margin deteriorates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03004</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03004</id><created>2016-01-12</created><authors><author><keyname>Don</keyname><forenames>Rasika Lakmal Hettiarachchige</forenames></author><author><keyname>Samarabandu</keyname><forenames>Jagath</forenames></author></authors><title>Novel velocity model to improve indoor localization using inertial
  navigation with sensors on a smartphone</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalized velocity model to improve localization when using an
Inertial Navigation System (INS). This algorithm was applied to correct the
velocity of a smart phone based indoor INS system to increase the accuracy by
counteracting the accumulation of large drift caused by sensor reading errors.
We investigated the accuracy of the algorithm with three different velocity
models which were derived from the actual velocity measured at the hip of
walking person. Our results show that the proposed method with Gaussian
velocity model achieves competitive accuracy with a 50\% less variance over
Step and Heading approach proving the accuracy and robustness of proposed
method. We also investigated the frequency of applying corrections and found
that a minimum of 5\% corrections per step is sufficient for improved accuracy.
The proposed method is applicable in indoor localization and tracking
applications based on smart phone where traditional approaches such as GNSS
suffers from many issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03022</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03022</id><created>2016-01-12</created><authors><author><keyname>Navarro-Sune</keyname><forenames>X</forenames></author><author><keyname>Hudson</keyname><forenames>A. L.</forenames></author><author><keyname>Fallani</keyname><forenames>F. De Vico</forenames></author><author><keyname>Martinerie</keyname><forenames>J.</forenames></author><author><keyname>Witon</keyname><forenames>A.</forenames></author><author><keyname>Pouget</keyname><forenames>P.</forenames></author><author><keyname>Raux</keyname><forenames>M.</forenames></author><author><keyname>Similowski</keyname><forenames>T.</forenames></author><author><keyname>Chavez</keyname><forenames>M.</forenames></author></authors><title>Riemannian geometry applied to detection of respiratory states from EEG
  signals: the basis for a brain-ventilator interface</title><categories>cs.HC q-bio.NC</categories><comments>9 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During mechanical ventilation, patient-ventilator disharmony is frequently
observed and may result in increased breathing effort, compromising the
patient's comfort and recovery. This circumstance requires clinical
intervention and becomes challenging when patients are sedated or verbal
communication is difficult. In this work, we propose a brain computer interface
(BCI) to automatically and non-invasively detect patient-ventilator disharmony
from electroencephalographic (EEG) signals: a brain-ventilator interface (BVI).
Our framework exploits the cortical activation provoked by the inspiratory
compensation when the patient and the ventilator are desynchronized. Use of a
semi-supervised approach and Riemannian geometry of EEG covariance matrices
allows effective classification of respiratory state. The BVI is validated on
nine healthy subjects that performed different respiratory tasks that mimic a
patient-ventilator disharmony. Results evidence that performance, in terms of
areas under ROC curves (AUC), are significantly improved using EEG signals
(AUC=0.91) compared to traditional detection based on air flow (AUC=0.76).
Reduction in the number of electrodes that can achieve discrimination can often
be desirable (e.g. for portable BCI systems). By using an iterative channel
selection technique, the Common Highest Order Ranking (CHOrRa), we find that a
reduced set of electrodes (n=6) can slightly improve AUC to 0.95 for an
intra-subject configuration, and it still provides fairly good performances
(AUC $\geqslant$ 0.82) for a general inter-subject setting. In light of the
promising results, the proposed framework opens the door to brain-ventilator
interfaces for monitoring patient's breathing comfort and adapting ventilator
parameters to patient respiratory needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03027</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03027</id><created>2016-01-12</created><authors><author><keyname>Roland</keyname><forenames>Michael</forenames></author><author><keyname>H&#xf6;lzl</keyname><forenames>Michael</forenames></author></authors><title>Open Mobile API: Accessing the UICC on Android Devices</title><categories>cs.CR cs.OS</categories><comments>University of Applied Sciences Upper Austria, JR-Center u'smile,
  Technical report, 76 pages, 12 figures</comments><acm-class>C.2.0; C.3; C.5.3; D.2.7; D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report gives an overview of secure element integration into Android
devices. It focuses on the Open Mobile API as an open interface to access
secure elements from Android applications. The overall architecture of the Open
Mobile API is described and current Android devices are analyzed with regard to
the availability of this API. Moreover, this report summarizes our efforts of
reverse engineering the stock ROM of a Samsung Galaxy S3 in order to analyze
the integration of the Open Mobile API and the interface that is used to
perform APDU-based communication with the UICC (Universal Integrated Circuit
Card). It further provides a detailed explanation on how to integrate this
functionality into CyanogenMod (an after-market firmware for Android devices).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03030</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03030</id><created>2016-01-12</created><authors><author><keyname>Crosson</keyname><forenames>Elizabeth</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author></authors><title>Simulated Quantum Annealing Can Be Exponentially Faster than Classical
  Simulated Annealing</title><categories>quant-ph cond-mat.stat-mech cs.DS math.PR</categories><comments>18 pages, preliminary version</comments><report-no>MIT-CTP/4760</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulated Quantum Annealing (SQA) is a Markov Chain Monte-Carlo algorithm
that samples the equilibrium thermal state of a Quantum Annealing (QA)
Hamiltonian. In addition to simulating quantum systems, SQA has also been
proposed as another physics-inspired classical algorithm for combinatorial
optimization, alongside classical simulated annealing. However, in many cases
it remains an open challenge to determine the performance of both QA and SQA.
  One piece of evidence for the strength of QA over classical simulated
annealing comes from an example by Farhi, Goldstone and Gutmann . There a
bit-symmetric cost function with a thin, high energy barrier was designed to
show an exponential seperation between classical simulated annealing, for which
thermal fluctuations take exponential time to climb the barrier, and quantum
annealing which passes through the barrier and reaches the global minimum in
poly time, arguably by taking advantage of quantum tunneling. In this work we
apply a comparison method to rigorously show that the Markov chain underlying
SQA efficiently samples the target distribution and finds the global minimum of
this spike cost function in polynomial time.
  Our work provides evidence for the growing consensus that SQA inherits at
least some of the advantages of tunneling in QA, and so QA is unlikely to
achieve exponential speedups over classical computing solely by the use of
quantum tunneling. Since we analyze only a particular model this evidence is
not decisive. However, techniques applied here---including warm starts from the
adiabatic path and the use of the quantum ground state probability distribution
to understand the stationary distribution of SQA---may be valuable for future
studies of the performance of SQA on cost functions for which QA is efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03038</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03038</id><created>2016-01-12</created><authors><author><keyname>Duong</keyname><forenames>Phan Thi Ha</forenames></author></authors><title>Linear time algorithm for computing the rank of divisors on cactus
  graphs</title><categories>math.CO cs.DM cs.DS</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank of divisor on graph was introduced in 2007 and it quickly attracts many
attentions. Recently, in 2015 the problem for computing this quantity was
proved to be NP-hard. In this paper, we describe a linear time algorithm for
this problem limited on cactus graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03055</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03055</id><created>2016-01-12</created><updated>2016-02-29</updated><authors><author><keyname>Hou</keyname><forenames>Yuqing</forenames></author></authors><title>Image Annotation combining Subspace Clustering , Matrix Completion and
  Inhomogeneous Errors</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the experiment. Besides, the author will update some theoretical issues in
  the paper, too</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image annotation methods have greatly facilitated image management
applications. However, existing methods are still suffering from the
degradation of the missing and noisy tags provided by users. In this study, we
propose an image annotation method which performs tag completion and refinement
sequentially. We assume that images are sampled from a union of subspaces.
Images sampled from the same subspace, as well as their corresponding tags,
should form a compatible image-tag sub-matrix. Thus we segment the subspaces by
the Sparse Subspace Clustering (SSC) method and share tags in each subspace. A
novel matrix completion model is designed for tag refinement, taking visual-tag
correlation, semantic-tag correlation and the inhomogeneous errors property,
which is explored in this field for the first time, into consideration. We
exploit CNN features to improve the model. The proposed algorithm outperforms
state-of-the-art approaches when handling missing and noisy tags on multiple
benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03061</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03061</id><created>2016-01-12</created><authors><author><keyname>Kumar</keyname><forenames>Akshay</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>An Online Delay Efficient Multi-Class Packet Scheduler for Heterogeneous
  M2M Uplink Traffic</title><categories>cs.NI cs.IT math.IT</categories><comments>8 pages, 9 figures, Submitted to WiOpt 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Machine-to-Machine (M2M) traffic exhibit heterogeneity in several
dimensions such as packet arrival rate, delay tolerance requirements etc.
Therefore, motivated by this, we study the delay-performance of a heterogeneous
M2M uplink channel from the sensors to the Programmable Logic Controller (PLC)
in an industrial automation scenario. Besides the heterogeneity across the
sensors, the data from each sensor is also heterogeneous and can be classified
as either Event Driven (ED) or Periodic Update (PU) data. While the ED arrivals
are random and do not have hard service deadlines, the periodic PU data needs
to be processed before a predetermined deadline. We map these contrasting delay
requirements of PU and ED packets using step and sigmoidal functions of service
delay respectively. The aggregated traffic at PLC forms multiple queues/classes
based on the delay requirements, which are then served so as to maximize a
proportionally-fair system utility. Specifically the proposed multi-class
scheduler gives priority to ED data from different sensors as long as the
deadline is met for the PU data. We minimize successive PU failures using a
novel exponential penalty function and clear the failed PU packets so as to
reduce network congestion. Using extensive simulations, we compare the
performance of our scheme with various state-of-the art packet schedulers and
show that it outperforms other schedulers when network congestion increases. We
show that this performance gap increases with heterogeneity in delay
requirements and increasing penalization for successive PU failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03065</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03065</id><created>2016-01-08</created><authors><author><keyname>Subbotin</keyname><forenames>Igor Ya.</forenames></author><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author></authors><title>An Application of the Generalized Rectangular Fuzzy Model to Critical
  Thinking Assessment</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The authors apply the Generalized Rectangular Model to assessing critical
thinking skills and its relations with their language competency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03073</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03073</id><created>2016-01-12</created><authors><author><keyname>Reddy</keyname><forenames>Gautam</forenames></author><author><keyname>Celani</keyname><forenames>Antonio</forenames></author><author><keyname>Vergassola</keyname><forenames>Massimo</forenames></author></authors><title>Infomax strategies for an optimal balance between exploration and
  exploitation</title><categories>cs.LG cs.IT math.IT physics.data-an q-bio.PE stat.ML</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proper balance between exploitation and exploration is what makes good
decisions, which achieve high rewards like payoff or evolutionary fitness. The
Infomax principle postulates that maximization of information directs the
function of diverse systems, from living systems to artificial neural networks.
While specific applications are successful, the validity of information as a
proxy for reward remains unclear. Here, we consider the multi-armed bandit
decision problem, which features arms (slot-machines) of unknown probabilities
of success and a player trying to maximize cumulative payoff by choosing the
sequence of arms to play. We show that an Infomax strategy (Info-p) which
optimally gathers information on the highest mean reward among the arms
saturates known optimal bounds and compares favorably to existing policies. The
highest mean reward considered by Info-p is not the quantity actually needed
for the choice of the arm to play, yet it allows for optimal tradeoffs between
exploration and exploitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03080</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03080</id><created>2016-01-12</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hou</keyname><forenames>Qing-Hu</forenames></author><author><keyname>Labahn</keyname><forenames>George</forenames></author><author><keyname>Wang</keyname><forenames>Rong-Hua</forenames></author></authors><title>Existence Problem of Telescopers: Beyond the Bivariate Case</title><categories>cs.SC math.CO</categories><comments>19 pages</comments><msc-class>33F10</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we solve the existence problem of telescopers for rational
functions in three discrete variables. We reduce the problem to that of
deciding the summability of bivariate rational functions, which has been solved
recently. The existence criteria we present is needed for detecting the
termination of Zeilberger's algorithm to the function classes studied in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03083</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03083</id><created>2016-01-04</created><authors><author><keyname>Rolnick</keyname><forenames>David</forenames></author><author><keyname>Sober&#xf3;n</keyname><forenames>Pablo</forenames></author></authors><title>Algorithmic aspects of Tverberg's Theorem</title><categories>cs.CG math.CO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of finding Tverberg partitions within various
convexity spaces. For the classic geometric version of Tverberg's theorem, we
obtain probabilistic algorithms that are weakly polynomial in both the
dimension and the number of points. These algorithms extend to other
variations, such as the integer version of Tverberg's theorem. For geodetic
convexity on graphs, we show that the general problem of finding Radon
partitions is NP-hard, and present efficient algorithms for certain special
classes of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03085</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03085</id><created>2016-01-12</created><authors><author><keyname>Kang</keyname><forenames>Wang</forenames></author><author><keyname>Huang</keyname><forenames>Yangqi</forenames></author><author><keyname>Zhang</keyname><forenames>Xichao</forenames></author><author><keyname>Zhou</keyname><forenames>Yan</forenames></author><author><keyname>Lv</keyname><forenames>Weifeng</forenames></author><author><keyname>Zhao</keyname><forenames>Weisheng</forenames></author></authors><title>Skyrmions as Compact, Robust and Energy-Efficient Interconnects for
  Domain Wall (DW)-based Systems</title><categories>cond-mat.mes-hall cs.ET</categories><comments>3 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic domain-wall (DW) has been widely investigated for future memory and
computing systems. However, energy efficiency and stability become two major
challenges of DW-based systems. In this letter, we first propose exploiting
skyrmions as on-chip and inter-chip interconnects for DW-based systems, owing
to the topological stability, small size and ultra-low depinning current
density. In the proposed technique, data are processed in the form of DWs but
are transmitted instead in the form of skyrmions. The reversible conversion
between a skyrmion and a DW pair can be physically achieved by connecting a
wide and a narrow magnetic nanowire. Our proposed technique can realize highly
compact, robust and energy-efficient on-chip and inter-chip interconnects for
DW-based systems, enabling the system to take advantages of both the DW and
skyrmion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03094</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03094</id><created>2016-01-12</created><authors><author><keyname>Bento</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>A metric for sets of trajectories that is practical and mathematically
  consistent</title><categories>cs.CV cs.SY math.OC</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metrics on the space of sets of trajectories are important for scientists in
the field of computer vision, machine learning, robotics and general artificial
intelligence. Yet existing notions of closeness are either mathematically
inconsistent or of limited practical use. In this paper we outline the
limitations in the existing mathematically-consistent metrics, which are based
on Schuhmacher et al. 2008, and the inconsistencies in the heuristic notions of
closeness used in practice, whose main ideas are common to the CLEAR MOT
measures widely used in computer vision. In two steps we then propose a new
intuitive metric between sets of trajectories and address these problems. First
we explain a natural solution that leads to a metric that is hard to compute.
Then we modify this formulation to obtain a metric that is easy to compute and
keeps all the good properties of the previous metric. In particular, our notion
of closeness is the first that has the following three properties: it can be
quickly computed, it incorporates confusion of trajectories' identity in an
optimal way and it is a metric in the mathematical sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03095</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03095</id><created>2016-01-12</created><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Singer</keyname><forenames>Yaron</forenames></author></authors><title>Submodular Optimization under Noise</title><categories>cs.DS cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing monotone submodular functions under
noise, which to the best of our knowledge has not been studied in the past.
There has been a great deal of work on optimization of submodular functions
under various constraints, with many algorithms that provide desirable
approximation guarantees. However, in many applications we do not have access
to the submodular function we aim to optimize, but rather to some erroneous or
noisy version of it. This raises the question of whether provable guarantees
are obtainable in presence of error and noise. We provide initial answers, by
focusing on the question of maximizing a monotone submodular function under
cardinality constraints when given access to a noisy oracle of the function. We
show that:
  For a cardinality constraint $k \geq 2$, there is an approximation algorithm
whose approximation ratio is arbitrarily close to $1-1/e$;
  For $k=1$ there is an approximation algorithm whose approximation ratio is
arbitrarily close to $1/2$ in expectation. No randomized algorithm can obtain
an approximation ratio in expectation better than $1/2+O(1/\sqrt n)$ and $(2k -
1)/2k + O(1/\sqrt{n})$ for general $k$;
  If the noise is adversarial, no non-trivial approximation guarantee can be
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03115</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03115</id><created>2016-01-12</created><authors><author><keyname>Wu</keyname><forenames>Caesar</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Ramamohanarao</keyname><forenames>Kotagiri</forenames></author></authors><title>Big Data Analytics = Machine Learning + Cloud Computing</title><categories>cs.DC</categories><comments>27 pages, 23 figures. a Book Chapter in &quot;Big Data: Principles and
  Paradigms, R. Buyya, R. Calheiros, and A. Dastjerdi (eds), Morgan Kaufmann,
  Burlington, Massachusetts, USA, 2016.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data can mean different things to different people. The scale and
challenges of Big Data are often described using three attributes, namely
Volume, Velocity and Variety (3Vs), which only reflect some of the aspects of
data. In this chapter we review historical aspects of the term &quot;Big Data&quot; and
the associated analytics. We augment 3Vs with additional attributes of Big Data
to make it more comprehensive and relevant. We show that Big Data is not just
3Vs, but 32 Vs, that is, 9 Vs covering the fundamental motivation behind Big
Data, which is to incorporate Business Intelligence (BI) based on different
hypothesis or statistical models so that Big Data Analytics (BDA) can enable
decision makers to make useful predictions for some crucial decisions or
researching results. History of Big Data has demonstrated that the most cost
effective way of performing BDA is to employ Machine Learning (ML) on the Cloud
Computing (CC) based infrastructure or simply, ML + CC -&gt; BDA. This chapter is
devoted to help decision makers by defining BDA as a solution and opportunity
to address their business needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03116</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03116</id><created>2016-01-12</created><authors><author><keyname>Li</keyname><forenames>Muyuan</forenames></author><author><keyname>McArdle</keyname><forenames>Daniel E</forenames></author><author><keyname>Murphy</keyname><forenames>Jeffrey C</forenames></author><author><keyname>Shivkumar</keyname><forenames>Bhargav</forenames></author><author><keyname>Ziarek</keyname><forenames>Lukasz</forenames></author></authors><title>Adding Real-time Capabilities to a SML Compiler</title><categories>cs.PL</categories><comments>6 pages, 9 figures, ACM SIGBED</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much recent interest in adopting functional and reactive
programming for use in real-time system design. Moving toward a more
declarative methodology for developing real-time systems purports to improve
the fidelity of software. To study the benefits of functional and reactive
programming for real-time systems, real-time aware functional compilers and
language runtimes are required. In this paper we examine the necessary changes
to a modern Standard ML compiler, MLton, to provide basic support for real-time
execution. We detail our current progress in modifying MLton with a threading
model that supports priorities, a chunked object model to support real-time
garbage collection, and low level modification to execute on top of a real-time
operating system. We present preliminary numbers and our work in progress
prototype, which is able to boot ML programs compiled with MLton on x86
machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03117</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03117</id><created>2016-01-12</created><authors><author><keyname>Zhu</keyname><forenames>Fengyuan</forenames></author><author><keyname>Chen</keyname><forenames>Guangyong</forenames></author><author><keyname>Hao</keyname><forenames>Jianye</forenames></author><author><keyname>Heng</keyname><forenames>Pheng-Ann</forenames></author></authors><title>Blind Image Denoising via Dependent Dirichlet Process Tree</title><categories>cs.CV stat.ML</categories><comments>25 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing image denoising approaches assumed the noise to be homogeneous
white Gaussian distributed with known intensity. However, in real noisy images,
the noise models are usually unknown beforehand and can be much more complex.
This paper addresses this problem and proposes a novel blind image denoising
algorithm to recover the clean image from noisy one with the unknown noise
model. To model the empirical noise of an image, our method introduces the
mixture of Gaussian distribution, which is flexible enough to approximate
different continuous distributions. The problem of blind image denoising is
reformulated as a learning problem. The procedure is to first build a two-layer
structural model for noisy patches and consider the clean ones as latent
variable. To control the complexity of the noisy patch model, this work
proposes a novel Bayesian nonparametric prior called &quot;Dependent Dirichlet
Process Tree&quot; to build the model. Then, this study derives a variational
inference algorithm to estimate model parameters and recover clean patches. We
apply our method on synthesis and real noisy images with different noise
models. Comparing with previous approaches, ours achieves better performance.
The experimental results indicate the efficiency of the proposed algorithm to
cope with practical image denoising tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03118</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03118</id><created>2016-01-12</created><authors><author><keyname>Yuan</keyname><forenames>Weijie</forenames></author><author><keyname>Wu</keyname><forenames>Nan</forenames></author><author><keyname>Etzlinger</keyname><forenames>Bernhard</forenames></author><author><keyname>Wang</keyname><forenames>Hua</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author></authors><title>Cooperative Joint Localization and Clock Synchronization Based on
  Gaussian Message Passing in Asynchronous Wireless Networks</title><categories>cs.IT math.IT</categories><comments>38 pages one column, To appear in IEEE Transactions on Vehicular
  Technology</comments><doi>10.1109/TVT.2016.2518185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization and synchronization are very important in many wireless
applications such as monitoring and vehicle tracking. Utilizing the same time
of arrival (TOA) measurements for simultaneous localization and synchronization
is challenging. In this paper, we present a factor graph (FG) representation of
the joint localization and time synchronization problem based on TOA
measurements, in which the non-line-of-sight measurements are also taken into
consideration. On this FG, belief propagation (BP) message passing and
variational message passing (VMP) are applied to derive two fully distributed
cooperative algorithms with low computational requirements. Due to the
nonlinearity in the observation function, it is intractable to compute the
messages in closed form and most existing solutions rely on Monte Carlo
methods, e.g., particle filtering. We linearize a specific nonlinear term in
the expressions of messages, which enables us to use a Gaussian representation
for all messages. Accordingly, only the mean and variance have to be updated
and transmitted between neighboring nodes, which significantly reduces the
communication overhead and computational complexity. A message passing schedule
scheme is proposed to trade off between estimation performance and
communication overhead. Simulation results show that the proposed algorithms
perform very close to particle-based methods with much lower complexity
especially in densely connected networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03121</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03121</id><created>2016-01-12</created><authors><author><keyname>Asadi</keyname><forenames>Behzad</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>A Unified Inner Bound for the Two-Receiver Memoryless Broadcast Channel
  with Channel State and Message Side Information</title><categories>cs.IT math.IT</categories><comments>Extended version of the same-titled paper submitted to the 2016 IEEE
  International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the two-receiver memoryless broadcast channel with states where
each receiver requests both common and private messages, and may know part of
the private message requested by the other receiver as receiver message side
information (RMSI). We address two categories of the channel (i) channel with
states known causally to the transmitter, and (ii) channel with states known
non-causally to the transmitter. Starting with the channel without RMSI, we
first propose a transmission scheme and derive an inner bound for the causal
category. We then unify our inner bound for the causal category and the
best-known inner bound for the non-causal category, although their transmission
schemes are different. Moving on to the channel with RMSI, we first apply a
pre-coding to the transmission schemes of the causal and non-causal categories
without RMSI. We then derive a unified inner bound as a result of having a
unified inner bound when there is no RMSI, and applying the same pre-coding to
both categories. We show that our inner bound is tight for some new cases as
well as the cases whose capacity region was known previously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03124</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03124</id><created>2016-01-12</created><authors><author><keyname>Chen</keyname><forenames>Guangyong</forenames></author><author><keyname>Zhu</keyname><forenames>Fengyuan</forenames></author><author><keyname>Heng</keyname><forenames>Pheng Ann</forenames></author></authors><title>Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization</title><categories>cs.LG stat.ML</categories><comments>26 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dyadic Data Prediction (DDP) is an important problem in many research areas.
This paper develops a novel fully Bayesian nonparametric framework which
integrates two popular and complementary approaches, discrete mixed membership
modeling and continuous latent factor modeling into a unified Heterogeneous
Matrix Factorization~(HeMF) model, which can predict the unobserved dyadics
accurately. The HeMF can determine the number of communities automatically and
exploit the latent linear structure for each bicluster efficiently. We propose
a Variational Bayesian method to estimate the parameters and missing data. We
further develop a novel online learning approach for Variational inference and
use it for the online learning of HeMF, which can efficiently cope with the
important large-scale DDP problem. We evaluate the performance of our method on
the EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.
The experiment shows that, our model outperforms state-of-the-art methods on
all benchmarks. Compared with Stochastic Gradient Method (SGD), our online
learning approach achieves significant improvement on the estimation accuracy
and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03128</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03128</id><created>2016-01-12</created><authors><author><keyname>Mishra</keyname><forenames>Anand</forenames></author><author><keyname>Alahari</keyname><forenames>Karteek</forenames></author><author><keyname>Jawahar</keyname><forenames>C. V.</forenames></author></authors><title>Enhancing Energy Minimization Framework for Scene Text Recognition with
  Top-Down Cues</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing scene text is a challenging problem, even more so than the
recognition of scanned documents. This problem has gained significant attention
from the computer vision community in recent years, and several methods based
on energy minimization frameworks and deep learning approaches have been
proposed. In this work, we focus on the energy minimization framework and
propose a model that exploits both bottom-up and top-down cues for recognizing
cropped words extracted from street images. The bottom-up cues are derived from
individual character detections from an image. We build a conditional random
field model on these detections to jointly model the strength of the detections
and the interactions between them. These interactions are top-down cues
obtained from a lexicon-based prior, i.e., language statistics. The optimal
word represented by the text image is obtained by minimizing the energy
function corresponding to the random field model. We evaluate our proposed
algorithm extensively on a number of cropped scene text benchmark datasets,
namely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word,
and show better performance than comparable methods. We perform a rigorous
analysis of all the steps in our approach and analyze the results. We also show
that state-of-the-art convolutional neural network features can be integrated
in our framework to further improve the recognition performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03141</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03141</id><created>2016-01-13</created><authors><author><keyname>Ketseoglou</keyname><forenames>Thomas</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Linear Precoding for MIMO Channels with QAM Constellations and Reduced
  Complexity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of designing a linear precoder for Multiple-Input
Multiple-Output (MIMO) systems in conjunction with Quadrature Amplitude
Modulation (QAM) is addressed. First, a novel and efficient methodology to
evaluate the input-output mutual information for a general Multiple-Input
Multiple-Output (MIMO) system as well as its corresponding gradients is
presented, based on the Gauss-Hermite quadrature rule. Then, the method is
exploited in a block coordinate gradient ascent optimization process to
determine the globally optimal linear precoder with respect to the MIMO
input-output mutual information for QAM systems with relatively moderate MIMO
channel sizes. The proposed methodology is next applied in conjunction with the
complexity-reducing per-group processing (PGP) technique, which is
semi-optimal, to both perfect channel state information at the transmitter
(CSIT) as well as statistical channel state information (SCSI) scenarios, with
high transmitting and receiving antenna size, and for constellation size up to
$M=64$. We show by numerical results that the precoders developed offer
significantly better performance than the configuration with no precoder, and
the maximum diversity precoder for QAM with constellation sizes $M=16,~32$, and
$~64$ and for MIMO channel size $100\times100$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03144</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03144</id><created>2016-01-13</created><authors><author><keyname>Merry</keyname><forenames>Bruce</forenames></author></authors><title>A Performance Comparison of Sort and Scan Libraries for GPUs</title><categories>cs.DC</categories><comments>Preprint of an article published in Parallel Processing Letters, 25,
  4, 2015, 1550007 DOI: 10.1142/S0129626415500073 \copyright\ copyright World
  Scientific Publishing Company
  &lt;http://www.worldscientific.com/worldscinet/ppl&gt;</comments><journal-ref>Parallel Processing Letters, 25(4), 2015, p. 1550007</journal-ref><doi>10.1142/S0129626415500073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sorting and scanning are two fundamental primitives for constructing highly
parallel algorithms. A number of libraries now provide implementations of these
primitives for GPUs, but there is relatively little information about the
performance of these implementations.
  We benchmark seven libraries for 32-bit integer scan and sort, and sorting
32-bit values by 32-bit integer keys. We show that there is a large variation
in performance between the libraries, and that no one library has both optimal
performance and portability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03147</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03147</id><created>2016-01-13</created><authors><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Khonji</keyname><forenames>Majid</forenames></author><author><keyname>Aftab</keyname><forenames>Muhammad</forenames></author></authors><title>Online Algorithms for Information Aggregation from Distributed and
  Correlated Sources</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a fundamental trade-off between the communication cost and latency
in information aggregation. Aggregating multiple communication messages over
time can alleviate overhead and improve energy efficiency on one hand, but
inevitably incurs information delay on the other hand. In the presence of
uncertain future inputs, this trade-off should be balanced in an online manner,
which is studied by the classical dynamic TCP ACK problem for a single
information source. In this paper, we extend dynamic TCP ACK problem to a
general setting of collecting aggregate information from distributed and
correlated information sources. In this model, distributed sources observe
correlated events, whereas only a small number of reports are required from the
sources. The sources make online decisions about their reporting operations in
a distributed manner without prior knowledge of the local observations at
others. Our problem captures a wide range of applications, such as sensor
networks, anycast acknowledgement and distributed caching. We present simple
threshold-based competitive distributed online algorithms under different
settings of intercommunication. Our algorithms match the theoretical lower
bounds in order of magnitude. We observe that our algorithms can produce
satisfactory performance in simulations and practical testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03162</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03162</id><created>2016-01-13</created><authors><author><keyname>Avramopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Jump-starting coordination in a stag hunt: Motivation, mechanisms, and
  their analysis</title><categories>cs.GT cs.NI</categories><comments>Some overlap with http://arxiv.org/abs/1210.7789</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stag hunt (or assurance game) is a simple game that has been used as a
prototype of a variety of social coordination problems (ranging from the social
contract to the adoption of technical standards). Players have the option to
either use a superior cooperative strategy whose payoff depends on the other
players' choices or use an inferior strategy whose payoff is independent of
what other players do; the cooperative strategy may incur a loss if
sufficiently many other players do not cooperate. Stag hunts have two (strict)
pure Nash equilibria, namely, universal cooperation and universal defection (as
well as a mixed equilibrium of low predictive value). Selection of the inferior
(pure) equilibrium is called a coordination failure. In this paper, we present
and analyze using game-theoretic techniques mechanisms aiming to avert
coordination failures and incite instead selection of the superior equilibrium.
Our analysis is based on the solution concepts of Nash equilibrium, dominance
solvability, as well as a formalization of the notion of &quot;incremental
deployability,&quot; which is shown to be keenly relevant to the sink equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03164</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03164</id><created>2016-01-13</created><authors><author><keyname>Martin-Vega</keyname><forenames>Francisco J.</forenames></author><author><keyname>Gomez</keyname><forenames>Gerardo</forenames></author><author><keyname>Aguayo-Torres</keyname><forenames>Mari Carmen</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author></authors><title>Analytical Modeling of Interference Aware Power Control for the Uplink
  of Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><comments>13 pages, 1 table and 7 figures. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-cell interference is one of the main limiting factors in current
Heterogeneous Cellular Networks (HCNs). Uplink Fractional Power Control (FPC)
is a well known method that aims to cope with such limiting factor as well as
to save battery live. In order to do that, the path losses associated with
Mobile Terminal (MT) transmissions are partially compensated so that a lower
interference is leaked towards neighboring cells. Classical FPC techniques only
consider a set of parameters that depends on the own MT transmission, like
desired received power at the Base Station (BS) or the path loss between the MT
and its serving BS, among others. Contrary to classical FPC, in this paper we
use stochastic geometry to analyze a power control mechanism that keeps the
interference generated by each MT under a given threshold. We also consider a
maximum transmitted power and a partial compensation of the path loss.
Interestingly, our analysis reveals that such Interference Aware (IA) method
can reduce the average power consumption and increase the average spectral
efficiency. Additionally, the variance of the interference is reduced, thus
improving the performance of Adaptive Modulation and Coding (AMC) since the
interference can be better estimated at the MT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03174</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03174</id><created>2016-01-13</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author></authors><title>Graph Editing to a Given Degree Sequence</title><categories>cs.DS cs.CC cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:1311.4768</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the parameterized complexity of the graph editing problem
called Editing to a Graph with a Given Degree Sequence, where the aim is to
obtain a graph with a given degree sequence \sigma by at most k vertex or edge
deletions and edge additions. We show that the problem is W[1]-hard when
parameterized by k for any combination of the allowed editing operations. From
the positive side, we show that the problem can be solved in time
2^{O(k(\Delta+k)^2)}n^2 log n for n-vertex graphs, where \Delta=max \sigma,
i.e., the problem is FPT when parameterized by k+\Delta. We also show that
Editing to a Graph with a Given Degree Sequence has a polynomial kernel when
parameterized by k+\Delta if only edge additions are allowed, and there is no
polynomial kernel unless NP\subseteq coNP/poly for all other combinations of
allowed editing operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03192</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03192</id><created>2016-01-13</created><authors><author><keyname>Gagl</keyname><forenames>Benjamin</forenames></author></authors><title>Blue hypertext is a perfect design decision: No perceptual disadvantage
  in reading and successful highlighting of relevant information</title><categories>cs.HC q-bio.NC</categories><comments>15 pages, 1 figure, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Highlighted text in the Internet (i.e. Hypertext) is predominantly blue and
underlined. The percept of these hypertext characteristics were heavily
questioned by applied research and empirical tests resulted in inconclusive
results. The ability to identify blue text in foveal and parafoveal vision was
identified as potentially constrained by the low number of foveally centered
blue light sensitive retinal cells. The present study investigates if foveal
and parafoveal perceptibility of hypertext is reduced during reading. A
silent-sentence reading study with simultaneous eye movement recordings and the
invisible boundary paradigm, which allows the investigation of foveal and
parafoveal perceptibility, separately, was realized. Target words in sentences
were presented in either black or blue and either underlined or normal. No
effect of color and underlining, but a preview benefit could be detected for
first pass reading measures (comparing fixation times after degraded vs. un-
degraded parafoveal previews). Fixation time measures that included re-reading
(i.e., total viewing times) showed, in addition to a preview effect, a reduced
fixation time for not highlighted (black not underlined) in contrast to
highlighted target words (either blue or underlined or both). Thus, the present
pattern reflects no detectable perceptual disadvantage of hyperlink stimuli but
increased attraction of attention resources, after first pass reading, through
highlighting. Blue or underlined text allows readers to easily perceive
hypertext and at the same time readers re-visited hypertext longer as a
consequence of highlighting. On the basis of the present evidence blue
hypertext can be safely recommended to web designers for future use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03195</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03195</id><created>2016-01-13</created><authors><author><keyname>Molinari</keyname><forenames>A.</forenames></author><author><keyname>Montanari</keyname><forenames>A.</forenames></author><author><keyname>Murano</keyname><forenames>A.</forenames></author><author><keyname>Perelli</keyname><forenames>G.</forenames></author><author><keyname>Peron</keyname><forenames>A.</forenames></author></authors><title>Checking Interval Properties of Computations</title><categories>cs.LO cs.CC</categories><comments>In Journal: Acta Informatica, Springer Berlin Heidelber, 2015</comments><doi>10.1007/s00236-015-0250-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking is a powerful method widely explored in formal verification.
Given a model of a system, e.g., a Kripke structure, and a formula specifying
its expected behaviour, one can verify whether the system meets the behaviour
by checking the formula against the model.
  Classically, system behaviour is expressed by a formula of a temporal logic,
such as LTL and the like. These logics are &quot;point-wise&quot; interpreted, as they
describe how the system evolves state-by-state. However, there are relevant
properties, such as those constraining the temporal relations between pairs of
temporally extended events or involving temporal aggregations, which are
inherently &quot;interval-based&quot;, and thus asking for an interval temporal logic.
  In this paper, we give a formalization of the model checking problem in an
interval logic setting. First, we provide an interpretation of formulas of
Halpern and Shoham's interval temporal logic HS over finite Kripke structures,
which allows one to check interval properties of computations. Then, we prove
that the model checking problem for HS against finite Kripke structures is
decidable by a suitable small model theorem, and we provide a lower bound to
its computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03201</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03201</id><created>2016-01-13</created><authors><author><keyname>Szab&#xf3;</keyname><forenames>D&#xe1;vid</forenames></author><author><keyname>Csoma</keyname><forenames>Attila</forenames></author><author><keyname>Megyesi</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Guly&#xe1;s</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Fitzek</keyname><forenames>Frank H. P.</forenames></author></authors><title>Network Coding as a Service</title><categories>cs.NI cs.IT cs.PF math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Coding (NC) shows great potential in various communication scenarios
through changing the packet forwarding principles of current networks. It can
improve not only throughput, latency, reliability and security but also
alleviates the need of coordination in many cases. However, it is still
controversial due to widespread misunderstandings on how to exploit the
advantages of it. The aim of the paper is to facilitate the usage of NC by
$(i)$ explaining how it can improve the performance of the network (regardless
the existence of any butterfly in the network), $(ii)$ showing how Software
Defined Networking (SDN) can resolve the crucial problems of deployment and
orchestration of NC elements, and $(iii)$ providing a prototype architecture
with measurement results on the performance of our network coding capable
software router implementation compared by fountain codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03202</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03202</id><created>2016-01-13</created><authors><author><keyname>Molinari</keyname><forenames>A.</forenames></author><author><keyname>Montanari</keyname><forenames>A.</forenames></author><author><keyname>Peron</keyname><forenames>A.</forenames></author></authors><title>Complexity of ITL model checking: some well-behaved fragments of the
  interval logic HS</title><categories>cs.LO cs.CC cs.DS</categories><journal-ref>In proceedings of 22nd TIME, Pages 90-100, 2015 IEEE</journal-ref><doi>10.1109/TIME.2015.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking has been successfully used in many computer science fields,
including artificial intelligence, theoretical computer science, and databases.
Most of the proposed solutions make use of classical, point-based temporal
logics, while little work has been done in the interval temporal logic setting.
Recently, a non-elementary model checking algorithm for Halpern and Shoham's
modal logic of time intervals HS over finite Kripke structures (under the
homogeneity assumption) and an EXPSPACE model checking procedure for two
meaningful fragments of it have been proposed. In this paper, we show that more
efficient model checking procedures can be developed for some expressive enough
fragments of HS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03204</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03204</id><created>2016-01-13</created><authors><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Chakraborty</keyname><forenames>Abhishek</forenames></author><author><keyname>Manoj</keyname><forenames>B. S.</forenames></author></authors><title>Graph Fourier Transform based on Directed Laplacian</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we redefine the Graph Fourier Transform (GFT) under the
DSP$_\mathrm{G}$ framework. We consider the Jordan eigenvectors of the directed
Laplacian as graph harmonics and the corresponding eigenvalues as the graph
frequencies. For this purpose, we propose a shift operator based on the
directed Laplacian of a graph. Based on our shift operator, we then define
total variation of graph signals, which is used in frequency ordering. We
achieve natural frequency ordering and interpretation via the proposed
definition of GFT. Moreover, we show that our proposed shift operator makes the
LSI filters under DSP$_\mathrm{G}$ to become polynomial in the directed
Laplacian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03206</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03206</id><created>2016-01-13</created><authors><author><keyname>Kop</keyname><forenames>Cynthia</forenames></author></authors><title>Termination of LCTRSs</title><categories>cs.LO</categories><comments>proceedings for WST 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logically Constrained Term Rewriting Systems (LCTRSs) provide a general
framework for term rewriting with constraints. We discuss a simple dependency
pair approach to prove termination of LCTRSs. We see that existing techniques
transfer to the constrained setting in a natural way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03210</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03210</id><created>2016-01-13</created><authors><author><keyname>G&#xf3;mez-Rodr&#xed;guez</keyname><forenames>Carlos</forenames></author><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>The scarcity of crossing dependencies: a direct outcome of a specific
  constraint?</title><categories>cs.CL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crossing syntactic dependencies have been observed to be infrequent in
natural language, to the point that some syntactic theories and formalisms
disregard them entirely. This leads to the question of whether the scarcity of
crossings in languages arises from an independent and specific constraint on
crossings. We provide statistical evidence suggesting that this is not the
case, as the proportion of dependency crossings in a wide range of natural
language treebanks can be accurately estimated by a simple predictor based on
the local probability that two dependencies cross given their lengths. The
relative error of this predictor never exceeds 5% on average, whereas a
baseline predictor assuming a random ordering of the words of a sentence incurs
a relative error that is at least 6 times greater. Our results suggest that the
low frequency of crossings in natural languages is neither originated by hidden
knowledge of language nor by the undesirability of crossings per se, but as a
mere side effect of the principle of dependency length minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03214</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03214</id><created>2016-01-13</created><authors><author><keyname>Nagaraj</keyname><forenames>Nithin</forenames></author><author><keyname>Sahasranand</keyname><forenames>K. R.</forenames></author></authors><title>Neural Signal Multiplexing via Compressed Sensing</title><categories>cs.IT math.IT nlin.CD q-bio.NC</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transport of neural signals in the brain is challenging, owing to neural
interference and neural noise. There is experimental evidence of multiplexing
of sensory information across population of neurons, particularly in the
vertebrate visual and olfactory systems. Recently, it has been discovered that
in lateral intraparietal cortex of the brain, decision signals are multiplexed
with decision-irrelevant visual signals. Furthermore, it is well known that
several cortical neurons exhibit chaotic spiking patterns. Multiplexing of
chaotic neural signals and their successful demultiplexing in the neurons
amidst interference and noise, is difficult to explain. In this work, a novel
compressed sensing model for efficient multiplexing of chaotic neural signals
constructed using the Hindmarsh-Rose spiking model is proposed. The signals are
multiplexed from a pre-synaptic neuron to its neighbouring post-synaptic
neuron, in the presence of $10^4$ interfering noisy neural signals and
demultiplexed using compressed sensing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03224</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03224</id><created>2016-01-13</created><authors><author><keyname>Canton</keyname><forenames>A.</forenames></author><author><keyname>Fernandez-Jambrina</keyname><forenames>L.</forenames></author><author><keyname>Maria</keyname><forenames>E. Rosado</forenames></author><author><keyname>Vazquez-Gallo</keyname><forenames>M. J.</forenames></author></authors><title>Implicit equations of non-degenerate rational Bezier quadric triangles</title><categories>cs.GR math.NA</categories><comments>10 pages, 8th International Conference Curves and Surfaces, Paris,
  France, June 12-18, 2014</comments><msc-class>65D17</msc-class><journal-ref>Lecture Notes in Computer Science 9213, 70-79 (2015)</journal-ref><doi>10.1007/978-3-319-22804-4_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we review the derivation of implicit equations for
non-degenerate quadric patches in rational Bezier triangular form. These are
the case of Steiner surfaces of degree two. We derive the bilinear forms for
such quadrics in a coordinate-free fashion in terms of their control net and
their list of weights in a suitable form. Our construction relies on projective
geometry and is grounded on the pencil of quadrics circumscribed to a
tetrahedron formed by vertices of the control net and an additional point which
is required for the Steiner surface to be a non-degenerate quadric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03225</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03225</id><created>2016-01-13</created><authors><author><keyname>Frey</keyname><forenames>Davide</forenames></author><author><keyname>Lakhlef</keyname><forenames>Hicham</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author></authors><title>Optimal Collision/Conflict-free Distance-2 Coloring in Synchronous
  Broadcast/Receive Tree Networks</title><categories>cs.DC cs.NI</categories><comments>19 pages including one appendix. One Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is on message-passing systems where communication is (a)
synchronous and (b) based on the &quot;broadcast/receive&quot; pair of communication
operations. &quot;Synchronous&quot; means that time is discrete and appears as a sequence
of time slots (or rounds) such that each message is received in the very same
round in which it is sent. &quot;Broadcast/receive&quot; means that during a round a
process can either broadcast a message to its neighbors or receive a message
from one of them. In such a communication model, no two neighbors of the same
process, nor a process and any of its neighbors, must be allowed to broadcast
during the same time slot (thereby preventing message collisions in the first
case, and message conflicts in the second case). From a graph theory point of
view, the allocation of slots to processes is know as the distance-2 coloring
problem: a color must be associated with each process (defining the time slots
in which it will be allowed to broadcast) in such a way that any two processes
at distance at most 2 obtain different colors, while the total number of colors
is &quot;as small as possible&quot;. The paper presents a parallel message-passing
distance-2 coloring algorithm suited to trees, whose roots are dynamically
defined. This algorithm, which is itself collision-free and conflict-free, uses
$\Delta + 1$ colors where $\Delta$ is the maximal degree of the graph (hence
the algorithm is color-optimal). It does not require all processes to have
different initial identities, and its time complexity is $O(d \Delta)$, where d
is the depth of the tree. As far as we know, this is the first distributed
distance-2 coloring algorithm designed for the broadcast/receive round-based
communication model, which owns all the previous properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03229</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03229</id><created>2016-01-13</created><updated>2016-01-14</updated><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Xiao</keyname><forenames>Xiaokui</forenames></author><author><keyname>Xie</keyname><forenames>Xing</forenames></author></authors><title>PrivTree: A Differentially Private Algorithm for Hierarchical
  Decompositions</title><categories>cs.DB</categories><comments>A short version of this paper will appear in SIGMOD 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set D of tuples defined on a domain Omega, we study differentially
private algorithms for constructing a histogram over Omega to approximate the
tuple distribution in D. Existing solutions for the problem mostly adopt a
hierarchical decomposition approach, which recursively splits Omega into
sub-domains and computes a noisy tuple count for each sub-domain, until all
noisy counts are below a certain threshold. This approach, however, requires
that we (i) impose a limit h on the recursion depth in the splitting of Omega
and (ii) set the noise in each count to be proportional to h. This leads to
inferior data utility due to the following dilemma: if we use a small h, then
the resulting histogram would be too coarse-grained to provide an accurate
approximation of data distribution; meanwhile, a large h would yield a
fine-grained histogram, but its quality would be severely degraded by the
increased amount of noise in the tuple counts.
  To remedy the deficiency of existing solutions, we present PrivTree, a
histogram construction algorithm that also applies hierarchical decomposition
but features a crucial (and somewhat surprising) improvement: when deciding
whether or not to split a sub-domain, the amount of noise required in the
corresponding tuple count is independent of the recursive depth. This enables
PrivTree to adaptively generate high-quality histograms without even asking for
a pre-defined threshold on the depth of sub-domain splitting. As concrete
examples, we demonstrate an application of PrivTree in modelling spatial data,
and show that it can also be extended to handle sequence data (where the
decision in sub-domain splitting is not based on tuple counts but a more
sophisticated measure). Our experiments on a variety of real datasets show that
PrivTree significantly outperforms the states of the art in terms of data
utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03230</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03230</id><created>2016-01-13</created><authors><author><keyname>Kumar</keyname><forenames>Pawan</forenames></author></authors><title>An Optimal Block Diagonal Preconditioner for Heterogeneous Saddle Point
  Problems in Phase Separation</title><categories>cs.NA</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phase separation processes are typically modeled by Cahn-Hilliard
equations. This equation was originally introduced to model phase separation in
binary alloys, where phase stands for concentration of different components in
alloy. When the binary alloy under preparation is subjected to a rapid
reduction in temperature below a critical temperature, it has been
experimentally observed that the concentration changes from a mixed state to a
visibly distinct spatially separated two phase for binary alloy. This rapid
reduction in the temperature, the so-called &quot;deep quench limit&quot;, is modeled
effectively by obstacle potential. The discretization of Cahn-Hilliard equation
with obstacle potential leads to a block $2 \times 2$ {\em non-linear} system,
where the $(1,1)$ block has a non-linear and non-smooth term. Recently a
globally convergent Newton Schur method was proposed for the non-linear Schur
complement corresponding to this non-linear system. The proposed method is
similar to an inexact active set method in the sense that the active sets are
first approximately identified by solving a quadratic obstacle problem
corresponding to the $(1,1)$ block of the block $2 \times 2$ system, and later
solving a reduced linear system by annihilating the rows and columns
corresponding to identified active sets. For solving the quadratic obstacle
problem, various optimal multigrid like methods have been proposed. In this
paper, we study a non-standard norm that is equivalent to applying a block
diagonal preconditioner to the reduced linear systems. Numerical experiments
confirm the optimality of the solver and convergence independent of problem
parameters on sufficiently fine mesh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03239</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03239</id><created>2016-01-13</created><authors><author><keyname>Schetinger</keyname><forenames>Victor</forenames></author><author><keyname>Iuliani</keyname><forenames>Massimo</forenames></author><author><keyname>Piva</keyname><forenames>Alessandro</forenames></author><author><keyname>Oliveira</keyname><forenames>Manuel M.</forenames></author></authors><title>Digital Image Forensics vs. Image Composition: An Indirect Arms Race</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of image composition is constantly trying to improve the ways in
which an image can be altered and enhanced. While this is usually done in the
name of aesthetics and practicality, it also provides tools that can be used to
maliciously alter images. In this sense, the field of digital image forensics
has to be prepared to deal with the influx of new technology, in a constant
arms-race. In this paper, the current state of this arms-race is analyzed,
surveying the state-of-the-art and providing means to compare both sides. A
novel scale to classify image forensics assessments is proposed, and
experiments are performed to test composition techniques in regards to
different forensics traces. We show that even though research in forensics
seems unaware of the advanced forms of image composition, it possesses the
basic tools to detect it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03240</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03240</id><created>2016-01-13</created><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>Counting Answers to Existential Positive Queries: A Complexity
  Classification</title><categories>cs.DB cs.CC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1501.07195</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existential positive formulas form a fragment of first-order logic that
includes and is semantically equivalent to unions of conjunctive queries, one
of the most important and well-studied classes of queries in database theory.
We consider the complexity of counting the number of answers to existential
positive formulas on finite structures and give a trichotomy theorem on query
classes, in the setting of bounded arity. This theorem generalizes and unifies
several known results on the complexity of conjunctive queries and unions of
conjunctive queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03249</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03249</id><created>2015-12-23</created><authors><author><keyname>L&#xf6;ber</keyname><forenames>Jakob</forenames></author></authors><title>Optimal trajectory tracking</title><categories>math.OC cs.SY</categories><comments>240 pages, 36 figures, PhD thesis</comments><doi>10.14279/depositonce-4926</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis investigates optimal trajectory tracking of nonlinear dynamical
systems with affine controls. The control task is to enforce the system state
to follow a prescribed desired trajectory as closely as possible. The concept
of so-called exactly realizable trajectories is proposed. For exactly
realizable desired trajectories exists a control signal which enforces the
state to exactly follow the desired trajectory. For a given affine control
system, these trajectories are characterized by the so-called constraint
equation. This approach does not only yield an explicit expression for the
control signal in terms of the desired trajectory, but also identifies a
particularly simple class of nonlinear control systems. Based on that insight,
the regularization parameter is used as the small parameter for a perturbation
expansion. This results in a reinterpretation of affine optimal control
problems with small regularization term as singularly perturbed differential
equations. The small parameter originates from the formulation of the control
problem and does not involve simplifying assumptions about the system dynamics.
Combining this approach with the linearizing assumption, approximate and partly
linear equations for the optimal trajectory tracking of arbitrary desired
trajectories are derived. For vanishing regularization parameter, the state
trajectory becomes discontinuous and the control signal diverges. On the other
hand, the analytical treatment becomes exact and the solutions are exclusively
governed by linear differential equations. Thus, the possibility of linear
structures underlying nonlinear optimal control is revealed. This fact enables
the derivation of exact analytical solutions to an entire class of nonlinear
trajectory tracking problems with affine controls. This class comprises
mechanical control systems in one spatial dimension and the FitzHugh-Nagumo
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03271</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03271</id><created>2016-01-13</created><authors><author><keyname>Viso</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Bonelli</keyname><forenames>Eduardo</forenames></author><author><keyname>Ayala-Rinc&#xf3;n</keyname><forenames>Mauricio</forenames></author></authors><title>Type Soundness for Path Polymorphism</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path polymorphism is the ability to define functions that can operate
uniformly over arbitrary recursively specified data structures. Its essence is
captured by patterns of the form $x\,y$ which decompose a compound data
structure into its parts. Typing these kinds of patterns is challenging since
the type of a compound should determine the type of its components. We propose
a static type system (i.e. no run-time analysis) for a pattern calculus that
captures this feature. Our solution combines type application, constants as
types, union types and recursive types. We address the fundamental properties
of Subject Reduction and Progress that guarantee a well-behaved dynamics. Both
these results rely crucially on a notion of pattern compatibility and also on a
coinductive characterisation of subtyping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03277</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03277</id><created>2016-01-12</created><authors><author><keyname>da Silva</keyname><forenames>Adenilton J.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Wilson R.</forenames></author><author><keyname>Ludermir</keyname><forenames>Teresa B.</forenames></author></authors><title>Weightless neural network parameters and architecture selection in a
  quantum computer</title><categories>quant-ph cs.NE</categories><journal-ref>Neurocomputing, Volume 183, 26 March 2016, Pages 13-22</journal-ref><doi>10.1016/j.neucom.2015.05.139</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training artificial neural networks requires a tedious empirical evaluation
to determine a suitable neural network architecture. To avoid this empirical
process several techniques have been proposed to automatise the architecture
selection process. In this paper, we propose a method to perform parameter and
architecture selection for a quantum weightless neural network (qWNN). The
architecture selection is performed through the learning procedure of a qWNN
with a learning algorithm that uses the principle of quantum superposition and
a non-linear quantum operator. The main advantage of the proposed method is
that it performs a global search in the space of qWNN architecture and
parameters rather than a local search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03278</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03278</id><created>2016-01-13</created><authors><author><keyname>Yang</keyname><forenames>Longqi</forenames></author><author><keyname>Freed</keyname><forenames>Diana</forenames></author><author><keyname>Wu</keyname><forenames>Alex</forenames></author><author><keyname>Wu</keyname><forenames>Judy</forenames></author><author><keyname>Pollak</keyname><forenames>JP</forenames></author><author><keyname>Estrin</keyname><forenames>Deborah</forenames></author></authors><title>Your Activities of Daily Living (YADL): An Image-based Survey Technique
  for Patients with Arthritis</title><categories>cs.CY cs.HC</categories><acm-class>H.4; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Healthcare professionals use Activities of Daily Living (ADL) to characterize
a patient's functional status and to evaluate the effectiveness of treatment
plans. ADLs are traditionally measured using standardized text-based
questionnaires and the only form of personalization is in the form of question
branching logic. Pervasive smartphone adoption makes it feasible to consider
more frequent patient-reporting on ADLs. However, asking generic sets of
questions repeatedly introduces user burden and fatigue that threatens to
interfere with their utility. We introduce an approach called YADL (Your
Activities of Daily Living) which uses images of ADLs and personalization to
improve survey efficiency and the patient-experience. It offers several
potential benefits: wider coverage of ADLs, improved engagement, and accurate
capture of individual health situations. In this paper, we discuss our system
design and the wide applicability of the design process for survey tools in
healthcare and beyond. Interactions with with a small number of patients with
Arthritis throughout the design process have been promising and we share
detailed insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03288</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03288</id><created>2016-01-13</created><authors><author><keyname>Van Asch</keyname><forenames>Vincent</forenames></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author></authors><title>Predicting the Effectiveness of Self-Training: Application to Sentiment
  Classification</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to investigate the connection between the
performance gain that can be obtained by selftraining and the similarity
between the corpora used in this approach. Self-training is a semi-supervised
technique designed to increase the performance of machine learning algorithms
by automatically classifying instances of a task and adding these as additional
training material to the same classifier. In the context of language processing
tasks, this training material is mostly an (annotated) corpus. Unfortunately
self-training does not always lead to a performance increase and whether it
will is largely unpredictable. We show that the similarity between corpora can
be used to identify those setups for which self-training can be beneficial. We
consider this research as a step in the process of developing a classifier that
is able to adapt itself to each new test corpus that it is presented with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03290</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03290</id><created>2016-01-13</created><authors><author><keyname>Tang</keyname><forenames>Yutao</forenames></author></authors><title>Coordination of Multi-Agent Systems under Switching Topologies via
  Disturbance Observer Based Approach</title><categories>math.OC cs.SY</categories><comments>12 pages, 4 figures, 2 tables</comments><doi>10.1080/00207721.2015.1135360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a leader-following coordination problem of heterogeneous
multi-agent systems is considered under switching topologies where each agent
is subject to some local (unbounded) disturbances. While these unknown
disturbances may disrupt the performance of agents, a disturbance observer
based approach is employed to estimate and reject them. Varying communication
topologies are also taken into consideration, and their byproduct difficulties
are overcome by using common Lyapunov function techniques. According to the
available information in difference cases, two disturbance observer based
protocols are proposed to solve this problem. Their effectiveness is verified
by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03295</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03295</id><created>2016-01-13</created><authors><author><keyname>Csurka</keyname><forenames>Gabriela</forenames></author></authors><title>Document image classification, with a specific view on applications of
  patent images</title><categories>cs.CV</categories><comments>Paper submitted in 2014 as book chapter of Current Challenges in
  Patent Information Retrieval, Second edition by M. Lupu et al (eds.). To
  appear in 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main focus of this paper is document image classification and retrieval,
where we analyze and compare different parameters for the RunLeght Histogram
(RL) and Fisher Vector (FV) based image representations. We do an exhaustive
experimental study using different document image datasets, including the MARG
benchmarks, two datasets built on customer data and the images from the Patent
Image Classification task of the Clef-IP 2011. The aim of the study is to give
guidelines on how to best choose the parameters such that the same features
perform well on different tasks. As an example of such need, we describe the
Image-based Patent Retrieval task's of Clef-IP 2011, where we used the same
image representation to predict the image type and retrieve relevant patents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03311</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03311</id><created>2016-01-13</created><updated>2016-02-02</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>Garg</keyname><forenames>Shashwat</forenames></author></authors><title>Improved Algorithmic Bounds for Discrepancy of Sparse Set Systems</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a low discrepancy coloring for sparse set
systems where each element lies in at most $t$ sets. We give an algorithm that
finds a coloring with discrepancy $O((t \log n \log s)^{1/2})$ where $s$ is the
maximum cardinality of a set. This improves upon the previous constructive
bound of $O(t^{1/2} \log n)$ based on algorithmic variants of the partial
coloring method, and for small $s$ (e.g.$s=\textrm{poly}(t)$) comes close to
the non-constructive $O((t \log n)^{1/2})$ bound due to Banaszczyk. Previously,
no algorithmic results better than $O(t^{1/2}\log n)$ were known even for $s =
O(t^2)$. Our method is quite robust and we give several refinements and
extensions. For example, the coloring we obtain satisfies the stronger
size-sensitive property that each set $S$ in the set system incurs an $O((t
\log n \log |S|)^{1/2})$ discrepancy. Another variant can be used to
essentially match Banaszczyk's bound for a wide class of instances even where
$s$ is arbitrarily large. Finally, these results also extend directly to the
more general Koml\'{o}s setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03313</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03313</id><created>2016-01-13</created><updated>2016-01-20</updated><authors><author><keyname>Kassarnig</keyname><forenames>Valentin</forenames></author></authors><title>Political Speech Generation</title><categories>cs.CL</categories><comments>15 pages, class project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we present a system that can generate political speeches for a
desired political party. Furthermore, the system allows to specify whether a
speech should hold a supportive or opposing opinion. The system relies on a
combination of several state-of-the-art NLP methods which are discussed in this
report. These include n-grams, Justeson &amp; Katz POS tag filter, recurrent neural
networks, and latent Dirichlet allocation. Sequences of words are generated
based on probabilities obtained from two underlying models: A language model
takes care of the grammatical correctness while a topic model aims for textual
consistency. Both models were trained on the Convote dataset which contains
transcripts from US congressional floor debates. Furthermore, we present a
manual and an automated approach to evaluate the quality of generated speeches.
In an experimental evaluation generated speeches have shown very high quality
in terms of grammatical correctness and sentence transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03316</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03316</id><created>2016-01-13</created><authors><author><keyname>Kawase</keyname><forenames>Yasushi</forenames></author><author><keyname>Matsui</keyname><forenames>Tomomi</forenames></author><author><keyname>Miyauchi</keyname><forenames>Atsushi</forenames></author></authors><title>Additive Approximation Algorithms for Modularity Maximization</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>23 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modularity is a quality function in community detection, which was
introduced by Newman and Girvan (2004). Community detection in graphs is now
often conducted through modularity maximization: given an undirected graph
$G=(V,E)$, we are asked to find a partition $\mathcal{C}$ of $V$ that maximizes
the modularity. Although numerous algorithms have been developed to date, most
of them have no theoretical approximation guarantee. Recently, to overcome this
issue, the design of modularity maximization algorithms with provable
approximation guarantees has attracted significant attention in the computer
science community.
  In this study, we further investigate the approximability of modularity
maximization. More specifically, we propose a polynomial-time
$\left(\cos\left(\frac{3-\sqrt{5}}{4}\pi\right) -
\frac{1+\sqrt{5}}{8}\right)$-additive approximation algorithm for the
modularity maximization problem. Note here that
$\cos\left(\frac{3-\sqrt{5}}{4}\pi\right) - \frac{1+\sqrt{5}}{8} &lt; 0.42084$
holds. This improves the current best additive approximation error of $0.4672$,
which was recently provided by Dinh, Li, and Thai (2015). Interestingly, our
analysis also demonstrates that the proposed algorithm obtains a nearly-optimal
solution for any instance with a very high modularity value. Moreover, we
propose a polynomial-time $0.16598$-additive approximation algorithm for the
maximum modularity cut problem. It should be noted that this is the first
non-trivial approximability result for the problem. Finally, we demonstrate
that our approximation algorithm can be extended to some related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03317</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03317</id><created>2016-01-13</created><updated>2016-01-21</updated><authors><author><keyname>Feng</keyname><forenames>Shi</forenames></author><author><keyname>Liu</keyname><forenames>Shujie</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author></authors><title>Implicit Distortion and Fertility Models for Attention-based
  Encoder-Decoder NMT Model</title><categories>cs.CL</categories><comments>11 pages, updated details</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural machine translation has shown very promising results lately. Most NMT
models follow the encoder-decoder framework. To make encoder-decoder models
more flexible, attention mechanism was introduced to machine translation and
also other tasks like speech recognition and image captioning. We observe that
the quality of translation by attention-based encoder-decoder can be
significantly damaged when the alignment is incorrect. We attribute these
problems to the lack of distortion and fertility models. Aiming to resolve
these problems, we propose new variations of attention-based encoder-decoder
and compare them with other models on machine translation. Our proposed method
achieved an improvement of 2 BLEU points over the original attention-based
encoder-decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03323</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03323</id><created>2016-01-13</created><authors><author><keyname>McKay</keyname><forenames>John</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Raj</keyname><forenames>Raghu</forenames></author></authors><title>Localized Dictionary design for Geometrically Robust Sonar ATR</title><categories>cs.CV</categories><comments>Submitted to IGARSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancements in Sonar image capture have opened the door to powerful
classification schemes for automatic target recognition (ATR. Recent work has
particularly seen the application of sparse reconstruction-based classification
(SRC) to sonar ATR, which provides compelling accuracy rates even in the
presence of noise and blur. Existing sparsity based sonar ATR techniques
however assume that the test images exhibit geometric pose that is consistent
with respect to the training set. This work addresses the outstanding open
challenge of handling inconsistently posed test sonar images relative to
training. We develop a new localized block-based dictionary design that can
enable geometric, i.e. pose robustness. Further, a dictionary learning method
is incorporated to increase performance and efficiency. The proposed SRC with
Localized Pose Management (LPM), is shown to outperform the state of the art
SIFT feature and SVM approach, due to its power to discern background clutter
in Sonar images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03333</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03333</id><created>2016-01-13</created><authors><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>A Score-level Fusion Method for Eye Movement Biometrics</title><categories>cs.CV</categories><comments>11 pages, 6 figures, In press, Pattern Recognition Letters</comments><doi>10.1016/j.patrec.2015.11.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel framework for the use of eye movement patterns
for biometric applications. Eye movements contain abundant information about
cognitive brain functions, neural pathways, etc. In the proposed method, eye
movement data is classified into fixations and saccades. Features extracted
from fixations and saccades are used by a Gaussian Radial Basis Function
Network (GRBFN) based method for biometric authentication. A score fusion
approach is adopted to classify the data in the output layer. In the evaluation
stage, the algorithm has been tested using two types of stimuli: random dot
following on a screen and text reading. The results indicate the strength of
eye movement pattern as a biometric modality. The algorithm has been evaluated
on BioEye 2015 database and found to outperform all the other methods. Eye
movements are generated by a complex oculomotor plant which is very hard to
spoof by mechanical replicas. Use of eye movement dynamics along with iris
recognition technology may lead to a robust counterfeit-resistant person
identification system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03341</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03341</id><created>2016-01-13</created><authors><author><keyname>Malazgirt</keyname><forenames>Gorker Alp</forenames></author><author><keyname>Candas</keyname><forenames>Deniz</forenames></author><author><keyname>Yurdakul</keyname><forenames>Arda</forenames></author></authors><title>Taxim: A Toolchain for Automated and Configurable Simulation for
  Embedded Multiprocessor Design</title><categories>cs.DC</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/5</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicore embedded systems have been constantly researched to improve the
efficiency by changing certain metrics, such as processor, memory, cache
hierarchies and their cache configurations. Using Multi2Sim and McPAT
simulators in combination allows the user to design various multiprocessing
architectures and estimate performance, power, area and timing metrics.
However, the design time required to simulate these systems is daunting and
prone to human error. In this paper, we introduce Taxim, a toolchain that can
automatically create requested multicore on-chip topologies along with
minimizing the simulation time due to repetitive tasks between architectural
power, energy and timing simulations. Taxim's decision-tree-based topology
synthesis tool creates processor configuration files that can be highly
erroneous when generated manually. The toolchain also automates the steps from
design entry to output report extraction by running automation scripts, and
listing the results. Our experiments show that multiprocessing architectures
with 32 cores and irregular cache hierarchies are more than 1k lines of code in
Multi2Sim's processor configuration format and Taxim can create such a file in
less than 10 milliseconds. The source code is freely available at
https://github.com/bouncaslab/TaXim/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03348</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03348</id><created>2016-01-13</created><authors><author><keyname>Moharreri</keyname><forenames>Kayhan</forenames></author><author><keyname>Ha</keyname><forenames>Minsu</forenames></author><author><keyname>Nehm</keyname><forenames>Ross H</forenames></author></authors><title>EvoGrader: an online formative assessment tool for automatically
  evaluating written evolutionary explanations</title><categories>cs.CL</categories><journal-ref>Evolution: Education and Outreach, vol. 7, pp. 1-14, 2014</journal-ref><doi>10.1186/s12052-014-0015-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EvoGrader is a free, online, on-demand formative assessment service designed
for use in undergraduate biology classrooms. EvoGrader's web portal is powered
by Amazon's Elastic Cloud and run with LightSIDE Lab's open-source
machine-learning tools. The EvoGrader web portal allows biology instructors to
upload a response file (.csv) containing unlimited numbers of evolutionary
explanations written in response to 86 different ACORNS (Assessing COntextual
Reasoning about Natural Selection) instrument items. The system automatically
analyzes the responses and provides detailed information about the scientific
and naive concepts contained within each student's response, as well as overall
student (and sample) reasoning model types. Graphs and visual models provided
by EvoGrader summarize class-level responses; downloadable files of raw scores
(in .csv format) are also provided for more detailed analyses. Although the
computational machinery that EvoGrader employs is complex, using the system is
easy. Users only need to know how to use spreadsheets to organize student
responses, upload files to the web, and use a web browser. A series of
experiments using new samples of 2,200 written evolutionary explanations
demonstrate that EvoGrader scores are comparable to those of trained human
raters, although EvoGrader scoring takes 99% less time and is free. EvoGrader
will be of interest to biology instructors teaching large classes who seek to
emphasize scientific practices such as generating scientific explanations, and
to teach crosscutting ideas such as evolution and natural selection. The
software architecture of EvoGrader is described as it may serve as a template
for developing machine-learning portals for other core concepts within biology
and across other disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03354</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03354</id><created>2016-01-13</created><authors><author><keyname>Grigorev</keyname><forenames>Alexey</forenames></author></authors><title>Identifier Namespaces in Mathematical Notation</title><categories>cs.IR</categories><comments>Master Thesis defended at TU Berlin in Summer 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we look at the problem of assigning each identifier of a
document to a namespace. At the moment, there does not exist a special dataset
where all identifiers are grouped to namespaces, and therefore we need to
create such a dataset ourselves.
  To do that, we need to find groups of documents that use identifiers in the
same way. This can be done with cluster analysis methods. We argue that
documents can be represented by the identifiers they contain, and this approach
is similar to representing textual information in the Vector Space Model.
Because of this, we can apply traditional document clustering techniques for
namespace discovery.
  Because the problem is new, there is no gold standard dataset, and it is hard
to evaluate the performance of our method. To overcome it, we first use Java
source code as a dataset for our experiments, since it contains the namespace
information. We verify that our method can partially recover namespaces from
source code using only information about identifiers.
  The algorithms are evaluated on the English Wikipedia, and the proposed
method can extract namespaces on a variety of topics. After extraction, the
namespaces are organized into a hierarchical structure by using existing
classification schemes such as MSC, PACS and ACM. We also apply it to the
Russian Wikipedia, and the results are consistent across the languages.
  To our knowledge, the problem of introducing namespaces to mathematics has
not been studied before, and prior to our work there has been no dataset where
identifiers are grouped into namespaces. Thus, our result is not only a good
start, but also a good indicator that automatic namespace discovery is
possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03370</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03370</id><created>2016-01-13</created><updated>2016-03-02</updated><authors><author><keyname>Zaichenkov</keyname><forenames>Pavel</forenames></author><author><keyname>Tveretina</keyname><forenames>Olga</forenames></author><author><keyname>Shafarenko</keyname><forenames>Alex</forenames></author></authors><title>A Constraint Satisfaction Method for Configuring Non-Local Service
  Interfaces</title><categories>cs.LO</categories><comments>the short version of the paper is to appear in iFM 2016 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity and decontextualisation are core principles of a service-oriented
architecture. However, the principles are often lost when it comes to an
implementation of services, as a result of a rigidly defined service interface.
The interface, which defines a data format, is typically specific to a
particular context and its change entails significant redevelopment costs. This
paper focuses on a two-fold problem. On the one hand, the interface description
language must be flexible enough for maintaining service compatibility in a
variety of different contexts without modification of the service itself. On
the other hand, the composition of interfaces in a distributed environment must
be provably consistent. The existing approaches for checking compatibility of
service choreographies are either inflexible (WS-CDL and WSCI) or require
behaviour specification associated with each service, which is often impossible
to provide in practice.
  We present a novel approach for automatic interface configuration in
distributed stream-connected components operating as closed-source services
(i.e. the behavioural protocol is unknown). We introduce a Message Definition
Language (MDL), which can extend the existing interfaces description languages,
such as WSDL, with support of subtyping, inheritance and polymorphism. The MDL
supports configuration variables that link input and output interfaces of a
service and propagate requirements over an application graph. We present an
algorithm that solves the interface reconciliation problem using constraint
satisfaction that relies on Boolean satisfiability as a subproblem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03375</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03375</id><created>2016-01-13</created><authors><author><keyname>Wang</keyname><forenames>Yinong</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Burns</keyname><forenames>Joseph E.</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Multi-Atlas Segmentation with Joint Label Fusion of Osteoporotic
  Vertebral Compression Fractures on CT</title><categories>cs.CV</categories><comments>MICCAI 2015 Computational Methods and Clinical Applications for Spine
  Imaging Workshop</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The precise and accurate segmentation of the vertebral column is essential in
the diagnosis and treatment of various orthopedic, neurological, and
oncological traumas and pathologies. Segmentation is especially challenging in
the presence of pathology such as vertebral compression fractures. In this
paper, we propose a method to produce segmentations for osteoporotic
compression fractured vertebrae by applying a multi-atlas joint label fusion
technique for clinical CT images. A total of 170 thoracic and lumbar vertebrae
were evaluated using atlases from five patients with varying degrees of spinal
degeneration. In an osteoporotic cohort of bundled atlases, registration
provided an average Dice coefficient and mean absolute surface distance of
2.7$\pm$4.5% and 0.32$\pm$0.13mm for osteoporotic vertebrae, respectively, and
90.9$\pm$3.0% and 0.36$\pm$0.11mm for compression fractured vertebrae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03378</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03378</id><created>2016-01-13</created><updated>2016-02-29</updated><authors><author><keyname>Wagh</keyname><forenames>Sameer</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author></authors><title>Root ORAM: A Tunable Differentially Private Oblivious RAM</title><categories>cs.CR</categories><comments>13+5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art mechanisms for oblivious RAM (ORAM) suffer from significant
bandwidth overheads (greater than 100x) that impact the throughput and latency
of memory accesses. This renders their deployment in high-performance and
bandwidth-constrained applications difficult, motivating the design of
low-overhead approaches for memory access obfuscation. We introduce and
formalize the notion of a differentially private ORAM that provides statistical
privacy guarantees, and which to the extent of our knowledge, is the first of
its kind. We present Root ORAM, a family of practical ORAMs that allow tunable
trade-offs between system security and desired bandwidth overhead, and that
provide the rigorous privacy guarantees of a differentially private ORAM. We
demonstrate the practicality of Root ORAM using theoretical analysis,
simulations, as well as real world experiments on Amazon EC2. Our theoretical
analysis rigorously quantifies the privacy offered by Root ORAM and provably
bounds the information leaked from ob- serving memory access patterns. We also
show that the simplest protocol in the Root ORAM family requires a bandwidth of
a mere 10 blocks, at the cost of rigorously quantified security loss, and that
this number is independent of the number of outsourced blocks N. This is an
order of magnitude improvement over the state-of-the-art ORAM schemes such as
Path ORAM, which incurs a bandwidth overhead of 10 log N blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03390</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03390</id><created>2016-01-13</created><authors><author><keyname>Mouradian</keyname><forenames>Carla</forenames></author><author><keyname>Saha</keyname><forenames>Tonmoy</forenames></author><author><keyname>Sahoo</keyname><forenames>Jagruti</forenames></author><author><keyname>Abu-Lebdeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author><author><keyname>Morrow</keyname><forenames>Monique</forenames></author><author><keyname>Polakos</keyname><forenames>Paul</forenames></author></authors><title>Network Functions Virtualization Architecture for Gateways for
  Virtualized Wireless Sensor and Actuator Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization enables multiple applications to share the same wireless
sensor and actuator network (WSAN). However, in heterogeneous environments,
virtualized wireless sensor and actuator networks (VWSAN) raise new challenges,
such as the need for on-the-fly, dynamic, elastic, and scalable provisioning of
gateways. Network Functions Virtualization (NFV) is a paradigm emerging to help
tackle these new challenges. It leverages standard virtualization technology to
consolidate special-purpose network elements on commodity hardware. This
article presents NFV architecture for VWSAN gateways, in which software
instances of gateway modules are hosted in NFV infrastructure operated and
managed by a VWSAN gateway provider. We consider several VWSAN providers, each
with its own brand or combination of brands of sensors and actuators/robots.
These sensors and actuators can be accessed by a variety of applications, each
may have different interface and QoS (i.e., latency, throughput, etc.)
requirements. The NFV infrastructure allows dynamic, elastic, and scalable
deployment of gateway modules in this heterogeneous VWSAN environment.
Furthermore, the proposed architecture is flexible enough to easily allow new
sensors and actuators integration and new application domains accommodation. We
present a prototype that is built using the OpenStack platform. Besides, the
performance results are discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03411</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03411</id><created>2016-01-13</created><updated>2016-03-01</updated><authors><author><keyname>MacFie</keyname><forenames>Andrew</forenames></author></authors><title>Analysis of Algorithms and Partial Algorithms</title><categories>cs.AI cs.DS</categories><acm-class>I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an alternative methodology for the analysis of algorithms, based
on the concept of expected discounted reward. This methodology naturally
handles algorithms that do not always terminate, so it can (theoretically) be
used with partial algorithms for undecidable problems, such as those found in
artificial general intelligence (AGI) and automated theorem proving. We mention
new approaches to self-improving AGI and logical uncertainty enabled by this
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03428</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03428</id><created>2016-01-13</created><authors><author><keyname>Elser</keyname><forenames>Veit</forenames></author></authors><title>The complexity of bit retrieval</title><categories>cs.DS math.CO math.NA math.OC</categories><comments>42 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bit retrieval is the problem of reconstructing a binary sequence from its
periodic autocorrelation, with applications in cryptography and x-ray
crystallography. After defining the problem, with and without noise, we
describe and compare various algorithms for solving it. A geometrical
constraint satisfaction algorithm, relaxed-reflect-reflect, is currently the
best algorithm for noisy bit retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03439</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03439</id><created>2016-01-13</created><authors><author><keyname>Pivaro</keyname><forenames>Gabriel</forenames></author><author><keyname>Kumar</keyname><forenames>Santosh</forenames></author><author><keyname>Fraidenraich</keyname><forenames>Gustavo</forenames></author></authors><title>On the Exact Distribution of Mutual Information of Two-user MIMO MAC
  Based on Quotient Distribution of Wishart Matrices</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the exact calculation of the probability density function (PDF)
and cumulative distribution function (CDF) of mutual information (MI) for a
two-user MIMO MAC network over block Rayleigh fading channels. So far the PDF
and CDF have been numerically evaluated since MI depends on the quotient of two
Wishart matrices, and no closed-form for this quotient was available.
  We derive exact results for the PDF and CDF of extreme (the smallest/the
largest) eigenvalues. Based on the results of quotient ensemble the exact
calculation for PDF and CDF of mutual information is presented via Laplace
transform approach and by direct integration of joint PDF of quotient
ensemble's eigenvalues. Furthermore, our derivations also provide the
parameters to apply the Gaussian approximation method, which is comparatively
easier to implement. We show that approximation matches the exact results
remarkably well for outage probability, i.e. CDF, above 10%. However, the
approximation could also be used for 1% outage probability with a relatively
small error.
  We apply the derived expressions to analyze the effects of adding receiving
antennas on the receiver's performance. By supposing no channel knowledge at
transmitters and successive decoding at receiver, the capacity of the first
user increases and outage probability decreases with extra antennas, as
expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03458</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03458</id><created>2016-01-13</created><authors><author><keyname>Matsui</keyname><forenames>Tomomi</forenames></author><author><keyname>Hamaguchi</keyname><forenames>Takayoshi</forenames></author></authors><title>Characterizing a Set of Popular Matchings Defined by Preference Lists
  with Ties</title><categories>cs.DS</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a simple characterization of a set of popular
matchings defined by preference lists with ties. By employing our
characterization, we propose a polynomial time algorithm for finding a minimum
cost popular matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03461</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03461</id><created>2016-01-13</created><authors><author><keyname>Ferdosian</keyname><forenames>Nasim</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author><author><keyname>Ali</keyname><forenames>Borhanuddin Mohd</forenames></author><author><keyname>Lun</keyname><forenames>Kweh Yeah</forenames></author></authors><title>Greedy-Knapsack Algorithm for Optimal Downlink Resource Allocation in
  LTE Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Wireless Networks, 2015</comments><doi>10.1007/s11276-015-1042-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Long Term Evolution (LTE) as a mobile broadband technology supports a
wide domain of communication services with different requirements. Therefore,
scheduling of all flows from various applications in overload states in which
the requested amount of bandwidth exceeds the limited available spectrum
resources is a challenging issue. Accordingly, in this paper, a greedy
algorithm is presented to evaluate user candidates which are waiting for
scheduling and select an optimal set of the users to maximize system
performance, without exceeding available bandwidth capacity. The
greedy-knapsack algorithm is defined as an optimal solution to the resource
allocation problem, formulated based on the fractional knapsack problem. A
compromise between throughput and QoS provisioning is obtained by proposing a
class-based ranking function, which is a combination of throughput and QoS
related parameters defined for each application. The simulation results show
that the proposed method provides high performance in terms of throughput, loss
and delay for different classes of QoS over the existing ones, especially under
overload traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03466</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03466</id><created>2016-01-13</created><updated>2016-02-15</updated><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Dynamic Privacy For Distributed Machine Learning Over Network</title><categories>cs.LG</categories><comments>22 pages, 8 figures Corrected typos. Revised argument in section 4,
  results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy-preserving distributed machine learning becomes increasingly
important due to the rapid growth of amount of data and the importance of
distributed learning. This paper develops algorithms to provide
privacy-preserving learning for classification problem using the regularized
empirical risk minimization (ERM) objective function in a distributed fashion.
We use the definition of differential privacy, developed by Dwork et al.
privacy to capture the notion of privacy of our algorithm. We provide two
methods. We first propose the dual variable perturbation} which perturbs the
dual variable before next intermediate minimization of augmented Lagrange
function over the classifier in every ADMM iteration. In the second method, we
apply the output perturbation to the primal variable before releasing it to
neighboring nodes. We call the second method primal variable perturbation.
Under certain conditions on the convexity and differentiability of the loss
function and regularizer, our algorithms is proved to provide differential
privacy through the entire learning process. We also provide theoretical
results for the accuracy of the algorithm, and prove that both algorithms
converges in distribution. The theoretical results show that the dual variable
perturbation outperforms the primal case. The tradeoff between privacy and
accuracy is examined in the numerical experiment. Our experiment shows that
both algorithms performs similar in managing the privacy-accuracy tradeoff, and
primal variable perturbaiton is slightly better than the dual case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03468</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03468</id><created>2016-01-13</created><authors><author><keyname>Wu</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Xueqi</forenames></author><author><keyname>Wang</keyname><forenames>Baoyun</forenames></author></authors><title>Energy Harvesting in Secure MIMO Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures, will submit to TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problems of energy harvesting in wireless com- munication systems have
recently drawn much attention. In this paper, we focus on the investigation of
energy harvesting maximization (EHM) in the important secrecy multi-input
multi- output (MIMO) systems where little research has been done due to their
complexity. Particularly, this paper studies the resource allocation strategies
in MIMO wiretap channels, wherein we attempt to maximize the harvested energy
by one or multiple multi-antenna energy receivers (ERs) (potential
eavesdropper) while guaranteeing the secure communication for the multi-
antenna information receiver (IR). Two types of IR, with and without the
capability to cancel the interference from energy signals, are taken into
account. In the scenario of single energy receiver (ER), we consider the joint
design of the transmis- sion information and energy covariances for EHM. Both
of the optimization problems for the two types of IR are non- convex, and
appear to be difficult. To circumvent them, the combination of first order
Taylor approximation and sequential convex optimization approach is proposed.
Then, we extend our attention to the scenario with multiple ERs, where the
artificial noise (AN) aided weighted sum-energy harvesting maximization
(WS-EHM) problem is considered. Other than the approaches adopted in solving
the EHM problems, an algorithm conducts in an alternating fashion is proposed
to handle this problem. In particular, we first perform a judicious
transformation of the WS- EHM problem. Then, a block Gauss-Seidel (GS)
algorithm based on logarithmic barrier method and gradient projection (GP) is
derived to obtain the optimal solution of the reformulation by solving convex
problems alternately. Furthermore, the resulting block GS method is proven to
converge to a Karush-Kuhn-Tucker (KKT) point of the original WS-EHM problem...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03478</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03478</id><created>2015-09-14</created><authors><author><keyname>Baqapuri</keyname><forenames>Afroze Ibrahim</forenames></author></authors><title>Deep Learning Applied to Image and Text Matching</title><categories>cs.LG cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to describe images with natural language sentences is the
hallmark for image and language understanding. Such a system has wide ranging
applications such as annotating images and using natural sentences to search
for images.In this project we focus on the task of bidirectional image
retrieval: such asystem is capable of retrieving an image based on a sentence
(image search) andretrieve sentence based on an image query (image annotation).
We present asystem based on a global ranking objective function which uses a
combinationof convolutional neural networks (CNN) and multi layer perceptrons
(MLP).It takes a pair of image and sentence and processes them in different
channels,finally embedding it into a common multimodal vector space. These
embeddingsencode abstract semantic information about the two inputs and can be
comparedusing traditional information retrieval approaches. For each such pair,
the modelreturns a score which is interpretted as a similarity metric. If this
score is high,the image and sentence are likely to convey similar meaning, and
if the score is low then they are likely not to.
  The visual input is modeled via deep convolutional neural network. On
theother hand we explore three models for the textual module. The first one
isbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a
combination of trigram &amp; skip-grams) with an MLP. The third is morespecialized
deep network specific for modeling variable length sequences (SSE).We report
comparable performance to recent work in the field, even though ouroverall
model is simpler. We also show that the training time choice of how wecan
generate our negative samples has a significant impact on performance, and can
be used to specialize the bi-directional system in one particular task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03479</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03479</id><created>2016-01-13</created><authors><author><keyname>Jain</keyname><forenames>Anoop</forenames></author><author><keyname>Ghose</keyname><forenames>Debasish</forenames></author></authors><title>Limited Communication Stabilization of Multi-Agent Systems to
  Synchronized and Balanced Phase Arrangements at Desired Angular Frequency</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers circular motion of multi-agent systems in which all the
agents are required to traverse different circles or a common circle at a
desired angular frequency. It is required to achieve these collective motions
with the heading angles of the agents synchronized or balanced. In
synchronization, the agents and their centroid have a common velocity
direction, while in balancing, the movement of agents causes the location of
the centroid to become stationary. It is assumed that the agents are subjected
to limited communication constraints, and exchange relative information
according to a time-invariant undirected graph. The feedback control laws to
achieve these collective motions are obtained by using Lyapunov theory and
LaSalles invariance principle. Simulations are given to illustrate the
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03481</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03481</id><created>2015-09-19</created><authors><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author><author><keyname>Behera</keyname><forenames>H. S.</forenames></author></authors><title>A Fuzzy MLP Approach for Non-linear Pattern Classification</title><categories>cs.NE</categories><comments>The final version of this paper has been published in &quot;International
  Conference on Communication and Computing (ICC-2014)&quot;
  [http://www.elsevierst.com/conference_book_download_chapter.php?cbid=86#chapter41]</comments><journal-ref>In Proc: K.R. Venugopal, S.C. Lingareddy (eds.) International
  Conference on Communication and Computing (ICC- 2014), Bangalore, India (June
  12-14, 2014), Computer Networks and Security, 314-323</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In case of decision making problems, classification of pattern is a complex
and crucial task. Pattern classification using multilayer perceptron (MLP)
trained with back propagation learning becomes much complex with increase in
number of layers, number of nodes and number of epochs and ultimate increases
computational time [31]. In this paper, an attempt has been made to use fuzzy
MLP and its learning algorithm for pattern classification. The time and space
complexities of the algorithm have been analyzed. A training performance
comparison has been carried out between MLP and the proposed fuzzy-MLP model by
considering six cases. Results are noted against different learning rates
ranging from 0 to 1. A new performance evaluation factor 'convergence gain' has
been introduced. It is observed that the number of epochs drastically reduced
and performance increased compared to MLP. The average and minimum gain has
been found to be 93% and 75% respectively. The best gain is found to be 95% and
is obtained by setting the learning rate to 0.55.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03483</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03483</id><created>2015-09-22</created><authors><author><keyname>de Amorim</keyname><forenames>Renato Cordeiro</forenames></author></authors><title>A survey on feature weighting based K-Means algorithms</title><categories>cs.LG</categories><comments>Journal of Classification (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a real-world data set there is always the possibility, rather high in our
opinion, that different features may have different degrees of relevance. Most
machine learning algorithms deal with this fact by either selecting or
deselecting features in the data preprocessing phase. However, we maintain that
even among relevant features there may be different degrees of relevance, and
this should be taken into account during the clustering process. With over 50
years of history, K-Means is arguably the most popular partitional clustering
algorithm there is. The first K-Means based clustering algorithm to compute
feature weights was designed just over 30 years ago. Various such algorithms
have been designed since but there has not been, to our knowledge, a survey
integrating empirical evidence of cluster recovery ability, common flaws, and
possible directions for future research. This paper elaborates on the concept
of feature weighting and addresses these issues by critically analysing some of
the most popular, or innovative, feature weighting mechanisms based in K-Means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03505</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03505</id><created>2016-01-14</created><authors><author><keyname>Zhang</keyname><forenames>Shan</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhang</keyname><forenames>Ning</forenames><affiliation>Sherman</affiliation></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames><affiliation>Sherman</affiliation></author><author><keyname>Gong</keyname><forenames>Jie</forenames><affiliation>Sherman</affiliation></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xuemin</keyname><affiliation>Sherman</affiliation></author><author><keyname>Shen</keyname></author></authors><title>Energy-Aware Traffic Offloading for Green Heterogeneous Networks</title><categories>cs.NI</categories><comments>IEEE JSAC (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With small cell base stations (SBSs) densely deployed in addition to
conventional macro base stations (MBSs), the heterogeneous cellular network
(HCN) architecture can effectively boost network capacity. To support the huge
power demand of HCNs, renewable energy harvesting technologies can be
leveraged. In this paper, we aim to make efficient use of the harvested energy
for on-grid power saving while satisfying the quality of service (QoS)
requirement. To this end, energy-aware traffic offloading schemes are proposed,
whereby user associations, ON-OFF states of SBSs, and power control are jointly
optimized according to the statistical information of energy arrival and
traffic load. Specifically, for the single SBS case, the power saving gain
achieved by activating the SBS is derived in closed form, based on which the
SBS activation condition and optimal traffic offloading amount are obtained.
Furthermore, a two-stage energy-aware traffic offloading (TEATO) scheme is
proposed for the multiple-SBS case, considering various operating
characteristics of SBSs with different power sources. Simulation results
demonstrate that the proposed scheme can achieve more than 50% power saving
gain for typical daily traffic and solar energy profiles, compared with the
conventional traffic offloading schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03516</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03516</id><created>2016-01-14</created><authors><author><keyname>Salnikov</keyname><forenames>Vsevolod</forenames></author><author><keyname>Schaub</keyname><forenames>Michael T.</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>Using higher-order Markov models to reveal flow-based communities in
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems made of interacting elements are commonly abstracted as
networks, in which nodes are associated with dynamic state variables, whose
evolution is driven by interactions mediated by the edges. Markov processes
have been the prevailing paradigm to model such a network-based dynamics, for
instance in the form of random walks or other types of diffusions. Despite the
success of this modelling perspective for numerous applications, it represents
an over-simplification of several real-world systems. Importantly, simple
Markov models lack memory in their dynamics, an assumption often not realistic
in practice. Here, we explore possibilities to enrich the system description by
means of second-order Markov models, exploiting empirical pathway information.
We focus on the problem of community detection and show that standard network
algorithms can be generalized in order to extract novel temporal information
about the system under investigation. We also apply our methodology to temporal
networks, where we can uncover communities shaped by the temporal correlations
in the system. Finally, we discuss relations of the framework of second order
Markov processes and the recently proposed formalism of using non-backtracking
matrices for community detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03521</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03521</id><created>2016-01-14</created><authors><author><keyname>Laurent</keyname><forenames>Monique</forenames></author><author><keyname>Seminaroti</keyname><forenames>Matteo</forenames></author></authors><title>Similarity-First Search: a new algorithm with application to Robinsonian
  matrix recognition</title><categories>cs.DM math.CO</categories><comments>35 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new efficient combinatorial algorithm for recognizing if a given
symmetric matrix is Robinsonian, i.e., if its rows and columns can be
simultaneously reordered so that entries are monotone nondecreasing in rows and
columns when moving toward the diagonal. As main ingredient we introduce a new
algorithm, named Similarity-First-Search (SFS), which extends Lexicographic
Breadth-First Search (Lex-BFS) to weighted graphs and which we use in a
multisweep algorithm to recognize Robinsonian matrices. Since Robinsonian
binary matrices correspond to unit interval graphs, our algorithm can be seen
as a generalization to weighted graphs of the 3-sweep Lex-BFS algorithm of
Corneil for recognizing unit interval graphs. This new recognition algorithm is
extremely simple and, for an $n\times n$ nonnegative matrix with $m$ nonzero
entries, it terminates in $n-1$ SFS sweeps, with overall running time $O(n^2
+nm\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03523</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03523</id><created>2016-01-14</created><authors><author><keyname>Shrivastava</keyname><forenames>Ashish</forenames></author><author><keyname>Rangan</keyname><forenames>C. Pandu</forenames></author></authors><title>Stable Marriage Problem with Ties and Incomplete bounded length
  preference list under social stability</title><categories>cs.DS</categories><comments>9 pages, Sixth International Conference on Computer Science and
  Information Technology (CCSIT-2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variant of socially stable marriage problem where preference
lists may be incomplete, may contain ties and may have bounded length. In real
world application like NRMP and Scottish medical matching scheme such
restrictions arise very frequently where set of agents (man/woman) is very
large and providing a complete and strict order preference list is practically
in-feasible. In presence of ties in preference lists, the most common solution
is weakly socially stable matching. It is a fact that in an instance, weakly
stable matching can have different sizes. This motivates the problem of finding
a maximum cardinality weakly socially stable matching.
  In this paper, we find maximum size weakly socially stable matching for an
instance of Stable Marriage problem with Ties and Incomplete bounded length
preference list with Social Stability. The motivation to consider this instance
is the known fact, any larger instance of this problem is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03528</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03528</id><created>2016-01-14</created><authors><author><keyname>Bock</keyname><forenames>Florian</forenames></author><author><keyname>Homm</keyname><forenames>Daniel</forenames></author><author><keyname>Siegl</keyname><forenames>Sebastian</forenames></author><author><keyname>German</keyname><forenames>Reinhard</forenames></author></authors><title>A Taxonomy for Tools, Processes and Languages in Automotive Software
  Engineering</title><categories>cs.SE</categories><comments>16 pages, 11 figures, conference paper</comments><doi>10.5121/csit.2016.60121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the growing domain of software engineering in the automotive sector,
the number of used tools, processes, methods and languages has increased
distinctly in the past years. To be able to choose proper methods for
particular development use cases, factors like the intended use, key-features
and possible limitations have to be evaluated. This requires a taxonomy that
aids the decision making. An analysis of the main existing taxonomies revealed
two major deficiencies: the lack of the automotive focus and the limitation to
particular engineering method types. To face this, a graphical taxonomy is
proposed based on two well-established engineering approaches and enriched with
additional classification information. It provides a self-evident and
-explanatory overview and comparison technique for engineering methods in the
automotive domain. The taxonomy is applied to common automotive engineering
methods. The resulting diagram classifies each method and enables the reader to
select appropriate solutions for given project requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03531</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03531</id><created>2016-01-14</created><authors><author><keyname>Al-Kadi</keyname><forenames>O. S.</forenames></author><author><keyname>Chung</keyname><forenames>Daniel Y. F.</forenames></author><author><keyname>Carlisle</keyname><forenames>Robert C.</forenames></author><author><keyname>Coussios</keyname><forenames>Constantin C.</forenames></author><author><keyname>Noble</keyname><forenames>J. Alison</forenames></author></authors><title>Quantification of Ultrasonic Texture heterogeneity via Volumetric
  Stochastic Modeling for Tissue Characterization</title><categories>cs.CV</categories><comments>Supplementary data associated with this article can be found, in the
  online version, at http://dx.doi.org/10.1016/j.media.2014.12. 004</comments><journal-ref>Medical Image Analysis, vol. 21(1), pp. 59-71, 2015</journal-ref><doi>10.1016/j.media.2014.12.004</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Intensity variations in image texture can provide powerful quantitative
information about physical properties of biological tissue. However, tissue
patterns can vary according to the utilized imaging system and are
intrinsically correlated to the scale of analysis. In the case of ultrasound,
the Nakagami distribution is a general model of the ultrasonic backscattering
envelope under various scattering conditions and densities where it can be
employed for characterizing image texture, but the subtle intra-heterogeneities
within a given mass are difficult to capture via this model as it works at a
single spatial scale. This paper proposes a locally adaptive 3D
multi-resolution Nakagami-based fractal feature descriptor that extends
Nakagami-based texture analysis to accommodate subtle speckle spatial frequency
tissue intensity variability in volumetric scans. Local textural fractal
descriptors - which are invariant to affine intensity changes - are extracted
from volumetric patches at different spatial resolutions from voxel
lattice-based generated shape and scale Nakagami parameters. Using ultrasound
radio-frequency datasets we found that after applying an adaptive fractal
decomposition label transfer approach on top of the generated Nakagami voxels,
tissue characterization results were superior to the state of art. Experimental
results on real 3D ultrasonic pre-clinical and clinical datasets suggest that
describing tumor intra-heterogeneity via this descriptor may facilitate
improved prediction of therapy response and disease characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03533</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03533</id><created>2016-01-14</created><authors><author><keyname>Zwattendorfer</keyname><forenames>Bernd</forenames></author><author><keyname>Slamanig</keyname><forenames>Daniel</forenames></author></authors><title>The Austrian eID Ecosystem in the Public Cloud: How to Obtain Privacy
  While Preserving Practicality</title><categories>cs.CR</categories><comments>47 pages, 5 figures, Journal of Information Security and
  Applications, 2015</comments><doi>10.1016/j.jisa.2015.11.004.</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Austrian eID system constitutes a main pillar within the Austrian
e-Government strategy. The eID system ensures unique identification and secure
authentication for citizens protecting access to applications where sensitive
and personal data is involved. In particular, the Austrian eID system supports
three main use cases: Identification and authentication of Austrian citizens,
electronic representation, and foreign citizen authentication at Austrian
public sector applications. For supporting all these use cases, several
components -- either locally deployed in the applications' domain or centrally
deployed -- need to communicate with each other. While local deployments have
some advantages in terms of scalability, still a central deployment of all
involved components would be advantageous, e.g. due to less maintenance
efforts. However, a central deployment can easily lead to load bottlenecks
because theoretically the whole Austrian population as well as -- for foreign
citizens -- the whole EU population could use the provided services. To
mitigate the issue on scalability, in this paper we propose the migration of
main components of the ecosystem into a public cloud. However, a move of
trusted services into a public cloud brings up new obstacles, particular with
respect to privacy. To bypass the issue on privacy, in this paper we propose an
approach on how the complete Austrian eID ecosystem can be moved into a public
cloud in a privacy-preserving manner by applying selected cryptographic
technologies (in particular using proxy re-encryption and redactable
signatures). Applying this approach, no sensitive data will be disclosed to a
public cloud provider by still supporting all three main eID system use cases.
We finally discuss our approach based on selected criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03541</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03541</id><created>2016-01-14</created><updated>2016-02-16</updated><authors><author><keyname>Shekarpour</keyname><forenames>Saeedeh</forenames></author><author><keyname>Lukovnikov</keyname><forenames>Denis</forenames></author><author><keyname>Kumar</keyname><forenames>Ashwini Jaya</forenames></author><author><keyname>Endris</keyname><forenames>Kemele</forenames></author><author><keyname>Singh</keyname><forenames>Kuldeep</forenames></author><author><keyname>Thakkar</keyname><forenames>Harsh</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author></authors><title>Question Answering on Linked Data: Challenges and Future Directions</title><categories>cs.IR</categories><comments>Submitted to Question Answering And Activity Analysis in
  Participatory Sites (Q4APS) 2016</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Question Answering (QA) systems are becoming the inspiring model for the
future of search engines. While recently, underlying datasets for QA systems
have been promoted from unstructured datasets to structured datasets with
highly semantic-enriched metadata, but still question answering systems involve
serious challenges which cause to be far beyond desired expectations. In this
paper, we raise the challenges for building a Question Answering (QA) system
especially with the focus of employing structured data (i.e. knowledge graph).
This paper provide an exhaustive insight of the known challenges, so far. Thus,
it helps researchers to easily spot open rooms for the future research agenda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03545</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03545</id><created>2016-01-14</created><authors><author><keyname>Champneys</keyname><forenames>Alan R</forenames></author><author><keyname>Varkonyi</keyname><forenames>Peter L</forenames></author></authors><title>The Painleve paradox in contact mechanics</title><categories>physics.class-ph cs.RO math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 120-year old so-called Painleve paradox involves the loss of determinism
in models of planar rigid bodies in point contact with a rigid surface, subject
to Coulomb-like dry friction. The phenomenon occurs due to coupling between
normal and rotational degrees-of-freedom such that the effective normal force
becomes attractive rather than repulsive. Despite a rich literature, the
forward evolution problem remains unsolved other than in certain restricted
cases in 2D with single contact points. Various practical consequences of the
theory are revisited, including models for robotic manipulators, and the
strange behaviour of chalk when pushed rather than dragged across a blackboard.
  Reviewing recent theory, a general formulation is proposed, including a
Poisson or energetic impact law. The general problem in 2D with a single point
of contact is discussed and cases or inconsistency or indeterminacy enumerated.
Strategies to resolve the paradox via contact regularisation are discussed from
a dynamical systems point of view. By passing to the infinite stiffness limit
and allowing impact without collision, inconsistent and indeterminate cases are
shown to be resolvable for all open sets of conditions. However, two
unavoidable ambiguities that can be reached in finite time are discussed in
detail, so called dynamic jam and reverse chatter. A partial review is given of
2D cases with two points of contact showing how a greater complexity of
inconsistency and indeterminacy can arise. Extension to fully three-dimensional
analysis is briefly considered and shown to lead to further possible
singularities. In conclusion, the ubiquity of the \pain paradox is highlighted
and open problems are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03555</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03555</id><created>2016-01-14</created><authors><author><keyname>Cao</keyname><forenames>Yue</forenames></author><author><keyname>Wei</keyname><forenames>Kaimin</forenames></author><author><keyname>Min</keyname><forenames>Geyong</forenames></author><author><keyname>Weng</keyname><forenames>Jian</forenames></author><author><keyname>Yang</keyname><forenames>Xin</forenames></author><author><keyname>Sun</keyname><forenames>Zhili</forenames></author></authors><title>A Geographic Multi-Copy Routing Scheme for DTNs With Heterogeneous
  Mobility</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous geographic routing schemes in Delay/Disruption Tolerant Networks
(DTNs) only consider the homogeneous scenario where nodal mobility is
identical. Motivated by this gap, we turn to design a DTN based geographic
routing scheme in heterogeneous scenario. Systematically, our target is
achieved via two steps: 1) We first propose &quot;The-Best-Geographic-Relay (TBGR)&quot;
routing scheme to relay messages via a limited number of copies, under the
homogeneous scenario. We further overcome the local maximum problem of TBGR
given a sparse network density, different from those efforts in dense networks
like clustered Wireless Sensor Networks (WSNs). 2) We next extend TBGR for
heterogeneous scenario, and propose &quot;The-Best-Heterogeneity-Geographic-Relay
(TBHGR)&quot; routing scheme considering individual nodal visiting preference
(referred to non-identical nodal mobility). Extensive results under a realistic
heterogeneous scenario show the advantage of TBHGR over literature works in
terms of reliable message delivery, while with low routing overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03597</identifier>
 <datestamp>2016-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03597</id><created>2016-01-14</created><authors><author><keyname>Engwer</keyname><forenames>Christian</forenames></author><author><keyname>N&#xfc;&#xdf;ing</keyname><forenames>Andreas</forenames></author></authors><title>Geometric Integration Over Irregular Domains with topologic Guarantees</title><categories>cs.NA math.NA</categories><acm-class>G.1.2; G.1.4; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implicitly described domains are a well established tool in the simulation of
time dependent problems, e.g. using level-set methods. In order to solve
partial differential equations on such domains, a range of numerical methods
was developed, e.g. the Immersed Boundary method, Unfitted Finite Element or
Unfitted discontinuous Galerkin methods, eXtended or Generalised Finite Element
methods, just to name a few. Many of these methods involve integration over
cut-cells or their boundaries, as they are described by sub-domains of the
original level-set mesh. We present a new algorithm to geometrically evaluate
the integrals over domains described by a first-order, conforming level-set
function. The integration is based on a polyhedral reconstruction of the
implicit geometry, following the concepts of the Marching Cubes algorithm. The
algorithm preserves various topological properties of the implicit geometry in
its polyhedral reconstruction, making it suitable for Finite Element
computations. Numerical experiments show second order accuracy of the
integration. An implementation of the algorithm is available as free software,
which allows for an easy incorporation into other projects. The software is in
productive use within the DUNE framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03603</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03603</id><created>2016-01-14</created><authors><author><keyname>Matuschke</keyname><forenames>Jannik</forenames></author><author><keyname>McCormick</keyname><forenames>S. Thomas</forenames></author><author><keyname>Oriolo</keyname><forenames>Gianpaolo</forenames></author><author><keyname>Peis</keyname><forenames>Britta</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>Protection of flows under targeted attacks</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the importance of robustness in many real-world optimization problems,
the field of robust optimization has gained a lot of attention over the past
decade. We concentrate on maximum flow problems and introduce a novel robust
optimization model which, compared to known models from the literature,
features several advantageous properties: (i) We consider a general class of
path-based flow problems which can be used to model a large variety of network
routing problems (and other packing problems). (ii) We aim at solutions that
are robust against targeted attacks by a potent adversary who may attack any
flow path of his choice on any edge of the network. (iii) In contrast to
previous robust maximum flow models, for which no efficient algorithms are
known, optimal robust flows for the most important basic variants of our model
can be found in polynomial time.
  We also consider generalizations where the flow player can spend a budget to
protect the network against the interdictor. Here, we show that the problem can
be solved efficiently when the interdiction costs are determined by the flow
player from scratch. However, the problem becomes hard to approximate when the
flow player has to improve an initial protection infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03613</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03613</id><created>2016-01-14</created><authors><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Non-orthogonal Multiple Access in Large-Scale Underlay Cognitive Radio
  Networks</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, non-orthogonal multiple access (NOMA) is applied to
large-scale underlay cognitive radio (CR) networks with randomly deployed
users. In order to characterize the performance of the considered network, new
closed-form expressions of the outage probability are derived using
stochastic-geometry. More importantly, by carrying out the diversity analysis,
new insights are obtained under the two scenarios with different power
constraints: 1) fixed transmit power of the primary transmitters (PTs), and 2)
transmit power of the PTs being proportional to that of the secondary base
station. For the first scenario, a diversity order of $m$ is experienced at the
$m$-th ordered NOMA user. For the second scenario, there is an asymptotic error
floor for the outage probability. Simulation results are provided to verify the
accuracy of the derived results. A pivotal conclusion is reached that by
carefully designing target data rates and power allocation coefficients of
users, NOMA can outperform conventional orthogonal multiple access in underlay
CR networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03617</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03617</id><created>2016-01-14</created><updated>2016-01-29</updated><authors><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Interactive Communication for Data Exchange</title><categories>cs.IT math.IT</categories><comments>28 pages, 4 figures, a longer version of ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two parties observing correlated data seek to exchange their data using
interactive communication. How many bits must they communicate? We propose a
new interactive protocol for data exchange which increases the communication
size in steps until the task is done. Next, we derive a lower bound on the
minimum number of bits that is based on relating the data exchange problem to
the secret key agreement problem. Our single-shot analysis applies to all
discrete random variables and yields upper and lower bound of a similar form.
In fact, the bounds are asymptotically tight and lead to a characterization of
the optimal rate of communication needed for data exchange for a general
sequence such as mixture of IID random variables as well as the optimal
second-order asymptotic term in the length of communication needed for data
exchange for the IID random variables, when the probability of error is fixed.
This gives a precise characterization of the asymptotic reduction in the length
of optimal communication due to interaction; in particular, two-sided
Slepian-Wolf compression is strictly suboptimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03619</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03619</id><created>2016-01-07</created><authors><author><keyname>Uribe</keyname><forenames>Daniel</forenames></author></authors><title>P vs. NP</title><categories>cs.CC</categories><comments>1 cover page, 1 page table of contents, 38 pages of content, 1 page
  bibliography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method for analyzing algorithmic runtime complexity using decision trees
is discussed using the sorting algorithm. This method is then extended to
optimal algorithms which may find all cliques of size q in network N, or simply
the first clique of size q in network N. Finally, the lower bound of such
decision trees is demonstrated to not be in P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03623</identifier>
 <datestamp>2016-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03623</id><created>2016-01-14</created><authors><author><keyname>Prugger</keyname><forenames>Martina</forenames></author><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author><author><keyname>Ostermann</keyname><forenames>Alexander</forenames></author></authors><title>Evaluation of the Partitioned Global Address Space (PGAS) model for an
  inviscid Euler solver</title><categories>cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we evaluate the performance of Unified Parallel C (which
implements the partitioned global address space programming model) using a
numerical method that is widely used in fluid dynamics. In order to evaluate
the incremental approach to parallelization (which is possible with UPC) and
its performance characteristics, we implement different levels of optimization
of the UPC code and compare it with an MPI parallelization on four different
clusters of the Austrian HPC infrastructure (LEO3, LEO3E, VSC2, VSC3) and on an
Intel Xeon Phi. We find that UPC is significantly easier to develop in compared
to MPI and that the performance achieved is comparable to MPI in most
situations. The obtained results show worse performance (on VSC2), competitive
performance (on LEO3, LEO3E and VSC3), and superior performance (on the Intel
Xeon Phi).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03633</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03633</id><created>2016-01-12</created><authors><author><keyname>van der Geer</keyname><forenames>Joris</forenames></author></authors><title>Transit directions at global scale</title><categories>cs.OH</categories><comments>13 pages, 3 tables</comments><msc-class>90-04 Operations research</msc-class><acm-class>I.2.8</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A novel approach to integrated ground and air public transport journey
planning, operating at continent scale. Flexible date search, prerequisite for
long distance trips given their typical low and irregular service frequencies,
is core functionality. The algorithm is especially suited for irregular and
poorly structured networks. Almost all of the described functionality is
implemented in a working prototype. Using ground transport only, the system is
on par with Google Transit on random country-wide trips in the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03642</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03642</id><created>2016-01-12</created><authors><author><keyname>Thoma</keyname><forenames>Martin</forenames></author></authors><title>Creativity in Machine Learning</title><categories>cs.CV cs.LG</categories><comments>5 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent machine learning techniques can be modified to produce creative
results. Those results did not exist before; it is not a trivial combination of
the data which was fed into the machine learning system. The obtained results
come in multiple forms: As images, as text and as audio.
  This paper gives a high level overview of how they are created and gives some
examples. It is meant to be a summary of the current work and give people who
are new to machine learning some starting points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03648</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03648</id><created>2016-01-14</created><authors><author><keyname>Marinho</keyname><forenames>Zita</forenames></author><author><keyname>Dragan</keyname><forenames>Anca</forenames></author><author><keyname>Byravan</keyname><forenames>Arun</forenames></author><author><keyname>Boots</keyname><forenames>Byron</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author></authors><title>Functional Gradient Motion Planning in Reproducing Kernel Hilbert Spaces</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a functional gradient descent trajectory optimization algorithm
for robot motion planning in Reproducing Kernel Hilbert Spaces (RKHSs).
Functional gradient algorithms are a popular choice for motion planning in
complex many-degree-of-freedom robots, since they (in theory) work by directly
optimizing within a space of continuous trajectories to avoid obstacles while
maintaining geometric properties such as smoothness. However, in practice,
functional gradient algorithms typically commit to a fixed, finite
parameterization of trajectories, often as a list of waypoints. Such a
parameterization can lose much of the benefit of reasoning in a continuous
trajectory space: e.g., it can require taking an inconveniently small step size
and large number of iterations to maintain smoothness. Our work generalizes
functional gradient trajectory optimization by formulating it as minimization
of a cost functional in an RKHS. This generalization lets us represent
trajectories as linear combinations of kernel functions, without any need for
waypoints. As a result, we are able to take larger steps and achieve a locally
optimal trajectory in just a few iterations. Depending on the selection of
kernel, we can directly optimize in spaces of trajectories that are inherently
smooth in velocity, jerk, curvature, etc., and that have a low-dimensional,
adaptively chosen parameterization. Our experiments illustrate the
effectiveness of the planner for different kernels, including Gaussian RBFs,
Laplacian RBFs, and B-splines, as compared to the standard discretized waypoint
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03649</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03649</id><created>2016-01-14</created><authors><author><keyname>Gardner</keyname><forenames>Brian</forenames></author><author><keyname>Gr&#xfc;ning</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Optimal Supervised Learning in Spiking Neural Networks for Precise
  Temporal Encoding</title><categories>cs.NE q-bio.NC</categories><comments>26 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise spike timing as a means to encode information in neural networks is
biologically supported, and is advantageous over frequency-based codes by
processing input features on a much shorter time-scale. For these reasons, much
recent attention has been focused on the development of supervised learning
rules for spiking neural networks that utilise a temporal coding scheme.
However, despite significant progress in this area, there still lack rules that
are theoretically justified, and yet can be considered biologically relevant.
Here we examine the general conditions under which optimal synaptic plasticity
takes place to support the supervised learning of a precise temporal code. As
part of our analysis we introduce two analytically derived learning rules, one
of which relies on an instantaneous error signal to optimise synaptic weights
in a network (INST rule), and the other one relying on a filtered error signal
to minimise the variance of synaptic weight modifications (FILT rule). We test
the optimality of the solutions provided by each rule with respect to their
temporal encoding precision, and then measure the maximum number of input
patterns they can learn to memorise using the precise timings of individual
spikes. Our results demonstrate the optimality of the FILT rule in most cases,
underpinned by the rule's error-filtering mechanism which provides smooth
convergence during learning. We also find the FILT rule to be most efficient at
performing input pattern memorisations, and most noticeably when patterns are
identified using spikes with sub-millisecond temporal precision. In comparison
with existing work, we determine the performance of the FILT rule to be
consistent with that of the highly efficient E-learning Chronotron rule, but
with the distinct advantage that our FILT rule is also implementable as an
online method for increased biological realism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03650</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03650</id><created>2016-01-14</created><updated>2016-02-25</updated><authors><author><keyname>Van Bui</keyname><forenames>Vuong</forenames></author><author><keyname>Le</keyname><forenames>Cuong Anh</forenames></author></authors><title>Smoothing the parameter of IBM word alignment models: the framework and
  its learning approaches</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IBM models are very popular word alignment models in Machine Translation.
They play critical roles in the systems of this field. These models follow
Maximum Likelihood principle to estimate their parameters. However, in many
case, the models will be too fit the training data that may result in wrong
word alignments on testing data. Smoothing is a popular solution to the
overfitting problem when the causes are rare events. While this technique is
very common in Language Model which is another problem in Machine Translation,
there is still lack of studies for the problem of word alignment.
\cite{moore2004improving} reported a study on a simple method of additive
smoothing, in which the amount to add is learnt from annotated data. This basic
technique gives a significant improvement over the unsmoothed version. With
such a good motivation, in this paper, we propose a more general framework by
varying the amount to add rather than adding only a constant amount as the
original additive smoothing. In term of learning method, we also experience a
method to learn the parameter of smoothing from unannotated data with a deep
analysis and comparision between different learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03651</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03651</id><created>2016-01-14</created><authors><author><keyname>Xu</keyname><forenames>Yan</forenames></author><author><keyname>Jia</keyname><forenames>Ran</forenames></author><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Chen</keyname><forenames>Yunchuan</forenames></author><author><keyname>Lu</keyname><forenames>Yangyang</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Improved Relation Classification by Deep Recurrent Neural Networks with
  Data Augmentation</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, neural networks play an important role in the task of relation
classification. By designing different neural architectures, researchers have
improved the performance to a large extent, compared with traditional methods.
However, existing neural networks for relation classification are usually of
shallow architectures (e.g., one-layer convolution neural networks or recurrent
networks). They may fail to explore the potential representation space in
different abstraction levels. In this paper, we propose deep recurrent neural
networks (DRNNs) to tackle this challenge. Further, we propose a data
augmentation method by leveraging the directionality of relations. We evaluate
our DRNNs on the SemEval-2010 Task 8, and achieve an $F_1$-score of 85.81%,
outperforming state-of-the-art recorded results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03660</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03660</id><created>2016-01-14</created><authors><author><keyname>Goldfeld</keyname><forenames>Ziv</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author></authors><title>Arbitrarily Varying Wiretap Channels with Type Constrained States</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The arbitrarily varying wiretap channel (AVWTC) is among the most challenging
open problems in the field of information-theoretic secrecy. Not only does it
capture the difficulty of the compound wiretap channel (another open problem)
as a special case, it also requires that secrecy is ensured with respect to
exponentially many possible channel state sequences. However, a stronger
version of Wyner's soft-covering lemma, recently derived by the authors, lays
the groundwork for overcoming the second aforementioned difficulty. To that
end, we consider an AVWTC with a type constraint on the allowed state
sequences, and derive a single-letter characterization of its correlated-random
(CR) assisted semantic-security (SS) capacity. The allowed state sequences are
the ones in a typical set around the type constraint. SS is establish by
showing that the information leakage to the eavesdropper is negligible even
when maximized over all message distributions, choices of state sequences and
realizations of the CR-code. Both the achievability and the converse proofs of
the type constrained coding theorem rely on stronger claims than actually
required. The direct part establishes a novel single-letter lower bound on the
CR-assisted SS-capacity of an AVWTC with state sequences constrained by any
convex and closed set of state probability mass functions (PMFs). SS follows by
leveraging a heterogeneous version of the stronger soft-covering lemma and a
CR-code reduction argument. Optimality is a consequence of an upper bound on
the CR-assisted SS-capacity of an AVWTC with state sequences constrained to any
collection of type-classes. The proof of the upper bound uses a novel
distribution coupling argument. The capacity formula shows that the legitimate
users effectively see an averaged main channel, while security must be ensured
versus an eavesdropper with perfect channel state information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03664</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03664</id><created>2016-01-14</created><updated>2016-01-20</updated><authors><author><keyname>Barton</keyname><forenames>Richard J.</forenames></author></authors><title>MIMO Architectures for Efficient Communication in Space</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characteristics of a space-to-space multiple-input, multiple-output
(MIMO) communication channel that distinguish it from terrestrial MIMO
communication channels are summarized and discussed primarily from an
information-theoretic viewpoint. The implications of these characteristics for
the design and application of future space-based communication systems are also
discussed, and it is shown that in general, either energy-efficient or
spectrally-efficient communication in space can only be achieved using a
distributed MIMO architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03675</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03675</id><created>2016-01-14</created><updated>2016-01-20</updated><authors><author><keyname>Barton</keyname><forenames>Richard J.</forenames></author></authors><title>Properties of Space MIMO Communication Channels</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1601.03664</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the characteristics of a space-to-space multiple-input,
multiple-output (MIMO) communication channel that distinguish it from more
common terrestrial MIMO communication channels. These characteristics imply
that the channel matrices for space communication channels have a particularly
simple structure that leads to statistical characteristics that are both
predictable and readily controllable using clusters of satellites as
distributed communication nodes. Furthermore, the extremely high cost of
launching mass into space introduces a constraint into the channel capacity
equation that leads to a spectral-efficiency vs. energy-efficiency tradeoff for
space MIMO communication that is fundamentally different from the tradeoff that
is generally considered applicable to terrestrial MIMO communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03676</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03676</id><created>2016-01-14</created><authors><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Romero</keyname><forenames>Jazm&#xed;n</forenames></author></authors><title>Arbitrary Overlap Constraints in Graph Packing Problems</title><categories>cs.DS</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In earlier versions of the community discovering problem, the overlap between
communities was restricted by a simple count upper-bound [17,5,11,8]. In this
paper, we introduce the $\Pi$-Packing with $\alpha()$-Overlap problem to allow
for more complex constraints in the overlap region than those previously
studied. Let $\mathcal{V}^r$ be all possible subsets of vertices of $V(G)$ each
of size at most $r$, and $\alpha: \mathcal{V}^r \times \mathcal{V}^r \to
\{0,1\}$ be a function. The $\Pi$-Packing with $\alpha()$-Overlap problem seeks
at least $k$ induced subgraphs in a graph $G$ subject to: (i) each subgraph has
at most $r$ vertices and obeys a property $\Pi$, and (ii) for any pair
$H_i,H_j$, with $i\neq j$, $\alpha(H_i, H_j) = 0$ (i.e., $H_i,H_j$ do not
conflict). We also consider a variant that arises in clustering applications:
each subgraph of a solution must contain a set of vertices from a given
collection of sets $\mathcal{C}$, and no pair of subgraphs may share vertices
from the sets of $\mathcal{C}$. In addition, we propose similar formulations
for packing hypergraphs. We give an $O(r^{rk} k^{(r+1)k} n^{cr})$ algorithm for
our problems where $k$ is the parameter and $c$ and $r$ are constants, provided
that: i) $\Pi$ is computable in polynomial time in $n$ and ii) the function
$\alpha()$ satisfies specific conditions. Specifically, $\alpha()$ is
hereditary, applicable only to overlapping subgraphs, and computable in
polynomial time in $n$. Motivated by practical applications we give several
examples of $\alpha()$ functions which meet those conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03679</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03679</id><created>2016-01-14</created><authors><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Long</keyname><forenames>Guodong</forenames></author><author><keyname>Zhang</keyname><forenames>Chengqi</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander G.</forenames></author></authors><title>Dynamic Concept Composition for Zero-Example Event Detection</title><categories>cs.CV</categories><comments>7 pages, AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on automatically detecting events in unconstrained
videos without the use of any visual training exemplars. In principle,
zero-shot learning makes it possible to train an event detection model based on
the assumption that events (e.g. \emph{birthday party}) can be described by
multiple mid-level semantic concepts (e.g. &quot;blowing candle&quot;, &quot;birthday cake&quot;).
Towards this goal, we first pre-train a bundle of concept classifiers using
data from other sources. Then we evaluate the semantic correlation of each
concept \wrt the event of interest and pick up the relevant concept
classifiers, which are applied on all test videos to get multiple prediction
score vectors. While most existing systems combine the predictions of the
concept classifiers with fixed weights, we propose to learn the optimal weights
of the concept classifiers for each testing video by exploring a set of online
available videos with free-form text descriptions of their content. To validate
the effectiveness of the proposed approach, we have conducted extensive
experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset.
The experimental results confirm the superiority of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03689</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03689</id><created>2016-01-14</created><authors><author><keyname>Steinruecken</keyname><forenames>Christian</forenames></author></authors><title>Compressing combinatorial objects</title><categories>cs.IT math.CO math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the world's digital data is currently encoded in a sequential form,
and compression methods for sequences have been studied extensively. However,
there are many types of non-sequential data for which good compression
techniques are still largely unexplored. This paper contributes insights and
concrete techniques for compressing various kinds of non-sequential data via
arithmetic coding, and derives re-usable probabilistic data models from fairly
generic structural assumptions. Near-optimal compression methods are described
for certain types of permutations, combinations and multisets; and the
conditions for optimality are made explicit for each method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03708</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03708</id><created>2016-01-14</created><authors><author><keyname>Dziurzanski</keyname><forenames>Piotr</forenames></author><author><keyname>Singh</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Indrusiak</keyname><forenames>Leandro S.</forenames></author><author><keyname>Saballus</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Benchmarking, System Design and Case-studies for Multi-core based
  Embedded Automotive Systems</title><categories>cs.DC cs.SE</categories><comments>2nd International Workshop on Dynamic Resource Allocation and
  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016
  (arXiv:cs/1449078)</comments><report-no>DREAMCloud/2016/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, using of automotive use cases as benchmarks for real-time
system design has been proposed. The use cases are described in a format
supported by AMALTHEA platform, which is a model based open source development
environment for automotive multi-core systems. An example of a simple
Electronic Control Unit has been analysed and presented with enough details to
reconstruct this system in any format. For researchers willing to use AMALTHEA
file format directly, an appropriate parser has been developed and offered. An
example of applying this parser and benchmark for optimising makespan while not
violating the timing constraints by allocating functionality to different
Network on Chip resource is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03712</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03712</id><created>2016-01-14</created><authors><author><keyname>Yang</keyname><forenames>Dehui</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Super-Resolution of Complex Exponentials from Modulations with Unknown
  Waveforms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Super-resolution is generally referred to as the task of recovering fine
details from coarse information. Motivated by applications such as
single-molecule imaging, radar imaging, etc., we consider parameter estimation
of complex exponentials from their modulations with unknown waveforms, allowing
for non-stationary blind super-resolution. This problem, however, is ill-posed
since both the parameters associated with the complex exponentials and the
modulating waveforms are unknown. To alleviate this, we assume that the unknown
waveforms live in a common low-dimensional subspace. Using a lifting trick, we
recast the blind super-resolution problem as a structured low-rank matrix
recovery problem. Atomic norm minimization is then used to enforce the
structured low-rankness, and is reformulated as a semidefinite program that is
solvable in polynomial time. We show that, up to scaling ambiguities, exact
recovery of both of the complex exponential parameters and the unknown
waveforms is possible when the waveform subspace is random and the number of
measurements is proportional to the number of degrees of freedom in the
problem. Numerical simulations support our theoretical findings, showing that
non-stationary blind super-resolution using atomic norm minimization is
possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03714</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03714</id><created>2016-01-14</created><authors><author><keyname>Joos</keyname><forenames>Felix</forenames></author><author><keyname>Perarnau</keyname><forenames>Guillem</forenames></author><author><keyname>Rautenbach</keyname><forenames>Dieter</forenames></author><author><keyname>Reed</keyname><forenames>Bruce</forenames></author></authors><title>How to determine if a random graph with a fixed degree sequence has a
  giant component</title><categories>math.CO cs.DM math.PR</categories><comments>40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a fixed degree sequence $\mathcal{D}=(d_1,...,d_n)$, let $G(\mathcal{D})$
be a uniformly chosen (simple) graph on $\{1,...,n\}$ where the vertex $i$ has
degree $d_i$. In this paper we determine whether $G(\mathcal{D})$ has a giant
component, essentially imposing no conditions on $\mathcal{D}$. We simply
insist that the sum of the degrees in $\mathcal{D}$ which are not 2 is at least
$\lambda(n)$ for some function $\lambda$ going to infinity with $n$. This is a
relatively minor technical condition, and the typical structure of
$G(\mathcal{D})$ when $\mathcal{D}$ does not satisfy this condition, is simple
and also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03746</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03746</id><created>2016-01-14</created><updated>2016-01-19</updated><authors><author><keyname>Carrazza</keyname><forenames>Stefano</forenames></author><author><keyname>Ferrara</keyname><forenames>Alfio</forenames></author><author><keyname>Salini</keyname><forenames>Silvia</forenames></author></authors><title>Research infrastructures in the LHC era: a scientometric approach</title><categories>physics.soc-ph cs.DL hep-ex hep-ph</categories><comments>39 pages, 9 figures</comments><report-no>CERN-PH-TH-2015-246, TIF-UNIMI-2015-17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a research infrastructure is funded and implemented, new information and
new publications are created. This new information is the measurable output of
discovery process. In this paper, we describe the impact of infrastructure for
physics experiments in terms of publications and citations. In particular, we
consider the Large Hadron Collider (LHC) experiments (ATLAS, CMS, ALICE, LHCb)
and compare them to the Large Electron Positron Collider (LEP) experiments
(ALEPH, DELPHI, L3, OPAL) and the Tevatron experiments (CDF, D0). We provide an
overview of the scientific output of these projects over time and highlight the
role played by remarkable project results in the publication-citation
distribution trends. The methodological and technical contribution of this work
provides a starting point for the development of a theoretical model of modern
scientific knowledge propagation over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03754</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03754</id><created>2016-01-14</created><authors><author><keyname>Curtin</keyname><forenames>Ryan R.</forenames></author></authors><title>Dual-tree $k$-means with bounded iteration runtime</title><categories>cs.DS cs.LG</categories><comments>supplementary material included; submitted to ICML '16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  k-means is a widely used clustering algorithm, but for $k$ clusters and a
dataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time.
Although there are existing techniques to accelerate single Lloyd iterations,
none of these are tailored to the case of large $k$, which is increasingly
common as dataset sizes grow. We propose a dual-tree algorithm that gives the
exact same results as standard $k$-means; when using cover trees, we use
adaptive analysis techniques to, under some assumptions, bound the
single-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledge
these are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then show
that this theoretically favorable algorithm performs competitively in practice,
especially for large $N$ and $k$ in low dimensions. Further, the algorithm is
tree-independent, so any type of tree may be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03763</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03763</id><created>2016-01-14</created><authors><author><keyname>Ao</keyname><forenames>Weng Chon</forenames></author><author><keyname>Wang</keyname><forenames>Chenwei</forenames></author><author><keyname>Bursalioglu</keyname><forenames>Ozgun Y.</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos</forenames></author></authors><title>Compressed Sensing-based Pilot Assignment and Reuse for Mobile UEs in
  mmWave Cellular Systems</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, submittd to IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technologies for mmWave communication are at the forefront of investigations
in both industry and academia, as the mmWave band offers the promise of orders
of magnitude additional available bandwidths to what has already been allocated
to cellular networks. The much larger number of antennas that can be supported
in a small footprint at mmWave bands can be leveraged to harvest massive-MIMO
type beamforming and spatial multiplexing gains. Similar to LTE systems, two
prerequisites for harvesting these benefits are detecting users and acquiring
user channel state information (CSI) in the training phase. However, due to the
fact that mmWave channels encounter much harsher propagation and decorrelate
much faster, the tasks of user detection and CSI acquisition are both
imperative and much more challenging than in LTE bands.
  In this paper, we investigate the problem of fast user detection and CSI
acquisition in the downlink of small cell mmWave networks. We assume TDD
operation and channel-reciprocity based CSI acquisition. To achieve
densification benefits we propose pilot designs and channel estimators that
leverage a combination of aggressive pilot reuse with fast user detection at
the base station and compressed sensing channel estimation. As our simulations
show, the number of users that can be simultaneously served by the entire
mmWave-band network with the proposed schemes increases substantially with
respect to traditional compressed sensing based approaches with conventional
pilot reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03764</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03764</id><created>2016-01-14</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Yuanzhi</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>Linear Algebraic Structure of Word Senses, with Applications to Polysemy</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word embeddings are ubiquitous in NLP and information retrieval, but it's
unclear what they represent when the word is polysemous, i.e., has multiple
senses. Here it is shown that multiple word senses reside in linear
superposition within the word embedding and can be recovered by simple sparse
coding.
  The success of the method ---which applies to several embedding methods
including word2vec--- is mathematically explained using the random walk on
discourses model (Arora et al., 2015). A novel aspect of our technique is that
each word sense is also accompanied by one of about 2000 &quot;discourse atoms&quot; that
give a succinct description of which other words co-occur with that word sense.
Discourse atoms seem of independent interest, and make the method potentially
more useful than the traditional clustering-based approaches to polysemy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03766</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03766</id><created>2016-01-14</created><authors><author><keyname>Mellor</keyname><forenames>Andrew</forenames></author><author><keyname>Mobilia</keyname><forenames>Mauro</forenames></author><author><keyname>Zia</keyname><forenames>R. K. P.</forenames></author></authors><title>Characterization of the Nonequilibrium Steady State of a Heterogeneous
  Nonlinear $q$-Voter Model with Zealotry</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO q-bio.PE</categories><comments>6 pages, 2 figures, supplementary material and movie available at
  https://dx.doi.org/10.6084/m9.figshare.2060595</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an heterogeneous nonlinear $q$-voter model with zealots and two
types of susceptible voters, and study its non-equilibrium properties when the
population is finite and well mixed. In this two-opinion model, each individual
supports one of two parties and is either a zealot or a susceptible voter of
type $q_1$ or $q_2$. While here zealots never change their opinion, a
$q_i$-susceptible voter ($i=1,2$) consults a group of $q_i$ neighbors at each
time step, and adopts their opinion if all group members agree. We show that
this model violates the detailed balance whenever $q_1 \neq q_2$ and has
surprisingly rich properties. Here, we focus on the characterization of the
model's non-equilibrium stationary state (NESS) in terms of its probability
distribution and currents in the distinct regimes of low and high density of
zealotry. We unveil the NESS properties in each of these phases by computing
the opinion distribution and the circulation of probability currents, as well
as the two-point correlation functions at unequal times (formally related to a
&quot;probability angular momentum&quot;). Our analytical calculations obtained in the
realm of a linear Gaussian approximation are compared with numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03767</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03767</id><created>2016-01-14</created><authors><author><keyname>Coti</keyname><forenames>Camille</forenames></author><author><keyname>Lakos</keyname><forenames>Charles</forenames></author><author><keyname>Petrucci</keyname><forenames>Laure</forenames></author></authors><title>Formally Proving and Enhancing a Self-Stabilising Distributed Algorithm</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the benefits of formal modelling and verification
techniques for self-stabilising distributed algorithms. An algorithm is
studied, that takes a set of processes connected by a tree topology and
converts it to a ring configuration. The Coloured Petri net model not only
facilitates the proof that the algorithm is correct and self-stabilising but
also easily shows that it enjoys new properties of termination and silentness.
Further, the formal results show how the algorithm can be simplified without
loss of generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03768</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03768</id><created>2016-01-14</created><authors><author><keyname>Moehle</keyname><forenames>Nicholas</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author></authors><title>Optimal Current Waveforms for Switched-Reluctance Motors</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of finding current waveforms for a
switched reluctance motor that minimize a user-defined combination of torque
ripple and RMS current. The motor model we use is fairly general, and includes
magnetic saturation, voltage and current limits, and highly coupled magnetics
(and therefore, unconventional geometries and winding patterns). We solve this
problem by approximating it as a mixed-integer convex program, which we solve
globally using branch and bound. We demonstrate our approach on an
experimentally verified model of a fully pitched switched reluctance motor, for
which we find the globally optimal waveforms, even for high rotor speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03769</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03769</id><created>2016-01-14</created><authors><author><keyname>Johnston</keyname><forenames>Kyle B</forenames></author><author><keyname>Oluseyi</keyname><forenames>Hakeem M</forenames></author></authors><title>Generation of a Supervised Classification Algorithm for Time-Series
  Variable Stars with an Application to the LINEAR Dataset</title><categories>astro-ph.IM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of digital astronomy, new benefits and new problems have been
presented to the modern day astronomer. While data can be captured in a more
efficient and accurate manor using digital means, the efficiency of data
retrieval has led to an overload of scientific data for processing and storage.
This paper will focus on the construction and application of a supervised
pattern classification algorithm for the identification of variable stars.
Given the reduction of a survey of stars into a standard feature space, the
problem of using prior patterns to identify new observed patterns can be
reduced to time tested classification methodologies and algorithms. Such
supervised methods, so called because the user trains the algorithms prior to
application using patterns with known classes or labels, provide a means to
probabilistically determine the estimated class type of new observations. This
paper will demonstrate the construction and application of a supervised
classification algorithm on variable star data. The classifier is applied to a
set of 192,744 LINEAR data points. Of the original samples, 34,451 unique stars
were classified with high confidence (high level of probability of being the
true class).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03778</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03778</id><created>2016-01-14</created><updated>2016-02-15</updated><authors><author><keyname>Zhang</keyname><forenames>Baichuan</forenames></author><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author><author><keyname>Ning</keyname><forenames>Xia</forenames></author><author><keyname>Agarwal</keyname><forenames>Khushbu</forenames></author><author><keyname>Purohit</keyname><forenames>Sumit</forenames></author><author><keyname>Cabrera</keyname><forenames>Paola Pesntez</forenames></author></authors><title>Trust from the past: Bayesian Personalized Ranking based Link Prediction
  in Knowledge Graphs</title><categories>cs.LG cs.AI cs.IR</categories><comments>SDM Workshop on Mining Networks and Graphs (MNG 2016), Miami, FL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction, or predicting the likelihood of a link in a knowledge graph
based on its existing state is a key research task. It differs from a
traditional link prediction task in that the links in a knowledge graph are
categorized into different predicates and the link prediction performance of
different predicates in a knowledge graph generally varies widely. In this
work, we propose a latent feature embedding based link prediction model which
considers the prediction task for each predicate disjointly. To learn the model
parameters it utilizes a Bayesian personalized ranking based optimization
technique. Experimental results on large-scale knowledge bases such as YAGO2
show that our link prediction approach achieves substantially higher
performance than several state-of-art approaches. We also show that for a given
predicate the topological properties of the knowledge graph induced by the
given predicate edges are key indicators of the link prediction performance of
that predicate in the knowledge graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03783</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03783</id><created>2016-01-14</created><authors><author><keyname>Altinok</keyname><forenames>Duygu</forenames></author></authors><title>Towards Turkish ASR: Anatomy of a rule-based Turkish g2p</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper describes the architecture and implementation of a rule-based
grapheme to phoneme converter for Turkish. The system accepts surface form as
input, outputs SAMPA mapping of the all parallel pronounciations according to
the morphological analysis together with stress positions. The system has been
implemented in Python
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03785</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03785</id><created>2016-01-14</created><authors><author><keyname>Farias</keyname><forenames>A. Diego S.</forenames></author><author><keyname>Costa</keyname><forenames>Valdigleis S.</forenames></author><author><keyname>Lopes</keyname><forenames>Luiz Ranyer A.</forenames></author><author><keyname>Bedregal</keyname><forenames>Benjam&#xed;n</forenames></author><author><keyname>Santiago</keyname><forenames>Regivan</forenames></author></authors><title>A Method for Image Reduction Based on a Generalization of Ordered
  Weighted Averaging Functions</title><categories>cs.AI</categories><comments>32 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a special type of aggregation function which
generalizes the notion of Ordered Weighted Averaging Function - OWA. The
resulting functions are called Dynamic Ordered Weighted Averaging Functions ---
DYOWAs. This generalization will be developed in such way that the weight
vectors are variables depending on the input vector. Particularly, this
operators generalize the aggregation functions: Minimum, Maximum, Arithmetic
Mean, Median, etc, which are extensively used in image processing. In this
field of research two problems are considered: The determination of methods to
reduce images and the construction of techniques which provide noise reduction.
The operators described here are able to be used in both cases. In terms of
image reduction we apply the methodology provided by Patermain et al. We use
the noise reduction operators obtained here to treat the images obtained in the
first part of the paper, thus obtaining images with better quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03790</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03790</id><created>2016-01-14</created><authors><author><keyname>Zhu</keyname><forenames>Junan</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Multi-Processor Approximate Message Passing with Lossy Compression</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider large-scale linear inverse problems in Bayesian settings. Our
general approach follows a recent line of work that applies the approximate
message passing (AMP) framework in multi-processor (MP) computational systems
by storing and processing a subset of rows of the measurement matrix along with
corresponding measurements at each MP node. In each MP-AMP iteration, nodes of
the MP system and its fusion center exchange messages pertaining to their
estimates of the input. Unfortunately, communicating these messages is often
costly in applications such as sensor networks. To reduce the communication
costs, we apply lossy compression to the messages. To improve compression, we
realize that the rate distortion trade-off differs among MP-AMP iterations, and
use dynamic programming (DP) to optimize per-iteration coding rates. Numerical
results confirm that the proposed coding rates offer significant and often
dramatic reductions in communication costs. That said, computational costs
involve two matrix vector products per MP-AMP iteration, which may be
significant for large matrices. Therefore, we further improve the trade-off
between computational and communication costs with a modified DP scheme. Case
studies involving sensor networks and large-scale computing systems highlight
the potential of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03793</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03793</id><created>2016-01-14</created><authors><author><keyname>Chen</keyname><forenames>Zheng</forenames></author><author><keyname>Qiu</keyname><forenames>Ling</forenames></author><author><keyname>Liang</keyname><forenames>Xiaowen</forenames></author></authors><title>Stochastic Geometry Analysis of Multi-Antenna Two-Tier Cellular Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the key properties of multi-antenna two-tier networks
under different system configurations. Based on stochastic geometry, we derive
the expressions and approximations for the users' average data rate. Through
the more tractable approximations, the theoretical analysis can be greatly
simplified. We find that the differences in density and transmit power between
two tiers, together with range expansion bias significantly affect the users'
data rate. Besides, for the purpose of area spectral efficiency (ASE)
maximization, we find that the optimal number of active users for each tier is
approximately fixed portion of the sum of the number of antennas plus one.
Interestingly, the optimal settings are insensitive to different configurations
between two tiers. Last but not the least, if the number of antennas of macro
base stations (MBSs) is sufficiently larger than that of small cell base
stations (SBSs), we find that range expansion will improve ASE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03797</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03797</id><created>2016-01-14</created><authors><author><keyname>Krishnan</keyname><forenames>Sanjay</forenames></author><author><keyname>Wang</keyname><forenames>Jiannan</forenames></author><author><keyname>Wu</keyname><forenames>Eugene</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Goldberg</keyname><forenames>Ken</forenames></author></authors><title>ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models</title><categories>cs.DB cs.LG</categories><comments>Pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data cleaning is often an important step to ensure that predictive models,
such as regression and classification, are not affected by systematic errors
such as inconsistent, out-of-date, or outlier data. Identifying dirty data is
often a manual and iterative process, and can be challenging on large datasets.
However, many data cleaning workflows can introduce subtle biases into the
training processes due to violation of independence assumptions. We propose
ActiveClean, a progressive cleaning approach where the model is updated
incrementally instead of re-training and can guarantee accuracy on partially
cleaned data. ActiveClean supports a popular class of models called convex loss
models (e.g., linear regression and SVMs). ActiveClean also leverages the
structure of a user's model to prioritize cleaning those records likely to
affect the results. We evaluate ActiveClean on five real-world datasets UCI
Adult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real and
synthetic errors. Our results suggest that our proposed optimizations can
improve model accuracy by up-to 2.5x for the same amount of data cleaned.
Furthermore for a fixed cleaning budget and on all real dirty datasets,
ActiveClean returns more accurate models than uniform sampling and Active
Learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03800</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03800</id><created>2016-01-14</created><authors><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author></authors><title>Rank-width: Algorithmic and structural results</title><categories>math.CO cs.DS</categories><comments>14 pages</comments><msc-class>05C75, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank-width is a width parameter of graphs describing whether it is possible
to decompose a graph into a tree-like structure by `simple' cuts. This survey
aims to summarize known algorithmic and structural results on rank-width of
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03803</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03803</id><created>2016-01-14</created><authors><author><keyname>Connelly</keyname><forenames>Joseph</forenames></author><author><keyname>Zeger</keyname><forenames>Kenneth</forenames></author></authors><title>A Class of Non-Linearly Solvable Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory January 14th
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For each integer $m \geq 2$, a network is constructed which is solvable over
an alphabet of size $m$ but is not solvable over any smaller alphabets. If $m$
is composite, then the network has no vector linear solution over any
$R$-module alphabet and is not asymptotically linear solvable over any
finite-field alphabet. The network's capacity is shown to equal one, and when
$m$ is composite, its linear capacity is shown to be bounded away from one for
all finite-field alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03805</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03805</id><created>2016-01-14</created><authors><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author></authors><title>Matrix Neural Networks</title><categories>cs.LG</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional neural networks assume vectorial inputs as the network is
arranged as layers of single line of computing units called neurons. This
special structure requires the non-vectorial inputs such as matrices to be
converted into vectors. This process can be problematic. Firstly, the spatial
information among elements of the data may be lost during vectorisation.
Secondly, the solution space becomes very large which demands very special
treatments to the network parameters and high computational cost. To address
these issues, we propose matrix neural networks (MatNet), which takes matrices
directly as inputs. Each neuron senses summarised information through bilinear
mapping from lower layer units in exactly the same way as the classic feed
forward neural networks. Under this structure, back prorogation and gradient
descent combination can be utilised to obtain network parameters efficiently.
Furthermore, it can be conveniently extended for multimodal inputs. We apply
MatNet to MNIST handwritten digits classification and image super resolution
tasks to show its effectiveness. Without too much tweaking MatNet achieves
comparable performance as the state-of-the-art methods in both tasks with
considerably reduced complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03809</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03809</id><created>2015-11-03</created><authors><author><keyname>Sayyed</keyname><forenames>Mostafa</forenames></author></authors><title>Artificial neural network approach for condition-based maintenance</title><categories>cs.NE cs.CY</categories><comments>108 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research, computerized maintenance management will be investigated.
The rise of maintenance cost forced the research community to look for more
effective ways to schedule maintenance operations. Using computerized models to
come up with optimal maintenance policy has led to better equipment utilization
and lower costs. This research adopts Condition-Based Maintenance model where
the maintenance decision is generated based on equipment conditions. Artificial
Neural Network technique is proposed to capture and analyze equipment condition
signals which lead to higher level of knowledge gathering. This knowledge is
used to accurately estimate equipment failure time. Based on these estimations,
an optimal maintenance management policy can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03810</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03810</id><created>2015-11-03</created><authors><author><keyname>Pahwa</keyname><forenames>Payal</forenames></author><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Kumar</keyname><forenames>Akshay</forenames></author><author><keyname>Sahil</keyname></author><author><keyname>Rathi</keyname><forenames>Vikas</forenames></author><author><keyname>Swami</keyname><forenames>Sunil</forenames></author></authors><title>Dynamic Cluster Head Selection Using Fuzzy Logic on Cloud in Wireless
  Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most vital activities to reduce energy consumption in wireless
sensor networks is clustering. In clustering, one node from a group of nodes is
selected to be a cluster head, which handles majority of the computation and
processing for the nodes in the cluster. This paper proposes an algorithm for
fuzzy based dynamic cluster head selection on cloud in wireless sensor
networks. The proposed algorithm calculates a Potential value for each node and
selects cluster heads with high potential. The proposed algorithm minimizes
cluster overlapping by spatial distribution of cluster heads and discards
malicious nodes i.e. never allows malicious nodes to be cluster heads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03817</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03817</id><created>2016-01-15</created><authors><author><keyname>Zhu</keyname><forenames>Weining</forenames></author></authors><title>Entity-oriented spatial coding and discrete topological spatial
  relations</title><categories>cs.CG cs.DM math.CO math.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a newly proposed spatial data model - spatial chromatic model (SCM),
we developed a spatial coding scheme, called full-coded ordinary arranged
chromatic diagram (full-OACD). Full-OACD is a type of spatial tessellation,
where space is partitioned into a number of subspaces such as cells, edges, and
vertexes. These subspaces are called spatial particles and assigned with unique
codes - chromatic codes. The generation, structures, computations, and
properties of full-OACD are introduced and relations between chromatic codes
and particle spatial topology are investigated, indicating that chromatic codes
provide a potential useful and meaningful tool not only for spatial analysis in
geographical information science, but also for other relevant disciplines such
as discrete mathematics, topology, and computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03821</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03821</id><created>2016-01-15</created><updated>2016-01-25</updated><authors><author><keyname>Zhang</keyname><forenames>Guangcong</forenames></author><author><keyname>Lilly</keyname><forenames>Mason J.</forenames></author><author><keyname>Vela</keyname><forenames>Patricio A.</forenames></author></authors><title>Learning Binary Features Online from Motion Dynamics for Incremental
  Loop-Closure Detection and Place Recognition</title><categories>cs.CV</categories><comments>Accepted to 2016 ICRA (IEEE International Conference on Robotics and
  Automation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple yet effective approach to learn visual features
online for improving loop-closure detection and place recognition, based on
bag-of-words frameworks. The approach learns a codeword in bag-of-words model
from a pair of matched features from two consecutive frames, such that the
codeword has temporally-derived perspective invariance to camera motion. The
learning algorithm is efficient: the binary descriptor is generated from the
mean image patch, and the mask is learned based on discriminative projection by
minimizing the intra-class distances among the learned feature and the two
original features. A codeword for bag-of-words models is generated by packaging
the learned descriptor and mask, with a masked Hamming distance defined to
measure the distance between two codewords. The geometric properties of the
learned codewords are then mathematically justified. In addition, hypothesis
constraints are imposed through temporal consistency in matched codewords,
which improves precision. The approach, integrated in an incremental
bag-of-words system, is validated on multiple benchmark data sets and compared
to state-of-the-art methods. Experiments demonstrate improved precision/recall
outperforming state of the art with little loss in runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03822</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03822</id><created>2016-01-15</created><authors><author><keyname>Keshavarz</keyname><forenames>Hossein</forenames></author><author><keyname>Scott</keyname><forenames>Clayton</forenames></author><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author></authors><title>On the consistency of inversion-free parameter estimation for Gaussian
  random fields</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>34 pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian random fields are a powerful tool for modeling environmental
processes. For high dimensional samples, classical approaches for estimating
the covariance parameters require highly challenging and massive computations,
such as the evaluation of the Cholesky factorization or solving linear systems.
Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast and
scalable algorithm which does not need such burdensome computations. The main
focus of this article is to study the asymptotic behavior of the algorithm of
Anitescu et al. (ACS) for regular and irregular grids in the increasing domain
setting. Consistency, minimax optimality and asymptotic normality of this
algorithm are proved under mild differentiability conditions on the covariance
function. Despite the fact that ACS's method entails a non-concave
maximization, our results hold for any stationary point of the objective
function. A numerical study is presented to evaluate the efficiency of this
algorithm for large data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03829</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03829</id><created>2016-01-15</created><authors><author><keyname>Ren</keyname><forenames>Binyin</forenames></author><author><keyname>Wang</keyname><forenames>Mao</forenames></author><author><keyname>Zhang</keyname><forenames>Jingjing</forenames></author><author><keyname>Yang</keyname><forenames>Wenjie</forenames></author><author><keyname>Zou</keyname><forenames>Jun</forenames></author><author><keyname>Hua</keyname><forenames>Min</forenames></author></authors><title>Cellular Communications on License-Exempt Spectrum: A Tutorial</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A traditional cellular system (e.g., LTE) operates only on the licensed
spectrum. This tutorial explains the concept of cellular communications on both
licensed and license-exempt spectrum under a unified architecture. The purpose
to extend a cellular system into the bandwidth-rich license-exempt spectrum is
to form a larger cellular network for all spectrum types. This would result in
an ultimate mobile converged cellular network. This tutorial examines the
benefits of this concept, the technical challenges, and provides a conceptual
LTE-based design example that helps to show how a traditional cellular system
like the LTE can adapt itself to a different spectrum type, conform to the
regulatory requirements, and harmoniously co-exist with the incumbent systems
such as Wi-Fi. In order to cope with the interference and regulation rules on
license-exempt spectrum, a special medium access mechanism is introduced into
the existing LTE transmission frame structure to exploit the full benefits of
coordinated and managed cellular architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03830</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03830</id><created>2016-01-15</created><authors><author><keyname>Azimi</keyname><forenames>Seyyed Mohammadreza</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Sahin</keyname><forenames>Onur</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Ultra-Reliable Cloud Mobile Computing with Service Composition and
  Superposition Coding</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures, To be presented at CISS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging requirement for 5G systems is the ability to provide wireless
ultra-reliable communication (URC) services with close-to-full availability for
cloud-based applications. Among such applications, a prominent role is expected
to be played by mobile cloud computing (MCC), that is, by the offloading of
computationally intensive tasks from mobile devices to the cloud. MCC allows
battery-limited devices to run sophisticated applications, such as for gaming
or for the &quot;tactile&quot; internet. This paper proposes to apply the framework of
reliable service composition to the problem of optimal task offloading in MCC
over fading channels, with the aim of providing layered, or composable,
services at differentiated reliability levels. Inter-layer optimization
problems, encompassing offloading decisions and communication resources, are
formulated and addressed by means of successive convex approximation methods.
The numerical results demonstrate the energy savings that can be obtained by a
joint allocation of computing and communication resources, as well as the
advantages of layered coding at the physical layer and the impact of channel
conditions on the offloading decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03835</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03835</id><created>2016-01-15</created><authors><author><keyname>Liu</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Yangjia</forenames></author><author><keyname>Wang</keyname><forenames>Shuling</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author></authors><title>A Theorem Prover for Quantum Hoare Logic and Its Applications</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum Hoare Logic (QHL) was introduced in Ying's work to specify and reason
about quantum programs. In this paper, we implement a theorem prover for QHL
based on Isabelle/HOL. By applying the theorem prover, verifying a quantum
program against a specification is transformed equivalently into an order
relation between matrices. Due to the limitation of Isabelle/HOL, the
calculation of the order relation is solved by calling an outside oracle
written in Python. To the best of our knowledge, this is the first theorem
prover for quantum programs. To demonstrate its power, the correctness of two
well-known quantum algorithms, i.e., Grover Quantum Search and Quantum Phase
Estimation (the key step in Shor's quantum algorithm of factoring in polynomial
time) are proved using the theorem prover. These are the first mechanized
proofs for both of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03854</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03854</id><created>2016-01-15</created><authors><author><keyname>Liaqat</keyname><forenames>Misbah</forenames></author><author><keyname>Ninoriya</keyname><forenames>Shalini</forenames></author><author><keyname>Shuja</keyname><forenames>Junaid</forenames></author><author><keyname>Ahmad</keyname><forenames>Raja Wasim</forenames></author><author><keyname>Gani</keyname><forenames>Abdullah</forenames></author></authors><title>Virtual Machine Migration Enabled Cloud Resource Management: A
  Challenging Task</title><categories>cs.DC</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization technology reduces cloud operational cost by increasing cloud
resource utilization level. The incorporation of virtualization within cloud
data centers can severely degrade cloud performance if not properly managed.
Virtual machine (VM) migration is a method that assists cloud service providers
to efficiently manage cloud resources while eliminating the need of human
supervision. VM migration methodology migrates current-hosted workload from one
server to another by either employing live or non-live migration pattern. In
comparison to non-live migration, live migration does not suspend application
services prior to VM migration process. VM migration enables cloud operators to
achieve various resource management goals, such as, green computing, load
balancing, fault management, and real time server maintenance. In this paper,
we have thoroughly surveyed VM migration methods and applications. We have
briefly discussed VM migration applications. Some open research issues have
been highlighted to represent future challenges in this domain. A queue based
migration model has been proposed and discussed to efficiently migrate VM
memory pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03855</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03855</id><created>2016-01-15</created><authors><author><keyname>Gajane</keyname><forenames>Pratik</forenames><affiliation>FT R and D</affiliation></author><author><keyname>Urvoy</keyname><forenames>Tanguy</forenames><affiliation>FT R and D</affiliation></author><author><keyname>Cl&#xe9;rot</keyname><forenames>Fabrice</forenames><affiliation>FT R and D</affiliation></author></authors><title>A Relative Exponential Weighing Algorithm for Adversarial Utility-based
  Dueling Bandits</title><categories>cs.LG</categories><proxy>ccsd</proxy><journal-ref>The 32nd International Conference on Machine Learning, Jul 2015,
  Lille, France. 37, pp.218-227, Proceedings of The 32nd International
  Conference on Machine Learning</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the K-armed dueling bandit problem which is a variation of the
classical Multi-Armed Bandit (MAB) problem in which the learner receives only
relative feedback about the selected pairs of arms. We propose a new algorithm
called Relative Exponential-weight algorithm for Exploration and Exploitation
(REX3) to handle the adversarial utility-based formulation of this problem.
This algorithm is a non-trivial extension of the Exponential-weight algorithm
for Exploration and Exploitation (EXP3) algorithm. We prove a finite time
expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a
general lower bound of order omega(sqrt(KT)). At the end, we provide
experimental results using real data from information retrieval applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03872</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03872</id><created>2016-01-15</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Subba</keyname><forenames>Lawan Thamsuhang</forenames></author><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Container-Based Cloud Virtual Machine Benchmarking</title><categories>cs.DC</categories><comments>Accepted to the IEEE International Conference on Cloud Engineering
  (IEEE IC2E), Berlin, Germany, 2016 - 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the availability of a wide range of cloud Virtual Machines (VMs) it is
difficult to determine which VMs can maximise the performance of an
application. Benchmarking is commonly used to this end for capturing the
performance of VMs. Most cloud benchmarking techniques are typically
heavyweight - time consuming processes which have to benchmark the entire VM in
order to obtain accurate benchmark data. Such benchmarks cannot be used in
real-time on the cloud and incur extra costs even before an application is
deployed.
  In this paper, we present lightweight cloud benchmarking techniques that
execute quickly and can be used in near real-time on the cloud. The exploration
of lightweight benchmarking techniques are facilitated by the development of
DocLite - Docker Container-based Lightweight Benchmarking. DocLite is built on
the Docker container technology which allows a user-defined portion (such as
memory size and the number of CPU cores) of the VM to be benchmarked. DocLite
operates in two modes, in the first mode, containers are used to benchmark a
small portion of the VM to generate performance ranks. In the second mode,
historic benchmark data is used along with the first mode as a hybrid to
generate VM ranks. The generated ranks are evaluated against three scientific
high-performance computing applications. The proposed techniques are up to 91
times faster than a heavyweight technique which benchmarks the entire VM. It is
observed that the first mode can generate ranks with over 90% and 86% accuracy
for sequential and parallel execution of an application. The hybrid mode
improves the correlation slightly but the first mode is sufficient for
benchmarking cloud VMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03874</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03874</id><created>2016-01-15</created><updated>2016-02-01</updated><authors><author><keyname>Szalachowski</keyname><forenames>Pawel</forenames></author><author><keyname>Chuat</keyname><forenames>Laurent</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author></authors><title>PKI Safety Net (PKISN): Addressing the Too-Big-to-Be-Revoked Problem of
  the TLS Ecosystem</title><categories>cs.CR</categories><comments>IEEE EuroS&amp;P 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a public-key infrastructure (PKI), clients must have an efficient and
secure way to determine whether a certificate was revoked (by an entity
considered as legitimate to do so), while preserving user privacy. A few
certification authorities (CAs) are currently responsible for the issuance of
the large majority of TLS certificates. These certificates are considered valid
only if the certificate of the issuing CA is also valid. The certificates of
these important CAs are effectively too big to be revoked, as revoking them
would result in massive collateral damage. To solve this problem, we redesign
the current revocation system with a novel approach that we call PKI Safety Net
(PKISN), which uses publicly accessible logs to store certificates (in the
spirit of Certificate Transparency) and revocations. The proposed system
extends existing mechanisms, which enables simple deployment. Moreover, we
present a complete implementation and evaluation of our scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03876</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03876</id><created>2016-01-15</created><authors><author><keyname>Destounis</keyname><forenames>Apostolos</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios S.</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Iordanis</forenames></author></authors><title>Streaming Big Data meets Backpressure in Distributed Network Computation</title><categories>cs.NI</categories><comments>24 pages, 5 figures, accepted in IEEE INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study network response to queries that require computation of remotely
located data and seek to characterize the performance limits in terms of
maximum sustainable query rate that can be satisfied. The available resources
include (i) a communication network graph with links over which data is routed,
(ii) computation nodes, over which computation load is balanced, and (iii)
network nodes that need to schedule raw and processed data transmissions. Our
aim is to design a universal methodology and distributed algorithm to
adaptively allocate resources in order to support maximum query rate. The
proposed algorithms extend in a nontrivial way the backpressure (BP) algorithm
to take into account computations operated over query streams. They contribute
to the fundamental understanding of network computation performance limits when
the query rate is limited by both the communication bandwidth and the
computation capacity, a classical setting that arises in streaming big data
applications in network clouds and fogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03890</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03890</id><created>2016-01-15</created><updated>2016-02-12</updated><authors><author><keyname>Xue</keyname><forenames>Hongyang</forenames></author><author><keyname>Cai</keyname><forenames>Deng</forenames></author></authors><title>Stereo Matching by Joint Energy Minimization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [18], Mozerov et al. propose to perform stereo matching as a two-step
energy minimization problem. For the first step they solve a fully connected
MRF model. And in the next step the marginal output is employed as the unary
cost for a locally connected MRF model.
  In this paper we intend to combine the two steps of energy minimization in
order to improve stereo matching results. We observe that the fully connected
MRF leads to smoother disparity maps, while the locally connected MRF achieves
superior results in fine-structured regions. Thus we propose to jointly solve
the fully connected and locally connected models, taking both their advantages
into account. The joint model is solved by mean field approximations. While
remaining efficient, our joint model outperforms the two-step energy
minimization approach in both time and estimation error on the Middlebury
stereo benchmark v3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03892</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03892</id><created>2016-01-15</created><authors><author><keyname>Cafaro</keyname><forenames>Massimo</forenames></author><author><keyname>Pulimeno</keyname><forenames>Marco</forenames></author><author><keyname>Epicoco</keyname><forenames>Italo</forenames></author><author><keyname>Aloisio</keyname><forenames>Giovanni</forenames></author></authors><title>Mining frequent items in the time fading model</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present FDCMSS, a new sketch based algorithm for mining frequent items in
data streams. The algorithm cleverly combines key ideas borrowed from forward
decay, the Count-Min and the Space Saving algorithms. It works in the time
fading model, mining data streams according to the cash register model. We
formally prove its correctness and show, through extensive experimental
results, that our algorithm outperforms $\lambda$-HCount, a recently developed
algorithm, with regard to speed, space used, precision attained and error
committed on both synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03896</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03896</id><created>2016-01-15</created><authors><author><keyname>Bernardi</keyname><forenames>Raffaella</forenames></author><author><keyname>Cakici</keyname><forenames>Ruket</forenames></author><author><keyname>Elliott</keyname><forenames>Desmond</forenames></author><author><keyname>Erdem</keyname><forenames>Aykut</forenames></author><author><keyname>Erdem</keyname><forenames>Erkut</forenames></author><author><keyname>Ikizler-Cinbis</keyname><forenames>Nazli</forenames></author><author><keyname>Keller</keyname><forenames>Frank</forenames></author><author><keyname>Muscat</keyname><forenames>Adrian</forenames></author><author><keyname>Plank</keyname><forenames>Barbara</forenames></author></authors><title>Automatic Description Generation from Images: A Survey of Models,
  Datasets, and Evaluation Measures</title><categories>cs.CL cs.CV</categories><comments>To appear in JAIR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic description generation from natural images is a challenging problem
that has recently received a large amount of interest from the computer vision
and natural language processing communities. In this survey, we classify the
existing approaches based on how they conceptualize this problem, viz., models
that cast description as either generation problem or as a retrieval problem
over a visual or multimodal representational space. We provide a detailed
review of existing models, highlighting their advantages and disadvantages.
Moreover, we give an overview of the benchmark image datasets and the
evaluation measures that have been developed to assess the quality of
machine-generated image descriptions. Finally we extrapolate future directions
in the area of automatic image description generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03907</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03907</id><created>2016-01-15</created><updated>2016-01-17</updated><authors><author><keyname>Sauer</keyname><forenames>Roger A.</forenames></author><author><keyname>Duong</keyname><forenames>Thang X.</forenames></author><author><keyname>Mandadapu</keyname><forenames>Kranthi K.</forenames></author><author><keyname>Steigmann</keyname><forenames>David J.</forenames></author></authors><title>A stabilized finite element formulation for liquid shells and its
  application to lipid bilayers</title><categories>cs.CE cond-mat.soft</categories><comments>Corrected typo in axes of Fig.3, results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new finite element (FE) formulation for liquid shells
that is based on an explicit, 3D surface discretization using $C^1$-continuous
finite elements constructed from NURBS interpolation. Both displacement-based
and mixed FE formulations are proposed. The latter is needed for
area-incompressible material behavior, where penalty-type regularizations can
lead to misleading results. In order to obtain quasi-static solutions, several
numerical stabilization schemes are proposed based on either stiffness,
viscosity or projection. Several numerical examples are considered in order to
illustrate the accuracy and the capabilities of the proposed formulation, and
to compare the different stabilization schemes. The presented formulation is
capable of simulating non-trivial surface shapes associated with tube formation
and protein-induced budding of lipid bilayers. In the latter case, the
presented formulation yields non-axisymmetric solutions, which have not been
observed in previous simulations. It is shown that those non-axisymmetric
shapes are preferred over axisymmetric ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03915</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03915</id><created>2016-01-15</created><authors><author><keyname>Moro</keyname><forenames>Federico</forenames></author><author><keyname>Fontanelli</keyname><forenames>Daniele</forenames></author><author><keyname>Passerone</keyname><forenames>Roberto</forenames></author><author><keyname>Prattichizzo</keyname><forenames>Domenico</forenames></author><author><keyname>Rizzon</keyname><forenames>Luca</forenames></author><author><keyname>Scheggi</keyname><forenames>Stefano</forenames></author><author><keyname>Targher</keyname><forenames>Stefano</forenames></author><author><keyname>De Angeli</keyname><forenames>Antonella</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author></authors><title>Follow, listen, feel and go: alternative guidance systems for a walking
  assistance device</title><categories>cs.RO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose several solutions to guide an older adult along a
safe path using a robotic walking assistant (the c-Walker). We consider four
different possibilities to execute the task. One of them is mechanical, with
the c-Walker playing an active role in setting the course. The other ones are
based on tactile or acoustic stimuli, and suggest a direction of motion that
the user is supposed to take on her own will. We describe the technological
basis for the hardware components implementing the different solutions, and
show specialized path following algorithms for each of them. The paper reports
an extensive user validation activity with a quantitative and qualitative
analysis of the different solutions. In this work, we test our system just with
young participants to establish a safer methodology that will be used in future
studies with older adults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03916</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03916</id><created>2016-01-15</created><authors><author><keyname>Hitschler</keyname><forenames>Julian</forenames></author><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author></authors><title>Multimodal Pivots for Image Caption Translation</title><categories>cs.CL</categories><comments>in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to improve statistical machine translation of image
descriptions by multimodal pivots defined in visual space. Image similarity is
computed by a convolutional neural network and incorporated into a target-side
translation memory retrieval model where descriptions of most similar images
are used to rerank translation outputs. Our approach does not depend on the
availability of in-domain parallel data and achieves improvements of 1.4 BLEU
over strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03921</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03921</id><created>2016-01-15</created><authors><author><keyname>Evans</keyname><forenames>Mark</forenames></author><author><keyname>Maglaras</keyname><forenames>Leandros A.</forenames></author><author><keyname>He</keyname><forenames>Ying</forenames></author><author><keyname>Janicke</keyname><forenames>Helge</forenames></author></authors><title>Human Behaviour as an aspect of Cyber Security Assurance</title><categories>cs.CR cs.CY</categories><comments>22 pages, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There continue to be numerous breaches publicised pertaining to cyber
security despite security practices being applied within industry for many
years. This article is intended to be the first in a number of articles as
research into cyber security assurance processes. This article is compiled
based on current research related to cyber security assurance and the impact of
the human element on it. The objective of this work is to identify elements of
cyber security that would benefit from further research and development based
on the literature review findings. The results outlined in this article present
a need for the cyber security field to look in to established industry areas to
benefit from effective practices such as human reliability assessment, along
with improved methods of validation such as statistical quality control in
order to obtain true assurance. The article proposes the development of a
framework that will be based upon defined and repeatable quantification,
specifically relating to the range of human aspect tasks that provide, or are
intended not to negatively affect cyber security posture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03925</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03925</id><created>2016-01-15</created><authors><author><keyname>Shuai</keyname><forenames>Wenjing</forenames></author><author><keyname>Maill&#xe9;</keyname><forenames>Patrick</forenames></author><author><keyname>Pelov</keyname><forenames>Alexander</forenames></author></authors><title>Charging Electric Vehicles in the Smart City: A Survey of Economy-driven
  Approaches</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric Vehicles (EVs), as their penetration increases, are not only
challenging the sustainability of the power grid, but also stimulating and
promoting its upgrading. Indeed, EVs can actively reinforce the development of
the Smart Grid if their charging processes are properly coordinated through
two-way communications, possibly benefiting all types of actors.
  Because grid systems involve a large number of actors with nonaligned
objectives, we focus on the economic and incentive aspects, where each actor
behaves in its own interest. We indeed believe that the market structure will
directly impact the actors' behaviors, and as a result the total benefits that
the presence of EVs can earn the society, hence the need for a careful design.
This survey provides an overview of economic models considering unidirectional
energy flows, but also bidirectional energy flows, i.e., with EVs temporarily
providing energy to the grid. We describe and compare the main approaches,
summarize the requirements on the supporting communication systems, and propose
a classification to highlight the most important results and lacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03926</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03926</id><created>2016-01-15</created><authors><author><keyname>Leconte</keyname><forenames>Mathieu</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Gkatzikis</keyname><forenames>Lazaros</forenames></author><author><keyname>Draief</keyname><forenames>Moez</forenames></author><author><keyname>Vassilaras</keyname><forenames>Spyridon</forenames></author><author><keyname>Chouvardas</keyname><forenames>Symeon</forenames></author></authors><title>Placing Dynamic Content in Caches with Small Population</title><categories>cs.NI</categories><comments>11 pages, 7 figures, accepted in IEEE INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a fundamental limitation for the adoption of caching for
wireless access networks due to small population sizes. This shortcoming is due
to two main challenges: (i) making timely estimates of varying content
popularity and (ii) inferring popular content from small samples. We propose a
framework which alleviates such limitations.
  To timely estimate varying popularity in a context of a single cache we
propose an Age-Based Threshold (ABT) policy which caches all contents requested
more times than a threshold $\widetilde N(\tau)$, where $\tau$ is the content
age. We show that ABT is asymptotically hit rate optimal in the many contents
regime, which allows us to obtain the first characterization of the optimal
performance of a caching system in a dynamic context. We then address small
sample sizes focusing on $L$ local caches and one global cache. On the one hand
we show that the global cache learns L times faster by aggregating all requests
from local caches, which improves hit rates. On the other hand, aggregation
washes out local characteristics of correlated traffic which penalizes hit
rate. This motivates coordination mechanisms which combine global learning of
popularity scores in clusters and LRU with prefetching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03928</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03928</id><created>2016-01-15</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>On an application of multidimensional arrays</title><categories>cs.DS math.CO</categories><msc-class>68P05, 05B20</msc-class><journal-ref>British Journal of Mathematics &amp; Computer Science, ISSN:
  2231-0851, 11(4): 1-7, 2015</journal-ref><doi>10.9734/BJMCS/2015/20372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses some difficulties in the implementation of
combinatorial algorithms associated with the choice of all elements with
certain properties among the elements of a set with great cardinality.The
problem has been resolved by using multidimensional arrays. Illustration of the
method is a solution of the problem of obtaining one representative from each
equivalence class with respect to the described in the article equivalence
relation in the set of all $m\sim n$ binary matrices. This equivalence relation
has an application in the mathematical modeling in the textile industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03937</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03937</id><created>2016-01-15</created><authors><author><keyname>Foss</keyname><forenames>Sergey</forenames></author><author><keyname>Kim</keyname><forenames>Dmitriy</forenames></author><author><keyname>Turlikov</keyname><forenames>Andrey</forenames></author></authors><title>Stability and instability of a random multiple access model with
  adaptive energy harvesting</title><categories>math.PR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for the classical synchronised multiple access system
with a single transmission channel and a randomised transmission protocol
(ALOHA). We assume in addition that there is an energy harvesting mechanism,
and any message transmission requires a unit of energy. Units of energy arrive
randomly and independently of anything else. We analyse stability and
instability conditions for this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03945</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03945</id><created>2016-01-15</created><authors><author><keyname>Escalante-B.</keyname><forenames>Alberto N.</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>Improved graph-based SFA: Information preservation complements the
  slowness principle</title><categories>cs.CV cs.LG stat.ML</categories><comments>40 pages, 9 figures, 9 tables, submitted to Pattern Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slow feature analysis (SFA) is an unsupervised-learning algorithm that
extracts slowly varying features from a multi-dimensional time series. A
supervised extension to SFA for classification and regression is graph-based
SFA (GSFA). GSFA is based on the preservation of similarities, which are
specified by a graph structure derived from the labels. It has been shown that
hierarchical GSFA (HGSFA) allows learning from images and other
high-dimensional data. The feature space spanned by HGSFA is complex due to the
composition of the nonlinearities of the nodes in the network. However, we show
that the network discards useful information prematurely before it reaches
higher nodes, resulting in suboptimal global slowness and an under-exploited
feature space.
  To counteract these problems, we propose an extension called hierarchical
information-preserving GSFA (HiGSFA), where information preservation
complements the slowness-maximization goal. We build a 10-layer HiGSFA network
to estimate human age from facial photographs of the MORPH-II database,
achieving a mean absolute error of 3.50 years, improving the state-of-the-art
performance. HiGSFA and HGSFA support multiple-labels and offer a rich feature
space, feed-forward training, and linear complexity in the number of samples
and dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature
slowness, estimation accuracy and input reconstruction, giving rise to a
promising hierarchical supervised-learning approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03958</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03958</id><created>2016-01-15</created><authors><author><keyname>Chamberlain</keyname><forenames>Benjamin Paul</forenames></author><author><keyname>Levy-Kramer</keyname><forenames>Josh</forenames></author><author><keyname>Humby</keyname><forenames>Clive</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>Real-Time Association Mining in Large Social Networks</title><categories>cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing realisation that to combat the waning effectiveness of
traditional marketing, social media platform owners need to find new ways to
monetise their data. Social media data contains rich information describing how
real world entities relate to each other. Understanding the allegiances,
communities and structure of key entities is of vital importance for decision
support in a swathe of industries that have hitherto relied on expensive, small
scale survey data. In this paper, we present a real-time method to query and
visualise regions of networks that are closely related to a set of input
vertices. The input vertices can define an industry, political party, sport
etc. The key idea is that in large digital social networks measuring similarity
via direct connections between nodes is not robust, but that robust
similarities between nodes can be attained through the similarity of their
neighbourhood graphs. We are able to achieve real-time performance by
compressing the neighbourhood graphs using minhash signatures and facilitate
rapid queries through Locality Sensitive Hashing. These techniques reduce query
times from hours using industrial desktop machines to milliseconds on standard
laptops. Our method allows analysts to interactively explore strongly
associated regions of large networks in real time. Our work has been deployed
in software that is actively used by analysts to understand social network
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03976</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03976</id><created>2016-01-15</created><authors><author><keyname>Hargreaves</keyname><forenames>Eduardo</forenames></author><author><keyname>Rodrigues</keyname><forenames>Paulo H De Aguiar</forenames></author><author><keyname>Menasch&#xe9;</keyname><forenames>Daniel S.</forenames></author></authors><title>Modeling and Analysis of Converged Network-Cloud Services</title><categories>cs.NI</categories><comments>XIII Workshop em Clouds e Aplica\c{c}\~oes (WCGA2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks connecting distributed cloud services through multiple data centers
are called cloud networks. These types of networks play a crucial role in cloud
computing and a holistic performance evaluation is essential before planning a
converged network-cloud environment. We analyze a specific case where some
resources can be centralized in one datacenter or distributed among multiple
data centers. The economy of scale in centralizing resources in a sin- gle pool
of resources can be overcome by an increase in communication costs. We propose
an analytical model to evaluate tradeoffs in terms of application requirements,
usage patterns, number of resources and communication costs. We numerically
evaluate the proposed model in a case study inspired by the oil and gas
industry, indicating how to cope with the tradeoff between statisti- cal
multiplexing advantages of centralization and the corresponding increase in
communication infrastructure costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03980</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03980</id><created>2016-01-15</created><authors><author><keyname>Kathiravelu</keyname><forenames>Pradeeban</forenames></author></authors><title>An Elastic Middleware Platform for Concurrent and Distributed Cloud and
  MapReduce Simulations</title><categories>cs.DC</categories><comments>Thesis to obtain the Master of Science Degree in Information Systems
  and Computer Engineering, Instituto Superior Tecnico, Universidade de Lisboa.
  2014 September</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing researches involve a tremendous amount of entities such as
users, applications, and virtual machines. Due to the limited access and often
variable availability of such resources, researchers have their prototypes
tested against the simulation environments, opposed to the real cloud
environments. Existing cloud simulation environments such as CloudSim and
EmuSim are executed sequentially, where a more advanced cloud simulation tool
could be created extending them, leveraging the latest technologies as well as
the availability of multi-core computers and the clusters in the research
laboratories. While computing has been evolving with multi-core programming,
MapReduce paradigms, and middleware platforms, cloud and MapReduce simulations
still fail to exploit these developments themselves. This research develops
Cloud2Sim, which tries to fill the gap between the simulations and the actual
technology that they are trying to simulate.
  First, Cloud2Sim provides a concurrent and distributed cloud simulator, by
extending CloudSim cloud simulator, using Hazelcast in-memory key-value store.
Then, it also provides a quick assessment to MapReduce implementations of
Hazelcast and Infinispan, adaptively distributing the execution to a cluster,
providing means of simulating MapReduce executions. The dynamic scaler solution
scales out the cloud and MapReduce simulations to multiple nodes running
Hazelcast and Infinispan, based on load. The distributed execution model and
adaptive scaling solution could be leveraged as a general purpose auto scaler
middleware for a multi-tenanted deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03984</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03984</id><created>2016-01-15</created><authors><author><keyname>Wachs</keyname><forenames>Matthias</forenames></author><author><keyname>Herold</keyname><forenames>Nadine</forenames></author><author><keyname>Posselt</keyname><forenames>Stephan-A.</forenames></author><author><keyname>Dold</keyname><forenames>Florian</forenames></author><author><keyname>Carle</keyname><forenames>Georg</forenames></author></authors><title>GPLMT: A Lightweight Experimentation and Testbed Management Framework</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conducting experiments in federated, distributed, and heterogeneous testbeds
is a challenging task for researchers. Researchers have to take care of the
whole experiment life cycle, ensure the reproducibility of each run, and the
comparability of the results. We present GPLMT, a flexible and lightweight
framework for managing testbeds and the experiment life cycle. \gplmt provides
an intuitive way to formalize experiments. The resulting experiment description
is portable across varying experimentation platforms. GPLMT enables researchers
to manage and control networked testbeds and resources, and conduct experiments
on large-scale, heterogeneous, and distributed testbeds. We state the
requirements and the design of GPLMT, describe the challenges of developing and
using such a tool, and present selected user studies along with their
experience of using GPLMT in varying scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03998</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.03998</id><created>2016-01-15</created><authors><author><keyname>Zander</keyname><forenames>Stefan</forenames></author><author><keyname>Heppner</keyname><forenames>Georg</forenames></author><author><keyname>Neugschwandtner</keyname><forenames>Georg</forenames></author><author><keyname>Awad</keyname><forenames>Ramez</forenames></author><author><keyname>Essinger</keyname><forenames>Marc</forenames></author><author><keyname>Ahmed</keyname><forenames>Nadia</forenames></author></authors><title>A Model-Driven Engineering Approach for ROS using Ontological Semantics</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2015 (arXiv:1601.00877), Stefan Zander, Georg
  Heppner, Georg Neugschwandtner, Ramez Awad, Marc Essinger and Nadia Ahmed: A
  Model-Driven Engineering Approach for ROS using Ontological Semantics</comments><report-no>DSLRob/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel ontology-driven software engineering approach for
the development of industrial robotics control software. It introduces the
ReApp architecture that synthesizes model-driven engineering with semantic
technologies to facilitate the development and reuse of ROS-based components
and applications. In ReApp, we show how different ontological classification
systems for hardware, software, and capabilities help developers in discovering
suitable software components for their tasks and in applying them correctly.
The proposed model-driven tooling enables developers to work at higher
abstraction levels and fosters automatic code generation. It is underpinned by
ontologies to minimize discontinuities in the development workflow, with an
integrated development environment presenting a seamless interface to the user.
First results show the viability and synergy of the selected approach when
searching for or developing software with reuse in mind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04011</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04011</id><created>2016-01-15</created><updated>2016-02-02</updated><authors><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Tightening the Sample Complexity of Empirical Risk Minimization via
  Preconditioned Stability</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tighten the sample complexity of empirical risk minimization (ERM)
associated with a class of generalized linear models that include linear and
logistic regression. In particular, we conclude that ERM attains the optimal
sample complexity for linear regression. Our analysis relies on a new notion of
stability, called preconditioned stability, which may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04012</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04012</id><created>2016-01-15</created><authors><author><keyname>Kalita</keyname><forenames>Jugal</forenames></author></authors><title>Detecting and Extracting Events from Text Documents</title><categories>cs.CL</categories><comments>This is work in progress. Please email jkalita@uccs.edu with any
  comments for improvement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Events of various kinds are mentioned and discussed in text documents,
whether they are books, news articles, blogs or microblog feeds. The paper
starts by giving an overview of how events are treated in linguistics and
philosophy. We follow this discussion by surveying how events and associated
information are handled in computationally. In particular, we look at how
textual documents can be mined to extract events and ancillary information.
These days, it is mostly through the application of various machine learning
techniques. We also discuss applications of event detection and extraction
systems, particularly in summarization, in the medical domain and in the
context of Twitter posts. We end the paper with a discussion of challenges and
future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04017</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04017</id><created>2016-01-15</created><authors><author><keyname>Maier</keyname><forenames>Tobias</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Dementiev</keyname><forenames>Roman</forenames></author></authors><title>Concurrent Hash Tables: Fast and General?(!)</title><categories>cs.DS</categories><acm-class>D.1.3; E.1; E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent hash tables are one of the most important concurrent data
structures with numerous applications. Since hash table accesses can dominate
the execution time of the overall application, we need implementations that
achieve good speedup. Unfortunately, currently available concurrent hashing
libraries turn out to be far away from this requirement in particular when
contention on some elements occurs.
  Our starting point for better performing data structures is a fast and simple
lock-free concurrent hash table based on linear probing that is limited to
word-sized key-value types and does not support dynamic size adaptation. We
explain how to lift these limitations in a provably scalable way and
demonstrate that dynamic growing has a performance overhead comparable to the
same generalization in sequential hash tables.
  We perform extensive experiments comparing the performance of our
implementations with six of the most widely used concurrent hash tables. Ours
are considerably faster than the best algorithms with similar restrictions and
an order of magnitude faster than the best more general tables. In some extreme
cases, the difference even approaches four orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04023</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04023</id><created>2016-01-15</created><updated>2016-01-19</updated><authors><author><keyname>Bazrafshan</keyname><forenames>Mohammadhafez</forenames></author><author><keyname>Gatsis</keyname><forenames>Nikolaos</forenames></author></authors><title>Decentralized Stochastic Optimal Power Flow in Radial Networks with
  Distributed Generation</title><categories>cs.SY</categories><comments>Journal paper accepted at the IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a power management scheme that jointly optimizes the real
power consumption of programmable loads and reactive power outputs of
photovoltaic (PV) inverters in distribution networks. The premise is to
determine the optimal demand response schedule that accounts for the stochastic
availability of solar power, as well as to control the reactive power
generation or consumption of PV inverters adaptively to the real power
injections of all PV units. These uncertain real power injections by PV units
are modeled as random variables taking values from a finite number of possible
scenarios. Through the use of second order cone relaxation of the power flow
equations, a convex stochastic program is formulated. The objectives are to
minimize the negative user utility, cost of power provision, and thermal
losses, while constraining voltages to remain within specified levels. To find
the global optimum point, a decentralized algorithm is developed via the
alternating direction method of multipliers that results in closed-form updates
per node and per scenario, rendering it suitable to implement in distribution
networks with large number of scenarios. Numerical tests and comparisons with
an alternative deterministic approach are provided for typical residential
distribution networks that confirm the efficiency of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04029</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04029</id><created>2016-01-15</created><authors><author><keyname>Ramos</keyname><forenames>Julian</forenames></author><author><keyname>Li</keyname><forenames>Zhen</forenames></author><author><keyname>Rosas</keyname><forenames>Johana</forenames></author><author><keyname>Banovic</keyname><forenames>Nikola</forenames></author><author><keyname>Mankoff</keyname><forenames>Jennifer</forenames></author><author><keyname>Dey</keyname><forenames>Anind</forenames></author></authors><title>Keyboard Surface Interaction: Making the keyboard into a pointing device</title><categories>cs.HC</categories><comments>8 figures, 12 pages</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pointing devices that reside on the keyboard can reduce the overall time
needed to perform mixed pointing and typing tasks, since the hand of the user
does not have to reach for the pointing device. However, previous
implementations of this kind of device have a higher movement time compared to
the mouse and trackpad due to large error rate, low speed and spatial
resolution. In this paper we introduce Keyboard Surface Interaction (KSI), an
interaction approach that turns the surface of a keyboard into an interaction
surface and allows users to rest their hands on the keyboard at all times to
minimize fatigue. We developed a proof-of-concept implementation, Fingers,
which we optimized over a series of studies. Finally, we evaluated Fingers
against the mouse and trackpad in a user study with 25 participants on a Fitts
law test style, mixed typing and pointing task. Results showed that for users
with more exposure to KSI, our KSI device had better performance (reduced
movement and homing time) and reduced discomfort compared to the trackpad. When
compared to the mouse, KSI had reduced homing time and reduced discomfort, but
increased movement time. This interaction approach is not only a new way to
capitalize on the space on top of the keyboard, but also a call to innovate and
think beyond the touchscreen, touchpad, and mouse as our main pointing devices.
The results of our studies serve as a specification for future KSI devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04033</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04033</id><created>2016-01-15</created><authors><author><keyname>Odena</keyname><forenames>Augustus</forenames></author></authors><title>Faster Asynchronous SGD</title><categories>stat.ML cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous distributed stochastic gradient descent methods have trouble
converging because of stale gradients. A gradient update sent to a parameter
server by a client is stale if the parameters used to calculate that gradient
have since been updated on the server. Approaches have been proposed to
circumvent this problem that quantify staleness in terms of the number of
elapsed updates. In this work, we propose a novel method that quantifies
staleness in terms of moving averages of gradient statistics. We show that this
method outperforms previous methods with respect to convergence speed and
scalability to many clients. We also discuss how an extension to this method
can be used to dramatically reduce bandwidth costs in a distributed training
context. In particular, our method allows reduction of total bandwidth usage by
a factor of 5 with little impact on cost convergence. We also describe (and
link to) a software library that we have used to simulate these algorithms
deterministically on a single machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04036</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04036</id><created>2016-01-15</created><authors><author><keyname>Harper</keyname><forenames>K. Eric</forenames></author><author><keyname>de Gooijer</keyname><forenames>Thijmen</forenames></author><author><keyname>Schmitt</keyname><forenames>Johannes O.</forenames></author><author><keyname>Cox</keyname><forenames>David</forenames></author></authors><title>Microdatabases for the Industrial Internet</title><categories>cs.DB cs.CY</categories><comments>5 pages, 2 figures, pending submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Industrial Internet market is targeted to grow by trillions of US dollars
by the year 2030, driven by adoption, deployment and integration of billions of
intelligent devices and their associated data. This digital expansion faces a
number of significant challenges, including reliable data management, security
and privacy. Realizing the benefits from this evolution is made more difficult
because a typical industrial plant includes multiple vendors and legacy
technology stacks. Aggregating all the raw data to a single data center before
performing analysis increases response times, raising performance concerns in
traditional markets and requiring a compromise between data duplication and
data access performance. Similar to the way microservices can integrate
disparate information technologies without imposing monolithic cross-cutting
architecture impacts, we propose microdatabases to manage the data
heterogeneity of the Industrial Internet while allowing records to be captured
and secured close to the industrial processes, but also be made available near
the applications that can benefit from the data. A microdatabase is an
abstraction of a data store that standardizes and protects the interactions
between distributed data sources, providers and consumers. It integrates an
information model with discoverable object types that can be browsed
interactively and programmatically, and supports repository instances that
evolve with their own lifecycles. The microdatabase abstraction is independent
of technology choice and was designed based on solicitation and review of
industry stakeholder concerns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04037</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04037</id><created>2016-01-15</created><authors><author><keyname>Majumdar</keyname><forenames>Anirudha</forenames></author><author><keyname>Tedrake</keyname><forenames>Russ</forenames></author></authors><title>Funnel Libraries for Real-Time Robust Feedback Motion Planning</title><categories>cs.RO cs.AI cs.SY math.DS math.OC</categories><comments>To be submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of generating motion plans for a robot
that are guaranteed to succeed despite uncertainty in the environment,
parametric model uncertainty, and disturbances. Furthermore, we consider the
case where these plans must be generated in real-time, because constraints such
as obstacles in the environment may not be known until they are perceived (with
a noisy sensor) at runtime. Previous work on feedback motion planning for
nonlinear systems was limited to offline planning due to the computational cost
of safety verification. Here we augment the traditional trajectory library
approach by designing locally stabilizing controllers for each nominal
trajectory in the library and providing guarantees on the resulting closed-loop
systems. We leverage sums-of-squares (SOS) programming to design these locally
stabilizing controllers by explicitly attempting to minimize the size of the
worst case reachable set of the closed-loop system subjected to bounded
disturbances and uncertainty. The reachable sets associated with each
trajectory in the library can be thought of as &quot;funnels&quot; that the system is
guaranteed to remain within. The resulting funnel library is then used to
sequentially compose motion plans at runtime while ensuring the safety of the
robot. A major advantage of the work presented here is that by explicitly
taking into account the effect of uncertainty, the robot can evaluate motion
plans based on how vulnerable they are to disturbances. We demonstrate our
method using thorough simulation experiments of a ground vehicle model
navigating through cluttered environments and also present extensive hardware
experiments validating the approach on a small fixed-wing airplane avoiding
obstacles at high speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04038</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04038</id><created>2016-01-15</created><authors><author><keyname>Schmitz</keyname><forenames>Heinz</forenames></author><author><keyname>Lykourentzou</keyname><forenames>Ioanna</forenames></author></authors><title>It's about time: Online Macrotask Sequencing in Expert Crowdsourcing</title><categories>cs.SI cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the problem of Task Assignment and Sequencing (TAS), which adds
the timeline perspective to expert crowdsourcing optimization. Expert
crowdsourcing involves macrotasks, like document writing, product design, or
web development, which take more time than typical binary microtasks, require
expert skills, assume varying degrees of knowledge over a topic, and require
crowd workers to build on each other's contributions. Current works usually
assume offline optimization models, which consider worker and task arrivals
known and do not take into account the element of time. Realistically however,
time is critical: tasks have deadlines, expert workers are available only at
specific time slots, and worker/task arrivals are not known a-priori. Our work
is the first to address the problem of optimal task sequencing for online,
heterogeneous, time-constrained macrotasks. We propose tas-online, an online
algorithm that aims to complete as many tasks as possible within budget,
required quality and a given timeline, without future input information
regarding job release dates or worker availabilities. Results, comparing
tas-online to four typical benchmarks, show that it achieves more completed
jobs, lower flow times and higher job quality. This work has practical
implications for improving the Quality of Service of current crowdsourcing
platforms, allowing them to offer cost, quality and time improvements for
expert tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04041</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04041</id><created>2016-01-15</created><authors><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Krichene</keyname><forenames>Walid</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre M.</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Differential Privacy of Populations in Routing Games</title><categories>cs.CR math.OC</categories><comments>Extended draft of paper that appears in 2015 IEEE CDC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As our ground transportation infrastructure modernizes, the large amount of
data being measured, transmitted, and stored motivates an analysis of the
privacy aspect of these emerging cyber-physical technologies. In this paper, we
consider privacy in the routing game, where the origins and destinations of
drivers are considered private. This is motivated by the fact that this
spatiotemporal information can easily be used as the basis for inferences for a
person's activities. More specifically, we consider the differential privacy of
the mapping from the amount of flow for each origin-destination pair to the
traffic flow measurements on each link of a traffic network. We use a
stochastic online learning framework for the population dynamics, which is
known to converge to the Nash equilibrium of the routing game. We analyze the
sensitivity of this process and provide theoretical guarantees on the
convergence rates as well as differential privacy values for these models. We
confirm these with simulations on a small example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04059</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04059</id><created>2016-01-15</created><authors><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Lampariello</keyname><forenames>Lorenzo</forenames></author><author><keyname>Song</keyname><forenames>Peiran</forenames></author><author><keyname>Sardellitti</keyname><forenames>Stefania</forenames></author></authors><title>Parallel and Distributed Methods for Nonconvex Optimization--Part II:
  Applications</title><categories>cs.IT cs.DC math.IT</categories><comments>Part I of this paper can be found at http://arxiv.org/abs/1410.4754</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part I of this paper, we proposed and analyzed a novel algorithmic
framework for the minimization of a nonconvex (smooth) objective function,
subject to nonconvex constraints, based on inner convex approximations. This
Part II is devoted to the application of the framework to some resource
allocation problems in communication networks. In particular, we consider two
non-trivial case-study applications, namely: (generalizations of) i) the rate
profile maximization in MIMO interference broadcast networks; and the ii) the
max-min fair multicast multigroup beamforming problem in a multi-cell
environment. We develop a new class of algorithms enjoying the following
distinctive features: i) they are \emph{distributed} across the base stations
(with limited signaling) and lead to subproblems whose solutions are computable
in closed form; and ii) differently from current relaxation-based schemes
(e.g., semidefinite relaxation), they are proved to always converge to
d-stationary solutions of the aforementioned class of nonconvex problems.
Numerical results show that the proposed (distributed) schemes achieve larger
worst-case rates (resp. signal-to-noise interference ratios) than
state-of-the-art centralized ones while having comparable computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04066</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04066</id><created>2016-01-14</created><authors><author><keyname>Andrew</keyname><forenames>Eluyefa Olanrewaju</forenames></author></authors><title>Human Computer Symbiosis</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human Computer Symbiosis is similar to Human Computer Interaction in the
sense that it is about how humans and computer interact with each other. For
this interaction to be made there needs to be a symbiotic relationship between
man and computer. Man can interact with computer in many ways, either just by
typing with the keyboard or surfing the web. The cyber-physical-socio space is
an important aspect to be looked into when referring to the interaction between
man and computer. This paper investigates various aspects related to human
computer symbiosis. Alongside the aspects related to the topic, this paper
would also look into the limitations of Human Computer Symbiosis and evaluate
some previously proposed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04071</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04071</id><created>2016-01-15</created><authors><author><keyname>Kleineberg</keyname><forenames>Kaj-Kolja</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author><author><keyname>Serrano</keyname><forenames>M. Angeles</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Fragkiskos</forenames></author></authors><title>Geometric correlations in real multiplex networks: multidimensional
  communities, trans-layer link prediction, and efficient navigation</title><categories>physics.soc-ph cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real networks often form interacting parts of larger and more complex
systems. Examples can be found in different domains, ranging from the Internet
to structural and functional brain networks. Here, we show that these multiplex
systems are not random combinations of single network layers. Instead, they are
organized in specific ways dictated by hidden geometric correlations between
the individual layers. We find that these correlations are strong in different
real multiplexes, and form a key framework for answering many important
questions. Specifically, we show that these geometric correlations facilitate:
(i) the definition and detection of multidimensional communities, which are
sets of nodes that are simultaneously similar in multiple layers; (ii) accurate
trans-layer link prediction, where connections in one layer can be predicted by
observing the hidden geometric space of another layer; and (iii) efficient
targeted navigation in the multilayer system using only local knowledge, which
outperforms navigation in the single layers only if the geometric correlations
are sufficiently strong. Our findings uncover fundamental organizing principles
behind real multiplexes and can have important applications in diverse domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04075</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04075</id><created>2016-01-15</created><authors><author><keyname>Podgorny</keyname><forenames>Igor A.</forenames></author></authors><title>Modification of Question Writing Style Influences Content Popularity in
  a Social Q&amp;A System</title><categories>cs.IR cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TurboTax AnswerXchange is a social Q&amp;A system supporting users working on
federal and state tax returns. Using 2015 data, we demonstrate that content
popularity (or number of views per AnswerXchange question) can be predicted
with reasonable accuracy based on attributes of the question alone. We also
employ probabilistic topic analysis and uplift modeling to identify question
features with the highest impact on popularity. We demonstrate that content
popularity is driven by behavioral attributes of AnswerXchange users and
depends on complex interactions between search ranking algorithms,
psycholinguistic factors and question writing style. Our findings provide a
rationale for employing popularity predictions to guide the users into
formulating better questions and editing the existing ones. For example,
starting question title with a question word or adding details to the question
increase number of views per question. Similar approach can be applied to
promoting AnswerXchange content indexed by Google to drive organic traffic to
TurboTax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04084</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04084</id><created>2016-01-15</created><authors><author><keyname>Sadoghi</keyname><forenames>Mohammad</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Bishwaranjan</forenames></author><author><keyname>Canim</keyname><forenames>Mustafa</forenames></author></authors><title>L-Store: A Real-time OLTP and OLAP System</title><categories>cs.DB</categories><comments>18 pages, 14 figures, 7 tables</comments><acm-class>H.2; H.2.2; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arguably data is a new natural resource in the enterprise world with an
unprecedented degree of proliferation. But to derive real-time actionable
insights from the data, it is important to bridge the gap between managing the
data that is being updated at a high velocity (i.e., OLTP) and analyzing a
large volume of data (i.e., OLAP). However, there has been a divide where
specialized solutions were often deployed to support either OLTP or OLAP
workloads but not both; thus, limiting the analysis to stale and possibly
irrelevant data. In this paper, we present Lineage-based Data Store (L-Store)
that combines the real-time processing of transactional and analytical
workloads within a single unified engine by introducing a novel lineage-based
storage architecture. We develop a contention-free and lazy staging of columnar
data from a write-optimized form (suitable for OLTP) into a read-optimized form
(suitable for OLAP) in a transactionally consistent approach that also supports
querying and retaining the current and historic data. Our working prototype of
L-Store demonstrates its superiority compared to state-of-the-art approaches
under a comprehensive experimental evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04094</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04094</id><created>2016-01-15</created><authors><author><keyname>Chatterjee</keyname><forenames>Avhishek</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Efficient and Flexible Crowdsourcing of Specialized Tasks with
  Precedence Constraints</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many companies now use crowdsourcing to leverage external (as well as
internal) crowds to perform specialized work, and so methods of improving
efficiency are critical. Tasks in crowdsourcing systems with specialized work
have multiple steps and each step requires multiple skills. Steps may have
different flexibilities in terms of obtaining service from one or multiple
agents, due to varying levels of dependency among parts of steps. Steps of a
task may have precedence constraints among them. Moreover, there are variations
in loads of different types of tasks requiring different skill-sets and
availabilities of different types of agents with different skill-sets.
Considering these constraints together necessitates the design of novel schemes
to allocate steps to agents. In addition, large crowdsourcing systems require
allocation schemes that are simple, fast, decentralized and offer customers
(task requesters) the freedom to choose agents. In this work we study the
performance limits of such crowdsourcing systems and propose efficient
allocation schemes that provably meet the performance limits under these
additional requirements. We demonstrate our algorithms on data from a
crowdsourcing platform run by a non-profit company and show significant
improvements over current practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04098</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04098</id><created>2016-01-15</created><authors><author><keyname>Azimipour</keyname><forenames>Sanaz</forenames></author><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author></authors><title>Lighthouse Principle for Diffusion in Social Networks</title><categories>math.LO cs.LO cs.MA cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article investigates influence relation between two sets of agents in a
social network. It proposes a logical system that captures propositional
properties of this relation valid in all threshold models of social networks
with the same topological structure. The logical system consists of Armstrong
axioms for functional dependence and an additional Lighthouse axiom. The main
results are soundness, completeness, and decidability theorems for this logical
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04102</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04102</id><created>2016-01-15</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Distributed Conjugate Gradient Strategies for Distributed
  Estimation Over Sensor Networks</title><categories>cs.DC cs.IT math.IT</categories><comments>23 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents distributed conjugate gradient algorithms for distributed
parameter estimation and spectrum estimation over wireless sensor networks. In
particular, distributed conventional conjugate gradient (CCG) and modified
conjugate gradient (MCG) are considered, together with incremental and
diffusion adaptive solutions. The distributed CCG and MCG algorithms have an
improved performance in terms of mean square error as compared with least--mean
square (LMS)--based algorithms and a performance that is close to recursive
least--squares (RLS) algorithms. In comparison with existing centralized or
distributed estimation strategies, key features of the proposed algorithms are:
1) more accurate estimates and faster convergence speed can be obtained; 2) the
design of preconditioners for CG algorithms, which have the ability to improve
the performance of the proposed CG algorithms is presented and 3) the proposed
algorithms are implemented in the area of distributed parameter estimation and
spectrum estimation. The performance of the proposed algorithms for distributed
estimation is illustrated via simulations and the resulting algorithms are
distributed, cooperative and able to respond in real time to change in the
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04105</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04105</id><created>2016-01-15</created><authors><author><keyname>Taheriyan</keyname><forenames>Mohsen</forenames></author><author><keyname>Knoblock</keyname><forenames>Craig A.</forenames></author><author><keyname>Szekely</keyname><forenames>Pedro</forenames></author><author><keyname>Ambite</keyname><forenames>Jose Luis</forenames></author></authors><title>Learning the Semantics of Structured Data Sources</title><categories>cs.AI</categories><comments>Web Semantics: Science, Services and Agents on the World Wide Web,
  2016</comments><doi>10.1016/j.websem.2015.12.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information sources such as relational databases, spreadsheets, XML, JSON,
and Web APIs contain a tremendous amount of structured data that can be
leveraged to build and augment knowledge graphs. However, they rarely provide a
semantic model to describe their contents. Semantic models of data sources
represent the implicit meaning of the data by specifying the concepts and the
relationships within the data. Such models are the key ingredients to
automatically publish the data into knowledge graphs. Manually modeling the
semantics of data sources requires significant effort and expertise, and
although desirable, building these models automatically is a challenging
problem. Most of the related work focuses on semantic annotation of the data
fields (source attributes). However, constructing a semantic model that
explicitly describes the relationships between the attributes in addition to
their semantic types is critical.
  We present a novel approach that exploits the knowledge from a domain
ontology and the semantic models of previously modeled sources to automatically
learn a rich semantic model for a new source. This model represents the
semantics of the new source in terms of the concepts and relationships defined
by the domain ontology. Given some sample data from the new source, we leverage
the knowledge in the domain ontology and the known semantic models to construct
a weighted graph that represents the space of plausible semantic models for the
new source. Then, we compute the top k candidate semantic models and suggest to
the user a ranked list of the semantic models for the new source. The approach
takes into account user corrections to learn more accurate semantic models on
future data sources. Our evaluation shows that our method generates expressive
semantic models for data sources and services with minimal user input. ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04114</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04114</id><created>2016-01-15</created><updated>2016-02-04</updated><authors><author><keyname>Mobahi</keyname><forenames>Hossein</forenames></author></authors><title>Training Recurrent Neural Networks by Diffusion</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a new algorithm for training recurrent neural networks
(although ideas are applicable to feedforward networks as well). The algorithm
is derived from a theory in nonconvex optimization related to the diffusion
equation. The contributions made in this work are two fold. First, we show how
some seemingly disconnected mechanisms used in deep learning such as smart
initialization, annealed learning rate, layerwise pretraining, and noise
injection (as done in dropout and SGD) arise naturally and automatically from
this framework, without manually crafting them into the algorithms. Second, we
present some preliminary results on comparing the proposed method against SGD.
It turns out that the new algorithm can achieve similar level of generalization
accuracy of SGD in much fewer number of epochs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04115</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04115</id><created>2016-01-15</created><authors><author><keyname>Ye</keyname><forenames>Chuyang</forenames></author><author><keyname>Zhuo</keyname><forenames>Jiachen</forenames></author><author><keyname>Gullapalli</keyname><forenames>Rao P.</forenames></author><author><keyname>Prince</keyname><forenames>Jerry L.</forenames></author></authors><title>Estimation of Fiber Orientations Using Neighborhood Information</title><categories>cs.CV</categories><comments>Journal paper under review. 35 pages and 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data from diffusion magnetic resonance imaging (dMRI) can be used to
reconstruct fiber tracts, for example, in muscle and white matter. Estimation
of fiber orientations (FOs) is a crucial step in the reconstruction process and
these estimates can be corrupted by noise. In this paper, a new method called
Fiber Orientation Reconstruction using Neighborhood Information (FORNI) is
described and shown to reduce the effects of noise and improve FO estimation
performance by incorporating spatial consistency. FORNI uses a fixed tensor
basis to model the diffusion weighted signals, which has the advantage of
providing an explicit relationship between the basis vectors and the FOs. FO
spatial coherence is encouraged using weighted l1-norm regularization terms,
which contain the interaction of directional information between neighbor
voxels. Data fidelity is encouraged using a squared error between the observed
and reconstructed diffusion weighted signals. After appropriate weighting of
these competing objectives, the resulting objective function is minimized using
a block coordinate descent algorithm, and a straightforward parallelization
strategy is used to speed up processing. Experiments were performed on a
digital crossing phantom, ex vivo tongue dMRI data, and in vivo brain dMRI data
for both qualitative and quantitative evaluation. The results demonstrate that
FORNI improves the quality of FO estimation over other state of the art
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04122</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04122</id><created>2016-01-15</created><authors><author><keyname>Gupta</keyname><forenames>Anindya</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Reduced Complexity Sum-Product Algorithm for Decoding Network Codes and
  In-Network Function Computation</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures and 1 table. arXiv admin note: substantial text
  overlap with arXiv:1510.03634</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the capacity, feasibility and methods to obtain codes for network
coding problems are well studied, the decoding procedure and complexity have
not garnered much attention. In this work, we pose the decoding problem at a
sink node in a network as a marginalize a product function (MPF) problem over a
Boolean semiring and use the sum-product (SP) algorithm on a suitably
constructed factor graph to perform iterative decoding. We use
\textit{traceback} to reduce the number of operations required for SP decoding
at sink node with general demands and obtain the number of operations required
for decoding using SP algorithm with and without traceback. For sinks demanding
all messages, we define \textit{fast decodability} of a network code and
identify a sufficient condition for the same. Next, we consider the in-network
function computation problem wherein the sink nodes do not demand the source
messages, but are only interested in computing a function of the messages. We
present an MPF formulation for function computation at the sink nodes in this
setting and use the SP algorithm to obtain the value of the demanded function.
The proposed method can be used for both linear and nonlinear as well as scalar
and vector codes for both decoding of messages in a network coding problem and
computing linear and nonlinear functions in an in-network function computation
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04125</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04125</id><created>2016-01-16</created><authors><author><keyname>Zhang</keyname><forenames>Zhujun</forenames></author><author><keyname>Sun</keyname><forenames>Qiang</forenames></author></authors><title>Complexity Analysis of 2-Heterogeneous Minimum Spanning Forest Problem</title><categories>cs.CC</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For complexity of the heterogeneous minimum spanning forest problem has not
been determined, we reduce 3-SAT which is NP-complete to 2-heterogeneous
minimum spanning forest problem to prove this problem is NP-hard and spread
result to general problem, which determines complexity of this problem. It
provides a theoretical basis for the future designing of approximation
algorithms for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04126</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04126</id><created>2016-01-16</created><authors><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author></authors><title>Engineering Safety in Machine Learning</title><categories>stat.ML cs.AI cs.CY cs.LG</categories><comments>2016 Information Theory and Applications Workshop, La Jolla,
  California</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning algorithms are increasingly influencing our decisions and
interacting with us in all parts of our daily lives. Therefore, just like for
power plants, highways, and myriad other engineered sociotechnical systems, we
must consider the safety of systems involving machine learning. In this paper,
we first discuss the definition of safety in terms of risk, epistemic
uncertainty, and the harm incurred by unwanted outcomes. Then we examine
dimensions, such as the choice of cost function and the appropriateness of
minimizing the empirical average training cost, along which certain real-world
applications may not be completely amenable to the foundational principle of
modern statistical machine learning: empirical risk minimization. In
particular, we note an emerging dichotomy of applications: ones in which safety
is important and risk minimization is not the complete story (we name these
Type A applications), and ones in which safety is not so critical and risk
minimization is sufficient (we name these Type B applications). Finally, we
discuss how four different strategies for achieving safety in engineering
(inherently safe design, safety reserves, safe fail, and procedural safeguards)
can be mapped to the machine learning context through interpretability and
causality of predictive models, objectives beyond expected prediction accuracy,
human involvement for labeling difficult or rare examples, and user experience
design of software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04131</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04131</id><created>2016-01-16</created><authors><author><keyname>Wu</keyname><forenames>Wenhao</forenames></author><author><keyname>Mittelmann</keyname><forenames>Hans</forenames></author><author><keyname>Ding</keyname><forenames>Zhi</forenames></author></authors><title>Statistical Analysis of a Posteriori Channel and Noise Distribution
  Based on HARQ Feedback</title><categories>cs.IT math.IT</categories><comments>15 pages, 2 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In response to a comment on one of our manuscript, this work studies the
posterior channel and noise distributions conditioned on the NACKs and ACKs of
all previous transmissions in HARQ system with statistical approaches. Our main
result is that, unless the coherence interval (time or frequency) is large as
in block-fading assumption, the posterior distribution of the channel and noise
either remains almost identical to the prior distribution, or it mostly follows
the same class of distribution as the prior one. In the latter case, the
difference between the posterior and prior distribution can be modeled as some
parameter mismatch, which has little impact on certain type of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04134</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04134</id><created>2016-01-16</created><authors><author><keyname>Abdullah</keyname><forenames>Ansari</forenames></author><author><keyname>Gajera</keyname><forenames>Hardik</forenames></author><author><keyname>Mahalanobis</keyname><forenames>Ayan</forenames></author></authors><title>On improvements of the $r$-adding walk in a finite field of
  characteristic 2</title><categories>cs.CR</categories><journal-ref>Journal of discrete mathematical sciences and cryptography, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is currently known from the work of Shoup and Nechaev that a generic
algorithm to solve the discrete logarithm problem in a group of prime order
must have complexity at least $k\sqrt{N}$ where $N$ is the order of the group.
In many collision search algorithms this complexity is achieved. So with
generic algorithms one can only hope to make the $k$ smaller. This $k$ depends
on the complexity of the iterative step in the generic algorithms. The
$\sqrt{N}$ comes from the fact there is about $\sqrt{N}$ iterations before a
collision. So if we can find ways that can reduce the amount of work in one
iteration then that is of great interest and probably the only possible
modification of a generic algorithm. The modified $r$-adding walk allegedly
does just that. It claims to reduce the amount of work done in one iteration of
the original $r$-adding walk. In this paper we study this modified $r$-adding
walk, we critically analyze it and we compare it with the original $r$-adding
walk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04135</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04135</id><created>2016-01-16</created><authors><author><keyname>Raamkumar</keyname><forenames>Aravind Sesagiri</forenames></author></authors><title>Whats in a Country Name - Twitter Hashtag Analysis of #singapore</title><categories>cs.SI</categories><comments>27 pages, 6 figures</comments><acm-class>H.3.1</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Twitter as a micro-blogging platform rose to instant fame mainly due to its
minimalist features that allow seamless communication between users. As the
conversations grew thick and faster, a placeholder feature called as Hashtags
became important as it captured the themes behind the tweets. Prior studies
have investigated the conversation dynamics, inter-play with other media
platforms and communication patterns between users for specific event-based
hashtags such as the #Occupy movement. Commonplace hashtags which are used on a
daily basis have been largely ignored due to their seemingly innocuous presence
in tweets and also due to the lack of connection with real-world events.
However, it can be postulated that utility of these hashtags is the main reason
behind their continued usage. This study is aimed at understanding the
rationale behind the usage of a particular type of commonplace hashtags -
location hashtags such as country and city name hashtags. Tweets with the
hashtag #singapore were extracted for one week duration. Manual and automatic
tweet classification was performed along with social network analysis, to
identify the underlying themes. Seven themes were identified. Findings indicate
that the hashtag is prominent in tweets about local events, local news, users
current location and landmark related information sharing. Users who share
content from social media sites such as Instagram make use of the hashtag in a
more prominent way when compared to users who post textual content. News
agencies, commercial bodies and celebrities make use of the hashtag more than
common individuals. Overall, the results show the non-conversational nature of
the hashtag. The findings are to be validated with other country names and
cross-validated with hashtag data from other social media platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04143</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04143</id><created>2016-01-16</created><authors><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Compositional Model based Fisher Vector Coding for Image Classification</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deriving from the gradient vector of a generative model of local features,
Fisher vector coding (FVC) has been identified as an effective coding method
for image classification. Most, if not all, FVC implementations employ the
Gaussian mixture model (GMM) to depict the generation process of local
features. However, the representative power of the GMM could be limited because
it essentially assumes that local features can be characterized by a fixed
number of feature prototypes and the number of prototypes is usually small in
FVC. To handle this limitation, in this paper we break the convention which
assumes that a local feature is drawn from one of few Gaussian distributions.
Instead, we adopt a compositional mechanism which assumes that a local feature
is drawn from a Gaussian distribution whose mean vector is composed as the
linear combination of multiple key components and the combination weight is a
latent random variable. In this way, we can greatly enhance the representative
power of the generative model of FVC. To implement our idea, we designed two
particular generative models with such a compositional mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04147</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04147</id><created>2016-01-16</created><authors><author><keyname>Yamada</keyname><forenames>Norihiro</forenames></author><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Dynamic Games and Strategies</title><categories>cs.LO cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we propose a variant of game semantics to characterize
the syntactic notion of reduction syntax-independently. For this purpose, we
introduce the notion of &quot;external&quot; and &quot;internal&quot; moves and the so-called
&quot;hiding operation&quot; in game semantics, resulting in a &quot;dynamic&quot; variant of games
and strategies. Categorically, the dynamic games and strategies give rise to a
cartesian closed bicategory which is a generalization of the category of
HO-games and strategies, where all the standard entities and constructions in
game semantics are accommodated. In formulating it, we obtained a
generalization of the existing notions and established some algebraic laws
which are new to the literature. As a future work, we shall establish the exact
correspondence between the hiding operation and reduction, which is the main
aim of the dynamic variant of game semantics. Also, we are planning to develop
it as a mathematical model of computation. Moreover, we shall consider
connections with homotopy type theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04149</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04149</id><created>2016-01-16</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Liu</keyname><forenames>Ding</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Ling</keyname><forenames>Qing</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>$\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of
  JPEG-Compressed Images</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a Deep Dual-Domain ($\mathbf{D^3}$) based fast
restoration model to remove artifacts of JPEG compressed images. It leverages
the large learning capacity of deep networks, as well as the problem-specific
expertise that was hardly incorporated in the past design of deep
architectures. For the latter, we take into consideration both the prior
knowledge of the JPEG compression scheme, and the successful practice of the
sparsity-based dual-domain approach. We further design the One-Step Sparse
Inference (1-SI) module, as an efficient and light-weighted feed-forward
approximation of sparse coding. Extensive experiments verify the superiority of
the proposed $D^3$ model over several state-of-the-art methods. Specifically,
our best model is capable of outperforming the latest deep model for around 1
dB in PSNR, and is 30 times faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04150</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04150</id><created>2016-01-16</created><authors><author><keyname>Abuella</keyname><forenames>Mohamed</forenames></author><author><keyname>Hatziadoniu</keyname><forenames>Constantine</forenames></author></authors><title>Selection of Most Effective Control Variables for Solving Optimal Power
  Flow Using Sensitivity Analysis in Particle Swarm Algorithm</title><categories>cs.CE cs.SY</categories><comments>This article is a partial work of the author's M.Sc thesis at
  department of Electrical and Computer Engineering Southern Illinois
  University Carbondale, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving the optimal power flow problem is one of the main objectives in
electrical power systems analysis and design. The modern optimization
algorithms such as the evolutionary algorithms are also adopted to solve this
problem, especially when the intermittency nature of generation resources are
included, as in wind and solar energy resources, where the models are
stochastic and non-linear. This paper uses the particle swarm optimization
algorithm for solving the optimal power flow for IEEE-30 bus system. In
addition to selection of the most effective control variables based on
sensitivity analysis to alleviate the violations and return the system back to
its normal state. This adopted strategy would decrease the optimal power flow
calculation burden by particle swarm optimization algorithm, especially with
large systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04153</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04153</id><created>2016-01-16</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Liu</keyname><forenames>Ding</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Studying Very Low Resolution Recognition Using Deep Networks</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual recognition research often assumes a sufficient resolution of the
region of interest (ROI). That is usually violated in practical scenarios,
inspiring us to explore the general Very Low Resolution Recognition (VLRR)
problem. Typically, the ROI in a VLRR problem can be smaller than $16 \times
16$ pixels, and is challenging to be recognized even by human experts. We
attempt to solve the VLRR problem using deep learning methods. Taking advantage
of techniques primarily in super resolution, domain adaptation and robust
regression, we formulate a dedicated deep learning method and demonstrate how
these techniques are incorporated step by step. That leads to a series of
well-motivated and powerful models. Any extra complexity due to the
introduction of a new model is fully justified by both analysis and simulation
results. The resulting \textit{Robust Partially Coupled Networks} achieves
feature enhancement and recognition simultaneously, while allowing for both the
flexibility to combat the LR-HR domain mismatch and the robustness to outliers.
Finally, the effectiveness of the proposed models is evaluated on three
different VLRR tasks, including face identification, digit recognition and font
recognition, all of which obtain very impressive performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04155</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04155</id><created>2016-01-16</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Dolcos</keyname><forenames>Florin</forenames></author><author><keyname>Beck</keyname><forenames>Diane</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Brain-Inspired Deep Networks for Image Aesthetics Assessment</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image aesthetics assessment has been challenging due to its subjective
nature. Being extensively inspired by the scientific advances in the human
visual perception and neuroaesthetics, we design the Brain-Inspired Deep
Networks (BDN) for this task. BDN first learns attributes through the parallel
supervised pathways, on a variety of selected feature dimensions. A high-level
synthesis network is trained to associate and transform those attributes into
the overall aesthetics rating. We then extend BDN to predicting the
distribution of human ratings, since aesthetics ratings often vary somewhat
from observer to observer. Another highlight is our first-of-its-kind study of
label-preserving transformations in the context of aesthetics assessment, which
leads to effective data augmentation approaches. Experimental results on the
AVA dataset show that our biological inspired, task-specific BDN model leads to
significantly improved performances, compared to other state-of-the-art models
with the same or higher parameter capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04169</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04169</id><created>2016-01-16</created><updated>2016-01-21</updated><authors><author><keyname>Bil&#xf2;</keyname><forenames>Davide</forenames></author><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>Multiple-Edge-Fault-Tolerant Approximate Shortest-Path Trees</title><categories>cs.DS</categories><comments>16 pages, 4 figures</comments><acm-class>G.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Let $G$ be an $n$-node and $m$-edge positively real-weighted undirected
graph. For any given integer $f \ge 1$, we study the problem of designing a
sparse \emph{f-edge-fault-tolerant} ($f$-EFT) $\sigma${\em -approximate
single-source shortest-path tree} ($\sigma$-ASPT), namely a subgraph of $G$
having as few edges as possible and which, following the failure of a set $F$
of at most $f$ edges in $G$, contains paths from a fixed source that are
stretched at most by a factor of $\sigma$. To this respect, we provide an
algorithm that efficiently computes an $f$-EFT $(2|F|+1)$-ASPT of size $O(f
n)$. Our structure improves on a previous related construction designed for
\emph{unweighted} graphs, having the same size but guaranteeing a larger
stretch factor of $3(f+1)$, plus an additive term of $(f+1) \log n$.
  Then, we show how to convert our structure into an efficient $f$-EFT
\emph{single-source distance oracle} (SSDO), that can be built in
$\widetilde{O}(f m)$ time, has size $O(fn \log^2 n)$, and is able to report,
after the failure of the edge set $F$, in $O(|F|^2 \log^2 n)$ time a
$(2|F|+1)$-approximate distance from the source to any node, and a
corresponding approximate path in the same amount of time plus the path's size.
Such an oracle is obtained by handling another fundamental problem, namely that
of updating a \emph{minimum spanning forest} (MSF) of $G$ after that a
\emph{batch} of $k$ simultaneous edge modifications (i.e., edge insertions,
deletions and weight changes) is performed. For this problem, we build in $O(m
\log^3 n)$ time a \emph{sensitivity} oracle of size $O(m \log^2 n)$, that
reports in $O(k^2 \log^2 n)$ time the (at most $2k$) edges either exiting from
or entering into the MSF. [...]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04174</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04174</id><created>2016-01-16</created><authors><author><keyname>Jiao</keyname><forenames>Yuling</forenames></author><author><keyname>Jin</keyname><forenames>Bangti</forenames></author><author><keyname>Lu</keyname><forenames>Xiliang</forenames></author></authors><title>Group Sparse Recovery via the $\ell^0(\ell^2)$ Penalty: Theory and
  Algorithm</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose and analyze a novel approach for recovering group
sparse signals, which arise naturally in a number of practical applications. It
is based on regularized least squares with an $\ell^0(\ell^2)$ penalty. One
distinct feature of the new approach is that it has the built-in decorrelation
mechanism within each group, and thus can handle the challenging strong
inner-group correlation. We provide a complete analysis of the regularized
model, e.g., the existence of global minimizers, invariance property, support
recovery, and characterization and properties of block coordinatewise
minimizers. Further, the regularized functional can be minimized efficiently
and accurately by a primal dual active set algorithm with provable global
convergence. In particular, at each iteration, it involves solving least
squares problems on the active set only, and merits fast local convergence,
which makes the method extremely efficient for recovering group sparse signals.
Extensive numerical experiments are presented to illustrate salient features of
the model and the efficiency and accuracy of the algorithm. A comparative
experimental study indicates that it is competitive with existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04179</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04179</id><created>2016-01-16</created><authors><author><keyname>Zhao</keyname><forenames>Yingbo</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>Jorge</forenames></author></authors><title>Network identification with latent nodes via auto-regressive models</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider linear time-invariant networks with unknown interaction topology
where only a subset of the nodes, termed manifest, can be directly controlled
and observed. The remaining nodes are termed latent and their number is also
unknown. Our goal is to identify the transfer function of the manifest
subnetwork and determine whether interactions between manifest nodes are direct
or mediated by latent nodes. We show that, if there are no inputs to the latent
nodes, then the manifest transfer function can be approximated arbitrarily well
in the $H_{\infty}$-norm sense by the transfer function of an auto-regressive
model. Motivated by this result, we present a least-squares estimation method
to construct the auto-regressive model from measured data. We establish that
the least-squares matrix estimate converges in probability to the matrix
sequence defining the desired auto-regressive model as the length of data and
the model order grow. We also show that the least-squares auto-regressive
method guarantees an arbitrarily small $H_\infty$-norm error in the
approximation of the manifest transfer function, exponentially decaying once
the model order exceeds a certain threshold. Finally, we show that when the
latent subnetwork is acyclic, the proposed method achieves perfect
identification of the manifest transfer function above a specific model order
as the length of the data increases. Various examples illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04180</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04180</id><created>2016-01-16</created><authors><author><keyname>Han</keyname><forenames>Duo</forenames></author><author><keyname>Mo</keyname><forenames>Yilin</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Robust State Estimation against Sparse Integrity Attacks</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1511.07218</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of robust state estimation in the presence of
integrity attacks. There are $m$ sensors monitoring a dynamical process.
Subject to the integrity attacks, $p$ out of $m$ measurements can be
arbitrarily manipulated. The classical approach such as the MMSE estimation in
the literature may not provide a reliable estimate under this so-called
$(p,m)$-sparse attack. In this work, we propose a robust estimation framework
where distributed local measurements are computed first and fused at the
estimator based on a convex optimization problem. We show the sufficient and
necessary conditions for robustness of the proposed estimator. The sufficient
and necessary conditions are shown to be tight, with a trivial gap. We also
present an upper bound on the damage an attacker can cause when the sufficient
condition is satisfied. Simulation results are also given to illustrate the
effectiveness of the estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04183</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04183</id><created>2016-01-16</created><authors><author><keyname>Diehl</keyname><forenames>Peter U.</forenames></author><author><keyname>Pedroni</keyname><forenames>Bruno U.</forenames></author><author><keyname>Cassidy</keyname><forenames>Andrew</forenames></author><author><keyname>Merolla</keyname><forenames>Paul</forenames></author><author><keyname>Neftci</keyname><forenames>Emre</forenames></author><author><keyname>Zarrella</keyname><forenames>Guido</forenames></author></authors><title>TrueHappiness: Neuromorphic Emotion Recognition on TrueNorth</title><categories>q-bio.NC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to constructing a neuromorphic device that responds to
language input by producing neuron spikes in proportion to the strength of the
appropriate positive or negative emotional response. Specifically, we perform a
fine-grained sentiment analysis task with implementations on two different
systems: one using conventional spiking neural network (SNN) simulators and the
other one using IBM's Neurosynaptic System TrueNorth. Input words are projected
into a high-dimensional semantic space and processed through a fully-connected
neural network (FCNN) containing rectified linear units trained via
backpropagation. After training, this FCNN is converted to a SNN by
substituting the ReLUs with integrate-and-fire neurons. We show that there is
practically no performance loss due to conversion to a spiking network on a
sentiment analysis test set, i.e. correlations between predictions and human
annotations differ by less than 0.02 comparing the original DNN and its spiking
equivalent. Additionally, we show that the SNN generated with this technique
can be mapped to existing neuromorphic hardware -- in our case, the TrueNorth
chip. Mapping to the chip involves 4-bit synaptic weight discretization and
adjustment of the neuron thresholds. The resulting end-to-end system can take a
user input, i.e. a word in a vocabulary of over 300,000 words, and estimate its
sentiment on TrueNorth with a power consumption of approximately 50 uW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04187</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04187</id><created>2016-01-16</created><authors><author><keyname>Diehl</keyname><forenames>Peter U.</forenames></author><author><keyname>Zarrella</keyname><forenames>Guido</forenames></author><author><keyname>Cassidy</keyname><forenames>Andrew</forenames></author><author><keyname>Pedroni</keyname><forenames>Bruno U.</forenames></author><author><keyname>Neftci</keyname><forenames>Emre</forenames></author></authors><title>Conversion of Artificial Recurrent Neural Networks to Spiking Neural
  Networks for Low-power Neuromorphic Hardware</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the field of neuromorphic low-power systems that consume
orders of magnitude less power gained significant momentum. However, their
wider use is still hindered by the lack of algorithms that can harness the
strengths of such architectures. While neuromorphic adaptations of
representation learning algorithms are now emerging, efficient processing of
temporal sequences or variable length-inputs remain difficult. Recurrent neural
networks (RNN) are widely used in machine learning to solve a variety of
sequence learning tasks. In this work we present a train-and-constrain
methodology that enables the mapping of machine learned (Elman) RNNs on a
substrate of spiking neurons, while being compatible with the capabilities of
current and near-future neuromorphic systems. This &quot;train-and-constrain&quot; method
consists of first training RNNs using backpropagation through time, then
discretizing the weights and finally converting them to spiking RNNs by
matching the responses of artificial neurons with those of the spiking neurons.
We demonstrate our approach by mapping a natural language processing task
(question classification), where we demonstrate the entire mapping process of
the recurrent layer of the network on IBM's Neurosynaptic System &quot;TrueNorth&quot;, a
spike-based digital neuromorphic hardware architecture. TrueNorth imposes
specific constraints on connectivity, neural and synaptic parameters. To
satisfy these constraints, it was necessary to discretize the synaptic weights
and neural activities to 16 levels, and to limit fan-in to 64 inputs. We find
that short synaptic delays are sufficient to implement the dynamical (temporal)
aspect of the RNN in the question classification task. The hardware-constrained
model achieved 74% accuracy in question classification while using less than
0.025% of the cores on one TrueNorth chip, resulting in an estimated power
consumption of ~17 uW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04191</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04191</id><created>2016-01-16</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author></authors><title>Non-Parametric Bayesian Rejuvenation of Smart-City Participation through
  Context-aware Internet-of-Things (IoT) Management</title><categories>cs.CY</categories><comments>NOMS 2016 Workshop-IEEE/IFIP International Workshop on Platforms and
  Applications for Smart Cities (PASC)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Tweaking citizen participation is vital in promoting Smart City services.
However, conventional practices deficit sufficient realization of personal
traits despite socio-economic promise. The recent trend of IoT-enabled
smart-objects/things and personalized services pave the way for context-aware
services. Eventually, the aim of this paper is to develop a context-aware model
in predicting participation of smart city service. Hence, major requirements
are identified for citizen participation, namely (a) unwrapping of contexts,
which are relevant, (b) scaling up (over time) of participation. However,
paramount challenges are imposed on this stipulation, such as,
un-observability, independence and composite relationship of contexts.
Therefore, a Non-parametric Bayesian model is proposed to address scalability
of contexts and its relationship with participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04203</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04203</id><created>2016-01-16</created><authors><author><keyname>Musco</keyname><forenames>Christopher</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Determining Tournament Payout Structures for Daily Fantasy Sports</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an exploding global market and the recent introduction of online cash
prize tournaments, fantasy sports contests are quickly becoming a central part
of the social gaming and sports industries. For sports fans and online media
companies, fantasy sports contests are an opportunity for large financial
gains. However, they present a host of technical challenges that arise from the
complexities involved in running a web-scale, prize driven fantasy sports
platform.
  We initiate the study of these challenges by examining one concrete problem
in particular: how to algorithmically generate contest payout structures that
are 1) economically motivating and appealing to contestants and 2) reasonably
structured and succinctly representable. We formalize this problem and present
a general two-staged approach for producing satisfying payout structures given
constraints on contest size, entry fee, prize bucketing, etc.
  We then propose and evaluate several potential algorithms for solving the
payout problem efficiently, including methods based on dynamic programming,
integer programming, and heuristic techniques. Experimental results show that a
carefully designed heuristic scales very well, even to contests with over
100,000 prize winners.
  Our approach extends beyond fantasy sports -- it is suitable for generating
engaging payout structures for any contest with a large number of entrants and
a large number of prize winners, including other massive online games, poker
tournaments, and real-life sports tournaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04207</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04207</id><created>2016-01-16</created><authors><author><keyname>Tyulepberdinova</keyname><forenames>G.</forenames></author><author><keyname>Gaziz</keyname><forenames>G.</forenames></author><author><keyname>Kerimbayev</keyname><forenames>N.</forenames></author><author><keyname>Abdykarimova</keyname><forenames>S.</forenames></author></authors><title>The discrete analogue of the method of quickest descent for an inverse
  acoustic problem in case of a smooth source</title><categories>cs.CE</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article considers the discrete analogue of the method of quickest descent
for an inverse Acoustics problem in case of a smooth source. The authors
derived the gradient of functional in differential and discrete cases,
described the algorithm of solving a problem, and compared gradients of
functional in continuous and discrete cases. In the article the improved
estimates of the rates of convergence of gradient-based methods are obtained,
which are very important for practice because they provide with the possibility
to make input data errors consistent with the iteration number. There is a
practical application of the proposed new method of deriving the gradient of
functional for an Acoustics discrete problem, for it provides with calculations
that are more accurate. The theoretical importance of the method is the
developed technique of deriving estimates and gradients of functional at a
discrete level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04213</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04213</id><created>2016-01-16</created><updated>2016-01-25</updated><authors><author><keyname>Fukuyama</keyname><forenames>Junichiro</forenames></author></authors><title>Partial-Match Queries with Random Wildcards: In Tries and Distributed
  Hash Tables</title><categories>cs.DS</categories><acm-class>F.2.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an $m$-bit query $q$ to a bitwise trie $T$. A wildcard $*$ is an
unspecified bit in $q$ for which the query asks the membership for both cases
$*=0$ and $*=1$. It is common that such partial-match queries with wildcards
are issued in tries. With uniformly random occurrences of $w$ wildcards in $q$
assumed, the obvious upper bound on the average number of traversal steps in
$T$ is $2^w m$. We show that the average does not exceed \[ \frac{m+1}{w+1}
\left( 2^{w+2} - 2 w - 4 \right) + m = O \left( \frac{2^w m}{w} \right), \] and
equals the value exactly when $T$ includes all the $m$-bit keys as the worst
case. Here the query $q$ performs with the naive backtracking algorithm in $T$.
It is similarly shown that the average is $O \left( \frac{k^w m}{w} \right)$ in
a general trie of maximum out-degree $k$. Our analysis for tries is extended to
a distributed hash table (DHT), which is among the most frequently used
decentralized data structures in networking. We show, under a natural
probabilistic assumption for the largest class of DHTs, that the average number
of hops required by an $m$-bit query $q$ to a DHT $D$ with random $w$ wildcards
meets the same asymptotic bound. As a result, $q$ is answered with average $O
\left( \frac{2^w m}{w} \right)$ hops rather than $\Theta \left( 2^w m \right)$
in the four major DHTs Chord, Pastry, Tapestry and Kademlia. In addition, with
a uniform key distribution for sufficiently many entries, we prove that a
lookup request to the DHT Chord is answered correctly with $O(m)$ hops and
probability $1 - 2^{-\Omega (m)}$. To the author's knowledge, the probability
$1 - 2^{-\Omega (m)}$ of correct lookup in Chord has not been identified so
far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04217</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04217</id><created>2016-01-16</created><authors><author><keyname>Feng</keyname><forenames>Mingjie</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author></authors><title>Harvest the potential of massive MIMO with multi-layer technologies</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Network, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is envisioned as a promising technology for 5G wireless networks
due to its high potential to improve both spectral and energy efficiency.
Although the massive MIMO system is based on innovations in the physical layer,
the upper layer techniques also play important roles on harvesting the
performance gains of massive MIMO. In this paper, we begin with an analysis of
the benefits and challenges of massive MIMO systems. We then investigate the
multi-layer techniques for incorporating massive MIMO in several important
network deployment scenarios. We conclude this paper with a discussion of open
and potential problems for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04219</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04219</id><created>2016-01-16</created><authors><author><keyname>Feng</keyname><forenames>Mingjie</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author></authors><title>BOOST: Base station on-off switching strategy for energy efficient
  massive MIMO HetNets</title><categories>cs.NI cs.IT math.IT</categories><comments>in Proc. IEEE INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of optimal base station (BS) ON-OFF
switching and user association in a heterogeneous network (HetNet) with massive
MIMO, with the objective to maximize the system energy efficiency (EE). The
joint BS ON-OFF switching and user association problem is formulated as an
integer programming problem. We first develop a centralized scheme, in which we
relax the integer constraints and employ a series of Lagrangian dual methods
that transform the original problem into a standard linear programming (LP)
problem. Due to the special structure of the LP, we prove that the optimal
solution to the relaxed LP is also feasible and optimal to the original
problem. We then propose a distributed scheme by formulating a repeated bidding
game for users and BS's, and prove that the game converges to a Nash
Equilibrium (NE). Simulation studies demonstrate that the proposed schemes can
achieve considerable gains in EE over several benchmark schemes in all the
scenarios considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04231</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04231</id><created>2016-01-16</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Deconinck</keyname><forenames>Geert</forenames></author><author><keyname>Lauwereins</keyname><forenames>Rudy</forenames></author></authors><title>An Algorithm for Tolerating Crash Failures in Distributed Systems</title><categories>cs.DC</categories><comments>Appeared in the Proceedings of the 7th Annual IEEE Int.l Conference
  and Workshop on the Engineering of Computer Based Systems (ECBS 2000),
  Edinburgh, Scotland, April 3, 2000</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of the ESPRIT project 28620 &quot;TIRAN&quot; (tailorable fault
tolerance frameworks for embedded applications), a toolset of error detection,
isolation, and recovery components is being designed to serve as a basic means
for orchestrating application-level fault tolerance. These tools will be used
either as stand-alone components or as the peripheral components of a
distributed application, that we call 'the backbone&quot;. The backbone is to run in
the background of the user application. Its objectives include (1) gathering
and maintaining error detection information produced by TIRAN components like
watchdog timers, trap handlers, or by external detection services working at
kernel or driver level, and (2) using this information at error recovery time.
In particular, those TIRAN tools related to error detection and fault masking
will forward their deductions to the backbone that, in turn, will make use of
this information to orchestrate error recovery, requesting recovery and
reconfiguration actions to those tools related to error isolation and recovery.
Clearly a key point in this approach is guaranteeing that the backbone itself
tolerates internal and external faults. In this article we describe one of the
means that are used within the TIRAN backbone to fulfill this goal: a
distributed algorithm for tolerating crash failures triggered by faults
affecting at most all but one of the components of the backbone or at most all
but one of the nodes of the system. We call this the algorithm of mutual
suspicion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04233</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04233</id><created>2016-01-16</created><authors><author><keyname>Aliakbarpour</keyname><forenames>Maryam</forenames></author><author><keyname>Biswas</keyname><forenames>Amartya Shankha</forenames></author><author><keyname>Gouleakis</keyname><forenames>Themistoklis</forenames></author><author><keyname>Peebles</keyname><forenames>John</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author><author><keyname>Yodpinyanee</keyname><forenames>Anak</forenames></author></authors><title>Sublinear-Time Algorithms for Counting Star Subgraphs with Applications
  to Join Selectivity Estimation</title><categories>cs.DS</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating the value of sums of the form $S_p
\triangleq \sum \binom{x_i}{p}$ when one has the ability to sample $x_i \geq 0$
with probability proportional to its magnitude. When $p=2$, this problem is
equivalent to estimating the selectivity of a self-join query in database
systems when one can sample rows randomly. We also study the special case when
$\{x_i\}$ is the degree sequence of a graph, which corresponds to counting the
number of $p$-stars in a graph when one has the ability to sample edges
randomly.
  Our algorithm for a $(1 \pm \varepsilon)$-multiplicative approximation of
$S_p$ has query and time complexities $\O(\frac{m \log \log n}{\epsilon^2
S_p^{1/p}})$. Here, $m=\sum x_i/2$ is the number of edges in the graph, or
equivalently, half the number of records in the database table. Similarly, $n$
is the number of vertices in the graph and the number of unique values in the
database table. We also provide tight lower bounds (up to polylogarithmic
factors) in almost all cases, even when $\{x_i\}$ is a degree sequence and one
is allowed to use the structure of the graph to try to get a better estimate.
We are not aware of any prior lower bounds on the problem of join selectivity
estimation.
  For the graph problem, prior work which assumed the ability to sample only
\emph{vertices} uniformly gave algorithms with matching lower bounds [Gonen,
Ron, and Shavitt. \textit{SIAM J. Comput.}, 25 (2011), pp. 1365-1411]. With the
ability to sample edges randomly, we show that one can achieve faster
algorithms for approximating the number of star subgraphs, bypassing the lower
bounds in this prior work. For example, in the regime where $S_p\leq n$, and
$p=2$, our upper bound is $\tilde{O}(n/S_p^{1/2})$, in contrast to their
$\Omega(n/S_p^{1/3})$ lower bound when no random edge queries are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04244</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04244</id><created>2015-11-07</created><authors><author><keyname>Al-Sarem</keyname><forenames>Mohammed</forenames></author></authors><title>Predictive and statistical analyses for academic advisory support</title><categories>cs.OH</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The ability to recognize weakness of students and solving any problem may
confront them in timely fashion is always a target of all educational
institutions. This study was designed to explore how can predictive and
statistical analysis support the academic work of adviser mainly in analysis
progress of students . The sample consisted of a total of 249 undergraduate
students: 46 % of them were Female and 54% Male. A one-way analysis of variance
and t-test were conducted to analysis if there was different behavior in
registering courses. Predictive data mining is used for support adviser in
decision making. Several classification techniques with 10-fold Cross
validation were applied. Among of them, C 4.5 constitutes the best agreement
among the finding results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04245</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04245</id><created>2015-11-07</created><authors><author><keyname>Hendel</keyname><forenames>Rim</forenames></author><author><keyname>Khaber</keyname><forenames>Farid</forenames></author><author><keyname>Essounbouli</keyname><forenames>Najib</forenames></author></authors><title>Adaptive type-2 fuzzy second order sliding mode control for nonlinear
  uncertain chaotic system</title><categories>cs.SY</categories><comments>14 pages, 13 figures, International Journal of Computational Science,
  Information Technology and Control Engineering (IJCSITCE) Vol.2, No.4,
  October 2015</comments><doi>10.5121/ijcsitce.2015.2401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a robust adaptive type-2 fuzzy higher order sliding mode
controller is designed to stabilize the unstable periodic orbits of uncertain
perturbed chaotic system with internal parameter uncertainties and external
disturbances. In Higher Order Sliding Mode Control (HOSMC),the chattering
phenomena of the control effort is reduced, by using Super Twisting algorithm.
Adaptive interval type-2 fuzzy systems are proposed to approximate the unknown
part of uncertain chaotic system and to generate the Super Twisting signals.
Based on Lyapunov criterion, adaptation laws are derived and the closed loop
system stability is guaranteed. An illustrative example is given to demonstrate
the effectiveness of the proposed controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04247</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04247</id><created>2016-01-16</created><authors><author><keyname>Luo</keyname><forenames>Yaming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Transmit Power Minimization for Wireless Networks with Energy Harvesting
  Relays</title><categories>cs.IT math.IT</categories><comments>14 pages, 5 figures, accepted by IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) has recently emerged as a key technology for green
communications as it can power wireless networks with renewable energy sources.
However, directly replacing the conventional non-EH transmitters by EH nodes
will be a challenge. In this paper, we propose to deploy extra EH nodes as
relays over an existing non-EH network. Specifically, the considered non-EH
network consists of multiple source-destination (S-D) pairs. The deployed EH
relays will take turns to assist each S-D pair, and energy diversity can be
achieved to combat the low EH rate of each EH relay. To make the best of these
EH relays, with the source transmit power minimization as the design objective,
we formulate a joint power assignment and relay selection problem, which,
however, is NP-hard. We thus propose a general framework to develop efficient
sub-optimal algorithms, which is mainly based on a sufficient condition for the
feasibility of the optimization problem. This condition yields useful design
insights and also reveals an energy hardening effect, which provides the
possibility to exempt the requirement of non-causal EH information. Simulation
results will show that the proposed cooperation strategy can achieve
near-optimal performance and provide significant power savings. Compared to the
greedy cooperation method that only optimizes the performance of the current
transmission block, the proposed strategy can achieve the same performance with
much fewer relays, and the performance gap increases with the number of S-D
pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04248</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04248</id><created>2015-11-27</created><authors><author><keyname>Sundaram</keyname><forenames>Tejeswini</forenames></author><author><keyname>Chabbra</keyname><forenames>Vyom</forenames></author></authors><title>Word Existence Algorithm</title><categories>cs.DS</categories><comments>2 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The current scenario in the field of computing is largely affected by the
speed at which data can be accessed and recalled. In this paper, we present the
word existence algorithm which is used to check if the word given as an input
is part of a particular database or not. We have taken the English language as
an example here. This algorithm tries to solve the problem of lookup by using a
uniformly distributed hash function. We have also addressed the problem of
clustering and collision. A further contribution is that we follow a direct
hashed model where each hash value is linked to another table if the continuity
for the function holds true. The core of the algorithm lies in the data model
being used during preordering. Our focus lies on the formation of a continuity
series and validating the words that exists in the database. This algorithm can
be used in applications where we there is a requirement to search for just the
existence of a word, example Artificial Intelligence responding to input ,look
up for neural networks and dictionary lookups and more. We have observed that
this algorithm provides a faster search time
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04249</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04249</id><created>2015-11-30</created><authors><author><keyname>Das</keyname><forenames>Jayanta Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author><author><keyname>Sahoo</keyname><forenames>Sudhakar</forenames></author></authors><title>Multi-Number CVT-XOR Arithmetic Operations in any Base System and its
  Significant Properties</title><categories>cs.OH</categories><comments>Pages-4, Tables-4, Figure-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carry Value Transformation (CVT) is a model of discrete dynamical system
which is one special case of Integral Value Transformations (IVTs). Earlier in
[5] it has been proved that sum of two non-negative integers is equal to the
sum of their CVT and XOR values in any base system. In the present study, this
phenomenon is extended to perform CVT and XOR operations for many non-negative
integers in any base system. To achieve that both the definition of CVT and XOR
are modified over the set of multiple integers instead of two. Also some
important properties of these operations have been studied. With the help of
cellular automata the adder circuit designed in [14] on using CVT-XOR
recurrence formula is used to design a parallel adder circuit for multiple
numbers in binary number system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04251</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04251</id><created>2016-01-17</created><authors><author><keyname>Romeres</keyname><forenames>Diego</forenames></author><author><keyname>Prando</keyname><forenames>Giulia</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author></authors><title>On-line Bayesian System Identification</title><categories>cs.SY cs.LG stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an on-line system identification setting, in which new data
become available at given time steps. In order to meet real-time estimation
requirements, we propose a tailored Bayesian system identification procedure,
in which the hyper-parameters are still updated through Marginal Likelihood
maximization, but after only one iteration of a suitable iterative optimization
algorithm. Both gradient methods and the EM algorithm are considered for the
Marginal Likelihood optimization. We compare this &quot;1-step&quot; procedure with the
standard one, in which the optimization method is run until convergence to a
local minimum. The experiments we perform confirm the effectiveness of the
approach we propose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04263</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04263</id><created>2016-01-17</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Taimoor</forenames></author><author><keyname>Serpanos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Shrobe</keyname><forenames>Howard</forenames></author></authors><title>Sound and Complete Runtime Security Monitor for Application Software</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional approaches for ensuring the security of application software at
run-time, through monitoring, either produce (high rates of) false alarms (e.g.
intrusion detection systems) or limit application performance (e.g. run-time
verification). We present a runtime security monitor that detects both known
and unknown cyber attacks by checking that the run-time behavior of the
application is consistent with the expected behavior modeled in application
specification. This is crucial because, even if the implementation is
consistent with its specification, the application may still be vulnerable due
to flaws in the supporting infrastructure (e.g. the language runtime system,
libraries and operating system). This runtime security monitor is sound and
complete, eliminating false alarms, as well as efficient, so that it does not
limit runtime application performance and so that it supports real-time
systems. The security monitor takes as input the application specification and
the application implementation, which may be expressed in different languages.
The specification language of the application software is formalized based on
monadic second order logic and event calculus interpreted over algebraic data
structures. This language allows us to express behavior of an application at
any desired (and practical) level of abstraction as well as with high degree of
modularity. The security monitor detects every attack by systematically
comparing the application execution and specification behaviors at runtime,
even though they operate at two different levels of abstraction. We define the
denotational semantics of the specification language and prove that the monitor
is sound and complete. Furthermore, the monitor is efficient because of the
modular application specification at appropriate level(s) of abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04271</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04271</id><created>2016-01-17</created><authors><author><keyname>Samadi</keyname><forenames>Zainalabedin</forenames></author><author><keyname>Tabatabavakili</keyname><forenames>Vahid</forenames></author><author><keyname>Haddadi</keyname><forenames>Farzan</forenames></author></authors><title>On feasibility of perfect interference alignment in interference
  networks</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1411.4399</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment(IA) is mostly achieved by coding interference over
multiple dimensions. Intuitively, the more interfering signals that need to be
aligned, the larger the number of dimensions needed to align them. This
dimensionality requirement poses a major challenge for IA in practical systems.
This work evaluates the necessary and sufficient conditions on channel
structure of a 3 user interference channel(IC) to make perfect IA feasible
within limited number of channel extensions. It is shown that if only one of
interfering channel coefficients can be designed to a specific value,
interference would be aligned perfectly at all receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04273</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04273</id><created>2016-01-17</created><updated>2016-01-27</updated><authors><author><keyname>Takabe</keyname><forenames>Satoshi</forenames></author><author><keyname>Hukushima</keyname><forenames>Koji</forenames></author></authors><title>Statistical-mechanical Analysis of Linear Programming Relaxation for
  Combinatorial Optimization Problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT</categories><comments>12 pages, 5 figures; typos are fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical behavior of the linear programming (LP) problem is studied as a
relaxation of the minimum vertex cover, a type of integer programming (IP)
problem. A lattice-gas model on the Erd\&quot;os-R\'enyi random graphs of
$\alpha$-uniform hyperedges is proposed to express both the LP and IP problems
of the min-VC in the common statistical-mechanical model with a one-parameter
family. Statistical-mechanical analyses reveal for $\alpha=2$ that the LP
optimal solution is typically equal to that given by the IP below the critical
average degree $c=e$ in the thermodynamic limit. The critical threshold for
good accuracy of the relaxation extends the mathematical result $c=1$, and
coincides with the replica symmetry-breaking threshold of the IP. The LP
relaxation for the minimum hitting sets with $\alpha\geq 3$, minimum vertex
covers on $\alpha$-uniform random graphs, is also studied. Analytic and
numerical results strongly suggest that the LP relaxation fails to estimate
optimal values above the critical average degree $c=e/(\alpha-1)$ where the
replica symmetry is broken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04276</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04276</id><created>2016-01-17</created><authors><author><keyname>Parizi</keyname><forenames>Mani Bastani</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Exact Random Coding Secrecy Exponents for the Wiretap Channel</title><categories>cs.IT math.IT</categories><comments>18 pages, 5 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the exact exponential decay rate of the expected amount of
information leaked to the wiretapper in Wyner's wiretap channel setting using
wiretap channel codes constructed from both i.i.d. and constant-composition
random codes. Our analysis for those sampled from i.i.d. random coding ensemble
shows that the previously-known achievable secrecy exponent using this ensemble
is indeed the exact exponent for an average code in the ensemble. Furthermore,
our analysis on wiretap channel codes constructed from the ensemble of
constant-composition random codes, leads to an exponent which, in addition to
being the exact exponent for an average code, is larger than the achievable
secrecy exponent that has been established so far in the literature for this
ensemble (which in turn was known to be smaller than that achievable by wiretap
channel codes sampled from i.i.d. random coding ensemble). We also show
examples where the exact secrecy exponent for the wiretap channel codes
constructed from random constant-composition codes is larger than that of those
constructed from i.i.d. random codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04280</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04280</id><created>2016-01-17</created><authors><author><keyname>Aizenbud</keyname><forenames>Yariv</forenames></author><author><keyname>Shabat</keyname><forenames>Gil</forenames></author><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author></authors><title>Randomized LU Decomposition Using Sparse Projections</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast algorithm for the approximation of a low rank LU decomposition is
presented. In order to achieve a low complexity, the algorithm uses sparse
random projections combined with FFT-based random projections. The asymptotic
approximation error of the algorithm is analyzed and a theoretical error bound
is presented. Finally, numerical examples illustrate that for a similar
approximation error, the sparse LU algorithm is faster than recent
state-of-the-art methods. The algorithm is completely parallelizable that
enables to run on a GPU. The performance is tested on a GPU card, showing a
significant improvement in the running time in comparison to sequential
execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04293</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04293</id><created>2016-01-17</created><authors><author><keyname>Rosenfeld</keyname><forenames>Amir</forenames></author><author><keyname>Ullman</keyname><forenames>Shimon</forenames></author></authors><title>Face-space Action Recognition by Face-Object Interactions</title><categories>cs.CV</categories><comments>our more recent work on a related topic is described in a separate
  paper : http://arxiv.org/abs/1511.03814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action recognition in still images has seen major improvement in recent years
due to advances in human pose estimation, object recognition and stronger
feature representations. However, there are still many cases in which
performance remains far from that of humans. In this paper, we approach the
problem by learning explicitly, and then integrating three components of
transitive actions: (1) the human body part relevant to the action (2) the
object being acted upon and (3) the specific form of interaction between the
person and the object. The process uses class-specific features and relations
not used in the past for action recognition and which use inherently two cycles
in the process unlike most standard approaches. We focus on face-related
actions (FRA), a subset of actions that includes several currently challenging
categories. We present an average relative improvement of 52% over state-of-the
art. We also make a new benchmark publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04294</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04294</id><created>2016-01-17</created><updated>2016-01-29</updated><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author></authors><title>Quantum superpositions and projective measurement in the lambda calculus</title><categories>cs.LO</categories><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an extension of simply typed lambda-calculus to handle some
properties of quantum computing. The equiprobable quantum superposition is
taken as a commutative pair and the quantum measurement as a non-deterministic
projection over it. Destructive interferences are achieved by introducing an
inverse symbol with respect to pairs. The no-cloning property is ensured by
using a combination of syntactic linearity with linear logic. Indeed, the
syntactic linearity is enough for unitary gates, while a function measuring its
argument needs to enforce that the argument is used only once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04296</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04296</id><created>2016-01-17</created><authors><author><keyname>Ammar</keyname><forenames>Adel</forenames></author><author><keyname>Labroue</keyname><forenames>Sylvie</forenames></author><author><keyname>Obligis</keyname><forenames>Estelle</forenames></author><author><keyname>Cr&#xe9;pon</keyname><forenames>Michel</forenames></author><author><keyname>Thiria</keyname><forenames>Sylvie</forenames></author></authors><title>Building a Learning Database for the Neural Network Retrieval of Sea
  Surface Salinity from SMOS Brightness Temperatures</title><categories>cs.NE physics.ao-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with an important aspect of the neural network retrieval
of sea surface salinity (SSS) from SMOS brightness temperatures (TBs). The
neural network retrieval method is an empirical approach that offers the
possibility of being independent from any theoretical emissivity model, during
the in-flight phase. A Previous study [1] has proven that this approach is
applicable to all pixels on ocean, by designing a set of neural networks with
different inputs. The present study focuses on the choice of the learning
database and demonstrates that a judicious distribution of the geophysical
parameters allows to markedly reduce the systematic regional biases of the
retrieved SSS, which are due to the high noise on the TBs. An equalization of
the distribution of the geophysical parameters, followed by a new technique for
boosting the learning process, makes the regional biases almost disappear for
latitudes between 40{\deg}S and 40{\deg}N, while the global standard deviation
remains between 0.6 psu (at the center of the of the swath) and 1 psu (at the
edges).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04299</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04299</id><created>2016-01-17</created><authors><author><keyname>Ahrens</keyname><forenames>Benedikt</forenames></author><author><keyname>Matthes</keyname><forenames>Ralph</forenames></author></authors><title>Heterogeneous substitution systems revisited</title><categories>cs.LO cs.PL</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matthes and Uustalu (TCS 327(1-2):155-174, 2004) presented a categorical
description of substitution systems capable of capturing syntax involving
binding which is independent of whether the syntax is made up from least or
greatest fixed points. We extend this work in two directions: we continue the
analysis by creating more categorical structure, in particular by organizing
substitution systems into a category and studying its properties, and we
develop the proofs of the results of the cited paper and our new ones in
UniMath, a recent library of univalent mathematics formalized in the Coq
theorem prover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04301</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04301</id><created>2016-01-17</created><authors><author><keyname>Ulltveit-Moe</keyname><forenames>Nils</forenames></author><author><keyname>Nergaard</keyname><forenames>Henrik</forenames></author><author><keyname>Erd&#xf6;di</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Gj&#xf8;s&#xe6;ter</keyname><forenames>Terje</forenames></author><author><keyname>Kolstad</keyname><forenames>Erland</forenames></author><author><keyname>Berg</keyname><forenames>P&#xe5;l</forenames></author></authors><title>Secure Information Sharing in an Industrial Internet of Things</title><categories>cs.CR</categories><comments>12 pages, 3 figures</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates how secure information sharing with external vendors
can be achieved in an Industrial Internet of Things (IIoT). It also identifies
necessary security requirements for secure information sharing based on
identified security challenges stated by the industry. The paper then proposes
a roadmap for improving security in IIoT which investigates both short-term and
long-term solutions for protecting IIoT devices. The short-term solution is
mainly based on integrating existing good practices. The paper also outlines a
long term solution for protecting IIoT devices with fine-grained access control
for sharing data between external entities that would support cloud-based data
storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04306</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04306</id><created>2016-01-17</created><authors><author><keyname>Jeavons</keyname><forenames>Peter</forenames></author><author><keyname>Scott</keyname><forenames>Alex</forenames></author><author><keyname>Xu</keyname><forenames>Lei</forenames></author></authors><title>Feedback from Nature: Simple Randomised Distributed Algorithms for
  Maximal Independent Set Selection and Greedy Colouring</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1211.0235</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose distributed algorithms for two well-established problems that
operate efficiently under extremely harsh conditions. Our algorithms achieve
state-of-the-art performance in a simple and novel way.
  Our algorithm for maximal independent set selection operates on a network of
identical anonymous processors. The processor at each node has no prior
information about the network. At each time step, each node can only broadcast
a single bit to all its neighbours, or remain silent. Each node can detect
whether one or more neighbours have broadcast, but cannot tell how many of its
neighbours have broadcast, or which ones. We build on recent work of Afek et
al. which was inspired by studying the development of a network of cells in the
fruit fly~\cite{Afek2011a}. However we incorporate for the first time another
important feature of the biological system: varying the probability value used
at each node based on local feedback from neighbouring nodes. Given any
$n$-node network, our algorithm achieves the optimal expected time complexity
of $O(\log n)$ rounds and the optimal expected message complexity of $O(1)$
single-bit messages broadcast by each node.We also show that the previous
approach, without feedback, cannot achieve better than $\Omega(\log^2 n)$
expected time complexity, whatever global scheme is used to choose the
probabilities.
  Our algorithm for distributed greedy colouring works under similar harsh
conditions: each identical node has no prior information about the network, can
only broadcast a single message to all neighbours at each time step
representing a desired colour, and can only detect whether at least one
neighbour has broadcast each colour value. We show that our algorithm has an
expected time complexity of $O(\Delta+\log n)$, where $\Delta$ is the maximum
degree of the network, and expected message complexity of $O(1)$ messages
broadcast by each node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04314</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04314</id><created>2016-01-17</created><authors><author><keyname>Blocq</keyname><forenames>Gideon</forenames></author><author><keyname>Orda</keyname><forenames>Ariel</forenames></author></authors><title>How Good is Bargained Routing?</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of networking, research has focused on non-cooperative games,
where the selfish agents cannot reach a binding agreement on the way they would
share the infrastructure. Many approaches have been proposed for mitigating the
typically inefficient operating points. However, in a growing number of
networking scenarios selfish agents are able to communicate and reach an
agreement. Hence, the degradation of performance should be considered at an
operating point of a cooperative game. Accordingly, our goal is to lay
foundations for the application of cooperative game theory to fundamental
problems in networking. We explain our choice of the Nash Bargaining Scheme
(NBS) as the solution concept, and introduce the Price of Selfishness (PoS),
which considers the degradation of performance at the worst NBS. We focus on
the fundamental load balancing game of routing over parallel links. First, we
consider agents with identical performance objectives. We show that, while the
PoA here can be large, through bargaining, all agents, and the system, strictly
improve their performance. Interestingly, in a two-agent system or when all
agents have identical demands, we establish that they reach social optimality.
We then consider agents with different performance objectives and demonstrate
that the PoS and PoA can be unbounded, yet we explain why both measures are
unsuitable. Accordingly, we introduce the Price of Heterogeneity (PoH), as an
extension of the PoA. We establish an upper-bound on the PoH and indicate its
further motivation for bargaining. Finally, we discuss network design
guidelines that follow from our findings
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04321</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04321</id><created>2016-01-17</created><authors><author><keyname>Roald</keyname><forenames>Line</forenames></author><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Chance Constrained Optimal Power Flow with Curtailment and Reserves from
  Wind Power Plants</title><categories>math.OC cs.SY</categories><comments>7 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past years, the share of electricity production from wind power
plants has increased to significant levels in several power systems across
Europe and the United States. In order to cope with the fluctuating and
partially unpredictable nature of renewable energy sources, transmission system
operators (TSOs) have responded by increasing their reserve capacity
requirements and by requiring wind power plants to be capable of providing
reserves or following active power set-point signals. This paper addresses the
issue of efficiently incorporating these new types of wind power control in the
day-ahead operational planning. We review the technical requirements the wind
power plants must fulfill, and propose a mathematical framework for modeling
wind power control. The framework is based on an optimal power flow formulation
with weighted chance constraints, which accounts for the uncertainty of wind
power forecasts and allows us to limit the risk of constraint violations. In a
case study based on the IEEE 118 bus system, we use the developed method to
assess the effectiveness of different types of wind power control in terms of
operational cost, system security and wind power curtailment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04363</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04363</id><created>2015-12-02</created><authors><author><keyname>Agrawal</keyname><forenames>Sharul</forenames></author><author><keyname>Mazumdar</keyname><forenames>Himanshu S</forenames></author></authors><title>Economic viability and Future Impact of Internet of Things in India: An
  Inevitable wave</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of things , sometimes referred as Internet of objects can be
stated as an environment in which any physical things or objects are assigned
with unique addresses and an internet connection to communicate to external
objects or people without requiring any human intervention. In year 2014 around
three billion people around the world are reported to use internet for using
social networking applications, ecommerce applications, surfing the web etc
which is tenfold times as compared to 1993. India is not only leading the IT
industry globally but it has also become the second largest country after china
by the number of people using internet nowadays. It is also very much
surpassing that the Internet usage is going beyond the literacy, which is clear
from the analysis done in 2015 that out of 50 million users, 50% i.e 26 million
users are from rural India. Many Initiatives are been launched by major
companies like Google India, Tata communication and Intel to bring internet to
the doorstep to educate rural women. As more and more masses will avail to such
plethora of information and communication platform, the day is not far when
Internet will be used as a infrastructure for allowing every objects and things
talk, communicate, coordinate compute and share information. Hence it is
important to study and predict the technologies associated with Internet
communication such as IoT. This paper studies the evolution of internet usage
and classifies the impact areas where internet will go beyond personal
communication or knowledge interface but it will provide communication and
knowledge base support to numerous gadgets and systems around us
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04364</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04364</id><created>2016-01-17</created><authors><author><keyname>Mauroy</keyname><forenames>A.</forenames></author><author><keyname>Hendrickx</keyname><forenames>J.</forenames></author></authors><title>Spectral identification of networks using sparse measurements</title><categories>math.DS cs.SY</categories><comments>35</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method to recover global information about a network of
interconnected dynamical systems based on observations made at a small number
(possibly one) of its nodes. In contrast to classical identification of full
graph topology, we focus on the identification of the spectral graph-theoretic
properties of the network, a framework that we call spectral network
identification.
  The main theoretical results connect the spectral properties of the network
to the spectral properties of the dynamics, which are well-defined in the
context of the so-called Koopman operator and can be extracted from data
through the Dynamic Mode Decomposition algorithm. These results are obtained
for networks of diffusively-coupled units that admit a stable equilibrium
state. For large networks, a statistical approach is considered, which focuses
on spectral moments of the network and is well-suited to the case of
heterogeneous populations.
  Our framework provides efficient numerical methods to infer global
information on the network from sparse local measurements at a few nodes.
Numerical simulations show for instance the possibility of detecting the mean
number of connections or the addition of a new vertex using measurements made
at one single node, that need not be representative of the other nodes'
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04366</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04366</id><created>2016-01-17</created><authors><author><keyname>Stra&#x17e;ar</keyname><forenames>Martin</forenames></author><author><keyname>Curk</keyname><forenames>Toma&#x17e;</forenames></author></authors><title>Learning the kernel matrix via predictive low-rank approximations</title><categories>cs.LG stat.ML</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient and accurate low-rank approximations to multiple data sources are
essential in the era of big data. The scaling of kernel-based learning
algorithms to large datasets is limited by the square complexity associated
with computation and storage of the kernel matrix, which is assumed to be
available in recent most multiple kernel learning algorithms. We propose a
method to learn simultaneous low-rank approximations of a set of base kernels
in regression tasks.
  We present the Mklaren algorithm, which approximates multiple kernel matrices
with least angle regression in the low-dimensional feature space. The idea is
based on entirely geometrical concepts and does not assume access to full
kernel matrices. The algorithm achieves linear complexity in the number of data
points as well as kernels, while it accounts for the correlations between
kernel matrices. When explicit feature space representation is available for
kernels, we use the relation between primal and dual regression weights to gain
model interpretation. Our approach outperforms contemporary kernel matrix
approximation approaches when learning with multiple kernels on standard
regression datasets, as well as improves selection of relevant kernels in
comparison to multiple kernel learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04373</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04373</id><created>2016-01-17</created><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Rate Maximization of Decode-and-Forward Relaying Systems with RF Energy
  Harvesting</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure</comments><journal-ref>IEEE Communications Letters, vol. 19, no. 12, pp: 2290-2293,
  December 2015</journal-ref><doi>10.1109/LCOMM.2015.2489213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a three-node decode-and-forward (DF) half-duplex relaying system,
where the source first harvests RF energy from the relay, and then uses this
energy to transmit information to the destination via the relay. We assume that
the information transfer and wireless power transfer phases alternate over time
in the same frequency band, and their {\it time fraction} (TF) may change or be
fixed from one transmission epoch (fading state) to the next. For this system,
we maximize the achievable average data rate. Thereby, we propose two schemes:
(1) jointly optimal power and TF allocation, and (2) optimal power allocation
with fixed TF. Due to the small amounts of harvested power at the source, the
two schemes achieve similar information rates, but yield significant
performance gains compared to a benchmark system with fixed power and fixed TF
allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04385</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04385</id><created>2016-01-17</created><authors><author><keyname>Djorgovski</keyname><forenames>S. G.</forenames></author><author><keyname>Graham</keyname><forenames>M. J.</forenames></author><author><keyname>Donalek</keyname><forenames>C.</forenames></author><author><keyname>Mahabal</keyname><forenames>A. A.</forenames></author><author><keyname>Drake</keyname><forenames>A. J.</forenames></author><author><keyname>Turmon</keyname><forenames>M.</forenames></author><author><keyname>Fuchs</keyname><forenames>T.</forenames></author></authors><title>Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys</title><categories>astro-ph.IM cs.DB</categories><comments>14 pages, an invited paper for a special issue of Future Generation
  Computer Systems, Elsevier Publ. (2015). This is an expanded version of a
  paper arXiv:1407.3502 presented at the IEEE e-Science 2014 conf., with some
  new content</comments><doi>10.1016/j.future.2015.10.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04386</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04386</id><created>2016-01-17</created><authors><author><keyname>Huang</keyname><forenames>Ying</forenames></author><author><keyname>Zheng</keyname><forenames>Hong</forenames></author><author><keyname>Ling</keyname><forenames>Haibin</forenames></author><author><keyname>Blasch</keyname><forenames>Erik</forenames></author><author><keyname>Yang</keyname><forenames>Hao</forenames></author></authors><title>A Comparative Study of Object Trackers for Infrared Flying Bird Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bird strikes present a huge risk for aircraft, especially since traditional
airport bird surveillance is mainly dependent on inefficient human observation.
Computer vision based technology has been proposed to automatically detect
birds, determine bird flying trajectories, and predict aircraft takeoff delays.
However, the characteristics of bird flight using imagery and the performance
of existing methods applied to flying bird task are not well known. Therefore,
we perform infrared flying bird tracking experiments using 12 state-of-the-art
algorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommend
feature analysis. We also develop a Struck-scale method to demonstrate the
effectiveness of multiple scale sampling adaption in handling the object of
flying bird with varying shape and scale. The general analysis can be used to
develop specialized bird tracking methods for airport safety, wildness and
urban bird population studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04388</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04388</id><created>2016-01-17</created><authors><author><keyname>Kumar</keyname><forenames>Vibhor</forenames></author><author><keyname>Heikkonen</keyname><forenames>Jukka</forenames></author></authors><title>Denoising with flexible histogram models on Minimum Description length
  principle</title><categories>cs.IT math.IT</categories><comments>in 13th International Conference on Systems, Signals and Image
  Processing, IWSSIP, 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising has always been theoretically considered as removal of high
frequency disturbances having Gaussian distribution. Here we relax this
assumption and the method used here is completely different from traditional
thresholding schemes. The data are converted to wavelet coefficients, a part of
which represents the denoised signal and the remaining part the noise. The
coefficients are distributed to bins in two types of histograms using the
principles of Minimum Description Lengthi(MDL). One histogram represents noise
which can not be compressed easily and the other represents data which can be
coded in small code length. The histograms made can have variable width for
bins. The proposed denoising method based on variable bin width histograms and
MDL principle is tested on simulated and real data and compared with other well
known denoising methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04396</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04396</id><created>2016-01-17</created><authors><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author><author><keyname>Li</keyname><forenames>Weiping</forenames></author></authors><title>Source-Channel Secrecy for Shannon Cipher System</title><categories>cs.IT math.IT</categories><comments>41 pages, Information Theory. arXiv admin note: text overlap with
  arXiv:1410.2881 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a rate-distortion theory based approach to measure secrecy has been
proposed by Schieler \emph{et al.} \cite{Schieler14-2}, in which an
eavesdropper is allowed to produce a list of $2^{mR_{L}}$ reconstruction
sequences and measure secrecy by the minimum distortion over the entire list.
Moreover, they studied it in the context of the Shannon cipher system with
\emph{noiseless} wiretap channel, and then derived the achievable region of
secret key rate, list rate, eavesdropper distortion, and distortion of
legitimate user. In this paper, we extend their results to the Shannon cipher
system with \emph{noisy} wiretap channel, and study the source-channel secrecy
for such system. Then we obtain an inner bound and an outer bound for the
achievable region of secret key rate, list rate, eavesdropper distortion, and
distortion of legitimate user. Furthermore, for the Shannon cipher system with
degraded wiretap channel, the inner bound and the outer bound coincide. Hence
in this case, the achievable region is exactly characterized. Besides, we also
consider the Shannon cipher system with Gaussian source transmitted over
Gaussian wiretap channel, and the achievable region for this case is also
characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04406</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04406</id><created>2016-01-18</created><authors><author><keyname>Bettadapura</keyname><forenames>Vinay</forenames></author><author><keyname>Castro</keyname><forenames>Daniel</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Discovering Picturesque Highlights from Egocentric Vacation Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for identifying picturesque highlights from large
amounts of egocentric video data. Given a set of egocentric videos captured
over the course of a vacation, our method analyzes the videos and looks for
images that have good picturesque and artistic properties. We introduce novel
techniques to automatically determine aesthetic features such as composition,
symmetry and color vibrancy in egocentric videos and rank the video frames
based on their photographic qualities to generate highlights. Our approach also
uses contextual information such as GPS, when available, to assess the relative
importance of each geographic location where the vacation videos were shot.
Furthermore, we specifically leverage the properties of egocentric videos to
improve our highlight detection. We demonstrate results on a new egocentric
vacation dataset which includes 26.5 hours of videos taken over a 14 day
vacation that spans many famous tourist destinations and also provide results
from a user-study to access our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04436</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04436</id><created>2016-01-18</created><authors><author><keyname>Rodriguez</keyname><forenames>Nancy</forenames><affiliation>ICAR</affiliation></author></authors><title>Development of a wheelchair simulator for children with multiple
  disabilities</title><categories>cs.HC cs.GR</categories><comments>from 3rd International Workshop on Virtual and Augmented Assistive
  Technology (VAAT), IEEE Virtual Reality 2015, 2015, Arles, France. 2015</comments><proxy>ccsd</proxy><doi>10.1109/VAAT.2015.7155405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual reality allows to create situations which can be experimented under
the control of the user, without risks, in a very flexible way. This allows to
develop skills and to have confidence to work in real conditions with real
equipment. VR is then widely used as a training and learning tool. More
recently, VR has also showed its potential in rehabilitation and therapy fields
because it provides users with the ability of repeat their actions several
times and to progress at their own pace. In this communication, we present our
work in the development of a wheelchair simulator designed to allow children
with multiple disabilities to familiarize themselves with the wheelchair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04448</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04448</id><created>2016-01-18</created><updated>2016-01-21</updated><authors><author><keyname>M&#xe4;cker</keyname><forenames>Alexander</forenames></author><author><keyname>Malatyali</keyname><forenames>Manuel</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author></authors><title>On Competitive Algorithms for Approximations of Top-k-Position
  Monitoring of Distributed Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the continuous distributed monitoring model in which $n$ distributed
nodes, receiving individual data streams, are connected to a designated server.
The server is asked to continuously monitor a function defined over the values
observed across all streams while minimizing the communication. We study a
variant in which the server is equipped with a broadcast channel and is
supposed to keep track of an approximation of the set of nodes currently
observing the $k$ largest values. Such an approximate set is exact except for
some imprecision in an $\varepsilon$-neighborhood of the $k$-th largest value.
This approximation of the Top-$k$-Position Monitoring Problem is of interest in
cases where marginal changes (e.g. due to noise) in observed values can be
ignored so that monitoring an approximation is sufficient and can reduce
communication.
  This paper extends our results from [IPDPS'15], where we have developed a
filter-based online algorithm for the (exact) Top-k-Position Monitoring
Problem. There we have presented a competitive analysis of our algorithm
against an offline adversary that also is restricted to filter-based
algorithms. Our new algorithms as well as their analyses use new methods. We
analyze their competitiveness against adversaries that use both exact and
approximate filter-based algorithms, and observe severe differences between the
respective powers of these adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04450</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04450</id><created>2016-01-18</created><authors><author><keyname>Cerd&#xe1;-Company</keyname><forenames>Xim</forenames></author><author><keyname>P&#xe1;rraga</keyname><forenames>C. Alejandro</forenames></author><author><keyname>Otazu</keyname><forenames>Xavier</forenames></author></authors><title>Which tone-mapping operator is the best? A comparative study of
  perceptual quality</title><categories>cs.GR</categories><msc-class>68U10</msc-class><acm-class>I.3.3; I.4.0; H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tone-mapping operators (TMO) are designed to generate perceptually similar
low-dynamic range images from high-dynamic range ones. We studied the
performance of fifteen TMOs in two psychophysical experiments where observers
compared the digitally generated tone-mapped images to their corresponding
physical scenes. All experiments were performed in a controlled environment and
the setups were designed to emphasise different image properties: in the first
experiment we evaluated the local relationships among intensity-levels, and in
the second one we evaluated global visual appearance among physical scenes and
tone-mapped images, which were presented side by side. We ranked the TMOs
according to how well they reproduce the results obtained in the physical
scene. Our results show that ranking position clearly depends on the adopted
evaluation criteria, which implies that, in general, these tone-mapping
algorithms consider either local or global image attributes but rarely both. We
conclude that a more thorough and standardized evaluation criteria are needed
to study all the characteristics of TMOs, as there is ample room for
improvement in future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04451</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04451</id><created>2016-01-18</created><authors><author><keyname>Duin</keyname><forenames>Robert P. W.</forenames></author><author><keyname>Pekalska</keyname><forenames>Elzbieta</forenames></author></authors><title>Zero-error dissimilarity based classifiers</title><categories>stat.ML cs.LG</categories><comments>5 pages. Paper originally written in 2003. Although it may proof an
  obvious fact, it is significant for understanding the essential conditions it
  is based on</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider general non-Euclidean distance measures between real world
objects that need to be classified. It is assumed that objects are represented
by distances to other objects only. Conditions for zero-error dissimilarity
based classifiers are derived. Additional conditions are given under which the
zero-error decision boundary is a continues function of the distances to a
finite set of training samples. These conditions affect the objects as well as
the distance measure used. It is argued that they can be met in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04453</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04453</id><created>2016-01-18</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Guo</keyname><forenames>Xuemei</forenames></author><author><keyname>Zhu</keyname><forenames>Shixin</forenames></author></authors><title>Some results of linear codes over the ring
  $\mathbb{Z}_4+u\mathbb{Z}_4+v\mathbb{Z}_4+uv\mathbb{Z}_4$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we mainly study the theory of linear codes over the ring $R
=\mathbb{Z}_4+u\mathbb{Z}_4+v\mathbb{Z}_4+uv\mathbb{Z}_4$. By the Chinese
Remainder Theorem, we have $R$ is isomorphic to the direct sum of four rings
$\mathbb{Z}_4$. We define a Gray map $\Phi$ from $R^{n}$ to
$\mathbb{Z}_4^{4n}$, which is a distance preserving map. The Gray image of a
cyclic code over $R^{n}$ is a linear code over $\mathbb{Z}_4$. Furthermore, we
study the MacWilliams identities of linear codes over $R$ and give the the
generator polynomials of cyclic codes over $R$. Finally, we discuss some
properties of MDS codes over $R$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04455</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04455</id><created>2016-01-18</created><authors><author><keyname>Falgas-Ravry</keyname><forenames>Victor</forenames></author><author><keyname>Larsson</keyname><forenames>Joel</forenames></author><author><keyname>Markstr&#xf6;m</keyname><forenames>Klas</forenames></author></authors><title>Speed and concentration of the covering time for structured coupon
  collectors</title><categories>math.PR cs.DM math.CO</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $V$ be an $n$-set, and let $X$ be a random variable taking values in the
powerset of $V$. Suppose we are given a sequence of random coupons $X_1, X_2,
\ldots $, where the $X_i$ are independent random variables with distribution
given by $X$. The covering time $T$ is the smallest integer $t\geq 0$ such that
$\bigcup_{i=1}^tX_i=V$. The distribution of $T$ is important in many
applications in combinatorial probability, and has been extensively studied.
However the literature has focussed almost exclusively on the case where $X$ is
assumed to be symmetric and/or uniform in some way.
  In this paper we study the covering time for much more general random
variables $X$; we give general criteria for $T$ being sharply concentrated
around its mean, precise tools to estimate that mean, as well as examples where
$T$ fails to be concentrated and when structural properties in the distribution
of $X$ allow for a very different behaviour of $T$ relative to the
symmetric/uniform case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04458</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04458</id><created>2016-01-18</created><authors><author><keyname>Zimmer</keyname><forenames>Christoph</forenames></author><author><keyname>Bergmann</keyname><forenames>Frank T.</forenames></author><author><keyname>Sahle</keyname><forenames>Sven</forenames></author></authors><title>Reducing local minima in fitness landscapes of parameter estimation by
  using piecewise evaluation and state estimation</title><categories>q-bio.QM cs.DS cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordinary differential equations (ODE) are widely used for modeling in Systems
Biology. As most commonly only some of the kinetic parameters are measurable or
precisely known, parameter estimation techniques are applied to parametrize the
model to experimental data. A main challenge for the parameter estimation is
the complexity of the parameter space, especially its high dimensionality and
local minima.
  Parameter estimation techniques consist of an objective function, measuring
how well a certain parameter set describes the experimental data, and an
optimization algorithm that optimizes this objective function. A lot of effort
has been spent on developing highly sophisticated optimization algorithms to
cope with the complexity in the parameter space, but surprisingly few articles
address the influence of the objective function on the computational complexity
in finding global optima. We extend a recently developed multiple shooting for
stochastic systems (MSS) objective function for parameter estimation of
stochastic models and apply it to parameter estimation of ODE models. This MSS
objective function treats the intervals between measurement points separately.
This separate treatment allows the ODE trajectory to stay closer to the data
and we show that it reduces the complexity of the parameter space.
  We use examples from Systems Biology, namely a Lotka-Volterra model, a
FitzHugh-Nagumo oscillator and a Calcium oscillation model, to demonstrate the
power of the MSS approach for reducing the complexity and the number of local
minima in the parameter space. The approach is fully implemented in the COPASI
software package and, therefore, easily accessible for a wide community of
researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04460</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04460</id><created>2016-01-18</created><authors><author><keyname>Vieira</keyname><forenames>Allan R.</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author></authors><title>Consequences of nonconformist behaviors in a continuous opinion model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>15 pages, 9 figures, to appear in JSTAT</comments><journal-ref>J. Stat. Mech. 023204 (2016)</journal-ref><doi>10.1088/1742-5468/2016/02/023204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate opinion formation in a kinetic exchange opinion model, where
opinions are represented by numbers in the real interval $[-1,1]$ and agents
are typified by the individual degree of conviction about the opinion that they
support. Opinions evolve through pairwise interactions governed by competitive
positive and negative couplings, that promote imitation and dissent,
respectively. The model contemplates also another type of nonconformity such
that agents can occasionally choose their opinions independently of the
interactions with other agents. The steady states of the model as a function of
the parameters that describe conviction, dissent and independence are analyzed,
with particular emphasis on the emergence of extreme opinions. Then, we
characterize the possible ordered and disordered phases and the occurrence or
suppression of phase transitions that arise spontaneously due to the disorder
introduced by the heterogeneity of the agents and/or their interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04467</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04467</id><created>2016-01-18</created><authors><author><keyname>Jin</keyname><forenames>Lingfei</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>New MDS Self-Dual Codes from Generalized Reed-Solomon Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both MDS and Euclidean self-dual codes have theoretical and practical
importance and the study of MDS self-dual codes has attracted lots of attention
in recent years. In particular, determining existence of $q$-ary MDS self-dual
codes for various lengths has been investigated extensively. The problem is
completely solved for the case where $q$ is even. The current paper focuses on
the case where $q$ is odd. We construct a few classes of new MDS self-dual code
through generalized Reed-Solomon codes. More precisely, we show that for any
given even length $n$ we have a $q$-ary MDS code as long as $q\equiv1\bmod{4}$
and $q$ is sufficiently large (say $q\ge 2^n\times n^2)$. Furthermore, we prove
that there exists a $q$-ary MDS self-dual code of length $n$ if $q=r^2$ and $n$
satisfies one of the three conditions: (i) $n\le r$ and $n$ is even; (ii) $q$
is odd and $n-1$ is an odd divisor of $q-1$; (iii) $r\equiv3\mod{4}$ and
$n=2tr$ for any $t\le (r-1)/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04468</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04468</id><created>2016-01-18</created><authors><author><keyname>Sokolov</keyname><forenames>Artem</forenames></author><author><keyname>Riezler</keyname><forenames>Stefan</forenames></author><author><keyname>Urvoy</keyname><forenames>Tanguy</forenames></author></authors><title>Bandit Structured Prediction for Learning from Partial Feedback in
  Statistical Machine Translation</title><categories>cs.CL cs.LG</categories><comments>In Proceedings of MT Summit XV, 2015. Miami, FL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to structured prediction from bandit feedback, called
Bandit Structured Prediction, where only the value of a task loss function at a
single predicted point, instead of a correct structure, is observed in
learning. We present an application to discriminative reranking in Statistical
Machine Translation (SMT) where the learning algorithm only has access to a
1-BLEU loss evaluation of a predicted translation instead of obtaining a gold
standard reference translation. In our experiment bandit feedback is obtained
by evaluating BLEU on reference translations without revealing them to the
algorithm. This can be thought of as a simulation of interactive machine
translation where an SMT system is personalized by a user who provides single
point feedback to predicted translations. Our experiments show that our
approach improves translation quality and is comparable to approaches that
employ more informative feedback in learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04469</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04469</id><created>2016-01-18</created><authors><author><keyname>Chitturi</keyname><forenames>Bhadrachalam</forenames></author><author><keyname>S</keyname><forenames>Krishnaveni K</forenames></author></authors><title>Adjacencies in Permutations</title><categories>cs.DM</categories><comments>20 pages. 5 tables</comments><msc-class>05A05, 20B10, 20B30, 20B40</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A permutation on an alphabet $ \Sigma $, is a sequence where every element in
$ \Sigma $ occurs precisely once. Given a permutation $ \pi $= ($\pi_{1} $, $
\pi_{2} $, $ \pi_{3} $,....., $ \pi_{n} $) over the alphabet $ \Sigma $ =$\{
$0, 1, . . . , n$-$1 $\}$ the elements in two consecutive positions in $ \pi $
e.g. $ \pi_{i} $ and $ \pi_{i+1} $ are said to form an \emph{adjacency} if $
\pi_{i+1} $ =$ \pi_{i} $+1. The concept of adjacencies is widely used in
computation. The set of permutations over $ \Sigma $ forms a symmetric group,
that we call P$ _{n} $. The identity permutation, I$ _{n}$ $\in$ P$_{n}$ where
I$_{n}$ =(0,1,2,...,n$-$1) has exactly n$ - $1 adjacencies. Likewise, the
reverse order permutation R$_{n} (\in P_{n})$=(n$-$1, n$-$2, n$-$3, n$-$4,
...,0) has no adjacencies. We denote the set of permutations in P$_{n} $ with
exactly k adjacencies with P$_{n} $(k). We study variations of adjacency. % A
transposition exchanges adjacent sublists; when one of the sublists is
restricted to be a prefix (suffix) then one obtains a prefix (suffix)
transposition. We call the operations: transpositions, prefix transpositions
and suffix transpositions as block-moves. A particular type of adjacency and a
particular block-move are closely related. In this article we compute the
cardinalities of P$_{n}$(k) i.e. $ \forall_k \mid $P$ _{n} $ (k) $ \mid $ for
each type of adjacency in $O(n^2)$ time. Given a particular adjacency and the
corresponding block-move, we show that $\forall_{k} \mid P_{n}(k)\mid$ and the
expected number of moves to sort a permutation in P$_{n} $ are closely related.
Consequently, we propose a model to estimate the expected number of moves to
sort a permutation in P$_{n} $ with a block-move. We show the results for
prefix transposition. Due to symmetry, these results are also applicable to
suffix transposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04471</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04471</id><created>2016-01-18</created><authors><author><keyname>Yemini</keyname><forenames>Michal</forenames></author><author><keyname>Somekh-Baruch</keyname><forenames>Anelia</forenames></author><author><keyname>Cohen</keyname><forenames>Reuven</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>On Simultaneous Percolation with Two Disk Types</title><categories>cs.NI math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the simultaneous percolation of two Gilbert disk
models. The two models are connected through excluding disks, which prevent
elements of the second model to be in the vicinity of the first model. Under
these assumptions we characterize the region of densities in which the two
models both have a unique infinite connected component. The motivation for this
work is the co-existence of two cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04473</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04473</id><created>2016-01-18</created><authors><author><keyname>Alvar</keyname><forenames>Saeed R.</forenames></author><author><keyname>Kamisli</keyname><forenames>Fatih</forenames></author></authors><title>Lossless Intra Coding in HEVC with 3-tap Filters</title><categories>cs.MM</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a pixel-by-pixel spatial prediction method for lossless
intra coding within High Efficiency Video Coding (HEVC). A well-known previous
pixel-by-pixel spatial prediction method uses only two neighboring pixels for
prediction, based on the angular projection idea borrowed from block-based
intra prediction in lossy coding. This paper explores a method which uses three
neighboring pixels for prediction according to a two-dimensional correlation
model, and the used neighbor pixels and prediction weights change depending on
intra mode. To find the best prediction weights for each intra mode, a
two-stage offline optimization algorithm is used and a number of implementation
aspects are discussed to simplify the proposed prediction method. The proposed
method is implemented in the HEVC reference software and experimental results
show that the explored 3-tap filtering method can achieve an average 11.34%
bitrate reduction over the default lossless intra coding in HEVC. The proposed
method also decreases average decoding time by 12.7% while it increases average
encoding time by 9.7%
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04485</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04485</id><created>2016-01-18</created><authors><author><keyname>Velasco</keyname><forenames>Jose</forenames></author><author><keyname>Pizarro</keyname><forenames>Daniel</forenames></author><author><keyname>Macias-Guarasa</keyname><forenames>Javier</forenames></author><author><keyname>Asaei</keyname><forenames>Afsaneh</forenames></author></authors><title>TDOA Matrices: Algebraic Properties and their Application to Robust
  Denoising with Missing Data</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the Time delay of Arrival (TDOA) between a set of sensors is the
basic setup for many applications, such as localization or signal beamforming.
This paper presents the set of TDOA matrices, which are built from noise-free
TDOA measurements, not requiring knowledge of the sensor array geometry. We
prove that TDOA matrices are rank-two and have a special SVD decomposition that
leads to a compact linear parametric representation. Properties of TDOA
matrices are applied in this paper to perform denoising, by finding the TDOA
matrix closest to the matrix composed with noisy measurements. The paper shows
that this problem admits a closed-form solution for TDOA measurements
contaminated with Gaussian noise which extends to the case of having missing
data. The paper also proposes a novel robust denoising method resistant to
outliers, missing data and inspired in recent advances in robust low-rank
estimation. Experiments in synthetic and real datasets show TDOA-based
localization, both in terms of TDOA accuracy estimation and localization error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04500</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04500</id><created>2016-01-18</created><authors><author><keyname>Zhou</keyname><forenames>Lin</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>Second-Order and Moderate Deviation Asymptotics for Successive
  Refinement</title><categories>cs.IT math.IT</categories><comments>short version submitted to ISIT 2016, this version submitted to IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the optimal second-order coding region and moderate deviations
constant for successive refinement source coding with a joint excess-distortion
probability constraint. We consider two scenarios: (i) a discrete memoryless
source (DMS) and arbitrary distortion measures at the decoders and (ii) a
Gaussian memoryless source (GMS) and quadratic distortion measures at the
decoders. For a DMS with arbitrary distortion measures, we prove an achievable
second-order coding region, using type covering lemmas by Kanlis and Narayan
and by No, Ingber and Weissman. We prove the converse using the perturbation
approach by Gu and Effros. When the DMS is successively refinable, the
expressions for the second-order coding region and the moderate deviations
constant are simplified and easily computable. For this case, we also obtain
new insights on the second-order behavior compared to the scenario where
separate excess-distortion proabilities are considered. For example, we
describe a DMS, for which the optimal second-order region transitions from
being characterizable by a bivariate Gaussian to a univariate Gaussian, as the
distortion levels are varied. We then consider a GMS with quadratic distortion
measures. To prove the direct part, we make use of the sphere covering theorem
by Verger-Gaugry, together with appropriately-defined Gaussian type classes. To
prove the converse, we generalize Kostina and Verd\'u's one-shot converse bound
for point-to-point lossy source coding. We remark that this proof is applicable
to general successively refinable sources. In the proofs of the moderate
deviations results for both scenarios, we follow a strategy similar to that for
the second-order asymptotics and use the moderate deviations principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04503</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04503</id><created>2016-01-18</created><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author></authors><title>Secure Network Coding over Small Fields</title><categories>cs.IT math.IT</categories><comments>single column, 37 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paradigm of network coding, the information-theoretic security problem
is encountered in the presence of a wiretapper, who has capability of accessing
an unknown channel-subset in communication networks. In order to combat this
eavesdropping attack, secure network coding is introduced to prevent
information from being leaked to adversaries. For any construction of secure
linear network codes (SLNCs) over a wiretap network, the based field size is a
very important index, because it largely determines the computational and space
complexities, and further the efficiency of network transmission. Further, it
is also very important for the process of secure network coding from
theoretical research to practical applications. In the present paper, we
proposed new lower bounds on the field size for constructing SLNCs, which shows
that the field size can be reduced considerably without giving up any security
and capacity. In addition, since the obtained lower bounds depend on network
topology, how to efficiently determine them is another very important and
attractive problem for explicit constructions of SLNCs. Motivated by a desire
to develop methods with low complexity and empirical behavior to solve this
problem, we propose efficient approaches and present a series of algorithms for
implementation. Subsequently, the complexity of our algorithms is analyzed,
which shows that they are polynomial-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04504</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04504</id><created>2016-01-18</created><authors><author><keyname>Raviv</keyname><forenames>Netanel</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Coding for Locality in Reconstructing Permutations</title><categories>cs.IT math.IT</categories><comments>Parts of this work were submitted to ISIT2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of storing permutations in a distributed manner arises in several
common scenarios, such as efficient updates of a large, encrypted, or
compressed data set. This problem may be addressed in either a combinatorial or
a coding approach. The former approach boils down to presenting large sets of
permutations with locality, that is, any symbol of the permutation can be
computed from a small set of other symbols. In the latter approach, a
permutation may be coded in order to achieve locality. Both approaches must
present low query complexity to allow the user to find an element efficiently.
We discuss both approaches, and give a particular focus to the combinatorial
one.
  In the combinatorial approach, we provide upper and lower bounds for the
maximal size of a set of permutations with locality, and provide several simple
constructions which attain the upper bound. In cases where the upper bound is
not attained, we provide alternative constructions using a variety of tools,
such as Reed-Solomon codes, permutation polynomials, and multi-permutations. In
addition, several low-rate constructions of particular interest are discussed.
  In the coding approach we discuss an alternative representation of
permutations, present a paradigm for supporting arbitrary powers of the stored
permutation, and conclude with a proof of concept that permutations may be
stored more efficiently than ordinary strings over the same alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04507</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04507</id><created>2016-01-18</created><authors><author><keyname>Napp</keyname><forenames>Diego</forenames></author><author><keyname>Pinto</keyname><forenames>Raquel</forenames></author><author><keyname>Toste</keyname><forenames>Marisa</forenames></author></authors><title>On MDS convolutional Codes over $\mathbb Z_{p^r}$</title><categories>cs.IT math.IT math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum Distance Separable (MDS) convolutional codes are cha- racterized
through the property that the free distance meets the generalized Singleton
bound. The existence of free MDS convolutional codes over Z p r was recently
discovered in [26] via the Hensel lift of a cyclic code. In this paper we
further investigate this important class of convolutional codes over Z p r from
a new perspective. We introduce the notions of p-standard form and r- optimal
parameters to derive a novel upper bound of Singleton type on the free
distance. Moreover, we present a constructive method for building general (non
necessarily free) MDS convolutional codes over Z p r for any given set of
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04512</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04512</id><created>2016-01-18</created><authors><author><keyname>&#x17d;ugec</keyname><forenames>P.</forenames></author><author><keyname>Wei&#xdf;</keyname><forenames>C.</forenames></author><author><keyname>Guerrero</keyname><forenames>C.</forenames></author><author><keyname>Gunsing</keyname><forenames>F.</forenames></author><author><keyname>Vlachoudis</keyname><forenames>V.</forenames></author><author><keyname>Sabate-Gilarte</keyname><forenames>M.</forenames></author><author><keyname>Stamatopoulos</keyname><forenames>A.</forenames></author><author><keyname>Wright</keyname><forenames>T.</forenames></author><author><keyname>Lerendegui-Marco</keyname><forenames>J.</forenames></author><author><keyname>Mingrone</keyname><forenames>F.</forenames></author><author><keyname>Ryan</keyname><forenames>J. A.</forenames></author><author><keyname>Warren</keyname><forenames>S. G.</forenames></author><author><keyname>Tsinganis</keyname><forenames>A.</forenames></author><author><keyname>Barbagallo</keyname><forenames>M.</forenames></author></authors><title>Pulse processing routines for neutron time-of-flight data</title><categories>physics.ins-det cs.OH nucl-ex</categories><comments>13 pages, 10 figures, 5 tables</comments><journal-ref>Nuclear Instruments and Methods in Physics Research A 812 (2016)
  134-144</journal-ref><doi>10.1016/j.nima.2015.12.054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pulse shape analysis framework is described, which was developed for
n_TOF-Phase3, the third phase in the operation of the n_TOF facility at CERN.
The most notable feature of this new framework is the adoption of generic pulse
shape analysis routines, characterized by a minimal number of explicit
assumptions about the nature of pulses. The aim of these routines is to be
applicable to a wide variety of detectors, thus facilitating the introduction
of the new detectors or types of detectors into the analysis framework. The
operational details of the routines are suited to the specific requirements of
particular detectors by adjusting the set of external input parameters. Pulse
recognition, baseline calculation and the pulse shape fitting procedure are
described. Special emphasis is put on their computational efficiency, since the
most basic implementations of these conceptually simple methods are often
computationally inefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04520</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04520</id><created>2016-01-18</created><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Mottet</keyname><forenames>Antoine</forenames></author></authors><title>Reducts of finitely bounded homogeneous structures, and lifting
  tractability from finite-domain constraint satisfaction</title><categories>math.LO cs.CC cs.LO</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many natural decision problems can be formulated as constraint satisfaction
problems for reducts of finitely bounded homogeneous structures. This class of
problems is a large generalisation of the class of CSPs over finite domains.
Our first result is a general polynomial-time reduction from such
infinite-domain CSPs to finite-domain CSPs. We use this reduction to obtain new
powerful polynomial-time tractability conditions that can be expressed in terms
of topological polymorphism clones. Moreover, we study the subclass C of CSPs
for structures that are first-order definable over equality with parameters.
Also this class C properly extends the class of all finite-domain CSPs. We
formulate a tractability conjecture for C and show that it is equivalent to the
finite-domain tractability conjecture. In our proof we develop new algebraic
techniques that help to prove continuity for certain clone homomorphisms needed
for showing hardness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04522</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04522</id><created>2016-01-18</created><authors><author><keyname>Li</keyname><forenames>Xinchao</forenames></author><author><keyname>Liu</keyname><forenames>Ju</forenames></author><author><keyname>Sun</keyname><forenames>Jiande</forenames></author><author><keyname>Yang</keyname><forenames>Xiaohui</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Multiple Watermarking Algorithm Based on Spread Transform Dither
  Modulation</title><categories>cs.MM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multiple watermarking technique, embedding several watermarks in one carrier,
has enabled many interesting applications. In this study, a novel multiple
watermarking algorithm is proposed based on the spirit of spread transform
dither modulation (STDM). It can embed multiple watermarks into the same region
and the same transform domain of one image; meanwhile, the embedded watermarks
can be extracted independently and blindly in the detector without any
interference. Furthermore, to improve the fidelity of the watermarked image,
the properties of the dither modulation quantizer and the proposed multiple
watermarks embedding strategy are investigated, and two practical optimization
methods are proposed. Finally, to enhance the application flexibility, an
extension of the proposed algorithm is proposed which can sequentially embeds
different watermarks into one image during each stage of its circulation.
Compared with the pioneering multiple watermarking algorithms, the proposed one
owns more flexibility in practical application and is more robust against
distortion due to basic operations such as random noise, JPEG compression and
volumetric scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04530</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04530</id><created>2016-01-18</created><authors><author><keyname>Duin</keyname><forenames>Robert P. W.</forenames></author><author><keyname>Pekalska</keyname><forenames>Elzbieta</forenames></author></authors><title>Domain based classification</title><categories>stat.ML cs.LG</categories><comments>8 pages, unpublished paper written in 2005, discussing a significant,
  still almost not studied problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of traditional classification ru les minimizing the expected
probability of error (0-1 loss) are inappropriate if the class probability
distributions are ill-defined or impossible to estimate. We argue that in such
cases class domains should be used instead of class distributions or densities
to construct a reliable decision function. Proposals are presented for some
evaluation criteria and classifier learning schemes, illustrated by an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04533</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04533</id><created>2016-01-18</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Gauss-Bonnet for multi-linear valuations</title><categories>cs.DM math.CO math.GN</categories><comments>71 pages, 14 figures</comments><msc-class>53A55, 05C99, 52C99, 57M15, 68R99, 53C6</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove Gauss-Bonnet and Poincare-Hopf formulas for multi-linear valuations
on finite simple graphs G=(V,E) and answer affirmatively a conjecture of
Gruenbaum from 1970 by constructing higher order Dehn-Sommerville valuations
which vanish for all d-graphs without boundary. An example of a quadratic
valuation is the Wu characteristic w(G) which sums (-1)^(dim(x)+dim(y)) over
all intersecting pairs of complete subgraphs x,y of a G. More generally, an
intersection number w(A,B) sums (-1)^(dim(x)+dim(y)) over pairs x,y, where x is
in A and y is in B and x,y intersect. w(G) is a quadratic Euler characteristic
X(G), where X sums (-1)^dim(x) over all complete subgraphs x of G. We prove
that w is multiplicative, like Euler characteristic: w(G x H) = w(G) w(H) for
any two graphs and that w is invariant under Barycentric refinements. We
construct a curvature K satisfying Gauss-Bonnet w(G) = sum K(a). We also prove
w(G) = X(G)-X(dG) for Euler characteristic X which holds for any d-graph G with
boundary dG. We also show higher order Poincare-Hopf formulas: there is for
every multi-linear valuation X and function f an index i(a) such that sum
i(a)=X(G). For d-graphs G and X=w it agrees with the Euler curvature. For the
vanishing multi-valuations which were conjectured to exist, like for the
quadratic valuation X(G) = (V X) Y with X=(1,-1,1,-1,1),Y=(0,-2,3,-4,5) on
4-graphs, discrete 4 manifolds, where V_{ij}(G) is the f-matrix counting the
number of i and j-simplices in G intersecting, the curvature is constant zero.
For all graphs and multi-linear Dehn-Sommerville relations, the
Dehn-Sommerville curvature K(v) at a vertex is a Dehn-Sommerville valuation on
the unit sphere S(v). We show X V(G) Y = v(G) Y for any linear valuation Y of a
d-graph G with f-vector v(G). This provides examples for the Gruenbaum
conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04535</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04535</id><created>2016-01-18</created><updated>2016-03-01</updated><authors><author><keyname>Souza</keyname><forenames>Th&#xe1;rsis T. P.</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author></authors><title>A nonlinear impact: evidences of causal effects of social media on
  market prices</title><categories>q-fin.ST cs.CY physics.data-an q-fin.CP</categories><comments>17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks offer a new way to investigate financial markets'
dynamics by enabling the large-scale analysis of investors' collective
behavior. We provide empirical evidence that suggests social media and stock
markets have a nonlinear causal relationship. We take advantage of an extensive
data set composed of social media messages related to DJIA index components. By
using information-theoretic measures to cope for possible nonlinear causal
coupling between social media and stock markets systems, we point out stunning
differences in the results with respect to linear coupling. Two main
conclusions are drawn: First, social media significant causality on stocks'
returns are purely nonlinear in most cases; Second, social media dominates the
directional coupling with stock market, an effect not observable within linear
modeling. Results also serve as empirical guidance on model adequacy in the
investigation of sociotechnical and financial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04547</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04547</id><created>2016-01-18</created><authors><author><keyname>Ruiz</keyname><forenames>Eduardo</forenames></author><author><keyname>Mayol-Cuevas</keyname><forenames>Walterio</forenames></author></authors><title>Towards an objective evaluation of underactuated gripper designs</title><categories>cs.RO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore state-of-the-art underactuated, compliant robot
gripper designs through looking at their performance on a generic grasping
task. Starting from a state of the art open gripper design, we propose design
modifications,and importantly, evaluate all designs on a grasping experiment
involving a selection of objects resulting in 3600 object-gripper interactions.
Interested in non-planned grasping but rather on a design's generic
performance, we explore the influence of object shape, pose and orientation
relative to the gripper and its finger number and configuration. Using
open-loop grasps we achieved up to 75% success rate over our trials. The
results indicate and support that under motion constraints and uncertainties
and without involving grasp planning, a 2-fingered underactuated compliant hand
outperforms higher multi-fingered configurations. To our knowledge this is the
first extended objective comparison of various multi-fingered underactuated
hand designs under generic grasping conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04549</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04549</id><created>2016-01-18</created><authors><author><keyname>Camoriano</keyname><forenames>Raffaello</forenames></author><author><keyname>Traversaro</keyname><forenames>Silvio</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author></authors><title>Incremental Semiparametric Inverse Dynamics Learning</title><categories>stat.ML cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for incremental semiparametric inverse
dynamics learning. In particular, we consider the mixture of two approaches:
Parametric modeling based on rigid body dynamics equations and nonparametric
modeling based on incremental kernel methods, with no prior information on the
mechanical properties of the system. This yields to an incremental
semiparametric approach, leveraging the advantages of both the parametric and
nonparametric models. We validate the proposed technique learning the dynamics
of one arm of the iCub humanoid robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04554</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04554</id><created>2016-01-14</created><authors><author><keyname>Paschero</keyname><forenames>Maurizio</forenames></author><author><keyname>Storti</keyname><forenames>Gian Luca</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author><author><keyname>Mascioli</keyname><forenames>Fabio Massimo Frattale</forenames></author><author><keyname>Rizzoni</keyname><forenames>Giorgio</forenames></author></authors><title>A novel mechanical analogy based battery model for SoC estimation using
  a multi-cell EKF</title><categories>cs.SY</categories><comments>8 page, 12 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The future evolution of technological systems dedicated to improve energy
efficiency will strongly depend on effective and reliable Energy Storage
Systems, as key components for Smart Grids, microgrids and electric mobility.
Besides possible improvements in chemical materials and cells design, the
Battery Management System is the most important electronic device that improves
the reliability of a battery pack. In fact, a precise State of Charge (SoC)
estimation allows the energy flows controller to exploit better the full
capacity of each cell. In this paper, we propose an alternative definition for
the SoC, explaining the rationales by a mechanical analogy. We introduce a
novel cell model, conceived as a series of three electric dipoles, together
with a procedure for parameters estimation relying only on voltage measures and
a given current profile. The three dipoles represent the quasi-stationary, the
dynamics and the istantaneous components of voltage measures. An Extended
Kalman Filer (EKF) is adopted as a nonlinear state estimator. Moreover, we
propose a multi-cell EKF system based on a round-robin approach to allow the
same processing block to keep track of many cells at the same time. Performance
tests with a prototype battery pack composed by 18 A123 cells connected in
series show encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04560</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04560</id><created>2016-01-18</created><updated>2016-02-05</updated><authors><author><keyname>Beir&#xf3;</keyname><forenames>M. G.</forenames></author><author><keyname>Panisson</keyname><forenames>A.</forenames></author><author><keyname>Tizzoni</keyname><forenames>M.</forenames></author><author><keyname>Cattuto</keyname><forenames>C.</forenames></author></authors><title>Predicting human mobility through the assimilation of social media
  traces into mobility models</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting human mobility flows at different spatial scales is challenged by
the heterogeneity of individual trajectories and the multi-scale nature of
transportation networks. As vast amounts of digital traces of human behaviour
become available, an opportunity arises to improve mobility models by
integrating into them proxy data on mobility collected by a variety of digital
platforms and location-aware services. Here we propose a hybrid model of human
mobility that integrates a large-scale publicly available dataset from a
popular photo-sharing system with the classical gravity model, under a stacked
regression procedure. We validate the performance and generalizability of our
approach using two ground-truth datasets on air travel and daily commuting in
the United States: using two different cross-validation schemes we show that
the hybrid model affords enhanced mobility prediction at both spatial scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04563</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04563</id><created>2016-01-18</created><authors><author><keyname>Visone</keyname><forenames>Ciro</forenames></author></authors><title>Superposition principle in linear networks with controlled sources</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The manuscript discusses a well-known issue that, despite its fundamental
role in basic electric circuit theory, seems to be tackled without the needful
attention. The question if the Principle of Superposition (POS) can be applied
to linear networks containing linear dependent sources still appears as an
addressed point unworthy to be further discussed. Conversely, the analysis of
this point has been recently re-proposed [5,6] and an alternative conclusion
has been drawn. From this result, the manuscript provides an alternative
approach to such issue from a more general point of view. It is oriented to
clarify the issue from the didactic viewpoint, rather than provide a more
efficient general technique for circuit analysis. By starting from a linear
system of equations, representing a general linear circuit containing
controlled sources, the correct interpretation of turning off the controlled
elements in terms of circuit equivalent is provided, so allowing a statement of
the POS for linear circuits in a wider context. Further, this approach is
sufficiently intuitive and straightforward to fit the needs of a Basic Electric
Circuit Theory class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04568</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04568</id><created>2016-01-18</created><authors><author><keyname>Yin</keyname><forenames>Rujie</forenames></author></authors><title>Content Aware Neural Style Transfer</title><categories>cs.CV</categories><msc-class>68T10</msc-class><acm-class>I.4.10; I.5.2; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a content-aware style transfer algorithm for paintings
and photos of similar content using pre-trained neural network, obtaining
better results than the previous work. In addition, the numerical experiments
show that the style pattern and the content information is not completely
separated by neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04574</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04574</id><created>2016-01-18</created><authors><author><keyname>Cuay&#xe1;huitl</keyname><forenames>Heriberto</forenames></author></authors><title>SimpleDS: A Simple Deep Reinforcement Learning Dialogue System</title><categories>cs.AI</categories><comments>International Workshop on Spoken Dialogue Systems (IWSDS), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents 'SimpleDS', a simple and publicly available dialogue
system trained with deep reinforcement learning. In contrast to previous
reinforcement learning dialogue systems, this system avoids manual feature
engineering by performing action selection directly from raw text of the last
system and (noisy) user responses. Our initial results, in the restaurant
domain, show that it is indeed possible to induce reasonable dialogue behaviour
with an approach that aims for high levels of automation in dialogue control
for intelligent interactive agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04580</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04580</id><created>2016-01-18</created><authors><author><keyname>Krishnan</keyname><forenames>Vinodh</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Nonparametric Bayesian Storyline Detection from Microtexts</title><categories>cs.CL cs.LG</categories><comments>This report is based on a rejected submission from the 2015
  Conference on Empirical Methods on Natural Language Processing, incorporating
  some of the reviewers' suggestions for improvement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  News events and social media are composed of evolving storylines, which
capture public attention for a limited period of time. Identifying these
storylines would enable many high-impact applications, such as tracking public
interest and opinion in ongoing crisis events. However, this requires
integrating temporal and linguistic information, and prior work takes a largely
heuristic approach. We present a novel online non-parametric Bayesian framework
for storyline detection, using the distance-dependent Chinese Restaurant
Process (dd-CRP). To ensure efficient linear-time inference, we employ a
fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate
our baseline and proposed models on the TREC Twitter Timeline Generation task
and show strong results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04583</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04583</id><created>2016-01-18</created><authors><author><keyname>Chen</keyname><forenames>Juntao</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>A Game-Theoretic Framework for Resilient and Distributed Generation
  Control of Renewable Energies in Microgrids</title><categories>cs.SY</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of microgrids that depend on the renewable distributed energy
resources with the current power systems is a critical issue in the smart grid.
In this paper, we propose a non-cooperative game-theoretic framework to study
the strategic behavior of distributed microgrids that generate renewable
energies and characterize the power generation solutions by using the Nash
equilibrium concept. Our framework not only incorporates economic factors but
also takes into account the stability and efficiency of the microgrids,
including the power flow constraints and voltage angle regulations. We develop
two decentralized update schemes for microgrids and show their convergence to a
unique Nash equilibrium. Also, we propose a novel fully distributed PMU-enabled
algorithm which only needs the information of voltage angle at the bus. To show
the resiliency of the distributed algorithm, we introduce two failure models of
the smart grid. Case studies based on the IEEE 14-bus system are used to
corroborate the effectiveness and resiliency of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04585</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04585</id><created>2016-01-18</created><authors><author><keyname>Alt</keyname><forenames>Helmut</forenames></author><author><keyname>Scharf</keyname><forenames>Nadja</forenames></author></authors><title>Approximating Smallest Containers for Packing Three-dimensional Convex
  Objects</title><categories>cs.CG</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of computing a minimal-volume container for the
non-overlapping packing of a given set of three-dimensional convex objects.
Already the simplest versions of the problem are NP-hard so that we cannot
expect to find exact polynomial time algorithms. We give constant ratio
approximation algorithms for packing axis-parallel (rectangular) cuboids under
translation into an axis-parallel (rectangular) cuboid as container, for
cuboids under rigid motions into an axis-parallel cuboid or into an arbitrary
convex container, and for packing convex polyhedra under rigid motions into an
axis-parallel cuboid or arbitrary convex container. This work gives the first
approximability results for the computation of minimal volume containers for
the objects described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04586</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04586</id><created>2016-01-18</created><updated>2016-01-20</updated><authors><author><keyname>Wang</keyname><forenames>Binhuan</forenames></author><author><keyname>Zhang</keyname><forenames>Yilong</forenames></author><author><keyname>Sun</keyname><forenames>Wei</forenames></author><author><keyname>Fang</keyname><forenames>Yixin</forenames></author></authors><title>Sparse Convex Clustering</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex clustering, a convex relaxation of k-means clustering and hierarchical
clustering, has drawn recent attentions since it nicely addresses the
instability issue of traditional nonconvex clustering methods. Although its
computational and statistical properties have been recently studied, the
performance of convex clustering has not yet been investigated in the
high-dimensional clustering scenario, where the data contains a large number of
features and many of them carry no information about the clustering structure.
In this paper, we demonstrate that the performance of convex clustering could
be distorted when the uninformative features are included in the clustering. To
overcome it, we introduce a new clustering method, referred to as Sparse Convex
Clustering, to simultaneously cluster observations and conduct feature
selection. The key idea is to formulate convex clustering in a form of
regularization, with an adaptive group-lasso penalty term on cluster centers.
In order to optimally balance the tradeoff between the cluster fitting and
sparsity, a tuning criterion based on clustering stability is developed. In
theory, we provide an unbiased estimator for the degrees of freedom of the
proposed sparse convex clustering method. Finally, the effectiveness of the
sparse convex clustering is examined through a variety of numerical experiments
and a real data application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04588</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04588</id><created>2016-01-18</created><authors><author><keyname>Mottelet</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Gaullier</keyname><forenames>Gil</forenames></author><author><keyname>Sadaka</keyname><forenames>Georges</forenames></author></authors><title>Metabolic Flux Analysis in Isotope Labeling Experiments using the
  Adjoint Approach</title><categories>q-bio.MN cs.NA math.OC</categories><comments>Preprint submitted to IEEE/ACM Transactions on Computational Biology
  and Bioinformatics</comments><msc-class>92C42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehension of metabolic pathways is considerably enhanced by metabolic
flux analysis (MFA-ILE) in isotope labeling experiments. The balance equations
are given by hundreds of algebraic (stationary MFA) or ordinary differential
equations (nonstationary MFA), and reducing the number of operations is
therefore a crucial part of reducing the computation cost. The main bottleneck
for deterministic algorithms is the computation of derivatives, particularly
for nonstationary MFA. In this article we explain how the overall
identification process may be speeded up by using the adjoint approach to
compute the gradient of the residual sum of squares. The proposed approach
shows significant improvements in terms of complexity and computation time when
it is compared with the usual (direct) approach. Numerical results are obtained
for the central metabolic pathways of Escherichia coli and are validated
against reference software in the stationary case. The methods and algorithms
described in this paper are included in the sysmetab software package
distributed under an Open Source license at
http://forge.scilab.org/index.php/p/sysmetab/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04589</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04589</id><created>2016-01-18</created><authors><author><keyname>Li</keyname><forenames>Chuan</forenames></author><author><keyname>Wand</keyname><forenames>Michael</forenames></author></authors><title>Combining Markov Random Fields and Convolutional Neural Networks for
  Image Synthesis</title><categories>cs.CV</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a combination of generative Markov random field (MRF)
models and discriminatively trained deep convolutional neural networks (dCNNs)
for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN
feature pyramid, controling the image layout at an abstract level. We apply the
method to both photographic and non-photo-realistic (artwork) synthesis tasks.
The MRF regularizer prevents over-excitation artifacts and reduces implausible
feature mixtures common to previous dCNN inversion approaches, permitting
synthezing photographic content with increased visual plausibility. Unlike
standard MRF-based texture synthesis, the combined system can both match and
adapt local features with considerable variability, yielding results far out of
reach of classic generative MRF methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04595</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04595</id><created>2016-01-18</created><authors><author><keyname>Han</keyname><forenames>Puxiao</forenames></author><author><keyname>Zhu</keyname><forenames>Junan</forenames></author><author><keyname>Niu</keyname><forenames>Ruixin</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Multi-Processor Approximate Message Passing Using Lossy Compression</title><categories>cs.DC cs.IT math.IT</categories><comments>to appear at icassp 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a communication-efficient multi-processor compressed sensing
framework based on the approximate message passing algorithm is proposed. We
perform lossy compression on the data being communicated between processors,
resulting in a reduction in communication costs with a minor degradation in
recovery quality. In the proposed framework, a new state evolution formulation
takes the quantization error into account, and analytically determines the
coding rate required in each iteration. Two approaches for allocating the
coding rate, an online back-tracking heuristic and an optimal allocation scheme
based on dynamic programming, provide significant reductions in communication
costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04602</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04602</id><created>2016-01-15</created><authors><author><keyname>Taylor-Sakyi</keyname><forenames>Kevin</forenames></author></authors><title>Big Data: Understanding Big Data</title><categories>cs.DC cs.DB</categories><comments>8 pages, Big Data Analytics, Data Storage, MapReduce,
  Knowledge-Space, Big Data Inconsistencies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steve Jobs, one of the greatest visionaries of our time was quoted in 1996
saying &quot;a lot of times, people do not know what they want until you show it to
them&quot; [38] indicating he advocated products to be developed based on human
intuition rather than research. With the advancements of mobile devices, social
networks and the Internet of Things, enormous amounts of complex data, both
structured and unstructured are being captured in hope to allow organizations
to make better business decisions as data is now vital for an organizations
success. These enormous amounts of data are referred to as Big Data, which
enables a competitive advantage over rivals when processed and analyzed
appropriately. However Big Data Analytics has a few concerns including
Management of Data-lifecycle, Privacy &amp; Security, and Data Representation. This
paper reviews the fundamental concept of Big Data, the Data Storage domain, the
MapReduce programming paradigm used in processing these large datasets, and
focuses on two case studies showing the effectiveness of Big Data Analytics and
presents how it could be of greater good in the future if handled
appropriately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04605</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04605</id><created>2016-01-18</created><authors><author><keyname>Sloan</keyname><forenames>Marc</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Dynamic Information Retrieval: Theoretical Framework and Application</title><categories>cs.IR</categories><comments>ACM SIGIR International Conference on the Theory of Information
  Retrieval (ICTIR), 10 pages, 4 figures, 2 algorithms, 3 tables. in
  Proceedings of the 2015 International Conference on The Theory of Information
  Retrieval</comments><acm-class>H.3.3</acm-class><doi>10.1145/2808194.2809457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretical frameworks like the Probability Ranking Principle and its more
recent Interactive Information Retrieval variant have guided the development of
ranking and retrieval algorithms for decades, yet they are not capable of
helping us model problems in Dynamic Information Retrieval which exhibit the
following three properties; an observable user signal, retrieval over multiple
stages and an overall search intent. In this paper a new theoretical framework
for retrieval in these scenarios is proposed. We derive a general dynamic
utility function for optimizing over these types of tasks, that takes into
account the utility of each stage and the probability of observing user
feedback. We apply our framework to experiments over TREC data in the dynamic
multi page search scenario as a practical demonstration of its effectiveness
and to frame the discussion of its use, its limitations and to compare it
against the existing frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04615</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04615</id><created>2016-01-18</created><updated>2016-01-19</updated><authors><author><keyname>Sloan</keyname><forenames>Marc</forenames></author><author><keyname>Yang</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>A Term-Based Methodology for Query Reformulation Understanding</title><categories>cs.IR</categories><comments>Information Retrieval Journal, 23 pages, 10 tables, 6 figures</comments><acm-class>H.3.3</acm-class><journal-ref>Information Retrieval Journal, April 2015, Volume 18, Issue 2, pp
  145-165</journal-ref><doi>10.1007/s10791-015-9251-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key to any research involving session search is the understanding of how a
user's queries evolve throughout the session. When a user creates a query
reformulation, he or she is consciously retaining terms from their original
query, removing others and adding new terms. By measuring the similarity
between queries we can make inferences on the user's information need and how
successful their new query is likely to be. By identifying the origins of added
terms we can infer the user's motivations and gain an understanding of their
interactions.
  In this paper we present a novel term-based methodology for understanding and
interpreting query reformulation actions. We use TREC Session Track data to
demonstrate how our technique is able to learn from query logs and we make use
of click data to test user interaction behavior when reformulating queries. We
identify and evaluate a range of term-based query reformulation strategies and
show that our methods provide valuable insight into understanding query
reformulation in session search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04619</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04619</id><created>2016-01-18</created><authors><author><keyname>Liang</keyname><forenames>Haoyi</forenames></author><author><keyname>Weller</keyname><forenames>Daniel S.</forenames></author></authors><title>Comparison-based Image Quality Assessment for Parameter Selection</title><categories>cs.CV</categories><comments>12 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image quality assessment (IQA) is traditionally classified into
full-reference (FR) IQA and no-reference (NR) IQA according to whether the
original image is required. Although NR-IQA is widely used in practical
applications, room for improvement still remains because of the lack of the
reference image. Inspired by the fact that in many applications, such as
parameter selection, a series of distorted images are available, the authors
propose a novel comparison-based image quality assessment (C-IQA) method. The
new comparison-based framework parallels FR-IQA by requiring two input images,
and resembles NR-IQA by not using the original image. As a result, the new
comparison-based approach has more application scenarios than FR-IQA does, and
takes greater advantage of the accessible information than the traditional
single-input NR-IQA does. Further, C-IQA is compared with other
state-of-the-art NR-IQA methods on two widely used IQA databases. Experimental
results show that C-IQA outperforms the other NR-IQA methods for parameter
selection, and the parameter trimming framework combined with C-IQA saves the
computation of iterative image reconstruction up to 80%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04621</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04621</id><created>2016-01-18</created><authors><author><keyname>Chamberlain</keyname><forenames>Benjamin Paul</forenames></author><author><keyname>Humby</keyname><forenames>Clive</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>Detecting the Age of Twitter Users</title><categories>cs.SI stat.ML</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter provides an extremely rich and open source of data for studying human
behaviour at scale. It has been used to advance our understanding of social
network structure, the viral flow of information and how new ideas develop.
Enriching Twitter with demographic information would permit more precise
science and better generalisation to the real world. The only demographic
indicators associated with a Twitter account are the free text name, location
and description fields. We show how the age of most Twitter accounts can be
inferred with high accuracy using the structure of the social graph. Besides
classical social science applications, there are obvious privacy and child
protection implications to this discovery. Previous work on Twitter age
detection has focussed on either user-name or linguistic features of tweets. A
shortcoming of the user-name approach is that it requires real names (Twitter
names are often false) and census data from each user's (unknown) birth
country. Problems with linguistic approaches are that most Twitter users do not
tweet (the median number of Tweets is 4) and a different model must be learnt
for each language. To address these issues, we devise a language-independent
methodology for determining the age of Twitter users from data that is native
to the Twitter ecosystem. Roughly 150,000 Twitter users specify an age in their
free text description field. We generalize this to the entire Twitter network
by showing that age can be predicted based on what or whom they follow. We
adopt a Bayesian classification paradigm, which offers a consistent framework
for handling uncertainty in our data, e.g., inaccurate age descriptions or
spurious edges in the graph. Working within this paradigm we have successfully
applied age detection to 700 million Twitter accounts with an F1 Score of 0.86.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04622</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04622</id><created>2016-01-18</created><authors><author><keyname>Ghaemmaghami</keyname><forenames>Seyed Salman Sajjadi</forenames></author><author><keyname>Haghbin</keyname><forenames>Afrooz</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author></authors><title>Investigating the Performances and Vulnerabilities of Two New Protocols
  Based on R-RAPSE</title><categories>cs.CR</categories><comments>5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio Frequency IDentification (RFID) is a pioneer technology which has
depicted a new lifestyle for humanity in all around the world. Every day we
observe an increase in the scope of RFID applications and no one cannot
withdraw its numerous usage around him/herself. An important issue which should
be considered is providing privacy and security requirements of an RFID system.
Recently in 2014, Cai et al. proposed two improved RFID authentication
protocols based on R-RAPS rules by the names of IHRMA and I2SRS. In this paper,
we investigate the privacy of the aforementioned protocols based on Ouafi and
Phan formal privacy model and show that both IHRMA and I2SRS protocols cannot
provide private authentication for RFID users. Moreover, we showthat these
protocols are vulnerable to impersonation, DoS and traceability attacks. Then,
by considering the drawbacks of the studied protocols and implementation of
messages with new structures, we present two improved efficient and secure
authentication protocols to ameliorate the performance of Cai et al schemes.
Our analysis illustrate that the existing weaknesses of the discussed protocols
are eliminated in our proposed protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04657</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04657</id><created>2016-01-18</created><authors><author><keyname>Wu</keyname><forenames>Youlong</forenames></author></authors><title>Achievable Rate Regions for Cooperative Relay Broadcast Channels with
  Rate-limited Feedback</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achievable rate regions for cooperative relay broadcast channels with
rate-limited feedback are proposed. Specifically, we consider two-receiver
memoryless broadcast channels where each receiver sends feedback signals to the
transmitter through a noiseless and rate-limited feedback link, and meanwhile,
acts as relay to transmit cooperative information to the other receiver. It's
shown that the proposed rate regions improve on the known regions that consider
either relaying cooperation or feedback communication, but not both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04661</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04661</id><created>2016-01-18</created><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author></authors><title>Efficient Quantile Computation in Markov Chains via Counting Problems
  for Parikh Images</title><categories>cs.FL cs.CC cs.DM cs.LO</categories><acm-class>F.4.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cost Markov chain is a Markov chain whose transitions are labelled with
non-negative integer costs. A fundamental problem on this model, with
applications in the verification of stochastic systems, is to compute
information about the distribution of the total cost accumulated in a run. This
includes the probability of large total costs, the median cost, and other
quantiles. While expectations can be computed in polynomial time, previous work
has demonstrated that the computation of cost quantiles is harder but can be
done in PSPACE. In this paper we show that cost quantiles in cost Markov chains
can be computed in the counting hierarchy, thus providing evidence that
computing those quantiles is likely not PSPACE-hard. We obtain this result by
exhibiting a tight link to a problem in formal language theory: counting the
number of words that are both accepted by a given automaton and have a given
Parikh image. Motivated by this link, we comprehensively investigate the
complexity of the latter problem. Among other techniques, we rely on the
so-called BEST theorem for efficiently computing the number of Eulerian
circuits in a directed graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04662</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04662</id><created>2016-01-18</created><authors><author><keyname>Perera</keyname><forenames>Sirani M.</forenames></author></authors><title>Signal Flow Graph Approach to Efficient DST I-IV Algorithms</title><categories>cs.IT math.IT</categories><msc-class>15A23, 15B10, 65F50, 65T50, 65Y05, 65Y20, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, fast and efficient discrete sine transformation (DST)
algorithms are presented based on the factorization of sparse, scaled
orthogonal, rotation, rotation-reflection, and butterfly matrices. These
algorithms are completely recursive and solely based on DST I-IV. The presented
algorithms have low arithmetic cost compared to the known fast DST algorithms.
Furthermore, the language of signal flow graph representation of digital
structures is used to describe these efficient and recursive DST algorithms
having $(n-1)$ points signal flow graph for DST-I and $n$ points signal flow
graphs for DST II-IV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04667</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04667</id><created>2016-01-18</created><authors><author><keyname>Eschenfeldt</keyname><forenames>Patrick</forenames></author><author><keyname>Schmidt</keyname><forenames>Dan</forenames></author><author><keyname>Draper</keyname><forenames>Stark</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan</forenames></author></authors><title>Proactive Message Passing on Memory Factor Networks</title><categories>cs.AI cs.CV</categories><comments>35 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new type of graphical model that we call a &quot;memory factor
network&quot; (MFN). We show how to use MFNs to model the structure inherent in many
types of data sets. We also introduce an associated message-passing style
algorithm called &quot;proactive message passing&quot;' (PMP) that performs inference on
MFNs. PMP comes with convergence guarantees and is efficient in comparison to
competing algorithms such as variants of belief propagation. We specialize MFNs
and PMP to a number of distinct types of data (discrete, continuous, labelled)
and inference problems (interpolation, hypothesis testing), provide examples,
and discuss approaches for efficient implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04669</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04669</id><created>2016-01-18</created><authors><author><keyname>Nishigaki</keyname><forenames>Morimichi</forenames></author><author><keyname>Ferm&#xfc;ller</keyname><forenames>Cornelia</forenames></author></authors><title>The Image Torque Operator for Contour Processing</title><categories>cs.CV</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contours are salient features for image description, but the detection and
localization of boundary contours is still considered a challenging problem.
This paper introduces a new tool for edge processing implementing the
Gestaltism idea of edge grouping. This tool is a mid-level image operator,
called the Torque operator, that is designed to help detect closed contours in
images. The torque operator takes as input the raw image and creates an image
map by computing from the image gradients within regions of multiple sizes a
measure of how well the edges are aligned to form closed convex contours.
Fundamental properties of the torque are explored and illustrated through
examples. Then it is applied in pure bottom-up processing in a variety of
applications, including edge detection, visual attention and segmentation and
experimentally demonstrated a useful tool that can improve existing techniques.
Finally, its extension as a more general grouping mechanism and application in
object recognition is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04672</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04672</id><created>2016-01-18</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Physical maze solvers. All twelve prototypes implement 1961 Lee
  algorithm</title><categories>cs.ET</categories><comments>Final version of the paper will be published in &quot;Emergent
  Computation. Festschrift for Selim Akl&quot; (Springer, 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We overview experimental laboratory prototypes of maze solvers. We speculate
that all maze solvers implement Lee algorithm by first developing a gradient of
values showing a distance from any site of the maze to the destination site and
then tracing a path from a given source site to the destination site. All
prototypes approximate a set of many-source-one-destination paths using
resistance, chemical and temporal gradients. They trace a path from a given
source site to the destination site using electrical current, fluidic, growth
of slime mould, Marangoni flow, crawling of epithelial cells, excitation waves
in chemical medium, propagating crystallisation patterns. Some of the
prototypes visualise the path using a stream of dye, thermal camera or glow
discharge; others require a computer to extract the path from time lapse images
of the tracing. We discuss the prototypes in terms of speed, costs and
durability of the path visualisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04675</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04675</id><created>2016-01-15</created><authors><author><keyname>Indrusiak</keyname><forenames>Leandro Soares</forenames></author><author><keyname>Dziurzanski</keyname><forenames>Piotr</forenames></author><author><keyname>Singh</keyname><forenames>Amit Kumar</forenames></author></authors><title>2nd International Workshop on Dynamic Resource Allocation and Management
  in Embedded, High Performance and Cloud Computing (DREAMCloud 2016)</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume represents the proceedings of the 2nd International Workshop on
Dynamic Resource Allocation and Management in Embedded, High Performance and
Cloud Computing (DREAMCloud 2016), co-located with HiPEAC 2016 on 19th January
2016 in Prague, Czech Republic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04684</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04684</id><created>2016-01-18</created><authors><author><keyname>Wei</keyname><forenames>Peng</forenames></author><author><keyname>Dan</keyname><forenames>Lilin</forenames></author><author><keyname>Xiao</keyname><forenames>Yue</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>A Class of Low-Interference N-Continuous OFDM Schemes</title><categories>cs.IT math.IT</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  N-continuous orthogonal frequency division multiplexing (NC-OFDM) was
demonstrated to provide significant sidelobe suppression for baseband OFDM
signals. However, it will introduce severe interference to the transmit
signals. Hence in this letter, we specifically design a class of
low-interference NC-OFDM schemes for alleviating the introduced interference.
Meanwhile, we also obtain an asymptotic spectrum analysis by a closed-form
expression. It is shown that the proposed scheme is capable of reducing the
interference to a negligible level, and hence to save the high complexity of
signal recovery at the receiver, while maintaining similar sidelobe suppression
performance compared to traditional NC-OFDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04689</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04689</id><created>2016-01-18</created><authors><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>&#x15e;a&#x15f;o&#x11f;lu</keyname><forenames>Eren</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Reed-Muller Codes Achieve Capacity on Erasure Channels</title><categories>cs.IT math.IT</categories><comments>This article combines our previous articles arXiv:1505.05123 and
  arXiv:1505.05831</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to proving that a sequence of deterministic
linear codes achieves capacity on an erasure channel under maximum a posteriori
decoding. Rather than relying on the precise structure of the codes our method
exploits code symmetry. In particular, the technique applies to any sequence of
linear codes where the blocklengths are strictly increasing, the code rates
converge, and the permutation group of each code is doubly transitive. In other
words, we show that symmetry alone implies near-optimal performance.
  An important consequence of this result is that a sequence of Reed-Muller
codes with increasing blocklength and converging rate achieves capacity. This
possibility has been suggested previously in the literature but it has only
been proven for cases where the limiting code rate is 0 or 1. Moreover, these
results extend naturally to all affine-invariant codes and, thus, to extended
primitive narrow-sense BCH codes. This also resolves, in the affirmative, the
existence question for capacity-achieving sequences of binary cyclic codes. The
primary tools used in the proof are the sharp threshold property for symmetric
monotone boolean functions and the area theorem for extrinsic information
transfer functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04692</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04692</id><created>2016-01-18</created><authors><author><keyname>Gallier</keyname><forenames>Jean</forenames></author></authors><title>Spectral Theory of Unsigned and Signed Graphs. Applications to Graph
  Clustering: a Survey</title><categories>cs.LG cs.DS</categories><comments>122 pages. arXiv admin note: substantial text overlap with
  arXiv:1311.2492</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a survey of the method of graph cuts and its applications to graph
clustering of weighted unsigned and signed graphs. I provide a fairly thorough
treatment of the method of normalized graph cuts, a deeply original method due
to Shi and Malik, including complete proofs. The main thrust of this paper is
the method of normalized cuts. I give a detailed account for K = 2 clusters,
and also for K &gt; 2 clusters, based on the work of Yu and Shi. I also show how
both graph drawing and normalized cut K-clustering can be easily generalized to
handle signed graphs, which are weighted graphs in which the weight matrix W
may have negative coefficients. Intuitively, negative coefficients indicate
distance or dissimilarity. The solution is to replace the degree matrix by the
matrix in which absolute values of the weights are used, and to replace the
Laplacian by the Laplacian with the new degree matrix of absolute values. As
far as I know, the generalization of K-way normalized clustering to signed
graphs is new. Finally, I show how the method of ratio cuts, in which a cut is
normalized by the size of the cluster rather than its volume, is just a special
case of normalized cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04724</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04724</id><created>2016-01-18</created><authors><author><keyname>Mollaebrahim</keyname><forenames>Siavash</forenames></author><author><keyname>Ghari</keyname><forenames>Pouya</forenames></author><author><keyname>Fazel</keyname><forenames>Mohammad Sadegh</forenames></author></authors><title>Interference Alignment in MIMO Interference Channels using SDP
  Relaxation</title><categories>cs.IT cs.NI math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern wireless networks, interference is common problem that can degrade
the performance of such networks. Interference alignment is a management
technique that align interference from other transmitters in the least possibly
dimension subspace at each receiver and as a result, provide the remaining
dimensions for free interference signal. Uncoordinated interference is as an
example of interference which cannot coordinately aligned with interference
from coordinated part. In this paper, we propose two rank minimization methods
to enhance the performance of interference alignment in the presence of
uncoordinated interference sources.Firstly, sparsity of the objective function
is studied and new objective function is proposed then, a new class of convex
relaxation is proposed with respect to uncoordinated interferences. Moreover,
we use schatten-p-norm as surrogate of rank function and we implement
iteratively reweighted algorithm to solve optimization problem.Finally, our
simulation results show that proposed methods can obtain considerably higher
multiplexing gain than other approaches in the interference alignment
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04734</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04734</id><created>2016-01-18</created><updated>2016-01-20</updated><authors><author><keyname>Niemeyer</keyname><forenames>Kyle E.</forenames></author><author><keyname>Smith</keyname><forenames>Arfon M.</forenames></author><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author></authors><title>The challenge and promise of software citation for credit,
  identification, discovery, and reuse</title><categories>cs.CY cs.DL</categories><comments>Challenge paper submitted to ACM Journal of Data and Information
  Quality. v2: corrected author order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present the challenge of software citation as a method to
ensure credit for and identification, discovery, and reuse of software in
scientific and engineering research. We discuss related work and key
challenges/research directions, including suggestions for metadata necessary
for software citation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04737</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04737</id><created>2016-01-18</created><updated>2016-02-25</updated><authors><author><keyname>Roosta-Khorasani</keyname><forenames>Farbod</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Sub-Sampled Newton Methods I: Globally Convergent Algorithms</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale optimization problems are ubiquitous in machine learning and data
analysis and there is a plethora of algorithms for solving such problems. Many
of these algorithms employ sub-sampling, as a way to either speed up the
computations and/or to implicitly implement a form of statistical
regularization. In this paper, we consider second-order iterative optimization
algorithms and we provide bounds on the convergence of the variants of Newton's
method that incorporate uniform sub-sampling as a means to estimate the
gradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our
algorithms are global and are guaranteed to converge from any initial iterate.
  Using random matrix concentration inequalities, one can sub-sample the
Hessian to preserve the curvature information. Our first algorithm incorporates
Hessian sub-sampling while using the full gradient. We also give additional
convergence results for when the sub-sampled Hessian is regularized by
modifying its spectrum or ridge-type regularization. Next, in addition to
Hessian sub-sampling, we also consider sub-sampling the gradient as a way to
further reduce the computational complexity per iteration. We use approximate
matrix multiplication results from randomized numerical linear algebra to
obtain the proper sampling strategy. In all these algorithms, computing the
update boils down to solving a large scale linear system, which can be
computationally expensive. As a remedy, for all of our algorithms, we also give
global convergence results for the case of inexact updates where such linear
system is solved only approximately.
  This paper has a more advanced companion paper, [42], in which we demonstrate
that, by doing a finer-grained analysis, we can get problem-independent bounds
for local convergence of these algorithms and explore trade-offs to improve
upon the basic results of the present paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04738</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04738</id><created>2016-01-18</created><updated>2016-02-25</updated><authors><author><keyname>Roosta-Khorasani</keyname><forenames>Farbod</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Sub-Sampled Newton Methods II: Local Convergence Rates</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many data-fitting applications require the solution of an optimization
problem involving a sum of large number of functions of high dimensional
parameter. Here, we consider the problem of minimizing a sum of $n$ functions
over a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both
$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$
can offer great amount of computational efficiency.
  Within the context of second order methods, we first give quantitative local
convergence results for variants of Newton's method where the Hessian is
uniformly sub-sampled. Using random matrix concentration inequalities, one can
sub-sample in a way that the curvature information is preserved. Using such
sub-sampling strategy, we establish locally Q-linear and Q-superlinear
convergence rates. We also give additional convergence results for when the
sub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type
regularization.
  Finally, in addition to Hessian sub-sampling, we consider sub-sampling the
gradient as way to further reduce the computational complexity per iteration.
We use approximate matrix multiplication results from randomized numerical
linear algebra (RandNLA) to obtain the proper sampling strategy and we
establish locally R-linear convergence rates. In such a setting, we also show
that a very aggressive sample size increase results in a R-superlinearly
convergent algorithm.
  While the sample size depends on the condition number of the problem, our
convergence rates are problem-independent, i.e., they do not depend on the
quantities related to the problem. Hence, our analysis here can be used to
complement the results of our basic framework from the companion paper, [38],
by exploring algorithmic trade-offs that are important in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04740</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04740</id><created>2016-01-18</created><updated>2016-02-19</updated><authors><author><keyname>Shvartzshnaider</keyname><forenames>Yan</forenames></author><author><keyname>Tong</keyname><forenames>Schrasing</forenames></author><author><keyname>Wies</keyname><forenames>Thomas</forenames></author><author><keyname>Kift</keyname><forenames>Paula</forenames></author><author><keyname>Nissenbaum</keyname><forenames>Helen</forenames></author><author><keyname>Subramanian</keyname><forenames>Lakshminarayanan</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author></authors><title>Crowdsourced, Actionable and Verifiable Contextual Informational Norms</title><categories>cs.CY</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is often a fundamental mismatch between programmable privacy
frameworks, on the one hand, and the ever shifting privacy expectations of
computer system users, on the other hand. Based on the theory of contextual
integrity (CI), our paper addresses this problem by proposing a privacy
framework that translates users' privacy expectations (norms) into a set of
actionable privacy rules that are rooted in the language of CI. These norms are
then encoded using Datalog logic specification to develop an information system
that is able to verify whether information flows are appropriate and the
privacy of users thus preserved. A particular benefit of our framework is that
it can automatically adapt as users' privacy expectations evolve over time.
  To evaluate our proposed framework, we conducted an extensive survey
involving more than 450 participants and 1400 questions to derive a set of
privacy norms in the educational context. Based on the crowdsourced responses,
we demonstrate that our framework can derive a compact Datalog encoding of the
privacy norms which can in principle be directly used for enforcing privacy of
information flows within this context. In addition, our framework can
automatically detect logical inconsistencies between individual users' privacy
expectations and the derived privacy logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04743</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04743</id><created>2016-01-18</created><authors><author><keyname>Williams</keyname><forenames>Ryan</forenames></author></authors><title>Strong ETH Breaks With Merlin and Arthur: Short Non-Interactive Proofs
  of Batch Evaluation</title><categories>cs.CC cs.CR</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient proof system for Multipoint Arithmetic Circuit
Evaluation: for every arithmetic circuit $C(x_1,\ldots,x_n)$ of size $s$ and
degree $d$ over a field ${\mathbb F}$, and any inputs $a_1,\ldots,a_K \in
{\mathbb F}^n$,
  $\bullet$ the Prover sends the Verifier the values $C(a_1), \ldots, C(a_K)
\in {\mathbb F}$ and a proof of $\tilde{O}(K \cdot d)$ length, and
  $\bullet$ the Verifier tosses $\textrm{poly}(\log(dK|{\mathbb
F}|/\varepsilon))$ coins and can check the proof in about $\tilde{O}(K \cdot(n
+ d) + s)$ time, with probability of error less than $\varepsilon$.
  For small degree $d$, this &quot;Merlin-Arthur&quot; proof system (a.k.a. MA-proof
system) runs in nearly-linear time, and has many applications. For example, we
obtain MA-proof systems that run in $c^{n}$ time (for various $c &lt; 2$) for the
Permanent, $\#$Circuit-SAT for all sublinear-depth circuits, counting
Hamiltonian cycles, and infeasibility of $0$-$1$ linear programs. In general,
the value of any polynomial in Valiant's class ${\sf VP}$ can be certified
faster than &quot;exhaustive summation&quot; over all possible assignments. These results
strongly refute a Merlin-Arthur Strong ETH and Arthur-Merlin Strong ETH posed
by Russell Impagliazzo and others.
  We also give a three-round (AMA) proof system for quantified Boolean formulas
running in $2^{2n/3+o(n)}$ time, nearly-linear time MA-proof systems for
counting orthogonal vectors in a collection and finding Closest Pairs in the
Hamming metric, and a MA-proof system running in $n^{k/2+O(1)}$-time for
counting $k$-cliques in graphs.
  We point to some potential future directions for refuting the
Nondeterministic Strong ETH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04745</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04745</id><created>2016-01-18</created><authors><author><keyname>Zhao</keyname><forenames>Xiaoxue</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>A Theoretical Analysis of Two-Stage Recommendation for Cold-Start
  Collaborative Filtering</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a theoretical framework for tackling the cold-start
collaborative filtering problem, where unknown targets (items or users) keep
coming to the system, and there is a limited number of resources (users or
items) that can be allocated and related to them. The solution requires a
trade-off between exploitation and exploration as with the limited
recommendation opportunities, we need to, on one hand, allocate the most
relevant resources right away, but, on the other hand, it is also necessary to
allocate resources that are useful for learning the target's properties in
order to recommend more relevant ones in the future. In this paper, we study a
simple two-stage recommendation combining a sequential and a batch solution
together. We first model the problem with the partially observable Markov
decision process (POMDP) and provide an exact solution. Then, through an
in-depth analysis over the POMDP value iteration solution, we identify that an
exact solution can be abstracted as selecting resources that are not only
highly relevant to the target according to the initial-stage information, but
also highly correlated, either positively or negatively, with other potential
resources for the next stage. With this finding, we propose an approximate
solution to ease the intractability of the exact solution. Our initial results
on synthetic data and the Movie Lens 100K dataset confirm the performance gains
of our theoretical development and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04746</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04746</id><created>2016-01-18</created><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Miller</keyname><forenames>Gary</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Scalable Constrained Clustering: A Generalized Spectral Method</title><categories>cs.SI</categories><comments>accepted to appear in AISTATS 2016. arXiv admin note: text overlap
  with arXiv:1504.00653</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple spectral approach to the well-studied constrained
clustering problem. It captures constrained clustering as a generalized
eigenvalue problem with graph Laplacians. The algorithm works in nearly-linear
time and provides concrete guarantees for the quality of the clusters, at least
for the case of 2-way partitioning. In practice this translates to a very fast
implementation that consistently outperforms existing spectral approaches both
in speed and quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04749</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04749</id><created>2016-01-18</created><authors><author><keyname>Khamse-Ashari</keyname><forenames>Jalal</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author><author><keyname>Zhao</keyname><forenames>Yiqiang</forenames></author></authors><title>Constrained Multi-user Multi-server Max-Min Fair Queuing</title><categories>cs.NI cs.PF</categories><comments>16 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multi-user multi-server queuing system is studied in which
each user is constrained to get service from a subset of servers. In the
studied system, rate allocation in the sense of max-min fairness results in
multi-level fair rates. To achieve such fair rates, we propose $CM^4FQ$
algorithm. In this algorithm users are chosen for service on a packet by packet
basis. The priority of each user $i$ to be chosen at time $t$ is determined
based on a parameter known as service tag (representing the amount of work
counted for user $i$ till time $t$). Hence, a free server will choose to serve
an eligible user with the minimum service tag. Based on such simple selection
criterion, $CM^4FQ$ aims at guaranteed fair throughput for each demanding user
without explicit knowledge of each server service rate. We argue that $CM^4FQ$
can be applied in a variety of practical queuing systems specially in mobile
cloud computing architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04755</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04755</id><created>2016-01-18</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Approximating the $k$-Level in Three-Dimensional Plane Arrangements</title><categories>cs.CG</categories><comments>Preliminary version appeared in SODA 16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\SetX}{\mathsf{X}} \newcommand{\eps}{\varepsilon}
\newcommand{\VorX}[1]{\mathcal{V} \pth{#1}} \newcommand{\Polygon}{\mathsf{P}}
\newcommand{\IntRange}[1]{[ #1 ]} \newcommand{\Space}{\ovebarline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)} \newcommand{\Arr}{{\cal A}}$
  Let $H$ be a set of $n$ planes in three dimensions, and let $r \leq n$ be a
parameter. We give a simple alternative proof of the existence of a
$(1/r)$-cutting of the first $n/r$ levels of $\Arr(H)$, which consists of
$O(r)$ semi-unbounded vertical triangular prisms. The same construction yields
an approximation of the $(n/r)$-level by a terrain consisting of $O(r/\eps^3)$
triangular faces, which lies entirely between the levels $(1\pm\eps)n/r$. The
proof does not use sampling, and exploits techniques based on planar separators
and various structural properties of levels in three-dimensional arrangements
and of planar maps. The proof is constructive, and leads to a simple randomized
algorithm, with expected near-linear running time. An application of this
technique allows us to mimic Matousek's construction of cuttings in the plane,
to obtain a similar construction of &quot;layered&quot; $(1/r)$-cutting of the entire
arrangement $\Arr(H)$, of optimal size $O(r^3)$. Another application is a
simplified optimal approximate range counting algorithm in three dimensions,
competing with that of Afshani and Chan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04756</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04756</id><created>2016-01-18</created><authors><author><keyname>Lauron</keyname><forenames>Maureen Lyndel C.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Improved Sampling Techniques for Learning an Imbalanced Data Set</title><categories>cs.LG</categories><comments>7 pages, 10 figures, 16th Philippine Computing Science Congress (PCSC
  2016)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents the performance of a classifier built using the stackingC
algorithm in nine different data sets. Each data set is generated using a
sampling technique applied on the original imbalanced data set. Five new
sampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax Random
Oversampling, Lax Random Undersampling, Combined-Lax Random Oversampling
Undersampling, and Combined-Lax Random Undersampling Oversampling) that were
based on the three sampling techniques (i.e., Random Undersampling, Random
Oversampling, and Synthetic Minority Oversampling Technique) usually used as
solutions in imbalance learning. The metrics used to evaluate the classifier's
performance were F-measure and G-mean. F-measure determines the performance of
the classifier for every class, while G-mean measures the overall performance
of the classifier. The results using F-measure showed that for the data without
a sampling technique, the classifier's performance is good only for the
majority class. It also showed that among the eight sampling techniques, RU and
LRU have the worst performance while other techniques (i.e., RO, C-LRUO and
C-LROU) performed well only on some classes. The best performing techniques in
all data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measure
values between 0.5 and 0.65. The results using G-mean showed that the
oversampling technique that attained the highest G-mean value is LRO (0.86),
next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83).
Combining the result of the two metrics (F-measure and G-mean), only the three
sampling techniques are considered as good performing (i.e., LRO, SMOTE, and
SMOTERandRep).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04770</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04770</id><created>2016-01-18</created><updated>2016-02-10</updated><authors><author><keyname>Luo</keyname><forenames>Enming</forenames></author><author><keyname>Chan</keyname><forenames>Stanley H.</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Q.</forenames></author></authors><title>Adaptive Image Denoising by Mixture Adaptation</title><categories>cs.CV stat.ME</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an adaptive learning procedure to learn effective image priors.
The new algorithm, called the Expectation-Maximization (EM) adaptation, takes a
generic prior learned from a generic external database and adapts it to the
image of interest to generate a specific prior. Different from existing methods
which combine internal and external statistics in an ad-hoc way, the proposed
algorithm learns a single unified prior through an adaptive process. There are
two major contributions in this paper. First, we rigorously derive the EM
adaptation algorithm from the Bayesian hyper-prior perspective and show that it
can be further simplified to improve the computational complexity. Second, in
the absence of the latent clean image, we show how EM adaptation can be
modified and applied on pre-filtered images. We discuss how to estimate
internal parameters and demonstrate how to improve the denoising performance by
running EM adaptation iteratively. Experimental results show that the adapted
prior is consistently better than the originally un-adapted prior, and is
superior than some state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04779</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04779</id><created>2016-01-18</created><authors><author><keyname>Sahu</keyname><forenames>Anit Kumar</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author></authors><title>Recursive Distributed Detection for Composite Hypothesis Testing:
  Algorithms and Asymptotics</title><categories>cs.IT math.IT math.PR</categories><comments>54 pages. Submitted for publication. Initial Submission: Jan. 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies recursive composite hypothesis testing in a network of
sparsely connected agents. The network objective is to test a simple null
hypothesis against a composite alternative concerning the state of the field,
modeled as a vector of (continuous) unknown parameters determining the
parametric family of probability measures induced on the agents' observation
spaces under the hypotheses. Specifically, under the alternative hypothesis,
each agent sequentially observes an independent and identically distributed
time-series consisting of a (nonlinear) function of the true but unknown
parameter corrupted by Gaussian noise, whereas, under the null, they obtain
noise only. Two distributed recursive generalized likelihood ratio test type
algorithms of the \emph{consensus+innovations} form are proposed, namely
$\mathcal{CILRT}$ and $\mathcal{CIGLRT}$, in which the agents estimate the
underlying parameter and in parallel also update their test decision statistics
by simultaneously processing the latest local sensed information and
information obtained from neighboring agents. For $\mathcal{CIGLRT}$, for a
broad class of nonlinear observation models and under a global observability
condition, algorithm parameters which ensure asymptotically decaying
probabilities of errors~(probability of miss and probability of false
detection) are characterized. For $\mathcal{CILRT}$, a linear observation model
is considered and large deviations decay exponents for the error probabilities
are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04780</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04780</id><created>2016-01-18</created><authors><author><keyname>Anshel</keyname><forenames>Iris</forenames></author><author><keyname>Atkins</keyname><forenames>Derek</forenames></author><author><keyname>Goldfeld</keyname><forenames>Dorian</forenames></author><author><keyname>Gunnells</keyname><forenames>Paul E.</forenames></author></authors><title>Defeating the Ben-Zvi, Blackburn, and Tsaban Attack on the Algebraic
  Eraser</title><categories>cs.CR</categories><msc-class>20F36, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Algebraic Eraser Diffie-Hellman (AEDH) protocol was introduced in 2005
and published in 2006 by Anshel-Anshel-Goldfeld-Lemieux as a protocol suitable
for use on platforms with constrained computational resources, such as FPGAs,
ASICs, and wireless sensors. It is a group-theoretic cryptographic protocol
that allows two users to construct a shared secret via a Diffie-Hellman-type
scheme over an insecure channel.
  Building on the refuted 2012 permutation-based attack of
Kalka-Teichner-Tsaban, in 2015 Ben-Zvi-Blackburn-Tsaban (BBT) presented a
heuristic attack that attempts to recover the AEDH shared secret. In their
paper BBT reference the AEDH protocol as presented to ISO for certification
(ISO 29167-20) by SecureRF. The ISO draft contains two profiles using the
Algebraic Eraser. One profile is unaffected by this attack; the second profile
is subject to their attack provided the attack runs in real time. This is not
the case in most practical deployments.
  The BBT attack is simply a targeted attack that does not attempt to break the
method, system parameters, or recover any private keys. Rather, its limited
focus is to recover the shared secret in a single transaction. In addition, the
BBT attack is based on several conjectures that are assumed to hold when
parameters are chosen according to standard distributions, which can be
mitigated, if not avoided. This paper shows how to choose special distributions
so that these conjectures do not hold making the BBT attack ineffective for
braid groups with sufficiently many strands. Further, the BBT attack assumes
that certain data is available to an attacker, but there are realistic
deployment scenarios where this is not the case, making the attack fail
completely. In summary, the BBT attack is flawed (with respect to the SecureRF
ISO draft) and, at a minimum, over-reaches as to its applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04794</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04794</id><created>2016-01-18</created><authors><author><keyname>Liu</keyname><forenames>Changqing</forenames></author></authors><title>Solutions to the Problem of K-SAT / K-COL Phase Transition Location</title><categories>cs.CC</categories><comments>Extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As general theories, currently there are concentration inequalities (of
random walk) only for the cases of independence and martingale differences. In
this paper, the concentration inequalities are extended to more general
situations. In terms of the theory presented in the paper, the condition of
independence is $\frac { \partial y } {\partial t}=$ constant and martingale
difference's is $\frac { \partial y } {\partial t} = 0 $. This paper relaxes
these conditions to $\frac { \partial^2 y } {\partial u_i \partial t} \le L$;
i.e. $\frac { \partial y } {\partial t}$ can vary. Further, the concentration
inequalities are extended to branching random walk, the applications of which
solve some long standing open problems, including the well known problems of
K-SAT and K-COL phase transition locations, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04795</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04795</id><created>2016-01-18</created><authors><author><keyname>Wei</keyname><forenames>Peng</forenames></author><author><keyname>Dan</keyname><forenames>Lilin</forenames></author><author><keyname>Xiao</keyname><forenames>Yue</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Improved N-continuous OFDM for 5G Wireless Communications</title><categories>cs.IT math.IT</categories><comments>16 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  N-continuous orthogonal frequency division multiplexing (NC-OFDM) is a
promising technique to obtain significant sidelobe suppression for baseband
OFDM signals, in future 5G wireless communications. However, the precoder of
NC-OFDM usually causes severe interference and high complexity. To reduce the
interference and complexity, this paper proposes an improved time-domain
N-continuous OFDM (TD-NC-OFDM) by shortening the smooth signal, which is
linearly combined by rectangularly pulsed OFDM basis signals truncated by a
smooth window. Furthermore, we obtain an asymptotic spectrum analysis of the
TD-NC-OFDM signals by a closed-form expression, calculate its low complexity in
OFDM transceiver, and derive a closed-form expression of the received
signal-to-interference-plus-noise ratio (SINR). Simulation results show that
the proposed low-interference TD-NC-OFDM can achieve similar suppression
performance but introduce negligible bit error rate (BER) degradation and much
lower computational complexity, compared to conventional NC-OFDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04797</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04797</id><created>2016-01-18</created><authors><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Sampei</keyname><forenames>Seiichi</forenames></author></authors><title>Wi-Fi/WiGig Coordination for Optimal WiGig Concurrent Transmissions in
  Random Access Scenario</title><categories>cs.NI cs.ET</categories><comments>5 Pages, 5 Figures, IEEE VTC Spring 2016. arXiv admin note: text
  overlap with arXiv:1506.05857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Gigabit (WiGig) access points (APs) using 60 GHz unlicensed
frequency band are considered as key enablers for future Gbps WLANs. Due to its
short range transmission with high susceptibility to path blocking, a multiple
number of WiGig APs should be installed to fully cover a typical target
environment. However, using autonomously operated WiGig APs with IEEE 802.11ad
DCF, the exhaustive search analog beamforming and the maximum received power
based autonomous users association prevent the establishment of optimal WiGig
concurrent links that maximize the total system throughput in random access
scenarios. In this paper, we formulate the problem of WiGig concurrent
transmissions in random access scenarios as an optimization problem, then we
propose a Wi-Fi/WiGig coordination architecture to solve it. The proposed
coordinated Wi-Fi/WiGig WLAN is based on a tight coordination between the 5 GHz
(Wi-Fi) and the 60 GHz (WiGig) unlicensed frequency bands. By which, the wide
coverage Wi-Fi band controls the establishment of the WiGig concurrent links.
Statistical learning using Wi-Fi fingerprinting is used for estimating the best
candidate AP and its best beam identification (ID) for establishing the WiGig
concurrent link without making any interference to the existing WiGig data
links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04798</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04798</id><created>2016-01-18</created><updated>2016-02-18</updated><authors><author><keyname>Jie</keyname><forenames>Zequn</forenames></author><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Lu</keyname><forenames>Wen Feng</forenames></author><author><keyname>Tay</keyname><forenames>Eng Hock Francis</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Scale-aware Pixel-wise Object Proposal Networks</title><categories>cs.CV</categories><comments>13 pages,14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object proposal is essential for current state-of-the-art object detection
pipelines. However, the existing proposal methods generally fail in producing
results with satisfying localization accuracy. The case is even worse for small
objects which however are quite common in practice. In this paper we propose a
novel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle the
challenges. The SPOP network can generate proposals with high recall rate and
average best overlap (ABO), even for small objects. In particular, in order to
improve the localization accuracy, a fully convolutional network is employed
which predicts locations of object proposals for each pixel. The produced
ensemble of pixel-wise object proposals enhances the chance of hitting the
object significantly without incurring heavy extra computational cost. To solve
the challenge of localizing objects at small scale, two localization networks
which are specialized for localizing objects with different scales are
introduced, following the divide-and-conquer philosophy. Location outputs of
these two networks are then adaptively combined to generate the final proposals
by a large-/small-size weighting network. Extensive evaluations on PASCAL VOC
2007 show the SPOP network is superior over the state-of-the-art models. The
high-quality proposals from SPOP network also significantly improve the mean
average precision (mAP) of object detection with Fast-RCNN framework. Finally,
the SPOP network (trained on PASCAL VOC) shows great generalization performance
when testing it on ILSVRC 2013 validation set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04800</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04800</id><created>2016-01-18</created><authors><author><keyname>Kang</keyname><forenames>Zhao</forenames></author><author><keyname>Peng</keyname><forenames>Chong</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author></authors><title>Top-N Recommender System via Matrix Completion</title><categories>cs.IR cs.AI cs.LG stat.ML</categories><comments>AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Top-N recommender systems have been investigated widely both in industry and
academia. However, the recommendation quality is far from satisfactory. In this
paper, we propose a simple yet promising algorithm. We fill the user-item
matrix based on a low-rank assumption and simultaneously keep the original
information. To do that, a nonconvex rank relaxation rather than the nuclear
norm is adopted to provide a better rank approximation and an efficient
optimization strategy is designed. A comprehensive set of experiments on real
datasets demonstrates that our method pushes the accuracy of Top-N
recommendation to a new level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04802</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04802</id><created>2016-01-19</created><updated>2016-01-24</updated><authors><author><keyname>Gan</keyname><forenames>Ting</forenames></author><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author><author><keyname>Kapur</keyname><forenames>Deepak</forenames></author><author><keyname>Chen</keyname><forenames>Mingshuai</forenames></author></authors><title>Interpolation synthesis for quadratic polynomial inequalities and
  combination with \textit{EUF}</title><categories>cs.LO</categories><comments>40 pages, 1 figures</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for generating interpolants for formulas which are conjunctions
of quadratic polynomial inequalities (both strict and nonstrict) is proposed.
The algorithm is based on a key observation that quadratic polynomial
inequalities can be linearized if they are concave. A generalization of
Motzkin's transposition theorem is proved, which is used to generate an
interpolant between two mutually contradictory conjunctions of polynomial
inequalities, using semi-definite programming in time complexity
$\mathcal{O}(n^3+nm))$, where $n$ is the number of variables and $m$ is the
number of inequalities. Using the framework proposed by \cite{SSLMCS2008} for
combining interpolants for a combination of quantifier-free theories which have
their own interpolation algorithms, a combination algorithm is given for the
combined theory of concave quadratic polynomial inequalities and the equality
theory over uninterpreted functions symbols (\textit{EUF}). The proposed
approach is applicable to all existing abstract domains like \emph{octagon},
\emph{polyhedra}, \emph{ellipsoid} and so on, therefore it can be used to
improve the scalability of existing verification techniques for programs and
hybrid systems. In addition, we also discuss how to extend our approach to
formulas beyond concave quadratic polynomials using Gr\&quot;{o}bner basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04805</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04805</id><created>2016-01-19</created><updated>2016-02-11</updated><authors><author><keyname>Ngo</keyname><forenames>Anh Cat Le</forenames></author><author><keyname>See</keyname><forenames>John</forenames></author><author><keyname>Phan</keyname><forenames>Raphael Chung-Wei</forenames></author></authors><title>Sparsity in Dynamics of Spontaneous Subtle Emotions: Analysis \&amp;
  Application</title><categories>cs.CV</categories><comments>IEEE Transaction of Affective Computing (2016)</comments><doi>10.1109/TAFFC.2016.2523996</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spontaneous subtle emotions are expressed through micro-expressions, which
are tiny, sudden and short-lived dynamics of facial muscles; thus poses a great
challenge for visual recognition. The abrupt but significant dynamics for the
recognition task are temporally sparse while the rest, irrelevant dynamics, are
temporally redundant. In this work, we analyze and enforce sparsity constrains
to learn significant temporal and spectral structures while eliminate
irrelevant facial dynamics of micro-expressions, which would ease the challenge
in the visual recognition of spontaneous subtle emotions. The hypothesis is
confirmed through experimental results of automatic spontaneous subtle emotion
recognition with several sparsity levels on CASME II and SMIC, the only two
publicly available spontaneous subtle emotion databases. The overall
performances of the automatic subtle emotion recognition are boosted when only
significant dynamics are preserved from the original sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04807</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04807</id><created>2016-01-19</created><authors><author><keyname>Shangguan</keyname><forenames>Chong</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Separating hash families: A Johnson-type bound and new constructions</title><categories>cs.DM cs.IT math.CO math.IT</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separating hash families are useful combinatorial structures which are
generalizations of many well-studied objects in combinatorics, cryptography and
coding theory. In this paper, using tools from graph theory and additive number
theory, we solve several open problems and conjectures concerning bounds and
constructions for separating hash families. Firstly, we discover that the
cardinality of a separating hash family satisfies a Johnson-type inequality. As
a result, we obtain a new upper bound, which is superior to all previous ones.
Secondly, we present a construction for an infinite class of perfect hash
families. It is based on the Hamming graphs in coding theory and generalizes
many constructions that appeared before. It provides an affirmative answer to
both Bazrafshan-Trung's open problem on separating hash families and
Alon-Stav's conjecture on parent-identifying codes. Thirdly, let $p_t(N,q)$
denote the maximal cardinality of a $t$-perfect hash family of length $N$ over
an alphabet of size $q$. Walker II and Colbourn conjectured that
$p_3(3,q)=o(q^2)$. We verify this conjecture by proving
$q^{2-o(1)}&lt;p_3(3,q)=o(q^2)$. Our proof can be viewed as an application of
Ruzsa-Szemer{\'e}di's (6,3)-theorem. We also prove
$q^{2-o(1)}&lt;p_4(4,q)=o(q^2)$. Two new notions in graph theory and additive
number theory, namely rainbow cycles and $R$-sum-free sets, are introduced to
prove this result. These two bounds support a question of Blackburn, Etzion,
Stinson and Zaverucha. Finally, we establish a bridge between perfect hash
families and hypergraph Tur{\'a}n problems. This connection has not been
noticed before. As a consequence, many new results and problems arise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04810</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04810</id><created>2016-01-19</created><updated>2016-02-26</updated><authors><author><keyname>Ge</keyname><forenames>Gennian</forenames></author><author><keyname>Shangguan</keyname><forenames>Chong</forenames></author></authors><title>Good traceability codes do exist</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the proof of Lemma 2.6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traceability codes are combinatorial objects introduced by Chor, Fiat and
Naor in 1994 to be used to trace the origin of digital content in traitor
tracing schemes.
  Let $F$ be an alphabet set of size $q$ and $n$ be a positive integer. A
$t$-traceability code is a code $\mathscr{C}\subseteq F^n$ which can be used to
catch at least one colluder from a collusion of at most $t$ traitors. It has
been shown that $t$-traceability codes do not exist for $q\le t$. When $q&gt;t^2$,
$t$-traceability codes with positive code rate can be constructed from error
correcting codes with large minimum distance. Therefore, Barg and Kabatiansky
asked in 2004 that whether there exist $t$-traceability codes with positive
code rate for $t+1\le q\le t^2$. In 2010, Blackburn, Etzion and Ng gave an
affirmative answer to this question for $q\ge t^2-\lceil t/2\rceil+1$, using
the probabilistic methods. However, they did not see how their probabilistic
methods can be used to answer this question for the remaining values of $q$.
They even suspected that there may be a `Plotkin bound' of traceability codes
that forbids the existence of such codes. In this paper, we give a complete
answer to Barg-Kabatiansky's question (in the affirmative). Surprisingly, our
construction is deterministic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04811</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04811</id><created>2016-01-19</created><updated>2016-01-19</updated><authors><author><keyname>Tu</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Liu</keyname><forenames>Xiaohua</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author></authors><title>Coverage-based Neural Machine Translation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention mechanism advanced state-of-the-art neural machine translation
(NMT) by jointly learning to align and translate. However, attentional NMT
ignores past alignment information, which leads to over-translation and
under-translation problems. In response to this problem, we maintain a coverage
vector to keep track of the attention history. The coverage vector is fed to
the attention model to help adjust the future attention, which guides NMT to
pay more attention to the untranslated source words. Experiments show that
coverage-based NMT significantly improves both alignment and translation
quality over NMT without coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04814</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04814</id><created>2016-01-19</created><updated>2016-03-08</updated><authors><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author></authors><title>Streaming Similarity Self-Join</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study the problem of computing the similarity self-join in a
streaming context (SSSJ), where the input is an unbounded stream of items
arriving continuously. The goal is to find all pairs of items in the stream
whose similarity is greater than a given threshold. The simplest formulation of
the problem requires unbounded memory, and thus, it is intractable. To make the
problem feasible, we introduce the notion of time-dependent similarity: the
similarity of two items decreases with the difference in their arrival time. By
leveraging the properties of this time-dependent similarity function, we design
two algorithmic frameworks to solve the sssj problem. The first one, MiniBatch
(MB), uses existing index-based filtering techniques for the static version of
the problem, and combines them in a pipeline. The second framework, Streaming
(STR), adds time filtering to the existing indexes, and integrates new
time-based bounds deeply in the working of the algorithms. We also introduce a
new indexing technique (L2), which is based on an existing state-of-the-art
indexing technique (L2AP), but is optimized for the streaming case. Extensive
experiments show that the STR algorithm, when instantiated with the L2 index,
is the most scalable option across a wide array of datasets and parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04816</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04816</id><created>2016-01-19</created><authors><author><keyname>Kaji</keyname><forenames>Shizuo</forenames></author></authors><title>Tetrisation of triangular meshes and its application in shape blending</title><categories>cs.GR cs.CG</categories><acm-class>I.3.5; I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The As-Rigid-As-Possible (ARAP) shape deformation framework is a versatile
technique for morphing, surface modelling, and mesh editing. We discuss an
improvement of the ARAP framework in a few aspects: 1. Given a triangular mesh
in 3D space, we introduce a method to associate a tetrahedral structure, which
encodes the geometry of the original mesh. 2. We use a Lie algebra based method
to interpolate local transformation, which provides better handling of rotation
with large angle. 3. We propose a new error function to compile local
transformations into a global piecewise linear map, which is rotation invariant
and easy to minimise. We implemented a shape blender based on our algorithm and
its MIT licensed source code is available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04820</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04820</id><created>2016-01-19</created><authors><author><keyname>Mostefaoui</keyname><forenames>Achour</forenames><affiliation>LINA</affiliation></author><author><keyname>Raynal</keyname><forenames>Michel</forenames><affiliation>ASAP</affiliation></author></authors><title>Time-Efficient Read/Write Register in Crash-prone Asynchronous
  Message-Passing Systems</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The atomic register is certainly the most basic object of computing science.
Its implementation on top of an n-process asynchronous message-passing system
has received a lot of attention. It has been shown that t \textless{} n/2
(where t is the maximal number of processes that may crash) is a necessary and
sufficient requirement to build an atomic register on top of a crash-prone
asynchronous message-passing system. Considering such a context, this paper
visits the notion of a fast implementation of an atomic register, and presents
a new time-efficient asynchronous algorithm. Its time-efficiency is measured
according to two different underlying synchrony assumptions. Whatever this
assumption, a write operation always costs a round-trip delay, while a read
operation costs always a round-trip delay in favorable circumstances
(intuitively, when it is not concurrent with a write). When designing this
algorithm, the design spirit was to be as close as possible to the one of the
famous ABD algorithm (proposed by Attiya, Bar-Noy, and Dolev).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04829</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04829</id><created>2016-01-19</created><authors><author><keyname>Kamga</keyname><forenames>Gervais N.</forenames></author><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>A&#xef;ssa</keyname><forenames>Sonia</forenames></author></authors><title>Spectral-Efficiency Analysis of Massive MIMO Systems in Centralized and
  Distributed Schemes</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications, accepted for publication,
  January 2016; 12 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the spectral efficiency of massive multiple-input
multiple-output (MIMO) systems in both centralized and distributed
configurations, referred to as C-MIMO and D-MIMO, respectively. By accounting
for real environmental parameters and antenna characteristics, namely, path
loss, shadowing effect, multi-path fading and antenna correlation, a novel
comprehensive channel model is first proposed in closed-form, which is
applicable to both types of MIMO schemes. Then, based on the proposed model,
the asymptotic behavior of the spectral efficiency of the MIMO channel under
both the centralized and distributed configurations is analyzed and compared in
exact forms, by exploiting the theory of very long random vectors. Afterwards,
a case study is performed by applying the obtained results into MIMO networks
with circular coverage. In such a case, it is attested that for the D-MIMO of
cell radius $r_{\mathrm{c}}$ and circular antenna array of
radius~$r_{\mathrm{a}}$, the optimal value of~$r_{\mathrm{a}}$ that maximizes
the average spectral efficiency is accurately established by
$r_{\mathrm{a}}^{\mathrm{opt}}=r_{\mathrm{c}}/1.31$. Monte Carlo simulation
results corroborate the developed spectral-efficiency analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04848</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04848</id><created>2016-01-19</created><authors><author><keyname>Scheitle</keyname><forenames>Quirin</forenames></author><author><keyname>Wachs</keyname><forenames>Matthias</forenames></author><author><keyname>Zirngibl</keyname><forenames>Johannes</forenames></author><author><keyname>Carle</keyname><forenames>Georg</forenames></author></authors><title>Analyzing Locality of Mobile Messaging Traffic using the MATAdOR
  Framework</title><categories>cs.NI</categories><comments>To appear at Passive and Active Measurements Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile messaging services have gained a large share in global
telecommunications. Unlike conventional services like phone calls, text
messages or email, they do not feature a standardized environment enabling a
federated and potentially local service architecture. We present an extensive
and large-scale analysis of communication patterns for four popular mobile
messaging services between 28 countries and analyze the locality of
communication and the resulting impact on user privacy. We show that server
architectures for mobile messaging services are highly centralized in single
countries. This forces messages to drastically deviate from a direct
communication path, enabling hosting and transfer countries to potentially
intercept and censor traffic. To conduct this work, we developed a measurement
framework to analyze traffic of such mobile messaging services. It allows to
conduct automated experiments with mobile messaging applications, is
transparent to those applications and does not require any modifications to the
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04859</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04859</id><created>2016-01-19</created><authors><author><keyname>Annamalai</keyname><forenames>N.</forenames></author><author><keyname>Durairajan</keyname><forenames>C.</forenames></author></authors><title>The Structure of Z_2[u]Z_2[u, v]-additive Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the algebraic structure of Z_2[u]Z_2[u, v]-additive
codes which are Z_2[u, v]-submodules where u^2 = v^2 = 0 and uv = vu. In
particular, we determine a Gray map from Z_2[u]Z_2 [u, v] to
Z_2^{2{\alpha}+8\b{eta}} and study generator and parity check matrices for
these codes. Further we study the structure of Z_2[u]Z_2[u, v]-additive cyclic
codes and constacyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04862</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04862</id><created>2016-01-19</created><authors><author><keyname>Richter</keyname><forenames>Christoph</forenames></author><author><keyname>Jentzsch</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Hostettler</keyname><forenames>Rafael</forenames></author><author><keyname>Garrido</keyname><forenames>Jes&#xfa;s A.</forenames></author><author><keyname>Ros</keyname><forenames>Eduardo</forenames></author><author><keyname>Knoll</keyname><forenames>Alois C.</forenames></author><author><keyname>R&#xf6;hrbein</keyname><forenames>Florian</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author><author><keyname>Conradt</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Scalability in Neural Control of Musculoskeletal Robots</title><categories>cs.RO cs.DC cs.NE cs.SY</categories><comments>Accepted at IEEE Robotics and Automation Magazine on 2015-12-31</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anthropomimetic robots are robots that sense, behave, interact and feel like
humans. By this definition, anthropomimetic robots require human-like physical
hardware and actuation, but also brain-like control and sensing. The most
self-evident realization to meet those requirements would be a human-like
musculoskeletal robot with a brain-like neural controller. While both
musculoskeletal robotic hardware and neural control software have existed for
decades, a scalable approach that could be used to build and control an
anthropomimetic human-scale robot has not been demonstrated yet. Combining
Myorobotics, a framework for musculoskeletal robot development, with SpiNNaker,
a neuromorphic computing platform, we present the proof-of-principle of a
system that can scale to dozens of neurally-controlled, physically compliant
joints. At its core, it implements a closed-loop cerebellar model which
provides real-time low-level neural control at minimal power consumption and
maximal extensibility: higher-order (e.g., cortical) neural networks and
neuromorphic sensors like silicon-retinae or -cochleae can naturally be
incorporated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04871</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04871</id><created>2016-01-19</created><updated>2016-01-24</updated><authors><author><keyname>Montazeri</keyname><forenames>Mitra</forenames></author><author><keyname>Montazeri</keyname><forenames>Mahdieh</forenames></author><author><keyname>Saryazdi</keyname><forenames>Saeid</forenames></author></authors><title>Eye detection in digital images: challenges and solutions</title><categories>cs.CV</categories><comments>2th National Conference of Electrical Engineering (NEEC2011),2011,
  Esfehan, Iran, in Persian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eye Detection has an important role in the field of biometric identification
and known as one method of person's identification. In recent years, many
efforts have been done which can detect eye automatically and with different
image conditions. However, each method has its own drawbacks which can control
some of these conditions. In this paper, different methods of eye detection
will be categorized and explained. In each category, the advantages and
disadvantages of each method will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04876</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04876</id><created>2016-01-19</created><authors><author><keyname>Brock-Nannestad</keyname><forenames>Taus</forenames></author><author><keyname>Ilik</keyname><forenames>Danko</forenames></author></authors><title>An Intuitionistic Formula Hierarchy Based on High-School Identities</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit intuitionistic proof theory from the point of view of the formula
isomorphisms arising from high-school identities. We first show how sequent
calculi for intuitionistic proposition logic, and in particular the G4ip
calculus of Vorob'ev, Hudelmaier, and Dyckhoff can be represented as a complete
proof calculus that nevertheless contains no invertible proof rules, called the
high-school (HS) variant of G4ip. We then show that all the rules of G4ip and
HS admit an arithmetical interpretation, namely each such proof rule can be
reduced to an inequality between exponential polynomials. Finally, we extend
the exponential polynomial analogy to first-order quantifiers, showing that it
gives rise to a simple intuitionistic hierarchy of formulas, the first one that
classifies formulas up to isomorphism, and proceeds along the same equivalences
that lead to the classical arithmetical hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04884</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04884</id><created>2016-01-19</created><authors><author><keyname>Srinivasulu</keyname><forenames>B.</forenames></author></authors><title>Z2-Triple cyclic codes and their duals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Z2-triple cyclic code of block length (r,s,t) is a binary code of length
r+s+t such that the code is partitioned into three parts of lengthsr,s andt
such that each of the three parts is invariant under the cyclic shifts of the
coordinates. Such a code can be viewed as Z2[x]-submodules of
Z_2[x]/&lt;x^r-1&gt;xZ_2[x]/&lt;x^s-1&gt;xZ_2[x]/&lt;x^t-1&gt;, in polynomial representation. In
this paper, we determine the structure of these codes. We have obtained the
form of the generators for such codes. Further, a minimal generating set for
such a code is obtained. Also, we study the structure of the duals of these
codes via the generators of the codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04888</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04888</id><created>2016-01-19</created><updated>2016-01-19</updated><authors><author><keyname>Wu</keyname><forenames>Tai-Pang</forenames></author><author><keyname>Yeung</keyname><forenames>Sai-Kit</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author><author><keyname>Tang</keyname><forenames>Chi-Keung</forenames></author><author><keyname>Medioni</keyname><forenames>Gerard</forenames></author></authors><title>A Closed-Form Solution to Tensor Voting: Theory and Applications</title><categories>cs.CV</categories><comments>Addendum appended to the TPAMI paper</comments><journal-ref>TPAMI 34(8): 1482-1495 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a closed-form solution to tensor voting (CFTV): given a point set in
any dimensions, our closed-form solution provides an exact, continuous and
efficient algorithm for computing a structure-aware tensor that simultaneously
achieves salient structure detection and outlier attenuation. Using CFTV, we
prove the convergence of tensor voting on a Markov random field (MRF), thus
termed as MRFTV, where the structure-aware tensor at each input site reaches a
stationary state upon convergence in structure propagation. We then embed
structure-aware tensor into expectation maximization (EM) for optimizing a
single linear structure to achieve efficient and robust parameter estimation.
Specifically, our EMTV algorithm optimizes both the tensor and fitting
parameters and does not require random sampling consensus typically used in
existing robust statistical techniques. We performed quantitative evaluation on
its accuracy and robustness, showing that EMTV performs better than the
original TV and other state-of-the-art techniques in fundamental matrix
estimation for multiview stereo matching. The extensions of CFTV and EMTV for
extracting multiple and nonlinear structures are underway. An addendum is
included in this arXiv version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04889</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04889</id><created>2016-01-19</created><updated>2016-03-01</updated><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Nikoloska</keyname><forenames>Ivana</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author></authors><title>Wireless Networks with Energy Harvesting and Power Transfer: Joint Power
  and Time Allocation</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures in IEEE Signal Processing Letters, vol. 23, no. 1,
  January 2016</comments><doi>10.1109/LSP.2015.2500340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider wireless powered communication networks which
could operate perpetually, as the base station (BS) broadcasts energy to the
multiple energy harvesting (EH) information transmitters. These employ &quot;harvest
then transmit&quot; mechanism, as they spend all of their energy harvested during
the previous BS energy broadcast to transmit the information towards the BS.
Assuming time division multiple access (TDMA), we propose a novel transmission
scheme for jointly optimal allocation of the BS broadcasting power and time
sharing among the wireless nodes, which maximizes the overall network
throughput, under the constraint of average transmit power and maximum transmit
power at the BS. The proposed scheme significantly outperforms &quot;state of the
art&quot; schemes that employ only the optimal time allocation. If a single EH
transmitter is considered, we generalize the optimal solutions for the case of
fixed circuit power consumption, which refers to a much more practical
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04890</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04890</id><created>2016-01-19</created><updated>2016-03-02</updated><authors><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Women Through the Glass Ceiling: Gender Asymmetries in Wikipedia</title><categories>cs.SI</categories><comments>23 pages. Published in EPJ Data Science 2016 5:5</comments><doi>10.1140/epjds/s13688-016-0066-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contributing to the writing of history has never been as easy as it is today
thanks to Wikipedia, a community-created encyclopedia that aims to document the
world's knowledge from a neutral point of view. Though everyone can participate
it is well known that the editor community has a narrow diversity, with a
majority of white male editors. While this participatory \emph{gender gap} has
been studied extensively in the literature, this work sets out to \emph{assess
potential gender inequalities in Wikipedia articles} along different
dimensions: notability, topical focus, linguistic bias, structural properties,
and meta-data presentation.
  We find that (i) women in Wikipedia are more notable than men, which we
interpret as the outcome of a subtle glass ceiling effect; (ii) family-,
gender-, and relationship-related topics are more present in biographies about
women; (iii) linguistic bias manifests in Wikipedia since abstract terms tend
to be used to describe positive aspects in the biographies of men and negative
aspects in the biographies of women; and (iv) there are structural differences
in terms of meta-data and hyperlinks, which have consequences for
information-seeking activities. While some differences are expected, due to
historical and social contexts, other differences are attributable to Wikipedia
editors. The implications of such differences are discussed having Wikipedia
contribution policies in mind. We hope that the present work will contribute to
increased awareness about, first, gender issues in the content of Wikipedia,
and second, the different levels on which gender biases can manifest on the
Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04893</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04893</id><created>2016-01-19</created><updated>2016-02-12</updated><authors><author><keyname>Hayes</keyname><forenames>Jamie</forenames></author></authors><title>Traffic Confirmation Attacks Despite Noise</title><categories>cs.CR</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a traffic confirmation attack on low-latency mix networks based on
computing robust real-time binary hashes of network traffic flows. Firstly, we
adapt the Coskun-Memon Algorithm to construct hashes that can withstand network
impairments to allow fast matching of network flows. The resulting attack has a
low startup cost and achieves a true positive match rate of 80% when matching
one flow out of 9000 with less than 2% false positives, showing traffic
confirmation attacks can be highly accurate even when only part of the network
traffic flow is seen. Secondly, we attack probabilistic padding schemes
achieving a match rate of over 90% from 9000 network traffic flows, showing
advanced padding techniques are still vulnerable to traffic confirmation
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04895</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04895</id><created>2016-01-19</created><authors><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author></authors><title>Bounds on the Lambert function and their application to the outage
  analysis of user cooperation</title><categories>cs.IT math.IT</categories><comments>4 pages, accepted for publication in the IEEE Communications Letters</comments><doi>10.1109/LCOMM.2013.070113.130972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems formulated in terms of logarithmic or exponential equations often
use the Lambert $W$ function in their solutions. Expansions, approximations and
bounds on $W$ have been derived in an effort to gain a better understanding of
the relationship between equation parameters. In this paper, we focus on one of
the branches of $W$, denoted as $W_{-1}$, we derive tractable upper and lower
bounds and we illustrate their usefulness in identifying conditions under which
user cooperation can yield a lower outage probability than non-cooperative
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04896</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04896</id><created>2016-01-19</created><authors><author><keyname>Rosales</keyname><forenames>Jose Luis</forenames></author><author><keyname>Martin</keyname><forenames>Vicente</forenames></author></authors><title>On the Quantum Simulation of the Factorization Problem</title><categories>quant-ph cs.CR math-ph math.MP</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feynman's prescription for a quantum computer was to find a Hamitonian for a
system that could serve as a computer. Here we concentrate in a system to solve
the problem of decomposing a large number $N$ into its prime factors. The
spectrum of this computer is exactly calculated obtaining the factors of $N$
from the arithmetic function that represents the energy of the computer.
  As a corollary, in the semi-classical large $N$ limit, we compute a new prime
counting asymptote $\pi(x|N)$, where $x$ is a candidate to factorize $N$, that
has no counterpart in analytic number theory. This rises the conjecture that
the quantum solution of factoring obtains prime numbers, thus reaching
consistency with Euclid's unique factorization theorem: primes should be
quantum numbers of a Feynman's factoring simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04902</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04902</id><created>2016-01-19</created><authors><author><keyname>Fuhl</keyname><forenames>Wolfgang</forenames></author><author><keyname>Santini</keyname><forenames>Thiago</forenames></author><author><keyname>Kasneci</keyname><forenames>Gjergji</forenames></author><author><keyname>Kasneci</keyname><forenames>Enkelejda</forenames></author></authors><title>PupilNet: Convolutional Neural Networks for Robust Pupil Detection</title><categories>cs.CV</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time, accurate, and robust pupil detection is an essential prerequisite
for pervasive video-based eye-tracking. However, automated pupil detection in
real-world scenarios has proven to be an intricate challenge due to fast
illumination changes, pupil occlusion, non centered and off-axis eye recording,
and physiological eye characteristics. In this paper, we propose and evaluate a
method based on a novel dual convolutional neural network pipeline. In its
first stage the pipeline performs coarse pupil position identification using a
convolutional neural network and subregions from a downscaled input image to
decrease computational costs. Using subregions derived from a small window
around the initial pupil position estimate, the second pipeline stage employs
another convolutional neural network to refine this position, resulting in an
increased pupil detection rate up to 25% in comparison with the best performing
state-of-the-art algorithm. Annotated data sets can be made available upon
request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04908</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04908</id><created>2016-01-19</created><updated>2016-01-25</updated><authors><author><keyname>Bankova</keyname><forenames>Desislava</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author><author><keyname>Lewis</keyname><forenames>Martha</forenames></author><author><keyname>Marsden</keyname><forenames>Daniel</forenames></author></authors><title>Graded Entailment for Compositional Distributional Semantics</title><categories>cs.CL cs.AI cs.LO math.CT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The categorical compositional distributional model of natural language
provides a conceptually motivated procedure to compute the meaning of
sentences, given grammatical structure and the meanings of its words. This
approach has outperformed other models in mainstream empirical language
processing tasks. However, until recently it has lacked the crucial feature of
lexical entailment -- as do other distributional models of meaning.
  In this paper we solve the problem of entailment for categorical
compositional distributional semantics. Taking advantage of the abstract
categorical framework allows us to vary our choice of model. This enables the
introduction of a notion of entailment, exploiting ideas from the categorical
semantics of partial knowledge in quantum computation.
  The new model of language uses density matrices, on which we introduce a
novel robust graded order capturing the entailment strength between concepts.
This graded measure emerges from a general framework for approximate
entailment, induced by any commutative monoid. Quantum logic embeds in our
graded order.
  Our main theorem shows that entailment strength lifts compositionally to the
sentence level, giving a lower bound on sentence entailment. We describe the
essential properties of graded entailment such as continuity, and provide a
procedure for calculating entailment strength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04920</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04920</id><created>2016-01-19</created><authors><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Understanding Deep Convolutional Networks</title><categories>stat.ML cs.CV cs.LG</categories><comments>17 pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional networks provide state of the art classifications and
regressions results over many high-dimensional problems. We review their
architecture, which scatters data with a cascade of linear filter weights and
non-linearities. A mathematical framework is introduced to analyze their
properties. Computations of invariants involve multiscale contractions, the
linearization of hierarchical symmetries, and sparse separations. Applications
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04935</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04935</id><created>2016-01-19</created><authors><author><keyname>Bonnet</keyname><forenames>&#xc9;douard</forenames></author><author><keyname>Egri</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Fixed-parameter Approximability of Boolean MinCSPs</title><categories>cs.CC</categories><msc-class>68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum unsatisfiability version of a constraint satisfaction problem
asks for an assignment where the number of unsatisfied constraints is minimum
possible, or equivalently, asks for a minimum-size set of constraints whose
deletion makes the instance satisfiable. For a finite set $\Gamma$ of
constraints, we denote by MinCSP($\Gamma$) the restriction of the problem where
each constraint is from $\Gamma$. The polynomial-time solvability and the
polynomial-time approximability of MinCSP($\Gamma$) were fully characterized by
Khanna et al. Here we study the fixed-parameter approximability of the problem:
given an instance and an integer $k$, one has to find a solution of size at
most $g(k)$ in time $f(k)\cdot n^{O(1)}$ if a solution of size at most $k$
exists. We especially focus on the case of constant-factor FP-approximability.
Our main result classifies each finite constraint language $\Gamma$ into one of
three classes: MinCSP($\Gamma$) has a constant-factor FP-approximation.
MinCSP($\Gamma$) has a (constant-factor) FP-approximation if and only if
Nearest Codeword has a (constant-factor) FP-approximation. MinCSP($\Gamma$) has
no FP-approximation, unless $FPT=W[P]$. We give some evidence that problems in
the second class do not have constant-factor FP-approximations: we show that
there is no such approximation if both the Exponential-Time Hypothesis (ETH)
and the Linear PCP Conjecture (LPC) are true, and we also show that such an
approximation would imply the existence of an FP-approximation for the
$k$-Densest Subgraph problem with ratio $1-\epsilon$ for any $\epsilon&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04943</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04943</id><created>2016-01-19</created><updated>2016-01-20</updated><authors><author><keyname>Staton</keyname><forenames>Sam</forenames></author><author><keyname>Yang</keyname><forenames>Hongseok</forenames></author><author><keyname>Heunen</keyname><forenames>Chris</forenames></author><author><keyname>Kammar</keyname><forenames>Ohad</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Semantics for probabilistic programming: higher-order functions,
  continuous distributions, and soft constraints</title><categories>cs.PL cs.AI</categories><acm-class>F.3.2; D.3.1; I.2.5; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the semantic foundation of expressive probabilistic programming
languages, that support higher-order functions, continuous distributions, and
soft constraints (such as Anglican, Church, and Venture). We define a
metalanguage (an idealised version of Anglican) for probabilistic computation
with the above features, develop both operational and denotational semantics,
and prove soundness, adequacy, and termination. They involve measure theory,
stochastic labelled transition systems, and functor categories, but admit
intuitive computational readings, one of which views sampled random variables
as dynamically allocated read-only variables. We apply our semantics to
validate nontrivial equations underlying the correctness of certain compiler
optimisations and inference algorithms such as sequential Monte Carlo
simulation. The language enables defining probability distributions on
higher-order functions, and we study their properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04950</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04950</id><created>2016-01-19</created><authors><author><keyname>Bensman</keyname><forenames>Stephen J.</forenames></author><author><keyname>Smolinsky</keyname><forenames>Lawrence J.</forenames></author></authors><title>Lotka's Inverse Square Law of Scientific Productivity: Its Methods and
  Statistics</title><categories>cs.DL</categories><comments>12 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This brief communication analyzes the statistics and methods Lotka used to
derive his inverse square law of scientific productivity from the standpoint of
modern theory. It finds that he violated the norms of this theory by extremely
truncating his data on the right. It also proves that Lotka himself played an
important role in establishing the commonly used method of identifying
power-law behavior by the R^2 fit to a regression line on a log-log plot that
modern theory considers unreliable by basing the derivation of his law on this
very method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04952</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04952</id><created>2016-01-19</created><authors><author><keyname>Trianni</keyname><forenames>Vito</forenames></author><author><keyname>De Simone</keyname><forenames>Daniele</forenames></author><author><keyname>Reina</keyname><forenames>Andreagiovanni</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author></authors><title>Emergence of Consensus in a Multi-Robot Network: from Abstract Models to
  Empirical Validation</title><categories>cs.MA cs.RO cs.SI physics.soc-ph</categories><comments>A supporting video is available here:
  https://mail.google.com/mail/u/0/#search/vito.trianni%40istc.cnr.it/15244cd6f27f0e99?projector=1</comments><journal-ref>Robotics and Automation Letters, IEEE , vol.PP, no.99, pp.1 (2016)</journal-ref><doi>10.1109/LRA.2016.2519537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consensus dynamics in decentralised multiagent systems are subject to intense
studies, and several different models have been proposed and analysed. Among
these, the naming game stands out for its simplicity and applicability to a
wide range of phenomena and applications, from semiotics to engineering.
Despite the wide range of studies available, the implementation of theoretical
models in real distributed systems is not always straightforward, as the
physical platform imposes several constraints that may have a bearing on the
consensus dynamics. In this paper, we investigate the effects of an
implementation of the naming game for the kilobot robotic platform, in which we
consider concurrent execution of games and physical interferences. Consensus
dynamics are analysed in the light of the continuously evolving communication
network created by the robots, highlighting how the different regimes crucially
depend on the robot density and on their ability to spread widely in the
experimental arena. We find that physical interferences reduce the benefits
resulting from robot mobility in terms of consensus time, but also result in
lower cognitive load for individual agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04961</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04961</id><created>2016-01-19</created><authors><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author></authors><title>Joint Crosstalk-Avoidance and Error-Correction Coding for Parallel Data
  Buses</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decreasing transistor sizes and lower voltage swings cause two distinct
problems for communication in integrated circuits. First, decreasing inter-wire
spacing increases interline capacitive coupling, which adversely affects
transmission energy and delay. Second, lower voltage swings render the
transmission susceptible to various noise sources. Coding can be used to
address both these problems. So-called crosstalk-avoidance codes mitigate
capacitive coupling, and traditional error-correction codes introduce
resilience against channel errors.
  Unfortunately, crosstalk-avoidance and error-correction codes cannot be
combined in a straightforward manner. On the one hand, crosstalk-avoidance
encoding followed by error-correction encoding destroys the crosstalk-avoidance
property. On the other hand, error-correction encoding followed by
crosstalk-avoidance encoding causes the crosstalk-avoidance decoder to fail in
the presence of errors. Existing approaches circumvent this difficulty by using
additional bus wires to protect the parities generated from the output of the
error-correction encoder, and are therefore inefficient.
  In this work we propose a novel joint crosstalk-avoidance and
error-correction coding and decoding scheme that provides higher bus
transmission rates compared to existing approaches. Our joint approach
carefully embeds the parities such that the crosstalk-avoidance property is
preserved. We analyze the rate and minimum distance of the proposed scheme. We
also provide a density evolution analysis and predict iterative decoding
thresholds for reliable communication under random bus erasures. This density
evolution analysis is nonstandard, since the crosstalk-avoidance constraints
are inherently nonlinear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04964</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04964</id><created>2016-01-19</created><authors><author><keyname>Duncan</keyname><forenames>Ross</forenames></author><author><keyname>Dunne</keyname><forenames>Kevin</forenames></author></authors><title>Interacting Frobenius Algebras are Hopf</title><categories>cs.LO math.CT quant-ph</categories><comments>32 pages; submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theories featuring the interaction between a Frobenius algebra and a Hopf
algebra have recently appeared in several areas in computer science: concurrent
programming, control theory, and quantum computing, among others. Bonchi,
Sobocinski, and Zanasi (2014) have shown that, given a suitable distributive
law, a pair of Hopf algebras forms two Frobenius algebras. Here we take the
opposite approach, and show that interacting Frobenius algebras form Hopf
algebras. We generalise (BSZ 2014) by including non-trivial dynamics of the
underlying object---the so-called phase group---and investigate the effects of
finite dimensionality of the underlying model. We recover the system of Bonchi
et al as a subtheory in the prime power dimensional case, but the more general
theory does not arise from a distributive law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04967</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04967</id><created>2016-01-19</created><updated>2016-02-04</updated><authors><author><keyname>Liu</keyname><forenames>Ling</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Polar Codes and Polar Lattices for Independent Fading Channels</title><categories>cs.IT math.IT</categories><comments>22 pages, 9 figures, extended version of ISIT 2016 submission. Some
  simulation results have been updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design polar codes and polar lattices for i.i.d. fading
channels when the channel state information is only available to the receiver.
For the binary input case, we propose a new design of polar codes through
single-stage polarization to achieve the ergodic capacity. For the non-binary
input case, polar codes are further extended to polar lattices to achieve the
egodic Poltyrev capacity, i.e., the capacity without power limit. When the
power constraint is taken into consideration, we show that polar lattices with
lattice Gaussian shaping achieve the egodic capacity of fading channels. The
coding and shaping are both explicit, and the overall complexity of encoding
and decoding is $O(N \log^2 N)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04974</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04974</id><created>2016-01-19</created><authors><author><keyname>Jetten</keyname><forenames>Laura</forenames></author><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author></authors><title>Nonbinary tree-based phylogenetic networks</title><categories>q-bio.PE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rooted phylogenetic networks are used to describe evolutionary histories that
contain non-treelike evolutionary events such as hybridization and horizontal
gene transfer. In some cases, such histories can be described by a phylogenetic
base-tree with additional linking arcs, which can for example represent gene
transfer events. Such phylogenetic networks are called tree-based. Here, we
consider two possible generalizations of this concept to nonbinary networks,
which we call tree-based and strictly-tree-based nonbinary phylogenetic
networks. We give simple graph-theoretic characterizations of tree-based and
strictly-tree-based nonbinary phylogenetic networks. Moreover, we show for each
of these two classes that it can be decided in polynomial time whether a given
network is contained in the class. Our approach also provides a new view on
tree-based binary phylogenetic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04975</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04975</id><created>2016-01-19</created><authors><author><keyname>Chevalier</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>Tight Bounds for Consensus Systems Convergence</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the asymptotic convergence of all infinite products of matrices
taken in a given finite set, by looking only at finite or periodic products. It
is known that when the matrices of the set have a common nonincreasing
polyhedral norm, all infinite products converge to zero if and only if all
infinite periodic products with period smaller than a certain value converge to
zero, and bounds exist on that value.
  We provide a stronger bound holding for both polyhedral norms and polyhedral
seminorms. In the latter case, the matrix products do not necessarily converge
to 0, but all trajectories of the associated system converge to a common
invariant space. We prove our bound to be tight, in the sense that for any
polyhedral seminorm, there is a set of matrices such that not all infinite
products converge, but every periodic product with period smaller than our
bound does converge.
  Our technique is based on an analysis of the combinatorial structure of the
face lattice of the unit ball of the nonincreasing seminorm. The bound we
obtain is equal to half the size of the largest antichain in this lattice.
Explicitly evaluating this quantity may be challenging in some cases. We
therefore link our problem with the Sperner property: the property that, for
some graded posets, -- in this case the face lattice of the unit ball -- the
size of the largest antichain is equal to the size of the largest rank level.
  We show that some sets of matrices with invariant polyhedral seminorms lead
to posets that do not have that Sperner property. However, this property holds
for the polyhedron obtained when treating sets of stochastic matrices, and our
bound can then be easily evaluated in that case. In particular, we show that
for the dimension of the space $n \geq 8$, our bound is smaller than the
previously known bound by a multiplicative factor of $\frac{3}{2 \sqrt{\pi
n}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.04980</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.04980</id><created>2016-01-19</created><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Nunes</keyname><forenames>Isabel</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Integrity Constraints for General-Purpose Knowledge Bases</title><categories>cs.DB</categories><comments>FoIKS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrity constraints in databases have been studied extensively since the
1980s, and they are considered essential to guarantee database integrity. In
recent years, several authors have studied how the same notion can be adapted
to reasoning frameworks, in such a way that they achieve the purpose of
guaranteeing a system's consistency, but are kept separate from the reasoning
mechanisms.
  In this paper we focus on multi-context systems, a general-purpose framework
for combining heterogeneous reasoning systems, enhancing them with a notion of
integrity constraints that generalizes the corresponding concept in the
database world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05002</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05002</id><created>2016-01-19</created><authors><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author><author><keyname>Sousa-Pinto</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Solvability of Matrix-Exponential Equations</title><categories>cs.DM cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a continuous analogue of Babai et al.'s and Cai et al.'s problem
of solving multiplicative matrix equations. Given $k+1$ square matrices $A_{1},
\ldots, A_{k}, C$, all of the same dimension, whose entries are real algebraic,
we examine the problem of deciding whether there exist non-negative reals
$t_{1}, \ldots, t_{k}$ such that \begin{align*} \prod \limits_{i=1}^{k}
\exp(A_{i} t_{i}) = C . \end{align*} We show that this problem is undecidable
in general, but decidable under the assumption that the matrices $A_{1},
\ldots, A_{k}$ commute. Our results have applications to reachability problems
for linear hybrid automata. Our decidability proof relies on a number of
theorems from algebraic and transcendental number theory, most notably those of
Baker, Kronecker, Lindemann, and Masser, as well as some useful geometric and
linear-algebraic results, including the Minkowski-Weyl theorem and a new (to
the best of our knowledge) result about the uniqueness of strictly upper
triangular matrix logarithms of upper unitriangular matrices. On the other
hand, our undecidability result is shown by reduction from Hilbert's Tenth
Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05003</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05003</id><created>2016-01-19</created><authors><author><keyname>Foucaud</keyname><forenames>Florent</forenames></author><author><keyname>Klasing</keyname><forenames>Ralf</forenames></author></authors><title>Parameterized and approximation complexity of the detection pair problem
  in graphs</title><categories>cs.DS cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the problem DETECTION PAIR. A detection pair of a
graph $G$ is a pair $(W,L)$ of sets of detectors with $W\subseteq V(G)$, the
watchers, and $L\subseteq V(G)$, the listeners, such that for every pair $u,v$
of vertices that are not dominated by a watcher of $W$, there is a listener of
$L$ whose distances to $u$ and to $v$ are different. The goal is to minimize
$|W|+|L|$. This problem generalizes the two classic problems DOMINATING SET and
METRIC DIMENSION, that correspond to the restrictions $L=\emptyset$ and
$W=\emptyset$, respectively. DETECTION PAIR was recently introduced by Finbow,
Hartnell and Young [A. S. Finbow, B. L. Hartnell and J. R. Young. The
complexity of monitoring a network with both watchers and listeners.
Manuscript, 2015], who proved it to be NP-complete on trees, a surprising
result given that both DOMINATING SET and METRIC DIMENSION are known to be
linear-time solvable on trees. It follows from an existing reduction by Hartung
and Nichterlein for METRIC DIMENSION that even on bipartite subcubic graphs of
arbitrarily large girth, DETECTION PAIR is NP-hard to approximate within a
sub-logarithmic factor and W[2]-hard (when parameterized by solution size). We
show, using a reduction to SET COVER, that DETECTION PAIR is approximable
within a factor logarithmic in the number of vertices of the input graph. Our
two main results are a linear-time $2$-approximation algorithm and an FPT
algorithm for DETECTION PAIR on trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05020</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05020</id><created>2016-01-19</created><updated>2016-03-08</updated><authors><author><keyname>Tischler</keyname><forenames>German</forenames></author></authors><title>Low Space External Memory Construction of the Succinct Permuted Longest
  Common Prefix Array</title><categories>cs.DS</categories><acm-class>G.2.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The longest common prefix (LCP) array is a versatile auxiliary data structure
in indexed string matching. It can be used to speed up searching using the
suffix array (SA) and provides an implicit representation of the topology of an
underlying suffix tree. The LCP array of a string of length $n$ can be
represented as an array of length $n$ words, or, in the presence of the SA, as
a bit vector of $2n$ bits plus asymptotically negligible support data
structures. External memory construction algorithms for the LCP array have been
proposed, but those proposed so far have a space requirement of $O(n)$ words
(i.e. $O(n \log n)$ bits) in external memory. This space requirement is in some
practical cases prohibitively expensive. We present an external memory
algorithm for constructing the $2n$ bit version of the LCP array which uses
$O(n \log \sigma)$ bits of additional space in external memory when given a
(compressed) BWT with alphabet size $\sigma$ and a sampled inverse suffix array
at sampling rate $O(\log n)$. This is often a significant space gain in
practice where $\sigma$ is usually much smaller than $n$ or even constant. We
also consider the case of computing succinct LCP arrays for circular strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05030</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05030</id><created>2016-01-19</created><authors><author><keyname>Balntas</keyname><forenames>Vassileios</forenames></author><author><keyname>Johns</keyname><forenames>Edward</forenames></author><author><keyname>Tang</keyname><forenames>Lilian</forenames></author><author><keyname>Mikolajczyk</keyname><forenames>Krystian</forenames></author></authors><title>PN-Net: Conjoined Triple Deep Network for Learning Local Image
  Descriptors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new approach for learning local descriptors for
matching image patches. It has recently been demonstrated that descriptors
based on convolutional neural networks (CNN) can significantly improve the
matching performance. Unfortunately their computational complexity is
prohibitive for any practical application. We address this problem and propose
a CNN based descriptor with improved matching performance, significantly
reduced training and execution time, as well as low dimensionality.
  We propose to train the network with triplets of patches that include a
positive and negative pairs. To that end we introduce a new loss function that
exploits the relations within the triplets. We compare our approach to recently
introduced MatchNet and DeepCompare and demonstrate the advantages of our
descriptor in terms of performance, memory footprint and speed i.e. when run in
GPU, the extraction time of our 128 dimensional feature is comparable to the
fastest available binary descriptors such as BRIEF and ORB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05047</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05047</id><created>2016-01-19</created><authors><author><keyname>Barthe</keyname><forenames>Gilles</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Gr&#xe9;goire</keyname><forenames>Benjamin</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Proving Differential Privacy via Probabilistic Couplings</title><categories>cs.LO cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop compositional methods for formally verifying
differential privacy for algorithms whose analysis goes beyond the composition
theorem. Our methods are based on the observation that differential privacy has
deep connections with a generalization of probabilistic couplings, an
established mathematical tool for reasoning about stochastic processes. Even
when the composition theorem is not helpful, we can often prove privacy by a
coupling argument.
  We demonstrate our methods on two algorithms: the Exponential mechanism and
the Above Threshold algorithm, the critical component of the famous Sparse
Vector algorithm. We verify these examples in a relational program logic
apRHL+, which can construct approximate couplings. This logic extends the
existing apRHL logic with more general rules for the Laplace mechanism and the
one-sided Laplace mechanism, and new structural rules enabling pointwise
reasoning about privacy; all the rules are inspired by the connection with
coupling. While our paper is presented from a formal verification perspective,
we believe that its main insight is of independent interest for the
differential privacy community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05050</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05050</id><created>2016-01-19</created><authors><author><keyname>Madjidian</keyname><forenames>Daria</forenames></author><author><keyname>Mirkin</keyname><forenames>Leonid</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author></authors><title>H2 Optimal Coordination of Homogeneous Agents Subject to Limited
  Information Exchange</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controllers with a diagonal-plus-low-rank structure constitute a scalable
class of controllers for multi-agent systems. Previous research has shown that
diagonal-plus-low-rank control laws appear as the optimal solution to a class
of multi-agent H2 coordination problems, which arise in the control of wind
farms. In this paper we show that this result extends to the case where the
information exchange between agents is subject to limitations. We also show
that the computational effort required to obtain the optimal controller is
independent of the number of agents and provide analytical expressions that
quantify the usefulness of information exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05052</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05052</id><created>2016-01-18</created><authors><author><keyname>Sclocco</keyname><forenames>Alessio</forenames></author><author><keyname>Bal</keyname><forenames>Henri E.</forenames></author><author><keyname>Hessels</keyname><forenames>Jason</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Joeri</forenames></author><author><keyname>van Nieuwpoort</keyname><forenames>Rob V.</forenames></author></authors><title>Auto-Tuning Dedispersion for Many-Core Accelerators</title><categories>cs.DC astro-ph.IM</categories><comments>10 pages, published in the proceedings of IPDPS 2014</comments><doi>10.1109/IPDPS.2014.101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the parallelization of the dedispersion algorithm on
many-core accelerators, including GPUs from AMD and NVIDIA, and the Intel Xeon
Phi. An important contribution is the computational analysis of the algorithm,
from which we conclude that dedispersion is inherently memory-bound in any
realistic scenario, in contrast to earlier reports. We also provide empirical
proof that, even in unrealistic scenarios, hardware limitations keep the
arithmetic intensity low, thus limiting performance. We exploit auto-tuning to
adapt the algorithm, not only to different accelerators, but also to different
observations, and even telescopes. Our experiments show how the algorithm is
tuned automatically for different scenarios and how it exploits and highlights
the underlying specificities of the hardware: in some observations, the tuner
automatically optimizes device occupancy, while in others it optimizes memory
bandwidth. We quantitatively analyze the problem space, and by comparing the
results of optimal auto-tuned versions against the best performing fixed codes,
we show the impact that auto-tuning has on performance, and conclude that it is
statistically relevant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05068</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05068</id><created>2016-01-19</created><authors><author><keyname>Karmoose</keyname><forenames>Mohammed</forenames></author><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Simplifying Wireless Social Caching</title><categories>cs.IT math.IT</categories><comments>Parts of this work were submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social groups give the opportunity for a new form of caching. In this paper,
we investigate how a social group of users can jointly optimize bandwidth
usage, by each caching parts of the data demand, and then opportunistically
share these parts among them upon meeting. We formulate this problem as a
Linear Program (LP) with exponential complexity. Based on the optimal solution,
we propose a simple heuristic inspired by the bipartite set-cover problem that
operates in polynomial time. Furthermore, we prove a worst case gap between the
heuristic and the LP solutions. Finally, we assess the performance of our
algorithm using real-world mobility traces from the MIT Reality Mining project
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05069</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05069</id><created>2016-01-19</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author></authors><title>Medium Access Control for Dynamic Spectrum Sharing in Cognitive Radio
  Networks</title><categories>cs.NI cs.IT math.IT math.OC</categories><comments>This is the Ph.D. Dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of wireless services and applications over the past decade
has led to the rapidly increasing demand in wireless spectrum. Hence, we have
been facing a critical spectrum shortage problem even though several
measurements have indicated that most licensed radio spectrum is very
underutilized. These facts have motivated the development of dynamic spectrum
access (DSA) and cognitive radio techniques to enhance the efficiency and
flexibility of spectrum utilization.
  In this dissertation, we investigate design, analysis, and optimization
issues for joint spectrum sensing and cognitive medium access control (CMAC)
protocol engineering for cognitive radio networks (CRNs). The joint spectrum
sensing and CMAC design is considered under the interweave spectrum sharing
paradigm and different communications settings. Our research has resulted in
four major research contributions, namely, the CMAC protocol design with
parallel spectrum sensing, the CMAC protocol with sequential sensing, the CMAC
protocol with cooperative sensing and the asynchronous Full-Duplex cognitive
MAC.
  We develop various analytical models for throughput performance analysis of
our proposed CMAC protocol designs. Based on these analytical models, we
develop different efficient algorithms to configure the CMAC protocol including
channel allocation, sensing time, transmit power, contention window to maximize
the total throughput of the secondary network. Furthermore, extensive numerical
results are presented to gain further insights and to evaluate the performance
of our CMAC protocol designs. Both the numerical and simulation results confirm
that our proposed CMAC protocols can achieve efficient spectrum utilization and
significant performance gains compared to existing and unoptimized designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05098</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05098</id><created>2016-01-19</created><updated>2016-02-03</updated><authors><author><keyname>Polese</keyname><forenames>Michele</forenames></author><author><keyname>Centenaro</keyname><forenames>Marco</forenames></author><author><keyname>Zanella</keyname><forenames>Andrea</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>On the Evaluation of LTE Random Access Channel Overload in a Smart City
  Scenario</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several studies assert that the random access procedure of the Long Term
Evolution (LTE) cellular standard may not be effective whenever a massive
number of synchronous connection attempts are performed by terminals, as may
happen in a typical Internet of Things or Smart City scenario. Nevertheless,
simulation studies in real deployment scenarios are missing because many
system-level simulators do not implement the LTE random access procedure in
detail. In this paper, we propose a patch for the LTE module of ns-3, one of
the most prominent open-source network simulators, to improve the accuracy of
the routine that simulates the LTE Random Access Channel (RACH). The patched
version of the random access procedure is compared with the default one and the
issues arising from massive synchronous access from mobile terminals in LTE are
assessed with a simulation campaign.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05100</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05100</id><created>2016-01-19</created><authors><author><keyname>Lee</keyname><forenames>Juyong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhong-Yuan</forenames></author><author><keyname>Lee</keyname><forenames>Jooyoung</forenames></author><author><keyname>Brooks</keyname><forenames>Bernard R.</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Link community detection through global optimization and the inverse
  resolution limit of partition density</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>20 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the possibility of global optimization-based overlapping
community detection, using link community framework. We first show that
partition density, the original quality function used in link community
detection method, is not suitable as a quality function for global optimization
because it prefers breaking communities into triangles except in highly limited
conditions. We analytically derive those conditions and confirm it with
computational results on direct optimization of various synthetic and
real-world networks. To overcome this limitation, we propose alternative
approaches combining the weighted line graph transformation and existing
quality functions for node-based communities. We suggest a new line graph
weighting scheme, a normalized Jaccard index. Computational results show that
community detection using the weighted line graphs generated with the
normalized Jaccard index leads to a more accurate community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05104</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05104</id><created>2016-01-18</created><authors><author><keyname>Womack</keyname><forenames>Ryan P.</forenames></author></authors><title>ARL Libraries and Research: Correlates of Grant Funding</title><categories>cs.DL stat.AP</categories><msc-class>62P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While providing the resources and tools that make advanced research possible
is a primary mission of academic libraries at large research universities, many
other elements also contribute to the success of the research enterprise, such
as institutional funding, staffing, labs, and equipment. This study focuses on
U.S. members of the ARL, the Association for Research Libraries. Research
success is measured by the total grant funding received by the University,
creating an ordered set of categories. Combining data from the NSF National
Center for Science and Engineering Statistics, ARL Statistics, and IPEDS, the
primary explanatory factors for research success are examined. Using linear
regression, logistic regression, and the cumulative logit model, the
best-fitting models generated by ARL data, NSF data, and the combined data set
for both nominal and per capita funding are compared. These models produce the
most relevant explanatory variables for research funding, which do not include
library-related variables in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05105</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05105</id><created>2016-01-19</created><authors><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>A Rate-Splitting Approach To Robust Multiuser MISO Transmission</title><categories>cs.IT math.IT</categories><comments>To appear in ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For multiuser MISO systems with bounded uncertainties in the Channel State
Information (CSI), we consider two classical robust design problems: maximizing
the minimum rate subject to a transmit power constraint, and power minimization
under a rate constraint. Contrary to conventional strategies, we propose a
Rate-Splitting (RS) strategy where each message is divided into two parts, a
common part and a private part. All common parts are packed into one super
common message encoded using a shared codebook and decoded by all users, while
private parts are independently encoded and retrieved by their corresponding
users. We prove that RS-based designs achieve higher max-min Degrees of Freedom
(DoF) compared to conventional designs (NoRS) for uncertainty regions that
scale with SNR. For the special case of non-scaling uncertainty regions, RS
contrasts with NoRS and achieves a non-saturating max-min rate. In the power
minimization problem, RS is shown to combat the feasibility problem arising
from multiuser interference in NoRS. A robust design of precoders for RS is
proposed, and performance gains over NoRS are demonstrated through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05106</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05106</id><created>2016-01-19</created><authors><author><keyname>Dunfield</keyname><forenames>Joshua</forenames></author><author><keyname>Krishnaswami</keyname><forenames>Neelakantan R.</forenames></author></authors><title>Sound and Complete Bidirectional Typechecking for Higher-Rank
  Polymorphism with Existentials and Indexed Types</title><categories>cs.PL cs.LO</categories><comments>23 pages, submitted to LICS 2016, plus lemmas and complete proofs
  (144 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional typechecking, in which terms either synthesize a type or are
checked against a known type, has become popular for its scalability, its error
reporting, and its ease of implementation. Following principles from proof
theory, bidirectional typing can be applied to many type constructs. The
principles underlying a bidirectional approach to indexed types (generalized
algebraic datatypes) are less clear. Building on proof-theoretic treatments of
equality, we give a declarative specification of typing based on focalization.
This approach permits declarative rules for coverage of pattern matching, as
well as support for first-class existential types using a focalized subtyping
judgment. We use refinement types to avoid explicitly passing equality proofs
in our term syntax, making our calculus close to languages such as Haskell and
OCaml. An explicit rule deduces when a type is principal, leading to reliable
substitution principles for a rich type system with significant type inference.
We also give a set of algorithmic typing rules, and prove that it is sound and
complete with respect to the declarative system. The proof requires a number of
technical innovations, including proving soundness and completeness in a
mutually-recursive fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05115</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05115</id><created>2016-01-19</created><authors><author><keyname>Moehle</keyname><forenames>Nicholas</forenames></author></authors><title>Value Function Approximation for Direct Control of Switched Power
  Converters</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of controlling switched-mode power converters using
model predictive control. Model predictive control requires solving
optimization problems in real time, limiting its application to systems with
small numbers of switches and a short horizon. We propose a technique for using
off-line computation to approximate the model predictive controller. This is
done by dividing the planning horizon into two segments, and using a quadratic
function to approximate the optimal cost over the second segment. The
approximate model predictive algorithm minimizes the true cost over the first
segment, and the approximate cost over the second segment, allowing the user to
adjust the computational requirements by changing the length of the first
segment. We conclude with two simulated examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05116</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05116</id><created>2016-01-19</created><authors><author><keyname>Mobahi</keyname><forenames>Hossein</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>A Theory of Local Matching: SIFT and Beyond</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Why has SIFT been so successful? Why its extension, DSP-SIFT, can further
improve SIFT? Is there a theory that can explain both? How can such theory
benefit real applications? Can it suggest new algorithms with reduced
computational complexity or new descriptors with better accuracy for matching?
We construct a general theory of local descriptors for visual matching. Our
theory relies on concepts in energy minimization and heat diffusion. We show
that SIFT and DSP-SIFT approximate the solution the theory suggests. In
particular, DSP-SIFT gives a better approximation to the theoretical solution;
justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive
new descriptors that have fewer parameters and are potentially better in
handling affine deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05118</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05118</id><created>2016-01-19</created><authors><author><keyname>Kamat</keyname><forenames>Niranjan</forenames></author><author><keyname>Nandi</keyname><forenames>Arnab</forenames></author></authors><title>Perfect and Maximum Randomness in Stratified Sampling over Joins</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supporting sampling in the presence of joins is an important problem in data
analysis. Pushing down the sampling operator through both sides of the join is
inherently challenging due to data skew and correlation issues between output
tuples. Joining simple random samples of base relations typically leads to
results that are non-random. Current solutions to this problem perform biased
sampling of one~(and not both) of the base relations to obtain a simple random
sample. These techniques are not always practical since they may result in the
sample size being greater than the size of the relations due to sample
inflation, rendering sampling counter-productive.
  This paper presents a unified strategy towards sampling over joins,
comprising two key contributions. First, in the case that perfect sampling is a
requirement, we introduce techniques to generate a \emph{perfect} random sample
from both sides of a join. We show that the challenges faced in sampling over
joins are ameliorated in the context of stratified random sampling as opposed
to simple random sampling. We reduce the dependency of feasibility of sampling
from relation level to strata level. Our technique minimizes the sample size
while maintaining perfect randomness.
  Second, in the case that random sampling is not a requirement but is still
preferred, we provide a novel sampling heuristic to \emph{maximize} randomness
of the join. It allows us to allocate a fixed sample size between multiple
relations consisting of multiple strata to maximize the join randomness. We
validate our techniques theoretically and empirically using synthetic datasets
and a standard benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05137</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05137</id><created>2016-01-19</created><updated>2016-01-23</updated><authors><author><keyname>Agarwal</keyname><forenames>Gaurav Kumar</forenames></author><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>On Secure Network Coding for Two Unicast Sessions</title><categories>cs.IT math.IT</categories><comments>Parts of this work were submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper characterizes the secret message capacity of three networks where
two unicast sessions share some of the communication resources. Each network
consists of erasure channels with state feedback. A passive eavesdropper is
assumed to wiretap any one of the links. The capacity achieving schemes as well
as the outer bounds are formulated as linear programs. The proposed strategies
are then numerically evaluated and shown to achieve higher rate performances
(up to a double single- or sum-rate) with respect to alternative strategies,
where the network resources are time-shared among the two sessions. These
results represent a step towards the secure capacity characterization for
general networks. Moreover, they show that, even in network configurations for
which network coding does not offer benefits in absence of security, it can
become beneficial under security constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05140</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05140</id><created>2016-01-19</created><authors><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author><author><keyname>Azaria</keyname><forenames>Amos</forenames></author><author><keyname>Durst</keyname><forenames>Skylar</forenames></author><author><keyname>Kagan</keyname><forenames>Vadim</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Zhu</keyname><forenames>Linhong</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Waltzman</keyname><forenames>Rand</forenames></author><author><keyname>Stevens</keyname><forenames>Andrew</forenames></author><author><keyname>Dekhtyar</keyname><forenames>Alexander</forenames></author><author><keyname>Gao</keyname><forenames>Shuyang</forenames></author><author><keyname>Hogg</keyname><forenames>Tad</forenames></author><author><keyname>Kooti</keyname><forenames>Farshad</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Shiralkar</keyname><forenames>Prashant</forenames></author><author><keyname>Vydiswaran</keyname><forenames>Vinod</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author><author><keyname>Huang</keyname><forenames>Tim</forenames></author></authors><title>The DARPA Twitter Bot Challenge</title><categories>cs.SI cs.AI cs.CY physics.data-an physics.soc-ph</categories><comments>IEEE Computer Magazine, in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of organizations ranging from terrorist groups such as ISIS to
politicians and nation states reportedly conduct explicit campaigns to
influence opinion on social media, posing a risk to democratic processes. There
is thus a growing need to identify and eliminate &quot;influence bots&quot; - realistic,
automated identities that illicitly shape discussion on sites like Twitter and
Facebook - before they get too influential. Spurred by such events, DARPA held
a 4-week competition in February/March 2015 in which multiple teams supported
by the DARPA Social Media in Strategic Communications program competed to
identify a set of previously identified &quot;influence bots&quot; serving as ground
truth on a specific topic within Twitter. Past work regarding influence bots
often has difficulty supporting claims about accuracy, since there is limited
ground truth (though some exceptions do exist [3,7]). However, with the
exception of [3], no past work has looked specifically at identifying influence
bots on a specific topic. This paper describes the DARPA Challenge and
describes the methods used by the three top-ranked teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05141</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05141</id><created>2016-01-19</created><authors><author><keyname>Tang</keyname><forenames>Mengfan</forenames></author><author><keyname>Agrawal</keyname><forenames>Pranav</forenames></author><author><keyname>Jain</keyname><forenames>Ramesh</forenames></author></authors><title>Habits vs Environment: What really causes asthma?</title><categories>cs.CY cs.LG</categories><comments>Presented at ACM WebSci 2015, Oxford UK</comments><acm-class>H.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite considerable number of studies on risk factors for asthma onset, very
little is known about their relative importance. To have a full picture of
these factors, both categories, personal and environmental data, have to be
taken into account simultaneously, which is missing in previous studies. We
propose a framework to rank the risk factors from heterogeneous data sources of
the two categories. Established on top of EventShop and Personal EventShop,
this framework extracts about 400 features, and analyzes them by employing a
gradient boosting tree. The features come from sources including personal
profile and life-event data, and environmental data on air pollution, weather
and PM2.5 emission sources. The top ranked risk factors derived from our
framework agree well with the general medical consensus. Thus, our framework is
a reliable approach, and the discovered rankings of relative importance of risk
factors can provide insights for the prevention of asthma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05142</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05142</id><created>2016-01-19</created><authors><author><keyname>Brunelle</keyname><forenames>Justin F.</forenames></author><author><keyname>Weigle</keyname><forenames>Michele C.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Adapting the Hypercube Model to Archive Deferred Representations and
  Their Descendants</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web is today's primary publication medium, making web archiving an
important activity for historical and analytical purposes. Web pages are
increasingly interactive, resulting in pages that are increasingly difficult to
archive. Client-side technologies (e.g., JavaScript) enable interactions that
can potentially change the client-side state of a representation. We refer to
representations that load embedded resources via JavaScript as deferred
representations. It is difficult to archive all of the resources in deferred
representations and the result is archives with web pages that are either
incomplete or that erroneously load embedded resources from the live web.
  We propose a method of discovering and crawling deferred representations and
their descendants (representation states that are only reachable through
client-side events). We adapt the Dincturk et al. Hypercube model to construct
a model for archiving descendants, and we measure the number of descendants and
requisite embedded resources discovered in a proof-of-concept crawl. Our
approach identified an average of 38.5 descendants per seed URI crawled, 70.9%
of which are reached through an onclick event. This approach also added 15.6
times more embedded resources than Heritrix to the crawl frontier, but at a
rate that was 38.9 times slower than simply using Heritrix. We show that our
dataset has two levels of descendants. We conclude with proposed crawl policies
and an analysis of the storage requirements for archiving descendants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05146</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05146</id><created>2016-01-19</created><authors><author><keyname>Ma</keyname><forenames>Chuang</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author></authors><title>Playing the role of weak clique property in link prediction: A friend
  recommendation model</title><categories>physics.soc-ph cs.SI</categories><comments>7 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important fact in studying the link prediction is that the structural
properties of networks have significant impacts on the performance of
algorithms. Therefore, how to improve the performance of link prediction with
the aid of structural properties of networks is an essential problem. By
analyzing many real networks, we find a common structure property: nodes are
preferentially linked to the nodes with the weak clique structure (abbreviated
as PWCS to simplify descriptions). Based on this PWCS phenomenon, we propose a
local friend recommendation (FR) index to facilitate link prediction. Our
experiments show that the performance of FR index is generally better than some
famous local similarity indices, such as Common Neighbor (CN) index,
Adamic-Adar (AA) index and Resource Allocation (RA) index. We then explain why
PWCS can give rise to the better performance of FR index in link prediction.
Finally, a mixed friend recommendation index (labelled MFR) is proposed by
utilizing the PWCS phenomenon, which further improves the accuracy of link
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05150</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05150</id><created>2016-01-19</created><authors><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Zhang</keyname><forenames>Cong</forenames></author><author><keyname>Yang</keyname><forenames>Xiaokang</forenames></author></authors><title>Factors in Finetuning Deep Model for object detection</title><categories>cs.CV</categories><comments>CVPR2016 submission. Our ImageNet large scale recognition challenge
  (ILSVRC15) object detection results (rank 3rd for provided data and 2nd for
  external data) are based on this method</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finetuning from a pretrained deep model is found to yield state-of-the-art
performance for many vision tasks. This paper investigates many factors that
influence the performance in finetuning for object detection.
  There is a long-tailed distribution of sample numbers for classes in object
detection. Our analysis and empirical results show that classes with more
samples have higher impact on the feature learning. And it is better to make
the sample number more uniform across classes. Generic object detection can be
considered as multiple equally important tasks. Detection of each class is a
task. These classes/tasks have their individuality in discriminative visual
appearance representation. Taking this individuality into account, we cluster
objects into visually similar class groups and learn deep representations for
these groups separately. A hierarchical feature learning scheme is proposed. In
this scheme, the knowledge from the group with large number of classes is
transferred for learning features in its sub-groups. Finetuned on the GoogLeNet
model, experimental results show 4.7% absolute mAP improvement of our approach
on the ImageNet object detection dataset without increasing much computational
cost at the testing stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05161</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05161</id><created>2016-01-19</created><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author></authors><title>On Network Simplification for Gaussian Half-Duplex Diamond Networks</title><categories>cs.IT math.IT</categories><comments>Parts of this work were submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of relay selection in Gaussian
Half-Duplex (HD) diamond networks. The goal is to answer the following
question: what is the minimum (worst-case) fraction of the total HD capacity
that one can always achieve by smartly selecting a subset of only $k$ relays,
out of the $N$ possible ones? We make progress on this problem for $k=1$ and
$k=2$, and show that for $N=k+1, \ k \in \{1,2\}$ at least $\frac{k}{k+1}$ of
the total HD capacity is always achieved up to a constant gap. Interestingly,
and differently from the Full-Duplex (FD) case, the ratio in HD depends on $N$,
and decreases as $N$ increases. For all values of $N$ and $k$ for which we
derive worst case fractions, we also show these to be tight. This is
accomplished by presenting $N$-relay Gaussian HD diamond networks for which the
best $k$-relay subnetwork has a HD capacity equal to the worst-case fraction of
the total HD capacity. Finally, we provide comparisons between the performance
of this simplification problem for HD and FD newtorks, which highlight their
different natures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05164</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05164</id><created>2016-01-19</created><authors><author><keyname>Behl</keyname><forenames>Madhur</forenames></author><author><keyname>Jain</keyname><forenames>Achin</forenames></author><author><keyname>Mangharam</keyname><forenames>Rahul</forenames></author></authors><title>Data-Driven Modeling, Control and Tools for Cyber-Physical Energy
  Systems</title><categories>cs.SY</categories><comments>To appear in the proceedings of ACM/IEEE 7th International Conference
  on Cyber-Physical Systems (ICCPS) 2016</comments><acm-class>C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand response (DR) is becoming increasingly important as the volatility on
the grid continues to increase. Current DR approaches are completely manual and
rule-based or involve deriving first principles based models which are
extremely cost and time prohibitive to build. We consider the problem of
data-driven end-user DR for large buildings which involves predicting the
demand response baseline, evaluating fixed rule based DR strategies and
synthesizing DR control actions. We provide a model based control with
regression trees algorithm (mbCRT), which allows us to perform closed-loop
control for DR strategy synthesis for large commercial buildings. Our
data-driven control synthesis algorithm outperforms rule-based DR by $17\%$ for
a large DoE commercial reference building and leads to a curtailment of $380$kW
and over $\$45,000$ in savings. Our methods have been integrated into an open
source tool called DR-Advisor, which acts as a recommender system for the
building's facilities manager and provides suitable control actions to meet the
desired load curtailment while maintaining operations and maximizing the
economic reward. DR-Advisor achieves $92.8\%$ to $98.9\%$ prediction accuracy
for 8 buildings on Penn's campus. We compare DR-Advisor with other data driven
methods and rank $2^{nd}$ on ASHRAE's benchmarking data-set for energy
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05176</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05176</id><created>2016-01-20</created><updated>2016-02-02</updated><authors><author><keyname>Gimbert</keyname><forenames>Hugo</forenames><affiliation>LaBRI</affiliation></author></authors><title>A Class of Zielonka Automata with a Decidable Controller Synthesis
  Problem</title><categories>cs.FL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decidability of the distributed version of the Ramadge and Wonham control
problem (Ramadge and Wonham 1989), where both the plant and the controllers are
modelled as Zielonka au-tomata (Zielonka 1987; Diekert and Rozenberg 1995) is a
challenging open problem (Muscholl et al. 2008). There exists three classes of
plants for which the existence of a correct controller has been shown decidable
in the distributed setting: when the dependency graph of actions is
series-parallel, when the processes are connectedly communicating and when the
dependency graph of processes is a tree. We generalize these three results by
showing that a larger class of plants, called broadcast plants, has a decidable
controller synthesis problem. We give new examples of plants for which
controller synthesis is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05187</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05187</id><created>2016-01-20</created><authors><author><keyname>Eggert</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author></authors><title>Dynamic Intransitive Noninterference Revisited</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies dynamic information flow security policies in an
automaton-based model. Two semantic interpretations of such policies are
developed, both of which generalize the notion of TA-security [van der Meyden
ESORICS 2007] for static intransitive noninterference policies. One of the
interpretations focuses on information flows permitted by policy edges, the
other focuses on prohibitions implied by absence of policy edges. In general,
the two interpretations differ, but necessary and sufficient conditions are
identified for the two interpretations to be equivalent. Sound and complete
proof techniques are developed for both interpretations. Two applications of
the theory are presented. The first is a general result showing that access
control mechanisms are able to enforce a dynamic information flow policy. The
second is a simple capability system motivated by the Flume operating system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05190</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05190</id><created>2016-01-20</created><authors><author><keyname>Bonnet</keyname><forenames>Edouard</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>A note on Edge Isoperimetric Numbers and Regular Graphs</title><categories>cs.DM math.CO</categories><comments>Accepted in International Journal of Foundations of Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note resolves an open problem asked by Bezrukov in the open problem
session of IWOCA 2014. It shows an equivalence between regular graphs and
graphs for which a sequence of invariants presents some symmetric property. We
extend this result to a few other sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05193</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05193</id><created>2016-01-20</created><authors><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Huang</keyname><forenames>Kechao</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>Systematic Block Markov Superposition Transmission of Repetition Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose systematic block Markov superposition transmission
of repetition~(BMST-R) codes, which can support a wide range of code rates but
maintain essentially the same encoding/decoding hardware structure. The
systematic BMST-R codes resemble the classical rate-compatible punctured
convolutional~(RCPC) codes, except that they are typically non-decodable by the
Viterbi algorithm due to the huge constraint length induced by the
block-oriented encoding process. The information sequence is partitioned
equally into blocks and transmitted directly, while their replicas are
interleaved and transmitted in a block Markov superposition manner. By taking
into account that the codes are systematic, we derive both upper and lower
bounds on the bit-error-rate~(BER) under maximum {\em a posteriori}~(MAP)
decoding. The derived lower bound reveals connections among BER, encoding
memory and code rate, which provides a way to design good systematic BMST-R
codes and also allows us to make trade-offs among efficiency, performance and
complexity. Numerical results show that:~1)~the proposed bounds are tight in
the high signal-to-noise ratio~(SNR) region;~2)~systematic BMST-R codes perform
well in a wide range of code rates, and~3)~systematic BMST-R codes outperform
spatially coupled low-density parity-check~(SC-LDPC) codes under an equal
decoding latency constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05194</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05194</id><created>2016-01-20</created><authors><author><keyname>Chen</keyname><forenames>Kuan-Yu</forenames></author><author><keyname>Liu</keyname><forenames>Shih-Hung</forenames></author><author><keyname>Chen</keyname><forenames>Berlin</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author></authors><title>Improved Spoken Document Summarization with Coverage Modeling Techniques</title><categories>cs.CL cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1506.04365</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extractive summarization aims at selecting a set of indicative sentences from
a source document as a summary that can express the major theme of the
document. A general consensus on extractive summarization is that both
relevance and coverage are critical issues to address. The existing methods
designed to model coverage can be characterized by either reducing redundancy
or increasing diversity in the summary. Maximal margin relevance (MMR) is a
widely-cited method since it takes both relevance and redundancy into account
when generating a summary for a given document. In addition to MMR, there is
only a dearth of research concentrating on reducing redundancy or increasing
diversity for the spoken document summarization task, as far as we are aware.
Motivated by these observations, two major contributions are presented in this
paper. First, in contrast to MMR, which considers coverage by reducing
redundancy, we propose two novel coverage-based methods, which directly
increase diversity. With the proposed methods, a set of representative
sentences, which not only are relevant to the given document but also cover
most of the important sub-themes of the document, can be selected
automatically. Second, we make a step forward to plug in several
document/sentence representation methods into the proposed framework to further
enhance the summarization performance. A series of empirical evaluations
demonstrate the effectiveness of our proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05205</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05205</id><created>2016-01-20</created><authors><author><keyname>M&#xfc;elich</keyname><forenames>Sven</forenames></author><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>M&#xf6;dinger</keyname><forenames>David</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author></authors><title>An Alternative Decoding Method for Gabidulin Codes in Characteristic
  Zero</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to IEEE International Symposium on Information
  Theory 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gabidulin codes, originally defined over finite fields, are an important
class of rank metric codes with various applications. Recently, their
definition was generalized to certain fields of characteristic zero and a
Welch--Berlekamp like algorithm with complexity $O(n^3)$ was given. We propose
a new application of Gabidulin codes over infinite fields: low-rank matrix
recovery. Also, an alternative decoding approach is presented based on a Gao
type key equation, reducing the complexity to at least $O(n^2)$. This method
immediately connects the decoding problem to well-studied problems, which have
been investigated in terms of coefficient growth and numerical stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05218</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05218</id><created>2016-01-20</created><updated>2016-01-25</updated><authors><author><keyname>Yehezkeally</keyname><forenames>Yonatan</forenames></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author></authors><title>Limited-Magnitude Error-Correcting Gray Codes for Rank Modulation</title><categories>cs.IT math.IT</categories><comments>Corrected typos in the introduction and equations in Table I, first
  paragraph of Section III-A and proof of Theorem II. Results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct Gray codes over permutations for the rank-modulation scheme,
which are also capable of correcting errors under the infinity-metric. These
errors model limited-magnitude or spike errors, for which only
single-error-detecting Gray codes are currently known. Surprisingly, the
error-correcting codes we construct achieve better asymptotic rates than that
of presently-known constructions not having the Gray property. We also cast the
problem of improving upon these results into the context of finding a certain
type of auxiliary codes in the symmetric group of even orders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05220</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05220</id><created>2016-01-20</created><authors><author><keyname>Oued</keyname><forenames>Mohammed El</forenames></author><author><keyname>Napp</keyname><forenames>Diego</forenames></author><author><keyname>Pinto</keyname><forenames>Raquel</forenames></author><author><keyname>Toste</keyname><forenames>Marisa</forenames></author></authors><title>The dual of convolutional codes over $\mathbb{Z}_{p^r}$</title><categories>math.RA cs.IT math.IT</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important class of codes widely used in applications is the class of
convolutional codes. Most of the literature of convolutional codes is devoted
to con- volutional codes over finite fields. The extension of the concept of
convolutional codes from finite fields to finite rings have attracted much
attention in recent years due to fact that they are the most appropriate codes
for phase modulation. However convolutional codes over finite rings are more
involved and not fully understood. Many results and features that are
well-known for convolutional codes over finite fields have not been fully
investigated in the context of finite rings. In this paper we focus in one of
these unexplored areas, namely, we investigate the dual codes of convolutional
codes over finite rings. In particular we study the p-dimension of the dual
code of a convolutional code over a finite ring. This contribution can be
considered a generalization and an extension, to the rings case, of the work
done by Forney and McEliece on the dimension of the dual code of a
convolutional code over a finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05228</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05228</id><created>2016-01-20</created><authors><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Klein</keyname><forenames>Felix</forenames></author></authors><title>A High-Level LTL Synthesis Format: TLSF v1.0</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Temporal Logic Synthesis Format (TLSF), a high-level format to
describe synthesis problems via Linear Temporal Logic (LTL). The format builds
upon standard LTL, but additionally allows to use high level constructs, such
as sets and functions, to provide a compact and human readable representation.
Furthermore, the format allows to identify parameters of a specification such
that a single description can be used to define a family of problems. We also
present a tool to automatically translate the format into plain LTL, which then
can be used for synthesis by a solver. The tool also allows to adjust
parameters of the specification and to apply standard transformations on the
resulting formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05247</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05247</id><created>2016-01-20</created><authors><author><keyname>Darabi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Roustaei</keyname><forenames>Amin</forenames></author></authors><title>Asymptotic Close to Optimal Resource Allocation in Centralized
  Multi-band Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns sub-channel allocation in multi-user wireless networks
with a view to increasing the network throughput. It is assumed there are some
sub-channels to be equally divided among active links, such that the total sum
rate increases, where it is assumed each link is subject to a maximum transmit
power constraint. This problem is found to be a non-convex optimization problem
and is hard to deal with for large number of sub channels and/or users.
However, relying on some approximation methods, it is demonstrated that the
proposed sub-optimal problem has roots in combinatorial optimization, termed as
Assignment problem which can be tackled through the so called Hungarian method.
Simulation results demonstrate that the proposed method outperforms existing
works addressed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05254</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05254</id><created>2016-01-20</created><authors><author><keyname>Perez-Marco</keyname><forenames>Ricardo</forenames></author></authors><title>Bitcoin and Decentralized Trust Protocols</title><categories>cs.CY math.HO</categories><comments>This is a general survey article submitted to the Newsletter of the
  EMS. 8 pages, 5 figures</comments><msc-class>68P25, 91G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is the first decentralized peer-to-peer (P2P) electronic currency. It
was created in November 2008 by Satoshi Nakamoto. Nakamoto released the first
implementation of the protocol in an open source client software and the
genesis of bitcoins began on January 9th 2009. The Bitcoin protocol is based on
clever ideas which solve a form of the Byzantine Generals Problem and sets the
foundation for Decentralized Trust Protocols. Still in its infancy, the
currency and the protocol have the potential to disrupt the international
financial system and other sectors where business is based on trusted third
parties. The security of the bitcoin protocol relies on strong cryptography and
one way hashing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05257</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05257</id><created>2016-01-20</created><authors><author><keyname>Kok</keyname><forenames>Manon</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author></authors><title>Magnetometer calibration using inertial sensors</title><categories>cs.SY cs.RO stat.AP</categories><comments>31 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a practical algorithm for calibrating a magnetometer
for the presence of magnetic disturbances and for magnetometer sensor errors.
To allow for combining the magnetometer measurements with inertial measurements
for orientation estimation, the algorithm also corrects for misalignment
between the magnetometer and the inertial sensor axes. The calibration
algorithm is formulated as the solution to a maximum likelihood problem and the
computations are performed offline. It is shown to give good results using data
from two different commercially available sensor units. Using the calibrated
magnetometer measurements in combination with the inertial sensors to determine
the sensor's orientation, is shown to lead to significantly improved heading
estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05266</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05266</id><created>2016-01-20</created><authors><author><keyname>Sermpezis</keyname><forenames>Pavlos</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Thrasyvoulos</forenames></author></authors><title>Effects of Content Popularity on the Performance of Content-Centric
  Opportunistic Networking: An Analytical Approach and Applications</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile users are envisioned to exploit direct communication opportunities
between their portable devices, in order to enrich the set of services they can
access through cellular or WiFi networks. Sharing contents of common interest
or providing access to resources or services between peers can enhance a mobile
node's capabilities, offload the cellular network, and disseminate information
to nodes without Internet access. Interest patterns, i.e. how many nodes are
interested in each content or service (popularity), as well as how many users
can provide a content or service (availability) impact the performance and
feasibility of envisioned applications. In this paper, we establish an
analytical framework to study the effects of these factors on the delay and
success probability of a content/service access request through opportunistic
communication. We also apply our framework to the mobile data offloading
problem and provide insights for the optimization of its performance. We
validate our model and results through realistic simulations, using datasets of
real opportunistic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05270</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05270</id><created>2016-01-20</created><authors><author><keyname>Faisal</keyname><forenames>Sidra</forenames></author><author><keyname>Endris</keyname><forenames>Kemele M.</forenames></author><author><keyname>Shekarpour</keyname><forenames>Saeedeh</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Co-evolution of RDF Datasets</title><categories>cs.DB</categories><comments>17 pages, 2 figures, submitted to ICWE, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many use cases it is not feasible to access RDF data in a truly federated
fashion. For consistency, latency and performance reasons data needs to be
replicated in order to be used locally. However, both a replica and its origin
dataset undergo changes over time. The concept of co-evolution refers to mutual
propagation of the changes between a replica and its origin dataset. The
co-evolution process addresses synchronization and conflict resolution issues.
In this article, we initially provide formal definitions of all the concepts
required for realizing co-evolution of RDF datasets. Then, we propose a
methodology to address the co-evolution of RDF datasets. We rely on a
property-oriented approach for employing the most suitable strategy or
functionality. This methodology was implemented and tested for a number of
different scenarios. The result of our experimental study shows the performance
and robustness aspect of this methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05271</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05271</id><created>2016-01-20</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Xu</keyname><forenames>Shenmeng</forenames></author><author><keyname>Fang</keyname><forenames>Zhichao</forenames></author></authors><title>Tracing Digital Footprints to Academic Articles: An Investigation of
  PeerJ Publication Referral Data</title><categories>cs.DL cs.IR cs.SI physics.bio-ph physics.soc-ph</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we propose a novel way to explore the patterns of people's
visits to academic articles. About 3.4 million links to referral source of
visitors of 1432 papers published in the journal of PeerJ are collected and
analyzed. We find that at least 57% visits are from external referral sources,
among which General Search Engine, Social Network, and News &amp; Blog are the top
three categories of referrals. Academic Resource, including academic search
engines and academic publishers' sites, is the fourth largest category of
referral sources. In addition, our results show that Google contributes
significantly the most in directing people to scholarly articles. This
encompasses the usage of Google the search engine, Google Scholar the academic
search engine, and diverse specific country domains of them. Focusing on
similar disciplines to PeerJ's publication scope, NCBI is the academic search
engine on which people are the most frequently directed to PeerJ. Correlation
analysis and regression analysis indicates that papers with more mentions are
expected to have more visitors, and Facebook, Twitter and Reddit are the most
commonly used social networking tools that refer people to PeerJ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05273</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05273</id><created>2016-01-20</created><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Dwyer</keyname><forenames>Chris</forenames></author><author><keyname>Lebeck</keyname><forenames>Alvin R.</forenames></author></authors><title>Combined Compute and Storage: Configurable Memristor Arrays to
  Accelerate Search</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging technologies present opportunities for system designers to meet the
challenges presented by competing trends of big data analytics and limitations
on CMOS scaling. Specifically, memristors are an emerging high-density
technology where the individual memristors can be used as storage or to perform
computation. The voltage applied across a memristor determines its behavior
(storage vs. compute), which enables a configurable memristor substrate that
can embed computation with storage.
  This paper explores accelerating point and range search queries as instances
of the more general configurable combined compute and storage capabilities of
memristor arrays. We first present MemCAM, a configurable memristor-based
content addressable memory for the cases when fast, infrequent searches over
large datasets are required. For frequent searches, memristor lifetime becomes
a concern. To increase memristor array lifetime we introduce hybrid data
structures that combine trees with MemCAM using conventional CMOS
processor/cache hierarchies for the upper levels of the tree and configurable
memristor technologies for lower levels.
  We use SPICE to analyze energy consumption and access time of memristors and
use analytic models to evaluate the performance of configurable hybrid data
structures. The results show that with acceptable energy consumption our
configurable hybrid data structures improve performance of search intensive
applications and achieve lifetime in years or decades under continuous queries.
Furthermore, the configurability of memristor arrays and the proposed data
structures provide opportunities to tune the trade- off between performance and
lifetime and the data structures can be easily adapted to future memristors or
other technologies with improved endurance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05274</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05274</id><created>2016-01-20</created><updated>2016-02-09</updated><authors><author><keyname>Esp&#xed;n-Noboa</keyname><forenames>Lisette</forenames></author><author><keyname>Lemmerich</keyname><forenames>Florian</forenames></author><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Discovering and Characterizing Mobility Patterns in Urban Spaces: A
  Study of Manhattan Taxi Data</title><categories>cs.SI cs.IR</categories><comments>Accepted at the Location and the Web (LocWeb) workshop at WWW2016</comments><doi>10.1145/2872518.2890468</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, human movement in urban spaces can be traced digitally in many
cases. It can be observed that movement patterns are not constant, but vary
across time and space. In this work,we characterize such spatio-temporal
patterns with an innovative combination of two separate approaches that have
been utilized for studying human mobility in the past. First, by using
non-negative tensor factorization (NTF), we are able to cluster human behavior
based on spatio-temporal dimensions. Second, for understanding these clusters,
we propose to use HypTrails, a Bayesian approach for expressing and comparing
hypotheses about human trails. To formalize hypotheses we utilize data that is
publicly available on the Web, namely Foursquare data and census data provided
by an open data platform. By applying this combination of approaches to taxi
data in Manhattan, we can discover and explain different patterns in human
mobility that cannot be identified in a collective analysis. As one example, we
can find a group of taxi rides that end at locations with a high number of
party venues (according to Foursquare) on weekend nights. Overall, our work
demonstrates that human mobility is not one-dimensional but rather contains
different facets both in time and space which we explain by utilizing online
data. The findings of this paper argue for a more fine-grained analysis of
human mobility in order to make more informed decisions for e.g., enhancing
urban structures, tailored traffic control and location-based recommender
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05281</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05281</id><created>2016-01-20</created><authors><author><keyname>Elshaer</keyname><forenames>Hisham</forenames></author><author><keyname>Kulkarni</keyname><forenames>Mandar N.</forenames></author><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author></authors><title>Downlink and Uplink Cell Association with Traditional Macrocells and
  Millimeter Wave Small Cells</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures. Submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) links will offer high capacity but are poor at
penetrating into or diffracting around solid objects. Thus, we consider a
hybrid cellular network with traditional sub 6 GHz macrocells coexisting with
denser mmWave small cells, where a mobile user can connect to either
opportunistically. We develop a general analytical model to characterize and
derive the uplink and downlink cell association in view of the SINR and rate
coverage probabilities in such a mixed deployment. We offer extensive
validation of these analytical results (which rely on several simplifying
assumptions) with simulation results. Using the analytical results, different
decoupled uplink and downlink cell association strategies are investigated and
their superiority is shown compared to the traditional coupled approach.
Finally, small cell biasing in mmWave is studied, and we show that
unprecedented biasing values are desirable due to the wide bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05283</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05283</id><created>2016-01-20</created><updated>2016-03-07</updated><authors><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author><author><keyname>Tao</keyname><forenames>Jia</forenames></author></authors><title>Marketing Impact on Diffusion in Social Networks</title><categories>cs.SI cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a way to add marketing into the standard threshold model
of social networks. Within this framework, the paper studies logical properties
of the influence relation between sets of agents in social networks. Two
different forms of this relation are considered: one for promotional marketing
and the other for preventive marketing. In each case a sound and complete
logical system describing properties of the influence relation is proposed.
Both systems could be viewed as extensions of Armstrong's axioms of functional
dependency from the database theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05313</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05313</id><created>2016-01-20</created><authors><author><keyname>Rodr&#xed;guez-S&#xe1;nchez</keyname><forenames>Rafael</forenames></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames></author></authors><title>Architecture-Aware Optimization of an HEVC decoder on Asymmetric
  Multicore Processors</title><categories>cs.DC cs.MM cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-power asymmetric multicore processors (AMPs) attract considerable
attention due to their appealing performance-power ratio for energy-constrained
environments. However, these processors pose a significant programming
challenge due to the integration of cores with different performance
capabilities, asking for an asymmetry-aware scheduling solution that carefully
distributes the workload.
  The recent HEVC standard, which offers several high-level parallelization
strategies, is an important application that can benefit from an implementation
tailored for the low-power AMPs present in many current mobile or hand-held
devices. In this scenario, we present an architecture-aware implementation of
an HEVC decoder that embeds a criticality-aware scheduling strategy tuned for a
Samsung Exynos 5422 system-on-chip furnished with an ARM big.LITTLE AMP. The
performance and energy efficiency of our solution is further enhanced by
exploiting the NEON vector engine available in the ARM big.LITTLE architecture.
Experimental results expose a 1080p real-time HEVC decoding at 24 frames/sec,
and a reduction of energy consumption over 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05329</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05329</id><created>2016-01-20</created><updated>2016-01-23</updated><authors><author><keyname>Moura</keyname><forenames>Jose</forenames></author><author><keyname>Hutchison</keyname><forenames>David</forenames></author></authors><title>Review and Analysis of Networking Challenges in Cloud Computing</title><categories>cs.NI</categories><journal-ref>Journal of Network and Computer Applications, vol. 60, pp.
  113-129, 2016</journal-ref><doi>10.1016/j.jnca.2015.11.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing offers virtualized computing, storage, and networking
resources, over the Internet, to organizations and individual users in a
completely dynamic way. These cloud resources are cheaper, easier to manage,
and more elastic than sets of local, physical, ones. This encourages customers
to outsource their applications and services to the cloud. The migration of
both data and applications outside the administrative domain of customers into
a shared environment imposes transversal, functional problems across distinct
platforms and technologies. This article provides a contemporary discussion of
the most relevant functional problems associated with the current evolution of
Cloud Computing, mainly from the network perspective. The paper also gives a
concise description of Cloud Computing concepts and technologies. It starts
with a brief history about cloud computing, tracing its roots. Then,
architectural models of cloud services are described, and the most relevant
products for Cloud Computing are briefly discussed along with a comprehensive
literature review. The paper highlights and analyzes the most pertinent and
practical network issues of relevance to the provision of high-assurance cloud
services through the Internet, including security. Finally, trends and future
research directions are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05330</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05330</id><created>2016-01-12</created><updated>2016-01-23</updated><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames></author></authors><title>Towards a Theory of Affect and Software Developers' Performance</title><categories>cs.SE cs.CY</categories><comments>PhD Dissertation as defended on January 12, 2016 at the Faculty of
  Computer Science of the Free University of Bozen-Bolzano (also after peer
  review). Hybrid between a monograph and a publication-based thesis. Contains
  text of arXiv:1306.1772, arXiv:1405.4422, arXiv:1408.1293, arXiv:1505.00922,
  arXiv:1505.04563, arXiv:1505.07240, and arXiv:1507.03767</comments><acm-class>D.2.9; H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For more than thirty years, it has been claimed that a way to improve
software developers' productivity and software quality is to focus on people.
The underlying assumption seems to be that &quot;happy and satisfied software
developers perform better&quot;. More specifically, affects-emotions and moods-have
an impact on cognitive activities and the working performance of individuals.
Development tasks are undertaken heavily through cognitive processes, yet
software engineering research (SE) lacks theory on affects and their impact on
software development activities. This PhD dissertation supports the advocates
of studying the human and social aspects of SE and the psychology of
programming. This dissertation aims to theorize on the link between affects and
software development performance. A mixed method approach was employed, which
comprises studies of the literature in psychology and SE, quantitative
experiments, and a qualitative study, for constructing a multifaceted theory of
the link between affects and programming performance. The theory explicates the
linkage between affects and analytical problem-solving performance of
developers, their software development task productivity, and the process
behind the linkage. The results are novel in the domains of SE and psychology,
and they fill an important lack that had been raised by both previous research
and by practitioners. The implications of this PhD lie in setting out the basic
building blocks for researching and understanding the affect of software
developers, and how it is related to software development performance. Overall,
the evidence hints that happy software developers perform better in analytic
problem solving, are more productive while developing software, are prone to
share their feelings in order to let researchers and managers understand them,
and are susceptible to interventions for enhancing their affects on the job.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05335</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05335</id><created>2016-01-20</created><authors><author><keyname>Kong</keyname><forenames>Hyeok</forenames></author><author><keyname>Jong</keyname><forenames>Cholyong</forenames></author><author><keyname>Ryang</keyname><forenames>Unhyok</forenames></author></authors><title>Implementation of Association Rule Mining for Network Intrusion
  Detection</title><categories>cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern intrusion detection systems are based on data mining and
database-centric architecture, where a number of data mining techniques have
been found. Among the most popular techniques, association rule mining is one
of the important topics in data mining research. This approach determines
interesting relationships between large sets of data items. This technique was
initially applied to the so-called market basket analysis, which aims at
finding regularities in shopping behaviour of customers of supermarkets. In
contrast to dataset for market basket analysis, which takes usually hundreds of
attributes, network audit databases face tens of attributes. So the typical
Apriori algorithm of association rule mining, which needs so many database
scans, can be improved, dealing with such characteristics of transaction
database. In this paper we propose an impoved Apriori algorithm, very useful in
practice, using scan of network audit database only once by transaction cutting
and hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05336</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05336</id><created>2016-01-20</created><updated>2016-03-07</updated><authors><author><keyname>Yamada</keyname><forenames>Norihiro</forenames></author></authors><title>Game-theoretic Interpretation of Type Theory Part I: Intuitionistic Type
  Theory with Universes</title><categories>cs.LO cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a game semantics for intuitionistic type theory. Concretely, we
propose categories with families of games and strategies for both extensional
and intensional type theories, which support dependent product, dependent sum,
and Id-types as well as universes. The intensional interpretation of the
Id-types in particular has interesting phenomena: It admits the principle of
uniqueness of identity proofs as well as Streicher's first and second Criteria
of Intensionality, but refutes the third criterion, the principles of equality
reflection and function extensionality, and the univalence axiom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05347</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05347</id><created>2016-01-20</created><authors><author><keyname>Sarfraz</keyname><forenames>M. Saquib</forenames></author><author><keyname>Stiefelhagen</keyname><forenames>Rainer</forenames></author></authors><title>Deep Perceptual Mapping for Cross-Modal Face Recognition</title><categories>cs.CV</categories><comments>This is the extended version (invited IJCV submission) with new
  results of our previous submission (arXiv:1507.02879)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross modal face matching between the thermal and visible spectrum is a much
desired capability for night-time surveillance and security applications. Due
to a very large modality gap, thermal-to-visible face recognition is one of the
most challenging face matching problem. In this paper, we present an approach
to bridge this modality gap by a significant margin. Our approach captures the
highly non-linear relationship between the two modalities by using a deep
neural network. Our model attempts to learn a non-linear mapping from visible
to thermal spectrum while preserving the identity information. We show
substantive performance improvement on three difficult thermal-visible face
datasets. The presented approach improves the state-of-the-art by more than
10\% on UND-X1 dataset and by more than 15-30\% on NVESD dataset in terms of
Rank-1 identification. Our method bridges the drop in performance due to the
modality gap by more than 40\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05350</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05350</id><created>2016-01-20</created><updated>2016-02-10</updated><authors><author><keyname>Chakrabarti</keyname><forenames>Subit</forenames></author><author><keyname>Bongiovanni</keyname><forenames>Tara</forenames></author><author><keyname>Judge</keyname><forenames>Jasmeet</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author><author><keyname>Ranka</keyname><forenames>Sanjay</forenames></author></authors><title>Disaggregation of SMAP L3 Brightness Temperatures to 9km using Kernel
  Machines</title><categories>cs.CV</categories><comments>14 Pages, 8 Figures, Submitted to IEEE Geoscience and Remote Sensing
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a machine learning algorithm is used for disaggregation of
SMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. It uses image
segmentation to cluster the study region based on meteorological and land cover
similarity, followed by a support vector machine based regression that computes
the value of the disaggregated T$_{\textrm{B}}$ at all pixels. High resolution
remote sensing products such as land surface temperature, normalized difference
vegetation index, enhanced vegetation index, precipitation, soil texture, and
land-cover were used for disaggregation. The algorithm was implemented in Iowa,
United States, from April to July 2015, and compared with the SMAP L3_SM_AP
T$_{\textrm{B}}$ product at 9km. It was found that the disaggregated
T$_{\textrm{B}}$ were very similar to the SMAP-T$_{\textrm{B}}$ product, even
for vegetated areas with a mean difference $\leq$ 5K. However, the standard
deviation of the disaggregation was lower by 7K than that of the AP product.
The probability density functions of the disaggregated T$_{\textrm{B}}$ were
similar to the SMAP-T$_{\textrm{B}}$. The results indicate that this algorithm
may be used for disaggregating T$_{\textrm{B}}$ using complex non-linear
correlations on a grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05353</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05353</id><created>2016-01-20</created><authors><author><keyname>Bazille</keyname><forenames>Hugo</forenames></author><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Gomaa</keyname><forenames>Walid</forenames></author><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author></authors><title>On The Complexity of Bounded Time Reachability for Piecewise Affine
  Systems</title><categories>cs.CC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reachability for piecewise affine systems is known to be undecidable,
starting from dimension $2$. In this paper we investigate the exact complexity
of several decidable variants of reachability and control questions for
piecewise affine systems. We show in particular that the region to region
bounded time versions leads to $NP$-complete or co-$NP$-complete problems,
starting from dimension $2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05356</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05356</id><created>2016-01-20</created><authors><author><keyname>Monti</keyname><forenames>Massimo</forenames></author><author><keyname>Sifalakis</keyname><forenames>Manolis</forenames></author><author><keyname>Tschudin</keyname><forenames>Christian F.</forenames></author><author><keyname>Luise</keyname><forenames>Marco</forenames></author></authors><title>Towards Programmable Network Dynamics: A Chemistry-Inspired Abstraction
  for Hardware Design</title><categories>cs.ET cs.SY</categories><comments>14 pages, non accepted version submitted to IEEE/ACM Transactions on
  Networking on May 2015 (after first submission on May 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemical algorithms are statistical algorithms described and represented as
chemical reaction networks. They are particularly attractive for traffic
shaping and general control of network dynamics; they are analytically
tractable, they reinforce a strict state-to-dynamics relationship, they have
configurable stability properties, and they are directly implemented in
state-space using a high-level (graphical) representation.
  In this paper, we present a direct implementation of chemical algorithms on
FPGA hardware. Besides substantially improving performance, we have achieved
hardware-level programmability and re-configurability of these algorithms at
runtime (not interrupting servicing) and in realtime (with sub-second latency).
This opens an interesting perspective for expanding the currently limited scope
of software defined networking and network virtualisation solutions, to include
programmable control of network dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05360</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05360</id><created>2016-01-20</created><authors><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Gra&#xe7;a</keyname><forenames>Daniel S.</forenames></author><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author></authors><title>Polynomial Time corresponds to Solutions of Polynomial Ordinary
  Differential Equations of Polynomial Length</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an implicit characterization of polynomial time computation in
terms of ordinary differential equations: we characterize the class
$\operatorname{PTIME}$ of languages computable in polynomial time in terms of
differential equations with polynomial right-hand side.
  This result gives a purely continuous (time and space) elegant and simple
characterization of $\operatorname{PTIME}$. This is the first time such classes
are characterized using only ordinary differential equations. Our
characterization extends to functions computable in polynomial time over the
reals in the sense of computable analysis. This extends to deterministic
complexity classes above polynomial time.
  This may provide a new perspective on classical complexity, by giving a way
to define complexity classes, like $\operatorname{PTIME}$, in a very simple
way, without any reference to a notion of (discrete) machine. This may also
provide ways to state classical questions about computational complexity via
ordinary differential equations, i.e.~by using the framework of analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05372</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05372</id><created>2016-01-20</created><authors><author><keyname>Dunn</keyname><forenames>Lawrence</forenames></author><author><keyname>Vicary</keyname><forenames>Jamie</forenames></author></authors><title>Surface proofs for linear logic</title><categories>cs.LO</categories><comments>We show how to represent proofs in linear logic as surfaces, such
  that proofs are logically equivalent just when their surfaces are
  geometrically equivalent</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a proof in multiplicative linear logic can be represented as a
decorated surface, such that two proofs are logically equivalent just when
their surfaces are geometrically equivalent. The technical basis is a coherence
theorem for Frobenius pseudomonoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05377</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05377</id><created>2016-01-20</created><updated>2016-01-28</updated><authors><author><keyname>Mukherjee</keyname><forenames>Manuj</forenames></author><author><keyname>Chan</keyname><forenames>Chung</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Zhou</keyname><forenames>Qiaoqiao</forenames></author></authors><title>Bounds on the Communication Rate Needed to Achieve SK Capacity in the
  Hypergraphical Source Model</title><categories>cs.IT math.IT</categories><comments>A shorter version has been submitted to IEEE ISIT 2016, Barcelona,
  Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the multiterminal source model of Csisz$\text{\'a}$r and Narayan, the
communication complexity, $R_{\text{SK}}$, for secret key (SK) generation is
the minimum rate of communication required to achieve SK capacity. An obvious
upper bound to $R_{\text{SK}}$ is given by $R_{\text{CO}}$, which is the
minimum rate of communication required for \emph{omniscience}. In this paper we
derive a better upper bound to $R_{\text{SK}}$ for the hypergraphical source
model, which is a special instance of the multiterminal source model. The upper
bound is based on the idea of fractional removal of hyperedges. It is further
shown that this upper bound can be computed in polynomial time. We conjecture
that our upper bound is tight. For the special case of a graphical source
model, we also give an explicit lower bound on $R_{\text{SK}}$. This bound,
however, is not tight, as demonstrated by a counterexample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05384</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05384</id><created>2016-01-20</created><authors><author><keyname>Gonnet</keyname><forenames>Pedro</forenames></author><author><keyname>Chalk</keyname><forenames>Aidan B. G.</forenames></author><author><keyname>Schaller</keyname><forenames>Matthieu</forenames></author></authors><title>QuickSched: Task-based parallelism with dependencies and conflicts</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes QuickSched, a compact and efficient Open-Source
C-language library for task-based shared-memory parallel programming.
QuickSched extends the standard dependency-only scheme of task-based
programming with the concept of task conflicts, i.e.~sets of tasks that can be
executed in any order, yet not concurrently. These conflicts are modelled using
exclusively lockable hierarchical resources. The scheduler itself prioritizes
tasks along the critical path of execution and is shown to perform and scale
well on a 64-core parallel shared-memory machine for two example problems: A
tiled QR decomposition and a task-based Barnes-Hut tree code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05400</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05400</id><created>2016-01-20</created><updated>2016-01-21</updated><authors><author><keyname>Kristensen</keyname><forenames>Mads R. B.</forenames></author><author><keyname>Lund</keyname><forenames>Simon A. F.</forenames></author><author><keyname>Blum</keyname><forenames>Troels</forenames></author><author><keyname>Avery</keyname><forenames>James</forenames></author></authors><title>Fusion of Array Operations at Runtime</title><categories>cs.DC cs.PL</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of fusing array operations based on criteria such as
shape compatibility, data reusability, and communication. We formulate the
problem as a graph partition problem that is general enough to handle loop
fusion, combinator fusion, and other types of subroutines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05403</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05403</id><created>2016-01-20</created><authors><author><keyname>Sedoc</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Gallier</keyname><forenames>Jean</forenames></author><author><keyname>Ungar</keyname><forenames>Lyle</forenames></author><author><keyname>Foster</keyname><forenames>Dean</forenames></author></authors><title>Semantic Word Clusters Using Signed Normalized Graph Cuts</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector space representations of words capture many aspects of word
similarity, but such methods tend to make vector spaces in which antonyms (as
well as synonyms) are close to each other. We present a new signed spectral
normalized graph cut algorithm, signed clustering, that overlays existing
thesauri upon distributionally derived vector representations of words, so that
antonym relationships between word pairs are represented by negative weights.
Our signed clustering algorithm produces clusters of words which simultaneously
capture distributional and synonym relations. We evaluate these clusters
against the SimLex-999 dataset (Hill et al.,2014) of human judgments of word
pair similarities, and also show the benefit of using our clusters to predict
the sentiment of a given text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05409</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05409</id><created>2016-01-20</created><authors><author><keyname>Montazeri</keyname><forenames>Mitra</forenames></author><author><keyname>Baghshah</keyname><forenames>Mahdieh Soleymani</forenames></author><author><keyname>Niknafs</keyname><forenames>Aliakbar</forenames></author></authors><title>Selecting Efficient Features via a Hyper-Heuristic Approach</title><categories>cs.CV cs.NE</categories><comments>The Fifth Iran Data Mining Conference (IDMC 2011), Amirkabir
  University of Technology, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By Emerging huge databases and the need to efficient learning algorithms on
these datasets, new problems have appeared and some methods have been proposed
to solve these problems by selecting efficient features. Feature selection is a
problem of finding efficient features among all features in which the final
feature set can improve accuracy and reduce complexity. One way to solve this
problem is to evaluate all possible feature subsets. However, evaluating all
possible feature subsets is an exhaustive search and thus it has high
computational complexity. Until now many heuristic algorithms have been studied
for solving this problem. Hyper-heuristic is a new heuristic approach which can
search the solution space effectively by applying local searches appropriately.
Each local search is a neighborhood searching algorithm. Since each region of
the solution space can have its own characteristics, it should be chosen an
appropriate local search and apply it to current solution. This task is tackled
to a supervisor. The supervisor chooses a local search based on the functional
history of local searches. By doing this task, it can trade of between
exploitation and exploration. Since the existing heuristic cannot trade of
between exploration and exploitation appropriately, the solution space has not
been searched appropriately in these methods and thus they have low convergence
rate. For the first time, in this paper use a hyper-heuristic approach to find
an efficient feature subset. In the proposed method, genetic algorithm is used
as a supervisor and 16 heuristic algorithms are used as local searches.
Empirical study of the proposed method on several commonly used data sets from
UCI data sets indicates that it outperforms recent existing methods in the
literature for feature selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05434</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05434</id><created>2016-01-20</created><authors><author><keyname>Cross</keyname><forenames>Andrew W.</forenames></author><author><keyname>Li</keyname><forenames>Ke</forenames></author><author><keyname>Smith</keyname><forenames>Graeme</forenames></author></authors><title>Uniform Additivity in Classical and Quantum Information</title><categories>quant-ph cs.IT math.IT</categories><comments>13 pages with 4 figures + 25 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory establishes the fundamental limits on data transmission,
storage, and processing. Quantum information theory unites information
theoretic ideas with an accurate quantum-mechanical description of reality to
give a more accurate and complete theory with new and more powerful
possibilities for information processing. The goal of both classical and
quantum information theory is to quantify the optimal rates of interconversion
of different resources. These rates are usually characterized in terms of
entropies. However, nonadditivity of many entropic formulas often makes finding
answers to information theoretic questions intractable. In a few auspicious
cases, such as the classical capacity of a classical channel, the capacity
region of a multiple access channel and the entanglement assisted capacity of a
quantum channel, additivity allows a full characterization of optimal rates.
Here we present a new mathematical property of entropic formulas, uniform
additivity, that is both easily evaluated and rich enough to capture all known
quantum additive formulas. We give a complete characterization of uniformly
additive functions using the linear programming approach to entropy
inequalities. In addition to all known quantum formulas, we find a new and
intriguing additive quantity: the completely coherent information. We also
uncover a remarkable coincidence---the classical and quantum uniformly additive
functions are identical; the tractable answers in classical and quantum
information theory are formally equivalent. Our techniques pave the way for a
deeper understanding of the tractability of information theory, from classical
multi-user problems like broadcast channels to the evaluation of quantum
channel capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05439</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05439</id><created>2016-01-20</created><authors><author><keyname>Treikalis</keyname><forenames>Antons</forenames></author><author><keyname>Merzky</keyname><forenames>Andre</forenames></author><author><keyname>Chen</keyname><forenames>Haoyuan</forenames></author><author><keyname>Lee</keyname><forenames>Tai-Sung</forenames></author><author><keyname>York</keyname><forenames>Darrin M.</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>RepEx: A Flexible Framework for Scalable Replica Exchange Molecular
  Dynamics Simulations</title><categories>cs.DC</categories><comments>12 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replica Exchange (RE) simulations have emerged as an important algorithmic
tool for the molecular sciences. RE simulations involve the concurrent
execution of independent simulations which infrequently interact and exchange
information. The next set of simulation parameters are based upon the outcome
of the exchanges.
  Typically RE functionality is integrated into the molecular simulation
software package. A primary motivation of the tight integration of RE
functionality with simulation codes has been performance. This is limiting at
multiple levels. First, advances in the RE methodology are tied to the
molecular simulation code. Consequently these advances remain confined to the
molecular simulation code for which they were developed. Second, it is
difficult to extend or experiment with novel RE algorithms, since expertise in
the molecular simulation code is typically required.
  In this paper, we propose the RepEx framework which address these
aforementioned shortcomings of existing approaches, while striking the balance
between flexibility (any RE scheme) and scalability (tens of thousands of
replicas) over a diverse range of platforms. RepEx is designed to use a
pilot-job based runtime system and support diverse RE Patterns and Execution
Modes. RE Patterns are concerned with synchronization mechanisms in RE
simulation, and Execution Modes with spatial and temporal mapping of workload
to the CPU cores. We discuss how the design and implementation yield the
following primary contributions of the RepEx framework: (i) its ability to
support different RE schemes independent of molecular simulation codes, (ii)
provide the ability to execute different exchange schemes and replica counts
independent of the specific availability of resources, (iii) provide a runtime
system that has first-class support for task-level parallelism, and (iv)
required scalability along multiple dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05447</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05447</id><created>2016-01-20</created><authors><author><keyname>Tripathi</keyname><forenames>Subarna</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Hwang</keyname><forenames>Youngbae</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong</forenames></author></authors><title>Detecting Temporally Consistent Objects in Videos through Object Class
  Label Propagation</title><categories>cs.CV</categories><comments>Accepted for publication in WACV 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object proposals for detecting moving or static video objects need to address
issues such as speed, memory complexity and temporal consistency. We propose an
efficient Video Object Proposal (VOP) generation method and show its efficacy
in learning a better video object detector. A deep-learning based video object
detector learned using the proposed VOP achieves state-of-the-art detection
performance on the Youtube-Objects dataset. We further propose a clustering of
VOPs which can efficiently be used for detecting objects in video in a
streaming fashion. As opposed to applying per-frame convolutional neural
network (CNN) based object detection, our proposed method called Objects in
Video Enabler thRough LAbel Propagation (OVERLAP) needs to classify only a
small fraction of all candidate proposals in every video frame through
streaming clustering of object proposals and class-label propagation. Source
code will be made available soon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05449</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05449</id><created>2016-01-20</created><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Gill</keyname><forenames>Lisa</forenames></author><author><keyname>Clayton</keyname><forenames>David</forenames></author></authors><title>Detailed temporal structure of communication networks in groups of
  songbirds</title><categories>q-bio.QM cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Animals in groups often exchange calls, in patterns whose temporal structure
may be influenced by contextual factors such as physical location and the
social network structure of the group. We introduce a model-based analysis for
temporal patterns of animal call timing, originally developed for networks of
firing neurons. This has advantages over cross-correlation analysis in that it
can correctly handle common-cause confounds and provides a generative model of
call patterns with explicit parameters for the influences between individuals.
It also has advantages over standard Markovian analysis in that it incorporates
detailed temporal interactions which affect timing as well as sequencing of
calls. Further, a fitted model can be used to generate novel synthetic call
sequences. We apply the method to calls recorded from groups of domesticated
zebra finch (Taenopyggia guttata) individuals. We find that the communication
network in these groups has stable structure that persists from one day to the
next, and that &quot;kernels&quot; reflecting the temporal range of influence have a
characteristic structure for a calling individual's effect on itself, its
partner, and on others in the group. We further find characteristic patterns of
influences by call type as well as by individual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05458</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05458</id><created>2016-01-06</created><authors><author><keyname>Meister</keyname><forenames>Benoit</forenames></author><author><keyname>Baskaran</keyname><forenames>Muthu</forenames></author><author><keyname>Pradelle</keyname><forenames>Benoit</forenames></author><author><keyname>Henretty</keyname><forenames>Thomas</forenames></author><author><keyname>Lethin</keyname><forenames>Richard</forenames></author></authors><title>Efficient Compilation to Event-Driven Task Programs</title><categories>cs.DC</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As illustrated by the emergence of a class of new languages and runtimes, it
is expected that a large portion of the programs to run on extreme scale
computers will need to be written as graphs of event-driven tasks (EDTs). EDT
runtime systems, which schedule such collections of tasks, enable more
concurrency than traditional runtimes by reducing the amount of inter-task
synchronization, improving dynamic load balancing and making more operations
asynchronous.
  We present an efficient technique to generate such task graphs from a
polyhedral representation of a program, both in terms of compilation time and
asymptotic execution time. Task dependences become materialized in different
forms, depending upon the synchronization model available with the targeted
runtime.
  We explore the different ways of programming EDTs using each synchronization
model, and identify important sources of overhead associated with them. We
evaluate these programming schemes according to the cost they entail in terms
of sequential start-up, in-flight task management, space used for
synchronization objects, and garbage collection of these objects.
  While our implementation and evaluation take place in a polyhedral compiler,
the presented overhead cost analysis is useful in the more general context of
automatic code generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05472</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05472</id><created>2016-01-20</created><authors><author><keyname>Yerebakan</keyname><forenames>Halid Ziya</forenames></author><author><keyname>Reda</keyname><forenames>Fitsum</forenames></author><author><keyname>Zhan</keyname><forenames>Yiqiang</forenames></author><author><keyname>Shinagawa</keyname><forenames>Yoshihisa</forenames></author></authors><title>Hierarchical Latent Word Clustering</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new Bayesian non-parametric model by extending the
usage of Hierarchical Dirichlet Allocation to extract tree structured word
clusters from text data. The inference algorithm of the model collects words in
a cluster if they share similar distribution over documents. In our
experiments, we observed meaningful hierarchical structures on NIPS corpus and
radiology reports collected from public repositories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05480</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05480</id><created>2016-01-20</created><authors><author><keyname>Kawase</keyname><forenames>Yasushi</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author><author><keyname>Seimi</keyname><forenames>Kento</forenames></author></authors><title>Optimal Composition Ordering Problems for Piecewise Linear Functions</title><categories>cs.DS cs.DM</categories><comments>19 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce maximum composition ordering problems. The input
is $n$ real functions $f_1,\dots,f_n:\mathbb{R}\to\mathbb{R}$ and a constant
$c\in\mathbb{R}$. We consider two settings: total and partial compositions. The
maximum total composition ordering problem is to compute a permutation
$\sigma:[n]\to[n]$ which maximizes $f_{\sigma(n)}\circ
f_{\sigma(n-1)}\circ\dots\circ f_{\sigma(1)}(c)$, where $[n]=\{1,\dots,n\}$.
The maximum partial composition ordering problem is to compute a permutation
$\sigma:[n]\to[n]$ and a nonnegative integer $k~(0\le k\le n)$ which maximize
$f_{\sigma(k)}\circ f_{\sigma(k-1)}\circ\dots\circ f_{\sigma(1)}(c)$.
  We propose $O(n\log n)$ time algorithms for the maximum total and partial
composition ordering problems for monotone linear functions $f_i$, which
generalize linear deterioration and shortening models for the time-dependent
scheduling problem. We also show that the maximum partial composition ordering
problem can be solved in polynomial time if $f_i$ is of form
$\max\{a_ix+b_i,c_i\}$ for some constants $a_i\,(\ge 0)$, $b_i$ and $c_i$. We
finally prove that there exists no constant-factor approximation algorithm for
the problems, even if $f_i$'s are monotone, piecewise linear functions with at
most two pieces, unless P=NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05483</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05483</id><created>2016-01-20</created><authors><author><keyname>Maguire</keyname><forenames>Joseph</forenames></author><author><keyname>Renaud</keyname><forenames>Karen</forenames></author></authors><title>Alternative Authentication in the Wild</title><categories>cs.HC</categories><doi>10.1109/STAST.2015.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alphanumeric authentication routinely fails to regulate access to resources
with the required stringency, primarily due to usability issues. Initial
deployment did not reveal the problems of passwords, deep and profound flaws
only emerged once passwords were deployed in the wild. The need for a
replacement is widely acknowledged yet despite over a decade of research into
knowledge-based alternatives, few, if any, have been adopted by industry.
Alternatives are unconvincing for three primary reasons. The first is that
alternatives are rarely investigated beyond the initial proposal, with only the
results from a constrained lab test provided to convince adopters of their
viability. The second is that alternatives are seldom tested realistically
where the authenticator mediates access to something of value. The third is
that the testing rarely varies the device or context beyond that initially
targeted. In the modern world different devices are used across a variety of
contexts. What works well in one context may easily fail in another.
Consequently, the contribution of this paper is an &quot;in the wild&quot; evaluation of
an alternative authentication mechanism that had demonstrated promise in its
lab evaluation. In the field test the mechanism was deployed to actual users to
regulate access to an application in a context beyond that initially proposed.
The performance of the mechanism is reported and discussed. We conclude by
reflecting on the value of field evaluations of alternative authentication
mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05484</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05484</id><created>2016-01-20</created><updated>2016-01-26</updated><authors><author><keyname>Correll</keyname><forenames>Nikolaus</forenames></author><author><keyname>Bekris</keyname><forenames>Kostas E.</forenames></author><author><keyname>Berenson</keyname><forenames>Dmitry</forenames></author><author><keyname>Brock</keyname><forenames>Oliver</forenames></author><author><keyname>Causo</keyname><forenames>Albert</forenames></author><author><keyname>Hauser</keyname><forenames>Kris</forenames></author><author><keyname>Okada</keyname><forenames>Kei</forenames></author><author><keyname>Rodriguez</keyname><forenames>Alberto</forenames></author><author><keyname>Romano</keyname><forenames>Joseph M.</forenames></author><author><keyname>Wurman</keyname><forenames>Peter R.</forenames></author></authors><title>Lessons from the Amazon Picking Challenge</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes lessons learned from the first Amazon Picking Challenge
in which 26 international teams designed robotic systems that competed to
retrieve items from warehouse shelves. This task is currently performed by
human workers, and there is hope that robots can someday help increase
efficiency and throughput while lowering cost. We report on a 28-question
survey posed to the teams to learn about each team's background, mechanism
design, perception apparatus, planning and control approach. We identify trends
in this data, correlate it with each team's success in the competition, and
discuss observations and lessons learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05494</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05494</id><created>2016-01-20</created><authors><author><keyname>Hitchcock</keyname><forenames>John M.</forenames></author><author><keyname>Shafei</keyname><forenames>Hadi</forenames></author></authors><title>Autoreducibility of NP-Complete Sets</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the polynomial-time autoreducibility of NP-complete sets and obtain
separations under strong hypotheses for NP. Assuming there is a p-generic set
in NP, we show the following:
  - For every $k \geq 2$, there is a $k$-T-complete set for NP that is $k$-T
autoreducible, but is not $k$-tt autoreducible or $(k-1)$-T autoreducible.
  - For every $k \geq 3$, there is a $k$-tt-complete set for NP that is $k$-tt
autoreducible, but is not $(k-1)$-tt autoreducible or $(k-2)$-T autoreducible.
  - There is a tt-complete set for NP that is tt-autoreducible, but is not
btt-autoreducible.
  Under the stronger assumption that there is a p-generic set in NP $\cap$
coNP, we show:
  - For every $k \geq 2$, there is a $k$-tt-complete set for NP that is $k$-tt
autoreducible, but is not $(k-1)$-T autoreducible.
  Our proofs are based on constructions from separating NP-completeness
notions. For example, the construction of a 2-T-complete set for NP that is not
2-tt-complete also separates 2-T-autoreducibility from 2-tt-autoreducibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05495</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05495</id><created>2016-01-20</created><authors><author><keyname>Khetan</keyname><forenames>Ashish</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author></authors><title>Data-driven Rank Breaking for Efficient Rank Aggregation</title><categories>cs.LG stat.ML</categories><comments>46 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank aggregation systems collect ordinal preferences from individuals to
produce a global ranking that represents the social preference. Rank-breaking
is a common practice to reduce the computational complexity of learning the
global ranking. The individual preferences are broken into pairwise comparisons
and applied to efficient algorithms tailored for independent paired
comparisons. However, due to the ignored dependencies in the data, naive
rank-breaking approaches can result in inconsistent estimates. The key idea to
produce accurate and unbiased estimates is to treat the pairwise comparisons
unequally, depending on the topology of the collected data. In this paper, we
provide the optimal rank-breaking estimator, which not only achieves
consistency but also achieves the best error bound. This allows us to
characterize the fundamental tradeoff between accuracy and complexity. Further,
the analysis identifies how the accuracy depends on the spectral gap of a
corresponding comparison graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05506</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05506</id><created>2016-01-20</created><authors><author><keyname>Jiang</keyname><forenames>Biaobin</forenames></author><author><keyname>Kloster</keyname><forenames>Kyle</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Gribskov</keyname><forenames>Michael</forenames></author></authors><title>AptRank: an adaptive PageRank model for protein function prediction on
  bi-relational graphs</title><categories>q-bio.MN cs.SI</categories><comments>17 pages</comments><msc-class>92-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion-based network models are widely used for protein function
prediction using protein network data, and have been shown to outperform
neighborhood-based and module-based methods. Recent studies have shown that
integrating the hierarchical structure of the Gene Ontology (GO) data
dramatically improves prediction accuracy. However, previous methods usually
used the GO hierarchy to refine the prediction results of multiple classifiers,
or flattened the hierarchy into a function-function similarity kernels. Only
rarely has the hierarchy been used as a second layer of the network connecting
the protein network with functional annotations.
  We first construct a Bi-relational graph (Birg) model comprised of both
protein-protein association and function-function hierarchical networks. We
then propose two diffusion-based methods, BirgRank and AptRank, both of which
use PageRank to diffuse information flows on this two-layer model. BirgRank is
a direct application of traditional PageRank with fixed decay parameters. In
contrast, AptRank uses an adaptive diffusion mechanism. We evaluate the ability
of both methods to predict protein function on yeast, fly, and human protein
datasets, and compare with four previous methods: GeneMANIA, TMC, ProteinRank
and clusDCA. We find that both BirgRank and AptRank outperform the previous
methods, especially when only 10% of the data are given for training.
  AptRank naturally combines protein-protein associations and function-function
relationships into a two-layer network model, and takes full advantage of the
hierarchical structure of the Gene Ontology, using directional diffusion
without flattening the ontological hierarchy into a similarity kernel.
Introducing an adaptive mechanism to the traditional, fixed-parameter model of
PageRank greatly improves the accuracy of protein function prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05511</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05511</id><created>2016-01-20</created><authors><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author><author><keyname>Ogunbona</keyname><forenames>Philip O.</forenames></author><author><keyname>Wang</keyname><forenames>Pichao</forenames></author><author><keyname>Tang</keyname><forenames>Chang</forenames></author></authors><title>RGB-D-based Action Recognition Datasets: A Survey</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human action recognition from RGB-D (Red, Green, Blue and Depth) data has
attracted increasing attention since the first work reported in 2010. Over this
period, many benchmark datasets have been created to facilitate the development
and evaluation of new algorithms. This raises the question of which dataset to
select and how to use it in providing a fair and objective comparative
evaluation against state-of-the-art methods. To address this issue, this paper
provides a comprehensive review of the most commonly used action recognition
related RGB-D video datasets, including 27 single-view datasets, 10 multi-view
datasets, and 7 multi-person datasets. The detailed information and analysis of
these datasets is a useful resource in guiding insightful selection of datasets
for future research. In addition, the issues with current algorithm evaluation
vis-\'{a}-vis limitations of the available datasets and evaluation protocols
are also highlighted; resulting in a number of recommendations for collection
of new datasets and use of evaluation protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05516</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05516</id><created>2016-01-21</created><authors><author><keyname>Song</keyname><forenames>Linqi</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>A Deterministic Algorithm for Pliable Index Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pliable index coding considers a server with m messages, and n clients where
each has as side information a subset of the messages. We seek to minimize the
number of transmissions the server should make, so that each client receives
(any) one message she does not already have. Previous work has shown that the
server can achieve this using O(\log^2(n)) transmissions and needs at least
\Omega(log(n)) transmissions in the worst case, but finding a code of optimal
length is NP-hard. In this paper, we propose a deterministic algorithm that we
prove achieves this upper bound, that is, in an order almost as the worst-case
optimal code length. We also establish a connection between the pliable index
coding problem and the minrank problem over a family of mixed matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05520</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05520</id><created>2016-01-21</created><authors><author><keyname>O'Connor</keyname><forenames>Liam</forenames></author><author><keyname>Rizkallah</keyname><forenames>Christine</forenames></author><author><keyname>Chen</keyname><forenames>Zilin</forenames></author><author><keyname>Amani</keyname><forenames>Sidney</forenames></author><author><keyname>Lim</keyname><forenames>Japheth</forenames></author><author><keyname>Nagashima</keyname><forenames>Yutaka</forenames></author><author><keyname>Sewell</keyname><forenames>Thomas</forenames></author><author><keyname>Hixon</keyname><forenames>Alex</forenames></author><author><keyname>Keller</keyname><forenames>Gabriele</forenames></author><author><keyname>Murray</keyname><forenames>Toby</forenames></author><author><keyname>Klein</keyname><forenames>Gerwin</forenames></author></authors><title>COGENT: Certified Compilation for a Functional Systems Language</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a self-certifying compiler for the COGENT systems language. COGENT
is a restricted, polymorphic, higher-order, and purely functional language with
linear types and without the need for a trusted runtime or garbage collector.
It compiles to efficient C code that is designed to interoperate with existing
C functions. The language is suited for layered systems code with minimal
sharing such as file systems or network protocol control code. For a well-typed
COGENT program, the compiler produces C code, a high-level shallow embedding of
its semantics in Isabelle/HOL, and a proof that the C code correctly implements
this embedding. The aim is for proof engineers to reason about the full
semantics of real-world systems code productively and equationally, while
retaining the interoperability and leanness of C. We describe the formal
verification stages of the compiler, which include automated formal refinement
calculi, a switch from imperative update semantics to functional value
semantics formally justified by the linear type system, and a number of
standard compiler phases such as type checking and monomorphisation. The
compiler certificate is a series of language-level meta proofs and per-program
translation validation phases, combined into one coherent top-level theorem in
Isabelle/HOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05527</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05527</id><created>2016-01-21</created><authors><author><keyname>John</keyname><forenames>Emmanuel</forenames></author><author><keyname>Safro</keyname><forenames>Ilya</forenames></author></authors><title>Single- and Multi-level Network Sparsification by Algebraic Distance</title><categories>cs.SI cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network sparsification methods play an important role in modern network
analysis when fast estimation of computationally expensive properties (such as
the diameter, centrality indices, and paths) is required. We propose a method
of network sparsification that preserves a wide range of structural properties.
Depending on the analysis goals, the method allows to distinguish between local
and global range edges that can be filtered out during the sparsification.
First we rank edges by their algebraic distances and then we sample them. We
also introduce a multilevel framework for sparsification that can be used to
control the sparsification process at various coarse-grained resolutions. Based
primarily on the matrix-vector multiplications, our method is easily
parallelized for different architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05532</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05532</id><created>2016-01-21</created><authors><author><keyname>Belyi</keyname><forenames>Alexander</forenames></author><author><keyname>Bojic</keyname><forenames>Iva</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Sitko</keyname><forenames>Izabela</forenames></author><author><keyname>Hawelka</keyname><forenames>Bartosz</forenames></author><author><keyname>Rudikova</keyname><forenames>Lada</forenames></author><author><keyname>Kurbatski</keyname><forenames>Alexander</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Global multi-layer network of human mobility</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>13 pages, 10 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent availability of geo-localized data capturing individual human activity
together with the statistical data on international migration opened up
unprecedented opportunities for a study on global mobility. In this paper we
consider it from the perspective of a multi-layer complex network, built using
a combination of three datasets: Twitter, Flickr and official migration data.
Those datasets provide different but equally important insights on the global
mobility: while the first two highlight short-term visits of people from one
country to another, the last one - migration - shows the long-term mobility
perspective, when people relocate for good. And the main purpose of the paper
is to emphasize importance of this multi-layer approach capturing both aspects
of human mobility at the same time. So we start from a comparative study of the
network layers, comparing short- and long- term mobility through the
statistical properties of the corresponding networks, such as the parameters of
their degree centrality distributions or parameters of the corresponding
gravity model being fit to the network. We also focus on the differences in
country ranking by their short- and long-term attractiveness, discussing the
most noticeable outliers. Finally, we apply this multi-layered human mobility
network to infer the structure of the global society through a community
detection approach and demonstrate that consideration of mobility from a
multi-layer perspective can reveal important global spatial patterns in a way
more consistent with other available relevant sources of international
connections, in comparison to the spatial structure inferred from each network
layer taken separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05533</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05533</id><created>2016-01-21</created><authors><author><keyname>Sugiyama</keyname><forenames>Mahito</forenames></author><author><keyname>Nakahara</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Tsuda</keyname><forenames>Koji</forenames></author></authors><title>Information Decomposition on Structured Space</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build information geometry for a partially ordered set of variables and
define orthogonal decomposition of information theoretic quantities. The
natural connection between information geometry and order theory leads to
efficient decomposition algorithms. It is a generalization of Amari's seminal
work on hierarchical decomposition of probability distributions on event
combinations and allows us to analyze high-order statistical interactions
arising in neuroscience, biology, and machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05535</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05535</id><created>2016-01-21</created><authors><author><keyname>Charbonnier</keyname><forenames>Pierre</forenames><affiliation>SYNTIM</affiliation></author><author><keyname>Tarel</keyname><forenames>Jean-Philippe</forenames><affiliation>SYNTIM</affiliation></author><author><keyname>Goulette</keyname><forenames>Francois</forenames><affiliation>CAOR</affiliation></author></authors><title>On the Diagnostic of Road Pathway Visibility</title><categories>cs.CV</categories><comments>in Transport Research Arena Europe, 2010, Bruxelles, France. 2010</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visibility distance on the road pathway plays a significant role in road
safety and in particular, has a clear impact on the choice of speed limits.
Visibility distance is thus of importance for road engineers and authorities.
While visibility distance criteria are routinely taken into account in road
design, only a few systems exist for estimating it on existing road networks.
Most existing systems comprise a target vehicle followed at a constant distance
by an observer vehicle, which only allows to check if a given, fixed visibility
distance is available. We propose two new approaches that allow estimating the
maximum available visibility distance, involving only one vehicle and based on
different sensor technologies, namely binocular stereovision and 3D range
sensing (LIDAR). The first approach is based on the processing of two views
taken by digital cameras onboard the diagnostic vehicle. The main stages of the
process are: road segmentation, edge registration between the two views, road
profile 3D reconstruction and finally, maximal road visibility distance
estimation. The second approach involves the use of a Terrestrial LIDAR Mobile
Mapping System. The triangulated 3D model of the road and its surroundings
provided by the system is used to simulate targets at different distances,
which allows estimating the maximum geometric visibility distance along the
pathway. These approaches were developed in the context of the SARI-VIZIR
PREDIT project. Both approaches are described, evaluated and compared. Their
pros and cons with respect to vehicle following systems are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05539</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05539</id><created>2016-01-21</created><authors><author><keyname>Wang</keyname><forenames>Xiang</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Constructions of Snake-in-the-Box Codes under $\ell_{\infty}$-metric for
  Rank Modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the rank modulation scheme, Gray codes are very useful in the realization
of flash memories. For a Gray code in this scheme, two adjacent codewords are
obtained by using one &quot;push-to-the-top&quot; operation. Moreover, snake-in-the-box
codes under the $\ell_{\infty}$-metric are Gray codes, which can be capable of
detecting one $\ell_{\infty}$-error. In this paper, we give two constructions
of $\ell_{\infty}$-snakes. On the one hand, inspired by Yehezkeally and
Schwartz's construction, we present a new construction of the
$\ell_{\infty}$-snake. The length of this $\ell_{\infty}$-snake is longer than
the length of the $\ell_{\infty}$-snake constructed by Yehezkeally and
Schwartz. On the other hand, we also give another construction of
$\ell_{\infty}$-snakes by using $\mathcal{K}$-snakes and obtain the longer
$\ell_{\infty}$-snakes than the previously known ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05557</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05557</id><created>2016-01-21</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author></authors><title>A New Approach for Testing Properties of Discrete Distributions</title><categories>cs.DS cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study problems in distribution property testing: Given sample access to
one or more unknown discrete distributions, we want to determine whether they
have some global property or are $\epsilon$-far from having the property in
$\ell_1$ distance. In this paper, we provide a simple and general approach to
obtain upper bounds in this setting, by reducing $\ell_1$-testing to
$\ell_2$-testing. Our reduction yields optimal $\ell_1$-testers, by using a
standard $\ell_2$-tester as a black-box.
  Using our framework, we obtain sample-optimal and computationally efficient
estimators for a wide variety of $\ell_1$ distribution testing problems,
including the following: identity testing to a fixed distribution, closeness
testing between two unknown distributions (with equal/unequal sample sizes),
independence testing (in any number of dimensions), closeness testing for
collections of distributions, and testing $k$-histograms. For most of these
problems, we give the first optimal testers in the literature. Moreover, our
estimators are significantly simpler to state and analyze compared to previous
approaches.
  As our second main contribution, we provide a direct general approach for
proving distribution testing lower bounds, by bounding the mutual information.
Our lower bound approach is not restricted to symmetric properties, and we use
it to prove tight lower bounds for the aforementioned problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05561</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05561</id><created>2016-01-21</created><updated>2016-01-21</updated><authors><author><keyname>Yu</keyname><forenames>Xinjia</forenames></author></authors><title>Emotional Interaction between Artificial Companion Agents and the
  Elderly</title><categories>cs.CY cs.HC</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial companion agents are defined as hardware or software entities
designed to provide companionship to a person. The senior population are facing
a special demand for companionship. Artificial companion agents have been
demonstrated to be useful in therapy, offering emotional companionship and
facilitating socialization. However, there is lack of empirical studies on what
the artificial agents should do and how they can communicate with human beings
better. To address these functional research problems, we attempt to establish
a model to guide artificial companion designers to meet the emotional needs of
the elderly through fulfilling absent roles in their social interactions. We
call this model the Role Fulfilling Model. This model aims to use role as a key
concept to analyse the demands from the elderly for functionalities from an
emotional perspective in artificial companion agent designs and technologies.
To evaluate the effectiveness of this model, we proposed a serious game
platform named Happily Aging in Place. This game will help us to involve a
large scale of senior users through crowdsourcing to test our model and
hypothesis.
  To improve the emotional communication between artificial companion agents
and users, This book draft addresses an important but largely overlooked aspect
of affective computing: how to enable companion agents to express mixed
emotions with facial expressions? And furthermore, for different users, do
individual heterogeneity affects the perception of the same facial expressions?
Some preliminary results about gender differences have been found. The
perception of facial expressions between different age groups or cultural
backgrounds will be held in future study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05569</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05569</id><created>2016-01-21</created><authors><author><keyname>Zambonelli</keyname><forenames>Franco</forenames></author></authors><title>Towards a General Software Engineering Methodology for the Internet of
  Things</title><categories>cs.SE cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As research in the Internet of Thing area progresses, and a multitude of
proposals exist to solve a variety of problems, the need for a general
principled software engineering approach for the systematic development of IoT
systems and applications arises. In this paper, by synthesizing form the state
of the art in the area, we attempt at framing the key concepts and abstractions
that revolve around the design and development of IoT systems and applications,
and draft a software engineering methodology centered on these abstractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05575</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05575</id><created>2016-01-21</created><authors><author><keyname>Shams</keyname><forenames>Bita</forenames></author><author><keyname>Haratizadeh</keyname><forenames>Saman</forenames></author></authors><title>SibRank: Signed Bipartite Network Analysis for Neighbor-based
  Collaborative Ranking</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative ranking is an emerging field of recommender systems that
utilizes users' preference data rather than rating values. Unfortunately,
neighbor-based collaborative ranking has gained little attention despite its
more flexibility and justifiability. This paper proposes a novel framework,
called SibRank that seeks to improve the state of the art neighbor-based
collaborative ranking methods. SibRank represents users' preferences as a
signed bipartite network, and finds similar users, through a novel personalized
ranking algorithm in signed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05585</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05585</id><created>2016-01-21</created><updated>2016-03-08</updated><authors><author><keyname>Rahmathullah</keyname><forenames>Abu Sajana</forenames></author><author><keyname>Garc&#xed;a-Fern&#xe1;ndez</keyname><forenames>&#xc1;ngel F.</forenames></author><author><keyname>Svensson</keyname><forenames>Lennart</forenames></author></authors><title>Generalized optimal sub-pattern assignment metric</title><categories>cs.SY cs.CV</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the generalized optimal sub-pattern assignment
(GOSPA) metric on the space of sets of targets. This metric is a generalized
version of the unnormalized optimal sub-pattern assignment (OSPA) metric. The
difference between unnormalized OSPA and GOSPA is that, in the proposed metric,
we can choose a range of values for the cardinality mismatch penalty for a
given cut-off distance c. We argue that in multiple target tracking, we should
select the cardinality mismatch of GOSPA in a specific way, which is different
from OSPA. In this case, the metric can be viewed as sum of target localization
error and error due to missed and false targets. We also extend the GOSPA
metric to the space of random finite sets, and show that both mean GOSPA and
root mean squared GOSPA are metrics, which are useful for performance
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05590</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05590</id><created>2016-01-21</created><authors><author><keyname>Yan</keyname><forenames>Da</forenames></author><author><keyname>Huang</keyname><forenames>Yuzhen</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Wu</keyname><forenames>Huanhuan</forenames></author></authors><title>Efficient Processing of Very Large Graphs in a Small Cluster</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the success of Google's Pregel, many systems have been developed
recently for iterative computation over big graphs. These systems provide a
user-friendly vertex-centric programming interface, where a programmer only
needs to specify the behavior of one generic vertex when developing a parallel
graph algorithm. However, most existing systems require the input graph to
reside in memories of the machines in a cluster, and the few out-of-core
systems suffer from problems such as poor efficiency for sparse computation
workload, high demand on network bandwidth, and expensive cost incurred by
external-memory join and group-by.
  In this paper, we introduce the GraphD system for a user to process very
large graphs with ordinary computing resources. GraphD fully overlaps
computation with communication, by streaming edges and messages on local disks,
while transmitting messages in parallel. For a broad class of Pregel algorithms
where message combiner is applicable, GraphD eliminates the need of any
expensive external-memory join or group-by. These key techniques allow GraphD
to achieve comparable performance to in-memory Pregel-like systems without
keeping edges and messages in memories. We prove that to process a graph G=(V,
E) with n machines using GraphD, each machine only requires O(|V|/n) memory
space, allowing GraphD to scale to very large graphs with a small cluster.
Extensive experiments show that GraphD beats existing out-of-core systems by
orders of magnitude, and achieves comparable performance to in-memory systems
running with enough memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05593</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05593</id><created>2016-01-21</created><authors><author><keyname>Pears</keyname><forenames>Nick</forenames></author><author><keyname>Duncan</keyname><forenames>Christian</forenames></author></authors><title>Automatic 3D modelling of craniofacial form</title><categories>cs.CV</categories><comments>57 pages</comments><acm-class>I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-dimensional models of craniofacial variation over the general
population are useful for assessing pre- and post-operative head shape when
treating various craniofacial conditions, such as craniosynostosis. We present
a new method of automatically building both sagittal profile models and full 3D
surface models of the human head using a range of techniques in 3D surface
image analysis; in particular, automatic facial landmarking using supervised
machine learning, global and local symmetry plane detection using a variant of
trimmed iterative closest points, locally-affine template warping (for full 3D
models) and a novel pose normalisation using robust iterative ellipse fitting.
The PCA-based models built using the new pose normalisation are more compact
than those using Generalised Procrustes Analysis and we demonstrate their
utility in a clinical case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05594</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05594</id><created>2016-01-21</created><authors><author><keyname>Elishco</keyname><forenames>Ohad</forenames></author><author><keyname>Meyerovitch</keyname><forenames>Tom</forenames></author><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author></authors><title>Encoding Semiconstrained Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semiconstrained systems were recently suggested as a generalization of
constrained systems, commonly used in communication and data-storage
applications that require certain offending subsequences be avoided. In an
attempt to apply techniques from constrained systems, we study sequences of
constrained systems that are contained in, or contain, a given semiconstrained
system, while approaching its capacity. In the case of contained systems we
describe to such sequences resulting in constant-to-constant bit-rate block
encoders and sliding-block encoders. Surprisingly, in the case of containing
systems we show that a &quot;generic&quot; semiconstrained system is never contained in a
proper fully-constrained system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05595</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05595</id><created>2016-01-21</created><authors><author><keyname>Hao</keyname><forenames>Jie</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author></authors><title>Bounds and Constructions of Locally Repairable Codes: Parity-check
  Matrix Approach</title><categories>cs.IT math.IT</categories><comments>17 pages, submitted to STOC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code symbol of a linear code is said to have locality r if this symbol
could be recovered by at most r other code symbols. An (n,k,r) locally
repairable code (LRC) with all symbol locality is a linear code with length n,
dimension k, and locality r for all symbols. Recently, there are lots of
studies on the bounds and constructions of LRCs, most of which are essentially
based on the generator matrix of the linear code. Up to now, the most important
bounds of minimum distance for LRCs might be the well-known Singleton-like
bound and the Cadambe-Mazumdar bound concerning the field size.
  In this paper, we study the bounds and constructions of LRCs from views of
parity-check matrices. Firstly, we set up a new characterization of the
parity-check matrix for an LRC. Then, the proposed parity-check matrix is
employed to analyze the minimum distance. We give an alternative simple proof
of the well-known Singleton-like bound for LRCs with all symbol locality, and
then easily generalize it to a more general bound, which essentially coincides
with the Cadambe-Mazumdar bound and includes the Singleton-like bound as a
specific case. Based on the proposed characterization of parity-check matrices,
necessary conditions of meeting the Singleton-like bound are obtained, which
naturally lead to a construction framework of good LRCs. Finally, two classes
of optimal LRCs based on linearized polynomial theories and Vandermonde
matrices are obtained under the construction framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05596</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05596</id><created>2016-01-21</created><authors><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>Karpuk</keyname><forenames>David A.</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>Decoding in Compute-and-Forward Relaying: Real Lattices and the Flatness
  of Lattice Sums</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a distributed communications scenario, a relay applying the
compute-and-forward strategy for the real-valued channel model aims to decode
an integer linear combination of transmitted messages, a task of very complex
nature for which general efficient algorithms for dimension n &gt; 1 have not yet
been developed. Nonetheless, the maximum-likelihood decoding metric related to
solving for the desired lattice point exhibits interesting properties which
lead to partial design criteria for lattice codes in compute-and-forward.
  This article generalizes maximum-likelihood decoding at the relay to allow
for arbitrary real lattice codes at the transmitters, and studies the behavior
of the resulting decoding metric using an approximation of the theta series of
a lattice, which is itself derived in this article. For the first time, the
resulting random sums of lattices over whose points the relay needs to perform
a sum are analyzed and furthermore, previous related work is extended to the
case of K &gt; 2 transmitters. The specific cases K = 2 and K = 3 are studied
empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05603</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05603</id><created>2016-01-21</created><authors><author><keyname>Elshaer</keyname><forenames>Hisham</forenames></author><author><keyname>Vlachos</keyname><forenames>Christoforos</forenames></author><author><keyname>Friderikos</keyname><forenames>Vasilis</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author></authors><title>Interference-Aware Decoupled Cell Association in Device-to-Device based
  5G Networks</title><categories>cs.NI</categories><comments>5 pages, 5 figures. Accepted in IEEE VTC spring 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell association in cellular networks is an important aspect that impacts
network capacity and eventually quality of experience. The scope of this work
is to investigate the different and generalized cell association (CAS)
strategies for Device-to-Device (D2D) communications in a cellular network
infrastructure. To realize this, we optimize D2D-based cell association by
using the notion of uplink and downlink decoupling that was proven to offer
significant performance gains. We propose an integer linear programming (ILP)
optimization framework to achieve efficient D2D cell association that minimizes
the interference caused by D2D devices onto cellular communications in the
uplink as well as improve the D2D resource utilization efficiency. Simulation
results based on Vodafone's LTE field trial network in a dense urban scenario
highlight the performance gains and render this proposal a candidate design
approach for future 5G networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05610</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05610</id><created>2016-01-21</created><authors><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author></authors><title>Reading Car License Plates Using Deep Convolutional Neural Networks and
  LSTMs</title><categories>cs.CV</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we tackle the problem of car license plate detection and
recognition in natural scene images. Inspired by the success of deep neural
networks (DNNs) in various vision applications, here we leverage DNNs to learn
high-level features in a cascade framework, which lead to improved performance
on both detection and recognition.
  Firstly, we train a $37$-class convolutional neural network (CNN) to detect
all characters in an image, which results in a high recall, compared with
conventional approaches such as training a binary text/non-text classifier.
False positives are then eliminated by the second plate/non-plate CNN
classifier. Bounding box refinement is then carried out based on the edge
information of the license plates, in order to improve the
intersection-over-union (IoU) ratio. The proposed cascade framework extracts
license plates effectively with both high recall and precision. Last, we
propose to recognize the license characters as a {sequence labelling} problem.
A recurrent neural network (RNN) with long short-term memory (LSTM) is trained
to recognize the sequential features extracted from the whole license plate via
CNNs. The main advantage of this approach is that it is segmentation free. By
exploring context information and avoiding errors caused by segmentation, the
RNN method performs better than a baseline method of combining segmentation and
deep CNN classification; and achieves state-of-the-art recognition accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05613</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05613</id><created>2016-01-21</created><updated>2016-01-30</updated><authors><author><keyname>Wang</keyname><forenames>Boyue</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author></authors><title>Partial Sum Minimization of Singular Values Representation on Grassmann
  Manifolds</title><categories>cs.CV</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a significant subspace clustering method, low rank representation (LRR)
has attracted great attention in recent years. To further improve the
performance of LRR and extend its applications, there are several issues to be
resolved. The nuclear norm in LRR does not sufficiently use the prior knowledge
of the rank which is known in many practical problems. The LRR is designed for
vectorial data from linear spaces, thus not suitable for high dimensional data
with intrinsic non-linear manifold structure. This paper proposes an extended
LRR model for manifold-valued Grassmann data which incorporates prior knowledge
by minimizing partial sum of singular values instead of the nuclear norm,
namely Partial Sum minimization of Singular Values Representation (GPSSVR). The
new model not only enforces the global structure of data in low rank, but also
retains important information by minimizing only smaller singular values. To
further maintain the local structures among Grassmann points, we also integrate
the Laplacian penalty with GPSSVR. An effective algorithm is proposed to solve
the optimization problem based on the GPSSVR model. The proposed model and
algorithms are assessed on some widely used human action video datasets and a
real scenery dataset. The experimental results show that the proposed methods
obviously outperform other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05630</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05630</id><created>2016-01-21</created><updated>2016-01-24</updated><authors><author><keyname>Palowitch</keyname><forenames>John</forenames></author><author><keyname>Bhamidi</keyname><forenames>Shankar</forenames></author><author><keyname>Nobel</keyname><forenames>Andrew B.</forenames></author></authors><title>The Continuous Configuration Model: A Null for Community Detection on
  Weighted Networks</title><categories>cs.SI physics.soc-ph stat.ME</categories><comments>Code and supplemental info available at
  http://stats.johnpalowitch.com/ccme. Version 2 changes: grant info added, 1
  reference added, bibliography section moved to end, condensed bib line
  spacing, corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is the process of grouping strongly connected nodes in a
network. Many community detection methods for un-weighted networks have a
theoretical basis in a null model, which provides an interpretation of
resulting communities in terms of statistical significance. In this paper, we
introduce a null for sparse weighted networks called the continuous
configuration model. We prove a Central Limit Theorem for sums of edge weights
under the model, and propose a community extraction method called CCME which
combines this result with an iterative multiple testing framework. To benchmark
the method, we provide a simulation framework that incorporates the continuous
configuration model as a way to plant null or &quot;background&quot; nodes in weighted
networks with communities. We show CCME to be competitive with existing methods
in accurately identifying both disjoint and overlapping communities, while
being particularly effective in ignoring background nodes when they exist. We
present two real-world data sets with potential background nodes and analyze
them with CCME, yielding results that correspond to known features of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05638</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05638</id><created>2016-01-21</created><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author></authors><title>Asymptotic Optimality of Massive MIMO Systems Using Densely Spaced
  Transmit Antennas</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a deterministic physical model of massive multiple-input
multiple-output (MIMO) systems with uniform linear antenna arrays. It is known
that the maximum spatial degrees of freedom is achieved by spacing antenna
elements at half the carrier wavelength. The purpose of this paper is to
investigate the impacts of spacing antennas more densely than the critical
separation. The achievable rates of MIMO systems are evaluated in the
large-system limit, where the lengths of transmit and receive antenna arrays
tend to infinity with the antenna separations kept constant. The main results
are twofold: One is that, under a mild assumption of channel instances, spacing
antennas densely cannot improve the capacity of MIMO systems normalized by the
spatial degrees of freedom. The other is that the normalized achievable rate of
quadrature phase-shift keying converges to the normalized capacity achieved by
optimal Gaussian signaling, as the transmit antenna separation tends to zero
after taking the large-system limit. The latter result is based on mathematical
similarity between MIMO transmission and faster-than-Nyquist signaling in
signal space representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05644</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05644</id><created>2016-01-21</created><authors><author><keyname>Peng</keyname><forenames>Weilong</forenames><affiliation>School of Computer Science, Tianjin University</affiliation></author><author><keyname>Feng</keyname><forenames>Zhiyong</forenames><affiliation>School of Computer Science, Tianjin University</affiliation></author><author><keyname>Xu</keyname><forenames>Chao</forenames><affiliation>School of Software, Tianjin University</affiliation></author></authors><title>B-spline Shape from Motion &amp; Shading: An Automatic Free-form Surface
  Modeling for Face Reconstruction</title><categories>cs.CV cs.GR</categories><comments>9 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, many methods have been proposed for face reconstruction from
multiple images, most of which involve fundamental principles of Shape from
Shading and Structure from motion. However, a majority of the methods just
generate discrete surface model of face. In this paper, B-spline Shape from
Motion and Shading (BsSfMS) is proposed to reconstruct continuous B-spline
surface for multi-view face images, according to an assumption that shading and
motion information in the images contain 1st- and 0th-order derivative of
B-spline face respectively. Face surface is expressed as a B-spline surface
that can be reconstructed by optimizing B-spline control points. Therefore,
normals and 3D feature points computed from shading and motion of images
respectively are used as the 1st- and 0th- order derivative information, to be
jointly applied in optimizing the B-spline face. Additionally, an IMLS
(iterative multi-least-square) algorithm is proposed to handle the difficult
control point optimization. Furthermore, synthetic samples and LFW dataset are
introduced and conducted to verify the proposed approach, and the experimental
results demonstrate the effectiveness with different poses, illuminations,
expressions etc., even with wild images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05647</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05647</id><created>2016-01-21</created><authors><author><keyname>Cernak</keyname><forenames>Milos</forenames></author><author><keyname>Asaei</keyname><forenames>Afsaneh</forenames></author><author><keyname>Bourlard</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>On Structured Sparsity of Phonological Posteriors for Linguistic Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The speech signal conveys information on different time scales from short
(20-40 ms) time scale or segmental, associated to phonological and phonetic
information to long (150-250 ms) time scale or supra segmental, associated to
syllabic and prosodic information. Linguistic and neurocognitive studies
recognize the phonological classes at segmental level as the essential and
invariant representations used in speech temporal organization. In the context
of speech processing, a deep neural network (DNN) is an effective computational
method to infer the probability of individual phonological classes from a short
segment of speech signal. A vector of all phonological class probabilities is
referred to as phonological posterior. There are only very few classes
comprising a short term speech signal; hence, the phonological posterior is a
sparse vector. Although the phonological posteriors are estimated at segmental
level, we claim that they convey supra-segmental information. Namely, we
demonstrate that phonological posteriors are indicative of syllabic and
prosodic events. Building on findings from converging linguistic evidence on
the gestural model of Articulatory Phonology as well as neural basis of speech
perception, we hypothesize that phonological posteriors convey properties of
linguistic classes at multiple time scales, and this information is embedded in
their support (index) of active coefficients. To verify this hypothesis, we
obtain a binary representation of phonological posteriors at segmental level
which is referred to as first-order sparsity structure; the high-order
structures are obtained by concatenation of first-order binary vectors. It is
then confirmed that classification of supra-segmental linguistic events, the
problem known as linguistic parsing, can be achieved with high accuracy using a
simple binary pattern matching of first-order or high-order structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05648</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05648</id><created>2016-01-21</created><updated>2016-02-11</updated><authors><author><keyname>Liu</keyname><forenames>Qingzhi</forenames></author><author><keyname>Y&#x131;ld&#x131;r&#x131;m</keyname><forenames>Kas&#x131;m Sinan</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Warnier</keyname><forenames>Martijn</forenames></author></authors><title>Safe and Secure Wireless Power Transfer Networks: Challenges and
  Opportunities in RF-Based Systems</title><categories>cs.NI cs.CR</categories><comments>Removed some references, added new references, corrected typos,
  revised some sections (mostly I-B and III-C)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RF-based wireless power transfer networks (WPTNs) are deployed to transfer
power to embedded devices over the air via RF waves. Up until now, a
considerable amount of effort has been devoted by researchers to design WPTNs
that maximize several objectives such as harvested power, energy outage and
charging delay. However, inherent security and safety issues are generally
overlooked and these need to be solved if WPTNs are to be become widespread.
This article focuses on safety and security problems related WPTNs and
highlight their cruciality in terms of efficient and dependable operation of
RF-based WPTNs. We provide a overview of new research opportunities in this
emerging domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05650</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05650</id><created>2016-01-21</created><updated>2016-01-24</updated><authors><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>Exponent Function for Source Coding with Side Information at the Decoder
  at Rates below the Rate Distortion Function</title><categories>cs.IT math.IT</categories><comments>17 pages, 1 figures, extended version of ISIT 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the rate distortion problem with side information at the decoder
posed and investigated by Wyner and Ziv. The rate distortion function
indicating the trade-off between the rate on the data compression and the
quality of data obtained at the decoder was determined by Wyner and Ziv. In
this paper, we study the error probability of decoding at rates below the rate
distortion function. We evaluate the probability of decoding such that the
estimation of source outputs by the decoder has a distortion not exceeding a
prescribed distortion level. We prove that when the rate on the data
compression is below the rate distortion function this probability goes to zero
exponentially and derive an explicit lower bound of this exponent function. On
the Wyner-Ziv source coding problem the strong converse coding theorem has not
been established yet. We prove this as a simple corollary of our result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05654</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05654</id><created>2016-01-21</created><authors><author><keyname>Gianniotis</keyname><forenames>Nikolaos</forenames></author><author><keyname>K&#xfc;gler</keyname><forenames>Sven D.</forenames></author><author><keyname>Ti&#x148;o</keyname><forenames>Peter</forenames></author><author><keyname>Polsterer</keyname><forenames>Kai L.</forenames></author></authors><title>Model-Coupled Autoencoder for Time Series Visualisation</title><categories>astro-ph.IM cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for the visualisation of a set of time series that
combines an echo state network with an autoencoder. For each time series in the
dataset we train an echo state network, using a common and fixed reservoir of
hidden neurons, and use the optimised readout weights as the new
representation. Dimensionality reduction is then performed via an autoencoder
on the readout weight representations. The crux of the work is to equip the
autoencoder with a loss function that correctly interprets the reconstructed
readout weights by associating them with a reconstruction error measured in the
data space of sequences. This essentially amounts to measuring the predictive
performance that the reconstructed readout weights exhibit on their
corresponding sequences when plugged back into the echo state network with the
same fixed reservoir. We demonstrate that the proposed visualisation framework
can deal both with real valued sequences as well as binary sequences. We derive
magnification factors in order to analyse distance preservations and
distortions in the visualisation space. The versatility and advantages of the
proposed method are demonstrated on datasets of time series that originate from
diverse domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05656</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05656</id><created>2016-01-21</created><authors><author><keyname>Kuznets</keyname><forenames>Roman</forenames></author></authors><title>Proving Craig and Lyndon Interpolation Using Labelled Sequent Calculi</title><categories>cs.LO math.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently presented a general method of proving the fundamental
logical properties of Craig and Lyndon Interpolation (IPs) by induction on
derivations in a wide class of internal sequent calculi, including sequents,
hypersequents, and nested sequents. Here we adapt the method to a more general
external formalism of labelled sequents and provide sufficient criteria on the
Kripke-frame characterization of a logic that guarantee the IPs. In particular,
we show that classes of frames definable by quantifier-free Horn formulas
correspond to logics with the IPs. These criteria capture the modal cube and
the infinite family of transitive Geach logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05661</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05661</id><created>2016-01-21</created><updated>2016-02-01</updated><authors><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author><author><keyname>Li</keyname><forenames>Weiping</forenames></author></authors><title>Distortion Bounds for Source Broadcast over Degraded Channel</title><categories>cs.IT math.IT</categories><comments>42 pages, 5 figures, Some new outer bounds and some new references
  are added in the updated version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint source-channel coding problem of sending a memoryless source over a
memoryless degraded broadcast channel is considered here. We derive an inner
bound and two outer bounds on the achievable distortion region. Moreover, when
specialized to Gaussian source broadcast or binary source broadcast, the inner
bound and outer bounds could recover the best known inner bound and outer
bound. Besides, we also extend the inner bound and outer bounds to Wyner-Ziv
source broadcast problem, i.e., source broadcast with degraded side information
available at decoders. We obtain some new bounds when specialized to Wyner-Ziv
Gaussian source broadcast and Wyner-Ziv binary source broadcast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05675</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05675</id><created>2016-01-21</created><authors><author><keyname>Calandriello</keyname><forenames>Daniele</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Valko</keyname><forenames>Michal</forenames></author><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author></authors><title>Incremental Spectral Sparsification for Large-Scale Graph-Based
  Semi-Supervised Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the harmonic function solution performs well in many semi-supervised
learning (SSL) tasks, it is known to scale poorly with the number of samples.
Recent successful and scalable methods, such as the eigenfunction method focus
on efficiently approximating the whole spectrum of the graph Laplacian
constructed from the data. This is in contrast to various subsampling and
quantization methods proposed in the past, which may fail in preserving the
graph spectra. However, the impact of the approximation of the spectrum on the
final generalization error is either unknown, or requires strong assumptions on
the data. In this paper, we introduce Sparse-HFS, an efficient
edge-sparsification algorithm for SSL. By constructing an edge-sparse and
spectrally similar graph, we are able to leverage the approximation guarantees
of spectral sparsification methods to bound the generalization error of
Sparse-HFS. As a result, we obtain a theoretically-grounded approximation
scheme for graph-based SSL that also empirically matches the performance of
known large-scale methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05677</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05677</id><created>2016-01-21</created><authors><author><keyname>Li</keyname><forenames>Yonglong</forenames></author><author><keyname>Kavcic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Han</keyname><forenames>Guangyue</forenames></author></authors><title>On the Capacity of Multilevel NAND Flash Memory Channels</title><categories>cs.IT math.IT</categories><comments>Shorter version submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we initiate a first information-theoretic study on multilevel
NAND flash memory channels with intercell interference. More specifically, for
a multilevel NAND flash memory channel under mild assumptions, we first prove
that such a channel is indecomposable and it features asymptotic equipartition
property; we then further prove that stationary processes achieve its
information capacity, and consequently, as its order tends to infinity, its
Markov capacity converges to its information capacity; eventually, we establish
that its operational capacity is equal to its information capacity. Our results
suggest that it is highly plausible to apply the ideas and techniques in the
computation of the capacity of finite-state channels, which are relatively
better explored, to that of the capacity of multilevel NAND flash memory
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05683</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05683</id><created>2016-01-21</created><authors><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Gra&#xe7;a</keyname><forenames>Daniel</forenames></author><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author></authors><title>Computing with Polynomial Ordinary Differential Equations</title><categories>cs.CC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1601.05360</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1941, Claude Shannon introduced the General Purpose Analog Computer(GPAC)
as a mathematical model of Differential Analysers, that is to say as a model of
continuous-time analog (mechanical, and later one electronic) machines of that
time.
  Following Shannon's arguments, functions generated by GPACs must be
differentially algebraic. As it is known that some computable functions like
Euler's $\Gamma(x)=\int_{0}^{\infty}t^{x-1}e^{-t}dt$ or Riemann's Zeta function
$\zeta(x)=\sum_{k=0}^\infty \frac1{k^x}$ are not differentially algebraic, this
argument has been often used to demonstrate in the past that the GPAC is less
powerful than digital computation.
  It was proved in JOC2007, that if a more modern notion of computation is
considered, i.e. in particular if computability is not restricted to real-time
generation of functions, the GPAC is actually equivalent to Turing machines.
  Our purpose is first to discuss the robustness of the notion of computation
involved in JOC2007, by establishing that natural variants of the notion of
computation from this paper leads to the same computability result.
  Second, to go considerations about (time) complexity, we explore several
natural variants for measuring time/space complexity of a computation.
  Rather surprisingly, whereas defining a robust time complexity for general
continuous time systems is a well known open problem, we prove that all
variants are actually equivalent even at the complexity level. As a
consequence, it seems that a robust and well defined notion of time complexity
exists for the GPAC, or equivalently for computations by polynomial ordinary
differential equations.
  Another side effect of our proof is also that we show in some way that
polynomial ordinary differential equations can be used as a kind of programming
model, and that there is a rather nice and robust notion of ODE programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05690</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05690</id><created>2016-01-21</created><authors><author><keyname>Wang</keyname><forenames>Chien-Yi</forenames></author><author><keyname>Lim</keyname><forenames>Sung Hoon</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>A New Converse Bound for Coded Caching</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, to be presented at ITA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An information-theoretic lower bound is developed for the caching system
studied by Maddah-Ali and Niesen. By comparing the proposed lower bound with
the decentralized coded caching scheme of Maddah-Ali and Niesen, the optimal
memory--rate tradeoff is characterized to within a multiplicative gap of $4.7$
for the worst case, improving the previous analytical gap of $12$. Furthermore,
for the case when users' requests follow the uniform distribution, the
multiplicative gap is tightened to $4.7$, improving the previous analytical gap
of $72$. As an independent result of interest, for the single-user average case
in which the user requests multiple files, it is proved that caching the most
requested files is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05695</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05695</id><created>2016-01-21</created><authors><author><keyname>Arakelyan</keyname><forenames>Erik</forenames></author><author><keyname>Serobyan</keyname><forenames>Aram</forenames></author><author><keyname>Jilavyan</keyname><forenames>Narek</forenames></author></authors><title>Fluid Dynamics Modeling : The Numerical Solution Of 2D Navier Hyperbolic
  Equations</title><categories>cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the following paper we will consider Navier-Stokes problem and it's
interpretation by hyperbolic waves, focusing on wave propagation. We will begin
with solution for linear waves, then present problem for non-linear waves.
Later we will derive for numerical solution using PDE's. Also we will design a
Matlab program to solve and simulate wave propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05706</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05706</id><created>2016-01-21</created><authors><author><keyname>Akitaya</keyname><forenames>Hugo A.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Hesterberg</keyname><forenames>Adam</forenames></author><author><keyname>Hurtado</keyname><forenames>Ferran</forenames></author><author><keyname>Ku</keyname><forenames>Jason S.</forenames></author><author><keyname>Lynch</keyname><forenames>Jayson</forenames></author></authors><title>Pachinko</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the Japanese game Pachinko, we study simple (perfectly
&quot;inelastic&quot; collisions) dynamics of a unit ball falling amidst point obstacles
(pins) in the plane. A classic example is that a checkerboard grid of pins
produces the binomial distribution, but what probability distributions result
from different pin placements? In the 50-50 model, where the pins form a subset
of this grid, not all probability distributions are possible, but surprisingly
the uniform distribution is possible for $\{1,2,4,8,16\}$ possible drop
locations. Furthermore, every probability distribution can be approximated
arbitrarily closely, and every dyadic probability distribution can be divided
by a suitable power of $2$ and then constructed exactly (along with extra
&quot;junk&quot; outputs). In a more general model, if a ball hits a pin off center, it
falls left or right accordingly. Then we prove a universality result: any
distribution of $n$ dyadic probabilities, each specified by $k$ bits, can be
constructed using $O(n k^2)$ pins, which is close to the information-theoretic
lower bound of $\Omega(n k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05725</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05725</id><created>2016-01-21</created><authors><author><keyname>Booth</keyname><forenames>Joshua Dennis</forenames></author><author><keyname>Rajamanickam</keyname><forenames>Sivasankaran</forenames></author><author><keyname>Thornquist</keyname><forenames>Heidi K.</forenames></author></authors><title>Basker: A Threaded Sparse LU Factorization Utilizing Hierarchical
  Parallelism and Data Layouts</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable sparse LU factorization is critical for efficient numerical
simulation of circuits and electrical power grids. In this work, we present a
new scalable sparse direct solver called Basker. Basker introduces a new
algorithm to parallelize the Gilbert-Peierls algorithm for sparse LU
factorization. As architectures evolve, there exists a need for algorithms that
are hierarchical in nature to match the hierarchy in thread teams, individual
threads, and vector level parallelism. Basker is designed to map well to this
hierarchy in architectures. There is also a need for data layouts to match
multiple levels of hierarchy in memory. Basker uses a two-dimensional
hierarchical structure of sparse matrices that maps to the hierarchy in the
memory architectures and to the hierarchy in parallelism. We present
performance evaluations of Basker on the Intel SandyBridge and Xeon Phi
platforms using circuit and power grid matrices taken from the University of
Florida sparse matrix collection and from Xyce circuit simulations. Basker
achieves a geometric mean speedup of 5.91x on CPU (16 cores) and 7.4x on Xeon
Phi (32 cores) relative to KLU. Basker outperforms Intel MKL Pardiso (PMKL) by
as much as 53x on CPU (16 cores) and 13.3x on Xeon Phi (32 cores) for low
fill-in circuit matrices. Furthermore, Basker provides 5.4x speedup on a
challenging matrix sequence taken from an actual Xyce simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05738</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05738</id><created>2016-01-21</created><authors><author><keyname>Sobhy</keyname><forenames>Dalia</forenames></author><author><keyname>Bahsoon</keyname><forenames>Rami</forenames></author></authors><title>Diversifying Software Architecture for Sustainability: A Value-based
  Perspective</title><categories>cs.SE</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the concept of software diversity has been thoroughly adopted by
software architects for many years, yet the advent of using diversity to
achieve sustainability is overlooked. We argue that option thinking is an
effective decision making tool to evaluate the trade-offs between architectural
strategies and their long-term values under uncertainty. Our method extends
cost-benefit analysis method CBAM. Unlike CBAM, our focus is on valuing the
options which diversification can embed in the architecture and their
corresponding value using real options pricing theory. The intuitive assumption
is that the value of these options can provide the architect with insights on
the long-term performance of these decisions in relation to some scenarios of
interest and use them as the basis for reasoning about sustainability. The
method aims to answer the following: (1) Is diversification of architectural
decisions beneficial and when they can help in sustaining the software, (2)
When, where and to what extent. The proposed model is illustrated and evaluated
using a case study from the literature referred to as GridStix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05744</identifier>
 <datestamp>2016-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05744</id><created>2016-01-20</created><authors><author><keyname>Weed</keyname><forenames>Jared</forenames></author></authors><title>Sub-Optimal Multi-Phase Path Planning: A Method for Solving Rubik's
  Revenge</title><categories>math.HO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rubik's Revenge, a 4x4x4 variant of the Rubik's puzzles, remains to date as
an unsolved puzzle. That is to say, we do not have a method or successful
categorization to optimally solve every one of its approximately $7.401 \times
10^{45}$ possible configurations. Rubik's Cube, Rubik's Revenge's predecessor
(3x3x3), with its approximately $4.33 \times 10^{19}$ possible configurations,
has only recently been completely solved by Rokicki et. al, further finding
that any configuration requires no more than 20 moves. With the sheer dimension
of Rubik's Revenge and its total configuration space, a brute-force method of
finding all optimal solutions would be in vain. Similar to the methods used by
Rokicki et. al on Rubik's Cube, in this paper we develop a method for solving
arbitrary configurations of Rubik's Revenge in phases, using a combination of a
powerful algorithm known as IDA* and a useful definition of distance in the
cube space. While time-series results were not successfully gathered, it will
be shown that this method far outweighs current human-solving methods and can
be used to determine loose upper bounds for the cube space. Discussion will
suggest that this method can also be applied to other puzzles with the proper
transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05747</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05747</id><created>2016-01-21</created><authors><author><keyname>Ku</keyname><forenames>Jason S.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author></authors><title>Folding Flat Crease Patterns with Thick Materials</title><categories>cs.CG</categories><doi>10.1115/1.4031954</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling folding surfaces with nonzero thickness is of practical interest for
mechanical engineering. There are many existing approaches that account for
material thickness in folding applications. We propose a new systematic and
broadly applicable algorithm to transform certain flat-foldable crease patterns
into new crease patterns with similar folded structure but with a
facet-separated folded state. We provide conditions on input crease patterns
for the algorithm to produce a thickened crease pattern avoiding local self
intersection, and provide bounds for the maximum thickness that the algorithm
can produce for a given input. We demonstrate these results in parameterized
numerical simulations and physical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05748</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05748</id><created>2016-01-21</created><authors><author><keyname>Wu</keyname><forenames>Wentao</forenames></author><author><keyname>Naughton</keyname><forenames>Jeffrey F.</forenames></author><author><keyname>Singh</keyname><forenames>Harneet</forenames></author></authors><title>Sampling-Based Query Re-Optimization</title><categories>cs.DB</categories><comments>This is the extended version of a paper with the same title and
  authors that appears in the Proceedings of the ACM SIGMOD International
  Conference on Management of Data (SIGMOD 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite of decades of work, query optimizers still make mistakes on
&quot;difficult&quot; queries because of bad cardinality estimates, often due to the
interaction of multiple predicates and correlations in the data. In this paper,
we propose a low-cost post-processing step that can take a plan produced by the
optimizer, detect when it is likely to have made such a mistake, and take steps
to fix it. Specifically, our solution is a sampling-based iterative procedure
that requires almost no changes to the original query optimizer or query
evaluation mechanism of the system. We show that this indeed imposes low
overhead and catches cases where three widely used optimizers (PostgreSQL and
two commercial systems) make large errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05754</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05754</id><created>2016-01-21</created><authors><author><keyname>Lima</keyname><forenames>Gerson F. M</forenames></author><author><keyname>Lamounier</keyname><forenames>Edgard</forenames></author><author><keyname>Barcelos</keyname><forenames>Sergio</forenames></author><author><keyname>Cardoso</keyname><forenames>Alexandre</forenames></author><author><keyname>Peretta</keyname><forenames>Igor</forenames></author><author><keyname>Muramoto</keyname><forenames>Willian</forenames></author><author><keyname>Barbara</keyname><forenames>Flavio</forenames></author></authors><title>Applying a Differential Evolutionary Algorithm to a Constraint-based
  System to support Separation of OTDR Superimposed Signal after Passive
  Optical Network Splitters</title><categories>cs.CE</categories><comments>16 pages, 17 figures, unpublished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FTTH (Fiber To The Home) market currently needs new network maintenance
technologies that can, economically and effectively, cope with massive fiber
plants. However, operating these networks requires adequate means for an
effective monitoring cost. Especially for troubleshooting faults that are
associated with the possibility of remote identification of fiber breaks, which
may exist in the network. Optical Time Domain Reflectometry (OTDR) techniques
are widely used in point-to-point optical network topologies. Nevertheless, it
has major limitations in tree-structured PONs (Passive Optical Networks), where
all different branches backscatter the light in just one conventional OTDR
trace with combined signals arriving on the OLT (Optical Line Terminal) side.
Furthermore, passive power splitters used in FTTH networks input large
attenuation, impoverishing the reflected signal. This makes the identification
of the very branch affected by the problem practically impossible, when
considering conventional analyses. The use of constraint-based techniques have
been applied in a large amount of applications for Engineering Design, where
the duties imposed for graphics and equations constraints result in valued
features to CAD/CAE software capabilities. Currently, it provides a faster
decision making capacity for engineers. This work applies the constraint based
approach along with a Differential Evolutionary Algorithm to separate the
superimposed OTDR signals, after the splitters of a FTTH Passive Optical
Networks. This research introduces a new set of algorithms performing a
coupling to an Optical Network (ON) CAD Design with its correspondent OTDR
measurement signal, considering its geographical distribution branches of
different lengths after the splitter. Results of this work are presented in a
FTTN (Fiber To The Node) prototype arrangement, using a 1:8 passive power
splitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05758</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05758</id><created>2016-01-21</created><updated>2016-01-29</updated><authors><author><keyname>Jetpipattanapong</keyname><forenames>Duangpen</forenames></author><author><keyname>Srijuntongsiri</keyname><forenames>Gun</forenames></author></authors><title>A New Pivot Selection Algorithm for Symmetric Indefinite Factorization
  Arising in Quadratic Programming with Block Constraint Matrices</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadratic programmingis a class of constrained optimization problem with
quadratic objective functions and linear constraints. It has applications in
many areas and is also used to solve nonlinear optimization problems. This
article focuses on the equality constrained quadratic programs whose constraint
matrices are block diagonal. Using the direct solution method, we propose a new
pivot selection algorithm for the factorization of the Karush-Kuhn-Tucker(KKT)
matrix for this problem that maintains the sparsity and stability of the
problem. Our experiments show that our pivot selection algorithm appears to
produce no fill-ins in the factorizationof such matrices. In addition, we
compare our method with MA57 and find that the factors produced by our
algorithm are sparser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05764</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05764</id><created>2016-01-21</created><authors><author><keyname>Fish</keyname><forenames>Benjamin</forenames></author><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author><author><keyname>Lelkes</keyname><forenames>&#xc1;d&#xe1;m D.</forenames></author></authors><title>A Confidence-Based Approach for Balancing Fairness and Accuracy</title><categories>cs.LG cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study three classical machine learning algorithms in the context of
algorithmic fairness: adaptive boosting, support vector machines, and logistic
regression. Our goal is to maintain the high accuracy of these learning
algorithms while reducing the degree to which they discriminate against
individuals because of their membership in a protected group.
  Our first contribution is a method for achieving fairness by shifting the
decision boundary for the protected group. The method is based on the theory of
margins for boosting. Our method performs comparably to or outperforms previous
algorithms in the fairness literature in terms of accuracy and low
discrimination, while simultaneously allowing for a fast and transparent
quantification of the trade-off between bias and error.
  Our second contribution addresses the shortcomings of the bias-error
trade-off studied in most of the algorithmic fairness literature. We
demonstrate that even hopelessly naive modifications of a biased algorithm,
which cannot be reasonably said to be fair, can still achieve low bias and high
accuracy. To help to distinguish between these naive algorithms and more
sensible algorithms we propose a new measure of fairness, called resilience to
random bias (RRB). We demonstrate that RRB distinguishes well between our naive
and sensible fairness algorithms. RRB together with bias and accuracy provides
a more complete picture of the fairness of an algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05767</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05767</id><created>2016-01-21</created><authors><author><keyname>Chakrabarti</keyname><forenames>Subit</forenames></author><author><keyname>Judge</keyname><forenames>Jasmeet</forenames></author><author><keyname>Bongiovanni</keyname><forenames>Tara</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author><author><keyname>Ranka</keyname><forenames>Sanjay</forenames></author></authors><title>Spatial Scaling of Satellite Soil Moisture using Temporal Correlations
  and Ensemble Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel algorithm is developed to downscale soil moisture (SM), obtained at
satellite scales of 10-40 km by utilizing its temporal correlations to
historical auxiliary data at finer scales. Including such correlations
drastically reduces the size of the training set needed, accounts for
time-lagged relationships, and enables downscaling even in the presence of
short gaps in the auxiliary data. The algorithm is based upon bagged regression
trees (BRT) and uses correlations between high-resolution remote sensing
products and SM observations. The algorithm trains multiple regression trees
and automatically chooses the trees that generate the best downscaled
estimates. The algorithm was evaluated using a multi-scale synthetic dataset in
north central Florida for two years, including two growing seasons of corn and
one growing season of cotton per year. The time-averaged error across the
region was found to be 0.01 $\mathrm{m}^3/\mathrm{m}^3$, with a standard
deviation of 0.012 $\mathrm{m}^3/\mathrm{m}^3$ when 0.02% of the data were used
for training in addition to temporal correlations from the past seven days, and
all available data from the past year. The maximum spatially averaged errors
obtained using this algorithm in downscaled SM were 0.005
$\mathrm{m}^3/\mathrm{m}^3$, for pixels with cotton land-cover. When land
surface temperature~(LST) on the day of downscaling was not included in the
algorithm to simulate &quot;data gaps&quot;, the spatially averaged error increased
minimally by 0.015 $\mathrm{m}^3/\mathrm{m}^3$ when LST is unavailable on the
day of downscaling. The results indicate that the BRT-based algorithm provides
high accuracy for downscaling SM using complex non-linear spatio-temporal
correlations, under heterogeneous micro meteorological conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05768</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05768</id><created>2016-01-21</created><authors><author><keyname>Christen</keyname><forenames>Daniel</forenames></author></authors><title>Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky
issues of syntactic ambiguity reduction and word sense disambiguation as well
as providing innovative and original solutions for constituent generation and
constraints management. To provide an insight into how it operates, the
system's general architecture and components, as well as its lexical, syntactic
and semantic resources are described. After that, the paper addresses the
mechanism that performs selective parsing through an interaction between
syntactic and semantic information, leading the parser to a coherent and
accurate interpretation of the input text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05769</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05769</id><created>2016-01-21</created><authors><author><keyname>Noorzad</keyname><forenames>Parham</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>Can Negligible Cooperation Increase Network Reliability? (Extended
  Version)</title><categories>cs.IT math.IT</categories><comments>14 pages, 2 figures. To be submitted to ISIT '16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network cooperation strategies, nodes work together with the aim of
increasing transmission rates or reliability. For example, cooperation can be
employed to obtain a code with small maximal-error from a code with small
average-error that does not make use of cooperation. In networks where rates
achievable under a maximal-error constraint differ from those achievable under
an average-error constraint, such a benefit can be potentially viewed as both
increasing reliability and increasing rate. Let us define the cooperation rate
as the number of bits per channel use shared with each node as part of the
cooperation strategy. We here demonstrate that even a negligible cooperation
rate can sometimes yield a non-negligible benefit. Precisely, we employ Dueck's
example of a multiple access channel whose maximal and average-error
sum-capacities differ, to show that there exists a network whose maximal-error
sum-capacity is not continuous with respect to its edge capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05775</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05775</id><created>2016-01-21</created><authors><author><keyname>van Laarhoven</keyname><forenames>Twan</forenames></author><author><keyname>Marchiori</keyname><forenames>Elena</forenames></author></authors><title>Local community detection by seed expansion: from conductance to
  weighted kernel 1-mean optimization</title><categories>cs.SI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In local community detection by seed expansion a single cluster concentrated
around few given query nodes in a graph is discovered in a localized way.
Conductance is a popular objective function used in many algorithms for local
community detection. Algorithms that directly optimize conductance usually add
or remove one node at a time to find a local optimum. This amounts to fix a
specific neighborhood structure over clusters. A natural way to avoid the
problem of choosing a specific neighborhood structure is to use a continuous
relaxation of conductance. This paper studies such a continuous relaxation of
conductance. We show that in this setting continuous optimization leads to hard
clusters. We investigate the relation of conductance with weighted kernel
k-means for a single cluster, which leads to the introduction of a weighted
kernel 1-mean objective function, called \sigma-conductance, where {\sigma} is
a parameter which influences the size of the community. Conductance is obtained
by setting {\sigma} to 0. Two algorithms for local optimization of
\sigma-conductance based on the expectation maximization and the projected
gradient descend method are developed, called EMc and PGDc, respectively. We
show that for \sigma=0 EMc corresponds to gradient descend with an infinite
step size at each iteration. We design a procedure to automatically select a
value for {\sigma}. Performance guarantee for these algorithms is proven for a
class of dense communities centered around the seeds and well separated from
the rest of the network. On this class we also prove that our algorithms stay
localized. A comparative experimental analysis on networks with ground-truth
communities is performed using state-of-the-art algorithms based on the graph
diffusion method. Our experiments indicate that EMc and PGDc stay localized and
produce communities most similar to the ground.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05776</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05776</id><created>2016-01-21</created><updated>2016-01-26</updated><authors><author><keyname>Ezzeldin</keyname><forenames>Yahya H.</forenames></author><author><keyname>Sengupta</keyname><forenames>Ayan</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Wireless Network Simplification : Beyond Diamond Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an arbitrary layered Gaussian relay network with $L$ layers of
$N$ relays each, from which we select subnetworks with $K$ relays per layer. We
prove that: (i) For arbitrary $L, N$ and $K = 1$, there always exists a
subnetwork that approximately achieves $\frac{2}{(L-1)N + 4}$
$\left(\mbox{resp.}\frac{2}{LN+2}\right)$ of the network capacity for odd $L$
(resp. even $L$), (ii) For $L = 2, N = 3, K = 2$, there always exists a
subnetwork that approximately achieves $\frac{1}{2}$ of the network capacity.
We also provide example networks where even the best subnetworks achieve
exactly these fractions (up to additive gaps). Along the way, we derive some
results on MIMO antenna selection and capacity decomposition that may also be
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05784</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05784</id><created>2016-01-21</created><authors><author><keyname>Ezzeldin</keyname><forenames>Yahya H.</forenames></author><author><keyname>Sengupta</keyname><forenames>Ayan</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>A Note on Antenna Selection in Gaussian MIMO Channels: Capacity
  Guarantees and Bounds</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of selecting $k_t \times k_r$ antennas from a
Gaussian MIMO channel with $n_t \times n_r$ antennas, where $k_t \leq n_t$ and
$k_r \leq n_r$. We prove the following two results that hold universally, in
the sense that they do not depend on the channel coefficients: (i) The capacity
of the best $k_t \times k_r$ subchannel is always lower bounded by a fraction
$\frac{k_t k_r}{n_t n_r}$ of the full capacity (with $n_t \times n_r$
antennas). This bound is tight as the channel coefficients diminish in
magnitude. (ii) There always exists a selection of $k_t \times k_r$ antennas
(including the best) that achieves a fraction greater than $\frac{\min(k_t
,k_r)}{\min(n_t,n_r)}$ of the full capacity within an additive constant that is
independent of the coefficients in the channel matrix. The key mathematical
idea that allows us to derive these universal bounds is to directly relate the
determinants of principle sub-matrices of a Hermitian matrix to the determinant
of the entire matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05792</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05792</id><created>2016-01-20</created><authors><author><keyname>Pendleton</keyname><forenames>Marcus</forenames></author><author><keyname>Garcia-Lebron</keyname><forenames>Richard</forenames></author><author><keyname>Xu</keyname><forenames>Shouhuai</forenames></author></authors><title>A Survey on Security Metrics</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of security metrics can hardly be overstated. Despite the
attention that has been paid by the academia, government and industry in the
past decades, this important problem stubbornly remains open. In this survey,
we present a survey of knowledge on security metrics. The survey is centered on
a novel taxonomy, which classifies security metrics into four categories:
metrics for measuring the system vulnerabilities, metrics for measuring the
defenses, metrics for measuring the threats, and metrics for measuring the
situations. The insight underlying the taxonomy is that situations (or outcomes
of cyber attack-defense interactions) are caused by certain threats (or
attacks) against systems that have certain vulnerabilities (including human
factors) and employ certain defenses. In addition to systematically reviewing
the security metrics that have been proposed in the literature, we discuss the
gaps between the state of the art and the ultimate goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05793</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05793</id><created>2016-01-20</created><authors><author><keyname>Bhandari</keyname><forenames>Ayush</forenames></author><author><keyname>Zayed</keyname><forenames>Ahmed I.</forenames></author></authors><title>Shift-Invariant and Sampling Spaces Associated with the Special Affine
  Fourier Transform</title><categories>cs.IT math.IT</categories><comments>Working paper with 22 pages and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Special Affine Fourier Transformation or the SAFT generalizes a number of
well known unitary transformations as well as signal processing and optics
related mathematical operations. Shift-invariant spaces also play an important
role in sampling theory, multiresolution analysis, and many other areas of
signal and image processing. Shannon's sampling theorem, which is at the heart
of modern digital communications, is a special case of sampling in
shift-invariant spaces. Furthermore, it is well known that the Poisson
summation formula is equivalent to the sampling theorem and that the Zak
transform is closely connected to the sampling theorem and the Poisson
summation formula. These results have been known to hold in the Fourier
transform domain for decades and were recently shown to hold in the Fractional
Fourier transform domain by A. Bhandari and A. Zayed.
  The main goal of this article is to show that these results also hold true in
the SAFT domain. We provide a short, self-contained proof of Shannon's theorem
for functions bandlimited in the SAFT domain and then show that sampling in the
SAFT domain is equivalent to orthogonal projection of functions onto a subspace
of bandlimited basis associated with the SAFT domain. This interpretation of
sampling leads to least-squares optimal sampling theorem. Furthermore, we show
that this approximation procedure is linked with convolution and semi-discrete
convolution operators that are associated with the SAFT domain. We conclude the
article with an application of fractional delay filtering of SAFT bandlimited
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05824</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05824</id><created>2016-01-21</created><authors><author><keyname>Stamatopoulos</keyname><forenames>Michail I.</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Christos-Nikolaos</forenames></author></authors><title>3D digital reassembling of archaeological ceramic pottery fragments
  based on their thickness profile</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reassembly of a broken archaeological ceramic pottery is an open and
complex problem, which remains a scientific process of extreme interest for the
archaeological community. Usually, the solutions suggested by various research
groups and universities depend on various aspects such as the matching process
of the broken surfaces, the outline of sherds or their colors and geometric
characteris-tics, their axis of symmetry, the corners of their contour, the
theme portrayed on the surface, the concentric circular rills that are left
during the base construction in the inner pottery side by the fingers of the
potter artist etc. In this work the reassembly process is based on a different
and more secure idea, since it is based on the thick-ness profile, which is
appropriately identified in every fragment. Specifically, our approach is based
on information encapsulated in the inner part of the sherd (i.e. thickness),
which is not -or at least not heavily- affected by the presence of harsh
environmental conditions, but is safely kept within the sherd itself. Our
method is verified in various use case experiments, using cutting edge
technologies such as 3D representations and precise measurements on surfaces
from the acquired 3D models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05826</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05826</id><created>2016-01-21</created><authors><author><keyname>Bihan</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Dickenstein</keyname><forenames>Alicia</forenames></author></authors><title>Descartes' Rule of Signs for Polynomial Systems supported on Circuits</title><categories>math.AG cs.SC</categories><comments>22 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first multivariate version of Descartes' rule of signs to bound
the number of positive real roots of a system of polynomial equations in n
variables with n+2 monomials, in terms of the sign variation of a sequence
associated both to the exponent vectors and the given coefficients. We show
that our bound is sharp and is related to the signature of the circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05833</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05833</id><created>2016-01-21</created><authors><author><keyname>Roland</keyname><forenames>Michael</forenames></author></authors><title>Executing Arbitrary Code in the Context of the Smartcard System Service</title><categories>cs.CR</categories><comments>University of Applied Sciences Upper Austria, JR-Center u'smile,
  Vulnerability report, associated CVE identifier is CVE-2015-6606, 28 pages, 6
  figures</comments><acm-class>C.3; C.5.3; D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report summarizes our findings regarding a severe weakness in
implementations of the Open Mobile API deployed on several Android devices. The
vulnerability allows arbitrary code coming from a specially crafted Android
application package (APK) to be injected into and executed by the smartcard
system service component (the middleware component of the Open Mobile API
implementation). This can be exploited to gain elevated capabilities, such as
privileges protected by signature- and system-level permissions assigned to
this service. The affected source code seems to originate from the
SEEK-for-Android open-source project and was adopted by various vendor-specific
implementations of the Open Mobile API, including the one that is used on the
Nexus 6 (as of Android version 5.1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05834</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05834</id><created>2016-01-21</created><authors><author><keyname>Wai</keyname><forenames>Hoi-To</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>Active Sensing of Social Networks</title><categories>cs.SI</categories><comments>Submitted to IEEE Trans. on Signal and Information Processing over
  Networks</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper develops an active sensing method to estimate the relative weight
(or trust) agents place on their neighbors' information in a social network.
The model used for the regression is based on the steady state equation in the
linear DeGroot model under the influence of stubborn agents; i.e., agents whose
opinions are not influenced by their neighbors. This method can be viewed as a
\emph{social RADAR}, where the stubborn agents excite the system and the latter
can be estimated through the reverberation observed from the analysis of the
agents' opinions. We show that the social network sensing problem can be viewed
as a blind compressed sensing problem with a sparse measurement matrix. We
prove that the network structure will be revealed when a sufficient number of
stubborn agents independently influence a number of ordinary (non-stubborn)
agents. We investigate the scenario with a deterministic or randomized DeGroot
model and propose a consistent estimator for the steady states. Simulation
results on synthetic and real world networks support our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05837</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05837</id><created>2016-01-21</created><authors><author><keyname>Lenberg</keyname><forenames>Per</forenames></author><author><keyname>Wallgren</keyname><forenames>Lars G&#xf6;ran</forenames></author><author><keyname>Feldt</keyname><forenames>Robert</forenames></author></authors><title>Software Engineers' Attitudes Towards Organizational Change - an
  Industrial Case Study</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to cope with a complex and changing environment, industries seek to
find new and more efficient ways to conduct their business. According to
previous research, many of these change efforts fail to achieve their intended
aims. Researchers have therefore sought to identify factors that increase the
likelihood of success and found that employees' attitude towards change is one
of the most critical.
  The ability to manage change is especially important in software engineering
organizations, where rapid changes in influential technologies and constantly
evolving methodologies create a turbulent environment. Nevertheless, to the
best of our knowledge, no studies exist that explore attitude towards change in
a software engineering organization.
  In this case study, we have used industry data to examine if the knowledge
about the intended change outcome, the understanding of the need for change,
and the feelings of participation affect software engineers' openness to change
and readiness for change respectively, two commonly used attitude constructs.
The result of two separate multiple regression analysis showed that openness to
change is predicted by all three concepts, while readiness for change is
predicted by need for change and participation. In addition, our research also
provides a hierarchy with respect to the three predictive constructs' degree of
impact.
  Ultimately, our result can help managers in software engineering
organizations to increase the likelihood of successfully implementing change
initiatives that result in a changed organizational behavior. However, the
first-order models we propose are to be recognized as early approximations that
captures the most significant effects and should therefore, in future research,
be extended to include additional software engineering unique factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05839</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05839</id><created>2016-01-21</created><authors><author><keyname>Chen</keyname><forenames>Cheng</forenames></author><author><keyname>Berry</keyname><forenames>Randall A.</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author><author><keyname>Subramanian</keyname><forenames>Vijay G.</forenames></author></authors><title>The Impact of Unlicensed Access on Small-Cell Resource Allocation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cells deployed in licensed spectrum and unlicensed access via WiFi
provide different ways of expanding wireless services to low mobility users.
That reduces the demand for conventional macro-cellular networks, which are
better suited for wide-area mobile coverage. The mix of these technologies seen
in practice depends in part on the decisions made by wireless service providers
that seek to maximize revenue, and allocations of licensed and unlicensed
spectrum by regulators. To understand these interactions we present a model in
which a service provider allocates available licensed spectrum across two
separate bands, one for macro- and one for small-cells, in order to serve two
types of users: mobile and fixed. We assume a service model in which the
providers can charge a (different) price per unit rate for each type of service
(macro- or small-cell); unlicensed access is free. With this setup we study how
the addition of unlicensed spectrum affects prices and the optimal allocation
of bandwidth across macro-/small-cells. We also characterize the optimal
fraction of unlicensed spectrum when new bandwidth becomes available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05850</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05850</id><created>2016-01-21</created><authors><author><keyname>Lin</keyname><forenames>Bin</forenames></author><author><keyname>Qian</keyname><forenames>Dejun</forenames></author></authors><title>Regression Testing of Virtual Prototypes Using Symbolic Execution</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently virtual platforms and virtual prototyping techniques have been
widely applied for accelerating software development in electronics companies.
It has been proved that these techniques can greatly shorten time-to-market and
improve product quality. One challenge is how to test and validate a virtual
prototype. In this paper, we present how to conduct regression testing of
virtual prototypes in different versions using symbolic execution. Suppose we
have old and new versions of a virtual prototype, we first apply symbolic
execution to the new version and collect all path constraints. Then the
collected path constraints are used for guiding the symbolic execution of the
old version. For each path explored, we compare the device states between two
versions to check if they behave the same. We have applied this approach to a
widely-used virtual prototype and detected numerous differences. The
experimental results show that our approach is useful and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05851</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05851</id><created>2016-01-21</created><authors><author><keyname>di Pietro</keyname><forenames>Roberto</forenames></author><author><keyname>Franzoni</keyname><forenames>Federico</forenames></author><author><keyname>Lombardi</keyname><forenames>Flavio</forenames></author></authors><title>HyBIS: Windows Guest Protection through Advanced Memory Introspection</title><categories>cs.OS cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effectively protecting the Windows OS is a challenging task, since most
implementation details are not publicly known. Windows has always been the main
target of malwares that have exploited numerous bugs and vulnerabilities.
Recent trusted boot and additional integrity checks have rendered the Windows
OS less vulnerable to kernel-level rootkits. Nevertheless, guest Windows
Virtual Machines are becoming an increasingly interesting attack target. In
this work we introduce and analyze a novel Hypervisor-Based Introspection
System (HyBIS) we developed for protecting Windows OSes from malware and
rootkits. The HyBIS architecture is motivated and detailed, while targeted
experimental results show its effectiveness. Comparison with related work
highlights main HyBIS advantages such as: effective semantic introspection,
support for 64-bit architectures and for latest Windows (8.x and 10), advanced
malware disabling capabilities. We believe the research effort reported here
will pave the way to further advances in the security of Windows OSes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05856</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05856</id><created>2016-01-21</created><authors><author><keyname>Yuan</keyname><forenames>Zhengdao</forenames></author><author><keyname>Zhang</keyname><forenames>Chuanzong</forenames></author><author><keyname>Wang</keyname><forenames>Zhongyong</forenames></author><author><keyname>Guo</keyname><forenames>Qinghua</forenames></author><author><keyname>Wu</keyname><forenames>Sheng</forenames></author><author><keyname>Wang</keyname><forenames>Xingye</forenames></author></authors><title>A Low-Complexity Receiver Using Combined BP-MF for Joint Channel
  Estimation and Decoding in OFDM Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed an iterative message passing receiver, which jointly performs
channel state information and noise precision estimation and decoding, for OFDM
systems. A message passing framework, which combines belief propagation (BP)
and mean field (MF), is exploited on a stretched factor graph, where MF message
update rule is used for observation factor nodes, so that noise precision (the
reciprocal of AWGN variable) can be estimated. We introduce low complexity
generalized approximate message passing (GAMP) method into the combined BP-MF
framework to carry out channel estimation in time-frequency domain. Our
numerical assessment demonstrates that the proposed receiver yields the same
performance with the state of art BP-MF based algorithm while have much lower
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05861</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05861</id><created>2016-01-21</created><authors><author><keyname>Bakry</keyname><forenames>Amr</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Manifold-Kernels Comparison in MKPLS for Visual Speech Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech recognition is a challenging problem. Due to the acoustic limitations,
using visual information is essential for improving the recognition accuracy in
real-life unconstraint situations. One common approach is to model the visual
recognition as nonlinear optimization problem. Measuring the distances between
visual units is essential for solving this problem. Embedding the visual units
on a manifold and using manifold kernels is one way to measure these distances.
This work is intended to evaluate the performance of several manifold kernels
for solving the problem of visual speech recognition. We show the theory behind
each kernel. We apply manifold kernel partial least squares framework to OuluVs
and AvLetters databases, and show empirical comparison between all kernels.
This framework provides convenient way to explore different kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05868</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05868</id><created>2016-01-21</created><authors><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>A Lattice Coding Scheme for Secret Key Generation from Gaussian Markov
  Tree Sources</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures. A 5-page version of this article has been
  submitted to the 2016 IEEE International Symposium on Information Theory
  (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study the problem of secret key generation in the
multiterminal source model, where the terminals have access to correlated
Gaussian sources. We assume that the sources form a Markov chain on a tree. We
give a nested lattice-based key generation scheme whose computational
complexity is polynomial in the number, N , of independent and identically
distributed samples observed by each source. We also compute the achievable
secret key rate and give a class of examples where our scheme is optimal in the
fine quantization limit. However, we also give examples that show that our
scheme is not always optimal in the limit of fine quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05871</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05871</id><created>2016-01-21</created><authors><author><keyname>Kim</keyname><forenames>Kyungjoo</forenames></author><author><keyname>Rajamanickam</keyname><forenames>Sivasankaran</forenames></author><author><keyname>Stelle</keyname><forenames>George</forenames></author><author><keyname>Edwards</keyname><forenames>H. Carter</forenames></author><author><keyname>Olivier</keyname><forenames>Stephen L.</forenames></author></authors><title>Task Parallel Incomplete Cholesky Factorization using 2D
  Partitioned-Block Layout</title><categories>cs.MS</categories><comments>25 pages</comments><report-no>SAND2016-0637 R</report-no><msc-class>68W10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a task-parallel algorithm for sparse incomplete Cholesky
factorization that utilizes a 2D sparse partitioned-block layout of a matrix.
Our factorization algorithm follows the idea of algorithms-by-blocks by using
the block layout. The algorithm-by-blocks approach induces a task graph for the
factorization. These tasks are inter-related to each other through their data
dependences in the factorization algorithm. To process the tasks on various
manycore architectures in a portable manner, we also present a portable tasking
API that incorporates different tasking backends and device-specific features
using an open-source framework for manycore platforms i.e., Kokkos. A
performance evaluation is presented on both Intel Sandybridge and Xeon Phi
platforms for matrices from the University of Florida sparse matrix collection
to illustrate merits of the proposed task-based factorization. Experimental
results demonstrate that our task-parallel implementation delivers about 26.6x
speedup (geometric mean) over single-threaded incomplete Cholesky-by-blocks and
19.2x speedup over serial Cholesky performance which does not carry tasking
overhead using 56 threads on the Intel Xeon Phi processor for sparse matrices
arising from various application problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05873</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05873</id><created>2016-01-21</created><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author></authors><title>Asymptotic Optimality of Massive MIMO Systems Using Densely Spaced
  Transmit Antennas</title><categories>cs.IT math.IT</categories><comments>submitted to ISIT 2016. arXiv admin note: text overlap with
  arXiv:1601.05638</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the performance of a massive multiple-input
multiple-output (MIMO) system that uses a large transmit antenna array with
antenna elements spaced densely. Under the assumption of idealized uniform
linear antenna arrays without mutual coupling, precoded quadrature phase-shift
keying (QPSK) transmission is proved to achieve the channel capacity of the
massive MIMO system when the transmit antenna separation tends to zero. This
asymptotic optimality is analogous to that of QPSK faster-than-Nyquist
signaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05875</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05875</id><created>2016-01-21</created><authors><author><keyname>Li</keyname><forenames>Cheuk Ting</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Distributed Simulation of Continuous Random Variables</title><categories>cs.IT math.IT</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the first known upper bound on the exact and Wyner's common
information of $n$ continuous random variables in terms of the dual total
correlation between them (which is a generalization of mutual information). In
particular, we show that when the pdf of the random variables is log-concave,
there is a constant gap of $n^{2}\log e+9n\log n$ between this upper bound and
the dual total correlation lower bound that does not depend on the
distribution. The upper bound is obtained using a computationally efficient
dyadic decomposition scheme for constructing a discrete common randomness
variable $W$ from which the $n$ random variables can be simulated in a
distributed manner. We then bound the entropy of $W$ using a new measure, which
we refer to as the erosion entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05879</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05879</id><created>2016-01-22</created><authors><author><keyname>Muramatsu</keyname><forenames>Jun</forenames></author></authors><title>Construction of a Channel Code from an Arbitrary Source Code with
  Decoder Side Information</title><categories>cs.IT math.IT</categories><comments>9 pages. A short version is submitted to ISIT2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of a channel code by using a source code with decoder side
information is introduced. For the construction, any pair of encoder and
decoder is available for a source code with decoder side information. A
constrained-random-number generator, which generates random numbers satisfying
a condition specified by a function and its value, is used to construct a
stochastic channel encoder. The result suggests that we can divide the channel
coding problem into the problems of channel encoding and source decoding with
side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05880</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05880</id><created>2016-01-22</created><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Collins</keyname><forenames>Austin</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Beta-Beta Achievability Bound with Applications</title><categories>cs.IT math.IT</categories><comments>extended version of a paper submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A channel coding achievability bound expressed in terms of the ratio between
two Neyman-Pearson $\beta$ functions is proposed. This bound is the dual of a
converse bound established earlier by Polyanskiy and Verd\'{u} (2014). The new
bound turns out to simplify considerably the analysis in situations where the
channel output distribution is not a product distribution, for example due to a
cost constraint or a structural constraint (such as orthogonality or constant
composition) on the channel inputs. Connections to existing bounds in the
literature are discussed. The bound is then used to derive 1) an achievability
bound on the channel dispersion of additive non-Gaussian noise channels with
random Gaussian codebooks, 2) the channel dispersion of the exponential-noise
channel, 3) a second-order expansion for the minimum energy per bit of an AWGN
channel, and 4) a lower bound on the maximum coding rate of a multiple-input
multiple-output Rayleigh-fading channel with perfect channel state information
at the receiver, which is the tightest known achievability result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05889</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05889</id><created>2016-01-22</created><authors><author><keyname>Zuo</keyname><forenames>Gong</forenames></author><author><keyname>Wong</keyname><forenames>Li</forenames></author></authors><title>A Review on Recent Active Vibration Control Techniques</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active vibration control has been introduced and used as one of the effective
approaches to suppress unwanted vibrations in different systems. Effective
performance of each vibration control method is contingent to accurate design
and proper dynamics selection of the control unit. These methods have been
extensively studied in various studies in recent years. Each of these new
methods are designed by a specific dynamics for a specific system. Here in this
paper, we aim to introduce some of these recent approaches in a brief
discussion, and familiarize the readers with these techniques. Engineers who
wish to design proper vibration controllers in different scales, from micro- to
macro applications, will certainly design a more successful vibration
controller if they know better about similar techniques, and they can implement
the novelties that other scholars have utilized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05893</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05893</id><created>2016-01-22</created><authors><author><keyname>Brunsting</keyname><forenames>Shawn</forenames></author><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author><author><keyname>Dolman</keyname><forenames>Remco</forenames></author><author><keyname>van Sprundel</keyname><forenames>Teun</forenames></author></authors><title>GeoTextTagger: High-Precision Location Tagging of Textual Documents
  using a Natural Language Processing Approach</title><categories>cs.AI cs.CL cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location tagging, also known as geotagging or geolocation, is the process of
assigning geographical coordinates to input data. In this paper we present an
algorithm for location tagging of textual documents. Our approach makes use of
previous work in natural language processing by using a state-of-the-art
part-of-speech tagger and named entity recognizer to find blocks of text which
may refer to locations. A knowledge base (OpenStreatMap) is then used to find a
list of possible locations for each block. Finally, one location is chosen for
each block by assigning distance-based scores to each location and repeatedly
selecting the location and block with the best score. We tested our geolocation
algorithm with Wikipedia articles about topics with a well-defined geographical
location that are geotagged by the articles' authors, where classification
approaches have achieved median errors as low as 11 km, with attainable
accuracy limited by the class size. Our approach achieved a 10th percentile
error of 490 metres and median error of 54 kilometres on the Wikipedia dataset
we used. When considering the five location tags with the greatest scores, 50%
of articles were assigned at least one tag within 8.5 kilometres of the
article's author-assigned true location. We also tested our approach on Twitter
messages that are tagged with the location from which the message was sent.
Twitter texts are challenging because they are short and unstructured and often
do not contain words referring to the location they were sent from, but we
obtain potentially useful results. We explain how we use the Spark framework
for data analytics to collect and process our test data. In general,
classification-based approaches for location tagging may be reaching their
upper accuracy limit, but our precision-focused approach has high accuracy for
some texts and shows significant potential for improvement overall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05900</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05900</id><created>2016-01-22</created><authors><author><keyname>Ackerman</keyname><forenames>Margareta</forenames></author><author><keyname>Moore</keyname><forenames>Jarrod</forenames></author></authors><title>When is Clustering Perturbation Robust?</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is a fundamental data mining tool that aims to divide data into
groups of similar items. Generally, intuition about clustering reflects the
ideal case -- exact data sets endowed with flawless dissimilarity between
individual instances.
  In practice however, these cases are in the minority, and clustering
applications are typically characterized by noisy data sets with approximate
pairwise dissimilarities. As such, the efficacy of clustering methods in
practical applications necessitates robustness to perturbations.
  In this paper, we perform a formal analysis of perturbation robustness,
revealing that the extent to which algorithms can exhibit this desirable
characteristic is inherently limited, and identifying the types of structures
that allow popular clustering paradigms to discover meaningful clusters in
spite of faulty data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05904</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05904</id><created>2016-01-22</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author><author><keyname>Xu</keyname><forenames>Liangliang</forenames></author></authors><title>Improving GPU-accelerated Adaptive IDW Interpolation Algorithm Using
  Fast kNN Search</title><categories>cs.DC</categories><comments>Submitted manuscript. 9 Figures, 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient parallel Adaptive Inverse Distance Weighting
(AIDW) interpolation algorithm on modern Graphics Processing Unit (GPU). The
presented algorithm is an improvement of our previous GPU-accelerated AIDW
algorithm by adopting fast k-Nearest Neighbors (kNN) search. In AIDW, it needs
to find several nearest neighboring data points for each interpolated point to
adaptively determine the power parameter; and then the desired prediction value
of the interpolated point is obtained by weighted interpolating using the power
parameter. In this work, we develop a fast kNN search approach based on the
space-partitioning data structure, even grid, to improve the previous
GPU-accelerated AIDW algorithm. The improved algorithm is composed of the
stages of kNN search and weighted interpolating. To evaluate the performance of
the improved algorithm, we perform five groups of experimental tests.
Experimental results show that: (1) the improved algorithm can achieve a
speedup of up to 1017 over the corresponding serial algorithm; (2) the improved
algorithm is at least two times faster than our previous GPU-accelerated AIDW
algorithm; and (3) the utilization of fast kNN search can significantly improve
the computational efficiency of the entire GPU-accelerated AIDW algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05908</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05908</id><created>2016-01-22</created><authors><author><keyname>Alrshah</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author><author><keyname>Ali</keyname><forenames>Borhanuddin</forenames></author><author><keyname>Hanapi</keyname><forenames>Zurina Mohd</forenames></author></authors><title>Agile-SD: A Linux-based TCP Congestion Control Algorithm for Supporting
  High-speed and Short-distance Networks</title><categories>cs.NI</categories><comments>12 Pages</comments><journal-ref>Journal of Network and Computer Applications, 55, pp.181-190
  (2015)</journal-ref><doi>10.1016/j.jnca.2015.05.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, high-speed and short-distance networks are widely deployed and
their necessity is rapidly increasing everyday. This type of networks is used
in several network applications; such as Local Area Networks (LAN) and Data
Center Networks (DCN). In LANs and DCNs, high-speed and short-distance networks
are commonly deployed to connect between computing and storage elements in
order to provide rapid services. Indeed, the overall performance of such
networks is significantly influenced by the Congestion Control Algorithm (CCA)
which suffers from the problem of bandwidth under-utilization, especially if
the applied buffer regime is very small. In this paper, a novel loss-based CCA
tailored for high-speed and Short-Distance (SD) networks, namely Agile-SD, has
been proposed. The main contribution of the proposed CCA is to implement the
mechanism of agility factor. Further, intensive simulation experiments have
been carried out to evaluate the performance of Agile-SD compared to Compound
and Cubic which are the default CCAs of the most commonly used operating
systems. The results of the simulation experiments show that the proposed CCA
outperforms the compared CCAs in terms of average throughput, loss ratio and
fairness, especially when a small buffer is applied. Moreover, Agile-SD shows
lower sensitivity to the buffer size change and packet error rate variation
which increases its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05909</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05909</id><created>2016-01-22</created><updated>2016-01-24</updated><authors><author><keyname>Wu</keyname><forenames>Huanhuan</forenames></author><author><keyname>Huang</keyname><forenames>Yuzhen</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Li</keyname><forenames>Jinfeng</forenames></author><author><keyname>Ke</keyname><forenames>Yiping</forenames></author></authors><title>Efficient Processing of Reachability and Time-Based Path Queries in a
  Temporal Graph</title><categories>cs.DB</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A temporal graph is a graph in which vertices communicate with each other at
specific time, e.g., $A$ calls $B$ at 11 a.m. and talks for 7 minutes, which is
modeled by an edge from $A$ to $B$ with starting time &quot;11 a.m.&quot; and duration &quot;7
mins&quot;. Temporal graphs can be used to model many networks with time-related
activities, but efficient algorithms for analyzing temporal graphs are severely
inadequate. We study fundamental problems such as answering reachability and
time-based path queries in a temporal graph, and propose an efficient indexing
technique specifically designed for processing these queries in a temporal
graph. Our results show that our method is efficient and scalable in both index
construction and query processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05911</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05911</id><created>2016-01-22</created><authors><author><keyname>Mayer</keyname><forenames>Norbert Michael</forenames></author></authors><title>Orthogonal Echo State Networks and stochastic evaluations of likelihoods</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we report the likelihood estimates that are performed on time
series using a echo state network with orthogonal recurrent connectivity. The
results indicate that the optimal performance depends on the way of balancing
the input strength with the recurrent activity, which also has an influence on
the network with regard to the quality of the short term prediction versus
prediction that accounts for influences that date back a long time in the input
history. Finally, sensitivity of such networks against noise/finite accuracy of
network states in the recurrent layer is investigated. In addition a measure
that bases on mutual information is introduced in order to best quantify the
performance of the network with the time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05917</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05917</id><created>2016-01-22</created><authors><author><keyname>Sima</keyname><forenames>Jin</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Polar Codes for Broadcast Channels with Receiver Message Side
  Information and Noncausal State Available at the Encoder</title><categories>cs.IT math.IT</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper polar codes are proposed for two receiver broadcast channels
with receiver message side information (BCSI) and noncausal state available at
the encoder, referred to as BCSI with noncausal state for short, where the two
receivers know a priori the private messages intended for each other. This
channel generalizes BCSI with common message and Gelfand-Pinsker problem and
has applications in cellular communication systems. We establish an achievable
rate region for BCSI with noncausal state and show that it is strictly larger
than the straightforward extension of the Gelfand-Pinsker result. To achieve
the established rate region with polar coding, we present polar codes for the
general Gelfand-Pinsker problem, which adopts chaining construction and
utilizes causal information to pre-transmit the frozen bits. It is also shown
that causal information is necessary to pre-transmit the frozen bits. Based on
the result of Gelfand-Pinsker problem, we use the chaining construction method
to design polar codes for BCSI with noncausal state. The difficulty is that
there are multiple chains sharing common information bit indices. To avoid
value assignment conflicts, a nontrivial polarization alignment scheme is
presented. It is shown that the proposed rate region is tight for degraded BCSI
with noncausal state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05921</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05921</id><created>2016-01-22</created><authors><author><keyname>Zeng</keyname><forenames>Zhiwen</forenames></author><author><keyname>Wang</keyname><forenames>Xiangke</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Zhao</keyname><forenames>Lina</forenames></author></authors><title>Edge Agreement of Second-order Multi-agent System with Dynamic
  Quantization via Directed Edge Laplacian</title><categories>cs.IT math.IT</categories><comments>21 pages; submitted to Nonlinear Analysis: Hybrid Systems, Ms. Ref.
  No.: NAHS-D-15-00161. arXiv admin note: substantial text overlap with
  arXiv:1501.06678</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores the edge agreement problem of second-order multi-agent
system with dynamic quantization under directed communication. To begin with,
by virtue of the directed edge laplacian, we derive a model reduction
representation of the closed-loop multi-agent system depended on the spanning
tree subgraph. Considering the limitations of the finite bandwidth channels,
the quantization effects of second-order multi-agent system under directed
graph are considered. Motivated by the observation that the static quantizer
always lead to the practical stability rather than the asymptotic stability,
the dynamic quantized communication strategy referring to the rooming
in-rooming out scheme is employed. Based on the reduced model associated with
the essential edge Laplacian, the asymptotic stability of second-order
multi-agent system under dynamic quantized effects with only finite
quantization level can be guaranteed. Finally, simulation results are provided
to verify the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05927</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05927</id><created>2016-01-22</created><authors><author><keyname>Czegledi</keyname><forenames>Cristian B.</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author><author><keyname>Johannisson</keyname><forenames>Pontus</forenames></author></authors><title>Modulation Format Independent Joint Polarization and Phase Tracking for
  Coherent Receivers</title><categories>cs.IT math.IT physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state of polarization and the carrier phase drift dynamically during
transmission in a random fashion in coherent optical fiber communications. The
typical digital signal processing solution to mitigate these impairments
consists of two separate blocks that track each phenomenon independently. Such
algorithms have been developed without taking into account mathematical models
describing the impairments. We study a blind, model-based tracking algorithm to
compensate for these impairments. The algorithm dynamically recovers the
carrier phase and state of polarization jointly for an arbitrary modulation
format. Simulation results show the effectiveness of the proposed algorithm,
having a fast convergence rate and an excellent tolerance to phase noise and
dynamic drift of the polarization. The computational complexity of the
algorithm is lower compared to state-of-the-art algorithms at similar or better
performance, which makes it a strong candidate for future optical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05928</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05928</id><created>2016-01-22</created><authors><author><keyname>Yao</keyname><forenames>Chuting</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Role of Large Scale Channel Information on Predictive Resource
  Allocation</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, WCNC 2016 accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the future achievable rate is perfectly known, predictive resource
allocation can provide high performance gain over traditional resource
allocation for the traffic without stringent delay requirement. However, future
channel information is hard to obtain in wireless channels, especially the
small-scale fading gains. In this paper, we analytically demonstrate that the
future large-scale channel information can capture almost all the performance
gain from knowing the future channel by taking an energy-saving resource
allocation as an example. This result is important for practical systems, since
large-scale channel gains can be easily estimated from the predicted trajectory
of mobile users and radio map. Simulation results validate our analysis and
illustrate the impact of the estimation errors of large-scale channel gains on
energy saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05929</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05929</id><created>2016-01-22</created><authors><author><keyname>Gro&#xdf;e</keyname><forenames>Peter</forenames></author><author><keyname>Schneider</keyname><forenames>Christian</forenames></author><author><keyname>Sommerkorn</keyname><forenames>Gerd</forenames></author><author><keyname>Thom&#xe4;</keyname><forenames>Reiner</forenames></author></authors><title>A Hybrid Channel Model based on WINNER for Vehicle-to-X Application</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, EURO-COST, IC1004, TD(13)07040</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  V2V and V2I channel modeling became recently more of interest. To provide
realistic radio channels either expensive measurements or complex ray tracing
simulations are mostly used. Stochastic channel models are of low complexity
but do not offer that deterministic repeatable realism. Based on the WINNER
channel model and a simple single path model, a hybrid model has been
developed. The concept relies on a layered structure featuring high flexibility
and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05936</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05936</id><created>2016-01-22</created><authors><author><keyname>Dighe</keyname><forenames>Pranay</forenames></author><author><keyname>Luyet</keyname><forenames>Gil</forenames></author><author><keyname>Asaei</keyname><forenames>Afsaneh</forenames></author><author><keyname>Bourlard</keyname><forenames>Herve</forenames></author></authors><title>Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic
  Modeling in Speech Recognition</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to model the acoustic space of deep neural network (DNN)
class-conditional posterior probabilities as a union of low-dimensional
subspaces. To that end, the training posteriors are used for dictionary
learning and sparse coding. Sparse representation of the test posteriors using
this dictionary enables projection to the space of training data. Relying on
the fact that the intrinsic dimensions of the posterior subspaces are indeed
very small and the matrix of all posteriors belonging to a class has a very low
rank, we demonstrate how low-dimensional structures enable further enhancement
of the posteriors and rectify the spurious errors due to mismatch conditions.
The enhanced acoustic modeling method leads to improvements in continuous
speech recognition task using hybrid DNN-HMM (hidden Markov model) framework in
both clean and noisy conditions, where upto 15.4% relative reduction in word
error rate (WER) is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05940</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05940</id><created>2016-01-22</created><authors><author><keyname>Bassias</keyname><forenames>Antonios</forenames></author><author><keyname>Chronopoulos</keyname><forenames>Anthony</forenames></author></authors><title>Statistical Performance Analysis of the MUSIC Algorithm in Angular
  Sectors</title><categories>stat.ME cs.IT math.IT</categories><journal-ref>Journal of Signal Processing, Vol.15, No.1, pp. 37-46, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the problem of the statistical performance analysis
of the MUSIC ( Multiple Signal Classification ) algorithm which is an eigen
decomposition based method for the estimation of the angles of arrival of
signals received by an array of sensors. In past work the performance of the
MUSIC algorithm was studied ( via an asymptotic statistical analysis of the
null spectrum of the algorithm ) for the case of two plane waves of equal power
in noise. In this article, a new theoretical formula is derived for the signal
to noise ratio resolution threshold of two uncorrelated, narrow band plane
waves with equal powers in angular sectors received by an array of sensors. The
accuracy of the formula is assessed using examples which compute the
theoretical signal to noise ratio resolution threshold and compare it with the
threshold obtained from simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05952</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05952</id><created>2016-01-22</created><authors><author><keyname>Sarda</keyname><forenames>Siddharth</forenames></author><author><keyname>Eickhoff</keyname><forenames>Carsten</forenames></author><author><keyname>Hofmann</keyname><forenames>Thomas</forenames></author></authors><title>Semantic Place Descriptors for Classification and Map Discovery</title><categories>cs.IR cs.SI</categories><comments>13 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban environments develop complex, non-obvious structures that are often
hard to represent in the form of maps or guides. Finding the right place to go
often requires intimate familiarity with the location in question and cannot
easily be deduced by visitors. In this work, we exploit large-scale samples of
usage information, in the form of mobile phone traces and geo-tagged Twitter
messages in order to automatically explore and annotate city maps via kernel
density estimation. Our experiments are based on one year's worth of mobile
phone activity collected by Nokia's Mobile Data Challenge (MDC). We show that
usage information can be a strong predictor of semantic place categories,
allowing us to automatically annotate maps based on the behavior of the local
user base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05961</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05961</id><created>2016-01-22</created><authors><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author><author><keyname>Babaoglu</keyname><forenames>Ozalp</forenames></author></authors><title>Power Consumption Modeling and Prediction in a Hybrid CPU-GPU-MIC
  Supercomputer (preliminary version)</title><categories>cs.DC</categories><comments>13 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power consumption is a major obstacle for High Performance Computing (HPC)
systems in their quest towards the holy grail of ExaFLOP performance.
Significant advances in power efficiency have to be made before this goal can
be attained and accurate modeling is an essential step towards power efficiency
by optimizing system operating parameters to match dynamic energy needs. In
this paper we present a study of power consumption by jobs in Eurora, a hybrid
CPU-GPU-MIC system installed at the largest Italian data center. Using data
from a dedicated monitoring framework, we build a data-driven model of power
consumption for each user in the system and use it to predict the power
requirements of future jobs. We are able to achieve good prediction results for
over 80% of the users in the system. For the remaining users, we identify
possible reasons why prediction performance is not as good. Possible
applications for our predictive modeling results include scheduling
optimization, power-aware billing and system-scale power modeling. All the
scripts used for the study have been made available on GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05962</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05962</id><created>2016-01-22</created><updated>2016-03-03</updated><authors><author><keyname>Giraudo</keyname><forenames>Samuele</forenames></author><author><keyname>Vialette</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Unshuffling Permutations</title><categories>cs.DS math.CO</categories><comments>13 pages</comments><journal-ref>Latin American Theoretical Informatics Symposium, LNCS 9644,
  509--521, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A permutation is said to be a square if it can be obtained by shuffling two
order-isomorphic patterns. The definition is intended to be the natural
counterpart to the ordinary shuffle of words and languages. In this paper, we
tackle the problem of recognizing square permutations from both the point of
view of algebra and algorithms. On the one hand, we present some algebraic and
combinatorial properties of the shuffle product of permutations. We follow an
unusual line consisting in defining the shuffle of permutations by means of an
unshuffling operator, known as a coproduct. This strategy allows to obtain easy
proofs for algebraic and combinatorial properties of our shuffle product. We
besides exhibit a bijection between square $(213,231)$-avoiding permutations
and square binary words. On the other hand, by using a pattern avoidance
criterion on oriented perfect matchings, we prove that recognizing square
permutations is $\mathbf{NP}$-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05973</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05973</id><created>2016-01-22</created><authors><author><keyname>Masters</keyname><forenames>Karen</forenames><affiliation>Portsmouth Business School</affiliation></author><author><keyname>Oh</keyname><forenames>Eun Young</forenames><affiliation>Portsmouth Business School</affiliation></author><author><keyname>Cox</keyname><forenames>Joe</forenames><affiliation>Portsmouth Business School</affiliation></author><author><keyname>Simmons</keyname><forenames>Brooke</forenames><affiliation>Oxford Astrophysics</affiliation></author><author><keyname>Lintott</keyname><forenames>Chris</forenames><affiliation>Oxford Astrophysics</affiliation></author><author><keyname>Graham</keyname><forenames>Gary</forenames></author><author><keyname>Greenhill</keyname><forenames>Anita</forenames></author><author><keyname>Holmes</keyname><forenames>Kate</forenames></author></authors><title>Science Learning via Participation in Online Citizen Science</title><categories>astro-ph.IM cs.CY physics.ed-ph</categories><comments>32 pages (9 pages of Appendix material). Accepted for publication in
  the Journal of Science Communication (JCOM; http://jcom.sissa.it/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the development of scientific content knowledge of volunteers
participating in online citizen science projects in the Zooniverse
(www.zooniverse.org), including the astronomy projects Galaxy Zoo
(www.galaxyzoo.org) and Planet Hunters (www.planethunters.org). We use
econometric methods to test how measures of project participation relate to
success in a science quiz, controlling for factors known to correlate with
scientific knowledge. Citizen scientists believe they are learning about both
the content and processes of science through their participation. Won't don't
directly test the latter, but we find evidence to support the former - that
more actively engaged participants perform better in a project-specific science
knowledge quiz, even after controlling for their general science knowledge. We
interpret this as evidence of learning of science content inspired by
participation in online citizen science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05976</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05976</id><created>2016-01-22</created><authors><author><keyname>Singer</keyname><forenames>Robert</forenames></author></authors><title>Business Process Modeling and Execution -- A Compiler for Distributed
  Microservices</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to rethink the dominant logic of how to model
business processes. We think that an actor based approach supports in a much
better way the fundamental nature of business processes. We present a proposal
for a compiler architecture to model and execute business processes as a set of
communicating microservices that are hosted on a general purpose virtual
machine for distributed execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05977</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05977</id><created>2016-01-22</created><updated>2016-01-28</updated><authors><author><keyname>Eden</keyname><forenames>Amnon H.</forenames></author></authors><title>The Singularity Controversy, Part I: Lessons Learned and Open Questions:
  Conclusions from the Battle on the Legitimacy of the Debate</title><categories>cs.AI cs.CY</categories><report-no>STR 2016-1</report-no><doi>10.13140/RG.2.1.3416.6809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report seeks to inform policy makers on the nature and the merit of the
arguments for and against the concerns associated with a potential
technological singularity.
  Part I describes the lessons learned from our investigation of the subject,
separating the argu-ments of merit from the fallacies and misconceptions that
confuse the debate and undermine its rational resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05978</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05978</id><created>2016-01-22</created><authors><author><keyname>Grabisch</keyname><forenames>Michel</forenames></author><author><keyname>Labreuche</keyname><forenames>Christophe</forenames></author></authors><title>On the decomposition of Generalized Additive Independence models</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GAI (Generalized Additive Independence) model proposed by Fishburn is a
generalization of the additive utility model, which need not satisfy mutual
preferential independence. Its great generality makes however its application
and study difficult. We consider a significant subclass of GAI models, namely
the discrete 2-additive GAI models, and provide for this class a decomposition
into nonnegative monotone terms. This decomposition allows a reduction from
exponential to quadratic complexity in any optimization problem involving
discrete 2-additive models, making them usable in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05982</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05982</id><created>2016-01-22</created><authors><author><keyname>Gong</keyname><forenames>Shiqi</forenames></author><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author></authors><title>Polarization Sensitive Array Based Physical-Layer Security</title><categories>cs.IT math.IT</categories><comments>35 Pages, 14 Figures, IEEE Journal of Signal Processing 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a comprehensive framework exploiting the
polarization sensitive array (PLA) to improve the physical layer security of
wireless communications. More specifically, the polarization difference among
signals are utilized to improve the secrecy rate of wireless communications,
especially when these signals are spatially indistinguishable. We firstly
investigate the PLA based secure communications for point-to-point wireless
systems from the perspectives of both total power minimization and secrecy rate
maximization. We then take a step further to apply the PLA based secure
beamforming designs to relaying networks. The secrecy rate maximization for
relaying networks is discussed in detail under both the perfect channel state
information and the polarization sensitive array pointing error. In the later
case, a robust scheme to achieve secure communications for relaying networks is
proposed. Simulation results show that the proposed PLA based algorithms
achieve lower total power consumption and better security performance compared
with the conventional scalar array designs, especially under challenging
environments where all received signals at destination are difficult to be
distinguished from the spatial domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05988</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05988</id><created>2016-01-22</created><authors><author><keyname>&#x130;lhan</keyname><forenames>Asl&#x131; G&#xfc;&#xe7;l&#xfc;kan</forenames></author><author><keyname>&#xdc;nl&#xfc;</keyname><forenames>&#xd6;zg&#xfc;n</forenames></author></authors><title>Multi-Valued Logic Gates, Continuous Sensitivity, Reversibility, and
  Threshold Functions</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an invariant of a multi-valued logic gate by considering the number
of certain threshold functions associated with the gate. We call this invariant
the continuous sensitivity of the gate. We discuss a method for analysing
continuous sensitivity of a multi-valued logic gate by using experimental data
about the gate. In particular, we will show that this invariant provides a
lower bound for the sensitivity of a boolean function considered as a
multi-valued logic gate. We also discuss how continuous sensitivity can be used
to understand the reversibility of the Fourier series expansion of a
multi-valued logic gate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05989</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05989</id><created>2016-01-22</created><authors><author><keyname>Bonnet</keyname><forenames>&#xc9;douard</forenames></author><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Flip Distance to a Non-crossing Perfect Matching</title><categories>cs.DM cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A perfect straight-line matching $M$ on a finite set $P$ of points in the
plane is a set of segments such that each point in $P$ is an endpoint of
exactly one segment. $M$ is non-crossing if no two segments in $M$ cross each
other. Given a perfect straight-line matching $M$ with at least one crossing,
we can remove this crossing by a flip operation. The flip operation removes two
crossing segments on a point set $Q$ and adds two non-crossing segments to
attain a new perfect matching $M'$. It is well known that after a finite number
of flips, a non-crossing matching is attained and no further flip is possible.
However, prior to this work, no non-trivial upper bound on the number of flips
was known. If $g(n)$ (resp.~$k(n)$) is the maximum length of the longest
(resp.~shortest) sequence of flips starting from any matching of size $n$, we
show that $g(n) = O(n^3)$ and $g(n) = \Omega(n^2)$ (resp.~$k(n) = O(n^2)$ and
$k(n) = \Omega (n)$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05991</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05991</id><created>2016-01-22</created><authors><author><keyname>Cernak</keyname><forenames>Milos</forenames></author><author><keyname>Benus</keyname><forenames>Stefan</forenames></author><author><keyname>Lazaridis</keyname><forenames>Alexandros</forenames></author></authors><title>Speech vocoding for laboratory phonology</title><categories>cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using phonological speech vocoding, we propose a platform for exploring
relations between phonology and speech processing, and in broader terms, for
exploring relations between the abstract and physical structures of a speech
signal. Our goal is to make a step towards bridging phonology and speech
processing and to contribute to the program of Laboratory Phonology. We show
three application examples for laboratory phonology: compositional phonological
speech modelling, a comparison of phonological systems and an experimental
phonological parametric text-to-speech (TTS) system. The featural
representations of the following three phonological systems are considered in
this work: (i) Government Phonology (GP), (ii) the Sound Pattern of English
(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded
speech, we conclude that the latter achieves slightly better results than the
former. However, GP - the most compact phonological speech representation -
performs comparably to the systems with a higher number of phonological
features. The parametric TTS based on phonological speech representation, and
trained from an unlabelled audiobook in an unsupervised manner, achieves
intelligibility of 85% of the state-of-the-art parametric speech synthesis. We
envision that the presented approach paves the way for researchers in both
fields to form meaningful hypotheses that are explicitly testable using the
concepts developed and exemplified in this paper. On the one hand, laboratory
phonologists might test the applied concepts of their theoretical models, and
on the other hand, the speech processing community may utilize the concepts
developed for the theoretical phonological models for improvements of the
current state-of-the-art applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.05994</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.05994</id><created>2016-01-22</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>He</keyname><forenames>Chuanjiang</forenames></author></authors><title>Depth and Reflection Total Variation for Single Image Dehazing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Haze removal has been a very challenging problem due to its ill-posedness,
which is more ill-posed if the input data is only a single hazy image. In this
paper, we present a new approach for removing haze from a single input image.
The proposed method combines the model widely used to describe the formation of
a haze image with the assumption in Retinex that an image is the product of the
illumination and the reflection. We assume that the depth and reflection
functions are spatially piecewise smooth in the model, where the total
variation is used for the regularization. The proposed model is defined as a
constrained optimization problem, which is solved by an alternating
minimization scheme and the fast gradient projection algorithm. Some theoretic
analyses are given for the proposed model and algorithm. Finally, numerical
examples are presented to demonstrate that our method can restore vivid and
contrastive hazy images effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06005</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06005</id><created>2016-01-22</created><authors><author><keyname>Shen</keyname><forenames>Jiajun</forenames></author><author><keyname>Xu</keyname><forenames>Bin</forenames></author><author><keyname>Pei</keyname><forenames>Mingtao</forenames></author><author><keyname>Jia</keyname><forenames>Yunde</forenames></author></authors><title>A Tele-Presence Wheelchair for Elderly People</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the architecture and implementation of a tele-presence
wheelchair based on tele-presence robot, intelligent wheelchair, and smart pad
TIUI technologies. The tele-presence wheelchair is just a commercial electric
wheelchair equipped with a tele-presence interaction system. The tele-presence
interaction system is an add-on module which consists of three parts:
tele-presence, tele-operation imaging, and user interface. The tele-presence
part is mounted on the front of the wheelchair for an elderly person to
communicate with family members through video and voice. The tele-operation
imaging part captures the live video for tele-operation and semi-autonomous
navigation. The user interface developed in our lab for a smart pad [12] is the
TIUI, which allows an operator to directly touch and push the wheelchair image
in the live video of a remote environment as him/her to do in the presence.
This paper also discusses the preliminary evaluation of the user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06008</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06008</id><created>2016-01-22</created><authors><author><keyname>Yousefi-Azar</keyname><forenames>Mahmood</forenames></author><author><keyname>Razzazi</keyname><forenames>Farbod</forenames></author></authors><title>A Robust Frame-based Nonlinear Prediction System for Automatic Speech
  Coding</title><categories>cs.SD cs.NE</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a neural-based coding scheme in which an artificial
neural network is exploited to automatically compress and decompress speech
signals by a trainable approach. Having a two-stage training phase, the system
can be fully specified to each speech frame and have robust performance across
different speakers and wide range of spoken utterances. Indeed, Frame-based
nonlinear predictive coding (FNPC) would code a frame in the procedure of
training to predict the frame samples. The motivating objective is to analyze
the system behavior in regenerating not only the envelope of spectra, but also
the spectra phase. This scheme has been evaluated in time and discrete cosine
transform (DCT) domains and the output of predicted phonemes show the
potentiality of the FNPC to reconstruct complicated signals. The experiments
were conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains
versus the number of neurons in the hidden layer. Experiments approve the FNPC
capability as an automatic coding system by which /b/d/g/ phonemes have been
reproduced with a good accuracy. Evaluations revealed that the performance of
FNPC system, trained to predict DCT coefficients is more desirable,
particularly for frames with the wider distribution of energy, compared to time
samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06009</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06009</id><created>2016-01-22</created><authors><author><keyname>Su</keyname><forenames>Dongcai</forenames></author></authors><title>Compressed sensing with corrupted observations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed a weighted l1 minimization to recover a sparse signal vector and
the corrupted noise vector from a linear measurement when the sensing matrix A
is an m by n row i.i.d subgaussian matrix. We obtain both uniform and
nonuniform recovery guarantees when the corrupted observations occupy a
constant fraction of the total measurement, provided that the signal vector is
sparse enough. In the uniform recovery guarantee, the upper-bound of the
cardinality of the signal vector required in this paper is asymptotically
optimal. While in the non-uniform recovery guarantee, we allow the proportion
of corrupted measurements grows arbitrarily close to 1, and the upper-bound of
the cardinality of the signal vector is better than those in a recent
literature [1] by a ln(n) factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06011</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06011</id><created>2016-01-22</created><authors><author><keyname>Su</keyname><forenames>Dongcai</forenames></author></authors><title>Data recovery from corrupted observations via l1 minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of recovering a signal vector and the
corrupted noise vector from a collection of corrupted linear measurements
through the solution of a l1 minimization, where the sensing matrix is a
partial Fourier matrix whose rows are selected randomly and uniformly from rows
of a full Fourier matrix. After choosing the parameter in the l1 minimization
appropriately, we show that the recovery can be successful even when a constant
fraction of the measurements are arbitrarily corrupted, moreover, the
proportion of corrupted measurement can grows arbitrarily close to 1, provided
that the signal vector is sparse enough. The upper-bound on the sparsity of the
signal vector required in this paper is asymptotically optimal and is better
than those achieved by recent literatures [1, 2] by a ln(n) factor.
Furthermore, the assumptions we impose on the signal vector and the corrupted
noise vector are loosest comparing to the existing literatures [1-3], which
lenders our recovery guarantees are more applicable. Extensive numerical
experiments based on synthesis as well as real world data are presented to
verify the conclusion of the proposed theorem and to demonstrate the potential
of the l1 minimization framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06014</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06014</id><created>2016-01-22</created><authors><author><keyname>D&#x119;bowski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Consistency of the Plug-In Estimator of the Entropy Rate for Ergodic
  Processes</title><categories>cs.IT math.IT</categories><comments>3 pages</comments><msc-class>94A17, 62F10, 94A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A plug-in estimator of entropy is the entropy of the distribution where the
probabilities of symbols or blocks have been replaced with their relative
frequencies in the sample. Consistency and asymptotic unbiasedness of the
plug-in estimator can be easily demonstrated in the IID case. In this paper, we
ask whether the plug-in estimator can be used for consistent estimation of the
entropy rate $h$ of a stationary ergodic process. The answer is positive if, to
estimate block entropy of order $k$, we use a sample longer than
$k2^{k(h+\epsilon)}$, whereas it is negative if we use a sample shorter than
$k2^{k(h-\epsilon)}$. In particular, if we do not know the entropy rate $h$, it
is sufficient to use a sample of length $k(A+\epsilon)^{k}$ where $A$ is the
alphabet size. The result is derived using $k$-block coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06016</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06016</id><created>2016-01-22</created><authors><author><keyname>Sahraei</keyname><forenames>Saeid</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Multi-Library Coded Caching</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of coded caching when the server has access to several
libraries and each user makes independent requests from every library. The
single-library scenario has been well studied and it has been proved that coded
caching can significantly improve the delivery rate compared to uncoded
caching. In this work we show that when all the libraries have the same number
of files, memory-sharing is optimal and the delivery rate cannot be improved
via coding across files from different libraries. In this setting, the optimal
memory-sharing strategy is one that divides the cache of each user proportional
to the size of the files in different libraries. As for the general case, when
the number of files in different libraries are arbitrary, we propose an
inner-bound based on memory-sharing and an outer-bound based on concatenation
of files from different libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06023</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06023</id><created>2016-01-22</created><updated>2016-02-04</updated><authors><author><keyname>Ak</keyname><forenames>Serkan</forenames></author><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Gaussian Approximation for the Downlink Interference in Heterogeneous
  Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives Gaussian approximation bounds for the standardized
aggregate wireless interference (AWI) in the downlink of K-tier heterogeneous
cellular networks when base stations in each tier are distributed over the
plane according to a (possibly non-homogeneous) Poisson process. The proposed
methodology is general enough to account for general bounded path-loss models
and fading statistics. The deviations of the distribution of the standardized
AWI from the standard normal distribution are measured in terms of the
Kolmogorov-Smirnov distance. An explicit expression bounding the
Kolmogorov-Smirnov distance between these two distributions is obtained as a
function of a broad range of network parameters such as per-tier transmission
power levels, base station locations, fading statistics and the path-loss
model. A simulation study is performed to corroborate the analytical results.
In particular, a good statistical match between the standardized AWI
distribution and its normal approximation occurs even for moderately dense
heterogeneous cellular networks. These results are expected to have important
ramifications on the characterization of performance upper and lower bounds for
emerging 5G network architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06028</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06028</id><created>2016-01-22</created><updated>2016-01-25</updated><authors><author><keyname>Hristova</keyname><forenames>Desislava</forenames></author><author><keyname>Rutherford</keyname><forenames>Alex</forenames></author><author><keyname>Anson</keyname><forenames>Jose</forenames></author><author><keyname>Luengo-Oroz</keyname><forenames>Miguel</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>The International Postal Network and Other Global Flows As Proxies for
  National Wellbeing</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The digital exhaust left by flows of physical and digital commodities
provides a rich measure of the nature, strength and significance of
relationships between countries in the global network. With this work, we
examine how these traces and the network structure can reveal the socioeconomic
profile of different countries. We take into account multiple international
networks of physical and digital flows, including the previously unexplored
international postal network. By measuring the position of each country in the
Trade, Postal, Migration, International Flights, IP and Digital Communications
networks, we are able to build proxies for a number of crucial socioeconomic
indicators such as GDP per capita and the Human Development Index ranking along
with twelve other indicators used as benchmarks of national wellbeing by the
United Nations and other international organisations. In this context, we have
also proposed and evaluated a global connectivity degree measure applying
multiplex theory across the six networks that accounts for the strength of
relationships between countries. We conclude with a multiplex community
analysis of the global flow networks, showing how countries with shared
community membership over multiple networks have similar socioeconomic
profiles. Combining multiple flow data sources into global multiplex networks
can help understand the forces which drive economic activity on a global level.
Such an ability to infer proxy indicators in a context of incomplete
information is extremely timely in light of recent discussions on measurement
of indicators relevant to the Sustainable Development Goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06032</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06032</id><created>2016-01-22</created><authors><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Wu</keyname><forenames>Xiaohe</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author></authors><title>Learning Support Correlation Filters for Visual Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling and budgeting training examples are two essential factors in
tracking algorithms based on support vector machines (SVMs) as a trade-off
between accuracy and efficiency. Recently, the circulant matrix formed by dense
sampling of translated image patches has been utilized in correlation filters
for fast tracking. In this paper, we derive an equivalent formulation of a SVM
model with circulant matrix expression and present an efficient alternating
optimization method for visual tracking. We incorporate the discrete Fourier
transform with the proposed alternating optimization process, and pose the
tracking problem as an iterative learning of support correlation filters (SCFs)
which find the global optimal solution with real-time performance. For a given
circulant data matrix with n^2 samples of size n*n, the computational
complexity of the proposed algorithm is O(n^2*logn) whereas that of the
standard SVM-based approaches is at least O(n^4). In addition, we extend the
SCF-based tracking algorithm with multi-channel features, kernel functions, and
scale-adaptive approaches to further improve the tracking performance.
Experimental results on a large benchmark dataset show that the proposed
SCF-based algorithms perform favorably against the state-of-the-art tracking
methods in terms of accuracy and speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06035</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06035</id><created>2016-01-22</created><authors><author><keyname>Stark</keyname><forenames>Cyril</forenames></author></authors><title>Recommender systems inspired by the structure of quantum theory</title><categories>cs.LG cs.IT math.IT math.OC quant-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physicists use quantum models to describe the behavior of physical systems.
Quantum models owe their success to their interpretability, to their relation
to probabilistic models (quantization of classical models) and to their high
predictive power. Beyond physics, these properties are valuable in general data
science. This motivates the use of quantum models to analyze general
nonphysical datasets. Here we provide both empirical and theoretical insights
into the application of quantum models in data science. In the theoretical part
of this paper, we firstly show that quantum models can be exponentially more
efficient than probabilistic models because there exist datasets that admit
low-dimensional quantum models and only exponentially high-dimensional
probabilistic models. Secondly, we explain in what sense quantum models realize
a useful relaxation of compressed probabilistic models. Thirdly, we show that
sparse datasets admit low-dimensional quantum models and finally, we introduce
a method to compute hierarchical orderings of properties of users (e.g.,
personality traits) and items (e.g., genres of movies). In the empirical part
of the paper, we evaluate quantum models in item recommendation and observe
that the predictive power of quantum-inspired recommender systems can compete
with state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,
we make use of the interpretability of quantum models by computing hierarchical
orderings of properties of users and items. This work establishes a connection
between data science (item recommendation), information theory (communication
complexity), mathematical programming (positive semidefinite factorizations)
and physics (quantum models).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06037</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06037</id><created>2016-01-22</created><authors><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author><author><keyname>Claridge</keyname><forenames>Jessica</forenames></author></authors><title>Finite field matrix channels for network coding</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><msc-class>94A40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2010, Silva, Kschischang and Koetter studied certain classes of finite
field matrix channels in order to model random linear network coding where
exactly $t$ random errors are introduced. The exact capacity of such channels
is hard to determine due to the many degrees of freedom involved: the naive
formula maximises over a probability distribution on the set of possible input
matrices, and this set is exponentially large.
  In this paper we introduce a generalisation of these matrix channels, that
allow the modelling of channels where a variable number of random errors are
introduced. We show that a capacity-achieving input distribution can always be
taken to have a very restricted form (the distribution should be uniform given
the rank of the input matrix). Nobrega, Silva and Uchoa-Filho proved a similar
result for a class of matrix channels that model network coding with link
erasures. One corollary of our result is that the capacity for these channels
may be expressed as a maximisation over probability distributions on the set of
possible ranks of input matrices: a set of linear rather than exponential size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06039</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06039</id><created>2016-01-22</created><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author></authors><title>Greedy Algorithms for Optimal Distribution Approximation</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approximation of a discrete probability distribution $\mathbf{t}$ by an
$M$-type distribution $\mathbf{p}$ is considered. The approximation error is
measured by the informational divergence
$\mathbb{D}(\mathbf{t}\Vert\mathbf{p})$, which is an appropriate measure, e.g.,
in the context of data compression. Properties of the optimal approximation are
derived and bounds on the approximation error are presented, which are
asymptotically tight. It is shown that $M$-type approximations that minimize
either $\mathbb{D}(\mathbf{t}\Vert\mathbf{p})$, or
$\mathbb{D}(\mathbf{p}\Vert\mathbf{t})$, or the variational distance
$\Vert\mathbf{p}-\mathbf{t}\Vert_1$ can all be found by using specific
instances of the same general greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06040</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06040</id><created>2016-01-22</created><authors><author><keyname>Fusco</keyname><forenames>Emanuele Guido</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author><author><keyname>Petreschi</keyname><forenames>Rossella</forenames></author></authors><title>Topology recognition with advice</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In topology recognition, each node of an anonymous network has to
deterministically produce an isomorphic copy of the underlying graph, with all
ports correctly marked. This task is usually unfeasible without any a priori
information. Such information can be provided to nodes as advice. An oracle
knowing the network can give a (possibly different) string of bits to each
node, and all nodes must reconstruct the network using this advice, after a
given number of rounds of communication. During each round each node can
exchange arbitrary messages with all its neighbors and perform arbitrary local
computations. The time of completing topology recognition is the number of
rounds it takes, and the size of advice is the maximum length of a string given
to nodes.
  We investigate tradeoffs between the time in which topology recognition is
accomplished and the minimum size of advice that has to be given to nodes. We
provide upper and lower bounds on the minimum size of advice that is sufficient
to perform topology recognition in a given time, in the class of all graphs of
size $n$ and diameter $D\le \alpha n$, for any constant $\alpha&lt; 1$. In most
cases, our bounds are asymptotically tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06041</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06041</id><created>2016-01-22</created><authors><author><keyname>Patroumpas</keyname><forenames>Kostas</forenames></author><author><keyname>Alevizos</keyname><forenames>Elias</forenames></author><author><keyname>Artikis</keyname><forenames>Alexander</forenames></author><author><keyname>Vodas</keyname><forenames>Marios</forenames></author><author><keyname>Pelekis</keyname><forenames>Nikos</forenames></author><author><keyname>Theodoridis</keyname><forenames>Yannis</forenames></author></authors><title>Online Event Recognition from Moving Vessel Trajectories</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system for online monitoring of maritime activity over streaming
positions from numerous vessels sailing at sea. It employs an online tracking
module for detecting important changes in the evolving trajectory of each
vessel across time, and thus can incrementally retain concise, yet reliable
summaries of its recent movement. In addition, thanks to its complex event
recognition module, this system can also offer instant notification to marine
authorities regarding emergency situations, such as risk of collisions,
suspicious moves in protected zones, or package picking at open sea. Not only
did our extensive tests validate the performance, efficiency, and robustness of
the system against scalable volumes of real-world and synthetically enlarged
datasets, but its deployment against online feeds from vessels has also
confirmed its capabilities for effective, real-time maritime surveillance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06043</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06043</id><created>2016-01-22</created><authors><author><keyname>Habib</keyname><forenames>Sana</forenames></author><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Ali</keyname><forenames>Anwaar</forenames></author><author><keyname>Habib</keyname><forenames>Durdana</forenames></author><author><keyname>Li</keyname><forenames>Ming</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author></authors><title>The Past, Present, and Future of Transport-Layer Multipath</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipathing in communication networks is gaining momentum due to its
attractive features of increased reliability, throughput, fault tolerance, and
load balancing capabilities. In particular, wireless environments and
datacenters are envisioned to become largely dependent on the power of
multipathing for seamless handovers, virtual machine (VM) migration and in
general, pooling less proficient resources together for achieving overall high
proficiency. The transport layer, with its knowledge about end-to-end path
characteristics, is well placed to enhance performance through better
utilization of multiple paths. Realizing the importance of transport-layer
multipath, this paper investigates the modernization of traditional connection
establishment, flow control, sequence number splitting, acknowledgement, and
flow scheduling mechanisms for use with multiple paths. Since congestion
control defines a fundamental feature of the transport layer, we study the
working of multipath rate control and analyze its stability and convergence. We
also discuss how various multipath congestion control algorithms differ in
their window increase and decrease functions, their TCP-friendliness, and
responsiveness. To the best of our knowledge, this is the first in-depth survey
paper that has chronicled the evolution of the transport layer of the Internet
from the traditional single-path TCP to the recent development of the modern
multipath TCP (MPTCP) protocol. Along with describing the history of this
evolution, we also highlight in this paper the remaining challenges and
research issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06044</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06044</id><created>2016-01-22</created><authors><author><keyname>Lopes</keyname><forenames>Wilder B.</forenames></author><author><keyname>Al-Nuaimi</keyname><forenames>Anas</forenames></author><author><keyname>Lopes</keyname><forenames>Cassio G.</forenames></author></authors><title>Geometric-Algebra LMS Adaptive Filter and its Application to Rotation
  Estimation</title><categories>cs.CV cs.CG</categories><comments>4 pages of content plus 1 of references; 4 figures. Supplementary
  material (codes and datasets) available at www.lps.usp.br/wilder</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper exploits Geometric (Clifford) Algebra (GA) theory in order to
devise and introduce a new adaptive filtering strategy. From a least-squares
cost function, the gradient is calculated following results from Geometric
Calculus (GC), the extension of GA to handle differential and integral
calculus. The novel GA least-mean-squares (GA-LMS) adaptive filter, which
inherits properties from standard adaptive filters and from GA, is developed to
recursively estimate a rotor (multivector), a hypercomplex quantity able to
describe rotations in any dimension. The adaptive filter (AF) performance is
assessed via a 3D point-clouds registration problem, which contains a rotation
estimation step. Calculating the AF computational complexity suggests that it
can contribute to reduce the cost of a full-blown 3D registration algorithm,
especially when the number of points to be processed grows. Moreover, the
employed GA/GC framework allows for easily applying the resulting filter to
estimating rotors in higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06048</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06048</id><created>2016-01-22</created><authors><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Comparing the Bit-MAP and Block-MAP Decoding Thresholds of Reed-Muller
  Codes on BMS Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to ISIT'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question whether RM codes are capacity-achieving is a long-standing open
problem in coding theory that was recently answered in the affirmative for
transmission over erasure channels [1], [2]. Remarkably, the proof does not
rely on specific properties of RM codes, apart from their symmetry. Indeed, the
main technical result consists in showing that any sequence of linear codes,
with doubly-transitive permutation groups, achieves capacity on the memoryless
erasure channel under bit-MAP decoding. Thus, a natural question is what
happens under block-MAP decoding. In [1], [2], by exploiting further symmetries
of the code, the bit-MAP threshold was shown to be sharp enough so that the
block erasure probability also converges to 0. However, this technique relies
heavily on the fact that the transmission is over an erasure channel.
  We present an alternative approach to strengthen results regarding the
bit-MAP threshold to block-MAP thresholds. This approach is based on a careful
analysis of the weight distribution of RM codes. In particular, the flavor of
the main result is the following: assume that the bit-MAP error probability
decays as $N^{-\delta}$, for some $\delta&gt;0$. Then, the block-MAP error
probability also converges to 0. This technique applies to transmission over
any binary memoryless symmetric channel. Thus, it can be thought of as a first
step in extending the proof that RM codes are capacity-achieving to the general
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06055</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06055</id><created>2016-01-22</created><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Finite-Blocklength Bounds for Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>extended version of a paper submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the maximal secrecy rate over a wiretap channel
subject to reliability and secrecy constraints at a given blocklength. New
achievability and converse bounds are derived, which are shown to be tighter
than existing bounds. The bounds also lead to the tightest second-order coding
rate for discrete memoryless and Gaussian wiretap channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06057</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06057</id><created>2016-01-22</created><authors><author><keyname>Zeppelzauer</keyname><forenames>Matthias</forenames></author><author><keyname>Zieli&#x144;ski</keyname><forenames>Bartosz</forenames></author><author><keyname>Juda</keyname><forenames>Mateusz</forenames></author><author><keyname>Seidl</keyname><forenames>Markus</forenames></author></authors><title>Topological descriptors for 3D surface analysis</title><categories>cs.CV</categories><comments>12 pages, 3 figures, CTIC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate topological descriptors for 3D surface analysis, i.e. the
classification of surfaces according to their geometric fine structure. On a
dataset of high-resolution 3D surface reconstructions we compute persistence
diagrams for a 2D cubical filtration. In the next step we investigate different
topological descriptors and measure their ability to discriminate structurally
different 3D surface patches. We evaluate their sensitivity to different
parameters and compare the performance of the resulting topological descriptors
to alternative (non-topological) descriptors. We present a comprehensive
evaluation that shows that topological descriptors are (i) robust, (ii) yield
state-of-the-art performance for the task of 3D surface analysis and (iii)
improve classification performance when combined with non-topological
descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06059</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06059</id><created>2016-01-22</created><authors><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Optimal Resource Allocation Over Time and Degree Classes for Maximizing
  Information Dissemination in Social Networks</title><categories>cs.SI cs.SY math.OC physics.soc-ph</categories><comments>14 + 4 pages, 11 figures. Author's version of the article accepted
  for publication in IEEE/ACM Transactions on Networking. This version includes
  4 pages of supplementary material containing proofs of theorems present in
  the article. Published version can be accessed at
  http://dx.doi.org/10.1109/TNET.2015.2512541</comments><doi>10.1109/TNET.2015.2512541</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal control problem of allocating campaigning resources over
the campaign duration and degree classes in a social network. Information
diffusion is modeled as a Susceptible-Infected epidemic and direct recruitment
of susceptible nodes to the infected (informed) class is used as a strategy to
accelerate the spread of information. We formulate an optimal control problem
for optimizing a net reward function, a linear combination of the reward due to
information spread and cost due to application of controls. The time varying
resource allocation and seeds for the epidemic are jointly optimized. A problem
variation includes a fixed budget constraint. We prove the existence of a
solution for the optimal control problem, provide conditions for uniqueness of
the solution, and prove some structural results for the controls (e.g. controls
are non-increasing functions of time). The solution technique uses Pontryagin's
Maximum Principle and the forward-backward sweep algorithm (and its
modifications) for numerical computations. Our formulations lead to large
optimality systems with up to about 200 differential equations and allow us to
study the effect of network topology (Erdos-Renyi/scale-free) on the controls.
Results reveal that the allocation of campaigning resources to various degree
classes depends not only on the network topology but also on system parameters
such as cost/abundance of resources. The optimal strategies lead to significant
gains over heuristic strategies for various model parameters. Our modeling
approach assumes uncorrelated network, however, we find the approach useful for
real networks as well. This work is useful in product advertising, political
and crowdfunding campaigns in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06060</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06060</id><created>2016-01-22</created><authors><author><keyname>Eidenbenz</keyname><forenames>Raphael</forenames></author><author><keyname>Locher</keyname><forenames>Thomas</forenames></author></authors><title>Task Allocation for Distributed Stream Processing</title><categories>cs.DC</categories><comments>Extended Version of the work published in the proceedings of IEEE
  International Conference on Computer Communications (INFOCOM), 10-15 April
  2016, San Francisco, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing demand for live, on-the-fly processing of increasingly
large amounts of data. In order to ensure the timely and reliable processing of
streaming data, a variety of distributed stream processing architectures and
platforms have been developed, which handle the fundamental tasks of
(dynamically) assigning processing tasks to the currently available physical
resources and routing streaming data between these resources. However, while
there are plenty of platforms offering such functionality, the theory behind it
is not well understood. In particular, it is unclear how to best allocate the
processing tasks to the given resources. In this paper, we establish a
theoretical foundation by formally defining a task allocation problem for
distributed stream processing, which we prove to be NP-hard. Furthermore, we
propose an approximation algorithm for the class of series-parallel
decomposable graphs, which captures a broad range of common stream processing
applications. The algorithm achieves a constant-factor approximation under the
assumptions that the number of resources scales at least logarithmically with
the number of computational tasks and the computational cost of the tasks
dominates the cost of communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06062</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06062</id><created>2016-01-22</created><authors><author><keyname>Hoffmann</keyname><forenames>Matthias</forenames></author><author><keyname>Kowalewski</keyname><forenames>Christopher</forenames></author><author><keyname>Maier</keyname><forenames>Andreas</forenames></author><author><keyname>Kurzidim</keyname><forenames>Klaus</forenames></author><author><keyname>Strobel</keyname><forenames>Norbert</forenames></author><author><keyname>Hornegger</keyname><forenames>Joachim</forenames></author></authors><title>3-D/2-D Registration of Cardiac Structures by 3-D Contrast Agent
  Distribution Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For augmented fluoroscopy during cardiac catheter ablation procedures, a
preoperatively acquired 3-D model of the left atrium of the patient can be
registered to X-ray images. Therefore the 3D-model is matched with the contrast
agent based appearance of the left atrium. Commonly, only small amounts of
contrast agent (CA) are used to locate the left atrium. This is why we focus on
robust registration methods that work also if the structure of interest is only
partially contrasted. In particular, we propose two similarity measures for
CA-based registration: The first similarity measure, explicit apparent edges,
focuses on edges of the patient anatomy made visible by contrast agent and can
be computed quickly on the GPU. The second novel similarity measure computes a
contrast agent distribution estimate (CADE) inside the 3-D model and rates its
consistency with the CA seen in biplane fluoroscopic images. As the CADE
computation involves a reconstruction of CA in 3-D using the CA within the
fluoroscopic images, it is slower. Using a combination of both methods, our
evaluation on 11 well-contrasted clinical datasets yielded an error of
7.9+/-6.3 mm over all frames. For 10 datasets with little CA, we obtained an
error of 8.8+/-6.7 mm. Our new methods outperform a registration based on the
projected shadow significantly (p&lt;0.05).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06065</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06065</id><created>2016-01-22</created><authors><author><keyname>Swamy</keyname><forenames>Peruru Subrahmanya</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Jagannathan</keyname><forenames>Krishna</forenames></author></authors><title>Adaptive CSMA under the SINR Model: Efficient Approximation Algorithms
  for Throughput and Utility Maximization</title><categories>cs.IT cs.NI math.IT math.PR stat.AP</categories><comments>Submitted to IEEE/ACM Transactions on Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a CSMA based scheduling algorithm for a single-hop wireless
network under a realistic SINR model for the interference. We propose two local
optimization based approximation algorithms to efficiently estimate certain
attempt rate parameters of CSMA called fugacities. It is known that adaptive
CSMA can achieve throughput optimality by sampling feasible schedules from a
Gibbs distribution, with appropriate fugacities. Unfortunately, obtaining these
optimal fugacities is an NP-hard problem. Further, the existing adaptive CSMA
algorithms use a stochastic gradient descent based method, which usually
entails an impractically slow (exponential in the size of the network)
convergence to the optimal fugacities. To address this issue, we first propose
an algorithm to estimate the fugacities, that can support a given set of
desired service rates. The convergence rate and the complexity of this
algorithm are independent of the network size, and depend only on the
neighborhood size of a link. Further, we show that the proposed algorithm
corresponds exactly to performing the well-known Bethe approximation to the
underlying Gibbs distribution. Then, we propose another local algorithm to
estimate the optimal fugacities under a utility maximization framework, and
characterize its accuracy. Numerical results indicate that the proposed methods
have a good degree of accuracy, and achieve extremely fast convergence to
near-optimal fugacities, and often outperform the convergence rate of the
stochastic gradient descent by a few orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06068</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06068</id><created>2016-01-22</created><authors><author><keyname>Narayan</keyname><forenames>Shashi</forenames></author><author><keyname>Reddy</keyname><forenames>Siva</forenames></author><author><keyname>Cohen</keyname><forenames>Shay B.</forenames></author></authors><title>Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the limitations of semantic parsing approaches to open-domain question
answering is the lexicosyntactic gap between natural language questions and
knowledge base entries -- there are many ways to ask a question, all with the
same answer. In this paper we propose to bridge this gap by generating
paraphrases of the input question with the goal that at least one of them will
be correctly mapped to a knowledge-base query. We introduce a novel grammar
model for paraphrase generation that does not require any sentence-aligned
paraphrase corpus. Our key idea is to leverage the flexibility and scalability
of latent-variable probabilistic context-free grammars to sample paraphrases.
We do an extrinsic evaluation of our paraphrases by plugging them into a
semantic parser for Freebase. Our evaluation experiments on the WebQuestions
benchmark dataset show that the performance of the semantic parser
significantly improves over strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06069</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06069</id><created>2016-01-22</created><authors><author><keyname>Ground</keyname><forenames>Larry</forenames></author><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Budd</keyname><forenames>Ray</forenames></author></authors><title>Coalition-based Planning of Military Operations: Adversarial Reasoning
  Algorithms in an Integrated Decision Aid</title><categories>cs.AI</categories><comments>A version of this paper appeared in proceedings of the 2002
  International Conference on Knowledge Systems for Coalition Operations (KSCO)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use of knowledge-based planning tools can help alleviate the challenges of
planning a complex operation by a coalition of diverse parties in an
adversarial environment. We explore these challenges and potential
contributions of knowledge-based tools using as an example the CADET system, a
knowledge-based tool capable of producing automatically (or with human
guidance) battle plans with realistic degree of detail and complexity. In
ongoing experiments, it compared favorably with human planners. Interleaved
planning, scheduling, routing, attrition and consumption processes comprise the
computational approach of this tool. From the coalition operations perspective,
such tools offer an important aid in rapid synchronization of assets and
actions of heterogeneous assets belonging to multiple organizations,
potentially with distinct doctrine and rules of engagement. In this paper, we
discuss the functionality of the tool, provide a brief overview of the
technical approach and experimental results, and outline the potential value of
such tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06070</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06070</id><created>2016-01-22</created><authors><author><keyname>L&#xe4;hner</keyname><forenames>Zorah</forenames></author><author><keyname>Rodol&#xe0;</keyname><forenames>Emanuele</forenames></author><author><keyname>Schmidt</keyname><forenames>Frank R.</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Efficient Globally Optimal 2D-to-3D Deformable Shape Matching</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the first algorithm for non-rigid 2D-to-3D shape matching, where
the input is a 2D shape represented as a planar curve and a 3D shape
represented as a surface; the output is a continuous curve on the surface. We
cast the problem as finding the shortest circular path on the prod- uct
3-manifold of the surface and the curve. We prove that the optimal matching can
be computed in polynomial time with a (worst-case) complexity of
$O(mn^2\log(n))$, where $m$ and $n$ denote the number of vertices on the
template curve and the 3D shape respectively. We also demonstrate that in
practice the runtime is essentially linear in $m\!\cdot\! n$ making it an
efficient method for shape analysis and shape retrieval. Quantitative
evaluation confirms that the method provides excellent results for sketch-based
deformable 3D shape re- trieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06071</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06071</id><created>2016-01-22</created><authors><author><keyname>Kim</keyname><forenames>Minje</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>Bitwise Neural Networks</title><categories>cs.LG cs.AI cs.NE</categories><comments>This paper was presented at the International Conference on Machine
  Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille,
  France, Jul. 6-11, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the assumption that there exists a neural network that efficiently
represents a set of Boolean functions between all binary inputs and outputs, we
propose a process for developing and deploying neural networks whose weight
parameters, bias terms, input, and intermediate hidden layer output signals,
are all binary-valued, and require only basic bit logic for the feedforward
pass. The proposed Bitwise Neural Network (BNN) is especially suitable for
resource-constrained environments, since it replaces either floating or
fixed-point arithmetic with significantly more efficient bitwise operations.
Hence, the BNN requires for less spatial complexity, less memory bandwidth, and
less power consumption in hardware. In order to design such networks, we
propose to add a few training schemes, such as weight compression and noisy
backpropagation, which result in a bitwise network that performs almost as well
as its corresponding real-valued network. We test the proposed network on the
MNIST dataset, represented using binary features, and show that BNNs result in
competitive performance while offering dramatic computational savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06075</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06075</id><created>2016-01-22</created><authors><author><keyname>Omodei</keyname><forenames>Elisa</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Evaluating the impact of interdisciplinary research: a multilayer
  network approach</title><categories>physics.soc-ph cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, scientific challenges usually require approaches that cross
traditional boundaries between academic disciplines, driving many researchers
towards interdisciplinarity. Despite its obvious importance, there is a lack of
studies on how to quantify the influence of interdisciplinarity on the research
impact, posing uncertainty in a proper evaluation for hiring and funding
purposes. Here we propose a method based on the analysis of bipartite
interconnected multilayer networks of citations and disciplines, to assess
scholars, institutions and countries interdisciplinary importance. Using data
about physics publications and US patents, we show that our method allows to
reveal, using a quantitative approach, that being more interdisciplinary causes
-- in the Granger sense -- benefits in scientific productivity and impact. The
proposed method could be used by funding agencies, universities and scientific
policy decision makers for hiring and funding purposes, and to complement
existing methods to rank universities and countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06076</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06076</id><created>2016-01-22</created><authors><author><keyname>Aumann</keyname><forenames>Quirin</forenames></author><author><keyname>Osorio</keyname><forenames>Carlos M.</forenames></author><author><keyname>Lai</keyname><forenames>Celeste</forenames></author></authors><title>Liquid Humans - Pedestrian Simulator based on the LWR-model</title><categories>cs.CE cs.MA</categories><comments>15 pages. Journal version of conference article arXiv:1511.00053</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense human flow has been a concern for the safety of public events for a
long time. Macroscopic pedestrian models, which are mainly based on fluid
dynamics, are often used to simulate huge crowds due to their low computational
costs. Similar approaches are used in the field of traffic simulations. A
combined macroscopic simulation of vehicles and pedestrians is extremely
helpful for all-encompassing traffic control. Therefore, we developed a hybrid
model that contains networks for vehicular traffic and human flow. This
comprehensive model supports concurrent multi-modal simulations of traffic and
pedestrians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06081</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06081</id><created>2016-01-22</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Strapparava</keyname><forenames>Carlo</forenames></author></authors><title>Why Do Urban Legends Go Viral?</title><categories>cs.CL cs.CY cs.SI</categories><comments>Preprint of paper in Journal of Information Processing and Management
  Volume 52, Issue 1, January 2016, Pages 163-172</comments><doi>10.1016/j.ipm.2015.05.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban legends are a genre of modern folklore, consisting of stories about
rare and exceptional events, just plausible enough to be believed, which tend
to propagate inexorably across communities. In our view, while urban legends
represent a form of &quot;sticky&quot; deceptive text, they are marked by a tension
between the credible and incredible. They should be credible like a news
article and incredible like a fairy tale to go viral. In particular we will
focus on the idea that urban legends should mimic the details of news (who,
where, when) to be credible, while they should be emotional and readable like a
fairy tale to be catchy and memorable. Using NLP tools we will provide a
quantitative analysis of these prototypical characteristics. We also lay out
some machine learning experiments showing that it is possible to recognize an
urban legend using just these simple features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06087</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06087</id><created>2016-01-22</created><authors><author><keyname>Ahmadi</keyname><forenames>Aria</forenames></author><author><keyname>Patras</keyname><forenames>Ioannis</forenames></author></authors><title>Unsupervised convolutional neural networks for motion estimation</title><categories>cs.CV</categories><comments>Submitted to ICIP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional methods for motion estimation estimate the motion field F between
a pair of images as the one that minimizes a predesigned cost function. In this
paper, we propose a direct method and train a Convolutional Neural Network
(CNN) that when, at test time, is given a pair of images as input it produces a
dense motion field F at its output layer. In the absence of large datasets with
ground truth motion that would allow classical supervised training, we propose
to train the network in an unsupervised manner. The proposed cost function that
is optimized during training, is based on the classical optical flow
constraint. The latter is differentiable with respect to the motion field and,
therefore, allows backpropagation of the error to previous layers of the
network. Our method is tested on both synthetic and real image sequences and
performs similarly to the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06094</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06094</id><created>2016-01-22</created><updated>2016-01-26</updated><authors><author><keyname>Jitsumatsu</keyname><forenames>Yutaka</forenames></author><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>An Iterative Algorithm for Computing the Optimal Exponent of Correct
  Decoding Probability for Rates below the Rate Distortion Function</title><categories>cs.IT math.IT</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The form of Dueck and K\&quot;orner's exponent function for correct decoding
probability for discrete memoryless channels at rates above the capacity is
similar to the form of Csisz\'ar and K\&quot;orner's exponent function for correct
decoding probability in lossy source coding for discrete memoryless sources at
rates below the rate distortion function. We recently gave a new algorithm for
computing Dueck and K\&quot;orner's exponent. In this paper, we give an algorithm
for computing Csisz\'ar and K\&quot;orner's exponent. The proposed algorithm can be
also used to compute cutoff rate and the rate distortion function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06095</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06095</id><created>2016-01-22</created><authors><author><keyname>Yang</keyname><forenames>Yaoqing</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author></authors><title>Energy Efficient Distributed Coding for Data Collection in a Noisy
  Sparse Network</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1508.01553</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of data collection in a two-layer network consisting
of (1) links between $N$ distributed agents and a remote sink node; (2) a
sparse network formed by these distributed agents. We study the effect of
inter-agent communications on the overall energy consumption. Despite the
sparse connections between agents, we provide an in-network coding scheme that
reduces the overall energy consumption by a factor of $\Theta(\log N)$ compared
to a naive scheme which neglects inter-agent communications. By providing lower
bounds on both the energy consumption and the sparseness (number of links) of
the network, we show that are energy-optimal except for a factor of
$\Theta(\log\log N)$. The proposed scheme extends a previous work of Gallager
on noisy broadcasting from a complete graph to a sparse graph, while bringing
in new techniques from error control coding and noisy circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06098</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06098</id><created>2016-01-22</created><authors><author><keyname>Melli&#xe8;s</keyname><forenames>Paul-Andr&#xe9;</forenames></author><author><keyname>Zeilberger</keyname><forenames>Noam</forenames></author></authors><title>A bifibrational reconstruction of Lawvere's presheaf hyperdoctrine</title><categories>cs.LO math.CT math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining insights from the study of type refinement systems and of monoidal
closed chiralities, we show how to reconstruct Lawvere's hyperdoctrine of
presheaves using a full and faithful embedding into a monoidal closed
bifibration living now over the compact closed category of small categories and
distributors. Besides revealing dualities which are not immediately apparent in
the traditional presentation of the presheaf hyperdoctrine, this reconstruction
leads us to an axiomatic treatment of directed equality predicates (modelled by
hom presheaves), realizing a vision initially set out by Lawvere (1970). It
also leads to a simple calculus of string diagrams (representing presheaves)
that is highly reminiscent of C. S. Peirce's existential graphs for predicate
logic, refining an earlier interpretation of existential graphs in terms of
Boolean hyperdoctrines by Brady and Trimble. Finally, we illustrate how this
work extends to a bifibrational setting a number of fundamental ideas of linear
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06101</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06101</id><created>2016-01-22</created><authors><author><keyname>Elkouss</keyname><forenames>David</forenames></author><author><keyname>P&#xe9;rez-Garc&#xed;a</keyname><forenames>David</forenames></author></authors><title>Uncomputability of the generalized capacity</title><categories>cs.IT cs.CC cs.FL math.IT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is the maximum rate at which we can send information over a channel? We
define the capacity of a channel as the answer to this question. For an
important family of channels the capacity is given by a simple optimization
problem as proven in Shannon's noisy coding theorem. Furthermore, for these
channels, we have the Blahut-Arimoto algorithm that allows to efficiently solve
the optimization problem and compute the capacity. In groundbreaking work,
Verdu and Han proved a coding theorem for general channels. However, despite
considerable effort, there is no equivalent to the Blahut-Arimoto algorithm for
computing the generalized capacity. Here, we show that such an algorithm can
not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06102</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06102</id><created>2016-01-22</created><authors><author><keyname>Samadi</keyname><forenames>Zainalabedin</forenames></author><author><keyname>Tabatabavakili</keyname><forenames>Vahid</forenames></author><author><keyname>Haddadi</keyname><forenames>Farzan</forenames></author></authors><title>Perfect Interference Alignment for an Interference Network with General
  Message Demands</title><categories>cs.IT math.IT</categories><comments>19 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1101.3068 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality requirement poses a major challenge for Interference alignment
(IA) in practical systems. This work evaluates the necessary and sufficient
conditions on channel structure of a fully connected general interference
network to make perfect IA feasible within limited number of channel
extensions. So far, IA feasibility literature have mainly focused on network
topology, in contrast, this work makes use of the channel structure to achieve
total number of degrees of freedom (DoF) of the considered network by extending
the channel aided IA scheme to the case of interference channel with general
message demands. We consider a single-hop interference network with $K$
transmitters and $N$ receivers each equipped with a single antenna. Each
transmitter emits an independent message and each receiver requests an
arbitrary subset of the messages. Obtained channel aiding conditions can be
considered as the optimal DoF feasibility conditions on channel structure. As a
byproduct, assuming optimal DoF assignment, it is proved that in a general
interference network, there is no user with a unique maximum number of DoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06103</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06103</id><created>2016-01-20</created><authors><author><keyname>Rahimian</keyname><forenames>M. Amin</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Bayesian Learning without Recall</title><categories>math.ST cs.SI cs.SY math.OC math.PR stat.TH</categories><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a model of learning and belief formation in networks in which
agents follow Bayes rule yet they do not recall their history of past
observations and cannot reason about how other agents' beliefs are formed. They
do so by making rational inferences about their observations which include a
sequence of independent and identically distributed private signals as well as
the actions of their neighboring agents at each time. Successive applications
of Bayes rule to the entire history of past observations lead to forebodingly
complex inferences: due to lack of knowledge about the global network
structure, and unavailability of private observations, as well as third party
interactions preceding every decision. Such difficulties make Bayesian updating
of beliefs an implausible mechanism for social learning. To address these
complexities, we consider a Bayesian without Recall model of inference. On the
one hand, this model provides a tractable framework for analyzing the behavior
of rational agents in social networks. On the other hand, this model also
provides a behavioral foundation for the variety of non-Bayesian update rules
in the literature. We present the implications of various choices for the
structure of the action space and utility functions for such agents and
investigate the properties of learning, convergence, and consensus in special
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06105</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06105</id><created>2016-01-22</created><authors><author><keyname>Root</keyname><forenames>Jonathan</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author><author><keyname>Qian</keyname><forenames>Jing</forenames></author></authors><title>Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1502.01783,
  arXiv:1405.0530</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a non-parametric anomaly detection algorithm for high dimensional
data. We first rank scores derived from nearest neighbor graphs on $n$-point
nominal training data. We then train limited complexity models to imitate these
scores based on the max-margin learning-to-rank framework. A test-point is
declared as an anomaly at $\alpha$-false alarm level if the predicted score is
in the $\alpha$-percentile. The resulting anomaly detector is shown to be
asymptotically optimal in that for any false alarm rate $\alpha$, its decision
region converges to the $\alpha$-percentile minimum volume level set of the
unknown underlying density. In addition, we test both the statistical
performance and computational efficiency of our algorithm on a number of
synthetic and real-data experiments. Our results demonstrate the superiority of
our algorithm over existing $K$-NN based anomaly detection algorithms, with
significant computational savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06108</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06108</id><created>2016-01-22</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Budd</keyname><forenames>Ray</forenames></author><author><keyname>Ground</keyname><forenames>Larry</forenames></author><author><keyname>Rebbapragada</keyname><forenames>Lakshmi</forenames></author><author><keyname>Langston</keyname><forenames>John</forenames></author></authors><title>Decision Aids for Adversarial Planning in Military Operations:
  Algorithms, Tools, and Turing-test-like Experimental Validation</title><categories>cs.AI</categories><comments>A version of this paper appeared in the Applied Intelligence journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use of intelligent decision aids can help alleviate the challenges of
planning complex operations. We describe integrated algorithms, and a tool
capable of translating a high-level concept for a tactical military operation
into a fully detailed, actionable plan, producing automatically (or with human
guidance) plans with realistic degree of detail and of human-like quality.
Tight interleaving of several algorithms -- planning, adversary estimates,
scheduling, routing, attrition and consumption estimates -- comprise the
computational approach of this tool. Although originally developed for Army
large-unit operations, the technology is generic and also applies to a number
of other domains, particularly in critical situations requiring detailed
planning within a constrained period of time. In this paper, we focus
particularly on the engineering tradeoffs in the design of the tool. In an
experimental evaluation, reminiscent of the Turing test, the tool's performance
compared favorably with human planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06113</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06113</id><created>2016-01-22</created><authors><author><keyname>Noorzad</keyname><forenames>Parham</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>The Unbounded Benefit of Encoder Cooperation for the $k$-User MAC
  (Extended Version)</title><categories>cs.IT math.IT</categories><comments>25 pages, 4 figures. To be submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation strategies can improve the performance of a network by allowing
nodes to work together. Since these strategies are usually studied in small
networks, it is not clear how their properties scale when applied to larger
networks. This paper generalizes a cooperation model from the 2-user MAC to the
$k$-user MAC extending prior capacity bounds and characterizing the set of all
MACs where the gain in sum-capacity as a result of encoder cooperation is much
larger than the total number of bits shared by the encoders in the cooperation
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06116</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06116</id><created>2016-01-22</created><authors><author><keyname>Mnatzaganian</keyname><forenames>James</forenames></author><author><keyname>Fokou&#xe9;</keyname><forenames>Ernest</forenames></author><author><keyname>Kudithipudi</keyname><forenames>Dhireesha</forenames></author></authors><title>A Mathematical Formalization of Hierarchical Temporal Memory Cortical
  Learning Algorithm's Spatial Pooler</title><categories>stat.ML cs.LG q-bio.NC</categories><comments>This work was submitted to IEEE Transactions on Neural Networks and
  Learning Systems. It is currently under review. For associated code, see
  https://github.com/tehtechguy/mHTM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical temporal memory (HTM) is an emerging machine learning algorithm,
with the potential to provide a means to perform predictions on spatiotemporal
data. The algorithm, inspired by the neocortex, currently does not have a
comprehensive mathematical framework. This work brings together all aspects of
the spatial pooler (SP), a critical learning component in HTM, under a single
unifying framework. The primary learning mechanism is explored, where a maximum
likelihood estimator for determining the degree of permanence update is
proposed. The boosting mechanisms are studied and found to be only relevant
during the initial few iterations of the network. Observations are made
relating HTM to well known algorithms such as competitive learning and
attribute bagging. Methods are provided for using the SP for classification as
well as dimensionality reduction. Empirical evidence verifies that given the
proper parameterizations, the SP may be used for feature learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06119</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06119</id><created>2016-01-22</created><authors><author><keyname>Roos</keyname><forenames>Stefanie</forenames></author><author><keyname>Beck</keyname><forenames>Martin</forenames></author><author><keyname>Strufe</keyname><forenames>Thorsten</forenames></author></authors><title>VOUTE-Virtual Overlays Using Tree Embeddings</title><categories>cs.CR</categories><comments>Extended version to Stefanie Roos, Martin Beck, Thorsten Strufe:
  'Anonymous Addresses for Efficient and Resilient Routing in F2F Overlays',
  Infocom 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Friend-to-friend (F2F) overlays, which restrict direct communication to
mutually trusted parties, are a promising substrate for privacy-preserving
communication due to their inherent membership-concealment and
Sybil-resistance. Yet, existing F2F overlays suffer from a low performance, are
vulnerable to denial-of-service attacks, or fail to provide anonymity. In
particular, greedy embeddings allow highly efficient communication in arbitrary
connectivity-restricted overlays but require communicating parties to reveal
their identity. In this paper, we present a privacy-preserving routing scheme
for greedy embeddings based on anonymous return addresses rather than
identifying node coordinates. We prove that the presented algorithm are highly
scalalbe, with regard to the complexity of both the routing and the
stabilization protocols. Furthermore, we show that the return addresses provide
plausible deniability for both sender and receiver. We further enhance the
routing's resilience by using multiple embeddings and propose a method for
efficient content addressing. Our simulation study on real-world data indicates
that our approach is highly efficient and effectively mitigates failures as
well as powerful denial-of-service attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06128</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06128</id><created>2016-01-22</created><authors><author><keyname>Bessa</keyname><forenames>Aline</forenames></author><author><keyname>Silva</keyname><forenames>Fernando de Mesentier</forenames></author><author><keyname>Nogueira</keyname><forenames>Rodrigo Frassetto</forenames></author><author><keyname>Bertini</keyname><forenames>Enrico</forenames></author><author><keyname>Freire</keyname><forenames>Juliana</forenames></author></authors><title>RioBusData: Outlier Detection in Bus Routes of Rio de Janeiro</title><categories>cs.HC</categories><comments>In Symposium on Visualization in Data Science (VDS at IEEE VIS),
  Chicago, Illinois, US, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Buses are the primary means of public transportation in the city of Rio de
Janeiro, carrying around 100 million passengers every month. Recently,
real-time GPS coordinates of all operating public buses has been made publicly
available - roughly 1 million GPS entries each captured each day. In an initial
study, we observed that a substantial number of buses follow trajectories that
do not follow the expected behavior. In this paper, we present RioBusData, a
tool that helps users identify and explore, through different visualizations,
the behavior of outlier trajectories. We describe how the system automatically
detects these outliers using a Convolutional Neural Network (CNN) and we also
discuss a series of case studies which show how RioBusData helps users better
understand not only the flow and service of outlier buses but also the bus
system as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06153</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06153</id><created>2016-01-22</created><authors><author><keyname>Kadhe</keyname><forenames>Swanand</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>Codes with Unequal Locality</title><categories>cs.IT math.IT</categories><comments>Longer version of the ISIT 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a code $\code$, its $i$-th symbol is said to have locality $r$ if its
value can be recovered by accessing some other $r$ symbols of $\code$. Locally
repairable codes (LRCs) are the family of codes such that every symbol has
locality $r$.
  In this paper, we focus on (linear) codes whose individual symbols can be
partitioned into disjoint subsets such that the symbols in one subset have
different locality than the symbols in other. We call such codes as &quot;codes with
unequal locality&quot;. For codes with &quot;unequal information locality&quot;, we compute a
tight upper bound on the minimum distance as a function of number of
information symbols of each locality. We demonstrate that the construction of
Pyramid codes can be adapted to design codes with unequal information locality
that achieve the minimum distance bound. This result generalizes the classical
result of Gopalan et al. for codes with unequal locality. Next, we consider
codes with &quot;unequal all symbol locality&quot;, and establish an upper bound on the
minimum distance as a function of number of symbols of each locality. We show
that the construction based on rank-metric codes by Silberstein et al. can be
adapted to obtain codes with unequal all symbol locality that achieve the
minimum distance bound. Finally, we introduce the concept of &quot;locality
requirement&quot; on a code, which can be viewed as a recoverability requirement on
symbols. Information locality requirement on a code essentially specifies the
minimum number of information symbols of different localities that must be
present in the code. We present a greedy algorithm that assigns localities to
information symbols so as to maximize the minimum distance among all codes that
satisfy a given locality requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06176</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06176</id><created>2016-01-22</created><authors><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>What is Information?</title><categories>nlin.AO cs.IT math.IT physics.bio-ph q-bio.QM</categories><comments>19 pages, 2 figures. To appear in Philosophical Transaction of the
  Royal Society A</comments><journal-ref>Philosophical Transaction of the Royal Society A 374 (2016)
  20150230</journal-ref><doi>10.1098/rsta.2015.0230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information is a precise concept that can be defined mathematically, but its
relationship to what we call &quot;knowledge&quot; is not always made clear. Furthermore,
the concepts &quot;entropy&quot; and &quot;information&quot;, while deeply related, are distinct
and must be used with care, something that is not always achieved in the
literature. In this elementary introduction, the concepts of entropy and
information are laid out one by one, explained intuitively, but defined
rigorously. I argue that a proper understanding of information in terms of
prediction is key to a number of disciplines beyond engineering, such as
physics and biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06180</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06180</id><created>2016-01-22</created><authors><author><keyname>Peharz</keyname><forenames>Robert</forenames></author><author><keyname>Gens</keyname><forenames>Robert</forenames></author><author><keyname>Pernkopf</keyname><forenames>Franz</forenames></author><author><keyname>Domingos</keyname><forenames>Pedro</forenames></author></authors><title>On the Latent Variable Interpretation in Sum-Product Networks</title><categories>cs.AI cs.LG</categories><comments>14 pages, 10 figures, 5 tables</comments><msc-class>62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the central themes in Sum-Product networks (SPNs) is the
interpretation of sum nodes as marginalized latent variables (LVs). This
interpretation yields an increased syntactic or semantic structure, allows the
application of the EM algorithm and to efficiently perform MPE inference. In
literature, the LV interpretation was justified by explicitly introducing the
indicator variables corresponding to the LVs' states. However, as pointed out
in this paper, this approach is in conflict with the completeness condition in
SPNs and does not fully specify the probabilistic model. We propose a remedy
for this problem by modifying the original approach for introducing the LVs,
which we call SPN augmentation. We discuss conditional independencies in
augmented SPNs, formally establish the probabilistic interpretation of the
sum-weights and give an interpretation of augmented SPNs as Bayesian networks.
Based on these results, we find a sound derivation of the EM algorithm for
SPNs, which was presented mistaken in literature. Furthermore, the
Viterbi-style algorithm for MPE proposed in literature was never proven to be
correct. We show that this is indeed a correct algorithm, when applied to
augmented SPNs. Our theoretical results are confirmed in experiments on
synthetic data and 103 real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06181</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06181</id><created>2016-01-22</created><authors><author><keyname>Nguyen</keyname><forenames>Viet T.</forenames></author><author><keyname>Jose</keyname><forenames>Jubin</forenames></author><author><keyname>Wu</keyname><forenames>Xinzhou</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author></authors><title>Secure Content Distribution in Vehicular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dedicated short range communication (DSRC) relies on secure distribution to
vehicles of a certificate revocation list (CRL) for enabling security
protocols. CRL distribution utilizing vehicle-to-vehicle (V2V) communications
is preferred to an infrastructure-only approach. One approach to V2V CRL
distribution, using rateless coding at the source and forwarding at vehicle
relays is vulnerable to a pollution attack in which a few malicious vehicles
forward incorrect packets which then spread through the network leading to
denial-of-service. This paper develops a new scheme called Precode-and-Hash
that enables efficient packet verification before forwarding thereby preventing
the pollution attack. In contrast to rateless codes, it utilizes a fixed
low-rate precode and random selection of packets from the set of precoded
packets. The fixed precode admits efficient hash verification of all encoded
packets. Specifically, hashes are computed for all precoded packets and sent
securely using signatures. We analyze the performance of the Precode-and-Hash
scheme for a multi-hop line network and provide simulation results for several
schemes in a more realistic vehicular model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06183</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06183</id><created>2016-01-22</created><authors><author><keyname>Platzer</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>A Complete Uniform Substitution Calculus for Differential Dynamic Logic</title><categories>cs.LO cs.PL math.LO</categories><comments>Long article extending the conference version that appeared at CADE
  2015, arXiv:1503.01981</comments><msc-class>03F03, 03B70, 34A38</msc-class><acm-class>F.4.1; F.3.1; F.3.2; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a relatively complete proof calculus for differential
dynamic logic (dL) that is entirely based on uniform substitution, a proof rule
that substitutes a formula for a predicate symbol everywhere. Uniform
substitutions make it possible to rely on concrete axioms rather than axiom
schemata, substantially simplifying implementations. Instead of subtle schema
variables and soundness-critical side conditions on the occurrence patterns of
logical variables to restrict their infinitely many schema instances to sound
ones, the resulting calculus adopts only a finite number of ordinary dL
formulas as axioms, which uniform substitutions instantiate soundly. The static
semantics of differential dynamic logic and the soundness-critical restrictions
it imposes on proof steps is captured exclusively in uniform substitutions and
variable renamings as opposed to being spread in delicate ways across the
prover implementation. In addition to sound uniform substitutions, this article
introduces differential forms for differential dynamic logic that make it
possible to internalize differential invariants, differential substitutions,
and derivations as first-class axioms to reason about differential equations.
The axiomatization is proved to be sound and relatively complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06184</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06184</id><created>2016-01-22</created><authors><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Qin</keyname><forenames>Minghai</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author><author><keyname>Jiang</keyname><forenames>Anxiao</forenames></author><author><keyname>Bandic</keyname><forenames>Zvonimir</forenames></author></authors><title>Joint Source-Channel Decoding of Polar Codes for Language-Based Source</title><categories>cs.IT math.IT</categories><comments>Single column, 20 pages, 8 figures, to be submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exploit the redundancy of the language-based source to help polar
decoding. By judging the validity of decoded words in the decoded sequence with
the help of a dictionary, the polar list decoder constantly detects erroneous
paths after every few bits are decoded. This path-pruning technique based on
joint decoding has advantages over stand-alone polar list decoding in that most
decoding errors in early stages are corrected. In order to facilitate the joint
decoding, we first propose a construction of dynamic dictionary using a trie
and show an efficient way to trace the dictionary during decoding. Then we
propose a joint decoding scheme of polar codes taking into account both
information from the channel and the source. The proposed scheme has the same
decoding complexity as the list decoding of polar codes. A list-size adaptive
joint decoding is further implemented to largely reduce the decoding
complexity. We conclude by simulation that the joint decoding schemes
outperform stand-alone polar codes with CRC-aided successive cancellation list
decoding by over 0.6 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06198</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06198</id><created>2016-01-22</created><authors><author><keyname>Bernardo</keyname><forenames>Marco</forenames></author><author><keyname>Miculan</keyname><forenames>Marino</forenames></author></authors><title>Disjunctive Probabilistic Modal Logic is Enough for Bisimilarity on
  Reactive Probabilistic Systems</title><categories>cs.LO</categories><acm-class>F.1.2; F.3.1; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Larsen and Skou characterized probabilistic bisimilarity over reactive
probabilistic systems with a logic including true, negation, conjunction, and a
diamond modality decorated with a probabilistic lower bound. Later on,
Desharnais, Edalat, and Panangaden showed that negation is not necessary to
characterize the same equivalence. In this paper, we prove that the logical
characterization holds also when conjunction is replaced by disjunction. To
this end, we introduce &quot;reactive probabilistic trees&quot;, a fully abstract model
for reactive probabilistic systems that allows us to demonstrate expressivity
of the disjunctive probabilistic modal logic, as well as of the previously
mentioned logics, by means of a compactness argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06201</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06201</id><created>2016-01-22</created><authors><author><keyname>Khanduri</keyname><forenames>Prashant</forenames></author><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Universal Collaboration Strategies for Signal Detection: A Sparse
  Learning Approach</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of high dimensional signal detection in a
large distributed network. In contrast to conventional distributed detection,
the nodes in the network can update their observations by combining
observations from other one-hop neighboring nodes (spatial collaboration).
Under the assumption that only a small subset of nodes are capable of
communicating with the Fusion Center (FC), our goal is to design optimal
collaboration strategies which maximize the detection performance at the FC.
Note that, if one optimizes the system for the detection of a single known
signal then the network cannot generalize well to other detection tasks. Hence,
we propose to design optimal collaboration strategies which are universal for a
class of equally probable deterministic signals. By establishing the
equivalence between the collaboration strategy design problem and Sparse PCA,
we seek the answers to the following questions: 1) How much do we gain from
optimizing the collaboration strategy? 2) What is the effect of dimensionality
reduction for different sparsity constraints? 3) How much do we lose in terms
of detection performance by adopting a universal system (cost of universality)?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06202</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06202</id><created>2016-01-22</created><authors><author><keyname>Moura</keyname><forenames>Jose</forenames></author><author><keyname>Edwards</keyname><forenames>Christopher</forenames></author></authors><title>Future Trends and Challenges for Mobile and Convergent Networks</title><categories>cs.NI</categories><comments>In book 4G &amp; Beyond: The Convergence of Networks, Devices and
  Services, Nova Science Publishers, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some traffic characteristics like real-time, location-based, and
community-inspired, as well as the exponential increase on the data traffic in
mobile networks, are challenging the academia and standardization communities
to manage these networks in completely novel and intelligent ways, otherwise,
current network infrastructures can not offer a connection service with an
acceptable quality for both emergent traffic demand and application requisites.
In this way, a very relevant research problem that needs to be addressed is how
a heterogeneous wireless access infrastructure should be controlled to offer a
network access with a proper level of quality for diverse flows ending at
multi-mode devices in mobile scenarios. The current chapter reviews recent
research and standardization work developed under the most used wireless access
technologies and mobile access proposals. It comprehensively outlines the
impact on the deployment of those technologies in future networking
environments, not only on the network performance but also in how the most
important requirements of several relevant players, such as, content providers,
network operators, and users/terminals can be addressed. Finally, the chapter
concludes referring the most notable aspects in how the environment of future
networks are expected to evolve like technology convergence, service
convergence, terminal convergence, market convergence, environmental awareness,
energy-efficiency, self-organized and intelligent infrastructure, as well as
the most important functional requisites to be addressed through that
infrastructure such as flow mobility, data offloading, load balancing and
vertical multihoming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06203</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06203</id><created>2016-01-22</created><authors><author><keyname>Moura</keyname><forenames>Jose</forenames></author><author><keyname>Batista</keyname><forenames>Fernando</forenames></author><author><keyname>Cardoso</keyname><forenames>Elsa</forenames></author><author><keyname>Nunes</keyname><forenames>Luis</forenames></author></authors><title>Intelligent Management and Efficient Operation of Big Data</title><categories>cs.NI</categories><comments>In book Handbook of Research on Trends and Future Directions in Big
  Data and Web Intelligence, IGI Global, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter details how Big Data can be used and implemented in networking
and computing infrastructures. Specifically, it addresses three main aspects:
the timely extraction of relevant knowledge from heterogeneous, and very often
unstructured large data sources, the enhancement on the performance of
processing and networking (cloud) infrastructures that are the most important
foundational pillars of Big Data applications or services, and novel ways to
efficiently manage network infrastructures with high-level composed policies
for supporting the transmission of large amounts of data with distinct
requisites (video vs. non-video). A case study involving an intelligent
management solution to route data traffic with diverse requirements in a wide
area Internet Exchange Point is presented, discussed in the context of Big
Data, and evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06206</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06206</id><created>2016-01-22</created><updated>2016-01-27</updated><authors><author><keyname>Moura</keyname><forenames>Jose</forenames></author><author><keyname>Serrao</keyname><forenames>Carlos</forenames></author></authors><title>Security and Privacy Issues of Big Data</title><categories>cs.CR</categories><comments>In book Handbook of Research on Trends and Future Directions in Big
  Data and Web Intelligence, IGI Global, 2015</comments><doi>10.4018/978-1-4666-8505-5.ch002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter revises the most important aspects in how computing
infrastructures should be configured and intelligently managed to fulfill the
most notably security aspects required by Big Data applications. One of them is
privacy. It is a pertinent aspect to be addressed because users share more and
more personal data and content through their devices and computers to social
networks and public clouds. So, a secure framework to social networks is a very
hot topic research. This last topic is addressed in one of the two sections of
the current chapter with case studies. In addition, the traditional mechanisms
to support security such as firewalls and demilitarized zones are not suitable
to be applied in computing systems to support Big Data. SDN is an emergent
management solution that could become a convenient mechanism to implement
security in Big Data systems, as we show through a second case study at the end
of the chapter. This also discusses current relevant work and identifies open
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06207</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06207</id><created>2016-01-22</created><updated>2016-03-08</updated><authors><author><keyname>Nalci</keyname><forenames>Alican</forenames></author><author><keyname>Fedorov</keyname><forenames>Igor</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least
  Squares Problem</title><categories>cs.LG stat.ML</categories><comments>Under Review by IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a hierarchical Bayesian framework to obtain sparse
and non-negative solutions to the sparse non-negative least squares problem
(S-NNLS). We introduce a new family of scale mixtures, the Rectified Gaussian
Scale Mixture (R-GSM), to model the sparsity enforcing prior distribution for
the signal of interest. One advantage of the R-GSM prior is that through proper
choice of the mixing density it encompasses a wide variety of heavy tailed
distributions, such as the rectified Laplacian and rectified Student's t
distributions. Similar to the Gaussian Scale Mixture (GSM) approach, a Type II
Expectation-Maximization framework is developed to estimate the
hyper-parameters and obtain a point estimate of the parameter of interest. In
the proposed method, called rectified Sparse Bayesian Learning (R-SBL), we
provide two ways to perform the Expectation step; Markov-Chain Monte-Carlo
(MCMC) simulations and a simple yet effective diagonal approximation approach
(DA). Through numerical experiments we show that R-SBL outperforms existing
S-NNLS solvers in terms of both signal and support recovery and that the
proposed DA approach admits both computational efficiency and numerical
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06208</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06208</id><created>2016-01-22</created><updated>2016-02-02</updated><authors><author><keyname>Biason</keyname><forenames>Alessandro</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Improved Active Sensing Performance in Wireless Sensor Networks via
  Channel State Information - Extended Version</title><categories>cs.IT math.IT</categories><comments>Partially submitted to 2016 IEEE International Symposium on
  Information Theory (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active sensing refers to the process of choosing or tuning a set of sensors
in order to track an underlying system in an efficient and accurate way. In a
wireless environment, among the several kinds of features extracted by
traditional sensors, the communication channel can be used to further boost the
tracking performance and save energy. A joint tracking problem which considers
traditional measurements and channel together for tracking purposes is set up
and solved. The system is modeled as a partially observable Markov decision
problem and the properties of the cost-to-go function are used to reduce the
problem complexity. Numerical results show the advantages of our proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06209</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06209</id><created>2016-01-22</created><authors><author><keyname>Forenza</keyname><forenames>Antonio</forenames></author><author><keyname>Perlman</keyname><forenames>Stephen</forenames></author><author><keyname>Saibi</keyname><forenames>Fadi</forenames></author><author><keyname>Di Dio</keyname><forenames>Mario</forenames></author><author><keyname>van der Laan</keyname><forenames>Roger</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Achieving Large Multiplexing Gain in Distributed Antenna Systems via
  Cooperation with pCell Technology</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Asilomar Conference on Signals, Systems, and Computers, Nov.
  8-11th 2015, Pacific Grove, CA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present pCellTM technology, the first commercial-grade
wireless system that employs cooperation between distributed transceiver
stations to create concurrent data links to multiple users in the same
spectrum. First we analyze the per-user signal-to-interference-plus-noise ratio
(SINR) employing a geometrical spatial channel model to define volumes in space
of coherent signal around user antennas (or personal cells, i.e., pCells). Then
we describe the system architecture consisting of a general-purpose-processor
(GPP) based software-defined radio (SDR) wireless platform implementing a
real-time LTE protocol stack to communicate with off-the-shelf LTE devices.
Finally we present experimental results demonstrating up to 16 concurrent
spatial channels for an aggregate average spectral efficiency of 59.3 bps/Hz in
the downlink and 27.5 bps/Hz in the uplink, providing data rates of 200 Mbps
downlink and 25 Mbps uplink in 5 MHz of TDD spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06214</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06214</id><created>2016-01-22</created><authors><author><keyname>Chun</keyname><forenames>Il Yong</forenames></author><author><keyname>Adcock</keyname><forenames>Ben</forenames></author></authors><title>Compressed sensing and parallel acquisition</title><categories>cs.IT math.FA math.IT</categories><comments>38 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel acquisition systems arise in various applications in order to
moderate problems caused by insufficient measurements in single-sensor systems.
These systems allow simultaneous data acquisition in multiple sensors, thus
alleviating such problems by providing more overall measurements. In this work
we consider the combination of compressed sensing with parallel acquisition. We
establish the theoretical improvements of such systems by providing recovery
guarantees for which, subject to appropriate conditions, the number of
measurements required per sensor decreases linearly with the total number of
sensors. Throughout, we consider two different sampling scenarios -- distinct
(corresponding to independent sampling in each sensor) and identical
(corresponding to dependent sampling between sensors) -- and a general
mathematical framework that allows for a wide range of sensing matrices (e.g.,
subgaussian random matrices, subsampled isometries, random convolutions and
random Toeplitz matrices). We also consider not just the standard sparse signal
model, but also the so-called sparse in levels signal model. This model
includes both sparse and distributed signals and clustered sparse signals. As
our results show, optimal recovery guarantees for both distinct and identical
sampling are possible under much broader conditions on the so-called sensor
profile matrices (which characterize environmental conditions between a source
and the sensors) for the sparse in levels model than for the sparse model. To
verify our recovery guarantees we provide numerical results showing phase
transitions for a number of different multi-sensor environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06215</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06215</id><created>2016-01-22</created><updated>2016-02-18</updated><authors><author><keyname>Bardet</keyname><forenames>Magali</forenames></author><author><keyname>Dragoi</keyname><forenames>Vlad</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Algebraic Properties of Polar Codes From a New Polynomial Formalism</title><categories>cs.IT math.IT</categories><comments>14 pages * A reference to the work of Bernhard Geiger has been added
  (arXiv:1506.05231) * Lemma 3 has been changed a little bit in order to prove
  that Proposition 7.1 in arXiv:1506.05231 holds for any binary input symmetric
  channel</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes form a very powerful family of codes with a low complexity
decoding algorithm that attain many information theoretic limits in error
correction and source coding. These codes are closely related to Reed-Muller
codes because both can be described with the same algebraic formalism, namely
they are generated by evaluations of monomials. However, finding the right set
of generating monomials for a polar code which optimises the decoding
performances is a hard task and channel dependent. The purpose of this paper is
to reveal some universal properties of these monomials. We will namely prove
that there is a way to define a nontrivial (partial) order on monomials so that
the monomials generating a polar code devised fo a binary-input symmetric
channel always form a decreasing set.
  This property turns out to have rather deep consequences on the structure of
the polar code. Indeed, the permutation group of a decreasing monomial code
contains a large group called lower triangular affine group. Furthermore, the
codewords of minimum weight correspond exactly to the orbits of the minimum
weight codewords that are obtained from (evaluations) of monomials of the
generating set. In particular, it gives an efficient way of counting the number
of minimum weight codewords of a decreasing monomial code and henceforth of a
polar code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06222</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06222</id><created>2016-01-22</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Zamansky</keyname><forenames>Anna</forenames></author><author><keyname>Farchi</keyname><forenames>Eitan</forenames></author></authors><title>Towards a Human-Centred Approach in Modelling and Testing of
  Cyber-Physical Systems</title><categories>cs.SE</categories><comments>Preprint. Accepted to the Workshop on Automated Testing for
  Cyber-Physical Systems in the Cloud at ICPADS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to capture different levels of abstraction in a system model is
especially important for remote integration, testing/verification, and
manufacturing of cyber-physical systems (CPSs). However, the complexity of
modelling and testing of CPSs makes these processes extremely prone to human
error. In this paper we present our ongoing work on introducing human-centred
considerations into modelling and testing of CPSs, which allow for agile
iterative refinement processes of different levels of abstraction when errors
are discovered or missing information is completed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06223</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06223</id><created>2016-01-22</created><authors><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author><author><keyname>Oren</keyname><forenames>Joel</forenames></author><author><keyname>Soundararajan</keyname><forenames>Kannan</forenames></author></authors><title>Shapley Values in Weighted Voting Games with Random Weights</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the distribution of the well-studied Shapley--Shubik values in
weighted voting games where the agents are stochastically determined. The
Shapley--Shubik value measures the voting power of an agent, in typical
collective decision making systems. While easy to estimate empirically given
the parameters of a weighted voting game, the Shapley values are notoriously
hard to reason about analytically.
  We propose a probabilistic approach in which the agent weights are drawn
i.i.d. from some known exponentially decaying distribution. We provide a
general closed-form characterization of the highest and lowest expected Shapley
values in such a game, as a function of the parameters of the underlying
distribution. To do so, we give a novel reinterpretation of the stochastic
process that generates the Shapley variables as a renewal process. We
demonstrate the use of our results on the uniform and exponential
distributions. Furthermore, we show the strength of our theoretical predictions
on several synthetic datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06224</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06224</id><created>2016-01-22</created><authors><author><keyname>Yang</keyname><forenames>Yaoqing</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author></authors><title>Rate Distortion for Lossy In-network Function Computation: Information
  Dissipation and Sequential Reverse Water-Filling</title><categories>cs.IT math.IT</categories><comments>This paper will be submitted to the IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed lossy linear function computation in a
tree network. We examine two cases: (i) data aggregation (only one sink node
computes) and (ii) consensus (all nodes compute the same function). By
quantifying the information dissipation in distributed computing, we obtain
fundamental limits on network computation rate as a function of incremental
distortions (and hence incremental information dissipation) along the edges of
the network. The above characterization, based on the idea of sequential
information dissipation, offers an improvement over classical cut-set type
techniques which are based on overall distortions instead of incremental
distortions. Surprisingly, this information dissipation happens even at
infinite blocklength. Combining this observation with an inequality on the
dominance of mean-square measures over relative-entropy measures, we obtain
outer bounds on the rate distortion function that are tighter than classical
cut-set bounds by a difference which can be arbitrarily large in both data
aggregation and consensus. We also obtain inner bounds on the optimal rate
using random Gaussian coding, which differ from the outer bounds by
$\mathcal{O}(\sqrt{D})$, where $D$ is the overall distortion. The obtained
inner and outer bounds can provide insights on rate (bit) allocations for both
the data aggregation problem and the consensus problem. We show that for tree
networks, the rate allocation results have a mathematical structure similar to
classical reverse water-filling for parallel Gaussian sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06229</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06229</id><created>2016-01-22</created><authors><author><keyname>Ponniah</keyname><forenames>Jonathan</forenames></author><author><keyname>Xie</keyname><forenames>Liang-Liang</forenames></author></authors><title>An Achievable Rate Region for the Two-Way Multiple-Relay Channel</title><categories>cs.IT math.IT</categories><comments>This work was submitted in part to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an achievable rate-region for the two-way multiple-relay channel
using decode-and-forward block Markovian coding. We identify a conflict between
the information flow in both directions. This conflict leads to an intractable
number of decode-forward schemes and achievable rate regions, none of which are
universally better than the others. We introduce a new concept in
decode-forward coding called ranking, and discover that there is an underlying
structure to all of these rate regions expressed in the rank assignment.
Through this discovery, we characterize the complete achievable rate region
that includes all of the rate regions corresponding to the particular
decode-forward schemes. This rate region is an extension of existing results
for the two-way one-relay channel and the two-way two-relay channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06230</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06230</id><created>2016-01-22</created><authors><author><keyname>Hou</keyname><forenames>Jinghua</forenames></author></authors><title>Coping with Prospective Memory Failures: An Optimal Reminder System
  Design</title><categories>cs.HC</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forgetting is in common in daily life, and 50-80% everyday's forgetting is
due to prospective memory failures, which have significant impacts on our life.
More seriously, some of these memory lapses can bring fatal consequences such
as forgetting a sleeping infant in the back seat of a car. People tend to use
various techniques to improve their prospective memory performance. Setting up
a reminder is one of the most important techniques. The existing studies
provide evidences in support of using reminders to cope with prospective memory
failures. However, people are not satisfied with existing reminders because of
their limitations in different aspects including reliability, optimization, and
adaption.
  Through analysing the functions and features of existing reminder systems,
this book draft summarizes their advantages and limitations. We are motivated
to improve the performance of reminder systems. For the improvements, the
relevant theories and mechanisms of prospective memory from psychology must be
complied with, incorporated, and applied in this new study.
  Based on the literature review, a new reminder model is proposed, which
includes a novel reminder planer, a prospective memory based agent, and a
personalized user model. The reminder planer is responsible for determining the
optimal reminder plan (including the optimal number of reminders, the optimal
reminding schedule and the optimal reminding way). The prospective memory agent
is responsible for executing the reminding processes. The personalized user
model is proposed to learn from users' behaviors and preferences based on
human-system interactions and is responsible for adapting the reminder plan to
meet users' preferences as much as possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06233</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06233</id><created>2016-01-23</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Abbasi</keyname><forenames>Ehsan</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Precise Error Analysis of Regularized M-estimators in High-dimensions</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular approach for estimating an unknown signal from noisy, linear
measurements is via solving a so called \emph{regularized M-estimator}, which
minimizes a weighted combination of a convex loss function and of a convex
(typically, non-smooth) regularizer. We accurately predict the squared error
performance of such estimators in the high-dimensional proportional regime. The
random measurement matrix is assumed to have entries iid Gaussian, only minimal
and rather mild regularity conditions are imposed on the loss function, the
regularizer, and on the noise and signal distributions. We show that the error
converges in probability to a nontrivial limit that is given as the solution to
a minimax convex-concave optimization problem on four scalar optimization
variables. We identify a new summary parameter, termed the Expected Moreau
envelope to play a central role in the error characterization. The
\emph{precise} nature of the results permits an accurate performance comparison
between different instances of regularized M-estimators and allows to optimally
tune the involved parameters (e.g. regularizer parameter, number of
measurements). The key ingredient of our proof is the \emph{Convex Gaussian
Min-max Theorem} (CGMT) which is a tight and strengthened version of a
classical Gaussian comparison inequality that was proved by Gordon in 1988.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06239</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06239</id><created>2016-01-23</created><authors><author><keyname>Chang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Divide and Conquer Local Average Regression</title><categories>cs.LG math.ST stat.TH</categories><comments>34 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The divide and conquer strategy, which breaks a massive data set into a
series of manageable data blocks, and then combines the independent results of
data blocks to obtain a final decision, has been recognized as a
state-of-the-art method to overcome challenges of massive data analysis. In
this paper, we merge the divide and conquer strategy with local average
regression methods to infer the regressive relationship of input-output pairs
from a massive data set. After theoretically analyzing the pros and cons, we
find that although the divide and conquer local average regression can reach
the optimal learning rate, the restriction to the number of data blocks is a
bit strong, which makes it only feasible for small number of data blocks. We
then propose two variants to lessen (or remove) this restriction. Our results
show that these variants can achieve the optimal learning rate with much milder
restriction (or without such restriction). Extensive experimental studies are
carried out to verify our theoretical assertions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06241</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06241</id><created>2016-01-23</created><updated>2016-01-29</updated><authors><author><keyname>Qian</keyname><forenames>Zhenzhi</forenames></author><author><keyname>Ji</keyname><forenames>Bo</forenames></author><author><keyname>Srinivasan</keyname><forenames>Kannan</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Achieving Delay Rate-function Optimality in OFDM Downlink with
  Time-correlated Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been recent attempts to develop scheduling schemes for downlink
transmission in a single cell of a multi-channel (e.g., OFDM-based) cellular
network. These works have been quite promising in that they have developed
low-complexity index scheduling policies that are delay-optimal (in a large
deviation rate-function sense). However, these policies require that the
channel is ON or OFF in each time-slot with a fixed probability (i.e., there is
no memory in the system), while the reality is that due to channel fading and
doppler shift, channels are often time-correlated in these cellular systems.
Thus, an important open question is whether one can find simple index
scheduling policies that are delay-optimal even when the channels are
time-correlated. In this paper, we attempt to answer this question for
time-correlated ON/OFF channels. In particular, we show that the class of
oldest packets first (OPF) policies that give a higher priority to packets with
a large delay is delay rate-function optimal under two conditions: 1) The
channel is non-negatively correlated, and 2) The distribution of the OFF period
is geometric. We use simulations to further elucidate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06242</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06242</id><created>2016-01-23</created><authors><author><keyname>Safilian</keyname><forenames>Aliakbar</forenames></author><author><keyname>Maibaum</keyname><forenames>Tom</forenames></author></authors><title>Multiset Theories of Cardinality-based Feature Diagrams</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software product line engineering is a very common method for designing
complex software systems. Feature modeling is the most common approach to
specify product lines. The main part of a feature model is a special tree of
features called a feature diagram. Cardinality-based feature diagrams provide
the most expressive tool among the current feature diagram languages. The most
common characterization of the semantics of a cardinality-based diagram is the
set of flat multisets over features satisfying the constraints. However, this
semantics provides a poor abstract view of the diagram. We address this problem
by proposing another multiset theory for the cardinality-based feature diagram,
called the hierarchical theory of the diagram. We show that this semantics
captures all information of the diagram so that one can retrieve the diagram
from its hierarchical semantics. We also characterize sets of multisets, which
can provide a hierarchical semantics of some diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06243</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06243</id><created>2016-01-23</created><authors><author><keyname>He</keyname><forenames>Shiying</forenames></author><author><keyname>Zhou</keyname><forenames>Haiwei</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Cao</keyname><forenames>Wenfei</forenames></author><author><keyname>Han</keyname><forenames>Zhi</forenames></author></authors><title>Super-resolution reconstruction of hyperspectral images via low rank
  tensor modeling and total variation regularization</title><categories>cs.CV</categories><comments>submitted to IGARSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel approach to hyperspectral image
super-resolution by modeling the global spatial-and-spectral correlation and
local smoothness properties over hyperspectral images. Specifically, we utilize
the tensor nuclear norm and tensor folded-concave penalty functions to describe
the global spatial-and-spectral correlation hidden in hyperspectral images, and
3D total variation (TV) to characterize the local spatial-and-spectral
smoothness across all hyperspectral bands. Then, we develop an efficient
algorithm for solving the resulting optimization problem by combing the local
linear approximation (LLA) strategy and alternative direction method of
multipliers (ADMM). Experimental results on one hyperspectral image dataset
illustrate the merits of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06244</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06244</id><created>2016-01-23</created><authors><author><keyname>Li</keyname><forenames>Siyao</forenames></author></authors><title>Multi-agent System Design for Dummies</title><categories>cs.MA</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent technology, a new paradigm in software engineering, has received
attention from research and industry since 1990s. However, it is still not used
widely to date because it requires expertise on both programming and agent
technology; gaps among requirements, agent design, and agent deployment also
pose more difficulties. Goal Net methodology attempts to solve these issues
with a goal-oriented approach that resembles human behaviours, and an agent
designer that supports agent development using this philosophy. However, there
are limitations on existing Goal Net Designer, the design and modelling
component of the agent designer. Those limitations, including limited access,
difficult deployment, inflexibility in user operations, design workflows
against typical Goal Net methodology workflows, and lack of data protection,
have inhibited widespread adoption of Goal Net methodology.
  Motivated by this, this book focuses on improvements on Goal Net Designer. In
this project, Goal Net Designer is completely re-implemented using new
technology with optimised software architecture and design. It allows access
from all major desktop operating systems, as well as in web environment via all
modern browsers. Enhancements such as refined workflows, model validation tool,
access control, team collaboration tool, and link to compiler make Goal Net
Designer a fully functional and powerful Integrated Development Environment.
User friendliness and usability are greatly enhanced by simplifying user's
actions to accomplish their tasks. User behaviour logging and quantitative
feedback channel are also included to allow Goal Net Designer to continuously
evolve with the power of big data analytics in future. To evaluate the new Goal
Net Designer, a teachable agent has been developed with the help of Goal Net
Designer and the development process is illustrated in a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06245</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06245</id><created>2016-01-23</created><authors><author><keyname>Zeng</keyname><forenames>Zhiwei</forenames></author></authors><title>Artificial Persuasion in Pedagogical Games</title><categories>cs.AI cs.CY</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Persuasive Teachable Agent (PTA) is a special type of Teachable Agent which
incorporates a persuasion theory in order to provide persuasive and more
personalized feedback to the student. By employing the persuasion techniques,
the PTA seeks to maintain the student in a high motivation and high ability
state in which he or she has higher cognitive ability and his or her changes in
attitudes are more persistent. However, the existing model of the PTA still has
a few limitations. Firstly, the existing PTA model focuses on modelling the
PTA's ability to persuade, while does not model its ability to be taught by the
student and to practice the knowledge it has learnt. Secondly, the quantitative
model for computational processes in the PTA has low reusability. Thirdly,
there is still a gap between theoretical models and practical implementation of
the PTA.
  To address these three limitations, this book proposes an improved agent
model which follows a goal-oriented approach and models the PTA in its totality
by integrating the Persuasion Reasoning of the PTA with the Teachability
Reasoning and the Practicability Reasoning. The project also proposes a more
abstract and generalized quantitative model for the computations in the PTA.
With higher level of abstraction, the reusability of the quantitative model is
also improved. New system architecture is introduced to bridge the gap between
theoretical models and implementation of the PTA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06248</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06248</id><created>2016-01-23</created><authors><author><keyname>Koumura</keyname><forenames>Takuya</forenames></author><author><keyname>Okanoya</keyname><forenames>Kazuo</forenames></author></authors><title>Automatic recognition of element classes and boundaries in the birdsong
  with variable sequences</title><categories>q-bio.NC cs.LG cs.SD</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Researches on sequential vocalization often require analysis of vocalizations
in long continuous sounds. In such studies as developmental ones or studies
across generations in which days or months of vocalizations must be analyzed,
methods for automatic recognition would be strongly desired. Although methods
for automatic speech recognition for application purposes have been intensively
studied, blindly applying them for biological purposes may not be an optimal
solution. This is because, unlike human speech recognition, analysis of
sequential vocalizations often requires accurate extraction of timing
information. In the present study we propose automated systems suitable for
recognizing birdsong, one of the most intensively investigated sequential
vocalizations, focusing on the three properties of the birdsong. First, a song
is a sequence of vocal elements, called notes, which can be grouped into
categories. Second, temporal structure of birdsong is precisely controlled,
meaning that temporal information is important in song analysis. Finally, notes
are produced according to certain probabilistic rules, which may facilitate the
accurate song recognition. We divided the procedure of song recognition into
three sub-steps: local classification, boundary detection, and global
sequencing, each of which corresponds to each of the three properties of
birdsong. We compared the performances of several different ways to arrange
these three steps. As results, we demonstrated a hybrid model of a deep neural
network and a hidden Markov model is effective in recognizing birdsong with
variable note sequences. We propose suitable arrangements of methods according
to whether accurate boundary detection is needed. Also we designed the new
measure to jointly evaluate the accuracy of note classification and boundary
detection. Our methods should be applicable, with small modification and
tuning, to the songs in other species that hold the three properties of the
sequential vocalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06251</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06251</id><created>2016-01-23</created><authors><author><keyname>Davoudi</keyname><forenames>Homa</forenames></author><author><keyname>Kabir</keyname><forenames>Ehsanollah</forenames></author></authors><title>Using compatible shape descriptor for lexicon reduction of printed Farsi
  subwords</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This Paper presents a method for lexicon reduction of Printed Farsi subwords
based on their holistic shape features. Because of the large number of Persian
subwords variously shaped from a simple letter to a complex combination of
several connected characters, it is not easy to find a fixed shape descriptor
suitable for all subwords. In this paper, we propose to select the descriptor
according to the input shape characteristics. To do this, a neural network is
trained to predict the appropriate descriptor of the input image. This network
is implemented in the proposed lexicon reduction system to decide on the
descriptor used for comparison of the query image with the lexicon entries.
Evaluating the proposed method on a dataset of Persian subwords allows one to
attest the effectiveness of the proposed idea of dealing differently with
various query shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06258</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06258</id><created>2016-01-23</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author></authors><title>Parametric Bayesian Rejuvenation in Ambient Assisted Living through
  Software-based Thematic 5G Management</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ameliorating elderly engagement is vital in rejuvenating independent living.
However, recommended practices lack realization of personal traits despite
socio -economic promise. The recent proliferation of IoT with the advent of
smart-objects/things and personalized services pave the way for context-aware
service management. Eventually, the major goal of this paper is to develop a
context-aware model in predicting engagement of elderly care. Hence, key
requirements are identified for elderly engagement, namely (a) discovery of
contexts, which are relevant (b) scaling up (over time) of engagement. However,
paramount challenges are imposed on this stipulation, such as,
un-observability, independence and composite relationship of contexts.
Therefore, a Topic-model based model is proposed to address scalability of
contexts and its conjugal relationship with engagement. Eventually, systematic
framework is demonstrated, which pinpoints key goals of context-aware services
by participants' opinions, usage and feed-back.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06259</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06259</id><created>2016-01-23</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Isenberg</keyname><forenames>David</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Minimax Lower Bounds for Linear Independence Testing</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear independence testing is a fundamental information-theoretic and
statistical problem that can be posed as follows: given $n$ points
$\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distribution
where $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^T
X$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in
\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n
\to \infty$, $(p+q)/n \leq \kappa &lt; \infty$, without sparsity assumptions). In
summary, our results imply that $n$ must be at least as large as $\sqrt
{pq}/\|\Sigma_{XY}\|_F^2$ for any procedure (test) to have non-trivial power,
where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also provide
some evidence that the lower bound is tight, by connections to two-sample
testing and regression in specific settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06260</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06260</id><created>2016-01-23</created><authors><author><keyname>Wang</keyname><forenames>Taiqing</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author><author><keyname>Zhu</keyname><forenames>Xiatian</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author></authors><title>Person Re-Identification by Discriminative Selection in Video Ranking</title><categories>cs.CV</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current person re-identification (ReID) methods typically rely on
single-frame imagery features, whilst ignoring space-time information from
image sequences often available in the practical surveillance scenarios.
Single-frame (single-shot) based visual appearance matching is inherently
limited for person ReID in public spaces due to the challenging visual
ambiguity and uncertainty arising from non-overlapping camera views where
viewing condition changes can cause significant people appearance variations.
In this work, we present a novel model to automatically select the most
discriminative video fragments from noisy/incomplete image sequences of people
from which reliable space-time and appearance features can be computed, whilst
simultaneously learning a video ranking function for person ReID. Using the
PRID$2011$, iLIDS-VID, and HDA+ image sequence datasets, we extensively
conducted comparative evaluations to demonstrate the advantages of the proposed
model over contemporary gait recognition, holistic image sequence matching and
state-of-the-art single-/multi-shot ReID methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06262</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06262</id><created>2016-01-23</created><authors><author><keyname>Keller</keyname><forenames>Matthias</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>Response-Time-Optimized Distributed Cloud Resource Allocation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A current trend in networking and cloud computing is to provide compute
resources over widely dispersed places exemplified by initiatives like Network
Function Virtualisation. This paves the way for a widespread service deployment
and can improve service quality; a nearby server can reduce the user-perceived
response times. But always using the nearest server is a bad decision if that
server is already highly utilized.
  This paper investigates the optimal assignment of users to widespread
resources -- a convex capacitated facility location problem with integrated
queuing systems. We determine the response times depending on the number of
used resources. This enables service providers to balance between resource
costs and the corresponding service quality. We also present a linear problem
reformulation showing small optimality gaps and faster solving times; this
speed-up enables a swift reaction to demand changes. Finally, we compare
solutions by either considering or ignoring queuing systems and discuss the
response time reduction by using the more complex model. Our investigations are
backed by large-scale numerical evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06274</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06274</id><created>2016-01-23</created><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author><author><keyname>Reinbacher</keyname><forenames>Christian</forenames></author><author><keyname>Graber</keyname><forenames>Gottfried</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>Solving Dense Image Matching in Real-Time using Discrete-Continuous
  Optimization</title><categories>cs.CV</categories><comments>21 st Computer Vision Winter Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dense image matching is a fundamental low-level problem in Computer Vision,
which has received tremendous attention from both discrete and continuous
optimization communities. The goal of this paper is to combine the advantages
of discrete and continuous optimization in a coherent framework. We devise a
model based on energy minimization, to be optimized by both discrete and
continuous algorithms in a consistent way. In the discrete setting, we propose
a novel optimization algorithm that can be massively parallelized. In the
continuous setting we tackle the problem of non-convex regularizers by a
formulation based on differences of convex functions. The resulting hybrid
discrete-continuous algorithm can be efficiently accelerated by modern GPUs and
we demonstrate its real-time performance for the applications of dense stereo
matching and optical flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06278</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06278</id><created>2016-01-23</created><authors><author><keyname>Pierre</keyname><forenames>Laurent</forenames></author></authors><title>Fortuitous sequences of flips of the top of a stack of n burnt pancakes
  for all n&gt;24</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Burnt pancakes problem was defined by Gates and Papadimitriou in 1979. A
stack S of pancakes with a burnt side must be sorted by size, the smallest on
top, and each pancake with burnt side down. Only operation allowed is to split
stack in two parts and flip upper part. g(S) is the minimal number of flips
needed to sort stack S. Stack S may be -In when pancakes are in right order but
upside down or -f_n when all pancakes are right side up but sorted in reverse
order. Gates et al. proved that g(-f_n)&gt;=3n/2-1.
  In 1995 Cohen and Blum proved that g(-I_n)=g(-f_n)+1&gt;=3n/2.
  In 1997 Heydari and Sudborough proved that g(-I_n)&lt;= 3(n+1)/2 whenever some
fortuitous sequence of flips exists. They gave fortuitous sequences for n=3,
15, 27 and 31. They showed that two fortuitous sequences S_n and S_n' may
combine into another fortuitous sequence S_n'' with n''=n+n'-3. So a fortuitous
sequence S_n gives a fortuitous sequence S_{n+12}. This proves that
g(-I_n)&lt;=3(n+1)/2 if n is congruent to 3 modulo 4 and n&gt;=23.
  In 2011 Josef Cibulka enhanced Gates and Papadimitriou's lower bound thanks
to a potential function. He got so g(-I_n)&gt;=3n/2+1 if n&gt;1 proving thereby, that
g(-I_n)=3(n+1)/2 if n is congruent to 3 modulo 4 and n&gt;=23.
  This paper explains how to build generalized fortuitous sequences for every
n&gt;=25 proving thereby that g(-I_n)=ceiling(3n/2)+1 for every n&gt;=25.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06280</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06280</id><created>2016-01-23</created><authors><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author></authors><title>Sub-Quadratic Decoding of Gabidulin Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to IEEE International Symposium on Information
  Theory 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how to decode errors and erasures with Gabidulin codes in
sub-quadratic time in the code length, improving previous algorithms which had
at least quadratic complexity. The complexity reduction is achieved by
accelerating operations on linearized polynomials. In particular, we present
fast algorithms for division, multi-point evaluation and interpolation of
linearized polynomials and show how to efficiently compute minimal subspace
polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06289</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06289</id><created>2016-01-23</created><authors><author><keyname>Cusick</keyname><forenames>James</forenames></author></authors><title>Considerations for Cloud Security Operations</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Security in Cloud Computing environments is explored. Cloud
Computing is presented, security needs are discussed, and mitigation approaches
are listed. Topics covered include Information Security, Cloud Computing,
Private Cloud, Public Cloud, SaaS, PaaS, IaaS, ISO 27001, OWASP, Secure SDLC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06291</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06291</id><created>2016-01-23</created><authors><author><keyname>Narayanaswamy</keyname><forenames>N S</forenames></author><author><keyname>Rahul</keyname><forenames>C S</forenames></author></authors><title>A Characterization for the Existence of Connected $f$-Factors of
  $\textit{ Large}$ Minimum Degree</title><categories>cs.DS cs.CC cs.DM</categories><comments>10 pages, Presented in 9th International colloquium on graph theory
  and combinatorics, 2014</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is well known that when $f(v)$ is a constant for each vertex $v$, the
connected $f$-factor problem is NP-Complete. In this note we consider the case
when $f(v) \geq \lceil \frac{n}{2.5}\rceil$ for each vertex $v$, where $n$ is
the number of vertices. We present a diameter based characterization of graphs
having a connected $f$-factor (for such $f$). We show that if a graph $G$ has a
connected $f$-factor and an $f$-factor with 2 connected components, then it has
a connected $f$-factor of diameter at least 3. This result yields a polynomial
time algorithm which first executes the Tutte's $f$-factor algorithm, and if
the output has 2 connected components, our algorithm searches for a connected
$f$-factor of diameter at least 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06292</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06292</id><created>2016-01-23</created><authors><author><keyname>Han</keyname><forenames>Qiwei</forenames></author><author><keyname>Ferreira</keyname><forenames>Pedro</forenames></author><author><keyname>Costeira</keyname><forenames>Jo&#xe3;o Paulo</forenames></author></authors><title>Asymmetric Peer Influence in Smartphone Adoption in a Large Mobile
  Network</title><categories>cs.SI cs.CY</categories><comments>12 pages, 3 figures, 14th International Conference on Mobile Business</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Understanding adoption patterns of smartphones is of vital importance to
telecommunication managers in today's highly dynamic mobile markets. In this
paper, we leverage the network structure and specific position of each
individual in the social network to account for and measure the potential
heterogeneous role of peer influence in the adoption of the iPhone 3G. We
introduce the idea of core/periphery as a meso-level organizational principle
to study the social network, which complements the use of centrality measures
derived from either global network properties (macro-level) or from each
individual's local social neighbourhood (micro-level). Using millions of call
detailed records from a mobile network operator in one country for a period of
eleven months, we identify overlapping social communities as well as core and
periphery individuals in the network. Our empirical analysis shows that core
users exert more influence on periphery users than vice versa. Our findings
provide important insights to help identify influential members in the social
network, which is potentially useful to design optimal targeting strategies to
improve current network-based marketing practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06296</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06296</id><created>2016-01-23</created><updated>2016-03-07</updated><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Weller</keyname><forenames>Katrin</forenames></author></authors><title>Think before you collect: Setting up a data collection approach for
  social media studies</title><categories>cs.CY cs.DL cs.SI</categories><comments>20 pages, 2 figures, accepted book chapter for the Handbook of Social
  Media Research Methods</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter discusses important challenges of designing the data collection
setup for social media studies. It outlines how it is necessary to carefully
think about which data to collect and to use, and to recognize the effects that
a specific data collection approach may have on the types of analyses that can
be carried out and the results that can be expected in a study. We will
highlight important questions one should ask before setting up a data
collection framework and relate them to the different options for accessing
social media data. The chapter will mainly be illustrated with examples from
studying Twitter and Facebook. A case study studying political communication
around the 2013 elections in Germany should serve as a practical application
scenario. In this case study we constructed several social media datasets based
on different collection approaches, using data from Facebook and Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06298</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06298</id><created>2016-01-23</created><authors><author><keyname>Sterling</keyname><forenames>Jonathan</forenames></author><author><keyname>Morrison</keyname><forenames>Darin</forenames></author></authors><title>Syntax and Semantics of Abstract Binding Trees</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contribution of this paper is the development of the syntax and semantics
of multi-sorted nominal abstract binding trees (abts), an extension of second
order universal algebra to support symbol-indexed families of operators.
Nominal abts are essential for correctly treating the syntax of languages with
generative phenomena, including exceptions and mutable state. Additionally we
have developed the categorical semantics for abstract binding trees formally in
Constructive Type Theory using the Agda proof assistant. Multi-sorted nominal
abts also form the syntactic basis for the upcoming version of the JonPRL proof
assistant, an implementation of an extensional constructive type theory in the
Nuprl tradition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06301</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06301</id><created>2016-01-23</created><updated>2016-02-06</updated><authors><author><keyname>Koczkodaj</keyname><forenames>Waldemar W.</forenames></author><author><keyname>Magnot</keyname><forenames>Jean-Pierre</forenames></author></authors><title>A Geometric Framework for the Inconsistency in Pairwise Comparisons</title><categories>math.LO cs.IT math.DG math.IT</categories><comments>v2: better presentation</comments><msc-class>03F25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a pairwise comparison matrix is generalized to the case when
coefficients create Lie group $G$, non necessarily abelian. A necessary and
sufficient criterion for pairwise comparisons matrices to be consistent is
provided. Basic criteria for finding a nearest consistent pairwise comparisons
matrix (extended to the class of group $G$) are proposed. A geometric
interpretation of pairwise comparisons matrices in terms of connections to a
simplex is given. Approximate reasoning is more effective when inconsistency in
data is reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06303</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06303</id><created>2016-01-23</created><authors><author><keyname>Kanovich</keyname><forenames>Max</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Stepan</forenames></author><author><keyname>Scedrov</keyname><forenames>Andre</forenames></author></authors><title>Undecidability of the Lambek calculus with a relevant modality</title><categories>math.LO cs.CL</categories><comments>12 pages</comments><msc-class>03B47</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morrill and Valentin in the paper &quot;Computational coverage of TLG:
Nonlinearity&quot; considered an extension of the Lambek calculus enriched by a
so-called &quot;exponential&quot; modality. This modality behaves in the style of
relevant logic. Namely, it allows contraction and permutation, but not
weakening. Morrill and Valentin pose the problem whether this system is
decidable. Here we give a negative answer. Our result remains valid if we
consider the fragment where all division operations have one direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06307</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06307</id><created>2016-01-23</created><authors><author><keyname>Hu</keyname><forenames>Xuegang</forenames></author><author><keyname>He</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Huizong</forenames></author><author><keyname>Pan</keyname><forenames>Jianhan</forenames></author></authors><title>Role-based Label Propagation Algorithm for Community Detection</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure of networks provides comprehensive insight into their
organizational structure and functional behavior. LPA is one of the most
commonly adopted community detection algorithms with nearly linear time
complexity. But it suffers from poor stability and occurrence of monster
community due to the introduced randomize. We note that different
community-oriented node roles impact the label propagation in different ways.
In this paper, we propose a role-based label propagation algorithm (roLPA), in
which the heuristics with regard to community-oriented node role were used. We
have evaluated the proposed algorithm on both real and artificial networks. The
result shows that roLPA is comparable to the state-of-the-art community
detection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06311</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06311</id><created>2016-01-23</created><authors><author><keyname>Das</keyname><forenames>Apurba</forenames></author><author><keyname>Svendsen</keyname><forenames>Michael</forenames></author><author><keyname>Tirthapura</keyname><forenames>Srikanta</forenames></author></authors><title>Change-Sensitive Algorithms for Maintaining Maximal Cliques in a Dynamic
  Graph</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the maintenance of the set of all maximal cliques in a dynamic
graph that is changing through the addition or deletion of edges. We present
nearly tight bounds on the magnitude of change in the set of maximal cliques,
as well as the first change-sensitive algorithms for clique maintenance, whose
runtime is proportional to the magnitude of the change in the set of maximal
cliques. We present experimental results showing that these algorithms are
efficient in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06312</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06312</id><created>2016-01-23</created><authors><author><keyname>Konstantinidis</keyname><forenames>Stavros</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rogerio</forenames></author></authors><title>Channels with Synchronization/Substitution Errors and Computation of
  Error Control Codes</title><categories>cs.IT math.IT</categories><msc-class>94B, 68P30</msc-class><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized algorithm that takes as input two positive integers
$N,\ell$ and a channel (=specification of the errors permitted), and computes
an error-detecting, or -correcting, block code having up to $N$ codewords of
length $\ell$. The channel could allow any rational combination of substitution
and synchronization errors. Moreover, if the algorithm finds less than $N$
codewords then those codewords constitute a code that, with high probability,
is close to maximal (in a certain precise sense defined here). We also present
some components of an open source Python package in which several code related
concepts have been implemented. A methodological contribution is the
presentation of how various error combinations can be expressed formally and
processed algorithmically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06316</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06316</id><created>2016-01-23</created><updated>2016-02-15</updated><authors><author><keyname>Silva</keyname><forenames>Arlei</forenames></author><author><keyname>Raghavendra</keyname><forenames>Ramya</forenames></author><author><keyname>Srivatsa</keyname><forenames>Mudhakar</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author></authors><title>Prediction-based Online Trajectory Compression</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent spatio-temporal data applications, such as car-shar\-ing and smart
cities, impose new challenges regarding the scalability and timeliness of data
processing systems. Trajectory compression is a promising approach for scaling
up spatio-temporal databases. However, existing techniques fail to address the
online setting, in which a compressed version of a trajectory stream has to be
maintained over time. In this paper, we introduce ONTRAC, a new framework for
map-matched online trajectory compression. ONTRAC learns prediction models for
suppressing updates to a trajectory database using training data. Two
prediction schemes are proposed, one for road segments via a Markov model and
another for travel-times by combining Quadratic Programming and Expectation
Maximization. Experiments show that ONTRAC outperforms the state-of-the-art
offline technique even when long update delays (4 mininutes) are allowed and
achieves up to 21 times higher compression ratio for travel-times. Moreover,
our approach increases database scalability by up to one order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06319</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06319</id><created>2016-01-23</created><updated>2016-01-27</updated><authors><author><keyname>Fenner</keyname><forenames>Stephen A.</forenames></author><author><keyname>Gurjar</keyname><forenames>Rohit</forenames></author><author><keyname>Thierauf</keyname><forenames>Thomas</forenames></author></authors><title>Bipartite Perfect Matching is in quasi-NC</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the bipartite perfect matching problem is in quasi-NC$^2$. That
is, it has uniform circuits of quasi-polynomial size $n^{O(\log n)}$, and
$O(log^2 n)$ depth. Previously, only an exponential upper bound was known on
the size of such circuits with poly-logarithmic depth.
  We obtain our result by an almost complete derandomization of the famous
Isolation Lemma when applied to yield an efficient randomized parallel
algorithm for the bipartite perfect matching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06321</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06321</id><created>2016-01-23</created><authors><author><keyname>Sadlier</keyname><forenames>Ronald J.</forenames></author><author><keyname>Humble</keyname><forenames>Travis S.</forenames></author></authors><title>Superdense Coding Interleaved with Forward Error Correction</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superdense coding promises increased classical capacity and communication
security but this advantage may be undermined by noise in the quantum channel.
We present a numerical study of how forward error correction (FEC) applied to
the encoded classical message can be used to mitigate against quantum channel
noise. By studying the bit error rate under different FEC codes, we identify
the unique role that burst errors play in superdense coding, and we show how
these can be mitigated against by interleaving the FEC codewords prior to
transmission. We conclude that classical FEC with interleaving is a useful
method to improve the performance in near-term demonstrations of superdense
coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06326</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06326</id><created>2016-01-23</created><authors><author><keyname>Arslan</keyname><forenames>Oktay</forenames></author><author><keyname>Berntorp</keyname><forenames>Karl</forenames></author><author><keyname>Tsiotras</keyname><forenames>Panagiotis</forenames></author></authors><title>Sampling-based Algorithms for Optimal Motion Planning Using Closed-loop
  Prediction</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion planning under differential constraints, kinodynamic motion planning,
is one of the canonical problems in robotics. Currently, state-of-the-art
methods evolve around kinodynamic variants of popular sampling-based
algorithms, such as Rapidly-exploring Random Trees (RRTs). However, there are
still challenges remaining, for example, how to include complex dynamics while
guaranteeing optimality. If the open-loop dynamics are unstable, exploration by
random sampling in control space becomes inefficient. We describe a new
sampling-based algorithm, called CL-RRT#, which leverages ideas from the RRT#
algorithm and a variant of the RRT algorithm that generates trajectories using
closed-loop prediction. The idea of planning with closed-loop prediction allows
us to handle complex unstable dynamics and avoids the need to find
computationally hard steering procedures. The search technique presented in the
RRT# algorithm allows us to improve the solution quality by searching over
alternative reference trajectories. Numerical simulations using a nonholonomic
system demonstrate the benefits of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06327</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06327</id><created>2016-01-23</created><authors><author><keyname>Kralevska</keyname><forenames>Katina</forenames></author><author><keyname>Gligoroski</keyname><forenames>Danilo</forenames></author><author><keyname>Oeverby</keyname><forenames>Harald</forenames></author></authors><title>Balanced XOR-ed Coding</title><categories>cs.IT math.IT</categories><comments>Advances in Communication Networking Volume 8115 of the series
  Lecture Notes in Computer Science pp 161-172</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns with the construction of codes over $GF(2)$ which reach
the max-flow for single source multicast acyclic networks with delay. The
coding is always a bitwise XOR of packets with equal lengths, and is based on
highly symmetrical and balanced designs. For certain setups and parameters, our
approach offers additional plausible security properties: an adversary needs to
eavesdrop at least max-flow links in order to decode at least one original
packet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06331</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06331</id><created>2016-01-23</created><updated>2016-03-07</updated><authors><author><keyname>Du</keyname><forenames>Yuhuan</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author></authors><title>Efficiency and Optimality of Largest Deficit First Prioritization:
  Resource Allocation for Real-Time Applications</title><categories>cs.NI</categories><comments>This is an extended version of this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of real-time applications with compute and/or
communication deadlines are being supported on shared infrastructure. Such
applications can often tolerate occasional deadline violations without
substantially impacting their Quality of Service (QoS). A fundamental problem
in such systems is deciding how to allocate shared resources so as to meet
applications' QoS requirements. A simple framework to address this problem is
to, (1) dynamically prioritize users as a possibly complex function of their
deficits (difference of achieved vs required QoS), and (2) allocate resources
so to expedite users with higher priority. This paper focuses on a general
class of systems using such priority-based resource allocation. We first
characterize the set of feasible QoS requirements and show the optimality of
max weight-like prioritization. We then consider simple weighted Largest
Deficit First (w-LDF) prioritization policies, where users with higher weighted
QoS deficits are given higher priority. The paper gives an inner bound for the
feasible set under w-LDF policies, and, under an additional monotonicity
assumption, characterizes its geometry leading to a sufficient condition for
optimality. Additional insights on the efficiency ratio of w-LDF policies, the
optimality of hierarchical-LDF and characterization of clustering of failures
are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06333</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06333</id><created>2016-01-23</created><updated>2016-03-07</updated><authors><author><keyname>Du</keyname><forenames>Yuhuan</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author></authors><title>Scheduling for Cloud-Based Computing Systems to Support Soft Real-Time
  Applications</title><categories>cs.NI</categories><comments>This is an extended version of this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud-based computing infrastructure provides an efficient means to support
real-time processing workloads, e.g., virtualized base station processing, and
collaborative video conferencing. This paper addresses resource allocation for
a computing system with multiple resources supporting heterogeneous soft
real-time applications subject to Quality of Service (QoS) constraints on
failures to meet processing deadlines. We develop a general outer bound on the
feasible QoS region for non-clairvoyant resource allocation policies, and an
inner bound for a natural class of policies based on dynamically prioritizing
applications' tasks by favoring those with the largest (QoS) deficits. This
provides an avenue to study the efficiency of two natural resource allocation
policies: (1) priority-based greedy task scheduling for applications with
variable workloads, and (2) priority-based task selection and optimal
scheduling for applications with deterministic workloads. The near-optimality
of these simple policies emerges when task processing deadlines are relatively
large and/or when the number of compute resources is large. Analysis and
simulations show substantial resource savings for such policies over
reservation-based designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06342</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06342</id><created>2016-01-23</created><authors><author><keyname>Hsieh</keyname><forenames>Sung-Hsien</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author><author><keyname>Pei</keyname><forenames>Soo-Chang</forenames></author></authors><title>Fast Binary Embedding via Circulant Downsampled Matrix -- A
  Data-Independent Approach</title><categories>cs.IT cs.CV cs.LG math.IT</categories><comments>8 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary embedding of high-dimensional data aims to produce low-dimensional
binary codes while preserving discriminative power. State-of-the-art methods
often suffer from high computation and storage costs. We present a simple and
fast embedding scheme by first downsampling N-dimensional data into
M-dimensional data and then multiplying the data with an MxM circulant matrix.
Our method requires O(N +M log M) computation and O(N) storage costs. We prove
if data have sparsity, our scheme can achieve similarity-preserving well.
Experiments further demonstrate that though our method is cost-effective and
fast, it still achieves comparable performance in image applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06345</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06345</id><created>2016-01-24</created><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Sung</keyname><forenames>Meng-Hsuan</forenames></author><author><keyname>Cheng</keyname><forenames>Shin-Ming</forenames></author></authors><title>Buffer Occupancy and Delivery Reliability Tradeoffs for Epidemic Routing</title><categories>cs.NI cs.SI</categories><comments>32 pages, 13 figures, submitted to IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve end-to-end delivery in intermittently connected networks, epidemic
routing is proposed for data delivery at the price of excessive buffer
occupancy due to its store-and-forward nature. The ultimate goal of epidemic
routing protocol design is to reduce system resource usage (e.g., buffer
occupancy) while simultaneously providing data delivery with statistical
guarantee. Therefore the tradeoffs between buffer occupancy and data delivery
reliability are of utmost importance. In this paper we investigate the
tradeoffs for two representative schemes: the global timeout scheme and the
antipacket dissemination scheme that are proposed for lossy and lossless data
delivery, respectively. For lossy data delivery, we show that with the
suggested global timeout value, the per-node buffer occupancy only depends on
the maximum tolerable packet loss rate and pairwise meeting rate. For lossless
data delivery, we show that the buffer occupancy can be significantly reduced
via fully antipacket dissemination. The developed tools therefore offer new
insights for epidemic routing protocol designs and performance evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06352</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06352</id><created>2016-01-24</created><authors><author><keyname>Chang</keyname><forenames>Kevin Kai-Wei</forenames></author><author><keyname>Lee</keyname><forenames>Donghyuk</forenames></author><author><keyname>Chishti</keyname><forenames>Zeshan</forenames></author><author><keyname>Alameldeen</keyname><forenames>Alaa R.</forenames></author><author><keyname>Wilkerson</keyname><forenames>Chris</forenames></author><author><keyname>Kim</keyname><forenames>Yoongu</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Reducing Performance Impact of DRAM Refresh by Parallelizing Refreshes
  with Accesses</title><categories>cs.AR</categories><comments>3 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern DRAM cells are periodically refreshed to prevent data loss due to
leakage. Commodity DDR DRAM refreshes cells at the rank level. This degrades
performance significantly because it prevents an entire rank from serving
memory requests while being refreshed. DRAM designed for mobile platforms,
LPDDR DRAM, supports an enhanced mode, called per-bank refresh, that refreshes
cells at the bank level. This enables a bank to be accessed while another in
the same rank is being refreshed, alleviating part of the negative performance
impact of refreshes. However, there are two shortcomings of per-bank refresh.
First, the per-bank refresh scheduling scheme does not exploit the full
potential of overlapping refreshes with accesses across banks because it
restricts the banks to be refreshed in a sequential round-robin order. Second,
accesses to a bank that is being refreshed have to wait.
  To mitigate the negative performance impact of DRAM refresh, we propose two
complementary mechanisms, DARP (Dynamic Access Refresh Parallelization) and
SARP (Subarray Access Refresh Parallelization). The goal is to address the
drawbacks of per-bank refresh by building more efficient techniques to
parallelize refreshes and accesses within DRAM. First, instead of issuing
per-bank refreshes in a round-robin order, DARP issues per-bank refreshes to
idle banks in an out-of-order manner. Furthermore, DARP schedules refreshes
during intervals when a batch of writes are draining to DRAM. Second, SARP
exploits the existence of mostly-independent subarrays within a bank. With
minor modifications to DRAM organization, it allows a bank to serve memory
accesses to an idle subarray while another subarray is being refreshed.
Extensive evaluations show that our mechanisms improve system performance and
energy efficiency compared to state-of-the-art refresh policies and the benefit
increases as DRAM density increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06357</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06357</id><created>2016-01-24</created><authors><author><keyname>Wang</keyname><forenames>Xiaomin</forenames></author><author><keyname>Yao</keyname><forenames>Bing</forenames></author><author><keyname>Xu</keyname><forenames>Jin</forenames></author></authors><title>Connections Between Several Distributions of Scale-free Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our empirical works, we find that there exists the equivalency of the
cumulative degree distribution and edge-cumulative distribution. Furthermore,
we employ three network models of the recursive graphs, Sierpinksi networks and
Apollonian networks to verify our conjecture: \emph{Both the cumulative degree
distribution and the edge-cumulative distribution are equivalent to each other
in deterministic network models}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06359</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06359</id><created>2016-01-24</created><authors><author><keyname>Sarcar</keyname><forenames>Sayan</forenames></author></authors><title>Usability Evaluation of Dwell-free Eye Typing Techniques</title><categories>cs.HC</categories><comments>11 Pages (with reference), accepted in IDHF 2014 conference held in
  Kochi, Japan</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dwelling is an essential task to be performed to select keys from an
on-screen keyboard present in the eye typing interface. This selection task can
be performed by fixing eye gaze on a key for a prolonged time. Spending
sufficient amount of time on each key effectively decreases the overall eye
typing rate. To address the problem, researchers proposed mechanisms, which
diminish the dwell time. We conducted a within-subject usability evaluation of
four dwell-free eye typing techniques. The results of first-time usability
study, longitudinal study and subjective evaluation conducted with 15
participants confirm the superiority of controlled eye movement based advanced
eye typing method (Adv-EyeK) than the other three techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06362</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06362</id><created>2016-01-24</created><authors><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Progress on High-rate MSR Codes: Enabling Arbitrary Number of Helper
  Nodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a construction for high-rate MDS codes that enable
bandwidth-efficient repair of a single node. Such MDS codes are also referred
to as the minimum storage regenerating (MSR) codes in the distributed storage
literature. The construction presented in this paper generates MSR codes for
all possible number of helper nodes $d$ as $d$ is a design parameter in the
construction. Furthermore, the obtained MSR codes have polynomial
sub-packetization (a.k.a. node size) $\alpha$. The construction is built on the
recent code proposed by Sasidharan et al. [1], which works only for $d = n-1$,
i.e., where all the remaining nodes serve as the helper nodes for the
bandwidth-efficient repair of a single node. The results of this paper broaden
the set of parameters where the constructions of MSR codes were known earlier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06368</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06368</id><created>2016-01-24</created><authors><author><keyname>Kolesov</keyname><forenames>A. E.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>P. N.</forenames></author></authors><title>Splitting schemes with respect to physical processes for double-porosity
  poroelasticity problems</title><categories>cs.CE math.NA</categories><comments>24 pages, 12 figures</comments><msc-class>34Q74, 65M12, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unsteady poroelasticity problem in fractured porous medium within
the classical Barenblatt double-porosity model. For numerical solution of
double-porosity poroelasticity problems we construct splitting schemes with
respect to physical processes, where transition to a new time level is
associated with solving separate problem for the displacements and fluid
pressures in pores and fractures. The stability of schemes is achieved by
switching to three-level explicit-implicit difference scheme with some of the
terms in the system of equations taken from the lower time level and by
choosing a weight parameter used as a regularization parameter. The
computational algorithm is based on the finite element approximation in space.
The investigation of stability of splitting schemes is based on the general
stability (well-posedness) theory of operator-difference schemes. A priori
estimates for proposed splitting schemes and the standard two-level scheme are
provided. The accuracy and stability of considered schemes are demonstrated by
numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06370</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06370</id><created>2016-01-24</created><authors><author><keyname>Jencova</keyname><forenames>Anna</forenames></author></authors><title>Comparison of quantum channels and statistical experiments</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, submitted to ISIT 2016. arXiv admin note: text overlap with
  arXiv:1512.07016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a pair of quantum channels with the same input space, we show that the
possibility of approximation of one channel by post-processings of the other
channel can be characterized by comparing the success probabilities for the two
ensembles obtained as outputs for any ensemble on the input space coupled with
an ancilla. This provides an operational interpretation to a natural extension
of Le Cam's deficiency to quantum channels. In particular, we obtain a version
of the randomization criterion for quantum statistical experiments. The proofs
are based on some properties of the diamond norm and its dual, which are of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06371</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06371</id><created>2016-01-24</created><authors><author><keyname>Degeling</keyname><forenames>Martin</forenames></author><author><keyname>Herrmann</keyname><forenames>Thomas</forenames></author></authors><title>Your Interests According to Google - A Profile-Centered Analysis for
  Obfuscation of Online Tracking Profiles</title><categories>cs.CY cs.CR</categories><acm-class>K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Profiling users for the purpose of targeted advertisements or other kinds of
personalization is very popular on the internet. But besides the benefits of
individually tailored news feeds and shopping recommendation research has shown
that many users consider this practice as privacy infringement. Profiling is
often conducted without consent and services offer neither information about
what the profile looks like nor which effects it might have. In this paper we
argue that understanding profiling and thus interacting with the resulting
profiles fosters a privacy literacy that is necessary for users to stay
autonomous in the information society. We analyze the extent of interest
profiling by google and develop a countermeasures that helps to obfuscate these
profiles. To do so we analyzed a links lists from a social bookmarking service
with regard to the interests they reveal. We found that, although the profiling
by google is very unstable we can still use the information to obfuscate the
profile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06372</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06372</id><created>2016-01-24</created><authors><author><keyname>Yiu</keyname><forenames>Neo C. K.</forenames></author></authors><title>An NFC-Enabled Anti-Counterfeiting System for Wine Industry</title><categories>cs.CY</categories><comments>65 pages</comments><doi>10.13140/RG.2.1.3762.2809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wine counterfeiting is not a new problem, however, the situation in China has
been going worse even after Hong Kong manifested itself as a wine trading and
distribution center with abolishing all taxes on wine in 2008. The most basic
method, printing a fake label with a subtly misspelled brand name or a slightly
different logo in hopes of fooling wine consumers, has been common to other
luxury-goods markets prone to counterfeiting. More ambitious counterfeiters
might remove an authentic label and place it on a bottle with a similar shape,
usually from the same vineyard, which contains a cheaper wine. Savvy buyers
could identify if the cork does not match the label, but how many normal
consumers like us could manage to identify the fake with only eye scanning?
  NFC facilitates processing of wine products information, making it a
promising technology for anti-counterfeiting. The proposed system is aimed at
relatively high-end consumer products like wine, and it helps protect genuine
wine by maintaining the product pedigree such as the transaction records and
the supply chain integrity. As such, consumers can safeguard their stake by
authenticating a specific wine with their NFC-enabled smartphones before making
payment at retail points.
  NFC has emerged as a potential tool to combat wine and spirit counterfeiting,
undermining international wine trading market and even the global economy
hugely. Recently, a number of anti-counterfeiting approaches have been proposed
and adopted utilising different authentication technologies for such purpose.
The project presents an NFC-enabled anti-counterfeiting system, and addresses
possible implementation issues, such as tag selection, tag programming and
encryption, setup of back-end database servers and the design of NFC mobile
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06375</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06375</id><created>2016-01-24</created><authors><author><keyname>Du</keyname><forenames>Xiaoni</forenames></author><author><keyname>Wan</keyname><forenames>Yunqi</forenames></author></authors><title>Linear code derived from the primage of quadratic function</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1506.06830 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes have been an interesting topic in both theory and practice for
many years. In this paper, for an odd prime power $q$, we construct some class
of linear code over finite field $\mathbb{F}_q$ with defining set be the
preimage of general quadratic form function and determine the explicit complete
weight enumerators of the linear codes. Our construction cover all the
corresponding result with quadratic form function and they may have
applications in cryptography and secret sharing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06376</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06376</id><created>2016-01-24</created><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Throughput Maximization for Mobile Relaying Systems</title><categories>cs.IT math.IT</categories><comments>submitted for possible conference publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a novel mobile relaying technique, where relays of high
mobility are employed to assist the communications from source to destination.
By exploiting the predictable channel variations introduced by relay mobility,
we study the throughput maximization problem in a mobile relaying system via
dynamic rate and power allocations at the source and relay. An optimization
problem is formulated for a finite time horizon, subject to an
information-causality constraint, which results from the data buffering
employed at the relay. It is found that the optimal power allocations across
the different time slots follow a &quot;stair-case&quot; water filling (WF) structure,
with non-increasing and non-decreasing water levels at the source and relay,
respectively. For the special case where the relay moves unidirectionally from
source to destination, the optimal power allocations reduce to the conventional
WF with constant water levels. Numerical results show that with appropriate
trajectory design, mobile relaying is able to achieve tremendous throughput
gain over the conventional static relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06377</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06377</id><created>2016-01-24</created><authors><author><keyname>Zhu</keyname><forenames>Jinxiao</forenames></author><author><keyname>Chen</keyname><forenames>Yin</forenames></author><author><keyname>Sasaki</keyname><forenames>Masahide</forenames></author></authors><title>Average Secrecy Capacity of Free-Space Optical Communication Systems
  with On-Off Keying Modulation and Threshold Detection</title><categories>cs.IT math.IT</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers secure communication over free-space optical (FSO) links
suffering from turbulence-induced fading. In particular, we study the average
secrecy capacity of applying physical layer security to achieve security in an
FSO communication system with binary-level on-off keying intensity modulation
and threshold detection, a widely used system in FSO communication. We first
define the instantaneous secrecy capacity for our system and then provide a
lower bound of it, which is achieved based on a simple design principle for
practical applications and yields a simpler equation. We next derive a general
expression of the average secrecy capacity and the corresponding lower bound
based on bit error probabilities. Finally, numerical analysis is conducted for
the special case of the correlated log-normal fading model to show the impacts
of fading and channel correlation on the average secrecy capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06383</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06383</id><created>2016-01-24</created><updated>2016-01-26</updated><authors><author><keyname>Wan</keyname><forenames>Kai</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>On Caching with More Users than Files</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching appears to be an efficient way to reduce peak hour network traffic
congestion by storing some content at the user's cache without knowledge of
later demands. Recently, Maddah-Ali and Niesen proposed a two-phase, placement
and delivery phase, coded caching strategy for centralized systems (where
coordination among users is possible in the placement phase), and for
decentralized systems. This paper investigates the same setup under the further
assumption that the number of users is larger than the number of files. By
using the same uncoded placement strategy of Maddah-Ali and Niesen, a novel
coded delivery strategy is proposed to profit from the multicasting
opportunities that arise because a file may be demanded by multiple users. The
proposed delivery method is proved to be optimal under the constraint of
uncoded placement for centralized systems with two files, moreover it is shown
to outperform known caching strategies for both centralized and decentralized
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06389</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06389</id><created>2016-01-24</created><authors><author><keyname>Kambhampaty</keyname><forenames>Shankar</forenames></author><author><keyname>Kambhampaty</keyname><forenames>Sasirekha</forenames></author></authors><title>Reference Architecture for SMAC solutions</title><categories>cs.NI cs.CY</categories><comments>9 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web and internet computing is evolving into a combination of social media,
mobile, analytics and cloud (SMAC) solutions. There is a need for an integrated
approach when developing solutions that address web scale requirements with
technologies that enable SMAC solutions. This paper presents an architecture
model for the integrated approach that can form the basis for solutions and
result in reuse, integration and agility for the business and IT in an
enterprise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06396</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06396</id><created>2016-01-24</created><updated>2016-02-09</updated><authors><author><keyname>Dokuchaev</keyname><forenames>Nikolai</forenames></author></authors><title>On detecting and quantification of randomness for one-sided sequences</title><categories>cs.IT math.IT math.PR</categories><msc-class>42A38, 93E10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies discrete time processes and their predictability and
randomness in deterministic pathwise setting, without using probabilistic
assumptions on the ensemble. We suggest some frequency based criterions of
predicability based on bandlimitness and some approaches to quantification of
randomness based on frequency analysis of two-sided and one-sided sequences. We
develop procedure allowing to represent an one-sided sequences as a sum of
left-bandlimited and predictable sequences and a non-reducible noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06403</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06403</id><created>2016-01-24</created><authors><author><keyname>Moharrer</keyname><forenames>Ali</forenames></author><author><keyname>Wei</keyname><forenames>Shuangqing</forenames></author><author><keyname>Amariucai</keyname><forenames>George T.</forenames></author><author><keyname>Deng</keyname><forenames>Jing</forenames></author></authors><title>Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An
  Information Theoretic Approach</title><categories>cs.IT cs.CV math.IT stat.ML</categories><comments>12 pages, 7 figures, Submitted to ISIT 2016 conference, Barcelona,
  Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Gaussian tree model learning algorithms lack in fully recovering the
sign information regarding pairwise correlation values between variables. Such
information is vital since it completely determines the direction in which two
variables are associated. In this work, we resort to information theoretical
approaches to quantify information in regard to lost correlation signs in the
recovered model. %We model the graphical model as a communication channel and
apply the tools in information theory to model the lost sign information. We
model the graphical model as a communication channel and propose a new layered
encoding framework to synthesize observed data using top layer Gaussian inputs
and independent Bernoulli correlation sign inputs from each layer. We show that
by maximizing the inferred information about the correlation sign information,
one may also find the largest achievable rate region for the rate tuples of
multi-layer latent Gaussian messages and missing correlation sign messages to
synthesize the desired observables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06405</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06405</id><created>2016-01-24</created><authors><author><keyname>Haddad</keyname><forenames>Serj</forenames></author><author><keyname>Leveque</keyname><forenames>Olivier</forenames></author></authors><title>Communication Tradeoffs in Wireless Networks</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1509.05856</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the maximum achievable broadcast rate in a wireless network
under various fading assumptions. Our result exhibits a duality between the
performance achieved in this context by collaborative beamforming strategies
and the number of degrees of freedom available in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06410</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06410</id><created>2016-01-24</created><updated>2016-01-26</updated><authors><author><keyname>Shenoy</keyname><forenames>K Gautam</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Finite Blocklength Achievable Rates for Energy Harvesting AWGN Channels
  with Infinite Buffer</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure; corrected typos and submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an additive White Gaussian channel where the transmitter is
powered by an energy harvesting source. For such a system, we provide a lower
bound on the maximal code book at finite code lengths that improves upon
previously known bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06421</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06421</id><created>2016-01-24</created><updated>2016-02-24</updated><authors><author><keyname>Kipnis</keyname><forenames>Alon</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Fundamental Distortion Limits of Analog-to-Digital Compression</title><categories>cs.IT math.IT</categories><comments>19 pages, 14 figures. Submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theory of minimizing distortion in reconstructing a stationary signal from
its compressed samples at a given bitrate is developed. We first analyze the
optimal sampling frequency required in order to achieve the optimal
distortion-rate tradeoff for a stationary bandlimited signal. To this end, we
consider a combined sampling and source coding problem in which an analog
Gaussian source is reconstructed from its encoded samples. We study this
problem under uniform filter-bank sampling and nonuniform sampling with
time-varying pre-processing. We show that for processes whose energy is not
uniformly distributed over their spectral support, each point on the
distortion-rate curve of the process corresponds to a sampling frequency
smaller than the Nyquist rate. This characterization can be seen as an
extension of the classical sampling theorem for bandlimited random processes in
the sense that it describes the minimal amount of excess distortion in the
reconstruction due to lossy compression of the samples, and provides the
minimal sampling frequency $f_{DR}$ required in order to achieve that
distortion. We compare the fundamental limits of combined source coding and
sampling, which we coin analog-to-digital compression, to the performance in
pulse code modulation (PCM), where each sample is quantized by a scalar
quantizer using a fixed number of bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06422</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06422</id><created>2016-01-24</created><authors><author><keyname>Davenport</keyname><forenames>Mark A.</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>An overview of low-rank matrix recovery from incomplete observations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrices play a fundamental role in modeling and computational
methods for signal processing and machine learning. In many applications where
low-rank matrices arise, these matrices cannot be fully sampled or directly
observed, and one encounters the problem of recovering the matrix given only
incomplete and indirect observations. This paper provides an overview of modern
techniques for exploiting low-rank structure to perform matrix recovery in
these settings, providing a survey of recent advances in this
rapidly-developing field. Specific attention is paid to the algorithms most
commonly used in practice, the existing theoretical guarantees for these
algorithms, and representative practical applications of these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06423</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06423</id><created>2016-01-24</created><authors><author><keyname>Zhu</keyname><forenames>Yan</forenames></author></authors><title>On Layered Erasure Interference Channels without CSI at Transmitters</title><categories>cs.IT math.IT</categories><comments>a long version of ISIT 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a layered erasure model for two-user interference
channels, which can be viewed as a simplified version of Gaussian fading
interference channel. It is assumed that channel state information~(CSI) is
only available at receivers but not at transmitters. Under such assumption, an
outer bound is derived for the capacity region of such interference channel.
The new outer bound is tight in many circumstances. For the remaining open
cases, the outer bound extends previous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06425</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06425</id><created>2016-01-24</created><authors><author><keyname>Gupta</keyname><forenames>Varun</forenames></author><author><keyname>Gutterman</keyname><forenames>Craig</forenames></author><author><keyname>Bejerano</keyname><forenames>Yigal</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Experimental Evaluation of Large Scale WiFi Multicast Rate Control</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiFi multicast to very large groups has gained attention as a solution for
multimedia delivery in crowded areas. Yet, most recently proposed schemes do
not provide performance guarantees and none have been tested at scale. To
address the issue of providing high multicast throughput with performance
guarantees, we present the design and experimental evaluation of the Multicast
Dynamic Rate Adaptation (MuDRA) algorithm. MuDRA balances fast adaptation to
channel conditions and stability, which is essential for multimedia
applications. MuDRA relies on feedback from some nodes collected via a
light-weight protocol and dynamically adjusts the rate adaptation response
time. Our experimental evaluation of MuDRA on the ORBIT testbed with over 150
nodes shows that MuDRA outperforms other schemes and supports high throughput
multicast flows to hundreds of receivers while meeting quality requirements.
MuDRA can support multiple high quality video streams, where 90% of the nodes
report excellent or very good video quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06426</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06426</id><created>2016-01-24</created><authors><author><keyname>Kalantari</keyname><forenames>Kousha</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand</forenames></author></authors><title>The Optimal Differential Privacy Mechanism under Hamming Distortion for
  Universal Memoryless Source Classes</title><categories>cs.IT math.IT</categories><comments>Extended abstract of ISIT 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To be considered for the 2016 IEEE Jack Keil Wolf ISIT Student Paper Award.
We develop the tradeoff between privacy, quantified using local differential
privacy (L-DP), and utility, quantified using Hamming distortion, for specific
classes of universal memoryless finite-alphabet sources. In particular, for the
class of permutation invariant sources (i.e., sources whose distributions are
invariant under permutations), the optimal L-DP mechanism is obtained. On the
other hand, for the class of sources with ordered statistics (i.e., for every
distribution $P=(P_1,P_2,...,P_M) \in \mathcal{P}, P_1 \ge P_2 \ge P_3 \ge
\ldots \ge P_M$), upper and lower bounds on the achievable local differential
privacy are derived with optimality results for specific range of distortions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06437</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06437</id><created>2016-01-24</created><updated>2016-03-08</updated><authors><author><keyname>Fritschek</keyname><forenames>Rick</forenames></author><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author></authors><title>Towards a Constant-Gap Sum-Capacity Result for the Gaussian Wiretap
  Channel with a Helper</title><categories>cs.IT math.IT</categories><comments>7 pages, short version submitted to ISIT 16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent investigations have shown that the sum secure degrees of freedom of
the Gaussian wiretap channel with a helper is $\tfrac{1}{2}$. The achievable
scheme for this result is based on the real interference alignment approach.
While providing a good way to show degrees of freedom results, this technique
has the disadvantage of relying on the Khintchine-Groshev theorem and is
therefore limited to {\it almost all channel gains}. This means that there are
infinitely many channel gains, where the scheme fails. Furthermore, the real
interference alignment approach cannot be used to yield stronger constant-gap
results. We approach this topic from a signal-scale alignment perspective and
use the linear deterministic model as a first approximation. Here we can show a
constant-gap sum capacity for certain channel gain parameters. We transfer
these results to the Gaussian model and discuss the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06439</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06439</id><created>2016-01-20</created><authors><author><keyname>Nwana</keyname><forenames>Amandianeze O.</forenames></author><author><keyname>Chen</keyname><forenames>Tsuhan</forenames></author></authors><title>Who Ordered This?: Exploiting Implicit User Tag Order Preferences for
  Personalized Image Tagging</title><categories>cs.IR cs.HC cs.MM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What makes a person pick certain tags over others when tagging an image? Does
the order that a person presents tags for a given image follow an implicit bias
that is personal? Can these biases be used to improve existing automated image
tagging systems? We show that tag ordering, which has been largely overlooked
by the image tagging community, is an important cue in understanding user
tagging behavior and can be used to improve auto-tagging systems. Inspired by
the assumption that people order their tags, we propose a new way of measuring
tag preferences, and also propose a new personalized tagging objective function
that explicitly considers a user's preferred tag orderings. We also provide a
(partially) greedy algorithm that produces good solutions to our new objective
and under certain conditions produces an optimal solution. We validate our
method on a subset of Flickr images that spans 5000 users, over 5200 tags, and
over 90,000 images. Our experiments show that exploiting personalized tag
orders improves the average performance of state-of-art approaches both on
per-image and per-user bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06440</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06440</id><created>2016-01-20</created><authors><author><keyname>Nwana</keyname><forenames>Amandianeze O.</forenames></author><author><keyname>Chen</keyname><forenames>Tsuhan</forenames></author></authors><title>QUOTE: &quot;Querying&quot; Users as Oracles in Tag Engines - A Semi-Supervised
  Learning Approach to Personalized Image Tagging</title><categories>cs.IR cs.LG cs.MM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One common trend in image tagging research is to focus on visually relevant
tags, and this tends to ignore the personal and social aspect of tags,
especially on photoblogging websites such as Flickr. Previous work has
correctly identified that many of the tags that users provide on images are not
visually relevant (i.e. representative of the salient content in the image) and
they go on to treat such tags as noise, ignoring that the users chose to
provide those tags over others that could have been more visually relevant.
Another common assumption about user generated tags for images is that the
order of these tags provides no useful information for the prediction of tags
on future images. This assumption also tends to define usefulness in terms of
what is visually relevant to the image. For general tagging or labeling
applications that focus on providing visual information about image content,
these assumptions are reasonable, but when considering personalized image
tagging applications, these assumptions are at best too rigid, ignoring user
choice and preferences.
  We challenge the aforementioned assumptions, and provide a machine learning
approach to the problem of personalized image tagging with the following
contributions: 1.) We reformulate the personalized image tagging problem as a
search/retrieval ranking problem, 2.) We leverage the order of tags, which does
not always reflect visual relevance, provided by the user in the past as a cue
to their tag preferences, similar to click data, 3.) We propose a technique to
augment sparse user tag data (semi-supervision), and 4.) We demonstrate the
efficacy of our method on a subset of Flickr images, showing improvement over
previous state-of-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06444</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06444</id><created>2016-01-24</created><updated>2016-02-18</updated><authors><author><keyname>Drakopoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Kontopoulos</keyname><forenames>Stavros</forenames></author><author><keyname>Makris</keyname><forenames>Christos</forenames></author><author><keyname>Megalooikonomou</keyname><forenames>Vasileios</forenames></author></authors><title>Large Graph Models: A Review</title><categories>cs.SI physics.soc-ph</categories><comments>We have identified errors in some equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large graphs can be found in a wide array of scientific fields ranging from
sociology and biology to scientometrics and computer science. Their analysis is
by no means a trivial task due to their sheer size and complex structure. Such
structure encompasses features so diverse as diameter shrinking, power law
degree distribution and self similarity, edge interdependence, and communities.
When the adjacency matrix of a graph is considered, then new, spectral
properties arise such as primary eigenvalue component decay function,
eigenvalue decay function, eigenvalue sign alternation around zero, and
spectral gap. Graph mining is the scientific field which attempts to extract
information and knowledge from graphs through their structural and spectral
properties. Graph modeling is the associated field of generating synthetic
graphs with properties similar to those of real graphs in order to simulate the
latter. Such simulations may be desirable because of privacy concerns, cost, or
lack of access to real data. Pivotal to simulation are low- and high-level
software packages offering graph analysis and visualization capabilities. This
survey outlines the most important structural and spectral graph properties, a
considerable number of graph models, as well the most common graph mining and
graph learning tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06447</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06447</id><created>2016-01-24</created><authors><author><keyname>Li</keyname><forenames>Shang</forenames></author><author><keyname>Li</keyname><forenames>Xiaoou</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Liu</keyname><forenames>Jingchen</forenames></author></authors><title>Sequential Hypothesis Test with Online Usage-Constrained Sensor
  Selection</title><categories>stat.AP cs.IT math.IT</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the sequential hypothesis testing problem with online
sensor selection and sensor usage constraints. That is, in a sensor network,
the fusion center sequentially acquires samples by selecting one &quot;most
informative&quot; sensor at each time until a reliable decision can be made. In
particular, the sensor selection is carried out in the online fashion since it
depends on all the previous samples at each time. Our goal is to develop the
sequential test (i.e., stopping rule and decision function) and sensor
selection strategy that minimize the expected sample size subject to the
constraints on the error probabilities and sensor usages. To this end, we first
recast the usage-constrained formulation into a Bayesian optimal stopping
problem with different sampling costs for the usage-contrained sensors. The
Bayesian problem is then studied under both finite- and infinite-horizon
setups, based on which, the optimal solution to the original usage-constrained
problem can be readily established. Moreover, by capitalizing on the structures
of the optimal solution, a lower bound is obtained for the optimal expected
sample size. In addition, we also propose algorithms to approximately evaluate
the parameters in the optimal sequential test so that the sensor usage and
error probability constraints are satisfied. Finally, numerical experiments are
provided to illustrate the theoretical findings, and compare with the existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06448</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06448</id><created>2016-01-24</created><authors><author><keyname>Jog</keyname><forenames>Varun</forenames></author><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author></authors><title>Analysis of centrality in sublinear preferential attachment trees via
  the CMJ branching process</title><categories>math.PR cs.DM cs.IT cs.SI math.IT math.ST stat.TH</categories><comments>21 pages</comments><msc-class>60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate centrality properties and the existence of a finite confidence
set for the root node in growing random tree models. We show that a continuous
time branching processes called the Crump-Mode-Jagers (CMJ) branching process
is well-suited to analyze such random trees, and establish centrality and root
inference properties of sublinear preferential attachment trees. We show that
with probability 1, there exists a persistent tree centroid; i.e., a vertex
that remains the tree centroid after a finite amount of time. Furthermore, we
show that the same centrality criterion produces a finite-sized $1 - \epsilon$
confidence set for the root node, for any $\epsilon &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06449</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06449</id><created>2016-01-24</created><authors><author><keyname>Gligoroski</keyname><forenames>Danilo</forenames></author><author><keyname>Kralevska</keyname><forenames>Katina</forenames></author><author><keyname>Oeverby</keyname><forenames>Harald</forenames></author></authors><title>Minimal Header Overhead for Random Linear Network Coding</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Communication Workshop (ICCW), 2015</comments><doi>10.1109/ICCW.2015.7247260</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy used to transmit a single bit of data between the devices in
wireless networks is equal to the energy for performing hundreds of
instructions in those devices. Thus the reduction of the data necessary to
transmit, while keeping the same functionality of the employed algorithms is a
formidable and challenging scientific task. We describe an algorithm called
Small Set of Allowed Coefficients (SSAC) that produces the shortest header
overhead in random linear network coding schemes compared with all other
approaches reported in the literature. The header overhead length is 2 to 7
times shorter than the length achieved by related compression techniques. For
example, SSAC algorithm compresses the length of the header overhead in a
generation of 128 packets to 24 bits, while the closest best result achieved by
an algorithm based on error correcting codes has a header overhead length of 84
bits in $GF(16)$ and 224 bits in $GF(256)$. We show that the header length in
SSAC does not depend on the size of the finite field where the operations are
performed, i.e., it just depends on the number of combined packets $m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06453</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06453</id><created>2016-01-24</created><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author></authors><title>Novel Lower Bounds on the Entropy Rate of Binary Hidden Markov Processes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Samorodnitsky proved a strengthened version of Mrs. Gerber's Lemma,
where the output entropy of a binary symmetric channel is bounded in terms of
the average entropy of the input projected on a random subset of coordinates.
Here, this result is applied for deriving novel lower bounds on the entropy
rate of binary hidden Markov processes. For symmetric underlying Markov
processes, our bound improves upon the best known bound in the very noisy
regime. The nonsymmetric case is also considered, and explicit bounds are
derived for Markov processes that satisfy the $(1,\infty)$-RLL constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06454</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06454</id><created>2016-01-24</created><authors><author><keyname>Melis</keyname><forenames>Luca</forenames></author><author><keyname>Asghar</keyname><forenames>Hassan Jameel</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author></authors><title>Private Processing of Outsourced Network Functions: Feasibility and
  Constructions</title><categories>cs.CR</categories><comments>A preliminary version of this paper appears in the 1st ACM
  International Workshop on Security in Software Defined Networks &amp; Network
  Function Virtualization. This is the full version</comments><doi>10.1145/2876019.2876021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aiming to reduce the cost and complexity of maintaining networking
infrastructures, organizations are increasingly outsourcing their network
functions (e.g., firewalls, traffic shapers and intrusion detection systems) to
the cloud, and a number of industrial players have started to offer network
function virtualization (NFV)-based solutions. Alas, outsourcing network
functions in its current setting implies that sensitive network policies, such
as firewall rules, are revealed to the cloud provider. In this paper, we
investigate the use of cryptographic primitives for processing outsourced
network functions, so that the provider does not learn any sensitive
information. More specifically, we present a cryptographic treatment of
privacy-preserving outsourcing of network functions, introducing security
definitions as well as an abstract model of generic network functions, and then
propose a few instantiations using partial homomorphic encryption and
public-key encryption with keyword search. We include a proof-of-concept
implementation of our constructions and show that network functions can be
privately processed by an untrusted cloud provider in a few milliseconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06456</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06456</id><created>2016-01-24</created><authors><author><keyname>Chen</keyname><forenames>Herman Z. Q.</forenames></author><author><keyname>Kitaev</keyname><forenames>Sergey</forenames></author><author><keyname>Sun</keyname><forenames>Brian Y.</forenames></author></authors><title>On universal partial words over binary alphabets</title><categories>math.CO cs.FL cs.IT math.IT</categories><comments>14 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be a non-empty alphabet and the symbol $\diamond\not\in A$. A partial
word (or pword) of length $n$ is $u=u_1\cdots u_n$ where $u_i\in
A\cup\{\diamond\}$. Partial words are studied in the literature in various
contexts.
  Let $A^n$ (resp., $A_\diamond^n$) be the set of all words (resp., pwords) of
length $n$ over $A$ (resp., $A\cup\{\diamond\}$). A word $u=u_1\cdots u_n\in
A^n$ occurs in a word $v=v_1\cdots v_m\in A_\diamond^m$ as a factor for $m\geq
n$, if there exists $i$ such that $u_j=v_{i+j}$ for $1\leq j\leq n$ whenever
$v_{i+j}\in A$. A universal word for $A^n$ is a word $w$ such that all factors
of $w$ of length $n$ are distinct and the set of these factors coincides with
$A^n$. It is a well known fact that universal words for $A^n$ exist for any
$n$.
  This paper extends the study of universal words to universal pwords
(u-pwords) defined as pwords containing exactly once each word in $A^n$ as a
factor. Our focus is in studying (non-)existence of u-pwords containing exactly
one $\diamond$, or two consecutive $\diamond$'s, over a binary alphabet. We
provide a number of general results, and conjecture complete classifications in
these cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06463</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06463</id><created>2016-01-24</created><authors><author><keyname>Davoodi</keyname><forenames>Arash Gholami</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Generalized Degrees of Freedom of the Symmetric K-User Interference
  Channel under Finite Precision CSIT</title><categories>cs.IT math.IT</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized degrees of freedom (GDoF) characterization of the symmetric
K-user interference channel is obtained under finite precision channel state
information at the transmitters (CSIT). The symmetric setting is where each
cross channel is capable of carrying degrees of freedom (DoF) while each direct
channel is capable of carrying 1 DoF. Remarkably, under finite precision CSIT
the symmetric K-user interference channel loses all the GDoF benefits of
interference alignment. The GDoF per user diminish with the number of users
everywhere except in the very strong (optimal for every receiver to decode all
messages) and very weak (optimal to treat all interference as noise)
interference regimes. The result stands in sharp contrast to prior work on the
symmetric setting under perfect CSIT, where the GDoF per user remain
undiminished due to interference alignment. The result also stands in contrast
to prior work on a subclass of asymmetric settings under finite precision CSIT,
i.e., the topological interference management problem, where interference
alignment plays a crucial role and provides substantial GDoF benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06466</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06466</id><created>2016-01-24</created><authors><author><keyname>Shin</keyname><forenames>Donghwan</forenames></author><author><keyname>Bae</keyname><forenames>Doo-Hwan</forenames></author></authors><title>A Theoretical Framework for Understanding Mutation-Based Testing Methods</title><categories>cs.SE</categories><comments>To be appear in ICST 2016</comments><acm-class>D.2.5, F.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of mutation analysis, mutation is the systematic generation of
mutated programs (i.e., mutants) from an original program. The concept of
mutation has been widely applied to various testing problems, including test
set selection, fault localization, and program repair. However, surprisingly
little focus has been given to the theoretical foundation of mutation-based
testing methods, making it difficult to understand, organize, and describe
various mutation-based testing methods.
  This paper aims to consider a theoretical framework for understanding
mutation-based testing methods. While there is a solid testing framework for
general testing, this is incongruent with mutation-based testing methods,
because it focuses on the correctness of a program for a test, while the
essence of mutation-based testing concerns the differences between programs
(including mutants) for a test.
  In this paper, we begin the construction of our framework by defining a novel
testing factor, called a test differentiator, to transform the paradigm of
testing from the notion of correctness to the notion of difference. We formally
define behavioral differences of programs for a set of tests as a mathematical
vector, called a d-vector. We explore the multi-dimensional space represented
by d-vectors, and provide a graphical model for describing the space. Based on
our framework and formalization, we interpret existing mutation-based fault
localization methods and mutant set minimization as applications, and identify
novel implications for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06468</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06468</id><created>2016-01-24</created><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>New Error Correcting Codes for Informed Receivers</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 tables. Keywords: Cyclic codes, index coding, informed
  receivers, maximum distance separable codes, side information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct error correcting codes for jointly transmitting a finite set of
independent messages to an 'informed receiver' which has prior knowledge of the
values of some subset of the messages as side information. The transmitter is
oblivious to the message subset already known to the receiver and performs
encoding in such a way that any possible side information can be used
efficiently at the decoder. We construct and identify several families of
algebraic error correcting codes for this problem using cyclic and maximum
distance separable (MDS) codes. The proposed codes are of short block length,
many of them provide optimum or near-optimum error correction capabilities and
guarantee larger minimum distances than known codes of similar parameters for
informed receivers. The constructed codes are also useful as error correcting
codes for index coding when the transmitter does not know the side information
available at the receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06473</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06473</id><created>2016-01-24</created><updated>2016-01-25</updated><authors><author><keyname>Wan</keyname><forenames>Weiwei</forenames></author><author><keyname>Lu</keyname><forenames>Feng</forenames></author><author><keyname>Wu</keyname><forenames>Zepei</forenames></author><author><keyname>Harada</keyname><forenames>Kensuke</forenames></author></authors><title>Teaching Robots to Do Object Assembly using Multi-modal 3D Vision</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The motivation of this paper is to develop a smart system using multi-modal
vision for next-generation mechanical assembly. It includes two phases where in
the first phase human beings teach the assembly structure to a robot and in the
second phase the robot finds objects and grasps and assembles them using AI
planning. The crucial part of the system is the precision of 3D visual
detection and the paper presents multi-modal approaches to meet the
requirements: AR markers are used in the teaching phase since human beings can
actively control the process. Point cloud matching and geometric constraints
are used in the robot execution phase to avoid unexpected noises. Experiments
are performed to examine the precision and correctness of the approaches. The
study is practical: The developed approaches are integrated with graph
model-based motion planning, implemented on an industrial robots and applicable
to real-world scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06474</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06474</id><created>2016-01-24</created><authors><author><keyname>Margolies</keyname><forenames>Robert</forenames></author><author><keyname>Grebla</keyname><forenames>Guy</forenames></author><author><keyname>Chen</keyname><forenames>Tingjun</forenames></author><author><keyname>Rubenstein</keyname><forenames>Dan</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Panda: Neighbor Discovery on a Power Harvesting Budget</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object tracking applications are gaining popularity and will soon utilize
Energy Harvesting (EH) low-power nodes that will consume power mostly for
Neighbor Discovery (ND) (i.e., identifying nodes within communication range).
Although ND protocols were developed for sensor networks, the challenges posed
by emerging EH low-power transceivers were not addressed. Therefore, we design
an ND protocol tailored for the characteristics of a representative EH
prototype: the TI eZ430-RF2500-SEH. We present a generalized model of ND
accounting for unique prototype characteristics (i.e., energy costs for
transmission/reception, and transceiver state switching times/costs). Then, we
present the Power Aware Neighbor Discovery Asynchronously (Panda) protocol in
which nodes transition between the sleep, receive, and transmit states. We
analyze \name and select its parameters to maximize the ND rate subject to a
homogeneous power budget. We also present Panda-D, designed for non-homogeneous
EH nodes. We perform extensive testbed evaluations using the prototypes and
study various design tradeoffs. We demonstrate a small difference (less then
2%) between experimental and analytical results, thereby confirming the
modeling assumptions. Moreover, we show that Panda improves the ND rate by up
to 3x compared to related protocols. Finally, we show that Panda-D operates
well under non-homogeneous power harvesting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06476</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06476</id><created>2016-01-24</created><authors><author><keyname>Hou</keyname><forenames>Jack P.</forenames></author><author><keyname>Emad</keyname><forenames>Amin</forenames></author><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author><author><keyname>Ma</keyname><forenames>Jian</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>A new correlation clustering method for cancer mutation analysis</title><categories>cs.LG q-bio.QM</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cancer genomes exhibit a large number of different alterations that affect
many genes in a diverse manner. It is widely believed that these alterations
follow combinatorial patterns that have a strong connection with the underlying
molecular interaction networks and functional pathways. A better understanding
of the generative mechanisms behind the mutation rules and their influence on
gene communities is of great importance for the process of driver mutations
discovery and for identification of network modules related to cancer
development and progression. We developed a new method for cancer mutation
pattern analysis based on a constrained form of correlation clustering.
Correlation clustering is an agnostic learning method that can be used for
general community detection problems in which the number of communities or
their structure is not known beforehand. The resulting algorithm, named $C^3$,
leverages mutual exclusivity of mutations, patient coverage, and driver network
concentration principles; it accepts as its input a user determined combination
of heterogeneous patient data, such as that available from TCGA (including
mutation, copy number, and gene expression information), and creates a large
number of clusters containing mutually exclusive mutated genes in a particular
type of cancer. The cluster sizes may be required to obey some useful soft size
constraints, without impacting the computational complexity of the algorithm.
To test $C^3$, we performed a detailed analysis on TCGA breast cancer and
glioblastoma data and showed that our algorithm outperforms the
state-of-the-art CoMEt method in terms of discovering mutually exclusive gene
modules and identifying driver genes. Our $C^3$ method represents a unique tool
for efficient and reliable identification of mutation patterns and driver
pathways in large-scale cancer genomics studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06496</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06496</id><created>2016-01-25</created><authors><author><keyname>Yan</keyname><forenames>Da</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author></authors><title>Lightweight Fault Tolerance in Large-Scale Distributed Graph Processing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of Google's Pregel framework in distributed graph processing has
inspired a surging interest in developing Pregel-like platforms featuring a
user-friendly &quot;think like a vertex&quot; programming model. Existing Pregel-like
systems support a fault tolerance mechanism called checkpointing, which
periodically saves computation states as checkpoints to HDFS, so that when a
failure happens, computation rolls back to the latest checkpoint. However, a
checkpoint in existing systems stores a huge amount of data, including vertex
states, edges, and messages sent by vertices, which significantly degrades the
failure-free performance. Moreover, the high checkpointing cost prevents
frequent checkpointing, and thus recovery has to replay all the computations
from a state checkpointed some time ago.
  In this paper, we propose a novel checkpointing approach which only stores
vertex states and incremental edge updates to HDFS as a lightweight checkpoint
(LWCP), so that writing an LWCP is typically tens of times faster than writing
a conventional checkpoint. To recover from the latest LWCP, messages are
generated from the vertex states, and graph topology is recovered by replaying
incremental edge updates. We show how to realize lightweight checkpointing with
minor modifications of the vertex-centric programming interface. We also apply
the same idea to a recently-proposed log-based approach for fast recovery, to
make it work efficiently in practice by significantly reducing the cost of
garbage collection of logs. Extensive experiments on large real graphs verified
the effectiveness of LWCP in improving both failure-free performance and the
performance of recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06497</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06497</id><created>2016-01-25</created><authors><author><keyname>Yan</keyname><forenames>Da</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>&#xd6;zsu</keyname><forenames>M. Tamer</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Lu</keyname><forenames>Yi</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Zhang</keyname><forenames>Qizhen</forenames></author><author><keyname>Ng</keyname><forenames>Wilfred</forenames></author></authors><title>Quegel: A General-Purpose Query-Centric Framework for Querying Big
  Graphs</title><categories>cs.DC cs.DB</categories><comments>This is a full version of our VLDB paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pioneered by Google's Pregel, many distributed systems have been developed
for large-scale graph analytics. These systems expose the user-friendly &quot;think
like a vertex&quot; programming interface to users, and exhibit good horizontal
scalability. However, these systems are designed for tasks where the majority
of graph vertices participate in computation, but are not suitable for
processing light-workload graph queries where only a small fraction of vertices
need to be accessed. The programming paradigm adopted by these systems can
seriously under-utilize the resources in a cluster for graph query processing.
In this work, we develop a new open-source system, called Quegel, for querying
big graphs, which treats queries as first-class citizens in the design of its
computing model. Users only need to specify the Pregel-like algorithm for a
generic query, and Quegel processes light-workload graph queries on demand
using a novel superstep-sharing execution model to effectively utilize the
cluster resources. Quegel further provides a convenient interface for
constructing graph indexes, which significantly improve query performance but
are not supported by existing graph-parallel systems. Our experiments verified
that Quegel is highly efficient in answering various types of graph queries and
is up to orders of magnitude faster than existing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06502</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06502</id><created>2016-01-25</created><authors><author><keyname>Maitin-Shepard</keyname><forenames>Jeremy</forenames></author><author><keyname>Tibouchi</keyname><forenames>Mehdi</forenames></author><author><keyname>Aranha</keyname><forenames>Diego</forenames></author></authors><title>Elliptic Curve Multiset Hash</title><categories>cs.CR</categories><comments>Implementation available on https://github.com/jbms/ecmh</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A homomorphic, or incremental, multiset hash function, associates a hash
value to arbitrary collections of objects (with possible repetitions) in such a
way that the hash of the union of two collections is easy to compute from the
hashes of the two collections themselves: it is simply their sum under a
suitable group operation. In particular, hash values of large collections can
be computed incrementally and/or in parallel. Homomorphic hashing is thus a
very useful primitive with applications ranging from database integrity
verification to streaming set/multiset comparison and network coding.
  Unfortunately, constructions of homomorphic hash functions in the literature
are hampered by two main drawbacks: they tend to be much longer than usual hash
functions at the same security level (e.g. to achieve a collision resistance of
2^128, they are several thousand bits long, as opposed to 256 bits for usual
hash functions), and they are also quite slow.
  In this paper, we introduce the Elliptic Curve Multiset Hash (ECMH), which
combines a usual bit string-valued hash function like BLAKE2 with an efficient
encoding into binary elliptic curves to overcome both difficulties. On the one
hand, the size of ECMH digests is essentially optimal: 2m-bit hash values
provide O(2^m) collision resistance. On the other hand, we demonstrate a
highly-efficient software implementation of ECMH, which our thorough empirical
evaluation shows to be capable of processing over 3 million set elements per
second on a 4 GHz Intel Haswell machine at the 128-bit security level---many
times faster than previous practical methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06503</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06503</id><created>2016-01-25</created><authors><author><keyname>Dongale</keyname><forenames>T. D.</forenames></author><author><keyname>Patil</keyname><forenames>P. J.</forenames></author><author><keyname>Desai</keyname><forenames>N. K.</forenames></author><author><keyname>Chougule</keyname><forenames>P. P.</forenames></author><author><keyname>Kumbhar</keyname><forenames>S. M.</forenames></author><author><keyname>Waifalkar</keyname><forenames>P. P.</forenames></author><author><keyname>Patil</keyname><forenames>P. B.</forenames></author><author><keyname>Vhatkar</keyname><forenames>R. S.</forenames></author><author><keyname>Takale</keyname><forenames>M. V.</forenames></author><author><keyname>Gaikwad</keyname><forenames>P. K.</forenames></author><author><keyname>Kamat</keyname><forenames>R. K.</forenames></author></authors><title>TiO2 based Nanostructured Memristor for RRAM and Neuromorphic
  Applications: A Simulation Approach</title><categories>cond-mat.mtrl-sci cs.ET</categories><comments>11 pages, 8 figures</comments><msc-class>65Zxx, 74K35, 82Dxx</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We report simulation of nanostructured memristor device using piecewise
linear and nonlinear window functions for RRAM and neuromorphic applications.
The linear drift model of memristor has been exploited for the simulation
purpose with the linear and non-linear window function as the mathematical and
scripting basis. The results evidences that the piecewise linear window
function can aptly simulate the memristor characteristics pertaining to RRAM
application. However, the nonlinear window function could exhibit the nonlinear
phenomenon in simulation only at the lower magnitude of control parameter. This
has motivated us to propose a new nonlinear window function for emulating the
simulation model of the memristor. Interestingly, the proposed window function
is scalable up to f(x)=1 and exhibits the nonlinear behavior at higher
magnitude of control parameter. Moreover, the simulation results of proposed
nonlinear window function are encouraging and reveals the smooth nonlinear
change from LRS to HRS and vice versa and therefore useful for the neuromorphic
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06504</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06504</id><created>2016-01-25</created><authors><author><keyname>Li</keyname><forenames>Yongming</forenames></author></authors><title>Quantitative Model Checking of Linear-Time Properties Based on
  Generalized Possibility Measures</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1409.6466</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking of linear-time properties based on possibility measures was
studied in previous work (Y. Li and L. Li, Model checking of linear-time
properties based on possibility measure, IEEE Transactions on Fuzzy Systems,
21(5)(2013), 842-854). However, the linear-time properties considered in the
previous work was classical and qualitative, possibility information of the
systems was not considered at all. We shall study quantitative model checking
of fuzzy linear-time properties based on generalized possibility measures in
the paper. Both the model of the system, as well as the properties the system
needs to adhere to, are described using possibility information to identify the
uncertainty in the model/properties. The systems are modeled by {\sl
generalized possibilistic Kripke structures} (GPKS, in short), and the
properties are described by fuzzy linear-time properties. Concretely, fuzzy
linear-time properties about reachability, always reachability, constrain
reachability, repeated reachability and persitence in GPKSs are introduced and
studied. Fuzzy regular safety properties and fuzzy $\omega-$regular properties
in GPKSs are introduced, the verification of fuzzy regular safety properties
and fuzzy $\omega-$regular properties using fuzzy finite automata are
thoroughly studied. It has been shown that the verification of fuzzy regular
safety properties and fuzzy $\omega-$regular properties in a finite GPKS can be
transformed into the verification of (always) reachability properties and
repeated reachability (persistence) properties in the product GPKS introduced
in this paper. Several examples are given to illustrate the methods presented
in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06517</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06517</id><created>2016-01-25</created><updated>2016-02-04</updated><authors><author><keyname>Lin</keyname><forenames>Yong</forenames></author><author><keyname>Henz</keyname><forenames>Martin</forenames></author></authors><title>Programmable Restoration Granularity in Constraint Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most constraint programming systems, a limited number of search engines is
offered while the programming of user-customized search algorithms requires
low-level efforts, which complicates the deployment of such algorithms. To
alleviate this limitation, concepts such as computation spaces have been
developed. Computation spaces provide a coarse-grained restoration mechanism,
because they store all information contained in a search tree node. Other
granularities are possible, and in this paper we make the case for dynamically
adapting the restoration granularity during search. In order to elucidate
programmable restoration granularity, we present restoration as an aspect of a
constraint programming system, using the model of aspect-oriented programming.
A proof-of-concept implementation using Gecode shows promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06521</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06521</id><created>2016-01-25</created><authors><author><keyname>Kafle</keyname><forenames>Bishoksan</forenames></author><author><keyname>Gallagher</keyname><forenames>John P.</forenames></author></authors><title>Interpolant tree automata and their application in Horn clause
  verification</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the role of interpolant tree automata in Horn clause
verification. In an abstraction-refinement scheme for Horn clause verification,
an interpolant tree automaton serves as a generalisation of a spurious
counterexample during refinement. We apply them to verify Horn clauses with
respect to some integrity constraints and present some experimental results on
some software verification benchmarks. The results show some improvements over
the previous approaches.
  Keywords: Interpolant tree automata, Horn clauses, Abstraction-refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06522</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06522</id><created>2016-01-25</created><authors><author><keyname>Baumeler</keyname><forenames>&#xc4;min</forenames></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames></author></authors><title>Non-causal computation avoiding the grandfather and information
  antinomies</title><categories>quant-ph cs.CC cs.LO</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation models such as circuits describe sequences of computation steps
that are carried out one after the other. In other words, algorithm design is
traditionally subject to the restriction imposed by a fixed causal order. We
address a novel computing paradigm, replacing this assumption by mere logical
consistency: We study non-causal circuits, where a fixed time structure within
a gate is locally assumed whilst the global causal structure between the gates
is dropped. We present examples of logically consistent non-causal circuits
outperforming all causal ones; they imply that suppressing loops entirely is
more restrictive than just avoiding the contradictions they can give rise to.
That fact is already known for correlations as well as for communication, and
we here extend it to computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06524</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06524</id><created>2016-01-25</created><authors><author><keyname>Tang</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoliang</forenames></author><author><keyname>Nguyen</keyname><forenames>Cam-Tu</forenames></author><author><keyname>Lu</keyname><forenames>Sanglu</forenames></author></authors><title>Constructing Sub-exponentially Large Optical Priority Queues with
  Switches and Fiber Delay Lines</title><categories>cs.IT math.IT</categories><comments>Submitted to 2016 IEEE Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical switching has been considered as a natural choice to keep pace with
growing fiber link capacity. One key research issue of all-optical switching is
the design of optical queues by using optical crossbar switches and fiber delay
lines (SDL). In this paper, we focus on the construction of an optical priority
queue with a single $(M+2)\times (M+2)$ crossbar switch and $M$ fiber delay
lines, and evaluate it in terms of the buffer size of the priority queue.
Currently, the best known upper bound of the buffer size is $O(2^M)$, while
existing methods can only construct a priority queue with buffer $O(M^3)$.
  In this paper, we make a great step towards closing the above huge gap. We
propose a very efficient construction of priority queues with buffer
$2^{\Theta(\sqrt{M})}$. We use 4-to-1 multiplexers with different buffer sizes,
which can be constructed efficiently with SDL, as intermediate building blocks
to simplify the design. The key idea in our construction is to route each
packet entering the switch to some group of four 4-to-1 multiplexers according
to its current priority, which is shown to be collision-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06527</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06527</id><created>2016-01-25</created><authors><author><keyname>Held</keyname><forenames>Pascal</forenames></author><author><keyname>Kruse</keyname><forenames>Rudolf</forenames></author></authors><title>Online Community Detection by Using Nearest Hubs</title><categories>cs.SI physics.soc-ph</categories><comments>Presented as poster at the NetSciX 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community and cluster detection is a popular field of social network
analysis. Most algorithms focus on static graphs or series of snapshots.
  In this paper we present an algorithm, which detects communities in dynamic
graphs. The method is based on shortest paths to high-connected nodes, so
called hubs. Due to local message passing we can update the clustering results
with low computational power.
  The presented algorithm is compared with other for some static social
networks. The reached modularity is not as high as the Louvain method, but even
higher then spectral clustering. For large-scale real-world datasets with given
ground truth, we could reconstruct most of the given community structure. The
advantage of the algorithm is the good performance in dynamic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06548</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06548</id><created>2016-01-25</created><authors><author><keyname>Cerna</keyname><forenames>David</forenames></author><author><keyname>Leitsch</keyname><forenames>Alexander</forenames></author></authors><title>Schematic Cut elimination and the Ordered Pigeonhole Principle [Extended
  Version]</title><categories>math.LO cs.LO</categories><comments>Submitted to IJCAR 2016. Will be a reference for Appendix material in
  that paper. arXiv admin note: substantial text overlap with arXiv:1503.08551</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, an attempt was made to apply the schematic CERES method [8]
to a formal proof with an arbitrary number of {\Pi} 2 cuts (a recursive proof
encapsulating the infinitary pigeonhole principle) [5]. However the derived
schematic refutation for the characteristic clause set of the proof could not
be expressed in the formal language provided in [8]. Without this formalization
a Herbrand system cannot be algorithmically extracted. In this work, we provide
a restriction of the proof found in [5], the ECA-schema (Eventually Constant
Assertion), or ordered infinitary pigeonhole principle, whose analysis can be
completely carried out in the framework of [8], this is the first time the
framework is used for proof analysis. From the refutation of the clause set and
a substitution schema we construct a Herbrand system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06551</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06551</id><created>2016-01-25</created><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Lin</keyname><forenames>Tian</forenames></author><author><keyname>Tan</keyname><forenames>Zihan</forenames></author><author><keyname>Zhao</keyname><forenames>Mingfei</forenames></author><author><keyname>Zhou</keyname><forenames>Xuren</forenames></author></authors><title>Robust Influence Maximization</title><categories>cs.SI cs.LG</categories><comments>12 pages, 4 figures, in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the important issue of uncertainty in the edge
influence probability estimates for the well studied influence maximization
problem --- the task of finding $k$ seed nodes in a social network to maximize
the influence spread. We propose the problem of robust influence maximization,
which maximizes the worse-case ratio between the influence spread of the chosen
seed set and the optimal seed set, given the uncertainty of the parameter
input. We design an algorithm that solves this problem with a
solution-dependent bound. We further study uniform sampling and adaptive
sampling methods to effectively reduce the uncertainty on parameters and
improve the robustness of the influence maximization task. Our empirical
results show that parameter uncertainty may greatly affect influence
maximization performance and prior studies that learned influence probabilities
could lead to poor performance in robust influence maximization due to
relatively large uncertainty in parameter estimates, and information cascade
based adaptive sampling method may be an effective way to improve the
robustness of influence maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06555</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06555</id><created>2016-01-25</created><updated>2016-01-26</updated><authors><author><keyname>Ram</keyname><forenames>Eshed</forenames></author><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On R\'enyi Entropy Power Inequalities</title><categories>cs.IT math.IT math.PR</categories><comments>Submitted to the IEEE Trans. on Information Theory, January 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a follow-up of a recent work by Bobkov and Chistyakov,
obtaining some improved R\'enyi entropy power inequalities (R-EPIs) for sums of
independent random vectors. The first improvement relies on the same bounding
techniques used in the former work, while the second significant improvement
relies on additional interesting properties from matrix theory. The
improvements obtained by the new R-EPIs are exemplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06562</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06562</id><created>2016-01-25</created><authors><author><keyname>Data</keyname><forenames>Deepesh</forenames></author></authors><title>Secure Computation of Randomized Functions</title><categories>cs.CR cs.IT math.IT</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two user secure computation of randomized functions is considered, where only
one user computes the output. Both the users are semi-honest; and computation
is such that no user learns any additional information about the other user's
input and output other than what cannot be inferred from its own input and
output. First we consider a scenario, where privacy conditions are against both
the users. In perfect security setting Kilian [STOC 2000] gave a
characterization of securely computable randomized functions, and we provide
rate-optimal protocols for such functions. We prove that the same
characterization holds in asymptotic security setting as well and give a
rate-optimal protocol. In another scenario, where privacy condition is only
against the user who is not computing the function, we provide rate-optimal
protocols. For perfect security in both the scenarios, our results are in terms
of chromatic entropies of different graphs. In asymptotic security setting, we
get single-letter expressions of rates in both the scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06569</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06569</id><created>2016-01-25</created><authors><author><keyname>Amin</keyname><forenames>Kareem</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Towards Resolving Unidentifiability in Inverse Reinforcement Learning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting for Inverse Reinforcement Learning (IRL) where the
learner is extended with the ability to actively select multiple environments,
observing an agent's behavior on each environment. We first demonstrate that if
the learner can experiment with any transition dynamics on some fixed set of
states and actions, then there exists an algorithm that reconstructs the
agent's reward function to the fullest extent theoretically possible, and that
requires only a small (logarithmic) number of experiments. We contrast this
result to what is known about IRL in single fixed environments, namely that the
true reward function is fundamentally unidentifiable. We then extend this
setting to the more realistic case where the learner may not select any
transition dynamic, but rather is restricted to some fixed set of environments
that it may try. We connect the problem of maximizing the information derived
from experiments to submodular function maximization and demonstrate that a
greedy algorithm is near optimal (up to logarithmic factors). Finally, we
empirically validate our algorithm on an environment inspired by behavioral
psychology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06576</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06576</id><created>2016-01-25</created><authors><author><keyname>Ouyang</keyname><forenames>Xing</forenames></author><author><keyname>Zhao</keyname><forenames>Jian</forenames></author></authors><title>Orthogonal Chirp Division Multiplexing</title><categories>cs.IT math.IT</categories><comments>27 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chirp waveform plays a significant role in radar and communication systems
for its ability of pulse compression and spread spectrum. This paper presents a
principle of orthogonally multiplexing a bank of linear chirp waveforms within
the same bandwidth. The amplitude and phase of the chirps are modulated for
information communication. As Fourier trans-form is the basis for orthogonal
frequency division multiplexing (OFDM), Fresnel transform underlies the
proposed orthogonal chirp division multiplexing (OCDM). Digital implementa-tion
of the OCDM system using discrete Fresnel transform is proposed. Based on the
con-volution theorem of the Fresnel transform, the transmission of the OCDM
signal is analyzed under the linear time-invariant or quasi-static channel with
additive noise, which can gener-alize typical linear transmission channels.
Based on the eigen-decomposition of Fresnel transform, efficient digital signal
processing algorithm is proposed for compensating chan-nel dispersion by linear
single- tap equalizers. The implementation details of the OCDM system is
discussed with emphasis on its compatibility to the OFDM system. Finally,
simula-tion are provided to validate the feasibility of the proposed OCDM under
wireless channels. It is shown that the OCDM system is able to utilize the
multipath diversity and outperforms the OFDM system under the multipath fading
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06578</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06578</id><created>2016-01-25</created><authors><author><keyname>Qin</keyname><forenames>Zhijin</forenames></author><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Gao</keyname><forenames>Yue</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author></authors><title>Throughput Analysis of Wireless Powered Cognitive Radio Networks with
  Compressive Sensing and Matrix Completion</title><categories>cs.IT math.IT</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a cognitive radio network in which energy
constrained secondary users (SUs) can harvest energy from the randomly deployed
power beacons (PBs). A new frame structure is proposed for the considered
network. A wireless power transfer (WPT) model and a compressive spectrum
sensing model are introduced. In the WPT model, a new WPT scheme is proposed,
and the closed-form expressions for the power outage probability are derived.
In compressive spectrum sensing model, two scenarios are considered: 1) Single
SU, and 2) Multiple SUs. In the single SU scenario, in order to reduce the
energy consumption at the SU, compressive sensing technique which enables
sub-Nyquist sampling is utilized. In the multiple SUs scenario, cooperative
spectrum sensing (CSS) is performed with adopting low-rank matrix completion
technique to obtain the complete matrix at the fusion center. Throughput
optimizations of the secondary network are formulated into two linear
constrained problems, which aim to maximize the throughput of single SU and the
CSS networks, respectively. Three methods are provided to obtain the maximal
throughput of secondary network by optimizing the time slots allocation and the
transmit power. Simulation results show that: 1) Multiple SUs scenario can
achieve lower power outage probability than single SU scenario; and 2) The
optimal throughput can be improved by implementing compressive spectrum sensing
in the proposed frame structure design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06579</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06579</id><created>2016-01-25</created><authors><author><keyname>Nguyen</keyname><forenames>Dong</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>A Kernel Independence Test for Geographical Language Variation</title><categories>cs.CL</categories><comments>In submission. 22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantifying the degree of spatial dependence for linguistic variables is a
key task for analyzing dialectal variation. However, existing approaches have
important drawbacks. First, they make unjustified assumptions about the nature
of spatial variation: some assume that the geographical distribution of
linguistic variables is Gaussian, while others assume that linguistic variation
is aligned to pre-defined geopolitical units such as states or counties.
Second, they are not applicable to all types of linguistic data: some
approaches apply only to frequencies, others to boolean indicators of whether a
linguistic variable is present. We present a new method for measuring
geographical language variation, which solves both of these problems. Our
approach builds on reproducing kernel Hilbert space (RKHS) representations for
nonparametric statistics, and takes the form of a test statistic that is
computed from pairs of individual geotagged observations without aggregation
into predefined geographical bins. We compare this test with prior work using
synthetic data as well as a diverse set of real datasets: a corpus of Dutch
tweets, a Dutch syntactic atlas, and a dataset of letters to the editor in
North American newspapers. Our proposed test is shown to support robust
inferences across a broad range of scenarios and types of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06580</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06580</id><created>2016-01-25</created><authors><author><keyname>Polap</keyname><forenames>Dawid</forenames></author><author><keyname>Wozniak</keyname><forenames>Marcin</forenames></author><author><keyname>Napoli</keyname><forenames>Christian</forenames></author><author><keyname>Tramontana</keyname><forenames>Emiliano</forenames></author></authors><title>Is swarm intelligence able to create mazes?</title><categories>cs.NE cs.AI</categories><msc-class>68T05, 68T10, 68T45, 68U10, 68W25, 68W99</msc-class><acm-class>I.2.6; I.2.10; I.4.8</acm-class><journal-ref>International Journal of Electronics and Telecommunications, Vol.
  6, n. 4, pp. 305-310 (2015)</journal-ref><doi>10.1515/eletel-2015-0039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the idea of applying Computational Intelligence in the process
of creation board games, in particular mazes, is presented. For two different
algorithms the proposed idea has been examined. The results of the experiments
are shown and discussed to present advantages and disadvantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06581</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06581</id><created>2016-01-25</created><updated>2016-01-28</updated><authors><author><keyname>Hwang</keyname><forenames>Kyuyeon</forenames></author><author><keyname>Sung</keyname><forenames>Wonyong</forenames></author></authors><title>Character-Level Incremental Speech Recognition with Recurrent Neural
  Networks</title><categories>cs.CL cs.LG cs.NE</categories><comments>To appear in ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-time speech recognition applications, the latency is an important
issue. We have developed a character-level incremental speech recognition (ISR)
system that responds quickly even during the speech, where the hypotheses are
gradually improved while the speaking proceeds. The algorithm employs a
speech-to-character unidirectional recurrent neural network (RNN), which is
end-to-end trained with connectionist temporal classification (CTC), and an
RNN-based character-level language model (LM). The output values of the
CTC-trained RNN are character-level probabilities, which are processed by beam
search decoding. The RNN LM augments the decoding by providing long-term
dependency information. We propose tree-based online beam search with
additional depth-pruning, which enables the system to process infinitely long
input speech with low latency. This system not only responds quickly on speech
but also can dictate out-of-vocabulary (OOV) words according to pronunciation.
The proposed model achieves the word error rate (WER) of 8.90% on the Wall
Street Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284
training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06602</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06602</id><created>2016-01-25</created><authors><author><keyname>Schneider</keyname><forenames>Markus</forenames></author><author><keyname>Ertel</keyname><forenames>Wolfgang</forenames></author><author><keyname>Ramos</keyname><forenames>Fabio</forenames></author></authors><title>Expected Similarity Estimation for Large-Scale Batch and Streaming
  Anomaly Detection</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm for anomaly detection on very large datasets and
data streams. The method, named EXPected Similarity Estimation (EXPoSE), is
kernel-based and able to efficiently compute the similarity between new data
points and the distribution of regular data. The estimator is formulated as an
inner product with a reproducing kernel Hilbert space embedding and makes no
assumption about the type or shape of the underlying data distribution. We show
that offline (batch) learning with EXPoSE can be done in linear time and online
(incremental) learning takes constant time per instance and model update.
Furthermore, EXPoSE can make predictions in constant time, while it requires
only constant memory. In addition we propose different methodologies for
concept drift adaptation on evolving data streams. On several real datasets we
demonstrate that our approach can compete with state of the art algorithms for
anomaly detection while being significant faster than techniques with the same
discriminant power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06603</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06603</id><created>2016-01-25</created><authors><author><keyname>Song</keyname><forenames>Sibo</forenames></author><author><keyname>Cheung</keyname><forenames>Ngai-Man</forenames></author><author><keyname>Chandrasekhar</keyname><forenames>Vijay</forenames></author><author><keyname>Mandal</keyname><forenames>Bappaditya</forenames></author><author><keyname>Lin</keyname><forenames>Jie</forenames></author></authors><title>Egocentric Activity Recognition with Multimodal Fisher Vector</title><categories>cs.MM cs.CV</categories><comments>5 pages, 4 figures, ICASSP 2016 accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing availability of wearable devices, research on egocentric
activity recognition has received much attention recently. In this paper, we
build a Multimodal Egocentric Activity dataset which includes egocentric videos
and sensor data of 20 fine-grained and diverse activity categories. We present
a novel strategy to extract temporal trajectory-like features from sensor data.
We propose to apply the Fisher Kernel framework to fuse video and temporal
enhanced sensor features. Experiment results show that with careful design of
feature extraction and fusion algorithm, sensor data can enhance
information-rich video data. We make publicly available the Multimodal
Egocentric Activity dataset to facilitate future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06608</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06608</id><created>2016-01-25</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author><author><keyname>Dandapat</keyname><forenames>Samarendra</forenames></author><author><keyname>Sinha</keyname><forenames>Rohit</forenames></author></authors><title>An Unsupervised Method for Detection and Validation of The Optic Disc
  and The Fovea</title><categories>cs.CV</categories><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we have presented a novel method for detection of retinal image
features, the optic disc and the fovea, from colour fundus photographs of
dilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map based
method was used to detect the optic disc followed by an unsupervised
probabilistic Latent Semantic Analysis for detection validation. The validation
concept is based on distinct vessels structures in the optic disc. By using the
clinical information of standard location of the fovea with respect to the
optic disc, the macula region is estimated. Accuracy of 100\% detection is
achieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\%
detection accuracy on STARE dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06610</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06610</id><created>2016-01-25</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Broekaert</keyname><forenames>Jan</forenames></author><author><keyname>Gabora</keyname><forenames>Liane</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Generalizing Prototype Theory: A Formal Quantum Framework</title><categories>cs.AI quant-ph</categories><comments>30 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theories of natural language and concepts have been unable to model the
flexibility, creativity, context-dependence, and emergence, exhibited by words,
concepts and their combinations. The mathematical formalism of quantum theory
has instead been successful in capturing these phenomena such as graded
membership, situational meaning, composition of categories, and also more
complex decision making situations, which cannot be modeled in traditional
probabilistic approaches. We show how a formal quantum approach to concepts and
their combinations can provide a powerful extension of prototype theory. We
explain how prototypes can interfere in conceptual combinations as a
consequence of their contextual interactions, and provide an illustration of
this using an intuitive wave-like diagram. This quantum-conceptual approach
gives new life to original prototype theory, without however making it a
privileged concept theory, as we explain at the end of our paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06611</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06611</id><created>2016-01-25</created><authors><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>&quot;Pretty strong&quot; converse for the private capacity of degraded quantum
  wiretap channels</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, IEEEtran.cls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the vein of the recent &quot;pretty strong&quot; converse for the quantum and
private capacity of degradable quantum channels [Morgan/Winter, IEEE Trans.
Inf. Theory 60(1):317-333, 2014], we use the same techniques, in particular the
calculus of min-entropies, to show a pretty strong converse for the private
capacity of degraded classical-quantum-quantum (cqq-)wiretap channels, which
generalize Wyner's model of the degraded classical wiretap channel.
  While the result is not completely tight, leaving some gap between the region
of error and privacy parameters for which the converse bound holds, and a
larger no-go region, it represents a further step towards an understanding of
strong converses of wiretap channels [cf. Hayashi/Tyagi/Watanabe,
arXiv:1410.0443 for the classical case].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06615</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06615</id><created>2016-01-25</created><authors><author><keyname>Srinivas</keyname><forenames>Suraj</forenames></author><author><keyname>Sarvadevabhatla</keyname><forenames>Ravi Kiran</forenames></author><author><keyname>Mopuri</keyname><forenames>Konda Reddy</forenames></author><author><keyname>Prabhu</keyname><forenames>Nikita</forenames></author><author><keyname>Kruthiventi</keyname><forenames>Srinivas S S</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>A Taxonomy of Deep Convolutional Neural Nets for Computer Vision</title><categories>cs.CV cs.LG cs.MM</categories><comments>Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm)</comments><journal-ref>Frontiers in Robotics and AI 2(36), January 2016</journal-ref><doi>10.3389/frobt.2015.00036</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Traditional architectures for solving computer vision problems and the degree
of success they enjoyed have been heavily reliant on hand-crafted features.
However, of late, deep learning techniques have offered a compelling
alternative -- that of automatically learning problem-specific features. With
this new paradigm, every problem in computer vision is now being re-examined
from a deep learning perspective. Therefore, it has become important to
understand what kind of deep networks are suitable for a given problem.
Although general surveys of this fast-moving paradigm (i.e. deep-networks)
exist, a survey specific to computer vision is missing. We specifically
consider one form of deep networks widely used in computer vision -
convolutional neural networks (CNNs). We start with &quot;AlexNet&quot; as our base CNN
and then examine the broad variations proposed over time to suit different
applications. We hope that our recipe-style survey will serve as a guide,
particularly for novice practitioners intending to use deep-learning techniques
for computer vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06616</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06616</id><created>2016-01-25</created><authors><author><keyname>Zhang</keyname><forenames>Zhiwei</forenames></author><author><keyname>Fu</keyname><forenames>Chenglong</forenames></author></authors><title>Capturability-based Analysis of Legged Robot with Consideration of Swing
  Legs</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capturability of a robot determines whether it is able to capture a robot
within a number of steps. Current capturability analysis is based on stance leg
dynamics, without taking adequate consideration on swing leg. In this paper, we
combine capturability-based analysis with swing leg dynamics. We first
associate original definition of capturability with a time-margin, which
encodes a time sequence that can capture the robot. This time-margin
capturability requires consideration of swing leg, and we therefore introduce a
swing leg kernel that acts as a bridge between step time and step length. We
analyze N-step capturability with a combined model of swing leg kernels and a
linear inverted pendulum model. By analyzing swing leg kernels with different
parameters, we find that more powerful actuation and longer normalized step
length result in greater capturability. We also answer the question whether
more steps would give greater capturability. For a given disturbance, we find a
step sequence that minimizes actuation. This step sequence is whether a step
time sequence or a step length sequence, and this classification is based on
boundary value problem analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06626</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06626</id><created>2016-01-22</created><authors><author><keyname>Li</keyname><forenames>Yongbin</forenames></author></authors><title>Computing the decomposition group of a zero-dimensional ideal by
  elimination method</title><categories>math.AC cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that the decomposition group $Dec(I)$ of a
zero-dimensional radical ideal $I$ in ${\bf K}[x_1,\ldots,x_n]$ can be
represented as the direct sum of several symmetric groups of polynomials based
upon using Gr\&quot;{o}bner bases. The new method makes a theoretical contribution
to discuss the decomposition group of $I$ by using Computer Algebra without
considering the complexity. As one application, we also present an approach to
yield new triangular sets in computing triangular decomposition of polynomial
sets ${\mathbb P}$ if $Dec(&lt;{\mathbb P}&gt;)$ is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06650</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06650</id><created>2016-01-25</created><authors><author><keyname>Bogunovic</keyname><forenames>Ilija</forenames></author><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Time-Varying Gaussian Process Bandit Optimization</title><categories>stat.ML cs.LG</categories><comments>To appear in AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the sequential Bayesian optimization problem with bandit
feedback, adopting a formulation that allows for the reward function to vary
with time. We model the reward function using a Gaussian process whose
evolution obeys a simple Markov model. We introduce two natural extensions of
the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The
first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB,
instead forgets about old data in a smooth fashion. Our main contribution
comprises of novel regret bounds for these algorithms, providing an explicit
characterization of the trade-off between the time horizon and the rate at
which the function varies. We illustrate the performance of the algorithms on
both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB
to perform favorably compared to the sharp resetting of R-GP-UCB. Moreover,
both algorithms significantly outperform classical GP-UCB, since it treats
stale and fresh data equally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06652</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06652</id><created>2016-01-25</created><authors><author><keyname>Necciari</keyname><forenames>Thibaud</forenames></author><author><keyname>Holighaus</keyname><forenames>Nicki</forenames></author><author><keyname>Balazs</keyname><forenames>Peter</forenames></author><author><keyname>Prusa</keyname><forenames>Zdenek</forenames></author></authors><title>A Perceptually Motivated Filter Bank with Perfect Reconstruction for
  Audio Signal Processing</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many audio applications rely on filter banks (FBs) to analyze, process, and
re-synthesize sounds. To approximate the auditory frequency resolution in the
signal chain, some applications rely on perceptually motivated FBs, the
gammatone FB being a popular example. However, most perceptually motivated FBs
only allow partial signal reconstruction at high redundancies and/or do not
have good resistance to sub-channel processing. This paper introduces an
oversampled perceptually motivated FB enabling perfect reconstruction,
efficient FB design, and adaptable redundancy. The filters are directly
constructed in the frequency domain and linearly distributed on a perceptual
frequency scale (e.g. ERB, Bark, or Mel scale). The proposed design allows for
various filter shapes, uniform or non-uniform FB setting, and large
down-sampling factors. For redundancies $\geq$ 3 perfect reconstruction is
achieved by computing the canonical dual FB analytically. For lower
redundancies perfect reconstruction is achieved using an iterative method.
Experiments show performance improvements of the proposed approach when
compared to the gammatone FB in terms of reconstruction error and resistance to
sub-channel processing, especially at low redundancies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06664</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06664</id><created>2016-01-25</created><authors><author><keyname>Raza</keyname><forenames>Usman</forenames></author><author><keyname>Bogliolo</keyname><forenames>Alessandro</forenames></author><author><keyname>Freschi</keyname><forenames>Valerio</forenames></author><author><keyname>Lattanzi</keyname><forenames>Emanuele</forenames></author><author><keyname>Murphy</keyname><forenames>Amy L.</forenames></author></authors><title>A Two-Prong Approach to Energy-Efficient WSNs: Wake-Up Receivers plus
  Dedicated, Model-Based Sensing</title><categories>cs.NI</categories><comments>30 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy neutral operation of WSNs can be achieved by exploiting the idleness
of the workload to bring the average power consumption of each node below the
harvesting power available. This paper proposes a combination of
state-of-the-art low-power design techniques to minimize the local and global
impact of the two main activities of each node: sampling and communication.
Dynamic power management is adopted to exploit low-power modes during idle
periods, while asynchronous wake-up and prediction-based data collection are
used to opportunistically activate hardware components and network nodes only
when they are strictly required. Furthermore, the concept of &quot;model-based
sensing&quot; is introduced to push prediction-based data collection techniques as
close as possible to the sensing elements. The results achieved on
representative real-world WSN case studies show that the combined benefits of
the design techniques adopted is more than linear, providing an overall power
reduction of more than 3 orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06666</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06666</id><created>2016-01-25</created><authors><author><keyname>Aazam</keyname><forenames>Mohammad</forenames></author><author><keyname>Huh</keyname><forenames>Eui-Nam</forenames></author></authors><title>Impact of IPv4-IPv6 Coexistence in Cloud Virtualization Environment</title><categories>cs.NI</categories><comments>10 pages. 16 figures. 10 tables</comments><journal-ref>Annals of telecommunications-annales des t\'el\'ecommunications
  69.9-10 (2014): 485-496</journal-ref><doi>10.1007/s12243-013-0391-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since January 2011, IPv4 address space has exhausted and IPv6 is taking up
the place as successor. Coexistence of IPv4 and IPv6 bears problem of
incompatibility, as IPv6 and IPv4 headers are different from each other, thus,
cannot interoperate with each other directly. The IPv6 transitioning techniques
are still not mature, causing hindrance in the deployment of IPv6 and
development of next generation Internet. Until IPv6 completely takes over from
IPv4, they will both coexist. For IPv4-IPv6 coexistence, three solutions are
possible: a) making every device dual stack, b) translation, c) tunneling.
Tunneling stands out as the best possible solution. Among the IPv6 tunneling
techniques, this paper evaluates the impact of three recent IPv6 tunneling
techniques: 6to4, Teredo, and ISATAP, in cloud virtualization environment. In
virtual networks, these protocols were implemented on Microsoft Windows (MS
Windows 7 and MS Windows Server 2008) and Linux operating system. Each protocol
was implemented on the virtual network. UDP audio streaming, video streaming
and ICMP-ping traffic was run. Multiple runs of traffic were routed over the
setup for each protocol. The average of the data was taken to generate graphs
and final results. The performance of these tunneling techniques has been
evaluated on eight parameters, namely: throughput, end to end delay (E2ED),
jitter, round trip time (RTT), tunneling overhead, tunnel setup delay, query
delay, and auxiliary devices required. This evaluation shows the impact of
IPv4-IPv6 coexistence in virtualization environment for cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06672</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06672</id><created>2016-01-25</created><authors><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author><author><keyname>Yu</keyname><forenames>Jia Yuan</forenames></author></authors><title>Pricing Vehicle Sharing with Proximity Information</title><categories>math.OC cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For vehicle sharing schemes, where drop-off positions are not fixed, we
propose a pricing scheme, where the price depends in part on the distance
between where a vehicle is being dropped off and where the closest shared
vehicle is parked. Under certain restrictive assumptions, we show that this
pricing leads to a socially optimal spread of the vehicles within a region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06676</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06676</id><created>2016-01-25</created><authors><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod</forenames></author></authors><title>Plausible Deniability over Broadcast Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the notion of Plausible Deniability in an
information theoretic framework. We consider a scenario where an entity that
eavesdrops through a broadcast channel summons one of the parties in a
communication protocol to reveal their message (or signal vector). It is
desirable that the summoned party have the option to reveal as little of the
true message as possible by producing a fake output that is likely plausible
given the eavesdropper's observation. We examine three variants of this problem
-- Transmitter Deniability, Receiver Deniability, and Message Deniability. In
the first setting, the transmitter is summoned to produce the transmitted
codeword on the channel. Similarly, in the second and third settings, the
receiver and the message sender are required to produce the received codeword
and the message respectively. For each of these settings, we examine the
maximum communication rate that allows a minimum amount of plausible
deniability. For the Transmitter and Receiver Deniability problems, we give an
achievable region for general broadcast channels that is shown to be tight for
the Transmitter Deniability problem when the channel is degraded. For the
Message Deniability problem, we fully characterise the rate region for general
broadcast channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06680</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06680</id><created>2016-01-25</created><authors><author><keyname>Fonollosa</keyname><forenames>Jos&#xe9; A. R.</forenames></author></authors><title>Conditional distribution variability measures for causality detection</title><categories>stat.ML cs.LG</categories><comments>NIPS 2013 workshop on causality</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive variability measures for the conditional probability
distributions of a pair of random variables, and we study its application in
the inference of causal-effect relationships. We also study the combination of
the proposed measures with standard statistical measures in the the framework
of the ChaLearn cause-effect pair challenge. The developed model obtains an AUC
score of 0.82 on the final test database and ranked second in the challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06681</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06681</id><created>2016-01-25</created><updated>2016-01-26</updated><authors><author><keyname>Muralikrishnan</keyname><forenames>Sriramkrishnan</forenames></author><author><keyname>Tran</keyname><forenames>Minh-Binh</forenames></author><author><keyname>Bui-Thanh</keyname><forenames>Tan</forenames></author></authors><title>eHDG:An Exponentially Convergent Iterative Solver for HDG
  Discretizations of Hyperbolic Partial Differential Equations</title><categories>math.NA cs.NA</categories><comments>10 pages, 5 figures, submitted to 14th Copper mountain conference on
  iterative methods (student paper competition)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable and efficient iterative solver for high-order
hybridized discontinuous Galerkin (HDG) discretizations of hyperbolic partial
differential equations. It is an interplay between domain decomposition methods
and HDG discretizations. In particular, the method is a fixed-point approach
that requires only independent element-by-element local solves in each
iteration. As such, it is well-suited for current and future computing systems
with massive concurrencies. We rigorously show that the proposed method is
exponentially convergent in the number of iterations for transport and
linearized shallow water equations. Furthermore, the convergence is independent
of the solution order. Various 2D and 3D numerical results for steady and
time-dependent problems are presented to verify our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06683</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06683</id><created>2016-01-25</created><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Clustering from Sparse Pairwise Measurements</title><categories>cs.SI cond-mat.dis-nn cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of grouping items into clusters based on few random
pairwise comparisons between the items. We introduce three closely related
algorithms for this task: a belief propagation algorithm approximating the
Bayes optimal solution, and two spectral algorithms based on the
non-backtracking and Bethe Hessian operators. For the case of two symmetric
clusters, we show that the spectral approaches are asymptotically optimal in
that they detect the clusters as soon as it is information theoretically
possible to do so. As a by-product, we prove a conjecture on the detectability
transition of the labeled stochastic block model in a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06684</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06684</id><created>2016-01-25</created><authors><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Zhou</keyname><forenames>Cheng</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Large-Scale MIMOs are Capable of Eliminating Power-Thirsty Channel
  Coding for Wireless Transmission of HEVC/H.265 Video</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, 1 table, accepted to appear on IEEE Wireless
  Communications Magazine, Jan. 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wireless video transmission architecture relying on the emerging
large-scale multiple-input--multiple-output (LS-MIMO) technique is proposed.
Upon using the most advanced High Efficiency Video Coding (HEVC) (also known as
H.265), we demonstrate that the proposed architecture invoking the
low-complexity linear zero-forcing (ZF) detector and dispensing with any
channel coding is capable of significantly outperforming the conventional
small-scale MIMO based architecture, even if the latter employs the
high-complexity optimal maximum-likelihood (ML) detector and a rate-$1/3$
recursive systematic convolutional (RSC) channel codec. Specifically, compared
to the conventional small-scale MIMO system, the effective system throughput of
the proposed LS-MIMO based scheme is increased by a factor of up to three and
the quality of reconstructed video quantified in terms of the peak
signal-to-noise ratio (PSNR) is improved by about $22.5\, \text{dB}$ at a
channel-SNR of $E_b/N_0 \approx 6\,\text{dB}$ for delay-tolerant video-file
delivery applications, and about $20\,\text{dB}$ for lip-synchronized real-time
interactive video applications. Alternatively, viewing the attainable
improvement from a power-saving perspective, a channel-SNR gain as high as
$\Delta_{E_b/N_0}\approx 5\,\text{dB}$ is observed at a PSNR of $36\,
\text{dB}$ for the scenario of delay-tolerant video applications and again, an
even higher gain is achieved in the real-time video application scenario.
Therefore, we envisage that LS-MIMO aided wireless multimedia communications is
capable of dispensing with power-thirsty channel codec altogether!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06686</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06686</id><created>2016-01-25</created><updated>2016-01-30</updated><authors><author><keyname>Thakor</keyname><forenames>Satyajit</forenames></author><author><keyname>Abbas</keyname><forenames>Syed</forenames></author></authors><title>Upper Bounds on the Capacity of 2-Layer $N$-Relay Symmetric Gaussian
  Network</title><categories>cs.IT math.IT</categories><comments>submitted to ISIT2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gaussian parallel relay network, in which two parallel relays assist a
source to convey information to a destination, was introduced by Schein and
Gallager. An upper bound on the capacity can be obtained by considering
broadcast cut between the source and relays and multiple access cut between
relays and the destination. Niesen and Diggavi derived an upper bound for
Gaussian parallel $N$-relay network by considering all other possible cuts and
showed an achievability scheme that can attain rates close to the upper bound
in different channel gain regimes thus establishing approximate capacity. In
this paper we consider symmetric layered Gaussian relay networks in which there
can be many layers of parallel relays. The channel gains for the channels
between two adjacent layers are symmetrical (identical). Relays in each layer
broadcast information to the relays in the next layer. For 2-layer $N$-relay
Gaussian network we give upper bounds on the capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06689</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06689</id><created>2016-01-25</created><authors><author><keyname>Krishnan</keyname><forenames>Prasad</forenames></author><author><keyname>Lalitha</keyname><forenames>V.</forenames></author></authors><title>A class of index coding problems with rate 1/3</title><categories>cs.IT math.IT</categories><comments>Shorter version submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An index coding problem with $n$ messages has symmetric rate $R$ if all $n$
messages can be conveyed at rate $R$. In a recent work, a class of index coding
problems for which symmetric rate $\frac{1}{3}$ is achievable was characterised
using special properties of the side-information available at the receivers. In
this paper, we show a larger class of index coding problems (which includes the
previous class of problems) for which symmetric rate $\frac{1}{3}$ is
achievable. In the process, we also obtain a stricter necessary condition for
rate $\frac{1}{3}$ feasibility than what is known in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06693</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06693</id><created>2016-01-25</created><authors><author><keyname>Hernandez</keyname><forenames>Maritza</forenames></author><author><keyname>Zaribafiyan</keyname><forenames>Arman</forenames></author><author><keyname>Aramon</keyname><forenames>Maliheh</forenames></author><author><keyname>Naghibi</keyname><forenames>Mohammad</forenames></author></authors><title>A Novel Graph-based Approach for Determining Molecular Similarity</title><categories>cs.DS q-bio.QM quant-ph</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of measuring similarity among graphs
that represent real objects with noisy data. To account for noise, we relax the
definition of similarity using the maximum weighted co-$k$-plex relaxation
method, which allows dissimilarities among graphs up to a predetermined level.
We then formulate the problem as a novel quadratic unconstrained binary
optimization problem that can be solved by a quantum annealer. The context of
our study is molecular similarity where the presence of noise might be due to
regular errors in measuring molecular features. We develop a similarity measure
and use it to predict the mutagenicity of a molecule. Our results indicate that
the relaxed similarity measure, designed to accommodate the regular errors,
yields a higher prediction accuracy than the measure that ignores the noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06704</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06704</id><created>2016-01-25</created><authors><author><keyname>D'yachkov</keyname><forenames>A. G.</forenames></author><author><keyname>Vorobyev</keyname><forenames>I. V.</forenames></author><author><keyname>Polyanskii</keyname><forenames>N. A.</forenames></author><author><keyname>Shchukin</keyname><forenames>V. Yu.</forenames></author></authors><title>On a Hypergraph Approach to Multistage Group Testing Problems</title><categories>cs.IT math.CO math.IT</categories><comments>5 pages, IEEE conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group testing is a well known search problem that consists in detecting up to
$s$ defective elements of the set $[t]=\{1,\ldots,t\}$ by carrying out tests on
properly chosen subsets of $[t]$. In classical group testing the goal is to
find all defective elements by using the minimal possible number of tests. In
this paper we consider multistage group testing. We propose a general idea how
to use a hypergraph approach to searching defects. For the case $s=2$, we
design an explicit construction, which makes use of $2\log_2t(1+o(1))$ tests in
the worst case and consists of $4$ stages. For the general case $s&gt;2$, we
provide an explicit construction, which uses $(2s-1)\log_2t(1+o(1))$ tests and
consists of $2s-1$ rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06705</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06705</id><created>2016-01-25</created><updated>2016-02-14</updated><authors><author><keyname>D'yachkov</keyname><forenames>A. G.</forenames></author><author><keyname>Vorobyev</keyname><forenames>I. V.</forenames></author><author><keyname>Polyanskii</keyname><forenames>N. A.</forenames></author><author><keyname>Shchukin</keyname><forenames>V. Yu.</forenames></author></authors><title>On Multistage Learning a Hidden Hypergraph</title><categories>cs.IT math.IT</categories><comments>5 pages, IEEE conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a hidden hypergraph is a natural generalization of the classical
group testing problem that consists in detecting unknown hypergraph $H=(V,E)$
by carrying out edge-detecting tests. In the given paper we study a relaxation
of this problem. We wish to identify \textit{almost} all hypergraphs by using
the minimal number of tests. We focus our attention only on a specific family
of hypergraphs for which the total number of vertices $|V| = t$, the number of
edges $|E|\le s$, $s\ll t$, and the size of any hyperedge $|e|\le\l$, $\l\ll
t$. If the number of stages of group testing is two, then we provide an
algorithm that matches the information-theoretical bound, i.e., the total
number of queries of the algorithm in the worst case is at most $s\l\log_2
t(1+o(1))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06709</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06709</id><created>2016-01-25</created><authors><author><keyname>D'yachkov</keyname><forenames>A. G.</forenames></author><author><keyname>Vorobyev</keyname><forenames>I. V.</forenames></author><author><keyname>Polyanskii</keyname><forenames>N. A.</forenames></author><author><keyname>Shchukin</keyname><forenames>V. Yu.</forenames></author></authors><title>Threshold Disjunctive Codes</title><categories>cs.IT math.IT</categories><comments>9 pages, IEEE conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $1 \le s &lt; t$, $N \ge 1$ be integers and a complex electronic circuit of
size $t$ is said to be an $s$-active, $\; s \ll t$, and can work as a system
block if not more than $s$ elements of the circuit are defective. Otherwise,
the circuit is said to be an $s$-defective and should be substituted for the
similar $s$-active circuit. Suppose that there exists a possibility to check
the $s$-activity of the circuit using $N$ non-adaptive group tests identified
by a conventional disjunctive $s$-code $X$ of size~$t$ and length~$N$. As
usually, we say that any group test yields the positive response if the group
contains at least one defective element. In this case, there is no any interest
to look for the defective elements. We need to decide on the number of the
defective elements in the circuit without knowing the code~$X$. In addition,
the decision has the minimal possible complexity because it is based on the
simple comparison of a fixed threshold $T$, $0 \le T \le N - 1$, with the
number of positive responses $p$, $0 \le p \le N$, obtained after carrying out
$N$ non-adaptive tests prescribed by the disjunctive $s$-code~$X$. For the
introduced group testing problem, a new class of the well-known disjunctive
$s$-codes called the threshold disjunctive $s$-codes is defined. The aim of our
paper is to discuss both some constructions of suboptimal threshold disjunctive
$s$-codes and the best random coding bounds on the rate of threshold
disjunctive $s$-codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06711</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06711</id><created>2016-01-25</created><authors><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author></authors><title>Scalable Anomaly Ranking of Attributed Neighborhoods</title><categories>cs.SI physics.soc-ph</categories><comments>SDM16, 12 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph with node attributes, what neighborhoods are anomalous? To
answer this question, one needs a quality score that utilizes both structure
and attributes. Popular existing measures either quantify the structure only
and ignore the attributes (e.g., conductance), or only consider the
connectedness of the nodes inside the neighborhood and ignore the cross-edges
at the boundary (e.g., density).
  In this work we propose normality, a new quality measure for attributed
neighborhoods. Normality utilizes structure and attributes together to quantify
both internal consistency and external separability. It exhibits two key
advantages over other measures: (1) It allows many boundary-edges as long as
they can be &quot;exonerated&quot;; i.e., either (i) are expected under a null model,
and/or (ii) the boundary nodes do not exhibit the subset of attributes shared
by the neighborhood members. Existing measures, in contrast, penalize boundary
edges irrespectively. (2) Normality can be efficiently maximized to
automatically infer the shared attribute subspace (and respective weights) that
characterize a neighborhood. This efficient optimization allows us to process
graphs with millions of attributes.
  We capitalize on our measure to present a novel approach for Anomaly Mining
of Entity Neighborhoods (AMEN). Experiments on real-world attributed graphs
illustrate the effectiveness of our measure at anomaly detection, outperforming
popular approaches including conductance, density, OddBall, and SODA. In
addition to anomaly detection, our qualitative analysis demonstrates the
utility of normality as a powerful tool to contrast the correlation between
structure and attributes across different graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06719</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06719</id><created>2016-01-25</created><authors><author><keyname>Li</keyname><forenames>Guiying</forenames></author><author><keyname>Liu</keyname><forenames>Junlong</forenames></author><author><keyname>Jiang</keyname><forenames>Chunhui</forenames></author><author><keyname>Tang</keyname><forenames>Ke</forenames></author></authors><title>Relief Impression Image Detection : Unsupervised Extracting Objects
  Directly From Feature Arrangements of Deep CNN</title><categories>cs.CV</categories><comments>Draw version, many statements may be wrong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional sliding windows method for region proposals always focus on the
numerical feature values. Instead, we find that the arrangements of
convolutional features in deep CNN keep a lot of spatial information of objects
in image, and by extracting the positions of high value features as region
proposals, we get a unified structure of object detection only based on forward
CNN features. Our method has high speed and reasonable performance, since we
get the idea from the representations of relievo, so we call this relief
impression object detection. We also propose an assumption that the deep
detector can act as human's eye sight tracing without specialized training, we
verify this assumption by creating a process called recursive finetuen in our
method and get obvious promotion. Our method can get a good perfomance on the
limited region proposals situation with very high speed and less computation
resource. As far as we know, our work is the first one that notice the massive
spatial information stored by the CNN convolutional features arrangement, and
use them on the object detection task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06731</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06731</id><created>2016-01-25</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Abdelzaher</keyname><forenames>Tarek</forenames></author></authors><title>Resiliency and Robustness of Complex, Multi-Genre Networks</title><categories>cs.SI cs.NI physics.soc-ph</categories><comments>A version of this paper appeared as a book chapter in Adaptive,
  Dynamic, and Resilient Systems published by Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the resiliency and robustness of systems while viewing them as
complex, multi-genre networks. The term &quot;complex, multi-genre networks&quot; refers
to networks that combine several distinct genres - networks of physical
resources, communication networks, information networks, and social and
cognitive networks. We show that this perspective is fruitful and adds to our
understanding of fundamental challenges and tradeoffs in robustness and
resiliency, as well as potential solutions to the challenges. Study of systems
as multi-genre networks is relatively uncommon; instead, it is customary in
research and engineering literature to focus on a view of a network comprised
of homogeneous elements, (e.g., a network of communication devices, or a
network of social beings). Yet, most if not all real-world networks are
multi-genre - it is hard to find any real system of a significant complexity
that does not include a combination of interconnected physical elements,
communication devices and channels, data collections, and human users forming
an integrated, inter-dependent whole. Most approaches to improving resiliency
and robustness involve compromises, and the key challenge is to find a
favorable compromise. Such compromises involve reducing or managing the
complexity of the network: coupling, rigidity and dependency. We discuss
several of these compromises, e.g., performance vs resiliency; resiliency to
one type of disruption vs resiliency to another disruption type; and complexity
vs resiliency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06732</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06732</id><created>2016-01-25</created><authors><author><keyname>Lewis</keyname><forenames>Martha</forenames></author><author><keyname>Lawry</keyname><forenames>Jonathan</forenames></author></authors><title>Concept Generation in Language Evolution</title><categories>cs.AI cs.CL cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis investigates the generation of new concepts from combinations of
existing concepts as a language evolves. We give a method for combining
concepts, and will be investigating the utility of composite concepts in
language evolution and thence the utility of concept generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06733</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06733</id><created>2016-01-25</created><updated>2016-02-01</updated><authors><author><keyname>Cheng</keyname><forenames>Jianpeng</forenames></author><author><keyname>Dong</keyname><forenames>Li</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author></authors><title>Long Short-Term Memory-Networks for Machine Reading</title><categories>cs.CL cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teaching machines to process text with psycholinguistic insights is a
challenging task. We propose an attentive machine reader that reads text from
left to right, whilst linking the word at the current fixation point to
previous words stored in the memory as a kind of implicit information parsing
to facilitate understanding. The reader is equipped with a Long Short-Term
Memory architecture, which different from previous work, has a memory tape
(instead of a memory cell) to store the past information and adaptively use
them without severe information compression. In addition, we propose a new
attentional encoder-decoder that explicitly stores inter alignment information
in the memory of the decoder, from which we reap significant gains. We
demonstrate the excellent performance of the machine reader in language
modeling as well as the downstream sentiment analysis and natural language
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06738</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06738</id><created>2016-01-25</created><authors><author><keyname>Lewis</keyname><forenames>Martha</forenames></author><author><keyname>Lawry</keyname><forenames>Jonathan</forenames></author></authors><title>A Label Semantics Approach to Linguistic Hedges</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for the linguistic hedges `very' and `quite' within the
label semantics framework, and combined with the prototype and conceptual
spaces theories of concepts. The proposed model emerges naturally from the
representational framework we use and as such, has a clear semantic grounding.
We give generalisations of these hedge models and show that they can be
composed with themselves and with other functions, going on to examine their
behaviour in the limit of composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06740</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06740</id><created>2016-01-01</created><authors><author><keyname>Kelemen</keyname><forenames>Z&#xe1;dor D&#xe1;niel</forenames></author><author><keyname>Migl&#xe1;sz</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>50+ Metrics for Calendar Mining</title><categories>cs.OH</categories><comments>60 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we propose 50+ metrics which can be measured by organizations
in order to identify improvements in various areas such as meeting efficiency,
capacity planning or leadership skills, just to new a few. The notion of
calendar mining is introduced and support is provided for performing the
measurement by a reference data model and queries for all metrics defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06744</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06744</id><created>2016-01-25</created><authors><author><keyname>Wang</keyname><forenames>Yongge</forenames></author></authors><title>Octonion Algebra and Noise-Free Fully Homomorphic Encryption (FHE)
  Schemes</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brakerski showed that linearly decryptable fully homomorphic encryption (FHE)
schemes cannot be secure in the chosen plaintext attack (CPA) model. In this
paper, we show that linearly decryptable FHE schemes cannot be secure even in
the ciphertext only security model. Then we consider the maximum security that
a linearly decryptable FHE scheme could achieve. This paper designs fully
homomorphic symmetric key encryption (FHE) schemes without bootstrapping (that
is, noise-free FHE schemes). The proposed FHE schemes are based on
quaternion/octonion algebra and Jordan algebra over finite rings Z_n and are
secure in the weak ciphertext-only security model assuming the hardness of
solving multivariate quadratic equation systems and solving univariate high
degree polynomial equation systems in Z_n. It is up to our knowledge that this
is the first noise-free FHE scheme that has ever been designed with a security
proof (even in the weak ciphertext-only security model). It is argued that the
weak ciphertext-only security model is sufficient for various applications such
as privacy preserving computation in cloud. As an example, the proposed FHE
schemes are used to construct obfuscated programs. This example could be
further used to show that the scheme presented in this paper could be combined
with existing FHE schemes with bootstrapping to obtain more efficient FHE
schemes with bootstrapping in the fully CPA model. At the end of the paper, we
point out the insecurity of several recently proposed noise-free FHE schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06748</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06748</id><created>2016-01-25</created><authors><author><keyname>Spiteri</keyname><forenames>Kevin</forenames></author><author><keyname>Urgaonkar</keyname><forenames>Rahul</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh K.</forenames></author></authors><title>BOLA: Near-Optimal Bitrate Adaptation for Online Videos</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern video players employ complex algorithms to adapt the bitrate of the
video that is shown to the user. Bitrate adaptation requires a tradeoff between
reducing the probability that the video freezes and enhancing the quality of
the video shown to the user. A bitrate that is too high leads to frequent video
freezes (i.e., rebuffering), while a bitrate that is too low leads to poor
video quality. Video providers segment the video into short chunks and encode
each chunk at multiple bitrates. The video player adaptively chooses the
bitrate of each chunk that is downloaded, possibly choosing different bitrates
for successive chunks. While bitrate adaptation holds the key to a good quality
of experience for the user, current video players use ad-hoc algorithms that
are poorly understood. We formulate bitrate adaptation as a utility
maximization problem and devise an online control algorithm called BOLA that
uses Lyapunov optimization techniques to minimize rebuffering and maximize
video quality.We prove that BOLA achieves a time-average utility that is within
an additive term O(1/V) of the optimal value, for a control parameter V related
to the video buffer size. Further, unlike prior work, our algorithm does not
require any prediction of available network bandwidth. We empirically validate
our algorithm in a simulated network environment using an extensive collection
of network traces. We show that our algorithm achieves near-optimal utility and
in many cases significantly higher utility than current state-of-the-art
algorithms. Our work has immediate impact on real-world video players and for
the evolving DASH standard for video transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06750</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06750</id><created>2016-01-25</created><updated>2016-01-29</updated><authors><author><keyname>Padmanabhan</keyname><forenames>Divya</forenames></author><author><keyname>Bhat</keyname><forenames>Satyanath</forenames></author><author><keyname>Garg</keyname><forenames>Dinesh</forenames></author><author><keyname>Shevade</keyname><forenames>Shirish</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>A Robust UCB Scheme for Active Learning in Regression from Strategic
  Crowds</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of training an accurate linear regression model by
procuring labels from multiple noisy crowd annotators, under a budget
constraint. We propose a Bayesian model for linear regression in crowdsourcing
and use variational inference for parameter estimation. To minimize the number
of labels crowdsourced from the annotators, we adopt an active learning
approach. In this specific context, we prove the equivalence of well-studied
criteria of active learning like entropy minimization and expected error
reduction. Interestingly, we observe that we can decouple the problems of
identifying an optimal unlabeled instance and identifying an annotator to label
it. We observe a useful connection between the multi-armed bandit framework and
the annotator selection in active learning. Due to the nature of the
distribution of the rewards on the arms, we use the Robust Upper Confidence
Bound (UCB) scheme with truncated empirical mean estimator to solve the
annotator selection problem. This yields provable guarantees on the regret. We
further apply our model to the scenario where annotators are strategic and
design suitable incentives to induce them to put in their best efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06755</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06755</id><created>2016-01-25</created><authors><author><keyname>Lewis</keyname><forenames>Martha</forenames></author><author><keyname>Lawry</keyname><forenames>Jonathan</forenames></author></authors><title>The Utility of Hedged Assertions in the Emergence of Shared Categorical
  Labels</title><categories>cs.AI cs.CL cs.MA</categories><comments>AISB 2013, updated to include cross-reference to previous work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the emergence of shared concepts in a community of language
users using a multi-agent simulation. We extend results showing that negated
assertions are of use in developing shared categories, to include assertions
modified by linguistic hedges. Results show that using hedged assertions
positively affects the emergence of shared categories in two distinct ways.
Firstly, using contraction hedges like `very' gives better convergence over
time. Secondly, using expansion hedges such as `quite' reduces concept overlap.
However, both these improvements come at a cost of slower speed of development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06756</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06756</id><created>2016-01-25</created><authors><author><keyname>G&#xfc;nl&#xfc;</keyname><forenames>O.</forenames></author><author><keyname>Kramer</keyname><forenames>G.</forenames></author></authors><title>Privacy, Secrecy, and Storage with Noisy Identifiers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key-leakage-storage capacity regions for a hidden identifier's noisy
measurements at two terminals of a secrecy system are derived. The capacity
regions of binary sources with multiple measurements are obtained by applying
Mrs. Gerber's lemma twice in different directions to a Markov chain to show
gains in privacy-leakage as compared to assuming a noise-free identifier at the
encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06759</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06759</id><created>2016-01-25</created><updated>2016-02-29</updated><authors><author><keyname>Oord</keyname><forenames>Aaron van den</forenames></author><author><keyname>Kalchbrenner</keyname><forenames>Nal</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Pixel Recurrent Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling the distribution of natural images is a landmark problem in
unsupervised learning. This task requires an image model that is at once
expressive, tractable and scalable. We present a deep neural network that
sequentially predicts the pixels in an image along the two spatial dimensions.
Our method models the discrete probability of the raw pixel values and encodes
the complete set of dependencies in the image. Architectural novelties include
fast two-dimensional recurrent layers and an effective use of residual
connections in deep recurrent networks. We achieve log-likelihood scores on
natural images that are considerably better than the previous state of the art.
Our main results also provide benchmarks on the diverse ImageNet dataset.
Samples generated from the model appear crisp, varied and globally coherent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06762</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06762</id><created>2016-01-25</created><authors><author><keyname>Soorki</keyname><forenames>Mehdi Naderi</forenames></author><author><keyname>Yaghini</keyname><forenames>Mohammad</forenames></author><author><keyname>Manshaei</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Saidi</keyname><forenames>Hossein</forenames></author></authors><title>Energy-Aware Optimization and Mechanism Design for Cellular
  Device-to-Device Local Area Networks</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>To appear in the 50th Annual Conference on Information Sciences and
  Systems (CISS), Princeton, New Jersey, March 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a device-to-device (D2D) local area network (LAN), mobile users (MUs) must
cooperate to download common real-time content from a wireless cellular
network. However, sustaining such D2D LANs over cellular networks requires the
introduction of mechanisms that will incentivize the MUs to cooperate. In this
paper, the problem of energy-aware D2D LAN formation over cellular networks is
studied. The problem is formulated using a game-theoretic framework in which
each MU seeks to minimize its energy consumption while actively participating
in the D2D LAN. To account for the selfish behavior of the MUs, a punishment
and incentive protocol is proposed in order to ensure cooperation among MUs.
Within this protocol, an estimation algorithm is proposed to simulate the
process of D2D LAN formation and, then, adjust the mechanism parameters to
maintain cooperation. Simulation results show that the proposed framework can
improve energy efficiency up to 36% relative to the traditional multicast
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06763</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06763</id><created>2016-01-25</created><authors><author><keyname>Lewis</keyname><forenames>Martha</forenames></author><author><keyname>Lawry</keyname><forenames>Jonathan</forenames></author></authors><title>Emerging Dimension Weights in a Conceptual Spaces Model of Concept
  Combination</title><categories>cs.AI cs.CL cs.MA</categories><comments>AISB 2014, updated to include references to previous work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the generation of new concepts from combinations of properties
as an artificial language develops. To do so, we have developed a new framework
for conjunctive concept combination. This framework gives a semantic grounding
to the weighted sum approach to concept combination seen in the literature. We
implement the framework in a multi-agent simulation of language evolution and
show that shared combination weights emerge. The expected value and the
variance of these weights across agents may be predicted from the distribution
of elements in the conceptual space, as determined by the underlying
environment, together with the rate at which agents adopt others' concepts.
When this rate is smaller, the agents are able to converge to weights with
lower variance. However, the time taken to converge to a steady state
distribution of weights is longer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06780</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06780</id><created>2016-01-25</created><authors><author><keyname>Galas</keyname><forenames>David J.</forenames></author><author><keyname>Sakhanenko</keyname><forenames>Nikita A.</forenames></author></authors><title>Unifying formalism for multivariate information-related measures:
  M\&quot;obius operators on subset lattices</title><categories>cs.IT math.IT q-bio.OT</categories><comments>24 pages, 6 figures, 2 tabels</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-related measures are important tools of multi-variable data
analysis, as measures of dependence among variables and the description of
order in biological and physical systems. Informtion-related measures, like
marginal entropies, mutual / interaction / multi-information, are also relevant
to the theory of complexity of systems and biological data analysis. The
mathematical properties and relationships among these measures are therefore of
significant interest. A number of interesting relations between common
information measures include the duality relations based on M\&quot;obius inversion
on lattices, which are the direct consequence of the symmetries of the lattices
(subsets ordered by inclusion) of the sets of variables. In this paper we
define operators on functions on these lattices that map the functions into one
another, called M\&quot;obius operators. We show that these operators define a
simple group isomorphic to the symmetric group S3. Relations among the set of
functions on the lattice are transparently expressed in terms of this operator
algebra, and when applied to the information measures, can be used to describe
relationships among measures useful in data analysis, including conditional
probabilities. We describe a direct relation between sums of conditional
log-likelihoods and previously defined dependency/complexity measures. The
algebra can be generalized by defining operators for all possible lattice
reference elements (subsets), which yields a general set of relationships more
extensive and inclusive than the usual M\&quot;obius inversion relations. This
formalism provides a fundamental unification of information-related measures.
Isomorphism of all distributive lattices with the subset lattice implies broad
potential application of these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06805</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06805</id><created>2016-01-25</created><authors><author><keyname>Vidgen</keyname><forenames>Bertie</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author></authors><title>P-values: misunderstood and misused</title><categories>stat.AP cs.CY physics.data-an</categories><comments>Submitted to Frontiers in Physics, Interdisciplinary Physics,
  &quot;Research Topic&quot;: At the crossroads: lessons and challenges in Computational
  Social Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P-values are widely used in both the social and natural sciences to quantify
the statistical significance of observed results. The recent surge of big data
research has made p-value an even more popular tool to test the significance of
a study. However, substantial literature has been produced critiquing how
p-values are used and understood. In this paper we review this recent critical
literature, much of which is routed in the life sciences, and consider its
implications for social scientific research. We provide a coherent picture of
what the main criticisms are, and draw together and disambiguate common themes.
In particular, we explain how the False Discovery Rate is calculated, and how
this differs from a p-value. We also make explicit the Bayesian nature of many
recent criticisms, a dimension that is often underplayed or ignored. We also
identify practical steps to help remediate some of the concerns identified, and
argue that p-values need to be contextualised within (i) the specific study,
and (ii) the broader field of inquiry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06810</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06810</id><created>2016-01-25</created><authors><author><keyname>Elkayam</keyname><forenames>Nir</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>Variational formulas for the power of the binary hypothesis testing
  problem with applications</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two variational formulas for the power of the binary hypothesis testing
problem are derived. The first is given as the Legendre transform of a certain
function and the second, induced from the first, is given in terms of the
Cumulative Distribution Function (CDF) of the log-likelihood ratio. One
application of the first formula is an upper bound on the power of the binary
hypothesis testing problem in terms of the Re'nyi divergence. The second
formula provide a general framework for proving asymptotic and non-asymptotic
expressions for the power of the test utilizing corresponding expressions for
the CDF of the log-likelihood. The framework is demonstrated in the central
limit regime (i.e., for non-vanishing type I error) and in the large deviations
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06812</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06812</id><created>2016-01-25</created><authors><author><keyname>Jetpipattanapong</keyname><forenames>Duangpen</forenames></author><author><keyname>Srijuntongsiri</keyname><forenames>Gun</forenames></author></authors><title>New Pivot Selection for Sparse Symmetric Indefinite Factorization</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new pivot selection technique for symmetric indefinite
factorization of sparse matrices. Such factorization should maintain both
sparsity and numerical stability of the factors, both of which depend solely on
the choices of the pivots. Our method is based on the minimum degree algorithm
and also considers the stability of the factors at the same time. Our
experiments show that our method produces factors that are sparser than the
factors computed by MA57 and are stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06814</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06814</id><created>2016-01-25</created><authors><author><keyname>Sohrabi</keyname><forenames>Foad</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Hybrid Digital and Analog Beamforming Design for Large-Scale Antenna
  Arrays</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures, to appear in IEEE Journal of Selected Topics in
  Signal Processing, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential of using of millimeter wave (mmWave) frequency for future
wireless cellular communication systems has motivated the study of large-scale
antenna arrays for achieving highly directional beamforming. However, the
conventional fully digital beamforming methods which require one radio
frequency (RF) chain per antenna element is not viable for large-scale antenna
arrays due to the high cost and high power consumption of RF chain components
in high frequencies. To address the challenge of this hardware limitation, this
paper considers a hybrid beamforming architecture in which the overall
beamformer consists of a low-dimensional digital beamformer followed by an RF
beamformer implemented using analog phase shifters. Our aim is to show that
such an architecture can approach the performance of a fully digital scheme
with much fewer number of RF chains. Specifically, this paper establishes that
if the number of RF chains is twice the total number of data streams, the
hybrid beamforming structure can realize any fully digital beamformer exactly,
regardless of the number of antenna elements. For cases with fewer number of RF
chains, this paper further considers the hybrid beamforming design problem for
both the transmission scenario of a point-to-point multipleinput
multiple-output (MIMO) system and a downlink multiuser multiple-input
single-output (MU-MISO) system. For each scenario, we propose a heuristic
hybrid beamforming design that achieves a performance close to the performance
of the fully digital beamforming baseline. Finally, the proposed algorithms are
modified for the more practical setting in which only finite resolution phase
shifters are available. Numerical simulations show that the proposed schemes
are effective even when phase shifters with very low resolution are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06815</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06815</id><created>2016-01-25</created><authors><author><keyname>Highlander</keyname><forenames>Tyler</forenames></author><author><keyname>Rodriguez</keyname><forenames>Andres</forenames></author></authors><title>Very Efficient Training of Convolutional Neural Networks using Fast
  Fourier Transform and Overlap-and-Add</title><categories>cs.NE cs.LG</categories><comments>British Machine Vision Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) are currently state-of-the-art for
various classification tasks, but are computationally expensive. Propagating
through the convolutional layers is very slow, as each kernel in each layer
must sequentially calculate many dot products for a single forward and backward
propagation which equates to $\mathcal{O}(N^{2}n^{2})$ per kernel per layer
where the inputs are $N \times N$ arrays and the kernels are $n \times n$
arrays. Convolution can be efficiently performed as a Hadamard product in the
frequency domain. The bottleneck is the transformation which has a cost of
$\mathcal{O}(N^{2}\log_2 N)$ using the fast Fourier transform (FFT). However,
the increase in efficiency is less significant when $N\gg n$ as is the case in
CNNs. We mitigate this by using the &quot;overlap-and-add&quot; technique reducing the
computational complexity to $\mathcal{O}(N^2\log_2 n)$ per kernel. This method
increases the algorithm's efficiency in both the forward and backward
propagation, reducing the training and testing time for CNNs. Our empirical
results show our method reduces computational time by a factor of up to 16.3
times the traditional convolution implementation for a 8 $\times$ 8 kernel and
a 224 $\times$ 224 image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06823</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06823</id><created>2016-01-25</created><authors><author><keyname>Wang</keyname><forenames>Feng</forenames></author><author><keyname>Tax</keyname><forenames>David M. J.</forenames></author></authors><title>Survey on the attention based RNN model and its applications in computer
  vision</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recurrent neural networks (RNN) can be used to solve the sequence to
sequence problem, where both the input and the output have sequential
structures. Usually there are some implicit relations between the structures.
However, it is hard for the common RNN model to fully explore the relations
between the sequences. In this survey, we introduce some attention based RNN
models which can focus on different parts of the input for each output item, in
order to explore and take advantage of the implicit relations between the input
and the output items. The different attention mechanisms are described in
detail. We then introduce some applications in computer vision which apply the
attention based RNN models. The superiority of the attention based RNN model is
shown by the experimental results. At last some future research directions are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06825</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06825</id><created>2015-06-24</created><authors><author><keyname>Valdez</keyname><forenames>Nestor R.</forenames></author><author><keyname>Rivera</keyname><forenames>Marcelo V.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Experiences in Implementing an ICT-Augmented Reality as an Immersive
  Learning System for a Philippine HEI</title><categories>cs.CY</categories><comments>10 pages, 7 figures</comments><journal-ref>Asia Pacific Journal of Education, Arts and Sciences 2(2):82-91</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents the experiences in building and implementing a 3D
avatar-based virtual world (3D-AVW) as a VLE (3D-AVLE) for the Technological
University of the Philippines-Taguig (TUP-T), a higher education institution
(HEI) in the Philippines. Free and Open Source Software (FOSS) systems were
used, such as the OpenSimulator and various 3D renderers, to create a replica
of the TUP-T campus in a simulated 3D world. The 3D-AVLE runs in a single
server that is connected to the learners' computers via a simply-wired local
area network (LAN). The use of various networking optimization techniques was
experimented on to provide the learners and the instructors alike a seamless
experience and lag-less immersion within the 3D-AVLE. With the current LAN
setup in TUP-T, the optimal number of concurrent users that can be accommodated
without sacrificing connectivity and the quality of virtual experience was
found to be at 30 users, exactly the mean class size in TUP-T. The 3D-AVLE
allows for recording of the learners' experiences which provides the learners a
facility to review the lessons at a later time. Classes in fundamental topics
in engineering sciences were conducted using the usual teaching aid
technologies such as the presentation software, and by dragging-and-dropping
the presentation files to the 3D-AVLE, resulting to increased learning curve by
both the instructors and the learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06826</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06826</id><created>2016-01-25</created><updated>2016-01-27</updated><authors><author><keyname>Sheikholeslami</keyname><forenames>Azadeh</forenames></author><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Towsley</keyname><forenames>Donald</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author></authors><title>Covert Communication over Classical-Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the fundamental limits of covert, i.e., reliable-yet-undetectable,
communication have been established for general memoryless channels and for
lossy-noisy bosonic (quantum) channels with a quantum-limited adversary. The
key import of these results was the square-root law (SRL) for covert
communication, which states that $O(\sqrt{n})$ covert bits, but no more, can be
reliably transmitted over $n$ channel uses with $O(\sqrt{n})$ bits of secret
pre-shared between communicating parties. Here we prove the achievability of
the SRL for a general memoryless classical-quantum channel, showing that SRL
covert communication is achievable over any quantum communication channel with
a product-state transmission strategy. We leave open the converse, which, if
proven, would show that even using entangled transmissions and entangling
measurements, the SRL for covert communication cannot be surpassed over an
arbitrary quantum channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06834</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06834</id><created>2016-01-25</created><authors><author><keyname>Ho&#x10d;evar</keyname><forenames>Toma&#x17e;</forenames></author><author><keyname>Dem&#x161;ar</keyname><forenames>Janez</forenames></author></authors><title>Combinatorial algorithm for counting small induced graphs and orbits</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphlet analysis is an approach to network analysis that is particularly
popular in bioinformatics. We show how to set up a system of linear equations
that relate the orbit counts and can be used in an algorithm that is
significantly faster than the existing approaches based on direct enumeration
of graphlets. The algorithm requires existence of a vertex with certain
properties; we show that such vertex exists for graphlets of arbitrary size,
except for complete graphs and $C_4$, which are treated separately. Empirical
analysis of running time agrees with the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06838</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06838</id><created>2016-01-25</created><authors><author><keyname>Dehghan</keyname><forenames>Mostafa</forenames></author><author><keyname>Massoulie</keyname><forenames>Laurent</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Menasche</keyname><forenames>Daniel</forenames></author><author><keyname>Tay</keyname><forenames>Y. C.</forenames></author></authors><title>A Utility Optimization Approach to Network Cache Design</title><categories>cs.NI</categories><comments>IEEE INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In any caching system, the admission and eviction policies determine which
contents are added and removed from a cache when a miss occurs. Usually, these
policies are devised so as to mitigate staleness and increase the hit
probability. Nonetheless, the utility of having a high hit probability can vary
across contents. This occurs, for instance, when service level agreements must
be met, or if certain contents are more difficult to obtain than others. In
this paper, we propose utility-driven caching, where we associate with each
content a utility, which is a function of the corresponding content hit
probability. We formulate optimization problems where the objectives are to
maximize the sum of utilities over all contents. These problems differ
according to the stringency of the cache capacity constraint. Our framework
enables us to reverse engineer classical replacement policies such as LRU and
FIFO, by computing the utility functions that they maximize. We also develop
online algorithms that can be used by service providers to implement various
caching policies based on arbitrary utility functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06847</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06847</id><created>2016-01-25</created><updated>2016-01-28</updated><authors><author><keyname>Biason</keyname><forenames>Alessandro</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Battery-Powered Devices in WPCNs</title><categories>cs.IT math.IT</categories><comments>13 pages, 11 figures, submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless powered communication networks are becoming an effective solution
for improving self sustainability of mobile devices. In this context, a hybrid
access point transfers energy to a group of nodes, which use the harvested
energy to perform computation or transmission tasks. While the availability of
the wireless energy transfer mechanism opens up new frontiers, an appropriate
choice of the network parameters (e.g., transmission powers, transmission
duration, amount of transferred energy, etc.) is required in order to achieve
high performance. In this work, we study the throughput optimization problem in
a system composed of an access point which recharges the batteries of two
devices at different distances. In the literature, the main focus so far has
been on slot-oriented optimization, in which all the harvested energy is used
in the same slot in which it is harvested. However, this approach is strongly
sub-optimal because it does not exploit the possibility to store the energy and
use it at a later time. Thus, instead of considering the slot-oriented case, we
address the long-term maximization. This assumption greatly increases the
optimization complexity, requiring to consider, e.g., the channel state
realizations, its statistics and the batteries evolution. Our objective is to
find the best scheduling scheme, both for the energy transferred by the access
point and for the data sent by the two nodes. We discuss how to perform the
maximization with optimal as well as approximate techniques and show that the
slot-oriented policies proposed so far are strongly sub-optimal in the long
run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06859</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06859</id><created>2016-01-25</created><authors><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Gurewitz</keyname><forenames>Omer</forenames></author><author><keyname>Menasche</keyname><forenames>Daniel S.</forenames></author><author><keyname>Shifrin</keyname><forenames>Mark</forenames></author></authors><title>Optimal Dynamic Routing for the Wireless Relay Channel</title><categories>cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a communication network with a source, a relay and a destination.
Each time interval, the source may dynamically choose between a few possible
coding schemes, based on the channel state, traffic pattern and its own queue
status. For example, the source may choose between a direct route to the
destination and a relay-assisted scheme. Clearly, due to the difference in the
performance achieved, as well as the resources each scheme uses, a sender might
wish to choose the most appropriate one based on its status.
  In this work, we formulate the problem as a Semi-Markov Decision Process.
This formulation allows us to find an optimal policy, expressed as a function
of the number of packets in the source queue and other parameters. In
particular, we show a general solution which covers various configurations,
including different packet size distributions and varying channels.
Furthermore, for the case of exponential transmission times, we analytically
prove the optimal policy has a threshold structure, that is, there is a unique
value of a single parameter which determines which scheme (or route) is
optimal. Results are also validated with simulations for several interesting
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06862</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06862</id><created>2016-01-25</created><authors><author><keyname>Fauvel</keyname><forenames>Simon</forenames></author><author><keyname>Yu</keyname><forenames>Han</forenames></author></authors><title>A Survey on Artificial Intelligence and Data Mining for MOOCs</title><categories>cs.AI cs.CY</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive Open Online Courses (MOOCs) have gained tremendous popularity in the
last few years. Thanks to MOOCs, millions of learners from all over the world
have taken thousands of high-quality courses for free. Putting together an
excellent MOOC ecosystem is a multidisciplinary endeavour that requires
contributions from many different fields. Artificial intelligence (AI) and data
mining (DM) are two such fields that have played a significant role in making
MOOCs what they are today. By exploiting the vast amount of data generated by
learners engaging in MOOCs, DM improves our understanding of the MOOC ecosystem
and enables MOOC practitioners to deliver better courses. Similarly, AI,
supported by DM, can greatly improve student experience and learning outcomes.
In this survey paper, we first review the state-of-the-art artificial
intelligence and data mining research applied to MOOCs, emphasising the use of
AI and DM tools and techniques to improve student engagement, learning
outcomes, and our understanding of the MOOC ecosystem. We then offer an
overview of key trends and important research to carry out in the fields of AI
and DM so that MOOCs can reach their full potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06865</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06865</id><created>2016-01-25</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>van Garderen</keyname><forenames>Mereke</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Convex-Arc Drawings of Pseudolines</title><categories>cs.CG</categories><comments>11 pages, 8 figures. A preliminary announcement of these results was
  made as a poster at the 21st International Symposium on Graph Drawing,
  Bordeaux, France, September 2013, and published in Lecture Notes in Computer
  Science 8242, Springer, 2013, pp. 522--523</comments><msc-class>68R10, 05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weak pseudoline arrangement is a topological generalization of a line
arrangement, consisting of curves topologically equivalent to lines that cross
each other at most once. We consider arrangements that are outerplanar---each
crossing is incident to an unbounded face---and simple---each crossing point is
the crossing of only two curves. We show that these arrangements can be
represented by chords of a circle, by convex polygonal chains with only two
bends, or by hyperbolic lines. Simple but non-outerplanar arrangements
(non-weak) can be represented by convex polygonal chains or convex smooth
curves of linear complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06868</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06868</id><created>2016-01-25</created><authors><author><keyname>Wu</keyname><forenames>Jingxian</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author></authors><title>Quickest Change Detection with Mismatched Post-Change Models</title><categories>stat.ME cs.IT math.IT</categories><comments>Abbreviated version submitted to IEEE ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the quickest change detection with mismatched
post-change models. A change point is the time instant at which the
distribution of a random process changes. The objective of quickest change
detection is to minimize the detection delay of an unknown change point under
certain performance constraints, such as average run length (ARL) to false
alarm or probability of false alarm (PFA). Most existing change detection
procedures assume perfect knowledge of the random process distributions before
and after the change point. However, in many practical applications such as
anomaly detection, the post-change distribution is often unknown and needs to
be estimated with a limited number of samples. In this paper, we study the case
that there is a mismatch between the true post-change distribution and the one
used during detection. We analytically identify the impacts of mismatched
post-change models on two classical detection procedures, the cumulative sum
(CUSUM) procedure and the Shiryaev-Roberts (SR) procedure. The impacts of
mismatched models are characterized in terms of various finite or asymptotic
performance bounds on ARL, PFA, and average detection delay (ADD). It is shown
that post-change model mismatch results in an increase in ADD, and the rate of
performance degradation depends on the difference between two Kullback-Leibler
(KL) divergences, one is between the priori- and post-change distributions, and
the other one is between the true and mismatched post-change distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06869</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06869</id><created>2016-01-25</created><authors><author><keyname>Pan</keyname><forenames>Zhengxiang</forenames></author></authors><title>The Roles of Familiarity Design in Active Ageing</title><categories>cs.HC</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The elderly often struggle when interacting with technologies. This is
because the software and hardware components of the technologies are not
familiar to the elderly's mental model. This is a lack of empirical studies
about how the concept of familiarity can be infused into the design of
interactive technology systems to bridge the digital divide preventing today's
elderly people from actively engaging with such technologies. In this paper, a
multi pronged approach is utilized. We investigate the Effects of Familiarity
in Design on the Adoption of Wellness Games by the Elderly, familiarity in
productive ageing, familiarity in efficient collaborative crowdsourcing,
productive ageing through familiarity based Intelligent Personalized
Crowdsourcing and familiarity based Agent Augmented Inter-generational
Crowdsourcing. The results show that familiarity in design improves the
perceived satisfaction and adoption likelihood significantly among the elderly
users. These results can potentially benefit intelligent interface agent design
when such agents need to interact with elderly users. A Crowdsourcing
algorithm, CrowdAsm is developed. By using CrowdAsm we are able to dynamically
assemble teams of workers considering the budgets, the availability of workers
with the required skills and their track records to complete crowdsourcing
tasks requiring collaboration among workers with heterogeneous skills.
Theoretical analysis has shown that CrowdAsm can achieve close to optimal
profit for a collaborative crowdsourcing system if workers follow the
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06873</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06873</id><created>2016-01-25</created><authors><author><keyname>Li</keyname><forenames>Binglin</forenames></author><author><keyname>Wei</keyname><forenames>Shuangqing</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Yuan</keyname><forenames>Jian</forenames></author></authors><title>Chernoff Information of Bottleneck Gaussian Trees</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT2016, and this version contains proofs of the
  propositions in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, our objective is to find out the determining factors of
Chernoff information in distinguishing a set of Gaussian trees. In this set,
each tree can be attained via an edge removal and grafting operation from
another tree. This is equivalent to asking for the Chernoff information between
the most-likely confused, i.e. &quot;bottleneck&quot;, Gaussian trees, as shown to be the
case in ML estimated Gaussian tree graphs lately. We prove that the Chernoff
information between two Gaussian trees related through an edge removal and a
grafting operation is the same as that between two three-node Gaussian trees,
whose topologies and edge weights are subject to the underlying graph
operation. In addition, such Chernoff information is shown to be determined
only by the maximum generalized eigenvalue of the two Gaussian covariance
matrices. The Chernoff information of scalar Gaussian variables as a result of
linear transformation (LT) of the original Gaussian vectors is also uniquely
determined by the same maximum generalized eigenvalue. What is even more
interesting is that after incorporating the cost of measurements into a
normalized Chernoff information, Gaussian variables from LT have larger
normalized Chernoff information than the one based on the original Gaussian
vectors, as shown in our proved bounds
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06880</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06880</id><created>2016-01-25</created><authors><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author></authors><title>Bounds on Asymptotic Rate of Capacitive Crosstalk Avoidance Codes for
  On-chip Buses</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to prevent the capacitive crosstalk in on-chip buses, several types
of capacitive crosstalk avoidance codes have been devised. These codes are
designed to prohibit transition patterns prone to the capacity crosstalk from
any consecutive two words transmitted to on-chip buses. This paper provides a
rigorous analysis on the asymptotic rate of (p,q)-transition free word
sequences under the assumption that coding is based on a pair of a stateful
encoder and a stateless decoder. The symbols p and q represent k-bit transition
patterns that should not be appeared in any consecutive two words at the same
adjacent k-bit positions. It is proved that the maximum rate of the sequences
equals to the subgraph domatic number of (p,q)-transition free graph. Based on
the theoretical results on the subgraph domatic partition problem, a pair of
lower and upper bounds on the asymptotic rate is derived. We also present that
the asymptotic rate 0.8325 is achievable for the (10,01)-transition free word
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06882</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06882</id><created>2016-01-25</created><authors><author><keyname>Salamatian</keyname><forenames>Salman</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>Efficient Coding for Multi-source Networks using G\'acs-K\&quot;orner Common
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a multi-source network coding problem with correlated sources. While
the fundamental limits are known, achieving them, in general, involves a
computational burden due to the complex decoding process. Efficient solutions,
on the other hand, are by large based on source and network coding separation,
thus imposing strict topological constraints on the networks which can be
solved.
  In this work, we introduce a novel notion of separation of source and network
coding using G\'acs-K\&quot;orner Common Information (CI). Unlike existing notions
of separation, the sufficient condition for this separation to hold depends on
the source structure rather than the network topology. Using the suggested
separation scheme, we tackle three important multi-source problems. The first
is the multi-source multicast. We construct efficient, zero error source codes,
and via properties of the CI completely characterize the resulting rate region.
The second is broadcast with side information. We establish a duality between
this problem and the classical problem of degraded message set broadcast, and
give two code constructions and their associated regions. Finally, we consider
the Ahlswede-Korner problem in a network, and give an efficient solution which
is tight under the CI constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06884</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06884</id><created>2016-01-25</created><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Sricharan</keyname><forenames>Kumar</forenames></author><author><keyname>Greenewald</keyname><forenames>Kristjan</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Improving Convergence of Divergence Functional Ensemble Estimators</title><categories>cs.IT math.IT</categories><comments>19 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has focused on the problem of nonparametric estimation of
divergence functionals. Many existing approaches are restrictive in their
assumptions on the density support or require difficult calculations at the
support boundary which must be known a priori. We derive the MSE convergence
rate of a leave-one-out kernel density plug-in divergence functional estimator
for general bounded density support sets where knowledge of the support
boundary is not required. We then generalize the theory of optimally weighted
ensemble estimation to derive two estimators that achieve the parametric rate
when the densities are sufficiently smooth. The asymptotic distribution of
these estimators and some guidelines for tuning parameter selection are
provided. Based on the theory, we propose an empirical estimator of
R\'enyi-$\alpha$ divergence that outperforms the standard kernel density
plug-in estimator, especially in high dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06885</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06885</id><created>2016-01-25</created><authors><author><keyname>Gabrys</keyname><forenames>Ryan</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Codes in the Damerau Distance for DNA Storage</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the new problem of code design in the Damerau metric. The
Damerau metric is a generalization of the Levenshtein distance which also
allows for adjacent transposition edits. We first provide constructions for
codes that may correct either a single deletion or a single adjacent
transposition and then proceed to extend these results to codes that can
simultaneously correct a single deletion and multiple adjacent transpositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06887</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06887</id><created>2016-01-25</created><authors><author><keyname>Gabrys</keyname><forenames>Ryan</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Balanced Permutation Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by charge balancing constraints for rank modulation schemes, we
introduce the notion of balanced permutations and derive the capacity of
balanced permutation codes. We also describe simple interleaving methods for
permutation code constructions and show that they approach capacity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06892</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06892</id><created>2016-01-26</created><updated>2016-03-07</updated><authors><author><keyname>Kulkarni</keyname><forenames>Kuldeep</forenames></author><author><keyname>Lohit</keyname><forenames>Suhas</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author><author><keyname>Kerviche</keyname><forenames>Ronan</forenames></author><author><keyname>Ashok</keyname><forenames>Amit</forenames></author></authors><title>ReconNet: Non-Iterative Reconstruction of Images from Compressively
  Sensed Random Measurements</title><categories>cs.CV</categories><comments>Accepted at IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to present a non-iterative and more importantly an
extremely fast algorithm to reconstruct images from compressively sensed (CS)
random measurements. To this end, we propose a novel convolutional neural
network (CNN) architecture which takes in CS measurements of an image as input
and outputs an intermediate reconstruction. We call this network, ReconNet. The
intermediate reconstruction is fed into an off-the-shelf denoiser to obtain the
final reconstructed image. On a standard dataset of images we show significant
improvements in reconstruction results (both in terms of PSNR and time
complexity) over state-of-the-art iterative CS reconstruction algorithms at
various measurement rates. Further, through qualitative experiments on real
data collected using our block single pixel camera (SPC), we show that our
network is highly robust to sensor noise and can recover visually better
quality images than competitive algorithms at extremely low sensing rates of
0.1 and 0.04. To demonstrate that our algorithm can recover semantically
informative images even at a low measurement rate of 0.01, we present a very
robust proof of concept real-time visual tracking application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06895</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06895</id><created>2016-01-26</created><authors><author><keyname>Chen</keyname><forenames>Mingzhe</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Yin</keyname><forenames>Changchuan</forenames></author></authors><title>Echo State Networks for Self-Organizing Resource Allocation in LTE-U
  with Uplink-Downlink Decoupling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uplink-downlink decoupling in which users can be associated to different base
stations in the uplink and downlink of heterogeneous small cell networks (SCNs)
has attracted significant attention recently. However, most existing works
focus on simple association mechanisms in LTE SCNs that operate only in the
licensed band. In contrast, in this paper, the problem of resource allocation
with uplink-downlink decoupling is studied for an SCN that incorporates LTE in
the unlicensed band (LTE-U). Here, the users can access both licensed and
unlicensed bands while being associated to different base stations. This
problem is formulated as a noncooperative game that incorporates user
association, spectrum allocation, and load balancing. To solve this problem, a
distributed algorithm based on the machine learning framework of echo state
networks is proposed using which the small base stations autonomously choose
their optimal bands allocation strategies while having only limited information
on the network's and users' states. It is shown that the proposed algorithm
converges to a stationary mixed-strategy distribution which constitutes a mixed
strategy Nash equilibrium for the studied game. Simulation results show that
the proposed approach yields significant gains, in terms of the sum-rate of the
50th percentile of users, that reach up to 60% and 78%, respectively, compared
to Q-learning and Q-learning without decoupling. The results also show that ESN
significantly provides a considerable reduction of information exchange for the
wireless network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06899</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06899</id><created>2016-01-26</created><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author></authors><title>Coded Compressive Sensing: A Compute-and-Recover Approach</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose \textit{coded compressive sensing} that recovers an
$n$-dimensional integer sparse signal vector from a noisy and quantized
measurement vector whose dimension $m$ is far-fewer than $n$. The core idea of
coded compressive sensing is to construct a linear sensing matrix whose columns
consist of lattice codes. We present a two-stage decoding method named
\textit{compute-and-recover} to detect the sparse signal from the noisy and
quantized measurements. In the first stage, we transform such measurements into
noiseless finite-field measurements using the linearity of lattice codewords.
In the second stage, syndrome decoding is applied over the finite-field to
reconstruct the sparse signal vector. A sufficient condition of a perfect
recovery is derived. Our theoretical result demonstrates an interplay among the
quantization level $p$, the sparsity level $k$, the signal dimension $n$, and
the number of measurements $m$ for the perfect recovery. Considering 1-bit
compressive sensing as a special case, we show that the proposed algorithm
empirically outperforms an existing greedy recovery algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06903</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06903</id><created>2016-01-26</created><authors><author><keyname>Lee</keyname><forenames>Donghyuk</forenames></author><author><keyname>Kim</keyname><forenames>Yoongu</forenames></author><author><keyname>Seshadri</keyname><forenames>Vivek</forenames></author><author><keyname>Liu</keyname><forenames>Jamie</forenames></author><author><keyname>Subramanian</keyname><forenames>Lavanya</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Tiered-Latency DRAM (TL-DRAM)</title><categories>cs.AR</categories><comments>This is a summary of the original paper, entitled &quot;Tiered-Latency
  DRAM: A Low Latency and Low Cost DRAM Architecture&quot; which appears in HPCA
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the idea of Tiered-Latency DRAM, which was published in
HPCA 2013. The key goal of TL-DRAM is to provide low DRAM latency at low cost,
a critical problem in modern memory systems. To this end, TL-DRAM introduces
heterogeneity into the design of a DRAM subarray by segmenting the bitlines,
thereby creating a low-latency, low-energy, low-capacity portion in the
subarray (called the near segment), which is close to the sense amplifiers, and
a high-latency, high-energy, high-capacity portion, which is farther away from
the sense amplifiers. Thus, DRAM becomes heterogeneous with a small portion
having lower latency and a large portion having higher latency. Various
techniques can be employed to take advantage of the low-latency near segment
and this new heterogeneous DRAM substrate, including hardware-based caching and
software based caching and memory allocation of frequently used data in the
near segment. Evaluations with simple such techniques show significant
performance and energy-efficiency benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06906</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06906</id><created>2016-01-26</created><authors><author><keyname>Lombeyda</keyname><forenames>Santiago V.</forenames></author></authors><title>Distinct 3D Glyphs with Data Layering for Highly Dense Multivariate Data
  Plots</title><categories>cs.HC</categories><comments>6 pages. 9 figures. written in 2009. originally destined as a public
  internal CACR/Caltech report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A carefully constructed scatterplot can reveal plenty about an underlying
data set. However, in most cases visually mining and understanding a large
multivariate data set requires more finesse, and greater level of interactivity
to really grasp the full spectrum of the information being presented. We
present a paradigm for glyph design and use in the creation of single plots
presenting multiple variables of information. We center our design on two key
concepts. The first concept is that visually it is easier to discriminate
between completely distinct shapes rather than subtly different ones, specially
when partially occluded. The second one is that users ingest information in
layers, i.e. in an order of visual relevance. Using this paradigm, we present
complex data as binned into desired and relevant discrete categories. We show
results in the areas of high energy physics and security, displaying over 6
distinct data variables in each single plot, yielding a clear, highly readable,
and effective visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06908</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06908</id><created>2016-01-26</created><authors><author><keyname>Soro</keyname><forenames>Alexandre</forenames></author><author><keyname>Lacan</keyname><forenames>Jerome</forenames></author><author><keyname>Roca</keyname><forenames>Vincent</forenames></author><author><keyname>Savin</keyname><forenames>Valentin</forenames></author><author><keyname>Cunche</keyname><forenames>Mathieu</forenames></author></authors><title>Enhanced Recursive Reed-Muller Erasure Decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work have shown that Reed-Muller (RM) codes achieve the erasure
channel capacity. However, this performance is obtained with maximum-likelihood
decoding which can be costly for practical applications. In this paper, we
propose an encoding/decoding scheme for Reed-Muller codes on the packet erasure
channel based on Plotkin construction. We present several improvements over the
generic decoding. They allow, for a light cost, to compete with
maximum-likelihood decoding performance, especially on high-rate codes, while
significantly outperforming it in terms of speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06915</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06915</id><created>2016-01-26</created><updated>2016-01-27</updated><authors><author><keyname>AlBdaiwi</keyname><forenames>Bader</forenames></author><author><keyname>Hussain</keyname><forenames>Zaid</forenames></author><author><keyname>Cerny</keyname><forenames>Anton</forenames></author><author><keyname>Aldred</keyname><forenames>Robert</forenames></author></authors><title>Edge-Disjoint Node-Independent Spanning Trees in Dense Gaussian Networks</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent trees are used in building secure and/or fault-tolerant network
communication protocols. They have been investigated for different network
topologies including tori. Dense Gaussian networks are potential alternatives
for 2-dimensional tori. They have similar topological properties; however, they
are superiors in carrying communications due to their node-distance
distributions and smaller diameters. In this paper, we present constructions of
edge-disjoint node-independent spanning trees in dense Gaussian networks. Based
on the constructed trees, we design algorithms that could be used in
fault-tolerant routing or secure message distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06919</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06919</id><created>2016-01-26</created><authors><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Marino</keyname><forenames>Andrea</forenames></author><author><keyname>Santini</keyname><forenames>Massimo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>BUbiNG: Massive Crawling for the Masses</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although web crawlers have been around for twenty years by now, there is
virtually no freely available, opensource crawling software that guarantees
high throughput, overcomes the limits of single-machine systems and at the same
time scales linearly with the amount of resources available. This paper aims at
filling this gap, through the description of BUbiNG, our next-generation web
crawler built upon the authors' experience with UbiCrawler [Boldi et al. 2004]
and on the last ten years of research on the topic. BUbiNG is an opensource
Java fully distributed crawler; a single BUbiNG agent, using sizeable hardware,
can crawl several thousands pages per second respecting strict politeness
constraints, both host- and IP-based. Unlike existing open-source distributed
crawlers that rely on batch techniques (like MapReduce), BUbiNG job
distribution is based on modern high-speed protocols so to achieve very high
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06923</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06923</id><created>2016-01-26</created><updated>2016-02-24</updated><authors><author><keyname>Fu</keyname><forenames>Chen</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin L.</forenames></author><author><keyname>Chen</keyname><forenames>Bao Xin</forenames></author><author><keyname>Chen</keyname><forenames>Zhou Rong</forenames></author><author><keyname>Jin</keyname><forenames>Xiang Lan</forenames></author><author><keyname>Guo</keyname><forenames>Rong Juan</forenames></author><author><keyname>Chen</keyname><forenames>Zhi Gang</forenames></author><author><keyname>Zhang</keyname><forenames>Yun Ling</forenames></author></authors><title>Identification and classification of TCM syndrome types among patients
  with vascular mild cognitive impairment using latent tree analysis</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: To treat patients with vascular mild cognitive impairment (VMCI)
using TCM, it is necessary to classify the patients into TCM syndrome types and
to apply different treatments to different types. We investigate how to
properly carry out the classification using a novel data-driven method known as
latent tree analysis.
  Method: A cross-sectional survey on VMCI was carried out in several regions
in northern China from 2008 to 2011, which resulted in a data set that involves
803 patients and 93 symptoms. Latent tree analysis was performed on the data to
reveal symptom co-occurrence patterns, and the patients were partitioned into
clusters in multiple ways based on the patterns. The patient clusters were
matched up with syndrome types, and population statistics of the clusters are
used to quantify the syndrome types and to establish classification rules.
  Results: Eight syndrome types are identified: Qi Deficiency, Qi Stagnation,
Blood Deficiency, Blood Stasis, Phlegm-Dampness, Fire-Heat, Yang Deficiency,
and Yin Deficiency. The prevalence and symptom occurrence characteristics of
each syndrome type are determined. Quantitative classification rules are
established for determining whether a patient belongs to each of the syndrome
types.
  Conclusions: A solution for the TCM syndrome classification problem
associated with VMCI is established based on the latent tree analysis of
unlabeled symptom survey data. The results can be used as a reference in clinic
practice to improve the quality of syndrome differentiation and to reduce
diagnosis variances across physicians. They can also be used for patient
selection in research projects aimed at finding biomarkers for the syndrome
types and in randomized control trials aimed at determining the efficacy of TCM
treatments of VMCI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06925</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06925</id><created>2016-01-26</created><authors><author><keyname>Rosso</keyname><forenames>Osvaldo A.</forenames></author><author><keyname>Ospina</keyname><forenames>Raydonal</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author></authors><title>Classification and Verification of Online Handwritten Signatures with
  Time Causal Information Theory Quantifiers</title><categories>cs.IT cs.CV math.IT</categories><comments>Submitted to PLOS One</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach for online handwritten signature classification and
verification based on descriptors stemming from Information Theory. The
proposal uses the Shannon Entropy, the Statistical Complexity, and the Fisher
Information evaluated over the Bandt and Pompe symbolization of the horizontal
and vertical coordinates of signatures. These six features are easy and fast to
compute, and they are the input to an One-Class Support Vector Machine
classifier. The results produced surpass state-of-the-art techniques that
employ higher-dimensional feature spaces which often require specialized
software and hardware. We assess the consistency of our proposal with respect
to the size of the training sample, and we also use it to classify the
signatures into meaningful groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06931</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06931</id><created>2016-01-26</created><authors><author><keyname>Castro</keyname><forenames>F. M.</forenames></author><author><keyname>Mar&#xed;n-Jim&#xe9;nez</keyname><forenames>M. J.</forenames></author><author><keyname>Guil</keyname><forenames>N.</forenames></author><author><keyname>Mu&#xf1;oz-Salinas</keyname><forenames>R.</forenames></author></authors><title>Fisher Motion Descriptor for Multiview Gait Recognition</title><categories>cs.CV cs.AI</categories><comments>This paper extends with new experiments the one published at
  ICPR'2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to identify individuals by analyzing their gait.
Instead of using binary silhouettes as input data (as done in many previous
works) we propose and evaluate the use of motion descriptors based on densely
sampled short-term trajectories. We take advantage of state-of-the-art people
detectors to define custom spatial configurations of the descriptors around the
target person, obtaining a rich representation of the gait motion. The local
motion features (described by the Divergence-Curl-Shear descriptor) extracted
on the different spatial areas of the person are combined into a single
high-level gait descriptor by using the Fisher Vector encoding. The proposed
approach, coined Pyramidal Fisher Motion, is experimentally validated on
`CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and the
recent `AVA Multiview Gait' dataset. The results show that this new approach
achieves state-of-the-art results in the problem of gait recognition, allowing
to recognize walking people from diverse viewpoints on single and multiple
camera setups, wearing different clothes, carrying bags, walking at diverse
speeds and not limited to straight walking paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06933</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06933</id><created>2016-01-26</created><authors><author><keyname>Montazeri</keyname><forenames>Mohadeseh</forenames></author><author><keyname>Naji</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Montazeri</keyname><forenames>Mitra</forenames></author><author><keyname>Faraahi</keyname><forenames>Ahmad</forenames></author></authors><title>A Novel Memetic Feature Selection Algorithm</title><categories>cs.LG</categories><comments>M., Montazeri, H. R. Naji, M. Montazeri, A. Faraahi. A novel memetic
  feature selection algorithm. In Information and Knowledge Technology (IKT),
  2013 5th Conference on. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is a problem of finding efficient features among all
features in which the final feature set can improve accuracy and reduce
complexity. In feature selection algorithms search strategies are key aspects.
Since feature selection is an NP-Hard problem; therefore heuristic algorithms
have been studied to solve this problem. In this paper, we have proposed a
method based on memetic algorithm to find an efficient feature subset for a
classification problem. It incorporates a filter method in the genetic
algorithm to improve classification performance and accelerates the search in
identifying core feature subsets. Particularly, the method adds or deletes a
feature from a candidate feature subset based on the multivariate feature
information. Empirical study on commonly data sets of the university of
California, Irvine shows that the proposed method outperforms existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06939</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06939</id><created>2016-01-26</created><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Simple and Efficient Fully-Functional Succinct Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fully-functional succinct tree representation of Navarro and Sadakane
({\em ACM Transactions on Algorithms}, 2014) supports a large number of
operations in constant time using $2n+o(n)$ bits. However, the full idea it is
hard to implement. Only a simplified version with $O(\log n)$ operation time
has been implemented and shown to be practical and competitive. We describe a
new variant that is much simpler to implement and has worst-case time
$O(\log\log n)$ for the operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06945</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06945</id><created>2016-01-26</created><authors><author><keyname>Ulyantsev</keyname><forenames>Vladimir</forenames></author><author><keyname>Buzhinsky</keyname><forenames>Igor</forenames></author><author><keyname>Shalyto</keyname><forenames>Anatoly</forenames></author></authors><title>Exact Finite-State Machine Identification from Scenarios and Temporal
  Properties</title><categories>cs.SE cs.FL</categories><comments>19 pages, 7 figures, 6 tables, submitted to International Journal on
  Software Tools for Technology Transfer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite-state models, such as finite-state machines (FSMs), aid software
engineering in many ways. They are often used in formal verification and also
can serve as visual software models. The latter application is associated with
the problems of software synthesis and automatic derivation of software models
from specification. Smaller synthesized models are more general and are easier
to comprehend, yet the problem of minimum FSM identification has received
little attention in previous research.
  This paper presents four exact methods to tackle the problem of minimum FSM
identification from a set of test scenarios and a temporal specification
represented in linear temporal logic. The methods are implemented as an
open-source tool. Three of them are based on translations of the FSM
identification problem to SAT or QSAT problem instances. Accounting for
temporal properties is done via counterexample prohibition. Counterexamples are
either obtained from previously identified FSMs, or based on bounded model
checking. The fourth method uses backtracking. The proposed methods are
evaluated on several case studies and on a larger number of randomly generated
instances of increasing complexity. The results show that the Iterative
SAT-based method is the leader among the proposed methods. The methods are also
compared with existing inexact approaches, i.e. the ones which do not
necessarily identify the minimum FSM, and these comparisons show encouraging
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06950</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06950</id><created>2016-01-26</created><authors><author><keyname>Waechter</keyname><forenames>Michael</forenames></author><author><keyname>Beljan</keyname><forenames>Mate</forenames></author><author><keyname>Fuhrmann</keyname><forenames>Simon</forenames></author><author><keyname>Moehrle</keyname><forenames>Nils</forenames></author><author><keyname>Kopf</keyname><forenames>Johannes</forenames></author><author><keyname>Goesele</keyname><forenames>Michael</forenames></author></authors><title>Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction</title><categories>cs.CV cs.GR</categories><comments>10 pages, 12 figures, paper was submitted to ACM Transactions on
  Graphics for review</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ultimate goal of many image-based modeling systems is to render
photo-realistic novel views of a scene without visible artifacts. Existing
evaluation metrics and benchmarks focus mainly on the geometric accuracy of the
reconstructed model, which is, however, a poor predictor of visual accuracy.
Furthermore, using only geometric accuracy by itself does not allow evaluating
systems that either lack a geometric scene representation or utilize coarse
proxy geometry. Examples include light field or image-based rendering systems.
We propose a unified evaluation approach based on novel view prediction error
that is able to analyze the visual quality of any method that can render novel
views from input images. One of the key advantages of this approach is that it
does not require ground truth geometry. This dramatically simplifies the
creation of test datasets and benchmarks. It also allows us to evaluate the
quality of an unknown scene during the acquisition and reconstruction process,
which is useful for acquisition planning. We evaluate our approach on a range
of methods including standard geometry-plus-texture pipelines as well as
image-based rendering techniques, compare it to existing geometry-based
benchmarks, and demonstrate its utility for a range of use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06962</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06962</id><created>2016-01-26</created><authors><author><keyname>Fass</keyname><forenames>Didier</forenames><affiliation>MOSEL</affiliation></author><author><keyname>Gechter</keyname><forenames>Franck</forenames><affiliation>IRTES - SET</affiliation></author></authors><title>Towards a Theory for Bio - Cyber Physical Systems Modelling</title><categories>q-bio.QM cs.CY</categories><comments>HCI International 2015, Aug 2015, Los Angeles, United States. LNCS
  9184, 2015, LNCS - Digital Human Modeling and applications in Health, Safety,
  Ergonomics and Risk Management: Human Modelling (Part I)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-21073-5_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, CyberPhysical Systems (CPS) represents a great challenge for
automatic control and smart systems engineering on both theoretical and
practical levels. Designing CPS requires approaches involving multidisciplinary
competences. However they are designed to be autonomous, the CPS present a part
of uncertainty, which requires interaction with human for engineering,
monitoring, controlling, performing operational maintenance, etc. This
human-CPS interaction led naturally to the human in-the-loop (HITL) concept.
Nevertheless, this HITL concept , which stems from a reductionist point of
view, exhibits limitations due to the different natures of the systems
involved. As opposed to this classical approach, we propose, in this paper, a
model of Bio-CPS (i.e. systems based on an integration of computational
elements within biological systems) grounded on theoretical biology, physics
and computer sciences and based on the key concept of human systems
integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06971</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06971</id><created>2016-01-26</created><updated>2016-02-15</updated><authors><author><keyname>Kharde</keyname><forenames>Vishal. A.</forenames></author><author><keyname>Sonawane</keyname><forenames>Sheetal.</forenames></author></authors><title>Sentiment Analysis of Twitter Data :A Survey of Techniques</title><categories>cs.CL</categories><comments>This paper has been withdrawn by the author due to a crucial changes
  in results section. It had some inconsistency in results. It requires major
  changes and revisions in methods discussed. There are some major errors in
  effects of methods discussed</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of web technology and its growth, there is a huge volume
of data present in the web for internet users and a lot of data is generated
too. Internet has become a platform for online learning, exchanging ideas and
sharing opinions. Social networking sites like Twitter, Facebook, Google+ are
rapidly gaining popularity as they allow people to share and express their
views about topics,have discussion with different communities, or post messages
across the world. There has been lot of work in the field of sentiment analysis
of twitter data. This survey focuses mainly on sentiment analysis of twitter
data which is helpful to analyze the information in the tweets where opinions
are highly unstructured, heterogeneous and are either positive or negative, or
neutral in some cases. In this paper, we provide a survey and a comparative
analyses of existing techniques for opinion mining like machine learning and
lexicon-based approaches, together with evaluation metrics. Using various
machine learning algorithms like Naive Bayes, Max Entropy, and Support Vector
Machine, we provide a research on twitter data streams. We have also discussed
general challenges and applications of Sentiment Analysis on Twitter
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06974</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06974</id><created>2016-01-26</created><authors><author><keyname>Krenn</keyname><forenames>Willibald</forenames></author><author><keyname>Schlick</keyname><forenames>Rupert</forenames></author></authors><title>Mutation-driven Test Case Generation Using Short-lived Concurrent
  Mutants -- First Results</title><categories>cs.SE</categories><comments>19 pages, 8 figures, 7 industrial case studies with up to 2847
  concurrent UML state machines</comments><report-no>TR 2016-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of black-box testing, generating test cases through model
mutation is known to produce powerful test suites but usually has the drawback
of being prohibitively expensive. This paper presents a new version of the tool
MoMuT::UML (www.momut.org), which implements a scalable version of
mutation-driven test case generation (MDTCG). It is able to handle
industrial-sized UML models comprising networks of, e.g., 2800 interacting
state machines. To achieve the required scalability, the implemented algorithm
exploits the concurrency in MDTCG and combines it with a search based
generation strategy. For evaluation, we use seven case studies of different
application domains with an increasing level of difficulty, stopping at a model
of a railway station in Austria's national rail network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06978</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06978</id><created>2016-01-26</created><authors><author><keyname>Naresh</keyname><forenames>Y.</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>On Media-based Modulation using RF Mirrors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Media-based modulation (MBM) is a recently proposed modulation scheme which
uses radio frequency (RF) mirrors at the transmit antenna(s) in order to create
different channel fade realizations based on their ON/OFF status. These complex
fade realizations constitute the modulation alphabet. MBM has the advantage of
increased spectral efficiency and performance. In this paper, we investigate
the performance of some physical layer techniques when applied to MBM.
Particularly, we study the performance of $i)$ MBM with generalized spatial
modulation (GSM), $ii)$ MBM with mirror activation pattern (MAP) selection
based on an Euclidean distance (ED) based metric, and $iii)$ MBM with feedback
based phase compensation and constellation rotation. Our results show that, for
the same spectral efficiency, GSM-MBM can achieve better performance compared
to MIMO-MBM. Also, it is found that MBM with ED-based MAP selection results in
improved bit error performance, and that phase compensation and MBM
constellation rotation increases the ED between the MBM constellation points
and improves the performance significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.06993</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.06993</id><created>2016-01-26</created><authors><author><keyname>Mart&#xed;nez-Pe&#xf1;as</keyname><forenames>Umberto</forenames></author></authors><title>Rank equivalent and rank degenerate skew cyclic codes</title><categories>cs.IT math.IT</categories><msc-class>15B33, 94B15, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two skew cyclic codes can be equivalent for the Hamming metric only if they
have the same length, and only the zero code is degenerate. The situation is
completely different for the rank metric, where lengths of codes correspond to
the number of outgoing links from the source when applying the code on a
network. We study rank equivalences between skew cyclic codes of different
lengths and, with the aim of finding the skew cyclic code of smallest length
that is rank equivalent to a given one, we define different types of length for
a given skew cyclic code, relate them and compute them in most cases. We give
different characterizations of rank degenerate skew cyclic codes using
conventional polynomials and linearized polynomials. Some known results on the
rank weight hierarchy of cyclic codes for some lengths are obtained as
particular cases and extended to all lengths and to all skew cyclic codes.
Finally, we prove that the smallest length of a linear code that is rank
equivalent to a given skew cyclic code can be attained by a pseudo-skew cyclic
code. Throughout the paper, we find new relations between linear skew cyclic
codes and their Galois closures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07002</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07002</id><created>2016-01-26</created><authors><author><keyname>Boufkhad</keyname><forenames>Yacine</forenames><affiliation>LIAFA</affiliation></author><author><keyname>De La Paz</keyname><forenames>Ricardo</forenames><affiliation>GANG</affiliation></author><author><keyname>Linguaglossa</keyname><forenames>Leonardo</forenames><affiliation>GANG, LINCS</affiliation></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames><affiliation>LINCS</affiliation></author><author><keyname>Perino</keyname><forenames>Diego</forenames><affiliation>LINCS</affiliation></author><author><keyname>Viennot</keyname><forenames>Laurent</forenames><affiliation>LINCS, GANG</affiliation></author></authors><title>Forwarding Tables Verification through Representative Header Sets</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forwarding table verification consists in checking the distributed
data-structure resulting from the forwarding tables of a network. A classical
concern is the detection of loops. We study this problem in the context of
software-defined networking (SDN) where forwarding rules can be arbitrary
bitmasks (generalizing prefix matching) and where tables are updated by a
centralized controller. Basic verification problems such as loop detection are
NP-hard and most previous work solves them with heuristics or SAT solvers. We
follow a different approach based on computing a representation of the header
classes, i.e. the sets of headers that match the same rules. This
representation consists in a collection of representative header sets, at least
one for each class, and can be computed centrally in time which is polynomial
in the number of classes. Classical verification tasks can then be trivially
solved by checking each representative header set. In general, the number of
header classes can increase exponentially with header length, but it remains
polynomial in the number of rules in the practical case where rules are
constituted with predefined fields where exact, prefix matching or range
matching is applied in each field (e.g., IP/MAC addresses, TCP/UDP ports). We
propose general techniques that work in polynomial time as long as the number
of classes of headers is polynomial and that do not make specific assumptions
about the structure of the sets associated to rules. The efficiency of our
method rely on the fact that the data-structure representing rules allows
efficient computation of intersection, cardinal and inclusion. Finally, we
propose an algorithm to maintain such representation in presence of updates
(i.e., rule insert/update/removal). We also provide a local distributed
algorithm for checking the absence of black-holes and a proof labeling scheme
for locally checking the absence of loops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07009</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07009</id><created>2016-01-26</created><authors><author><keyname>Trpevski</keyname><forenames>Igor</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Greedy reduction of navigation time in random search processes</title><categories>cs.SI physics.soc-ph</categories><msc-class>05C81, 05C85</msc-class><acm-class>G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random search processes are instrumental in studying and understanding
navigation properties of complex networks, food search strategies of animals,
diffusion control of molecular processes in biological cells, and improving web
search engines. An essential part of random search processes and their
applications are various forms of (continuous or discrete time) random walk
models. The efficiency of a random search strategy in complex networks is
measured with the mean first passage time between two nodes or, more generally,
with the mean first passage time between two subsets of the vertex set. In this
paper we formulate a problem of adding a set of $k$ links between the two
subsets of the vertex set that optimally reduce the mean first passage time
between the sets. We demonstrate that the mean first passage time between two
sets is non-increasing and supermodular set function defined over the set of
links between the two sets. This allows us to use two greedy algorithms that
approximately solve the problem and we compare their performance against
several standard link prediction algorithms. We find that the proposed greedy
algorithms are better at choosing the links that reduce the navigation time
between the two sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07010</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07010</id><created>2016-01-26</created><authors><author><keyname>Iwen</keyname><forenames>M. A.</forenames></author><author><keyname>Ong</keyname><forenames>B. W.</forenames></author></authors><title>A Distributed and Incremental SVD Algorithm for Agglomerative Data
  Analysis on Large Networks</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the SVD of a matrix can be constructed
efficiently in a hierarchical approach. Our algorithm is proven to recover the
singular values and left singular vectors if the rank of the input matrix $A$
is known. Further, the hierarchical algorithm can be used to recover the $d$
largest singular values and left singular vectors with bounded error. We also
show that the proposed method is stable with respect to roundoff errors or
corruption of the original matrix entries. Numerical experiments validate the
proposed algorithms and parallel cost analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07011</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07011</id><created>2016-01-26</created><authors><author><keyname>Matta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Braca</keyname><forenames>Paolo</forenames></author><author><keyname>Marano</keyname><forenames>Stefano</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Distributed Detection over Adaptive Networks: Refined Asymptotics and
  the Role of Connectivity</title><categories>cs.MA cs.IT math.IT</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed detection problems over adaptive networks, where
dispersed agents learn continually from streaming data by means of local
interactions. The simultaneous requirements of adaptation and cooperation are
achieved by employing diffusion algorithms with constant step-size {\mu}. In
[1], [2] some main features of adaptive distributed detection were revealed. By
resorting to large deviations analysis, it was established that the Type-I and
Type-II error probabilities of all agents vanish exponentially as functions of
1/{\mu}, and that all agents share the same Type-I and Type-II error exponents.
However, numerical evidences presented in [1], [2] showed that the theory of
large deviations does not capture the fundamental impact of network
connectivity on performance, and that additional tools and efforts are required
to obtain accurate predictions for the error probabilities. This work addresses
these open issues and extends the results of [1], [2] in several directions. By
conducting a refined asymptotic analysis based on the mathematical framework of
exact asymptotics, we arrive at a revealing and powerful understanding of the
universal behavior of distributed detection over adaptive networks: as
functions of 1/{\mu}, the error (log-)probability curves corresponding to
different agents stay nearly-parallel to each other (as already discovered in
[1], [2]), however, these curves are ordered following a criterion reflecting
the degree of connectivity of each agent. Depending on the combination weights,
the more connected an agent is, the lower its error probability curve will be.
Interesting and somehow unexpected behaviors emerge, in terms of the interplay
between the network topology, the combination weights, and the inference
performance. The lesson learned is that connectivity matters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07014</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07014</id><created>2016-01-26</created><updated>2016-01-31</updated><authors><author><keyname>Milletari</keyname><forenames>Fausto</forenames></author><author><keyname>Ahmadi</keyname><forenames>Seyed-Ahmad</forenames></author><author><keyname>Kroll</keyname><forenames>Christine</forenames></author><author><keyname>Plate</keyname><forenames>Annika</forenames></author><author><keyname>Rozanski</keyname><forenames>Verena</forenames></author><author><keyname>Maiostre</keyname><forenames>Juliana</forenames></author><author><keyname>Levin</keyname><forenames>Johannes</forenames></author><author><keyname>Dietrich</keyname><forenames>Olaf</forenames></author><author><keyname>Ertl-Wagner</keyname><forenames>Birgit</forenames></author><author><keyname>B&#xf6;tzel</keyname><forenames>Kai</forenames></author><author><keyname>Navab</keyname><forenames>Nassir</forenames></author></authors><title>Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI
  and Ultrasound</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a novel approach to perform segmentation by
leveraging the abstraction capabilities of convolutional neural networks
(CNNs). Our method is based on Hough voting, a strategy that allows for fully
automatic localisation and segmentation of the anatomies of interest. This
approach does not only use the CNN classification outcomes, but it also
implements voting by exploiting the features produced by the deepest portion of
the network. We show that this learning-based segmentation method is robust,
multi-region, flexible and can be easily adapted to different modalities. In
the attempt to show the capabilities and the behaviour of CNNs when they are
applied to medical image analysis, we perform a systematic study of the
performances of six different network architectures, conceived according to
state-of-the-art criteria, in various situations. We evaluate the impact of
both different amount of training data and different data dimensionality (2D,
2.5D and 3D) on the final results. We show results on both MRI and transcranial
US volumes depicting respectively 26 regions of the basal ganglia and the
midbrain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07021</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07021</id><created>2016-01-26</created><authors><author><keyname>Feng</keyname><forenames>Qingxiang</forenames></author><author><keyname>Pan</keyname><forenames>Jeng-Shyang</forenames></author><author><keyname>Yang</keyname><forenames>Jar-Ferr</forenames></author><author><keyname>Chou</keyname><forenames>Yang-Ting</forenames></author></authors><title>Polyhedron Volume-Ratio-based Classification for Image Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel method, called polyhedron volume ratio classification
(PVRC) is proposed for image recognition
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07024</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07024</id><created>2016-01-26</created><authors><author><keyname>Falconet</keyname><forenames>Hugo</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Asymptotic analysis of downlink MIMO systems over Rician fading channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures. Published at the 41st IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP 2016),
  Shanghai, 20-25 March 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we focus on the ergodic sum rate in the downlink of a
single-cell large-scale multi-user MIMO system in which the base station
employs N antennas to communicate with $K$ single-antenna user equipments. A
regularized zero-forcing (RZF) scheme is used for precoding under the
assumption that each link forms a spatially correlated MIMO Rician fading
channel. The analysis is conducted assuming $N$ and $K$ grow large with a non
trivial ratio and perfect channel state information is available at the base
station. Recent results from random matrix theory and large system analysis are
used to compute an asymptotic expression of the signal-to-interference-
plus-noise ratio as a function of the system parameters, the spatial
correlation matrix and the Rician factor. Numerical results are used to
evaluate the performance gap in the finite system regime under different
operating conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07028</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07028</id><created>2016-01-26</created><authors><author><keyname>Yuan</keyname><forenames>Dong</forenames></author><author><keyname>Cui</keyname><forenames>Lizhen</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Fu</keyname><forenames>Erjiang</forenames></author><author><keyname>Yang</keyname><forenames>Yun</forenames></author></authors><title>A Cost-Effective Strategy for Storing Scientific Datasets with Multiple
  Service Providers in the Cloud</title><categories>cs.DC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing provides scientists a platform that can deploy computation
and data intensive applications without infrastructure investment. With
excessive cloud resources and a decision support system, large generated data
sets can be flexibly 1 stored locally in the current cloud, 2 deleted and
regenerated whenever reused or 3 transferred to cheaper cloud service for
storage. However, due to the pay for use model, the total application cost
largely depends on the usage of computation, storage and bandwidth resources,
hence cutting the cost of cloud based data storage becomes a big concern for
deploying scientific applications in the cloud. In this paper, we propose a
novel strategy that can cost effectively store large generated data sets with
multiple cloud service providers. The strategy is based on a novel algorithm
that finds the trade off among computation, storage and bandwidth costs in the
cloud, which are three key factors for the cost of data storage. Both general
(random) simulations conducted with popular cloud service providers pricing
models and three specific case studies on real world scientific applications
show that the proposed storage strategy is highly cost effective and practical
for run time utilization in the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07036</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07036</id><created>2016-01-26</created><authors><author><keyname>Kralevska</keyname><forenames>Katina</forenames></author><author><keyname>Oeverby</keyname><forenames>Harald</forenames></author><author><keyname>Gligoroski</keyname><forenames>Danilo</forenames></author></authors><title>Coded Packet Transport for Optical Packet/Burst Switched Networks</title><categories>cs.IT math.IT</categories><comments>IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Coded Packet Transport (CPT) scheme, a novel
transport mechanism for Optical Packet/Burst Switched (OPS/OBS) networks. The
CPT scheme exploits the combined benefits of source coding by erasure codes and
path diversity to provide efficient means for recovering from packet loss due
to contentions and path failures, and to provide non-cryptographic secrecy. In
the CPT scheme, erasure coding is employed at the OPS/OBS ingress node to form
coded packets, which are transmitted on disjoint paths from the ingress node to
an egress node in the network. The CPT scheme allows for a unified view of
Quality of Service (QoS) in OPS/OBS networks by linking the interactions
between survivability, performance and secrecy. We provide analytical models
that illustrate how QoS aspects of CPT are affected by the number of disjoint
paths, packet overhead and processing delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07047</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07047</id><created>2016-01-26</created><authors><author><keyname>Burkimsher</keyname><forenames>Andrew</forenames></author><author><keyname>Indrusiak</keyname><forenames>Leandro Soares</forenames></author></authors><title>Bidding policies for market-based HPC workflow scheduling</title><categories>cs.DC</categories><comments>2nd International Workshop on Dynamic Resource Allocation and
  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016
  (arXiv:cs/1601.04675), DREAMCloud/2016/06</comments><report-no>DREAMCloud/2016/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the scheduling of jobs on distributed, heterogeneous
High Performance Computing (HPC) clusters. Market-based approaches are known to
be efficient for allocating limited resources to those that are most prepared
to pay. This context is applicable to an HPC or cloud computing scenario where
the platform is overloaded. In this paper, jobs are composed of dependent
tasks. Each job has a non-increasing time-value curve associated with it. Jobs
are submitted to and scheduled by a market-clearing centralised auctioneer.
This paper compares the performance of several policies for generating task
bids. The aim investigated here is to maximise the value for the platform
provider while minimising the number of jobs that do not complete (or starve).
It is found that the Projected Value Remaining bidding policy gives the highest
level of value under a typical overload situation, and gives the lowest number
of starved tasks across the space of utilisation examined. It does this by
attempting to capture the urgency of tasks in the queue. At high levels of
overload, some alternative algorithms produce slightly higher value, but at the
cost of a hugely higher number of starved workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07059</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07059</id><created>2016-01-26</created><authors><author><keyname>Visk</keyname><forenames>Kristo</forenames></author><author><keyname>Riet</keyname><forenames>Ago-Erik</forenames></author></authors><title>Permutation codes, source coding and a generalisation of
  Bollob\'as-Lubell-Yamamoto-Meshalkin and Kraft inequalities</title><categories>cs.IT math.CO math.IT</categories><comments>5 pages, 1 figure</comments><msc-class>05D05</msc-class><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general framework to prove Kraft-type inequalities for
prefix-free permutation codes for source coding with various notions of
permutation code and prefix. We also show that the McMillan-type converse
theorem in most of these cases does not hold, and give a general form of a
counterexample. Our approach is more general and works for other structures
besides permutation codes. The classical Kraft inequality for prefix-free codes
as well as results about permutation codes follow as corollaries of our main
theorem and main counterexample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07065</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07065</id><created>2016-01-26</created><authors><author><keyname>Lim</keyname><forenames>Ser Ling</forenames></author><author><keyname>Goh</keyname><forenames>Ong Sing</forenames></author></authors><title>Intelligent Conversational Bot for Massive Online Open Courses (MOOCs)</title><categories>cs.AI</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Massive Online Open Courses (MOOCs) which were introduced in 2008 has since
drawn attention around the world for both its advantages as well as criticism
on its drawbacks. One of the issues in MOOCs which is the lack of interactivity
with the instructor has brought conversational bot into the picture to fill in
this gap. In this study, a prototype of MOOCs conversational bot, MOOC-bot is
being developed and integrated into MOOCs website to respond to the learner
inquiries using text or speech input. MOOC-bot is using the popular Artificial
Intelligence Markup Language (AIML) to develop its knowledge base, leverage
from AIML capability to deliver appropriate responses and can be quickly
adapted to new knowledge domains. The system architecture of MOOC-bot consists
of knowledge base along with AIML interpreter, chat interface, MOOCs website
and Web Speech API to provide speech recognition and speech synthesis
capability. The initial MOOC-bot prototype has the general knowledge from the
past Loebner Prize winner - ALICE, frequent asked questions, and a content
offered by Universiti Teknikal Malaysia Melaka (UTeM). The evaluation of
MOOC-bot based on the past competition questions from Chatterbox Challenge
(CBC) and Loebner Prize has shown that it was able to provide correct answers
most of the time during the test and demonstrated the capability to prolong the
conversation. The advantages of MOOC-bot such as able to provide 24-hour
service that can serve different time zones, able to have knowledge in multiple
domains, and can be shared by multiple sites simultaneously have outweighed its
existing limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07077</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07077</id><created>2015-12-24</created><authors><author><keyname>Schulz</keyname><forenames>Matthias</forenames></author><author><keyname>Wegemer</keyname><forenames>Daniel</forenames></author><author><keyname>Hollick</keyname><forenames>Matthias</forenames></author></authors><title>NexMon: A Cookbook for Firmware Modifications on Smartphones to Enable
  Monitor Mode</title><categories>cs.OH</categories><comments>Project website: https://seemoo.tu-darmstadt.de/nexmon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full control over a Wi-Fi chip for research purposes is often limited by its
firmware, which makes it hard to evolve communication protocols and test
schemes in practical environments. Monitor mode, which allows eavesdropping on
all frames on a wireless communication channel, is a first step to lower this
barrier. Use cases include, but are not limited to, network packet analyses,
security research and testing of new medium access control layer protocols.
Monitor mode is generally offered by SoftMAC drivers that implement the media
access control sublayer management entity (MLME) in the driver rather than in
the Wi-Fi chip. On smartphones, however, mostly FullMAC chips are used to
reduce power consumption, as MLME tasks do not need to wake up the main
processor. Even though, monitor mode is also possible in FullMAC scenarios, it
is generally not implemented in today's Wi-Fi firmwares used in smartphones.
This work focuses on bringing monitor mode to Nexus 5 smartphones to enhance
the interoperability between applications that require monitor mode and BCM4339
Wi-Fi chips. The implementation is based on our new C-based programming
framework to extend existing Wi-Fi firmwares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07086</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07086</id><created>2016-01-26</created><authors><author><keyname>Ganguly</keyname><forenames>Shirshendu</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Racz</keyname><forenames>Miklos Z.</forenames></author></authors><title>Sequence assembly from corrupted shotgun reads</title><categories>q-bio.GN cs.IT math.IT math.PR</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prevalent technique for DNA sequencing consists of two main steps:
shotgun sequencing, where many randomly located fragments, called reads, are
extracted from the overall sequence, followed by an assembly algorithm that
aims to reconstruct the original sequence. There are many different
technologies that generate the reads: widely-used second-generation methods
create short reads with low error rates, while emerging third-generation
methods create long reads with high error rates. Both error rates and error
profiles differ among methods, so reconstruction algorithms are often tailored
to specific shotgun sequencing technologies. As these methods change over time,
a fundamental question is whether there exist reconstruction algorithms which
are robust, i.e., which perform well under a wide range of error distributions.
  Here we study this question of sequence assembly from corrupted reads. We
make no assumption on the types of errors in the reads, but only assume a bound
on their magnitude. More precisely, for each read we assume that instead of
receiving the true read with no errors, we receive a corrupted read which has
edit distance at most $\epsilon$ times the length of the read from the true
read. We show that if the reads are long enough and there are sufficiently many
of them, then approximate reconstruction is possible: we construct a simple
algorithm such that for almost all original sequences the output of the
algorithm is a sequence whose edit distance from the original one is at most
$O(\epsilon)$ times the length of the original sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07087</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07087</id><created>2016-01-26</created><authors><author><keyname>Kim</keyname><forenames>Kyung Su</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>Greedy Subspace Pursuit for Joint Sparse Recovery</title><categories>cs.IT math.IT</categories><comments>55 pages, 8 figures, to be submitted to IEEE Transactions on
  Information theory, a shorter version was submitted to Proc. IEEE ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the sparse multiple measurement vector (MMV)
problem where the objective is to recover a set of sparse nonzero row vectors
or indices of a signal matrix from incomplete measurements. Ideally, regardless
of the number of columns in the signal matrix, the sparsity (k) plus one
measurements is sufficient for the uniform recovery of signal vectors for
almost all signals, i.e., excluding a set of Lebesgue measure zero. To approach
the &quot;k+1&quot; lower bound with computational efficiency even when the rank of
signal matrix is smaller than k, we propose a greedy algorithm called Two-stage
orthogonal Subspace Matching Pursuit (TSMP) whose theoretical results approach
the lower bound with less restriction than the Orthogonal Subspace Matching
Pursuit (OSMP) and Subspace-Augmented MUltiple SIgnal Classification (SA-MUSIC)
algorithms. We provide non-asymptotical performance guarantees of OSMP and TSMP
by covering both noiseless and noisy cases. Variants of restricted isometry
property and mutual coherence are used to improve the performance guarantees.
Numerical simulations demonstrate that the proposed scheme has low complexity
and outperforms most existing greedy methods. This shows that the minimum
number of measurements for the success of TSMP converges more rapidly to the
lower bound than the existing methods as the number of columns of the signal
matrix increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07089</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07089</id><created>2016-01-26</created><authors><author><keyname>Azad</keyname><forenames>Siavoosh Payandeh</forenames></author><author><keyname>Niazmand</keyname><forenames>Behrad</forenames></author><author><keyname>Raik</keyname><forenames>Jaan</forenames></author><author><keyname>Jervan</keyname><forenames>Gert</forenames></author><author><keyname>Hollstein</keyname><forenames>Thomas</forenames></author></authors><title>Holistic Approach for Fault-Tolerant Network-on-Chip based Many-Core
  Systems</title><categories>cs.DC</categories><comments>2nd International Workshop on Dynamic Resource Allocation and
  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016
  (arXiv:cs/1601.04675), DREAMCloud/2016/05</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a holistic approach for Fault-Tolerant
Network-on-Chip (NoC) based many-core systems that incorporates a System Health
Monitoring Unit (SHMU) which collects all the fault information from the
system, classifies them and provides different solutions for different fault
classes. A Mapper/Scheduler Unit (MSU) is used for online generation of
different mapping and scheduling solutions based on the current fault
configuration of the system. For detection of faults, we have leveraged
concurrent online checkers, able to capture faults with low detection latency
and providing the fault information for SHMU, which can be later used for the
recovery process. The experimentation setup is performed in an open source
tool, able to perform the mapping, scheduling and simulation of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07090</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07090</id><created>2016-01-26</created><authors><author><keyname>Magnusson</keyname><forenames>Sindri</forenames></author><author><keyname>Enyioha</keyname><forenames>Chinwendu</forenames></author><author><keyname>Heal</keyname><forenames>Kathryn</forenames></author><author><keyname>Li</keyname><forenames>Na</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Distributed Resource Allocation Using One-Way Communication with
  Applications to Power Networks</title><categories>cs.SY</categories><comments>6 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical coordination schemes for future power grids require two-way
communications. Since the number of end power-consuming devices is large, the
bandwidth requirements for such two-way communication schemes may be
prohibitive. Motivated by this observation, we study distributed coordination
schemes that require only one-way limited communications. In particular, we
investigate how dual descent distributed optimization algorithm can be employed
in power networks using one-way communication. In this iterative algorithm,
system coordinators broadcast coordinating (or pricing) signals to the
users/devices who update power consumption based on the received signal. Then
system coordinators update the coordinating signals based on the physical
measurement of the aggregate power usage. We provide conditions to guarantee
the feasibility of the aggregated power usage at each iteration so as to avoid
blackout. Furthermore, we prove the convergence of algorithms under these
conditions, and establish its rate of convergence. We illustrate the
performance of our algorithms using numerical simulations. These results show
that one-way limited communication may be viable for coordinating/operating the
future smart grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07091</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07091</id><created>2016-01-26</created><updated>2016-02-19</updated><authors><author><keyname>Padakandla</keyname><forenames>Arun</forenames></author></authors><title>On Transmitting Correlated Sources over a MAC</title><categories>cs.IT math.IT</categories><comments>Version 1 had a few errors in the rate expression which have been
  corrected here. Proof of the channel coding bounds is added. The diff.
  between the info-theoretic quantities relating the chosen test channel and
  the resulting pmf is bounded. A detailed proof of this is provided. An
  example is worked out in detail to prove that the derived sufficient
  conditions herein can be strictly less binding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint source channel coding problem of transmitting a pair of correlated
sources over a $2-$user MAC is considered. A new concatenated coding scheme,
comprising of an inner code of fixed block-length and an outer code of
arbitrarily large block-length, is proposed. Its information theoretic
performance is analyzed to derive a new set of sufficient conditions. An
example is identified to demonstrate that the proposed coding technique can
strictly outperform the current known best, which is due to Cover El Gamal and
Salehi [Cover, El Gamal Salehi, 1980]. Our findings are based on Dueck's
ingenious coding technique proposed for the particular example studied in
[Dueck, 1981]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07097</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07097</id><created>2016-01-26</created><authors><author><keyname>Enyioha</keyname><forenames>Chinwendu</forenames></author><author><keyname>Magnusson</keyname><forenames>Sindri</forenames></author><author><keyname>Heal</keyname><forenames>Kathryn</forenames></author><author><keyname>Li</keyname><forenames>Na</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Robustness Analysis for an Online Decentralized Descent Power allocation
  algorithm</title><categories>cs.SY</categories><comments>8 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As independent service providers shift from conventional energy to renewable
energy sources, the power distribution system will likely experience
increasingly significant fluctuation in supply, given the uncertain and
intermittent nature of renewable sources like wind and solar energy. These
fluctuations in power generation, coupled with time-varying consumer demands of
electricity and the massive scale of power distribution networks present the
need to not only design real-time decentralized power allocation algorithms,
but also characterize how effective they are given fast-changing consumer
demands and power generation capacities. In this paper, we present an Online
Decentralized Dual Descent (OD3) power allocation algorithm and determine (in
the worst case) how much of observed social welfare and price volatility can be
explained by fluctuations in generation capacity and consumer demand.
Convergence properties and performance guarantees of the OD3 algorithm are
analyzed by characterizing the difference between the online decision and the
optimal decision. The theoretical results in the paper are validated and
illustrated by numerical experiments using real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07101</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07101</id><created>2016-01-26</created><updated>2016-02-18</updated><authors><author><keyname>Akram</keyname><forenames>Raja Naeem</forenames></author><author><keyname>Gurulian</keyname><forenames>Iakovos</forenames></author><author><keyname>Shepherd</keyname><forenames>Carlton</forenames></author><author><keyname>Markantonakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Mayes</keyname><forenames>Keith</forenames></author></authors><title>Empirical Evaluation of Ambient Sensors as Proximity Detection Mechanism
  for Mobile Payments</title><categories>cs.CR</categories><comments>19 pages, 9 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Near Field Communication (NFC) has enabled mobile phones to emulate
contactless smart cards. Similar to contactless smart cards, they are also
susceptible to relay attacks. To counter these, a number of methods have been
proposed that rely primarily on ambient sensors as a proximity detection
mechanism (also known as an anti-relay mechanism). In this paper, we, for the
first time in academic literature, empirically evaluate a comprehensive set of
ambient sensors for their effectiveness as a proximity detection mechanism. We
selected 15 out of a total of 17 sensors available via the Google Android
platform for evaluation, with the other two sensors unavailable on widely-used
handsets. In existing academic literature, only 5 sensors have been proposed
with positive results as a potential proximity detection mechanism. Each
sensor, where feasible, was used to record the measurements of 1000 contactless
transactions at four different physical locations. A total of 252 random users,
random sample of the university student population, were involved during the
field trails. The analysis of these transactions provides an empirical
foundation to categorically answer whether ambient sensors provide a strong
proximity detection mechanism for security sensitive applications like banking,
transport and high-security access control. After careful analysis, we conclude
that no single evaluated mobile ambient sensor is suitable for such critical
applications in realistic deployment scenarios. Lastly, we identify a number of
potential avenues that may improve their effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07108</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07108</id><created>2016-01-26</created><authors><author><keyname>Gupta</keyname><forenames>Naveen</forenames></author><author><keyname>Singh</keyname><forenames>Anurag</forenames></author><author><keyname>Cherifi</keyname><forenames>Hocine</forenames></author></authors><title>Centrality Measures for Networks with Community Structure</title><categories>cs.SI physics.soc-ph</categories><comments>30 pages, 4 figures. Accepted for publication in Physica A. arXiv
  admin note: text overlap with arXiv:1411.6276</comments><doi>10.1016/j.physa.2016.01.066</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the network structure, and finding out the influential nodes is
a challenging issue in the large networks. Identifying the most influential
nodes in the network can be useful in many applications like immunization of
nodes in case of epidemic spreading, during intentional attacks on complex
networks. A lot of research is done to devise centrality measures which could
efficiently identify the most influential nodes in the network. There are two
major approaches to the problem: On one hand, deterministic strategies that
exploit knowledge about the overall network topology in order to find the
influential nodes, while on the other end, random strategies are completely
agnostic about the network structure. Centrality measures that can deal with a
limited knowledge of the network structure are required. Indeed, in practice,
information about the global structure of the overall network is rarely
available or hard to acquire. Even if available, the structure of the network
might be too large that it is too much computationally expensive to calculate
global centrality measures. To that end, a centrality measure is proposed that
requires information only at the community level to identify the influential
nodes in the network. Indeed, most of the real-world networks exhibit a
community structure that can be exploited efficiently to discover the
influential nodes. We performed a comparative evaluation of prominent global
deterministic strategies together with stochastic strategies with an available
and the proposed deterministic community-based strategy. Effectiveness of the
proposed method is evaluated by performing experiments on synthetic and
real-world networks with community structure in the case of immunization of
nodes for epidemic control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07110</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07110</id><created>2016-01-26</created><authors><author><keyname>Kirthi</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>The Narayana Universal Code</title><categories>cs.IT cs.CR math.IT</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method of universal coding based on the Narayana
series. The rules necessary to make such coding possible have been found and
the length of the resulting code has been determined to follow the Narayana
count.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07122</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07122</id><created>2016-01-26</created><updated>2016-02-02</updated><authors><author><keyname>Balaji</keyname><forenames>S. B.</forenames></author><author><keyname>Prasanth</keyname><forenames>K. P.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>Binary Codes with Locality for Multiple Erasures Having Short Block
  Length</title><categories>cs.IT math.IT</categories><comments>17 pages, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on linear, binary codes with locality having
locality parameter $r$, that are capable of recovering from $t\geq 2$ erasures
and that moreover, have short block length. Both sequential and parallel
(through orthogonal parity checks) recovery is considered here. In the case of
parallel repair, minimum-block-length constructions for general $t$ are
discussed. In the case of sequential repair, the results include (a) extending
and characterizing minimum-block-length constructions for $t=2$, (b) providing
improved bounds on block length for $t=3$ as well as a general construction for
$t=3$ having short block length, (c) providing short-block-length constructions
for general $r,t$ and (d) providing high-rate constructions for $r=2$ and $t$
in the range $4 \leq t \leq7$. Most of the constructions provided are of binary
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07124</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07124</id><created>2016-01-26</created><authors><author><keyname>Pontes</keyname><forenames>Elvys Linhares</forenames></author><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author><author><keyname>Linhares</keyname><forenames>Andr&#xe9;a Carneiro</forenames></author></authors><title>LIA-RAG: a system based on graphs and divergence of probabilities
  applied to Speech-To-Text Summarization</title><categories>cs.CL cs.IR</categories><comments>7 pages, 2 figures, CCCS Multiling 2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to introduces a new algorithm for automatic speech-to-text
summarization based on statistical divergences of probabilities and graphs. The
input is a text from speech conversations with noise, and the output a compact
text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus
are very encouraging
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07133</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07133</id><created>2016-01-26</created><authors><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Saa-Garriga</keyname><forenames>Albert</forenames></author><author><keyname>Carrabina</keyname><forenames>Jordi</forenames></author></authors><title>Energy Efficiency of Many-Soft-Core Processors</title><categories>cs.DC</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing capacity of integration allows to instantiate hundreds of
soft-core processors in a single FPGA to create a reconfigurable
multiprocessing system. Lately, FPGAs have been proven to give a higher energy
efficiency than alternative platforms like CPUs and GPGPUs for certain
workloads and are increasingly used in data-centers. In this paper we
investigate whether many-soft-core processors can achieve similar levels of
energy efficiency while providing a general purpose environment, more easily
programmed, and allowing to run other applications without reconfiguring the
device. With a simple application example we are able to create a
reconfigurable multiprocessing system achieving an energy efficiency 58 times
higher than a recent ultra-low-power processor and 124 times higher than a
recent high performance GPGPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07137</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07137</id><created>2016-01-23</created><authors><author><keyname>Ulrich</keyname><forenames>James L.</forenames></author></authors><title>Posner computing: a quantum neural network model</title><categories>quant-ph cs.ET</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model for (not necessarily universal) quantum computation given
in terms of a quantum neural network, based in turn on the interactions of
Posner molecules, as described in a recent article by Matthew P. A. Fisher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07140</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07140</id><created>2016-01-26</created><authors><author><keyname>Veit</keyname><forenames>Andreas</forenames></author><author><keyname>Matera</keyname><forenames>Tomas</forenames></author><author><keyname>Neumann</keyname><forenames>Lukas</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author></authors><title>COCO-Text: Dataset and Benchmark for Text Detection and Recognition in
  Natural Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the COCO-Text dataset. In recent years large-scale
datasets like SUN and Imagenet drove the advancement of scene understanding and
object recognition. The goal of COCO-Text is to advance state-of-the-art in
text detection and recognition in natural images. The dataset is based on the
MS COCO dataset, which contains images of complex everyday scenes. The images
were not collected with text in mind and thus contain a broad variety of text
instances. To reflect the diversity of text in natural scenes, we annotate text
with (a) location in terms of a bounding box, (b) fine-grained classification
into machine printed text and handwritten text, (c) classification into legible
and illegible text, (d) script of the text and (e) transcriptions of legible
text. The dataset contains over 173k text annotations in over 63k images. We
provide a statistical analysis of the accuracy of our annotations. In addition,
we present an analysis of three leading state-of-the-art photo Optical
Character Recognition (OCR) approaches on our dataset. While scene text
detection and recognition enjoys strong advances in recent years, we identify
significant shortcomings motivating future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07154</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07154</id><created>2016-01-05</created><authors><author><keyname>Bae</keyname><forenames>Arram</forenames></author><author><keyname>Park</keyname><forenames>Doheum</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author><author><keyname>Park</keyname><forenames>Juyong</forenames></author></authors><title>The Multi-Scale Network Landscape of Collaboration</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propelled by the increasing availability of large-scale high-quality data,
advanced data modeling and analysis techniques are enabling many novel and
significant scientific understanding of a wide range of complex social,
natural, and technological systems. These developments also provide
opportunities for studying cultural systems and phenomena -- which can be said
to refer to all products of human creativity and way of life. An important
characteristic of a cultural product is that it does not exist in isolation
from others, but forms an intricate web of connections on many levels. In the
creation and dissemination of cultural products and artworks in particular,
collaboration and communication of ideas play an essential role, which can be
captured in the heterogeneous network of the creators and practitioners of art.
In this paper we propose novel methods to analyze and uncover meaningful
patterns from such a network using the network of western classical musicians
constructed from a large-scale comprehensive Compact Disc recordings data. We
characterize the complex patterns in the network landscape of collaboration
between musicians across multiple scales ranging from the macroscopic to the
mesoscopic and microscopic that represent the diversity of cultural styles and
the individuality of the artists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07157</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07157</id><created>2016-01-26</created><updated>2016-01-27</updated><authors><author><keyname>Merkel</keyname><forenames>Robert</forenames></author><author><keyname>Georgeson</keyname><forenames>James</forenames></author></authors><title>Cloud-Based Distributed Mutation Analysis</title><categories>cs.SE</categories><comments>12 pages including appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutation Testing is a fault-based software testing technique which is too
computationally expensive for industrial use. Cloud-based distributed computing
clusters, taking advantage of the MapReduce programming paradigm, represent a
method by which the long running time can be reduced. In this paper, we
describe an architecture, and a prototype implementation, of such a cloud-based
distributed mutation testing system. To evaluate the system, we compared the
performance of the prototype, with various cluster sizes, to an existing
&quot;state-of-the-art&quot; non-distributed tool, PiT. We also analysed different
approaches to work distribution, to determine how to most efficiently divide
the mutation analysis task. Our tool outperformed PiT, and analysis of the
results showed opportunities for substantial further performance improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07163</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07163</id><created>2016-01-26</created><authors><author><keyname>Greenwald</keyname><forenames>Amy</forenames></author><author><keyname>Oyakawa</keyname><forenames>Takehiro</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Optimal Auctions with Convex Perceived Payments</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Myerson derived a simple and elegant solution to the single-parameter
revenue-maximization problem in his seminal work on optimal auction design
assuming the usual model of quasi-linear utilities. In this paper, we consider
a slight generalization of this usual model---from linear to convex &quot;perceived&quot;
payments. This more general problem does not appear to admit a solution as
simple and elegant as Myerson's.
  While some of Myerson's results extend to our setting, like his payment
formula (suitably adjusted), others do not. For example, we observe that the
solutions to the Bayesian and the robust (i.e., non-Bayesian) optimal auction
design problems in the convex perceived payment setting do not coincide like
they do in the case of linear payments. We therefore study the two problems in
turn.
  We derive an upper and a heuristic lower bound on expected revenue in our
setting. These bounds are easily computed pointwise, and yield monotonic
allocation rules, so can be supported by Myerson payments (suitably adjusted).
In this way, our bounds yield heuristics that approximate the optimal robust
auction, assuming convex perceived payments.
  We close with experiments, the final set of which massages the output of one
of the closed-form heuristics for the robust problem into an extremely fast,
near-optimal heuristic solution to the Bayesian optimal auction design problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07189</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07189</id><created>2016-01-24</created><authors><author><keyname>Porotsky</keyname><forenames>Sergey</forenames></author></authors><title>Rare-Event Estimation for Dynamic Fault Trees</title><categories>stat.AP cs.CE</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article describes the results of the development and using of Rare-Event
Monte-Carlo Simulation Algorithms for Dynamic Fault Trees Estimation. For Fault
Trees estimation usually analytical methods are used (Minimal Cut sets, Markov
Chains, etc.), but for complex models with Dynamic Gates it is necessary to use
Monte-Carlo simulation with combination of Importance Sampling method. Proposed
article describes approach for this problem solution according for specific
features of Dynamic Fault Trees. There are assumed, that failures are
non-repairable with general distribution functions of times to failures (there
may be Exponential distribution, Weibull, Normal and Log-Normal, etc.).
Expessions for Importance Sampling Re-Calculations are proposed and some
numerical results are considered
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07195</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07195</id><created>2016-01-26</created><authors><author><keyname>Smelyanskiy</keyname><forenames>Mikhail</forenames></author><author><keyname>Sawaya</keyname><forenames>Nicolas P. D.</forenames></author><author><keyname>Aspuru-Guzik</keyname><forenames>Al&#xe1;n</forenames></author></authors><title>qHiPSTER: The Quantum High Performance Software Testing Environment</title><categories>quant-ph cs.DC</categories><comments>9 pages, 10 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present qHiPSTER, the Quantum High Performance Software Testing
Environment. qHiPSTER is a distributed high-performance implementation of a
quantum simulator on a classical computer, that can simulate general
single-qubit gates and two-qubit controlled gates. We perform a number of
single- and multi-node optimizations, including vectorization, multi-threading,
cache blocking, as well as overlapping computation with communication. Using
the TACC Stampede supercomputer, we simulate quantum circuits (&quot;quantum
software&quot;) of up to 40 qubits. We carry out a detailed performance analysis to
show that our simulator achieves both high performance and high hardware
efficiency, limited only by the sustainable memory and network bandwidth of the
machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07200</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07200</id><created>2016-01-26</created><authors><author><keyname>Zhu</keyname><forenames>Linhong</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Attention Inequality in Social Media</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media can be viewed as a social system where the currency is
attention. People post content and interact with others to attract attention
and gain new followers. In this paper, we examine the distribution of attention
across a large sample of users of a popular social media site Twitter. Through
empirical analysis of these data we conclude that attention is very unequally
distributed: the top 20\% of Twitter users own more than 96\% of all followers,
93\% of the retweets, and 93\% of the mentions. We investigate the mechanisms
that lead to attention inequality and find that it results from the
&quot;rich-get-richer&quot; and &quot;poor-get-poorer&quot; dynamics of attention diffusion.
Namely, users who are &quot;rich&quot; in attention, because they are often mentioned and
retweeted, are more likely to gain new followers, while those who are &quot;poor&quot; in
attention are likely to lose followers. We develop a phenomenological model
that quantifies attention diffusion and network dynamics, and solve it to study
how attention inequality grows over time in a dynamic environment of social
media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07207</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07207</id><created>2016-01-26</created><authors><author><keyname>Cooklev</keyname><forenames>T.</forenames></author><author><keyname>Dogan</keyname><forenames>H.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Yildiz</keyname><forenames>H.</forenames></author></authors><title>A Generalized Prefix Construction for OFDM Systems Over Quasi-Static
  Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>19 pages</comments><journal-ref>IEEE Transactions on Vehicular Technology, vol. 60, no. 8,
  3684-3693, 2011</journal-ref><doi>10.1109/TVT.2011.2165741</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All practical OFDM systems require a prefix to eliminate inter-symbol
interference at the receiver. Cyclic prefix (CP) and zero-padding are
well-known prefix construction methods, the former being the most employed
technique in practice due to its lower complexity. In this paper we construct
an OFDM system with a generalized CP. It is shown that the proposed generalized
prefix effectively makes the channel experienced by the packet different from
the actual channel. Using an optimization procedure, lower bit error rates can
be achieved, outperforming other prefix construction techniques. At the same
time the complexity of the technique is comparable to the CP method. The
presented simulation results show that the proposed technique not only
outperforms the CP method, but is also more robust in the presence of channel
estimation errors and mobility. The proposed method is appropriate for
practical OFDM systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07213</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07213</id><created>2016-01-26</created><updated>2016-02-09</updated><authors><author><keyname>Ororbia</keyname><forenames>Alexander G.</forenames><suffix>II</suffix></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author><author><keyname>Kifer</keyname><forenames>Daniel</forenames></author></authors><title>Unifying Adversarial Training Algorithms with Flexible Deep Data
  Gradient Regularization</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present DataGrad, a general back-propagation style training procedure for
deep neural architectures that uses regularization of a deep Jacobian-based
penalty. It can be viewed as a deep extension of the layerwise contractive
auto-encoder penalty. More importantly, it unifies previous proposals for
adversarial training of deep neural nets -- this list includes directly
modifying the gradient, training on a mix of original and adversarial examples,
using contractive penalties, and approximately optimizing constrained
adversarial objective functions. In an experiment using a Deep Sparse Rectifier
Network, we find that the deep Jacobian regularization of DataGrad (which also
has L1 and L2 flavors of regularization) outperforms traditional L1 and L2
regularization both on the original dataset as well as on adversarial examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07215</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07215</id><created>2016-01-26</created><authors><author><keyname>Muthukumar</keyname><forenames>Prasanna Kumar</forenames></author><author><keyname>Black</keyname><forenames>Alan W</forenames></author></authors><title>Recurrent Neural Network Postfilters for Statistical Parametric Speech
  Synthesis</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last two years, there have been numerous papers that have looked into
using Deep Neural Networks to replace the acoustic model in traditional
statistical parametric speech synthesis. However, far less attention has been
paid to approaches like DNN-based postfiltering where DNNs work in conjunction
with traditional acoustic models. In this paper, we investigate the use of
Recurrent Neural Networks as a potential postfilter for synthesis. We explore
the possibility of replacing existing postfilters, as well as highlight the
ease with which arbitrary new features can be added as input to the postfilter.
We also tried a novel approach of jointly training the Classification And
Regression Tree and the postfilter, rather than the traditional approach of
training them independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07216</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07216</id><created>2016-01-26</created><updated>2016-01-30</updated><authors><author><keyname>Dahan</keyname><forenames>Mathieu</forenames></author><author><keyname>Amin</keyname><forenames>Saurabh</forenames></author></authors><title>Security Games in Network Flow Problems</title><categories>cs.GT cs.SY</categories><comments>41 pages, 14 figures, submitted to Mathematics of Operations Research</comments><msc-class>Primary: 91A05, 91A10, 91A43, secondary: 90C46, 05C21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers a two-player strategic game for network routing under
link disruptions. Player 1 (defender) routes flow through a network to maximize
her value of effective flow while facing transportation costs. Player 2
(attacker) simultaneously disrupts one or more links to maximize her value of
lost flow but also faces cost of disrupting links. Linear programming duality
in zero-sum games and the Max-Flow Min-Cut Theorem are applied to obtain
properties that are satisfied in any Nash equilibrium. A characterization of
the support of the equilibrium strategies is provided using graph-theoretic
arguments. Finally, conditions under which these results extend to
budget-constrained environments are also studied. These results extend the
classical minimum cost maximum flow problem and the minimum cut problem to a
class of security games on flow networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07223</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07223</id><created>2016-01-26</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Gram Schmidt Based Greedy Hybrid Precoding for Frequency Selective
  Millimeter Wave MIMO Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, to appear in ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid analog/digital precoding allows millimeter wave MIMO systems to
leverage large antenna array gains while permitting low cost and power
consumption hardware. Most prior work has focused on hybrid precoding for
narrow-band mmWave systems. MmWave systems, however, will likely operate on
wideband channels with frequency selectivity. Therefore, this paper considers
frequency selective hybrid precoding with RF beamforming vectors taken from a
quantized codebook. For this system, a low-complexity yet near-optimal greedy
algorithm is developed for the design of the hybrid analog/digital precoders.
The proposed algorithm greedily selects the RF beamforming vectors using
Gram-Schmidt orthogonalization. Simulation results show that the developed
precoding design algorithm achieves very good performance compared with the
unconstrained solutions while requiring less complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07224</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07224</id><created>2016-01-26</created><authors><author><keyname>Perov</keyname><forenames>Yura N</forenames></author></authors><title>Bachelor's thesis on generative probabilistic programming (in Russian
  language, June 2014)</title><categories>cs.AI cs.PL</categories><comments>49 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Bachelor's thesis, written in Russian, is devoted to a relatively new
direction in the field of machine learning and artificial intelligence, namely
probabilistic programming. The thesis gives a brief overview to the already
existing probabilistic programming languages: Church, Venture, and Anglican. It
also describes the results of the first experiments on the automatic induction
of probabilistic programs. The thesis was submitted, in June 2014, in partial
fulfilment of the requirements for the degree of Bachelor of Science in
Mathematics in the Department of Mathematics and Computer Science, Siberian
Federal University, Krasnoyarsk, Russia. The work, which is described in this
thesis, has been performing in 2012-2014 in the Massachusetts Institute of
Technology and in the University of Oxford by the colleagues of the author and
by himself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07227</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07227</id><created>2016-01-26</created><authors><author><keyname>Elser</keyname><forenames>Veit</forenames></author></authors><title>A network that learns Strassen multiplication</title><categories>math.NA cs.NE</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study neural networks whose only non-linear components are multipliers, to
test a new training rule in a context where the precise representation of data
is paramount. These networks are challenged to discover the rules of matrix
multiplication, given many examples. By limiting the number of multipliers, the
network is forced to discover the Strassen multiplication rules. This is the
mathematical equivalent of finding low rank decompositions of the $n\times n$
matrix multiplication tensor, $M_n$. We train these networks with the
conservative learning rule, which makes minimal changes to the weights so as to
give the correct output for each input at the time the input-output pair is
received. Conservative learning needs a few thousand examples to find the rank
7 decomposition of $M_2$, and $10^5$ for the rank 23 decomposition of $M_3$
(the lowest known). High precision is critical, especially for $M_3$, to
discriminate between true decompositions and &quot;border approximations&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07228</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07228</id><created>2016-01-26</created><authors><author><keyname>Tripathy</keyname><forenames>Ardhendu</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>On Computation Rates for Arithmetic Sum</title><categories>cs.IT math.IT</categories><comments>Full paper for ISIT 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For zero-error function computation over directed acyclic networks, existing
upper and lower bounds on the computation capacity are known to be loose. In
this work we consider the problem of computing the arithmetic sum over a
specific directed acyclic network that is not a tree. We assume the sources to
be i.i.d. Bernoulli with parameter $1/2$. Even in this simple setting, we
demonstrate that upper bounding the computation rate is quite nontrivial. In
particular, it requires us to consider variable length network codes and relate
the upper bound to equivalently lower bounding the entropy of descriptions
observed by the terminal conditioned on the function value. This lower bound is
obtained by further lower bounding the entropy of a so-called \textit{clumpy
distribution}. We also demonstrate an achievable scheme that uses variable
length network codes and in-network compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07229</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07229</id><created>2016-01-26</created><authors><author><keyname>Balaji</keyname><forenames>Bharathan</forenames></author><author><keyname>Koh</keyname><forenames>Jason</forenames></author><author><keyname>Weibel</keyname><forenames>Nadir</forenames></author><author><keyname>Agarwal</keyname><forenames>Yuvraj</forenames></author></authors><title>Genie: A Longitudinal Study Comparing Physical and Software-augmented
  Thermostats in Office Buildings</title><categories>cs.HC cs.SY</categories><comments>12 pages</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thermostats are primary interfaces for occupants of office buildings to
express their comfort preferences. However, standard thermostats are often
ineffective due to inaccessibility, lack of information, or limited
responsiveness, leading to occupant discomfort. Software thermostats based on
web or smartphone applications provide alternative interfaces to occupants with
minimal deployment cost. However, their usage and effectiveness have not been
studied extensively in real settings. In this paper we present Genie, a novel
software-augmented thermostat that we deployed and studied at our university
over a period of 21 months. Our data shows that providing wider thermal control
to users does not lead to system abuse and that the effect on energy
consumption is minimal while improving comfort and energy awareness. We believe
that increased introduction of software thermostats in office buildings will
have important effects on comfort and energy consumption and we provide key
design recommendations for their implementation and deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07232</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07232</id><created>2016-01-26</created><authors><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Cooklev</keyname><forenames>T. V.</forenames></author></authors><title>Robust Image Watermarking Using Non-Regular Wavelets</title><categories>cs.MM cs.CR</categories><comments>13 pages, 11 figures</comments><journal-ref>Signal, Image and Video Processing, September 2009, Volume 3,
  Issue 3, pp 241-250</journal-ref><doi>10.1007/s11760-008-0070-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to watermarking digital images using non-regular wavelets is
advanced. Non-regular transforms spread the energy in the transform domain. The
proposed method leads at the same time to increased image quality and increased
robustness with respect to lossy compression. The approach provides robust
watermarking by suitably creating watermarked messages that have energy
compaction and frequency spreading. Our experimental results show that the
application of non-regular wavelets, instead of regular ones, can furnish a
superior robust watermarking scheme. The generated watermarked data is more
immune against non-intentional JPEG and JPEG2000 attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07233</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07233</id><created>2016-01-26</created><authors><author><keyname>Schaumberg</keyname><forenames>Andrew</forenames></author><author><keyname>Yu</keyname><forenames>Angela</forenames></author><author><keyname>Koshi</keyname><forenames>Tatsuhiro</forenames></author><author><keyname>Zong</keyname><forenames>Xiaochan</forenames></author><author><keyname>Rayadhurgam</keyname><forenames>Santoshkalyan</forenames></author></authors><title>Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers
  on Subgraphs of Molecules</title><categories>stat.ML cs.LG</categories><comments>7 pages, 5 figures</comments><acm-class>I.2.1; J.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this study, we intend to solve a mutual information problem in interacting
molecules of any type, such as proteins, nucleic acids, and small molecules.
Using machine learning techniques, we accurately predict pairwise interactions,
which can be of medical and biological importance. Graphs are are useful in
this problem for their generality to all types of molecules, due to the
inherent association of atoms through atomic bonds. Subgraphs can represent
different molecular domains. These domains can be biologically significant as
most molecules only have portions that are of functional significance and can
interact with other domains. Thus, we use subgraphs as features in different
machine learning algorithms to predict if two drugs interact and predict
potential single molecule effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07239</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07239</id><created>2016-01-26</created><authors><author><keyname>Mori</keyname><forenames>Hiroki</forenames></author><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Performance Analysis based on Density Evolution on Fault Erasure Belief
  Propagation Decoder</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will present an analysis on the fault erasure BP decoders
based on the density evolution. In the fault BP decoder, messages exchanged in
a BP process are stochastically corrupted due to unreliable logic gates and
flip-flops; i.e., we assume circuit components with transient faults. We
derived a set of the density evolution equations for the fault erasure BP
processes. Our density evolution analysis reveals the asymptotic behaviors of
the estimation error probability of the fault erasure BP decoders. In contrast
to the fault free cases, it is observed that the error probabilities of the
fault erasure BP decoder converge to positive values, and that there exists a
discontinuity in an error curve corresponding to the fault BP threshold. It is
also shown that an message encoding technique provides higher fault BP
thresholds than those of the original decoders at the cost of increased circuit
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07241</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07241</id><created>2016-01-26</created><authors><author><keyname>Taha</keyname><forenames>Ayman</forenames></author></authors><title>Knowledge Discovery In GIS Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent geographic information system (IGIS) is one of the promising
topics in GIS field. It aims at making GIS tools more sensitive for large
volumes of data stored inside GIS systems by integrating GIS with other
computer sciences such as Expert system (ES) Data Warehouse (DW), Decision
Support System (DSS), or Knowledge Discovery Database (KDD). One of the main
branches of IGIS is the Geographic Knowledge Discovery (GKD) which tries to
discover the implicit knowledge in the spatial databases. The main difference
between traditional KDD techniques and GKD techniques is hidden in the nature
of spatial data sets. In other words in the traditional data set the values of
each object are supposed to be independent from other objects in the same data
set, whereas the spatial dataset tends to be highly correlated according to the
first law of geography. The spatial outlier detection is one of the most
popular spatial data mining techniques which is used to detect spatial objects
whose non-spatial attributes values are extremely different from those of their
neighboring objects. Analyzing the behavior of these objects may produce an
interesting knowledge, which has an effective role in the decision-making
process. In this thesis, a new definition for the spatial neighborhood
relationship by is proposed considering the weights of the most effective
parameters of neighboring objects in a given spatial dataset. The spatial
parameters taken into our consideration are; distance, cost, and number of
direct connections between neighboring objects. A new model to detect spatial
outliers is also presented based on the new definition of the spatial
neighborhood relationship. This model is adapted to be applied to polygonal
objects. The proposed model is applied to an existing project for supporting
literacy in Fayoum governorate in Arab Republic of Egypt (ARE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07243</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07243</id><created>2016-01-26</created><authors><author><keyname>Honorio</keyname><forenames>Jean</forenames></author></authors><title>On the Sample Complexity of Learning Sparse Graphical Games</title><categories>cs.GT cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the sample complexity of learning sparse graphical games from
purely behavioral data. That is, we assume that we can only observe the
players' joint actions and not their payoffs. We analyze the sufficient and
necessary number of samples for the correct recovery of the set of
pure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses on
sparse directed graphs with $n$ nodes and at most $k$ parents per node. We show
that if the number of samples is greater than ${O(k n \log^2{n})}$, then
maximum likelihood estimation correctly recovers the PSNE with high
probability. We also show that if the number of samples is less than ${O(k n
\log^2{n})}$, then any conceivable method fails to recover the PSNE with
arbitrary probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07250</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07250</id><created>2016-01-26</created><authors><author><keyname>Zheng</keyname><forenames>Yuting</forenames></author></authors><title>Living Innovation Laboratory Model Design and Implementation</title><categories>cs.HC cs.CY</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Living Innovation Laboratory (LIL) is an open and recyclable way for
multidisciplinary researchers to remote control resources and co-develop user
centered projects. In the past few years, there were several papers about LIL
published and trying to discuss and define the model and architecture of LIL.
People all acknowledge about the three characteristics of LIL: user centered,
co-creation, and context aware, which make it distinguished from test platform
and other innovation approaches. Its existing model consists of five phases:
initialization, preparation, formation, development, and evaluation.
  Goal Net is a goal-oriented methodology to formularize a progress. In this
thesis, Goal Net is adopted to subtract a detailed and systemic methodology for
LIL. LIL Goal Net Model breaks the five phases of LIL into more detailed steps.
Big data, crowd sourcing, crowd funding and crowd testing take place in
suitable steps to realize UUI, MCC and PCA throughout the innovation process in
LIL 2.0. It would become a guideline for any company or organization to develop
a project in the form of an LIL 2.0 project.
  To prove the feasibility of LIL Goal Net Model, it was applied to two real
cases. One project is a Kinect game and the other one is an Internet product.
They were both transformed to LIL 2.0 successfully, based on LIL goal net based
methodology. The two projects were evaluated by phenomenography, which was a
qualitative research method to study human experiences and their relations in
hope of finding the better way to improve human experiences. Through
phenomenographic study, the positive evaluation results showed that the new
generation of LIL had more advantages in terms of effectiveness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07252</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07252</id><created>2016-01-26</created><authors><author><keyname>Gupta</keyname><forenames>Anshul</forenames></author><author><keyname>Gutierrez-Osuna</keyname><forenames>Ricardo</forenames></author><author><keyname>Christy</keyname><forenames>Matthew</forenames></author><author><keyname>Furuta</keyname><forenames>Richard</forenames></author><author><keyname>Mandell</keyname><forenames>Laura</forenames></author></authors><title>Font Identification in Historical Documents Using Active Learning</title><categories>cs.CV cs.AI cs.DL stat.AP stat.ML</categories><acm-class>I.5; I.2</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Identifying the type of font (e.g., Roman, Blackletter) used in historical
documents can help optical character recognition (OCR) systems produce more
accurate text transcriptions. Towards this end, we present an active-learning
strategy that can significantly reduce the number of labeled samples needed to
train a font classifier. Our approach extracts image-based features that
exploit geometric differences between fonts at the word level, and combines
them into a bag-of-word representation for each page in a document. We evaluate
six sampling strategies based on uncertainty, dissimilarity and diversity
criteria, and test them on a database containing over 3,000 historical
documents with Blackletter, Roman and Mixed fonts. Our results show that a
combination of uncertainty and diversity achieves the highest predictive
accuracy (89% of test cases correctly classified) while requiring only a small
fraction of the data (17%) to be labeled. We discuss the implications of this
result for mass digitization projects of historical documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07254</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07254</id><created>2016-01-26</created><authors><author><keyname>Choudhary</keyname><forenames>Sunav</forenames></author><author><keyname>Kumar</keyname><forenames>Naveen</forenames></author><author><keyname>Narayanan</keyname><forenames>Srikanth</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Active Target Localization using Low-Rank Matrix Completion and Unimodal
  Regression</title><categories>cs.IT math.IT</categories><comments>24 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection and localization of a target from samples of its generated
field is a problem of interest in a broad range of applications. Often, the
target field admits structural properties that enable the design of lower
sample detection strategies with good performance. This paper designs a
sampling and localization strategy which exploits separability and unimodality
in target fields and theoretically analyzes the trade-off achieved between
sampling density, noise level and convergence rate of localization. In
particular, the strategy adopts an exploration-exploitation approach to target
detection and utilizes the theory of low-rank matrix completion, coupled with
unimodal regression, on decaying and approximately separable target fields. The
assumptions on the field are fairly generic and are applicable to many decay
profiles since no specific knowledge of the field is necessary, besides its
admittance of an approximately rank-one representation. Extensive numerical
experiments and comparisons are performed to test the efficacy and robustness
of the presented approach. Numerical results suggest that the proposed strategy
outperforms algorithms based on mean-shift clustering, surface interpolation
and naive low-rank matrix completion with peak detection, under low sampling
density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07255</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07255</id><created>2016-01-26</created><authors><author><keyname>Wu</keyname><forenames>Lin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>PersonNet: Person Re-identification with Deep Convolutional Neural
  Networks</title><categories>cs.CV</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a deep end-to-end neu- ral network to
simultaneously learn high-level features and a corresponding similarity metric
for person re-identification. The network takes a pair of raw RGB images as
input, and outputs a similarity value indicating whether the two input images
depict the same person. A layer of computing neighborhood range differences
across two input images is employed to capture local relationship between
patches. This operation is to seek a robust feature from input images. By
increasing the depth to 10 weight layers and using very small (3$\times$3)
convolution filters, our architecture achieves a remarkable improvement on the
prior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp)
gradient decent algorithm is integrated into our architecture, which is
beneficial to deep nets. Our method consistently outperforms state-of-the-art
on two large datasets (CUHK03 and Market-1501), and a medium-sized data set
(CUHK01).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07258</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07258</id><created>2016-01-26</created><authors><author><keyname>Kulkarni</keyname><forenames>Kuldeep</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author></authors><title>Fast Integral Image Estimation at 1% measurement rate</title><categories>cs.CV math.OC</categories><comments>Submitted to TPAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework called ReFInE to directly obtain integral image
estimates from a very small number of spatially multiplexed measurements of the
scene without iterative reconstruction of any auxiliary image, and demonstrate
their practical utility in visual object tracking. Specifically, we design
measurement matrices which are tailored to facilitate extremely fast estimation
of the integral image, by using a single-shot linear operation on the measured
vector. Leveraging a prior model for the images, we formulate a nuclear norm
minimization problem with second order conic constraints to jointly obtain the
measurement matrix and the linear operator. Through qualitative and
quantitative experiments, we show that high quality integral image estimates
can be obtained using our framework at very low measurement rates. Further, on
a standard dataset of 50 videos, we present object tracking results which are
comparable to the state-of-the-art methods, even at an extremely low
measurement rate of 1%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07260</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07260</id><created>2016-01-26</created><authors><author><keyname>Koh</keyname><forenames>Jason</forenames></author><author><keyname>Balaji</keyname><forenames>Bharathan</forenames></author><author><keyname>Akhlaghi</keyname><forenames>Vahideh</forenames></author><author><keyname>Agarwal</keyname><forenames>Yuvraj</forenames></author><author><keyname>Gupta</keyname><forenames>Rajesh</forenames></author></authors><title>Quiver: Using Control Perturbations to Increase the Observability of
  Sensor Data in Smart Buildings</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern buildings consist of hundreds of sensors and actuators for monitoring
and operation of systems such as HVAC, light and security. To enable portable
applications in next generation smart buildings, we need models and
standardized ontologies that represent these sensors across diverse types of
buildings. Recent research has shown that extracting information such as sensor
type with available metadata and timeseries data analysis is difficult due to
heterogeneity of systems and lack of support for interoperability. We propose
perturbations in the control system as a mechanism to increase the
observability of building systems to extract contextual information and develop
standardized models. We design Quiver, an experimental framework for actuation
of building HVAC system that enables us to perturb the control system safely.
Using Quiver, we demonstrate three applications using empirical experiments on
a real commercial building: colocation of data points, identification of point
type and mapping of dependency between actuators. Our results show that we can
colocate data points in HVAC terminal units with 98.4 % accuracy and 63 %
coverage. We can identify point types of the terminal units with 85.3 %
accuracy. Finally, we map the dependency links between actuators with an
accuracy of 73.5 %, with 8.1 % and 18.4 % false positives and false negatives
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07262</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07262</id><created>2016-01-26</created><authors><author><keyname>Zhu</keyname><forenames>Ye</forenames></author><author><keyname>Ng</keyname><forenames>Tian-Tsong</forenames></author><author><keyname>Shen</keyname><forenames>Xuanjing</forenames></author><author><keyname>Wen</keyname><forenames>Bihan</forenames></author></authors><title>Revisiting copy-move forgery detection by considering realistic image
  with similar but genuine objects</title><categories>cs.MM</categories><comments>The version of ICASSP2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many images, of natural or man-made scenes often contain Similar but Genuine
Objects (SGO). This poses a challenge to existing Copy-Move Forgery Detection
(CMFD) methods which match the key points / blocks, solely based on the pair
similarity in the scene. To address such issue, we propose a novel CMFD method
using Scaled Harris Feature Descriptors (SHFD) that preform consistently well
on forged images with SGO. It involves the following main steps: (i) Pyramid
scale space and orientation assignment are used to keep scaling and rotation
invariance; (ii) Combined features are applied for precise texture description;
(iii) Similar features of two points are matched and RANSAC is used to remove
the false matches. The experimental results indicate that the proposed
algorithm is effective in detecting SGO and copy-move forgery, which compares
favorably to existing methods. Our method exhibits high robustness even when an
image is operated by geometric transformation and post-processing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07264</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07264</id><created>2016-01-27</created><authors><author><keyname>Lim</keyname><forenames>Su Fang</forenames></author></authors><title>Persuasive Teachable Agent for Intergenerational Learning</title><categories>cs.HC cs.CY</categories><comments>This is a book draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teachable agents are computer agents based on the pedagogical concept of
learning-by-teaching. During the tutoring process, where students take on the
role of the tutor to teach a computer agent tutee, learners have been observed
to gain deeper understanding of the subject matter. Teachable agents are
commonly used in the areas of science and mathematics learning where learners
are able to learn complex concepts and deep reasoning by teaching the teachable
agent through graphic representation such as concept maps.
  Literature review on teachable agents as well as observations during field
studies conducted by the researcher, have shown that many current teachable
agents lack the interaction abilities required to keep learners engage in
learning tasks. The result of this is learners deviating from the teaching
process, and thus the learners are unable to benefit fully from learning with
the teachable agent. The applications of teachable agents are restricted to the
learning of academic subjects such as mathematics and science.
  In this book, we have proposed the Persuasive Teachable Agent (PTA), a
teachable agent based on the theoretical framework of persuasion, computational
and goal-oriented agent modelling. We argue that the PTA, an autonomous agent,
capable of encouraging attitude and behavioural change can offer a more
meaningful and engaging learning experiences for learners from different age
groups. Based on the findings from our research we argue that persuasive
feedback actions generated by the PTA provide significant influence over
learner's decision to participate in intergenerational learning. The PTA plays
a crucial role in the development of future persuasive technologies in
artificially intelligent agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07265</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07265</id><created>2016-01-27</created><authors><author><keyname>Huang</keyname><forenames>Siyu</forenames></author><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author><author><keyname>He</keyname><forenames>Zhouzhou</forenames></author><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Zhuang</keyname><forenames>Yueting</forenames></author></authors><title>Deep Learning Driven Visual Path Prediction from a Single Image</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capabilities of inference and prediction are significant components of visual
systems. In this paper, we address an important and challenging task of them:
visual path prediction. Its goal is to infer the future path for a visual
object in a static scene. This task is complicated as it needs high-level
semantic understandings of both the scenes and motion patterns underlying video
sequences. In practice, cluttered situations have also raised higher demands on
the effectiveness and robustness of the considered models. Motivated by these
observations, we propose a deep learning framework which simultaneously
performs deep feature learning for visual representation in conjunction with
spatio-temporal context modeling. After that, we propose a unified path
planning scheme to make accurate future path prediction based on the analytic
results of the context models. The highly effective visual representation and
deep context models ensure that our framework makes a deep semantic
understanding of the scene and motion pattern, consequently improving the
performance of the visual path prediction task. In order to comprehensively
evaluate the model's performance on the visual path prediction task, we
construct two large benchmark datasets from the adaptation of video tracking
datasets. The qualitative and quantitative experimental results show that our
approach outperforms the existing approaches and owns a better generalization
capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07267</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07267</id><created>2016-01-27</created><updated>2016-02-01</updated><authors><author><keyname>Avramopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Evolutionary stability implies asymptotic stability under multiplicative
  weights</title><categories>cs.GT cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that evolutionarily stable states in general (nonlinear) population
games (which can be viewed as continuous vector fields constrained on a
polytope) are asymptotically stable under a multiplicative weights dynamic
(under appropriate choices of a parameter called the learning rate or step
size, which we demonstrate to be crucial to achieve convergence, as otherwise
even chaotic behavior is possible to manifest). Our result implies that
evolutionary theories based on multiplicative weights are compatible (in
principle, more general) with those based on the notion of evolutionary
stability. However, our result further establishes multiplicative weights as a
nonlinear programming primitive (on par with standard nonlinear programming
methods) since various nonlinear optimization problems, such as finding
Nash/Wardrop equilibria in nonatomic congestion games, which are well-known to
be equipped with a convex potential function, and finding strict local maxima
of quadratic programming problems, are special cases of the problem of
computing evolutionarily stable states in nonlinear population games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07270</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07270</id><created>2016-01-27</created><authors><author><keyname>Nie</keyname><forenames>Xiushan</forenames></author><author><keyname>Yin</keyname><forenames>Yilong</forenames></author><author><keyname>Sun</keyname><forenames>Jiande</forenames></author></authors><title>Comprehensive Feature-based Robust Video Fingerprinting Using Tensor
  Model</title><categories>cs.CV</categories><comments>13pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-based near-duplicate video detection (NDVD) is essential for
effective search and retrieval, and robust video fingerprinting is a good
solution for NDVD. Most existing video fingerprinting methods use a single
feature or concatenating different features to generate video fingerprints, and
show a good performance under single-mode modifications such as noise addition
and blurring. However, when they suffer combined modifications, the performance
is degraded to a certain extent because such features cannot characterize the
video content completely. By contrast, the assistance and consensus among
different features can improve the performance of video fingerprinting.
Therefore, in the present study, we mine the assistance and consensus among
different features based on tensor model, and present a new comprehensive
feature to fully use them in the proposed video fingerprinting framework. We
also analyze what the comprehensive feature really is for representing the
original video. In this framework, the video is initially set as a high-order
tensor that consists of different features, and the video tensor is decomposed
via the Tucker model with a solution that determines the number of components.
Subsequently, the comprehensive feature is generated by the low-order tensor
obtained from tensor decomposition. Finally, the video fingerprint is computed
using this feature. A matching strategy used for narrowing the search is also
proposed based on the core tensor. The robust video fingerprinting framework is
resistant not only to single-mode modifications, but also to the combination of
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07273</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07273</id><created>2016-01-27</created><authors><author><keyname>Liu</keyname><forenames>Gangli</forenames></author><author><keyname>Feng</keyname><forenames>Ling</forenames></author></authors><title>A Method to Support Difficult Re-finding Tasks</title><categories>cs.IR cs.HC</categories><acm-class>H.3.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Re-finding electronic documents from a personal computer is a frequent demand
to users. In a simple re-finding task, people can use many methods to retrieve
a document, such as navigating directly to the document's folder, searching
with a desktop search engine, or checking the Recent Files List. However, when
encountering a difficult re-finding task, people usually cannot remember the
attributes used by conventional re-finding methods, such as file path, file
name, keywords etc., the re-finding would fail. We propose a new method to
support difficult re-finding tasks. When a user is reading a document, we
collect all kinds of possible memory pieces of the user about the document,
such as number of pages, number of images, number of math formulas, cumulative
reading time, reading frequency, printing experiences etc. If the user wants to
re-find a document later, we use these collected attributes to filter out the
target document. To alleviate the user's cognitive burden, we use a question
and answer wizard interface and provide recommendations to the answers for the
user, the recommendations are generated by analyzing the collected attributes
of each document and the user's experiences about them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07275</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07275</id><created>2016-01-27</created><authors><author><keyname>P</keyname><forenames>Resmi Suresh M</forenames></author><author><keyname>Sankaran</keyname><forenames>Ganesh</forenames></author><author><keyname>Joopudi</keyname><forenames>Sreeram</forenames></author><author><keyname>Narasimhan</keyname><forenames>Shankar</forenames></author><author><keyname>Choudhury</keyname><forenames>Suman Roy</forenames></author><author><keyname>Rengaswamy</keyname><forenames>Raghunathan</forenames></author></authors><title>Optimal Power Distribution Control for a Network of Fuel Cell Stacks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In power networks where multiple fuel cell stacks are employed to deliver the
required power, optimal sharing of the power demand between different stacks is
an important problem. This is because the total current collectively produced
by all the stacks is directly proportional to the fuel utilization, through
stoichiometry. As a result, one would like to produce the required power while
minimizing the total current produced. In this paper, an optimization
formulation is proposed for this power distribution control problem. An
algorithm that identifies the globally optimal solution for this problem is
developed. Through an analysis of the KKT conditions, the solution to the
optimization problem is decomposed into on-line and on-line computations. The
on-line computations reduce to simple equation solving. For an application with
a specific v-i function derived from data, we show that analytical solutions
exist for on-line computations. We also discuss the wider applicability of the
proposed approach for similar problems in other domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07279</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07279</id><created>2016-01-27</created><authors><author><keyname>Lauri</keyname><forenames>Mikko</forenames></author><author><keyname>Atanasov</keyname><forenames>Nikolay</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author><author><keyname>Ritala</keyname><forenames>Risto</forenames></author></authors><title>Myopic Policy Bounds for Information Acquisition POMDPs</title><categories>cs.SY</categories><comments>8 pages, 3 figures</comments><msc-class>90C40</msc-class><acm-class>G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of optimal control of robotic sensing
systems aimed at autonomous information gathering in scenarios such as
environmental monitoring, search and rescue, and surveillance and
reconnaissance. The information gathering problem is formulated as a partially
observable Markov decision process (POMDP) with a reward function that captures
uncertainty reduction. Unlike the classical POMDP formulation, the resulting
reward structure is nonlinear in the belief state and the traditional
approaches do not apply directly. Instead of developing a new approximation
algorithm, we show that if attention is restricted to a class of problems with
certain structural properties, one can derive (often tight) upper and lower
bounds on the optimal policy via an efficient myopic computation. These policy
bounds can be applied in conjunction with an online branch-and-bound algorithm
to accelerate the computation of the optimal policy. We obtain informative
lower and upper policy bounds with low computational effort in a target
tracking domain. The performance of branch-and-bounding is demonstrated and
compared with exact value iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07283</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07283</id><created>2016-01-27</created><authors><author><keyname>Halbawi</keyname><forenames>Wael</forenames></author><author><keyname>Liu</keyname><forenames>Zihan</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Balanced Reed-Solomon Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing linear Maximum Distance Separable
(MDS) error-correcting codes with generator matrices that are sparsest and
balanced. In this context, sparsest means that every row has the least possible
number of non-zero entries, and balanced means that every column contains the
same number of non-zero entries. Codes with this structure minimize the maximal
computation time of computing any code symbol, a property that is appealing to
systems where computational load-balancing is critical. The problem was studied
before by Dau et al. where it was shown that there always exists an MDS code
over a sufficiently large field such that its generator matrix is both sparsest
and balanced. However, the construction is not explicit and more importantly,
the resulting MDS codes do not lend themselves to efficient error correction.
With an eye towards explicit constructions with efficient decoding, we show in
this paper that the generator matrix of a cyclic Reed-Solomon code of length
$n$ and dimension $k$ can always be transformed to one that is both sparsest
and balanced, for all parameters $n$ and $k$ where $\frac{k}{n}(n - k + 1)$ is
an integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07285</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07285</id><created>2016-01-27</created><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Chan</keyname><forenames>Chung</forenames></author><author><keyname>Zhou</keyname><forenames>Qiaoqiao</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Fairness in Communication for Omniscience</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of how to fairly distribute the minimum sum-rate
among the users in communication for omniscience (CO). We formulate a problem
of minimizing a weighted quadratic function over a submodular base polyhedron
which contains all achievable rate vectors, or transmission strategies, for CO
that have the same sum-rate. By solving it, we can determine the rate vector
that optimizes the Jain's fairness measure, a more commonly used fairness index
than the Shapley value in communications engineering. We show that the
optimizer is a lexicographically optimal (lex-optimal) base and can be
determined by a decomposition algorithm (DA) that is based on submodular
function minimization (SFM) algorithm and completes in strongly polynomial
time. We prove that the lex-optimal minimum sum-rate strategy for CO can be
determined by finding the lex-optimal base in each user subset in the
fundamental partition and the complexity can be reduced accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07300</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07300</id><created>2016-01-27</created><updated>2016-02-04</updated><authors><author><keyname>Lin</keyname><forenames>Yong</forenames></author><author><keyname>Henz</keyname><forenames>Martin</forenames></author></authors><title>Recollection: an Alternative Restoration Technique for Constraint
  Programming Systems</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search is a key service within constraint programming systems, and it demands
the restoration of previously accessed states during the exploration of a
search tree. Restoration proceeds either bottom-up within the tree to roll back
previously performed operations using a trail, or top-down to redo them,
starting from a previously stored state and using suitable information stored
along the way. In this paper, we elucidate existing restoration techniques
using a pair of abstract methods and employ them to present a new technique
that we call recollection. The proposed technique stores the variables that
were affected by constraint propagation during fix points reasoning steps, and
it conducts neither operation roll-back nor recomputation, while consuming much
less memory than storing previous visited states. We implemented this idea as a
prototype within the Gecode solver. An empirical evaluation reveals that
constraint problems with expensive propagation and frequent failures can
benefit from recollection with respect to runtime at the expense of a marginal
increase in memory consumption, comparing with the most competitive variant of
recomputation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07315</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07315</id><created>2016-01-27</created><updated>2016-02-05</updated><authors><author><keyname>Ak</keyname><forenames>Serkan</forenames></author><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Downlink Outage Performance of Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives tight performance upper and lower bounds on the downlink
outage efficiency of K-tier heterogeneous cellular networks (HCNs) for general
signal propagation models with Poisson distributed base stations in each tier.
In particular, the proposed approach to analyze the outage metrics in a K-tier
HCN allows for the use of general bounded path-loss functions and random fading
processes of general distributions. Considering two specific base station (BS)
association policies, it is shown that the derived performance bounds track the
actual outage metrics reasonably well for a wide range of BS densities, with
the gap among them becoming negligibly small for denser HCN deployments. A
simulation study is also performed for 2-tier and 3-tier HCN scenarios to
illustrate the closeness of the derived bounds to the actual outage performance
with various selections of the HCN parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07322</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07322</id><created>2016-01-27</created><authors><author><keyname>Serbetci</keyname><forenames>Berksan</forenames></author><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author></authors><title>On Optimal Geographical Caching in Heterogeneous Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 6 figures, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate optimal geographical caching in heterogeneous
cellular networks. There is a content library containing files with the same
size with popularities following a probability distribution. There are
different types of base stations in the plane with different cache capacities.
The performance metric is the total hit probability which is the probability
that a user at an arbitrary location in the plane will find the content that he
requires in one of the base stations that he is covered by. We consider various
scenarios for the distribution of base stations in the plane. We consider the
problem of optimally placing content in a first type of base station
independent of the other base stations. The content for the next type is then
placed optimally depending on the placement strategy of the first type. We
demonstrate that these optimization problems are convex. As a result we are
able to show how the hit probability evolves as the deployment density of the
new type of base stations increases. We show that the heuristic of placing the
most popular content in the first type of base station is almost optimal. Also,
we show that for the second type of base station no such heuristic can be used;
optimal content placement is significantly better than storing the most popular
content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07325</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07325</id><created>2016-01-27</created><authors><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>DoF Analysis of the MIMO Broadcast Channel with Alternating/Hybrid CSIT</title><categories>cs.IT math.IT</categories><comments>14 pages, accepted for publication in IEEE Trans. on Information
  Theory. arXiv admin note: substantial text overlap with arXiv:1311.6647</comments><doi>10.1109/TIT.2016.2517060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a $K$-user multiple-input single-output (MISO) broadcast channel
(BC) where the channel state information (CSI) of user $i(i=1,2,\ldots,K)$ may
be instantaneously perfect (P), delayed (D) or not known (N) at the transmitter
with probabilities $\lambda_P^i$, $\lambda_D^i$ and $\lambda_N^i$,
respectively. In this setting, according to the three possible CSIT for each
user, knowledge of the joint CSIT of the $K$ users could have at most $3^K$
states. In this paper, given the marginal probabilities of CSIT (i.e.,
$\lambda_P^i$, $\lambda_D^i$ and $\lambda_N^i$), we derive an outer bound for
the DoF region of the $K$-user MISO BC. Subsequently, we tighten this outer
bound by taking into account a set of inequalities that capture some of the
$3^K$ states of the joint CSIT. One of the consequences of this set of
inequalities is that for $K\geq3$, it is shown that the DoF region is not
completely characterized by the marginal probabilities in contrast to the
two-user case. Afterwards, the tightness of these bounds are investigated
through the discussion on the achievability. Finally, a two user MIMO BC having
CSIT among P and N is considered in which an outer bound for the DoF region is
provided and it is shown that in some scenarios it is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07333</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07333</id><created>2016-01-27</created><authors><author><keyname>Mizrahi</keyname><forenames>Tal</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>OneClock to Rule Them All: Using Time in Networked Applications</title><categories>cs.NI cs.DC</categories><comments>This technical report is an extended version of &quot;OneClock to Rule
  Them All: Using Time in Networked Applications&quot;, which was accepted to
  IEEE/IFIP NOMS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces OneClock, a generic approach for using time in
networked applications. OneClock provides two basic time-triggered primitives:
the ability to schedule an operation at a remote host or device, and the
ability to receive feedback about the time at which an event occurred or an
operation was executed at a remote host or device. We introduce a novel
prediction-based scheduling approach that uses timing information collected at
runtime to accurately schedule future operations.
  Our work includes an extension to the Network Configuration protocol
(NETCONF), which enables OneClock in real-life systems. This extension has been
published as an Internet Engineering Task Force (IETF) RFC, and a prototype of
our NETCONF time extension is publicly available as open source.
  Experimental evaluation shows that our prediction-based approach allows
accurate scheduling in diverse and heterogeneous environments, with various
hardware capabilities and workloads. OneClock is a generic approach that can be
applied to any managed device: sensors, actuators, Internet of Things (IoT)
devices, routers, or toasters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07336</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07336</id><created>2016-01-27</created><authors><author><keyname>Yin</keyname><forenames>Ming</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Zhang</keyname><forenames>Yun</forenames></author></authors><title>Neighborhood Preserved Sparse Representation for Robust Classification
  on Symmetric Positive Definite Matrices</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1601.00414</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its promising classification performance, sparse representation based
classification(SRC) algorithm has attracted great attention in the past few
years. However, the existing SRC type methods apply only to vector data in
Euclidean space. As such, there is still no satisfactory approach to conduct
classification task for symmetric positive definite (SPD) matrices which is
very useful in computer vision. To address this problem, in this paper, a
neighborhood preserved kernel SRC method is proposed on SPD manifolds.
Specifically, by embedding the SPD matrices into a Reproducing Kernel Hilbert
Space (RKHS), the proposed method can perform classification on SPD manifolds
through an appropriate Log-Euclidean kernel. Through exploiting the geodesic
distance between SPD matrices, our method can effectively characterize the
intrinsic local Riemannian geometry within data so as to well unravel the
underlying sub-manifold structure. Despite its simplicity, experimental results
on several famous database demonstrate that the proposed method achieves better
classification results than the state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07340</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07340</id><created>2016-01-27</created><authors><author><keyname>Yu</keyname><forenames>Xianghao</forenames></author><author><keyname>Shen</keyname><forenames>Juei-Chin</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Alternating Minimization Algorithms for Hybrid Precoding in Millimeter
  Wave MIMO Systems</title><categories>cs.IT math.IT</categories><comments>16 pages,8 figures, to appear in IEEE Journal of Selected Topics in
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) communications has been regarded as a key enabling
technology for 5G networks. In contrast to conventional
multiple-input-multiple-output (MIMO) systems, precoding in mmWave MIMO cannot
be performed entirely at baseband using digital precoders, as only a limited
number of signal mixers and analog-to-digital converters (ADCs) can be
supported considering their cost and power consumption. As a cost-effective
alternative, a hybrid precoding transceiver architecture, combining a digital
precoder and an analog precoder, has recently received considerable attention.
However, the optimal design of such hybrid precoders has not been fully
understood. In this paper, treating the hybrid precoder design as a matrix
factorization problem, effective alternating minimization (AltMin) algorithms
will be proposed for two different hybrid precoding structures, i.e., the
fully-connected and partially-connected structures. In particular, for the
fully-connected structure, an AltMin algorithm based on manifold optimization
is proposed to approach the performance of the fully digital precoder, which,
however, has a high complexity. Thus, a low-complexity AltMin algorithm is then
proposed, by enforcing an orthogonal constraint on the digital precoder.
Furthermore, for the partially-connected structure, an AltMin algorithm is also
developed with the help of semidefinite relaxation. For practical
implementation, the proposed AltMin algorithms are further extended to the
broadband setting with orthogonal frequency division multiplexing (OFDM)
modulation. Simulation results will demonstrate significant performance gains
of the proposed AltMin algorithms over existing hybrid precoding algorithms.
Moreover, based on the proposed algorithms, simulation comparisons between the
two hybrid precoding structures will provide valuable design insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07341</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07341</id><created>2016-01-27</created><authors><author><keyname>Qu</keyname><forenames>Yuben</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Dong</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>Guo</keyname><forenames>Song</forenames></author><author><keyname>Tian</keyname><forenames>Chang</forenames></author></authors><title>Providing Probabilistic Robustness Guarantee for Crowdsensing</title><categories>cs.GT cs.SI</categories><comments>33 pages, 4 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Due to its flexible and pervasive sensing ability, crowdsensing has been
extensively studied recently in research communities. However, the fundamental
issue of how to meet the requirement of sensing robustness in crowdsensing
remains largely unsolved. Specifically, from the task owner's perspective, how
to minimize the total payment in crowdsensing while guaranteeing the sensing
data quality is a critical issue to be resolved. We elegantly model the
robustness requirement over sensing data quality as chance constraints, and
investigate both hard and soft chance constraints for different crowdsensing
applications. For the former, we reformulate the problem through Boole's
Inequality, and explore the optimal value gap between the original problem and
the reformulated problem. For the latter, we study a serial of a general
payment minimization problem, and propose a binary search algorithm that
achieves both feasibility and low payment. The performance gap between our
solution and the optimal solution is also theoretically analyzed. Extensive
simulations validate our theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07343</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07343</id><created>2016-01-27</created><authors><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>On extremal double circulant self-dual codes of lengths $90$ and $92$</title><categories>math.CO cs.IT math.IT</categories><comments>10 pages</comments><msc-class>94B</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classification of extremal double circulant self-dual codes of lengths up
to $88$ is known. We demonstrate that there is no extremal double circulant
self-dual code of length $90$. We give a classification of double circulant
self-dual $[90,45,14]$ codes. In addition, we demonstrate that every double
circulant self-dual $[90,45,14]$ code has no extremal self-dual neighbor of
length $90$. Finally, we give a classification of extremal double circulant
self-dual codes of length $92$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07352</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07352</id><created>2016-01-27</created><updated>2016-02-16</updated><authors><author><keyname>Nicolaou</keyname><forenames>Nicolas</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames></author></authors><title>CoVer-ability: Consistent Versioning for Concurrent Objects</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An object type characterizes the domain space and the operations that can be
invoked on an object of that type. In this paper we introduce a new property
for concurrent objects, we call coverability, that aims to provide precise
guarantees on the consistent evolution of an object. This new property is
suitable for a variety of distributed objects including concurrent file objects
that demand operations to manipulate the latest version of the object. We
propose two levels of coverability: (i) strong coverability and (ii) weak
coverability. Strong coverability requires that only a single operation can
modify the latest version of the object, i.e. &quot;covers&quot; the latest version with
a new version, imposing a total order on object modifications. Weak
coverability relaxes the strong requirements of strong coverability and allows
multiple operations to modify the same version of an object, where each
modification leads to a different version. Weak coverability preserves
consistent evolution of the object, by demanding any subsequent operation to
only modify one of the newly introduced versions. Coverability combined with
atomic guarantees yield to coverable atomic read/write registers. We also show
that strongly coverable atomic registers are equivalent in power to consensus.
Thus, we focus on weakly coverable registers, and we demonstrate their
importance by showing that they cannot be implemented using similar types of
registers, like ranked-registers. Furthermore we show that weakly coverable
registers may be used to implement basic (weak) read-modify-write and file
objects. Finally, we implement weakly coverable registers by modifying an
existing MWMR atomic register implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07355</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07355</id><created>2016-01-27</created><authors><author><keyname>Skvortsov</keyname><forenames>Pavel</forenames></author><author><keyname>Hoppe</keyname><forenames>Dennis</forenames></author><author><keyname>Tenschert</keyname><forenames>Axel</forenames></author><author><keyname>Gienger</keyname><forenames>Michael</forenames></author></authors><title>Monitoring in the Clouds: Comparison of ECO2Clouds and EXCESS Monitoring
  Approaches</title><categories>cs.DC</categories><comments>2nd International Workshop on Dynamic Resource Allocation and
  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016
  (arXiv:cs/1601.04675)</comments><report-no>DREAMCloud/2016/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing adoption of private cloud infrastructures by providers
and enterprises, the monitoring of these infrastructures is becoming crucial.
The rationale behind monitoring is manifold: reasons include saving energy,
lowering costs, and better maintenance. In the e-Science sector, moreover, the
collection of infrastructure and application-specific data at high resolutions
is immanent. In this paper, we present two monitoring approaches implemented
throughout two European projects: ECO2Clouds and EXCESS. The ECO2Clouds project
aims to minimize CO2 emissions caused by the execution of applications on the
cloud infrastructure. In order to allow for eco-aware deployment and scheduling
of applications, the ECO2Clouds monitoring framework provides the necessary set
of metrics on different layers including physical, virtual and application
layer. In turn, the EXCESS project introduces new energy-aware execution models
that improve energy-efficiency on a software level. Having in-depth knowledge
about the energy consumption and overall behavior of applications on a given
infrastructure, subsequent executions can be optimized to save energy. To
achieve this goal, the EXCESS monitoring framework provides APIs allowing
developers to collect application-specific data in addition to infrastructure
data at run-time. We perform a comparative analysis of both monitoring
approaches, and highlighting use cases including a hybrid approach which
benefits from both monitoring solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07358</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07358</id><created>2016-01-27</created><authors><author><keyname>Clausen</keyname><forenames>Jens</forenames></author><author><keyname>Briegel</keyname><forenames>Hans J.</forenames></author></authors><title>Quantum machine learning with glow for episodic tasks and decision games</title><categories>quant-ph cs.AI cs.LG</categories><comments>20 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a general class of models, where a reinforcement learning (RL)
agent learns from cyclic interactions with an external environment via
classical signals. Perceptual inputs are encoded as quantum states, which are
subsequently transformed by a quantum channel representing the agent's memory,
while the outcomes of measurements performed at the channel's output determine
the agent's actions. The learning takes place via stepwise modifications of the
channel properties. They are described by an update rule that is inspired by
the projective simulation (PS) model and equipped with a glow mechanism that
allows for a backpropagation of policy changes, analogous to the eligibility
traces in RL and edge glow in PS. In this way, the model combines features of
PS with the ability for generalization, offered by its physical embodiment as a
quantum system. We apply the agent to various setups of an invasion game and a
grid world, which serve as elementary model tasks allowing a direct comparison
with a basic classical PS agent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07376</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07376</id><created>2016-01-24</created><updated>2016-02-23</updated><authors><author><keyname>Almeida</keyname><forenames>Ricardo</forenames></author></authors><title>Variational Problems Involving a Caputo-Type Fractional Derivative</title><categories>math.OC cs.SY</categories><comments>This is a preprint of a paper whose final and definite form will be
  published in Journal of Optimization Theory and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to study certain problems of calculus of variations,
that are dependent upon a Lagrange function on a Caputo-type fractional
derivative. This type of fractional operator is a generalization of the Caputo
and the Caputo--Hadamard fractional derivatives, that are dependent on a real
parameter ro. Sufficient and necessary conditions of the first and second order
are presented. The cases of integral and holonomic constraints are also
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07377</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07377</id><created>2016-01-25</created><authors><author><keyname>Nguyen</keyname><forenames>Duong Tung</forenames></author></authors><title>Optimal Energy Management for SmartGrids Considering Thermal Load and
  Dynamic Pricing</title><categories>cs.SY</categories><comments>This is my Master thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More active participation of the demand side and efficient integration of
distributed energy resources (DERs) such as electric vehicles (trVs), energy
storage (ES), and renewable energy sources (RESs) into the existing power
systems are important design objectives of the future smart grid. In general,
effective demand side management (DSM) would benefit both system operators
(e.g., peak demand reduction) and electricity customers (e.g., cost saving).
For building and home energy scheduling design, heating, ventilation, and
air-conditioning (HVAC) systems play a very important role since HVAC power
consumption is very significant and the HVAC load can be scheduled flexibly
while still maintaining user comfort requirements. This thesis focuses on
energy scheduling design for two different application scenarios where HVAC and
various DERs are considered to optimize the benefits electric users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07381</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07381</id><created>2016-01-26</created><authors><author><keyname>Bianchi</keyname><forenames>Filippo Maria</forenames></author><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Alippi</keyname><forenames>Cesare</forenames></author></authors><title>Investigating echo state networks dynamics by means of recurrence
  analysis</title><categories>physics.data-an cs.LG nlin.CD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we elaborate over the well-known interpretability issue in
echo state networks. The idea is to investigate the dynamics of reservoir
neurons with time-series analysis techniques taken from research on complex
systems. Notably, we analyze time-series of neuron activations with Recurrence
Plots (RPs) and Recurrence Quantification Analysis (RQA), which permit to
visualize and characterize high-dimensional dynamical systems. We show that
this approach is useful in a number of ways. First, the two-dimensional
representation offered by RPs provides a way for visualizing the
high-dimensional dynamics of a reservoir. Our results suggest that, if the
network is stable, reservoir and input denote similar line patterns in the
respective RPs. Conversely, the more unstable the ESN, the more the RP of the
reservoir presents instability patterns. As a second result, we show that the
$\mathrm{L_{max}}$ measure is highly correlated with the well-established
maximal local Lyapunov exponent. This suggests that complexity measures based
on RP diagonal lines distribution provide a valuable tool to quantify the
degree of network stability. Finally, our analysis shows that all RQA measures
fluctuate on the proximity of the so-called edge of stability, where an ESN
typically achieves maximum computational capability. We verify that the
determination of the edge of stability provided by such RQA measures is more
accurate than the one offered by the maximal local Lyapunov exponent.
Therefore, we claim that RPs and RQA-based analyses can be used as valuable
tools to design an effective network given a specific problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07392</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07392</id><created>2016-01-27</created><updated>2016-02-28</updated><authors><author><keyname>Fangohr</keyname><forenames>Hans</forenames></author><author><keyname>Albert</keyname><forenames>Maximilian</forenames></author><author><keyname>Franchin</keyname><forenames>Matteo</forenames></author></authors><title>Nmag micromagnetic simulation tool - software engineering lessons
  learned</title><categories>cs.SE physics.comp-ph</categories><comments>7 pages, 5 figures, Software Engineering for Science, ICSE2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review design and development decisions and their impact for the open
source code Nmag from a software engineering in computational science point of
view. We summarise lessons learned and recommendations for future computational
science projects. Key lessons include that encapsulating the simulation
functionality in a library of a general purpose language, here Python, provides
great flexibility in using the software. The choice of Python for the top-level
user interface was very well received by users from the science and engineering
community. The from-source installation in which required external libraries
and dependencies are compiled from a tarball was remarkably robust. In places,
the code is a lot more ambitious than necessary, which introduces unnecessary
complexity and reduces main- tainability. Tests distributed with the package
are useful, although more unit tests and continuous integration would have been
desirable. The detailed documentation, together with a tutorial for the usage
of the system, was perceived as one of its main strengths by the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07399</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07399</id><created>2016-01-27</created><updated>2016-02-23</updated><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Network MIMO: Transmitters with no CSI Can Still be Very Useful</title><categories>cs.IT math.IT</categories><comments>Replacement of the conjecture by a proper outerbound</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Network MIMO channel under the so-called
Distributed Channel State Information at the Transmitters (D-CSIT)
configuration. In this setting, the precoder is designed in a distributed
manner at each Transmitter (TX) on the basis of the locally available
multi-user channel estimate. Although the use of simple Zero-Forcing (ZF) was
recently shown to reach the optimal DoF for a BC under noisy, yet centralized,
CSIT, it can turn very inefficient when faced with D-CSIT: The number of
Degrees-of-Freedom (DoF) achieved is then limited by the worst CSI accuracy
across TXs. To circumvent this effect, we develop a new robust transmission
scheme improving the DoF. A surprising result is uncovered by which, in the
regime of so-called weak CSIT, the proposed scheme is shown to be DoF-optimal
in that it achieves the so-called centralized outerbound consisting in the DoF
of a genie-aided centralized setting in which the CSI versions available at all
TXs are gathered. Building upon the insight obtained in the weak CSIT regime,
we develop a general D-CSIT robust scheme which improves over the DoF obtained
by conventional ZF approach in an arbitrary CSI quality regime and which is
shown to achieve the centralized outerbound in some practically relevant D-CSIT
configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07400</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07400</id><created>2016-01-27</created><authors><author><keyname>Angelou</keyname><forenames>Evangelos</forenames></author><author><keyname>Kaffes</keyname><forenames>Konstantinos</forenames></author><author><keyname>Asiki</keyname><forenames>Athanasia</forenames></author><author><keyname>Goumas</keyname><forenames>Georgios</forenames></author><author><keyname>Koziris</keyname><forenames>Nectarios</forenames></author></authors><title>Improving virtual host efficiency through resource and interference
  aware scheduling</title><categories>cs.DC</categories><comments>2nd International Workshop on Dynamic Resource Allocation and
  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016
  (arXiv:cs/1601.04675)</comments><report-no>DREAMCloud/2016/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Infrastructure-as-a-Service Clouds operate in a competitive
environment that caters to any user's requirements for computing resources. The
sharing of the various types of resources by diverse applications poses a
series of challenges in order to optimize resource utilization while avoiding
performance degradation caused by application interference. In this paper, we
present two scheduling methodologies enforcing consolidation techniques on
multicore physical machines. Our resource-aware and interference-aware
scheduling schemes aim at improving physical host efficiency while preserving
the application performance by taking into account host oversubscription and
the resulting workload interference. We validate our fully operational
framework through a set of real-life workloads representing a wide class of
modern cloud applications. The experimental results prove the efficiency of our
system in optimizing resource utilization and thus energy consumption even in
the presence of oversubscription. Both methodologies achieve significant
reductions of the CPU time consumed, reaching up to 50%, while at the same time
maintaining workload performance compared to widely used scheduling schemes
under a variety of representative cloud platform scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07403</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07403</id><created>2016-01-08</created><authors><author><keyname>Bulatov</keyname><forenames>Andrei A.</forenames></author></authors><title>Graphs of finite algebras, edges, and connectivity</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We refine and advance the study of the local structure of idempotent finite
algebras started in [A.Bulatov, The Graph of a Relational Structure and
Constraint Satisfaction Problems, LICS, 2004]. We introduce a graph-like
structure on an arbitrary finite idempotent algebra omitting type 1. We show
that this graph is connected, its edges can be classified into 3 types
corresponding to the local behavior (semilattice, majority, or affine) of
certain term operations, and that the structure of the algebra can be
`improved' without introducing type 1 by choosing an appropriate reduct of the
original algebra. Then we refine this structure demonstrating that the edges of
the graph of an algebra can be made `thin', that is, there are term operations
that behave very similar to semilattice, majority, or affine operations on
2-element subsets of the algebra. Finally, we prove certain connectivity
properties of the refined structures.
  This research is motivated by the study of the Constraint Satisfaction
Problem, although the problem itself does not really show up in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07409</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07409</id><created>2016-01-27</created><authors><author><keyname>Nguyen</keyname><forenames>Chi Mai</forenames></author><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author><author><keyname>Giorgini</keyname><forenames>Paolo</forenames></author><author><keyname>Mylopoulos</keyname><forenames>John</forenames></author></authors><title>Multi Object Reasoning with Constrained Goal Model</title><categories>cs.AI</categories><comments>41 pages (with appendices). Under journal submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal models have been widely used in Computer Science to represent software
requirements, business objectives, and design qualities. Existing goal modeling
techniques, however, have shown limitations of expressiveness and/or
tractability in coping with complex real-world problems. In this work, we
exploit advances in automated reasoning technologies, notably Satisfiability
and Optimization Modulo Theories (SMT/OMT), and we propose and formalize: (i)
an extended notion of goal model, namely Constrained Goal Model (CGM), which
makes explicit the notion of goal refinement and of domain assumption, allows
for expressing preferences between goals and refinements, and allows for
associating numerical attributes to goals and refinements for defining
constraints and multiple objective functions over goals, refinements and their
numerical attributes; (ii) a novel set of automated reasoning functionalities
over CGMs, allowing for automatically generating suitable refinements of input
CGMs, under user-specified assumptions and constraints, that also maximize
preferences and optimize given objective functions. We have implemented these
modeling and reasoning functionalities in a tool, named CGM-Tool, using the OMT
solver OptiMathSAT as automated reasoning backend. An empirical evaluation on
large CGMs supports the claim that our proposal scales well for goal models
capturing real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07414</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07414</id><created>2016-01-27</created><authors><author><keyname>Fournier</keyname><forenames>Ga&#xeb;tan</forenames></author><author><keyname>Scarsini</keyname><forenames>Marco</forenames></author></authors><title>Hotelling Games on Networks: Existence and Efficiency of Equilibria</title><categories>cs.GT math.OC</categories><msc-class>Primary 91A43, secondary 91A06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Hotelling game where a finite number of retailers choose a
location, given that their potential consumers are distributed on a network.
Retailers do not compete on price but only on location, therefore each consumer
shops at the closest store. We show that when the number of retailers is large
enough, the game admits a pure Nash equilibrium and we construct it. We then
compare the equilibrium cost borne by the consumers with the cost that could be
achieved if the retailers followed the dictate of a benevolent planner. We
perform this comparison in term of the induced price of anarchy, i.e., the
ratio of the worst equilibrium cost and the optimal cost, and the induced price
of stability, i.e., the ratio of the best equilibrium cost and the optimal
cost. We show that, asymptotically in the number of retailers, these ratios are
two and one, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07416</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07416</id><created>2016-01-27</created><authors><author><keyname>Brands</keyname><forenames>G.</forenames></author><author><keyname>Roellgen</keyname><forenames>C. B.</forenames></author><author><keyname>Vogel</keyname><forenames>K. U.</forenames></author></authors><title>QRKE: Resistance to Attacks using the Inverse of the Cosine
  Representation of Chebyshev Polynomials</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We've been able to show recently that Permutable Chebyshev polynomials (T
polynomials) defined over the field of real numbers can be used to create a
Diffie-Hellman-like key exchange algorithm and certificates. The cryptosystem
was theoretically proven to withstand attacks using quantum computers. We
additionally prove that attacks based on the inverse of the cosine
representation of T polynomials fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07417</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07417</id><created>2016-01-27</created><authors><author><keyname>Asoodeh</keyname><forenames>Shahab</forenames></author><author><keyname>Alajaji</keyname><forenames>Fady</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Privacy-Aware MMSE Estimation</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of the predictability of random variable $Y$ under
a privacy constraint dictated by random variable $X$, correlated with $Y$,
where both predictability and privacy are assessed in terms of the minimum
mean-squared error (MMSE). Given that $X$ and $Y$ are connected via a
binary-input symmetric-output (BISO) channel, we derive the \emph{optimal}
random mapping $P_{Z|Y}$ such that the MMSE of $Y$ given $Z$ is minimized while
the MMSE of $X$ given $Z$ is greater than $(1-\epsilon)\mathsf{var}(X)$ for a
given $\epsilon\geq 0$. We also consider the case where $(X,Y)$ are continuous
and $P_{Z|Y}$ is restricted to be an additive noise channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07420</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07420</id><created>2016-01-27</created><authors><author><keyname>Ursu</keyname><forenames>Roman</forenames></author><author><keyname>Latif</keyname><forenames>Khalid</forenames></author><author><keyname>Novo</keyname><forenames>David</forenames></author><author><keyname>Selva</keyname><forenames>Manuel</forenames></author><author><keyname>Gamatie</keyname><forenames>Abdoulaye</forenames></author><author><keyname>Sassatelli</keyname><forenames>Gilles</forenames></author><author><keyname>Khabi</keyname><forenames>Dmitry</forenames></author><author><keyname>Cheptsov</keyname><forenames>Alexey</forenames></author></authors><title>A Workflow for Fast Evaluation of Mapping Heuristics Targeting Cloud
  Infrastructures</title><categories>cs.DC</categories><comments>2nd International Workshop on Dynamic Resource Allocation and
  Management in Embedded, High Performance and Cloud Computing DREAMCloud 2016
  (arXiv:cs/1601.04675)</comments><report-no>DREAMCloud/2016/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource allocation is today an integral part of cloud infrastructures
management to efficiently exploit resources. Cloud infrastructures centers
generally use custom built heuristics to define the resource allocations. It is
an immediate requirement for the management tools of these centers to have a
fast yet reasonably accurate simulation and evaluation platform to define the
resource allocation for cloud applications. This work proposes a framework
allowing users to easily specify mappings for cloud applications described in
the AMALTHEA format used in the context of the DreamCloud European project and
to assess the quality for these mappings. The two quality metrics provided by
the framework are execution time and energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07424</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07424</id><created>2016-01-27</created><authors><author><keyname>Thomas</keyname><forenames>Yannis</forenames></author><author><keyname>Xylomenos</keyname><forenames>George</forenames></author><author><keyname>Tsilopoulos</keyname><forenames>Christos</forenames></author><author><keyname>Polyzos</keyname><forenames>George C.</forenames></author></authors><title>Object-oriented Packet Caching for ICN</title><categories>cs.NI</categories><comments>2nd International Conference on Information-Centric Networking, ACM,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most discussed features offered by Information-centric Networking
(ICN) architectures is the ability to support packet-level caching at every
node in the network. By individually naming each packet, ICN allows routers to
turn their queueing buffers into packet caches, thus exploiting the network's
existing storage resources. However, the performance of packet caching at
commodity routers is restricted by the small capacity of their SRAM, which
holds the index for the packets stored at the, slower, DRAM. We therefore
propose Object-oriented Packet Caching (OPC), a novel caching scheme that
overcomes the SRAM bottleneck, by combining object-level indexing in the SRAM
with packet-level storage in the DRAM. We implemented OPC and experimentally
evaluated it over various cache placement policies, showing that it can enhance
the impact of ICN packet-level caching, reducing both network and server load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07435</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07435</id><created>2016-01-27</created><updated>2016-02-08</updated><authors><author><keyname>Timm</keyname><forenames>Torsten</forenames></author></authors><title>Co-Occurrence Patterns in the Voynich Manuscript</title><categories>cs.CL cs.CR</categories><comments>19 pages; tables for sections of the VMS added; 'The Towneley plays'
  as example for English poetry added; revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Voynich Manuscript is a medieval book written in an unknown script. This
paper studies the distribution of similarly spelled words in the Voynich
Manuscript. It shows that the distribution of words within the manuscript is
not compatible with natural languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07444</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07444</id><created>2016-01-27</created><updated>2016-01-28</updated><authors><author><keyname>J&#xf6;rger</keyname><forenames>Thorbj&#xf6;rn</forenames></author><author><keyname>H&#xf6;flinger</keyname><forenames>Fabian</forenames></author><author><keyname>Gamm</keyname><forenames>Gerd Ulrich</forenames></author><author><keyname>Reindl</keyname><forenames>Leonhard M.</forenames></author></authors><title>Wireless distance estimation with low-power standard components in
  wireless sensor nodes</title><categories>cs.NI</categories><comments>8 pages, Proceedings of the 14th Mechatronics Forum International
  Conference, Mechatronics 2014</comments><acm-class>B.1.m</acm-class><journal-ref>Proceedings of the 14th Mechatronics Forum International
  Conference, Mechatronics 2014, 502-509</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of increasing use of moving wireless sensor nodes the interest
in localizing these nodes in their application environment is strongly rising.
For many applications, it is necessary to know the exact position of the nodes
in two- or three-dimensional space. Commonly used nodes use state-of-the-art
transceivers like the CC430 from Texas Instruments with integrated signal
strength measurement for this purpose. This has the disadvantage, that the
signal strength measurement is strongly dependent on the orientation of the
node through the antennas inhomogeneous radiation pattern as well as it has a
small accuracy on long ranges. Also, the nodes overall attenuation and output
power has to be calibrated and interference and multipath effects appear in
closed environments. Another possibility to trilaterate the position of a
sensor node is the time of flight measurement. This has the advantage, that the
position can also be estimated on long ranges, where signal strength methods
give only poor accuracy. In this paper we present an investigation of the
suitability of the state-of-the-art transceiver CC430 for a system based on
time of flight methods and give an overview of the optimal settings under
various circumstances for the in-field application. For this investigation, the
systematic and statistical errors in the time of flight measurements with the
CC430 have been investigated under a multitude of parameters. Our basic system
does not use any additional components but only the given standard hardware,
which can be found on the Texas Instruments evaluation board for a CC430. Thus,
it can be implemented on already existent sensor node networks by a simple
software upgrade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07446</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07446</id><created>2016-01-27</created><authors><author><keyname>Wozniak</keyname><forenames>Marcin</forenames></author><author><keyname>Polap</keyname><forenames>Dawid</forenames></author><author><keyname>Borowik</keyname><forenames>Grzegorz</forenames></author><author><keyname>Napoli</keyname><forenames>Christian</forenames></author></authors><title>A First Attempt to Cloud-Based User Verification in Distributed System</title><categories>cs.NE cs.AI cs.CR cs.DC</categories><comments>Final version published on: Asia-Pacific Conference on Computer Aided
  System Engineering (APCASE), pp. 226-231 (2015)</comments><msc-class>68T05, 68T10, 68T45, 68U10, 68W25, 68W99</msc-class><acm-class>I.2.6, I.2.10, I.4.8</acm-class><journal-ref>Asia-Pacific Conference on Computer Aided System Engineering
  (APCASE), pp. 226-231 (2015)</journal-ref><doi>10.1109/APCASE.2015.47</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the idea of client verification in distributed systems is
presented. The proposed solution presents a sample system where client
verification through cloud resources using input signature is discussed. For
different signatures the proposed method has been examined. Research results
are presented and discussed to show potential advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07457</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07457</id><created>2016-01-27</created><authors><author><keyname>Cattani</keyname><forenames>Marco</forenames></author><author><keyname>Protonotarios</keyname><forenames>Ioannis</forenames></author></authors><title>Gondola: a Parametric Robot Infrastructure for Repeatable Mobile
  Experiments</title><categories>cs.RO cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When deploying a testbed infrastructure for Wireless Sensor Networks (WSNs),
one of the most challenging feature is to provide repeatable mobility. Wheeled
robots, usually employed for such tasks, strive to adapt to the wide range of
environments where WSNs are deployed, from chaotic office spaces to potato
fields in the farmland. For this reson, these robot systems often require
expensive customization steps that, for example, adapt their localization and
navigation system. To avoid these issues, in this paper we present the design
of Gondola, a parametric robot infrastructure based on pulling wires, rather
than wheels, that avoids the most common problems of wheeled robot and easily
adapts to many WSN's scenarios. Different from wheeled robots, wich movements
are constrained on a 2-dimensional plane, Gondola can easily move in
3-dimensional spaces with no need of a complex localization system and an
accuracy that is comparable with off-the-shelf wheeled robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07460</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07460</id><created>2016-01-27</created><authors><author><keyname>Ghoshal</keyname><forenames>Asish</forenames></author><author><keyname>Honorio</keyname><forenames>Jean</forenames></author></authors><title>Information-theoretic lower bounds on learning the structure of Bayesian
  networks</title><categories>cs.LG cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the information theoretic limits of learning the
structure of Bayesian networks from data. We show that for Bayesian networks on
continuous as well as discrete random variables, there exists a
parameterization of the Bayesian network such that, the minimum number of
samples required to learn the &quot;true&quot; Bayesian network grows as
$\mathcal{O}(m)$, where $m$ is the number of variables in the network. Further,
for sparse Bayesian networks, where the number of parents of any variable in
the network is restricted to be at most $l$ for $l \ll m$, the minimum number
of samples required grows as $\mathcal{O}(l\log m)$. We discuss conditions
under which these limits are achieved. For Bayesian networks over continuous
variables, we obtain results for Gaussian regression and Gumbel Bayesian
networks. While for the discrete variables, we obtain results for Noisy-OR,
Conditional Probability Table (CPT) based Bayesian networks and Logistic
regression networks. Finally, as a byproduct, we also obtain lower bounds on
the sample complexity of feature selection in logistic regression and show that
the bounds are sharp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07468</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07468</id><created>2016-01-27</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Nam</keyname><forenames>Young-Han</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Massive MIMO Combining with Switches</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, accepted in IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is expected to play a central
role in future wireless systems. The deployment of large antenna arrays at the
base station and the mobile users offers multiplexing and beamforming gains
that boost system spectral efficiency. Unfortunately, the high cost and power
consumption of components like analog-to-digital converters makes assigning an
RF chain per antenna and applying typical fully digital precoding/combining
solutions difficult. In this paper, a novel architecture for massive MIMO
receivers, consisting of arrays of switches and constant (non-tunable) phase
shifters, is proposed. This architecture applies a quasi-coherent combining in
the RF domain to reduce the number of required RF chains. An algorithm that
designs the RF combining for this architecture is developed and analyzed.
Results show that the proposed massive MIMO combining model can achieve a
comparable performance to the fully-digital receiver architecture in
single-user and multi-user massive MIMO setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07471</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07471</id><created>2016-01-27</created><authors><author><keyname>Venkataraman</keyname><forenames>Vinay</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author></authors><title>Shape Distributions of Nonlinear Dynamical Systems for Video-based
  Inference</title><categories>cs.CV</categories><comments>IEEE Transactions on Pattern Analysis and Machine Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a shape-theoretic framework for dynamical analysis of
nonlinear dynamical systems which appear frequently in several video-based
inference tasks. Traditional approaches to dynamical modeling have included
linear and nonlinear methods with their respective drawbacks. A novel approach
we propose is the use of descriptors of the shape of the dynamical attractor as
a feature representation of nature of dynamics. The proposed framework has two
main advantages over traditional approaches: a) representation of the dynamical
system is derived directly from the observational data, without any inherent
assumptions, and b) the proposed features show stability under different
time-series lengths where traditional dynamical invariants fail. We illustrate
our idea using nonlinear dynamical models such as Lorenz and Rossler systems,
where our feature representations (shape distribution) support our hypothesis
that the local shape of the reconstructed phase space can be used as a
discriminative feature. Our experimental analyses on these models also indicate
that the proposed framework show stability for different time-series lengths,
which is useful when the available number of samples are small/variable. The
specific applications of interest in this paper are: 1) activity recognition
using motion capture and RGBD sensors, 2) activity quality assessment for
applications in stroke rehabilitation, and 3) dynamical scene classification.
We provide experimental validation through action and gesture recognition
experiments on motion capture and Kinect datasets. In all these scenarios, we
show experimental evidence of the favorable properties of the proposed
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07472</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07472</id><created>2016-01-27</created><authors><author><keyname>Cano</keyname><forenames>Guillaume</forenames></author><author><keyname>Cohen</keyname><forenames>Cyril</forenames></author><author><keyname>D&#xe9;n&#xe8;s</keyname><forenames>Maxime</forenames></author><author><keyname>M&#xf6;rtberg</keyname><forenames>Anders</forenames></author><author><keyname>Siles</keyname><forenames>Vincent</forenames></author></authors><title>Formalized linear algebra over Elementary Divisor Rings in Coq</title><categories>cs.LO math.RA</categories><acm-class>I.2.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Coq formalization of linear algebra over elementary
divisor rings, that is, rings where every matrix is equivalent to a matrix in
Smith normal form. The main results are the formalization that these rings
support essential operations of linear algebra, the classification theorem of
finitely presented modules over such rings and the uniqueness of the Smith
normal form up to multiplication by units. We present formally verified
algorithms computing this normal form on a variety of coefficient structures
including Euclidean domains and constructive principal ideal domains. We also
study different ways to extend B\'ezout domains in order to be able to compute
the Smith normal form of matrices. The extensions we consider are: adequacy
(i.e. the existence of a gdco operation), Krull dimension $\leq 1$ and
well-founded strict divisibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07473</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07473</id><created>2016-01-27</created><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author><author><keyname>Yang</keyname><forenames>Lin F.</forenames></author></authors><title>Streaming Space Complexity of Nearly All Functions of One Variable on
  Frequency Vectors</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in the theory of algorithms for data streams is to
determine which functions on a stream can be approximated in sublinear, and
especially sub-polynomial or poly-logarithmic, space. Given a function $g$, we
study the space complexity of approximating $\sum_{i=1}^n g(|f_i|)$, where
$f\in\mathbb{Z}^n$ is the frequency vector of a turnstile stream. This is a
generalization of the well-known frequency moments problem, and previous
results apply only when $g$ is monotonic or has a special functional form. Our
contribution is to give a condition such that, except for a narrow class of
functions $g$, there is a space-efficient approximation algorithm for the sum
if and only if $g$ satisfies the condition. The functions $g$ that we are able
to characterize include all convex, concave, monotonic, polynomial, and
trigonometric functions, among many others, and is the first such
characterization for non-monotonic functions. Thus, for nearly all functions of
one variable, we answer the open question from the celebrated paper of Alon,
Matias and Szegedy (1996).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07482</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07482</id><created>2016-01-27</created><authors><author><keyname>Merkel</keyname><forenames>Cory</forenames></author><author><keyname>Kudithipudi</keyname><forenames>Dhireesha</forenames></author></authors><title>Unsupervised Learning in Neuromemristive Systems</title><categories>cs.ET cs.LG stat.ML</categories><comments>To appear in the proceedings of the National Aerospace &amp; Electronics
  Conference &amp; Ohio Innovation Summit (NAECON-OIS'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromemristive systems (NMSs) currently represent the most promising
platform to achieve energy efficient neuro-inspired computation. However, since
the research field is less than a decade old, there are still countless
algorithms and design paradigms to be explored within these systems. One
particular domain that remains to be fully investigated within NMSs is
unsupervised learning. In this work, we explore the design of an NMS for
unsupervised clustering, which is a critical element of several machine
learning algorithms. Using a simple memristor crossbar architecture and
learning rule, we are able to achieve performance which is on par with MATLAB's
k-means clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07483</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07483</id><created>2016-01-27</created><updated>2016-01-28</updated><authors><author><keyname>Shekhar</keyname><forenames>Shashank</forenames></author><author><keyname>Khemani</keyname><forenames>Deepak</forenames></author></authors><title>Learning and Tuning Meta-heuristics in Plan Space Planning</title><categories>cs.AI</categories><comments>AAAI format, (10 pages), (1 figure), (4 tables)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, the planning community has observed that techniques for
learning heuristic functions have yielded improvements in performance. One
approach is to use offline learning to learn predictive models from existing
heuristics in a domain dependent manner. These learned models are deployed as
new heuristic functions. The learned models can in turn be tuned online using a
domain independent error correction approach to further enhance their
informativeness. The online tuning approach is domain independent but instance
specific, and contributes to improved performance for individual instances as
planning proceeds. Consequently it is more effective in larger problems.
  In this paper, we mention two approaches applicable in Partial Order Causal
Link (POCL) Planning that is also known as Plan Space Planning. First, we
endeavor to enhance the performance of a POCL planner by giving an algorithm
for supervised learning. Second, we then discuss an online error minimization
approach in POCL framework to minimize the step-error associated with the
offline learned models thus enhancing their informativeness. Our evaluation
shows that the learning approaches scale up the performance of the planner over
standard benchmarks, specially for larger problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07488</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07488</id><created>2016-01-27</created><updated>2016-01-28</updated><authors><author><keyname>Kelbert</keyname><forenames>Mark</forenames></author><author><keyname>Suhov</keyname><forenames>Yuri</forenames></author><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>On weighted Fisher information matrix properties</title><categories>cs.IT math.IT math.PR</categories><comments>The paper is wihdrawn as it needs improvement</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we review Fisher information matrices properties in weighted
version and discuss inequalities/bounds on it by using reduced weight
functions. In particular, an extended form of the Fisher information inequality
previously established in [6] is given. Further, along with generalized
De-Bruijn's identity, we provide new interpretation of the concavity for the
entropy power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07498</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07498</id><created>2016-01-27</created><authors><author><keyname>Makkuva</keyname><forenames>Ashok Vardhan</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>On additive-combinatorial affine inequalities for Shannon entropy and
  differential entropy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the question of to what extent do discrete entropy
inequalities for weighted sums of independent group-valued random variables
continue to hold for differential entropies. We show that all balanced (with
the sum of coefficients being zero) affine inequalities of Shannon entropy
extend to differential entropy; conversely, any affine inequality for
differential entropy must be balanced. In particular, this result shows that
recently proved differential entropy inequalities by Kontoyiannis and Madiman
\cite{KM14} can be deduced from their discrete counterparts due to Tao
\cite{Tao10} in a unified manner. Generalizations to certain abelian groups are
also obtained. Our proof relies on a result of R\'enyi \cite{Renyi59} which
relates the Shannon entropy of a finely discretized random variable to its
differential entropy and also helps in establishing the entropy of the sum of
quantized random variables is asymptotically equal to that of the quantized
sum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07505</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07505</id><created>2016-01-27</created><authors><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kotnis</keyname><forenames>Bhushan</forenames></author></authors><title>Game Theoretic Analysis of Tree Based Referrals for Crowd Sensing Social
  Systems with Passive Rewards</title><categories>cs.GT cs.MA cs.SI physics.soc-ph</categories><comments>6 pages, 3 figures. Presented in Social Networking Workshop at
  International Conference on Communication Systems and Networks (COMSNETS),
  Bangalore, India, January 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Participatory crowd sensing social systems rely on the participation of large
number of individuals. Since humans are strategic by nature, effective
incentive mechanisms are needed to encourage participation. A popular mechanism
to recruit individuals is through referrals and passive incentives such as
geometric incentive mechanisms used by the winning team in the 2009 DARPA
Network Challenge and in multi level marketing schemes. The effect of such
recruitment schemes on the effort put in by recruited strategic individuals is
not clear. This paper attempts to fill this gap. Given a referral tree and the
direct and passive reward mechanism, we formulate a network game where agents
compete for finishing crowd sensing tasks. We characterize the Nash equilibrium
efforts put in by the agents and derive closed form expressions for the same.
We discover free riding behavior among nodes who obtain large passive rewards.
This work has implications on designing effective recruitment mechanisms for
crowd sourced tasks. For example, usage of geometric incentive mechanisms to
recruit large number of individuals may not result in proportionate effort
because of free riding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07513</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07513</id><created>2016-01-27</created><authors><author><keyname>Tavangaran</keyname><forenames>Nima</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author></authors><title>Secret-Key Generation Using Compound Sources and One-Way Public
  Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical Secret-Key generation model, Common Randomness is generated
by two terminals based on the observation of correlated components of a common
source, while keeping it secret from a non-legitimate observer. It is assumed
that the statistics of the source are known to all participants. In this work,
the Secret-Key generation based on a compound source is studied where the
realization of the source statistic is unknown. The protocol should guarantee
the security and reliability of the generated Secret-Key, simultaneously for
all possible realizations of the compound source. A single-letter lower-bound
of the Secret-Key capacity for a finite compound source is derived as a
function of the public communication rate constraint. A multi-letter capacity
formula is further computed for a finite compound source for the case in which
the public communication is unconstrained. Finally a single-letter capacity
formula is derived for a degraded compound source with an arbitrary set of
source states and a finite set of marginal states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07518</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07518</id><created>2016-01-27</created><authors><author><keyname>Barvinok</keyname><forenames>Alexander</forenames></author></authors><title>Approximating permanents and hafnians of positive matrices</title><categories>math.CO cs.DS</categories><comments>15 pages</comments><msc-class>15A15, 68C25, 68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any 0 &lt; delta &lt; 1, we present a deterministic algorithm, which, given an
nxn real matrix A with entries between delta and 1 and an 0 &lt; epsilon &lt; 1
approximates the permanent of A within a relative error epsilon in n^{O(ln n
-ln epsilon)} time. A similar algorithm is constructed to approximate the
hafinian of a symmetric matrix with entries between delta and 1. In particular,
we prove that ln per A and ln haf A are approximated within error epsilon by a
polynomial of degree O(ln n - ln epsilon) in the entries of A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07532</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07532</id><created>2016-01-27</created><authors><author><keyname>Teney</keyname><forenames>Damien</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>Learning to Extract Motion from Videos in Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how to extract dense optical flow from videos with a
convolutional neural network (CNN). The proposed model constitutes a potential
building block for deeper architectures to allow using motion without resorting
to an external algorithm, \eg for recognition in videos. We derive our network
architecture from signal processing principles to provide desired invariances
to image contrast, phase and texture. We constrain weights within the network
to enforce strict rotation invariance and substantially reduce the number of
parameters to learn. We demonstrate end-to-end training on only 8 sequences of
the Middlebury dataset, orders of magnitude less than competing CNN-based
motion estimation methods, and obtain comparable performance to classical
methods on the Middlebury benchmark. Importantly, our method outputs a
distributed representation of motion that allows representing multiple,
transparent motions, and dynamic textures. Our contributions on network design
and rotation invariance offer insights nonspecific to motion estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07533</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07533</id><created>2016-01-27</created><authors><author><keyname>Wang</keyname><forenames>Yinong</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Burns</keyname><forenames>Joseph E.</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Osteoporotic and Neoplastic Compression Fracture Classification on
  Longitudinal CT</title><categories>cs.CV q-bio.TO</categories><comments>Contributed 4-Page Paper to be presented at the 2016 IEEE
  International Symposium on Biomedical Imaging (ISBI), April 13-16, 2016,
  Prague, Czech Republic</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Classification of vertebral compression fractures (VCF) having osteoporotic
or neoplastic origin is fundamental to the planning of treatment. We developed
a fracture classification system by acquiring quantitative morphologic and bone
density determinants of fracture progression through the use of automated
measurements from longitudinal studies. A total of 250 CT studies were acquired
for the task, each having previously identified VCFs with osteoporosis or
neoplasm. Thirty-six features or each identified VCF were computed and
classified using a committee of support vector machines. Ten-fold cross
validation on 695 identified fractured vertebrae showed classification
accuracies of 0.812, 0.665, and 0.820 for the measured, longitudinal, and
combined feature sets respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07539</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07539</id><created>2016-01-27</created><updated>2016-02-11</updated><authors><author><keyname>Wang</keyname><forenames>Xiaolan</forenames></author><author><keyname>Meliou</keyname><forenames>Alexandra</forenames></author><author><keyname>Wu</keyname><forenames>Eugene</forenames></author></authors><title>QFix: Diagnosing errors through query histories</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven applications rely on the correctness of their data to function
properly and effectively. Errors in data can be incredibly costly and
disruptive, leading to loss of revenue, incorrect conclusions, and misguided
policy decisions. While data cleaning tools can purge datasets of many errors
before the data is used, applications and users interacting with the data can
introduce new errors. Subsequent valid updates can obscure these errors and
propagate them through the dataset causing more discrepancies. Even when some
of these discrepancies are discovered, they are often corrected superficially,
on a case-by-case basis, further obscuring the true underlying cause, and
making detection of the remaining errors harder. In this paper, we propose
QFix, a framework that derives explanations and repairs for discrepancies in
relational data, by analyzing the effect of queries that operated on the data
and identifying potential mistakes in those queries. QFix is flexible, handling
scenarios where only a subset of the true discrepancies is known, and robust to
different types of update workloads. We make four important contributions: (a)
we formalize the problem of diagnosing the causes of data errors based on the
queries that operated on and introduced errors to a dataset; (b) we develop
exact methods for deriving diagnoses and fixes for identified errors using
state-of-the-art tools; (c) we present several optimization techniques that
improve our basic approach without compromising accuracy, and (d) we leverage a
tradeoff between accuracy and performance to scale diagnosis to large datasets
and query logs, while achieving near-optimal results. We demonstrate the
effectiveness of QFix through extensive evaluation over benchmark and synthetic
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07572</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07572</id><created>2016-01-27</created><authors><author><keyname>Eissa</keyname><forenames>M. M.</forenames></author><author><keyname>Elmesalawy</keyname><forenames>Mahmoud M.</forenames></author></authors><title>Analysis and Evaluation for the Performance of the Communication
  Infrastructure for Real Wide Area Monitoring Systems (WAMS) Based on 3G
  Technology</title><categories>cs.NI</categories><comments>IEEE International Conference on Smart Energy Grid Engineering
  (SEGE14), UOIT, Oshawa, ON, 11-13 August, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wide Area Monitoring Systems (WAMS) utilizing synchrophasor measurements is
considered one of the essential parts in smart grids that enable system
operators to monitor, operate, and control power systems in wide geographical
area. On the other hand, high-speed, reliable and scalable data communication
infrastructure is crucial in both construction and operation of WAMS. Universal
mobile Telecommunication System (UMTS), the 3G standard for mobile
communication networks, was developed to provide high speed data transmission
with reliable service performance for mobile users. Therefore, UMTS is
considered a promising solution for providing a communication infrastructure
for WAMS. 3G based EWAMS (Egyptian wide area Monitoring System) is designed and
implemented in Egypt through deployment a number of frequency disturbance
recorders (FDRs) devices on a live 220kV/500kV Egyptian grid in cooperation
with the Egyptian Electricity Transmission Company (EETC). The developed EWAMS
can gather information from 11 FDRs devices which are geographically dispersed
throughout the boundary of the Egyptian power grid and to a remote data
management center located at Helwan University. The communication performance
for the developed EWAMS in terms of communication time delay, throughput, and
percentage of wasted bandwidth are studied in this paper. The results showed
that the system can achieve successfully the communication requirements needed
by various wide area monitoring applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07576</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07576</id><created>2016-01-27</created><authors><author><keyname>Guo</keyname><forenames>Sheng</forenames></author><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Locally-Supervised Deep Hybrid Model for Scene Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNN) have recently achieved remarkable
successes in various image classification and understanding tasks. The deep
features obtained at the top fully-connected layer of the CNN (FC-features)
exhibit rich global semantic information and are extremely effective in image
classification. On the other hand, the convolutional features in the middle
layers of the CNN also contain meaningful local information, but are not fully
explored for image representation. In this paper, we propose a novel
Locally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and
explores the convolutional features for scene recognition. Firstly, we notice
that the convolutional features capture local objects and fine structures of
scene images, which yield important cues for discriminating ambiguous scenes,
whereas these features are significantly eliminated in the highly-compressed FC
representation. Secondly, we propose a new Local Convolutional Supervision
(LCS) layer to enhance the local structure of the image by directly propagating
the label information to the convolutional layers. Thirdly, we propose an
efficient Fisher Convolutional Vector (FCV) that successfully rescues the
orderless mid-level semantic information (e.g. objects and textures) of scene
image. The FCV encodes the large-sized convolutional maps into a fixed-length
mid-level representation, and is demonstrated to be strongly complementary to
the high-level FC-features. Finally, both the FCV and FC-features are
collaboratively employed in the LSDHM representation, which achieves
outstanding performance in our experiments. It obtains 83.75% and 67.56%
accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397
datasets, advancing the stat-of-the-art substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07586</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07586</id><created>2016-01-27</created><authors><author><keyname>Chen</keyname><forenames>Isabel</forenames></author><author><keyname>Benzi</keyname><forenames>Michele</forenames></author><author><keyname>Chang</keyname><forenames>Howard H.</forenames></author><author><keyname>Hertzberg</keyname><forenames>Vicki S.</forenames></author></authors><title>Dynamic communicability and epidemic spread: a case study on an
  empirical dynamic contact network</title><categories>physics.soc-ph cs.SI</categories><comments>30 pages, 14 figures, 11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a recently proposed temporal centrality measure applied to an
empirical network based on person-to-person contacts in an emergency department
of a busy urban hospital. We show that temporal centrality identifies a
distinct set of top-spreaders than centrality based on the time-aggregated
binarized contact matrix, so that taken together, the accuracy of capturing
top-spreaders improves significantly. However, with respect to predicting
epidemic outcome, the temporal measure does not necessarily outperform less
complex measures. Our results also show that other temporal markers such as
duration observed and the time of first appearance in the the network can be
used in a simple predictive model to generate predictions that capture the
trend of the observed data remarkably well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07593</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07593</id><created>2016-01-27</created><authors><author><keyname>Harremo&#xeb;s</keyname><forenames>Peter</forenames></author></authors><title>Sufficiency on the Stock Market</title><categories>cs.IT math.IT q-fin.PM</categories><msc-class>91B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that there are a number of relations between theoretical
finance theory and information theory. Some of these relations are exact and
some are approximate. In this paper we will explore some of these relations and
determine under which conditions the relations are exact. It turns out that
portfolio theory always leads to Bregman divergences. The Bregman divergence is
only proportional to information divergence in situations that are essentially
equal to the type of gambling studied by Kelly. This can be related an abstract
sufficiency condition.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="90000" completeListSize="102538">1122234|91001</resumptionToken>
</ListRecords>
</OAI-PMH>
