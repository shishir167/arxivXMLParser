<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:55:08Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|26001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1628</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1628</id><created>2011-11-07</created><authors><author><keyname>Al-Howaide</keyname><forenames>Ala'a Z.</forenames></author><author><keyname>Khaleel</keyname><forenames>Mohammed I.</forenames></author><author><keyname>Salhieh</keyname><forenames>Ayad M.</forenames></author></authors><title>Updatable Queue Protocol Based On TCP For Virtual Reality Environment</title><categories>cs.NI cs.PF</categories><comments>11 pages, 13 fugures, 13 reference</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.1, No.5, October 2011, Pages 35-46</journal-ref><doi>10.5121/ijcsea.2011.1504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variance in number and types of tasks required to be implemented within
Distributed Virtual Environments (DVE) highlights the needs for communication
protocols can achieve consistency. In addition, these applications have to
handle an increasing number of participants and deal with the difficult problem
of scalability. Moreover, the real-time requirements of these applications make
the scalability problem more difficult to solve. In this paper, we have
implemented Updatable Queue Abstraction protocol (UQA) on TCP (TCP-UQA) and
compared it with original TCP, UDP, and Updatable Queue Abstraction based on
UDP (UDP-UQA) protocols. Results showed that TCP-UQA was the best in queue
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1644</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1644</id><created>2011-11-07</created><authors><author><keyname>Aguilar</keyname><forenames>Carlos</forenames></author><author><keyname>Gaborit</keyname><forenames>Philippe</forenames></author><author><keyname>Schrek</keyname><forenames>Julien</forenames></author></authors><title>A new zero-knowledge code based identification scheme with reduced
  communication</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new 5-pass identification scheme with asymptotic
cheating probability 1/2 based on the syndrome decoding problem. Our protocol
is related to the Stern identification scheme but has a reduced communication
cost compared to previous code-based zero-knowledge schemes, moreover our
scheme permits to obtain a very low size of public key and secret key. The
contribution of this paper is twofold, first we propose a variation on the
Stern authentication scheme which permits to decrease asymptotically the
cheating probability to 1/2 rather than 2/3 (and very close to 1/2 in practice)
but with less communication. Our solution is based on deriving new challenges
from the secret key through cyclic shifts of the initial public key syndrome; a
new proof of soundness for this case is given Secondly we propose a new way to
deal with hashed commitments in zero-knowledge schemes based on Stern's scheme,
so that in terms of communication, on the average, only one hash value is sent
rather than two or three. Overall our new scheme has the good features of
having a zero-knowledge security proof based on well known hard problem of
coding theory, a small size of secret and public key (a few hundred bits), a
small calculation complexity, for an overall communication cost of 19kb for
authentication (for a $2^{16}$ security) and a signature of size of 93kb
(11.5kB) (for security $2^{80}$), an improvement of 40% compared to previous
schemes based on coding theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1647</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1647</id><created>2011-11-07</created><authors><author><keyname>Joshi</keyname><forenames>Sunil</forenames></author><author><keyname>Gupta</keyname><forenames>Deepak</forenames></author></authors><title>Throughput Performance of 2$\times$2 Mimo LTE Downlink in a Spatial
  Correlation Based Microcellular Channel for Wireless Broadband Networks</title><categories>cs.NI</categories><comments>11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Input Multiple Output (MIMO) technology is going to be a viable
alternative for future generation wireless broadband services in order to meet
the striving requirements for throughput and system robustness. In Long Term
Evolution (LTE), MIMO technologies have been broadly used to get better
downlink peak rate, cell coverage, as well as average cell throughput. In the
present paper a 2x2 MIMO is taken as baseline configuration for a LTE downlink
under a Microcellular propagation scenario considering a non physical
correlation based channel with Poor and rich scattering environment. The
throughput capacity of the downlink is obtained for poor and rich scattering
environments. Besides, two vital aspects of MIMO technique viz Spatial
Multiplexing (SM) and Transmit Diversity (TD) are investigated in order to see
their effect on throughput of the system. The effect of parameters like Speed
of mobile station, number of Multipath, Rician factor (K) on throughput of such
systems is reported and discussed. The investigations reported in this paper
helps in estimating the throughput capacity of LTE downlink under SM and TD
mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1648</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1648</id><created>2011-11-07</created><authors><author><keyname>Shukla</keyname><forenames>Archana</forenames></author></authors><title>Sentiment Analysis of Document Based on Annotation</title><categories>cs.IR cs.CL</categories><comments>14 pages, 14 figures, published in IJWEST Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a tool which tells the quality of document or its usefulness based
on annotations. Annotation may include comments, notes, observation,
highlights, underline, explanation, question or help etc. comments are used for
evaluative purpose while others are used for summarization or for expansion
also. Further these comments may be on another annotation. Such annotations are
referred as meta-annotation. All annotation may not get equal weightage. My
tool considered highlights, underline as well as comments to infer the
collective sentiment of annotators. Collective sentiments of annotators are
classified as positive, negative, objectivity. My tool computes collective
sentiment of annotations in two manners. It counts all the annotation present
on the documents as well as it also computes sentiment scores of all annotation
which includes comments to obtain the collective sentiments about the document
or to judge the quality of document. I demonstrate the use of tool on research
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1651</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1651</id><created>2011-11-07</created><updated>2012-09-26</updated><authors><author><keyname>Driemel</keyname><forenames>Anne</forenames></author><author><keyname>Haverkort</keyname><forenames>Herman J.</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo</forenames></author></authors><title>Flow Computations on Imprecise Terrains</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computation of the flow of water on imprecise terrains. We
consider two approaches to modeling flow on a terrain: one where water flows
across the surface of a polyhedral terrain in the direction of steepest
descent, and one where water only flows along the edges of a predefined graph,
for example a grid or a triangulation. In both cases each vertex has an
imprecise elevation, given by an interval of possible values, while its
(x,y)-coordinates are fixed. For the first model, we show that the problem of
deciding whether one vertex may be contained in the watershed of another is
NP-hard. In contrast, for the second model we give a simple O(n log n) time
algorithm to compute the minimal and the maximal watershed of a vertex, where n
is the number of edges of the graph. On a grid model, we can compute the same
in O(n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1665</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1665</id><created>2011-11-07</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Collette</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Fagerberg</keyname><forenames>Rolf</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author></authors><title>De-amortizing Binary Search Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general method for de-amortizing essentially any Binary Search
Tree (BST) algorithm. In particular, by transforming Splay Trees, our method
produces a BST that has the same asymptotic cost as Splay Trees on any access
sequence while performing each search in O(log n) worst case time. By
transforming Multi-Splay Trees, we obtain a BST that is O(log log n)
competitive, satisfies the scanning theorem, the static optimality theorem, the
static finger theorem, the working set theorem, and performs each search in
O(log n) worst case time. Moreover, we prove that if there is a dynamically
optimal BST algorithm, then there is a dynamically optimal BST algorithm that
answers every search in O(log n) worst case time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1666</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1666</id><created>2011-11-07</created><authors><author><keyname>Khlifi</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Performance Analysis of LS and LMMSE Channel Estimation Techniques for
  LTE Downlink Systems</title><categories>cs.NI</categories><comments>9 pages,8 figures</comments><acm-class>C.2.1; J.2</acm-class><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.3,
  No.5, October 2011, 141-149</journal-ref><doi>10.5121/ijwmn.2011.3511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this paper is to study the performance of two linear
channel estimators for LTE Downlink systems, the Least Square Error (LSE) and
the Linear Minimum Mean Square Error (LMMSE). As LTE is a MIMO-OFDM based
system, a cyclic prefix is inserted at the beginning of each transmitted OFDM
symbol in order to completely suppress both inter-carrier interference (ICI)
and inter-symbol interference (ISI). Usually, the cyclic prefix is equal to or
longer than the channel length but in some cases and because of some unforeseen
channel behaviour, the cyclic prefix can be shorter. Therefore, we propose to
study the performance of the two linear estimators under the effect of the
channel length. Computer simulations show that, in the case where the cyclic
prefix is equal to or longer than the channel length,LMMSE performs better than
LSE but at the cost of computational complexity.In the other case, LMMSE
continue to improve its performance only for low SNR values but it degrades for
high SNR values in which LS shows better performance for LTE Downlink systems.
MATLAB Monte-Carlo simulations are used to evaluate the performance of the
studied estimators in terms of Mean Square Error (MSE) and Bit Error Rate (BER)
for 2x2 LTE Downlink systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1672</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1672</id><created>2011-11-07</created><updated>2013-08-06</updated><authors><author><keyname>Fernandes</keyname><forenames>Cristina G.</forenames></author><author><keyname>Meira</keyname><forenames>Lu&#xed;s A. A.</forenames></author><author><keyname>Miyazawa</keyname><forenames>Fl&#xe1;vio K.</forenames></author><author><keyname>Pedrosa</keyname><forenames>Lehilton L. C.</forenames></author></authors><title>A Systematic Approach to Bound Factor-Revealing LPs and its Application
  to the Metric and Squared Metric Facility Location Problems</title><categories>cs.DS</categories><comments>Additional variants with powers of metrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A systematic technique to bound factor-revealing linear programs is
presented. We show how to derive a family of upper bound factor-revealing
programs (UPFRP), and show that each such program can be solved by a computer
to bound the approximation factor of an associated algorithm. Obtaining an
UPFRP is straightforward, and can be used as an alternative to analytical
proofs, that are usually very long and tedious. We apply this technique to the
Metric Facility Location Problem (MFLP) and to a generalization where the
distance function is a squared metric. We call this generalization the Squared
Metric Facility Location Problem (SMFLP) and prove that there is no
approximation factor better than 2.04, assuming P $\neq$ NP. Then, we analyze
the best known algorithms for the MFLP based on primal-dual and LP-rounding
techniques when they are applied to the SMFLP. We prove very tight bounds for
these algorithms, and show that the LP-rounding algorithm achieves a ratio of
2.04, and therefore has the best factor for the SMFLP. We use UPFRPs in the
dual-fitting analysis of the primal-dual algorithms for both the SMFLP and the
MFLP, improving some of the previous analysis for the MFLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1673</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1673</id><created>2011-11-07</created><authors><author><keyname>Clarke</keyname><forenames>Daoud</forenames></author></authors><title>Algebras over a field and semantics for context based reasoning</title><categories>cs.CL cs.LO</categories><comments>Draft chapter for a proposed Oxford University Press volume
  &quot;Compositional methods in Physics and Linguistics&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces context algebras and demonstrates their application to
combining logical and vector-based representations of meaning. Other approaches
to this problem attempt to reproduce aspects of logical semantics within new
frameworks. The approach we present here is different: We show how logical
semantics can be embedded within a vector space framework, and use this to
combine distributional semantics, in which the meanings of words are
represented as vectors, with logical semantics, in which the meaning of a
sentence is represented as a logical form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1684</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1684</id><created>2011-11-07</created><authors><author><keyname>Neogi</keyname><forenames>Biswarup</forenames></author><author><keyname>Ghosal</keyname><forenames>Soumya</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumyajit</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author><author><keyname>Tibarewala</keyname><forenames>D. N.</forenames></author></authors><title>Simulation Techniques and Prosthetic Approach Towards Biologically
  Efficient Artificial Sense Organs- An Overview</title><categories>cs.RO cs.SY</categories><comments>12 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An overview of the applications of control theory to prosthetic sense organs
including the senses of vision, taste and odor is being presented in this
paper. Simulation aspect nowadays has been the centre of research in the field
of prosthesis. There have been various successful applications of prosthetic
organs, in case of natural biological organs dis-functioning patients.
Simulation aspects and control modeling are indispensible for knowing system
performance, and to generate an original approach of artificial organs. This
overview focuses mainly on control techniques, by far a theoretical overview
and fusion of artificial sense organs trying to mimic the efficacies of
biologically active sensory organs. Keywords: virtual reality, prosthetic
vision, artificial
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1712</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1712</id><created>2011-11-07</created><authors><author><keyname>Kalise</keyname><forenames>Dante</forenames></author></authors><title>A study of a WENO-TVD finite volume scheme for the numerical simulation
  of atmospheric advective and convective phenomena</title><categories>math.NA cs.NA physics.ao-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a WENO-TVD scheme for the simulation of atmospheric phenomena. The
scheme considers a spatial discretization via a second-order TVD flux based
upon a flux-centered limiter approach, which makes use of high-order accurate
extrapolated values arising from a WENO reconstruction procedure. Time
discretization is performed with a third order RK-TVD scheme, and splitting is
used for the inclusion of source terms. We present a comprehensive performance
study of the method in atmospheric applications involving advective and
convective motion. We present a set of tests for space-dependent linear
advection, where we assess convergence and robustness with respect to the
parameters of the scheme. We apply the method to approximate the 2D Euler
equations in a series of tests for atmospheric convection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1713</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1713</id><created>2011-11-07</created><authors><author><keyname>Korman</keyname><forenames>Simon</forenames></author><author><keyname>Reichman</keyname><forenames>Daniel</forenames></author><author><keyname>Tsur</keyname><forenames>Gilad</forenames></author></authors><title>Tight Approximation of Image Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the {\em image matching} problem for two grayscale
$n \times n$ images, $M_1$ and $M_2$ (where pixel values range from 0 to 1).
Our goal is to find an affine transformation $T$ that maps pixels from $M_1$ to
pixels in $M_2$ so that the differences over pixels $p$ between $M_1(p)$ and
$M_2(T(p))$ is minimized. Our focus here is on sublinear algorithms that give
an approximate result for this problem, that is, we wish to perform this task
while querying as few pixels from both images as possible, and give a
transformation that comes close to minimizing the difference.
  We give an algorithm for the image matching problem that returns a
transformation $T$ which minimizes the sum of differences (normalized by $n^2$)
up to an additive error of $\epsilon$ and performs $\tilde{O}(n/\epsilon^2)$
queries. We give a corresponding lower bound of $\Omega(n)$ queries showing
that this is the best possible result in the general case (with respect to $n$
and up to low order terms).
  In addition, we give a significantly better algorithm for a natural family of
images, namely, smooth images. We consider an image smooth when the total
difference between neighboring pixels is O(n). For such images we provide an
approximation of the distance between the images to within an additive error of
$\epsilon$ using a number of queries depending polynomially on $1/\epsilon$ and
not on $n$. To do this we first consider the image matching problem for 2 and
3-dimensional {\em binary} images, and then reduce the grayscale image matching
problem to the 3-dimensional binary case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1738</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1738</id><created>2011-11-07</created><updated>2012-05-17</updated><authors><author><keyname>Lexa</keyname><forenames>Michael A.</forenames></author></authors><title>Quantization via Empirical Divergence Maximization</title><categories>cs.IT math.IT</categories><comments>26 single column, single spaced pages, 4 figures</comments><doi>10.1109/TSP.2012.2217136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical divergence maximization (EDM) refers to a recently proposed
strategy for estimating f-divergences and likelihood ratio functions. This
paper extends the idea to empirical vector quantization where one seeks to
empirically derive quantization rules that maximize the Kullback-Leibler
divergence between two statistical hypotheses. We analyze the estimator's error
convergence rate leveraging Tsybakov's margin condition and show that rates as
fast as 1/n are possible, where n equals the number of training samples. We
also show that the Flynn and Gray algorithm can be used to efficiently compute
EDM estimates and show that they can be efficiently and accurately represented
by recursive dyadic partitions. The EDM formulation have several advantages.
First, the formulation gives access to the tools and results of empirical
process theory that quantify the estimator's error convergence rate. Second,
the formulation provides a previously unknown derivation for the Flynn and Gray
algorithm. Third, the flexibility it affords allows one to avoid a small-cell
assumption common in other approaches. Finally, we illustrate the potential use
of the method through an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1750</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1750</id><created>2011-11-07</created><authors><author><keyname>Blelloch</keyname><forenames>Guy E.</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Tangwongsan</keyname><forenames>Kanat</forenames></author></authors><title>Near Linear-Work Parallel SDD Solvers, Low-Diameter Decomposition, and
  Low-Stretch Subgraphs</title><categories>cs.DS cs.DC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the design and analysis of a near linear-work parallel algorithm
for solving symmetric diagonally dominant (SDD) linear systems. On input of a
SDD $n$-by-$n$ matrix $A$ with $m$ non-zero entries and a vector $b$, our
algorithm computes a vector $\tilde{x}$ such that $\norm[A]{\tilde{x} - A^+b}
\leq \vareps \cdot \norm[A]{A^+b}$ in $O(m\log^{O(1)}{n}\log{\frac1\epsilon})$
work and $O(m^{1/3+\theta}\log \frac1\epsilon)$ depth for any fixed $\theta &gt;
0$.
  The algorithm relies on a parallel algorithm for generating low-stretch
spanning trees or spanning subgraphs. To this end, we first develop a parallel
decomposition algorithm that in polylogarithmic depth and $\otilde(|E|)$ work,
partitions a graph into components with polylogarithmic diameter such that only
a small fraction of the original edges are between the components. This can be
used to generate low-stretch spanning trees with average stretch
$O(n^{\alpha})$ in $O(n^{1+\alpha})$ work and $O(n^{\alpha})$ depth.
Alternatively, it can be used to generate spanning subgraphs with
polylogarithmic average stretch in $\otilde(|E|)$ work and polylogarithmic
depth. We apply this subgraph construction to derive a parallel linear system
solver. By using this solver in known applications, our results imply improved
parallel randomized algorithms for several problems, including single-source
shortest paths, maximum flow, minimum-cost flow, and approximate maximum flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1752</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1752</id><created>2011-11-07</created><authors><author><keyname>Lakehal</keyname><forenames>Abdelghni</forenames></author><author><keyname>Beqqali</keyname><forenames>Omar El</forenames></author></authors><title>New Method for 3D Shape Retrieval</title><categories>cs.CV</categories><comments>10 pages, 5 figures, publication paper</comments><doi>10.5121/ijcsit.2011.3508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent technological progress in acquisition, modeling and processing of
3D data leads to the proliferation of a large number of 3D objects databases.
Consequently, the techniques used for content based 3D retrieval has become
necessary. In this paper, we introduce a new method for 3D objects recognition
and retrieval by using a set of binary images CLI (Characteristic level
images). We propose a 3D indexing and search approach based on the similarity
between characteristic level images using Hu moments for it indexing. To
measure the similarity between 3D objects we compute the Hausdorff distance
between a vectors descriptor. The performance of this new approach is evaluated
at set of 3D object of well known database, is NTU (National Taiwan University)
database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1768</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1768</id><created>2011-11-07</created><authors><author><keyname>Ahamed</keyname><forenames>Syed V.</forenames></author><author><keyname>Rahman</keyname><forenames>Syed Shawon M.</forenames></author></authors><title>Architecture and Design of Medical Processor Units for Medical Networks</title><categories>cs.OH</categories><comments>17 pages</comments><journal-ref>International journal of Computer Networks &amp; Communications
  (IJCNC),Vol.2, No.6, November 2010, 13-29</journal-ref><doi>10.5121/ijcnc.2010.2602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces analogical and deductive methodologies for the design
medical processor units (MPUs). From the study of evolution of numerous earlier
processors, we derive the basis for the architecture of MPUs. These specialized
processors perform unique medical functions encoded as medical operational
codes (mopcs). From a pragmatic perspective, MPUs function very close to CPUs.
Both processors have unique operation codes that command the hardware to
perform a distinct chain of subprocesses upon operands and generate a specific
result unique to the opcode and the operand(s). In medical environments, MPU
decodes the mopcs and executes a series of medical sub-processes and sends out
secondary commands to the medical machine. Whereas operands in a typical
computer system are numerical and logical entities, the operands in medical
machine are objects such as such as patients, blood samples, tissues, operating
rooms, medical staff, medical bills, patient payments, etc. We follow the
functional overlap between the two processes and evolve the design of medical
computer systems and networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1769</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1769</id><created>2011-11-07</created><authors><author><keyname>Mohr</keyname><forenames>Stephen</forenames></author><author><keyname>Rahman</keyname><forenames>Syed Shawon</forenames></author></authors><title>IT Security Issues Within the Video Game Industry</title><categories>cs.CR cs.CY</categories><comments>16 pages</comments><doi>10.5121/ijcsit.2011.3501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IT security issues are an important aspect for each and every organization
within the video game industry. Within the video game industry alone, you might
not normally think of security risks being an issue. But as we can and have
seen in recent news, no company is immune to security risks no matter how big
or how small. While each of these organizations will never be exactly the same
as the next, there are common security issues that can and do affect each and
every video game company. In order to properly address those security issues,
one of the current leading video game companies was selected in order to
perform an initial security assessment. This security assessment provided a
starting point upon which specific goals and procedures were determined to help
mitigate those risks. The information contained within was initially completed
on the case study but has been generalized to allow the information to be
easily applied to any video game company.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1770</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1770</id><created>2011-11-07</created><authors><author><keyname>Hood</keyname><forenames>David</forenames></author><author><keyname>Rahman</keyname><forenames>Syed Shawon</forenames></author></authors><title>IT Security Plan for Flight Simulation Program</title><categories>cs.CR cs.NI</categories><comments>24 pages</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA),Vol.1, No.5, October 2011, 117-140</journal-ref><doi>10.5121/ijcsea.2011.1510</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information security is one of the most important aspects of technology, we
cannot protect the best interests of our organizations' assets (be that
personnel, data, or other resources), without ensuring that these assetsare
protected to the best of their ability. Within the Defense Department, this is
vital to the security of not just those assets but also the national security
of the United States. Compromise insecurity could lead severe consequences.
However, technology changes so rapidly that change has to be made to reflect
these changes with security in mind. This article outlines a growing
technological change (virtualization and cloud computing), and how to properly
address IT security concerns within an operating environment. By leveraging a
series of encrypted physical and virtual systems, andnetwork isolation
measures, this paper delivered a secured high performance computing environment
that efficiently utilized computing resources, reduced overall computer
processing costs, and ensures confidentiality, integrity, and availability of
systems within the operating environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1771</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1771</id><created>2011-11-07</created><authors><author><keyname>Schuett</keyname><forenames>Maria</forenames><affiliation>Shawon</affiliation></author><author><keyname>Syed</keyname><affiliation>Shawon</affiliation></author><author><keyname>Rahman</keyname><forenames>M.</forenames></author></authors><title>Information Security Synthesis in Online Universities</title><categories>cs.CR cs.CY cs.SI</categories><comments>20 pages</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.3, No.5, Sep 2011</journal-ref><doi>10.5121/ijnsa.2011.3501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information assurance is at the core of every initiative that an organization
executes. For online universities, a common and complex initiative is
maintaining user lifecycle and providing seamless access using one identity in
a large virtual infrastructure. To achieve information assurance the management
of user privileges affected by events in the user's identity lifecycle needs to
be the determining factor for access control. While the implementation of
identity and access management systems makes this initiative feasible, it is
the construction and maintenance of the infrastructure that makes it complex
and challenging. The objective of this paper1 is to describe the complexities,
propose a practical approach to building a foundation for consistent user
experience and realizing security synthesis in online universities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1772</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1772</id><created>2011-11-07</created><authors><author><keyname>Slaughter</keyname><forenames>Jason</forenames></author><author><keyname>Rahman</keyname><forenames>Syed Shawon M.</forenames></author></authors><title>Information Security Plan for Flight Simulator Applications</title><categories>cs.CR</categories><comments>15 pages; International Journal of Computer Science &amp; Information
  Technology (IJCSIT), Vol. 3, No 3, June 2011</comments><doi>10.5121/ijcsit.2011.3301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Department of Defense has a need for an identity management system that
uses two factor authentications to ensure that only the correct individuals get
access to their top secret flight simulator program. Currently the Department
of Defense does not have a web interface sign in system. We will be creating a
system that will allow them to access their programs, back office and
administrator functions remotely. A security plan outlining our security
architecture will be delivered prior to the final code roll out. The plan will
include responses to encryption used and the security architecture applied in
the final documentation. The code will be delivered in phases to work out any
issues that may occur during the implementation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1780</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1780</id><created>2011-11-07</created><updated>2013-01-08</updated><authors><author><keyname>Basu</keyname><forenames>Amitabh</forenames></author><author><keyname>Hildebrand</keyname><forenames>Robert</forenames></author><author><keyname>K&#xf6;ppe</keyname><forenames>Matthias</forenames></author></authors><title>The Triangle Closure is a Polyhedron</title><categories>math.OC cs.DM</categories><comments>39 pages; made self-contained by merging material from
  arXiv:1107.5068v1</comments><msc-class>90C11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, cutting planes derived from maximal lattice-free convex sets have
been studied intensively by the integer programming community. An important
question in this research area has been to decide whether the closures
associated with certain families of lattice-free sets are polyhedra. For a long
time, the only result known was the celebrated theorem of Cook, Kannan and
Schrijver who showed that the split closure is a polyhedron. Although some
fairly general results were obtained by Andersen, Louveaux and Weismantel [ An
analysis of mixed integer linear sets based on lattice point free convex sets,
Math. Oper. Res. 35 (2010), 233--256] and Averkov [On finitely generated
closures in the theory of cutting planes, Discrete Optimization 9 (2012), no.
4, 209--215], some basic questions have remained unresolved. For example,
maximal lattice-free triangles are the natural family to study beyond the
family of splits and it has been a standing open problem to decide whether the
triangle closure is a polyhedron. In this paper, we show that when the number
of integer variables $m=2$ the triangle closure is indeed a polyhedron and its
number of facets can be bounded by a polynomial in the size of the input data.
The techniques of this proof are also used to give a refinement of necessary
conditions for valid inequalities being facet-defining due to Cornu\'ejols and
Margot [On the facets of mixed integer programs with two integer variables and
two constraints, Mathematical Programming 120 (2009), 429--456] and obtain
polynomial complexity results about the mixed integer hull.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1784</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1784</id><created>2011-11-07</created><updated>2011-11-13</updated><authors><author><keyname>Ganti</keyname><forenames>Ravi</forenames></author><author><keyname>Gray</keyname><forenames>Alexander</forenames></author></authors><title>UPAL: Unbiased Pool Based Active Learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>20 pages, 4 figures, 2 tables, a few minor typos were corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of pool based active learning, and
provide an algorithm, called UPAL, that works by minimizing the unbiased
estimator of the risk of a hypothesis in a given hypothesis space. For the
space of linear classifiers and the squared loss we show that UPAL is
equivalent to an exponentially weighted average forecaster. Exploiting some
recent results regarding the spectra of random matrices allows us to establish
consistency of UPAL when the true hypothesis is a linear hypothesis. Empirical
comparison with an active learner implementation in Vowpal Wabbit, and a
previously proposed pool based active learner implementation show good
empirical performance and better scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1788</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1788</id><created>2011-11-07</created><authors><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Robust PCA as Bilinear Decomposition with Outlier-Sparsity
  Regularization</title><categories>stat.ML cs.IT math.IT</categories><comments>30 pages, submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2012.2204986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) is widely used for dimensionality
reduction, with well-documented merits in various applications involving
high-dimensional data, including computer vision, preference measurement, and
bioinformatics. In this context, the fresh look advocated here permeates
benefits from variable selection and compressive sampling, to robustify PCA
against outliers. A least-trimmed squares estimator of a low-rank bilinear
factor analysis model is shown closely related to that obtained from an
$\ell_0$-(pseudo)norm-regularized criterion encouraging sparsity in a matrix
explicitly modeling the outliers. This connection suggests robust PCA schemes
based on convex relaxation, which lead naturally to a family of robust
estimators encompassing Huber's optimal M-class as a special case. Outliers are
identified by tuning a regularization parameter, which amounts to controlling
sparsity of the outlier matrix along the whole robustification path of (group)
least-absolute shrinkage and selection operator (Lasso) solutions. Beyond its
neat ties to robust statistics, the developed outlier-aware PCA framework is
versatile to accommodate novel and scalable algorithms to: i) track the
low-rank signal subspace robustly, as new data are acquired in real time; and
ii) determine principal components robustly in (possibly) infinite-dimensional
feature spaces. Synthetic and real data tests corroborate the effectiveness of
the proposed robust PCA schemes, when used to identify aberrant responses in
personality assessment surveys, as well as unveil communities in social
networks, and intruders from video surveillance data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1797</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1797</id><created>2011-11-07</created><updated>2012-04-09</updated><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author></authors><title>Analysis of Thompson Sampling for the multi-armed bandit problem</title><categories>cs.LG cs.DS</categories><comments>This version corrects some minor errors, and reorganizes some content</comments><msc-class>68W40, 68Q25</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-armed bandit problem is a popular model for studying
exploration/exploitation trade-off in sequential decision problems. Many
algorithms are now available for this well-studied problem. One of the earliest
algorithms, given by W. R. Thompson, dates back to 1933. This algorithm,
referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic
idea is to choose an arm to play according to its probability of being the best
arm. Thompson Sampling algorithm has experimentally been shown to be close to
optimal. In addition, it is efficient to implement and exhibits several
desirable properties such as small regret for delayed feedback. However,
theoretical understanding of this algorithm was quite limited. In this paper,
for the first time, we show that Thompson Sampling algorithm achieves
logarithmic expected regret for the multi-armed bandit problem. More precisely,
for the two-armed bandit problem, the expected regret in time $T$ is
$O(\frac{\ln T}{\Delta} + \frac{1}{\Delta^3})$. And, for the $N$-armed bandit
problem, the expected regret in time $T$ is $O([(\sum_{i=2}^N
\frac{1}{\Delta_i^2})^2] \ln T)$. Our bounds are optimal but for the dependence
on $\Delta_i$ and the constant factors in big-Oh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1814</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1814</id><created>2011-11-08</created><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author></authors><title>On the Complexity of Connected (s, t)-Vertex Separator</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that minimum connected $(s,t)$-vertex separator ($(s,t)$-CVS) is
$\Omega(log^{2-\epsilon}n)$-hard for any $\epsilon &gt;0$ unless NP has
quasi-polynomial Las-Vegas algorithms. i.e., for any $\epsilon &gt;0$ and for some
$\delta &gt;0$, $(s,t)$-CVS is unlikely to have
$\delta.log^{2-\epsilon}n$-approximation algorithm. We show that $(s,t)$-CVS is
NP-complete on graphs with chordality at least 5 and present a polynomial-time
algorithm for $(s,t)$-CVS on bipartite chordality 4 graphs. We also present a
$\lceil\frac{c}{2}\rceil$-approximation algorithm for $(s,t)$-CVS on graphs
with chordality $c$. Finally, from the parameterized setting, we show that
$(s,t)$-CVS parameterized above the $(s,t)$-vertex connectivity is $W[2]$-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1817</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1817</id><created>2011-11-08</created><updated>2014-05-14</updated><authors><author><keyname>Karaman</keyname><forenames>Svebor</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Benois-Pineau</keyname><forenames>Jenny</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Dovgalecs</keyname><forenames>Vladislavs</forenames><affiliation>IMS</affiliation></author><author><keyname>M&#xe9;gret</keyname><forenames>R&#xe9;mi</forenames><affiliation>IMS</affiliation></author><author><keyname>Pinquier</keyname><forenames>Julien</forenames><affiliation>IRIT</affiliation></author><author><keyname>Andr&#xe9;-Obrecht</keyname><forenames>R&#xe9;gine</forenames><affiliation>IRIT</affiliation></author><author><keyname>Ga&#xeb;stel</keyname><forenames>Yann</forenames><affiliation>ISPED</affiliation></author><author><keyname>Dartigues</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Hierarchical Hidden Markov Model in Detecting Activities of Daily Living
  in Wearable Videos for Studies of Dementia</title><categories>cs.MM</categories><proxy>ccsd</proxy><journal-ref>Multimedia Tools and Applications, Volume 69, Issue 3, pp 743-771,
  June 2012</journal-ref><doi>10.1007/s11042-012-1117-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for indexing activities of daily living in
videos obtained from wearable cameras. In the context of dementia diagnosis by
doctors, the videos are recorded at patients' houses and later visualized by
the medical practitioners. The videos may last up to two hours, therefore a
tool for an efficient navigation in terms of activities of interest is crucial
for the doctors. The specific recording mode provides video data which are
really difficult, being a single sequence shot where strong motion and sharp
lighting changes often appear. Our work introduces an automatic motion based
segmentation of the video and a video structuring approach in terms of
activities by a hierarchical two-level Hidden Markov Model. We define our
description space over motion and visual characteristics of video and audio
channels. Experiments on real data obtained from the recording at home of
several patients show the difficulty of the task and the promising results of
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1823</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1823</id><created>2011-11-08</created><updated>2012-01-31</updated><authors><author><keyname>de Luca</keyname><forenames>Aldo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author></authors><title>A generalized palindromization map in free monoids</title><categories>cs.DM cs.FL math.CO</categories><comments>Final version, accepted for publication on Theoret. Comput. Sci</comments><msc-class>68R15</msc-class><acm-class>G.2.1; F.4.3</acm-class><journal-ref>Theoretical Computer Science 454 (2012) 109-128</journal-ref><doi>10.1016/j.tcs.2012.01.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The palindromization map $\psi$ in a free monoid $A^*$ was introduced in 1997
by the first author in the case of a binary alphabet $A$, and later extended by
other authors to arbitrary alphabets. Acting on infinite words, $\psi$
generates the class of standard episturmian words, including standard
Arnoux-Rauzy words. In this paper we generalize the palindromization map,
starting with a given code $X$ over $A$. The new map $\psi_X$ maps $X^*$ to the
set $PAL$ of palindromes of $A^*$. In this way some properties of $\psi$ are
lost and some are saved in a weak form. When $X$ has a finite deciphering delay
one can extend $\psi_X$ to $X^{\omega}$, generating a class of infinite words
much wider than standard episturmian words. For a finite and maximal code $X$
over $A$, we give a suitable generalization of standard Arnoux-Rauzy words,
called $X$-AR words. We prove that any $X$-AR word is a morphic image of a
standard Arnoux-Rauzy word and we determine some suitable linear lower and
upper bounds to its factor complexity.
  For any code $X$ we say that $\psi_X$ is conservative when
$\psi_X(X^{*})\subseteq X^{*}$. We study conservative maps $\psi_X$ and
conditions on $X$ assuring that $\psi_X$ is conservative. We also investigate
the special case of morphic-conservative maps $\psi_{X}$, i.e., maps such that
$\phi\circ \psi = \psi_X\circ \phi$ for an injective morphism $\phi$. Finally,
we generalize $\psi_X$ by replacing palindromic closure with
$\theta$-palindromic closure, where $\theta$ is any involutory antimorphism of
$A^*$. This yields an extension of the class of $\theta$-standard words
introduced by the authors in 2006.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1826</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1826</id><created>2011-11-08</created><authors><author><keyname>Prasad</keyname><forenames>R. Satya</forenames></author><author><keyname>Rao</keyname><forenames>Bandla Sreenivasa</forenames></author><author><keyname>Kantam</keyname><forenames>R. R. L.</forenames></author></authors><title>Monitoring Software Reliability using Statistical Process control: An
  MMLE approach</title><categories>cs.SE</categories><comments>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 3, No 5, Oct 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper consider an MMLE (Modified Maximum Likelihood Estimation) based
scheme to estimate software reliability using exponential distribution. The
MMLE is one of the generalized frameworks of software reliability models of Non
Homogeneous Poisson Processes (NHPPs). The MMLE gives analytical estimators
rather than an iterative approximation to estimate the parameters. In this
paper we proposed SPC (Statistical Process Control) Charts mechanism to
determine the software quality using inter failure times data. The Control
charts can be used to measure whether the software process is statistically
under control or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1827</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1827</id><created>2011-11-08</created><authors><author><keyname>Shariatpanahi</keyname><forenames>Seyed Pooya</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak Hossein</forenames></author><author><keyname>Alishahi</keyname><forenames>Kasra</forenames></author><author><keyname>Shah-Mansouri</keyname><forenames>Hamed</forenames></author></authors><title>One-Hop Throughput of Wireless Networks with Random Connections</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider one-hop communication in wireless networks with random
connections. In the random connection model, the channel powers between
different nodes are drawn from a common distribution in an i.i.d. manner. An
scheme achieving the throughput scaling of order $n^{1/3-\delta}$, for any
$\delta&gt;0$, is proposed, where $n$ is the number of nodes. Such achievable
throughput, along with the order $n^{1/3}$ upper bound derived by Cui et al.,
characterizes the throughput capacity of one-hop schemes for the class of
connection models with finite mean and variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1842</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1842</id><created>2011-11-08</created><authors><author><keyname>George</keyname><forenames>Laurent</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Bonnet</keyname><forenames>Laurent</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>L&#xe9;cuyer</keyname><forenames>Anatole</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Freeze the BCI until the user is ready: a pilot study of a BCI inhibitor</title><categories>cs.HC</categories><comments>5th International Brain-Computer Interface Workshop (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the concept of Brain-Computer Interface (BCI)
inhibitor, which is meant to standby the BCI until the user is ready, in order
to improve the overall performance and usability of the system. BCI inhibitor
can be defined as a system that monitors user's state and inhibits BCI
interaction until specific requirements (e.g. brain activity pattern, user
attention level) are met. In this pilot study, a hybrid BCI is designed and
composed of a classic synchronous BCI system based on motor imagery and a BCI
inhibitor. The BCI inhibitor initiates the control period of the BCI when
requirements in terms of brain activity are reached (i.e. stability in the beta
band). Preliminary results with four participants suggest that BCI inhibitor
system can improve BCI performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1854</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1854</id><created>2011-11-08</created><authors><author><keyname>Jansang</keyname><forenames>Aphirak</forenames></author><author><keyname>Phonphoem</keyname><forenames>Anan</forenames></author></authors><title>Framework Architecture for WLAN Testbed</title><categories>cs.NI</categories><comments>5 pages, 6 figures, 3rd Asian International Mobile Computing
  Conference (AMOC 2004), May 26-28, 2004, Thailand pp. 96-100 ISBN:
  974-537-487-3</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There has been a tremendous effort in improving wireless LAN for supporting
the demanding multimedia application. Many new protocols or ideas have been
proposed and proved by using a mathematical model or running a simulation
program. That is satisfactory but these proposed designs might not work in the
real world situation. Testbed is an option to alleviate this gap and present
the opportunity to see the real problem and ensure that the design works. A
framework architecture for building a testbed to test a new concept or design
is presented in this paper. The framework is designed in the modularity style
in such a way that can be easily exchanged or modified. A testbed based on the
framework that implements the polling based mechanism has been created and the
results have shown that the QoS of the real time traffic can be maintained in
the presence of the high non-real time traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1865</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1865</id><created>2011-11-08</created><authors><author><keyname>Neogy</keyname><forenames>Roshni</forenames></author><author><keyname>Chowdhury</keyname><forenames>Chandreyee</forenames></author><author><keyname>Neogy</keyname><forenames>Sarmistha</forenames></author></authors><title>Reliability of Mobile Agents for Reliable Service Discovery Protocol in
  MANET</title><categories>cs.NI</categories><journal-ref>IJWMN, Vol. 3, No.5, October 2011, pp. 229-243</journal-ref><doi>10.5121/ijwmn.2011.3518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently mobile agents are used to discover services in mobile ad-hoc network
(MANET) where agents travel through the network, collecting and sometimes
spreading the dynamically changing service information. But it is important to
investigate how reliable the agents are for this application as the
dependability issues(reliability and availability) of MANET are highly affected
by its dynamic nature.The complexity of underlying MANET makes it hard to
obtain the route reliability of the mobile agent systems (MAS); instead we
estimate it using Monte Carlo simulation. Thus an algorithm for estimating the
task route reliability of MAS (deployed for discovering services) is proposed,
that takes into account the effect of node mobility in MANET. That mobility
pattern of the nodes affects the MAS performance is also shown by considering
different mobility models. Multipath propagation effect of radio signal is
considered to decide link existence. Transient link errors are also considered.
Finally we propose a metric to calculate the reliability of service discovery
protocol and see how MAS performance affects the protocol reliability. The
experimental results show the robustness of the proposed algorithm. Here the
optimum value of network bandwidth (needed to support the agents) is calculated
for our application. However the reliability of MAS is highly dependent on link
failure probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1894</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1894</id><created>2011-11-08</created><authors><author><keyname>Shetty</keyname><forenames>Keerthi S.</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Cloud Based Application Development for Accessing Restaurant Information
  on Mobile Device using LBS</title><categories>cs.CY</categories><comments>11 pages, 10 figures</comments><journal-ref>International Journal of UbiComp (IJU), vol.2, no.4, 2011,
  pp.37-49</journal-ref><doi>10.5121/iju.2011.2404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past couple of years, the extent of the services provided on the
mobile devices has increased rapidly. A special class of service among them is
the Location Based Service(LBS) which depends on the geographical position of
the user to provide services to the end users. However, a mobile device is
still resource constrained, and some applications usually demand more resources
than a mobile device can a ord. To alleviate this, a mobile device should get
resources from an external source. One of such sources is cloud computing
platforms. We can predict that the mobile area will take on a boom with the
advent of this new concept. The aim of this paper is to exchange messages
between user and location service provider in mobile device accessing the cloud
by minimizing cost, data storage and processing power. Our main goal is to
provide dynamic location-based service and increase the information retrieve
accuracy especially on the limited mobile screen by accessing cloud
application. In this paper we present location based restaurant information
retrieval system and we have developed our application in Android.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1896</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1896</id><created>2011-11-08</created><updated>2012-03-01</updated><authors><author><keyname>Lehmann</keyname><forenames>Janette</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Ramasco</keyname><forenames>Jos&#xe9; J.</forenames></author><author><keyname>Cattuto</keyname><forenames>Ciro</forenames></author></authors><title>Dynamical Classes of Collective Attention in Twitter</title><categories>cs.SI cs.CY cs.HC physics.soc-ph</categories><comments>10 pages, 5 figures, 2 tables - To Appear in WWW'12</comments><journal-ref>WWW'12, 251 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro-blogging systems such as Twitter expose digital traces of social
discourse with an unprecedented degree of resolution of individual behaviors.
They offer an opportunity to investigate how a large-scale social system
responds to exogenous or endogenous stimuli, and to disentangle the temporal,
spatial and topical aspects of users' activity. Here we focus on spikes of
collective attention in Twitter, and specifically on peaks in the popularity of
hashtags. Users employ hashtags as a form of social annotation, to define a
shared context for a specific event, topic, or meme. We analyze a large-scale
record of Twitter activity and find that the evolution of hastag popularity
over time defines discrete classes of hashtags. We link these dynamical classes
to the events the hashtags represent and use text mining techniques to provide
a semantic characterization of the hastag classes. Moreover, we track the
propagation of hashtags in the Twitter social network and find that epidemic
spreading plays a minor role in hastag popularity, which is mostly driven by
exogenous factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1904</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1904</id><created>2011-11-08</created><authors><author><keyname>Ferry</keyname><forenames>Nicolas</forenames></author><author><keyname>Tigli</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Lavirotte</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Rey</keyname><forenames>Ga&#xeb;tan</forenames></author><author><keyname>Riveill</keyname><forenames>Michel</forenames></author></authors><title>Aspects of Assembly and Cascaded Aspects of Assembly: Logical and
  Temporal Properties</title><categories>cs.SE</categories><comments>14 pages, published in International Journal of Computer Science,
  Volume 8, issue 4, Jul 2011, ISSN 1694-0814</comments><journal-ref>International Journal of Computer Science (IJCSI), Volume 8, Issue
  4(1), JUL 2011, 1-15</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Highly dynamic computing environments, like ubiquitous and pervasive
computing environments, require frequent adaptation of applications. This has
to be done in a timely fashion, and the adaptation process must be as fast as
possible and mastered. Moreover the adaptation process has to ensure a
consistent result when finished whereas adaptations to be implemented cannot be
anticipated at design time. In this paper we present our mechanism for
self-adaptation based on the aspect oriented programming paradigm called Aspect
of Assembly (AAs). Using AAs: (1) the adaptations process is fast and its
duration is mastered; (2) adaptations' entities are independent of each other
thanks to the weaver logical merging mechanism; and (3) the high variability of
the software infrastructure can be managed using a mono or multi-cycle weaving
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1926</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1926</id><created>2011-11-08</created><updated>2012-01-02</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Mozaffari</keyname><forenames>Mohammad</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author></authors><title>Performance Analysis of Sequential Method for Handover in Cognitive
  Radio Systems</title><categories>cs.PF math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powerful spectrum handover schemes enable cognitive radios (CRs) to use
transmission opportunities in primary users' channels appropriately. In this
paper, we consider the cognitive access of primary channels by a secondary
user. We evaluate the average detection time and the maximum achievable average
throughput of the secondary user when the sequential method for hand-over
(SMHO) is used. We assume that a prior knowledge of the primary users' presence
and absence probabilities are available. When investigating the maximum
achievable throughput of the secondary user, we end into an optimization
problem, in which the optimum value of sensing time must be selected. In our
optimization problem, we take into account the spectrum hand over due to false
detection of the primary user. We also propose a weighted based hand-over
(WBHO) scheme in which the impacts of channels conditions and primary users'
presence probability are considered. This Spectrum handover scheme provides
higher average throughput for the SU than the SMHO method. The tradeoff between
the maximum achievable throughput and consumed energy is discussed, and finally
an energy efficient optimization formulation for finding a proper sensing time
is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1930</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1930</id><created>2011-11-08</created><authors><author><keyname>Boudhir</keyname><forenames>A. A.</forenames></author><author><keyname>Bouhorma</keyname><forenames>M.</forenames></author><author><keyname>Ahmed</keyname><forenames>M. Ben</forenames></author><author><keyname>Said</keyname><forenames>Elbrak</forenames></author></authors><title>The UWB Solution for Multimedia Traffic in Wireless Sensor Networks</title><categories>cs.NI cs.MM cs.OS</categories><comments>8 pages, 11 figures, IJWMN Journal</comments><acm-class>J.2.2</acm-class><journal-ref>International Journal of Wireless &amp; Mobile Networks October Issue
  2011</journal-ref><doi>10.5121/ijwmn</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several researches are focused on the QoS (Quality of Service) and Energy
consumption in wireless Multimedia Sensor Networks. Those research projects
invest in theory and practice in order to extend the spectrum of use of norms,
standards and technologies which are emerged in wireless communications. The
performance of these technologies is strongly related to domains of use and
limitations of their characteristics. In this paper, we give a comparison of
ZigBee technology, most widely used in sensor networks, and UWB (Ultra Wide
Band) which presents itself as competitor that present in these work better
results for audiovisual applications with medium-range and high throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1933</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1933</id><created>2011-11-08</created><authors><author><keyname>Bhattasali</keyname><forenames>Tapalina</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author></authors><title>Lightweight Hierarchical Model for HWSNET</title><categories>cs.NI</categories><comments>14 pages, 7 figures, AIRCC Journal</comments><doi>10.5121/ijassn.2011.1202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous wireless sensor networks (HWSNET) are more suitable for real
life applications as compared to the homogeneous counterpart. Security of
HWSNET becomes a very important issue with the rapid development of HWSNET.
Intrusion detection system is one of the major and efficient defensive methods
against attacks in HWSNET. Because of different constraints of sensor networks,
security solutions have to be designed with limited usage of computation and
resources. A particularly devastating attack is the sleep deprivation attack.
Here a malicious node forces legitimate nodes to waste their energy by
resisting the sensor nodes from going into low power sleep mode. The target of
this attack is to maximize the power consumption of the affected node, thereby
decreasing its battery life. Existing works on sleep deprivation attack have
mainly focused on mitigation using MAC based protocols, such as S-MAC (sensor
MAC), T-MAC (timeout MAC), B-MAC (Berkley MAC), G-MAC (gateway MAC). In this
article, a brief review of some of the recent intrusion detection systems in
wireless sensor network environment is presented. Finally, a framework of
cluster based layered countermeasure for Insomnia Detection has been proposed
for heterogeneous wireless sensor network (HWSNET) to efficiently detect sleep
deprivation attack. Simulation results on MATLAB exhibit the effectiveness of
the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1941</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1941</id><created>2011-11-08</created><authors><author><keyname>Fonou-Dombeu</keyname><forenames>Jean Vincent</forenames></author><author><keyname>Huisman</keyname><forenames>Magda</forenames></author></authors><title>Semantic-Driven e-Government: Application of Uschold and King Ontology
  Building Methodology for Semantic Ontology Models Development</title><categories>cs.AI</categories><comments>20 pages, 6 figures</comments><journal-ref>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.
  2, No. 4, October 2011, 1-20</journal-ref><doi>10.5121/ijwest.2011.2401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic government (e-government) has been one of the most active areas of
ontology development during the past six years. In e-government, ontologies are
being used to describe and specify e-government services (e-services) because
they enable easy composition, matching, mapping and merging of various
e-government services. More importantly, they also facilitate the semantic
integration and interoperability of e-government services. However, it is still
unclear in the current literature how an existing ontology building methodology
can be applied to develop semantic ontology models in a government service
domain. In this paper the Uschold and King ontology building methodology is
applied to develop semantic ontology models in a government service domain.
Firstly, the Uschold and King methodology is presented, discussed and applied
to build a government domain ontology. Secondly, the domain ontology is
evaluated for semantic consistency using its semi-formal representation in
Description Logic. Thirdly, an alignment of the domain ontology with the
Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) upper
level ontology is drawn to allow its wider visibility and facilitate its
integration with existing metadata standard. Finally, the domain ontology is
formally written in Web Ontology Language (OWL) to enable its automatic
processing by computers. The study aims to provide direction for the
application of existing ontology building methodologies in the Semantic Web
development processes of e-government domain specific ontology models; which
would enable their repeatability in other e-government projects and strengthen
the adoption of semantic technologies in e-government.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1947</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1947</id><created>2011-11-08</created><authors><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Srinivas</keyname><forenames>Umamahesh</forenames></author><author><keyname>Do</keyname><forenames>Thong T.</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Discriminative Local Sparse Representations for Robust Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key recent advance in face recognition models a test face image as a sparse
linear combination of a set of training face images. The resulting sparse
representations have been shown to possess robustness against a variety of
distortions like random pixel corruption, occlusion and disguise. This approach
however makes the restrictive (in many scenarios) assumption that test faces
must be perfectly aligned (or registered) to the training data prior to
classification. In this paper, we propose a simple yet robust local block-based
sparsity model, using adaptively-constructed dictionaries from local features
in the training data, to overcome this misalignment problem. Our approach is
inspired by human perception: we analyze a series of local discriminative
features and combine them to arrive at the final classification decision. We
propose a probabilistic graphical model framework to explicitly mine the
conditional dependencies between these distinct sparse local features. In
particular, we learn discriminative graphs on sparse representations obtained
from distinct local slices of a face. Conditional correlations between these
sparse features are first discovered (in the training phase), and subsequently
exploited to bring about significant improvements in recognition rates.
Experimental results obtained on benchmark face databases demonstrate the
effectiveness of the proposed algorithms in the presence of multiple
registration errors (such as translation, rotation, and scaling) as well as
under variations of pose and illumination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1958</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1958</id><created>2011-11-08</created><authors><author><keyname>Burbank</keyname><forenames>Noah</forenames></author><author><keyname>Dutta</keyname><forenames>Debojyoti</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Lee</keyname><forenames>David</forenames></author><author><keyname>Marschner</keyname><forenames>Eli</forenames></author><author><keyname>Shivakumar</keyname><forenames>Narayanan</forenames></author></authors><title>Widescope - A social platform for serious conversations on the Web</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are several web platforms that people use to interact and exchange
ideas, such as social networks like Facebook, Twitter, and Google+; Q&amp;A sites
like Quora and Yahoo! Answers; and myriad independent fora. However, there is a
scarcity of platforms that facilitate discussion of complex subjects where
people with divergent views can easily rationalize their points of view using a
shared knowledge base, and leverage it towards shared objectives, e.g. to
arrive at a mutually acceptable compromise.
  In this paper, as a first step, we present Widescope, a novel collaborative
web platform for catalyzing shared understanding of the US Federal and State
budget debates in order to help users reach data-driven consensus about the
complex issues involved. It aggregates disparate sources of financial data from
different budgets (i.e. from past, present, and proposed) and presents a
unified interface using interactive visualizations. It leverages distributed
collaboration to encourage exploration of ideas and debate. Users can propose
budgets ab-initio, support existing proposals, compare between different
budgets, and collaborate with others in real time.
  We hypothesize that such a platform can be useful in bringing people's
thoughts and opinions closer. Toward this, we present preliminary evidence from
a simple pilot experiment, using triadic voting (which we also formally analyze
to show that is better than hot-or-not voting), that 5 out of 6 groups of users
with divergent views (conservatives vs liberals) come to a consensus while
aiming to halve the deficit using Widescope. We believe that tools like
Widescope could have a positive impact on other complex, data-driven social
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1964</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1964</id><created>2011-11-08</created><authors><author><keyname>Hua</keyname><forenames>Sha</forenames></author><author><keyname>Liu</keyname><forenames>Pei</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra</forenames></author></authors><title>The Urge to Merge: When Cellular Service Providers Pool Capacity</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As cellular networks are turning into a platform for ubiquitous data access,
cellular operators are facing a severe data capacity crisis due to the
exponential growth of traffic generated by mobile users. In this work, we
investigate the benefits of sharing infrastructure and spectrum among two
cellular operators. Specifically, we provide a multi-cell analytical model
using stochastic geometry to identify the performance gain under different
sharing strategies, which gives tractable and accurate results. To validate the
performance using a realistic setting, we conduct extensive simulations for a
multi-cell OFDMA system using real base station locations. Both analytical and
simulation results show that even a simple cooperation strategy between two
similar operators, where they share spectrum and base stations, roughly
quadruples capacity as compared to the capacity of a single operator. This is
equivalent to doubling the capacity per customer, providing a strong incentive
for operators to cooperate, if not actually merge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1977</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1977</id><created>2011-11-08</created><updated>2013-03-13</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On Refined Versions of the Azuma-Hoeffding Inequality with Applications
  in Information Theory</title><categories>cs.IT math.IT</categories><comments>A survey paper with some original results that rely on some previous
  publications of the author. This has evolved to the joint paper with Maxim
  Raginsky in arXiv:1212.4663v3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a survey paper with some original results of the author on refined
versions of the Azuma-Hoeffding inequality with some examples that are related
to information theory. This work has evolved to the joint paper with Maxim
Raginsky in arXiv:1212.4663v3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1982</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1982</id><created>2011-11-08</created><updated>2012-07-15</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On the Concentration of the Crest Factor for OFDM Signals</title><categories>cs.IT math.IT</categories><comments>This work was presented at the Eighth International Symposium on
  Wireless Communication Systems (ISWCS 2011), Aachen, Germany, November 2011
  (pp. 784-788 in the proceedings)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies several concentration inequalities to prove concentration
results for the crest factor of OFDM signals. The considered approaches are, to
the best of our knowledge, new in the context of establishing concentration for
OFDM signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1992</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1992</id><created>2011-11-08</created><updated>2012-07-15</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On Concentration and Revisited Large Deviations Analysis of Binary
  Hypothesis Testing</title><categories>cs.IT math.IT</categories><comments>This paper (7 pages) is closely related to a presentation at the 2012
  Workshop on Information Theory and Applications, La Jolla, California, USA,
  February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper first introduces a refined version of the Azuma-Hoeffding
inequality for discrete-parameter martingales with uniformly bounded jumps. The
refined inequality is used to revisit the large deviations analysis of binary
hypothesis testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.1995</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.1995</id><created>2011-11-08</created><updated>2012-07-15</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Moderate Deviations Analysis of Binary Hypothesis Testing</title><categories>cs.IT math.IT</categories><comments>Presented at the 2012 IEEE International Symposium on Information
  Theory (ISIT 2012) at MIT, Boston, July 2012. It appears in the Proceedings
  of ISIT 2012 on pages 826-830</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is focused on the moderate-deviations analysis of binary
hypothesis testing. The analysis relies on a concentration inequality for
discrete-parameter martingales with bounded jumps, where this inequality forms
a refinement to the Azuma-Hoeffding inequality. Relations of the analysis to
the moderate deviations principle for i.i.d. random variables and to the
relative entropy are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2001</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2001</id><created>2011-11-08</created><authors><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Sundman</keyname><forenames>Dennis</forenames></author><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Projection-Based and Look Ahead Strategies for Atom Selection</title><categories>cs.SY</categories><comments>sparsity, compressive sensing; IEEE Trans on Signal Processing 2012</comments><doi>10.1109/TSP.2011.2173682</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we improve iterative greedy search algorithms in which atoms
are selected serially over iterations, i.e., one-by-one over iterations. For
serial atom selection, we devise two new schemes to select an atom from a set
of potential atoms in each iteration. The two new schemes lead to two new
algorithms. For both the algorithms, in each iteration, the set of potential
atoms is found using a standard matched filter. In case of the first scheme, we
propose an orthogonal projection strategy that selects an atom from the set of
potential atoms. Then, for the second scheme, we propose a look ahead strategy
such that the selection of an atom in the current iteration has an effect on
the future iterations. The use of look ahead strategy requires a higher
computational resource. To achieve a trade-off between performance and
complexity, we use the two new schemes in cascade and develop a third new
algorithm. Through experimental evaluations, we compare the proposed algorithms
with existing greedy search and convex relaxation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2018</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2018</id><created>2011-11-08</created><authors><author><keyname>Mitra</keyname><forenames>Bivas</forenames></author><author><keyname>Tabourier</keyname><forenames>Lionel</forenames></author><author><keyname>Roth</keyname><forenames>Camille</forenames></author></authors><title>Intrinsically Dynamic Network Communities</title><categories>cs.SI physics.soc-ph</categories><comments>27 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community finding algorithms for networks have recently been extended to
dynamic data. Most of these recent methods aim at exhibiting community
partitions from successive graph snapshots and thereafter connecting or
smoothing these partitions using clever time-dependent features and sampling
techniques. These approaches are nonetheless achieving longitudinal rather than
dynamic community detection. We assume that communities are fundamentally
defined by the repetition of interactions among a set of nodes over time.
According to this definition, analyzing the data by considering successive
snapshots induces a significant loss of information: we suggest that it blurs
essentially dynamic phenomena - such as communities based on repeated
inter-temporal interactions, nodes switching from a community to another across
time, or the possibility that a community survives while its members are being
integrally replaced over a longer time period. We propose a formalism which
aims at tackling this issue in the context of time-directed datasets (such as
citation networks), and present several illustrations on both empirical and
synthetic dynamic networks. We eventually introduce intrinsically dynamic
metrics to qualify temporal community structure and emphasize their possible
role as an estimator of the quality of the community detection - taking into
account the fact that various empirical contexts may call for distinct
`community' definitions and detection criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2077</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2077</id><created>2011-11-08</created><updated>2011-11-10</updated><authors><author><keyname>Noual</keyname><forenames>Mathilde</forenames></author><author><keyname>Sen&#xe9;</keyname><forenames>Sylvain</forenames></author></authors><title>Towards a theory of modelling with Boolean automata networks - I.
  Theorisation and observations</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although models are built on the basis of some observations of reality, the
concepts that derive theoretically from their definitions as well as from their
characteristics and properties are not necessarily direct consequences of these
initial observations. Indeed, many of them rather follow from chains of
theoretical inferences that are only based on the precise model definitions and
rely strongly, in addition, on some consequential working hypotheses. Thus, it
is important to address the question of which features of a model effectively
carry some modelling meaning and which only result from the task of formalising
observations of reality into a mathematical language. In this article, we
address this question with a theoretical point view that sets our discussion
strictly between the two stages of the modelling process that require knowledge
of real systems, that is, between the initial stage that chooses a global
theoretical framework to build the model and the final stage that exploits its
formal predictions by comparing them to the reality that the model was designed
to simulate. Taking Boolean automata networks as instances of models of systems
observed in reality, we analyse in this setting the remaining stages of the
modelling process and we show how the meaning of theoretical concepts can
subtly rely on formal choices such as definitions and hypotheses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2085</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2085</id><created>2011-11-08</created><updated>2013-01-17</updated><authors><author><keyname>de Castro</keyname><forenames>Alexandre</forenames></author></authors><title>Ag-dependent (in silico) approach implies a deterministic kinetics for
  homeostatic memory cell turnover</title><categories>q-bio.CB cs.NE</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verhulst-like mathematical modeling has been used to investigate several
complex biological issues, such as immune memory equilibrium and cell-mediated
immunity in mammals. The regulation mechanisms of both these processes are
still not sufficiently understood. In a recent paper, Choo et al. [J. Immunol.,
v. 185, pp. 3436-44, 2010], used an Ag-independent approach to quantitatively
analyze memory cell turnover from some empirical data, and concluded that
immune homeostasis behaves stochastically, rather than deterministically. In
the paper here presented, we use an in silico Ag-dependent approach to simulate
the process of antigenic mutation and study its implications for memory
dynamics. Our results have suggested a deterministic kinetics for homeostatic
equilibrium, what contradicts the Choo et al. findings. Accordingly, our
calculations are an indication that a more extensive empirical protocol for
studying the homeostatic turnover should be considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2092</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2092</id><created>2011-11-08</created><authors><author><keyname>Das</keyname><forenames>Sanmay</forenames></author><author><keyname>Lavoie</keyname><forenames>Allen</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Pushing Your Point of View: Behavioral Measures of Manipulation in
  Wikipedia</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a major source for information on virtually any topic, Wikipedia serves an
important role in public dissemination and consumption of knowledge. As a
result, it presents tremendous potential for people to promulgate their own
points of view; such efforts may be more subtle than typical vandalism. In this
paper, we introduce new behavioral metrics to quantify the level of controversy
associated with a particular user: a Controversy Score (C-Score) based on the
amount of attention the user focuses on controversial pages, and a Clustered
Controversy Score (CC-Score) that also takes into account topical clustering.
We show that both these measures are useful for identifying people who try to
&quot;push&quot; their points of view, by showing that they are good predictors of which
editors get blocked. The metrics can be used to triage potential POV pushers.
We apply this idea to a dataset of users who requested promotion to
administrator status and easily identify some editors who significantly changed
their behavior upon becoming administrators. At the same time, such behavior is
not rampant. Those who are promoted to administrator status tend to have more
stable behavior than comparable groups of prolific editors. This suggests that
the Adminship process works well, and that the Wikipedia community is not
overwhelmed by users who become administrators to promote their own points of
view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2097</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2097</id><created>2011-11-08</created><authors><author><keyname>G</keyname><forenames>Preetha K.</forenames></author></authors><title>A Novel Solution to the Short Range Bluetooth Communication</title><categories>cs.NI</categories><comments>10 pages; IJANS-2011 October Issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bluetooth is developed for short range communication. Bluetooth Devices are
normally having low power and low cost. This is a wireless communication
technology designed to connect phones, laptops and PDAs. The greater
availability of portable devices with Bluetooth connectivity imposes wireless
connection between enabled devices. On an average the range of Bluetooth
devices is about 10 meters. The basic limitation of the Bluetooth communication
is this range limitation. In this paper I have studied the limitations of
Bluetooth communication and consider range constraint as the major limitation.
I propose a new expanded Blue tooth network to overcome the range constraint of
Bluetooth device. This creates a network of Bluetooth enabled devices that will
include laptops, set top devices and also mobile phones. The main purpose of
this proposal is to establish a network will enable the users to communicate
outside the range without any range constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2098</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2098</id><created>2011-11-08</created><updated>2012-07-30</updated><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author></authors><title>The Half-Duplex AWGN Single-Relay Channel: Full Decoding or Partial
  Decoding?</title><categories>cs.IT math.IT</categories><comments>Authors' final version (to appear in IEEE Transactions on
  Communications)</comments><journal-ref>IEEE Transactions on Communications, Vol. 60, No. 11, pp.
  3156-3160, Nov. 2012</journal-ref><doi>10.1109/TCOMM.2012.081512.110166A</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares the partial-decode-forward and the
complete-decode-forward coding strategies for the half-duplex Gaussian
single-relay channel. We analytically show that the rate achievable by
partial-decode-forward outperforms that of the more straightforward
complete-decode-forward by at most 12.5%. Furthermore, in the following
asymptotic cases, the gap between the partial-decode-forward and the
complete-decode-forward rates diminishes: (i) when the relay is close to the
source, (ii) when the relay is close to the destination, and (iii) when the SNR
is low. In addition, when the SNR increases, this gap, when normalized to the
complete-decode-forward rate, also diminishes. Consequently, significant
performance improvements are not achieved by optimizing the fraction of data
the relay should decode and forward, over simply decoding the entire source
message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2102</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2102</id><created>2011-11-08</created><updated>2012-01-10</updated><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>The Capacity Region of the Restricted Two-Way Relay Channel with Any
  Deterministic Uplink</title><categories>cs.IT math.IT</categories><comments>author's final version (accepted and to appear in IEEE Communications
  Letters)</comments><journal-ref>IEEE Communications Letters, Vol. 16, No. 3, pp. 396-399, Mar.
  2012</journal-ref><doi>10.1109/LCOMM.2012.011312.112198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the two-way relay channel (TWRC) where two users
communicate via a relay. For the restricted TWRC where the uplink from the
users to the relay is any deterministic function and the downlink from the
relay to the users is any arbitrary channel, the capacity region is obtained.
The TWRC considered is restricted in the sense that each user can only transmit
a function of its message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2105</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2105</id><created>2011-11-09</created><updated>2013-10-21</updated><authors><author><keyname>Brazil</keyname><forenames>Marcus</forenames></author><author><keyname>Ras</keyname><forenames>Charl</forenames></author><author><keyname>Thomas</keyname><forenames>Doreen</forenames></author></authors><title>An exact algorithm for the bottleneck 2-connected $k$-Steiner network
  problem in $L_p$ planes</title><categories>math.MG cs.DS math.OC</categories><msc-class>68M10, 05C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first exact polynomial time algorithm for constructing optimal
geometric bottleneck 2-connected Steiner networks containing at most $k$
Steiner points, where $k&gt;2$ is a constant. Given a set of $n$ vertices embedded
in an $L_p$ plane, the objective of the problem is to find a 2-connected
network, spanning the given vertices and at most $k$ additional vertices, such
that the length of the longest edge is minimised. In contrast to the discrete
version of this problem the additional vertices may be located anywhere in the
plane. The problem is motivated by the modelling of relay-augmentation for the
optimisation of energy consumption in wireless ad hoc networks. Our algorithm
employs Voronoi diagrams and properties of block-cut-vertex decompositions of
graphs to find an optimal solution in $O(n^k\log^{\frac{5k}{2}}n)$ steps when
$1&lt;p&lt;\infty$ and in $O(n^2\log^{\frac{7k}{2}+1}n)$ steps when
$p\in\{1,\infty\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2108</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2108</id><created>2011-11-09</created><authors><author><keyname>Dai</keyname><forenames>Xiongping</forenames></author></authors><title>A criterion of simultaneously symmetrization and spectral finiteness for
  a finite set of real 2-by-2 matrices</title><categories>cs.SY cs.NA math.OC</categories><comments>5 pages</comments><msc-class>15B52, 65F15, 93D20, 37N30, 37N35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the simultaneously symmetrization and spectral
finiteness for a finite set of real 2-by-2 matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2109</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2109</id><created>2011-11-09</created><authors><author><keyname>Brazil</keyname><forenames>Marcus</forenames></author><author><keyname>Ras</keyname><forenames>Charl</forenames></author><author><keyname>Thomas</keyname><forenames>Doreen</forenames></author></authors><title>A Flow-dependent Quadratic Steiner Tree Problem in the Euclidean Plane</title><categories>math.MG cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a flow-dependent version of the quadratic Steiner tree problem
in the plane. An instance of the problem on a set of embedded sources and a
sink asks for a directed tree $T$ spanning these nodes and a bounded number of
Steiner points, such that $\displaystyle\sum_{e \in E(T)}f(e)|e|^2$ is a
minimum, where $f(e)$ is the flow on edge $e$. The edges are uncapacitated and
the flows are determined additively, i.e., the flow on an edge leaving a node
$u$ will be the sum of the flows on all edges entering $u$. Our motivation for
studying this problem is its utility as a model for relay augmentation of
wireless sensor networks. In these scenarios one seeks to optimise power
consumption -- which is predominantly due to communication and, in free space,
is proportional to the square of transmission distance -- in the network by
introducing additional relays. We prove several geometric and combinatorial
results on the structure of optimal and locally optimal solution-trees (under
various strategies for bounding the number of Steiner points) and describe a
geometric linear-time algorithm for constructing such trees with known
topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2111</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2111</id><created>2011-11-09</created><updated>2011-12-01</updated><authors><author><keyname>Liu</keyname><forenames>Song</forenames></author><author><keyname>Flach</keyname><forenames>Peter</forenames></author><author><keyname>Cristianini</keyname><forenames>Nello</forenames></author></authors><title>Generic Multiplicative Methods for Implementing Machine Learning
  Algorithms on MapReduce</title><categories>cs.DS cs.LG</categories><acm-class>D.1; F.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a generic model for multiplicative algorithms
which is suitable for the MapReduce parallel programming paradigm. We implement
three typical machine learning algorithms to demonstrate how similarity
comparison, gradient descent, power method and other classic learning
techniques fit this model well. Two versions of large-scale matrix
multiplication are discussed in this paper, and different methods are developed
for both cases with regard to their unique computational characteristics and
problem settings. In contrast to earlier research, we focus on fundamental
linear algebra techniques that establish a generic approach for a range of
algorithms, rather than specific ways of scaling up algorithms one at a time.
Experiments show promising results when evaluated on both speedup and accuracy.
Compared with a standard implementation with computational complexity $O(m^3)$
in the worst case, the large-scale matrix multiplication experiments prove our
design is considerably more efficient and maintains a good speedup as the
number of cores increases. Algorithm-specific experiments also produce
encouraging results on runtime performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2121</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2121</id><created>2011-11-09</created><updated>2012-02-04</updated><authors><author><keyname>Wang</keyname><forenames>Hui</forenames></author><author><keyname>Peng</keyname><forenames>Jie</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Kan</keyname><forenames>Haibin</forenames></author></authors><title>On $2k$-Variable Symmetric Boolean Functions with Maximum Algebraic
  Immunity $k$</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic immunity of Boolean function $f$ is defined as the minimal degree
of a nonzero $g$ such that $fg=0$ or $(f+1)g=0$. Given a positive even integer
$n$, it is found that the weight distribution of any $n$-variable symmetric
Boolean function with maximum algebraic immunity $\frac{n}{2}$ is determined by
the binary expansion of $n$. Based on the foregoing, all $n$-variable symmetric
Boolean functions with maximum algebraic immunity are constructed. The amount
is $(2\wt(n)+1)2^{\lfloor \log_2 n \rfloor}$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2125</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2125</id><created>2011-11-09</created><updated>2013-04-17</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Exploring Maps with Greedy Navigators</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures, 2 tables, the road network data set is available
  online at https://sites.google.com/site/lshlj82/road_data_2km.zip (readme
  file: https://sites.google.com/site/lshlj82/road_data_2km_readme.txt)</comments><journal-ref>Phys. Rev. Lett. 108, 128701 (2012)</journal-ref><doi>10.1103/PhysRevLett.108.128701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade of network research focusing on structural and
dynamical properties of networks, the role of network users has been more or
less underestimated from the bird's-eye view of global perspective. In this era
of global positioning system equipped smartphones, however, a user's ability to
access local geometric information and find efficient pathways on networks
plays a crucial role, rather than the globally optimal pathways. We present a
simple greedy spatial navigation strategy as a probe to explore spatial
networks. These greedy navigators use directional information in every move
they take, without being trapped in a dead end based on their memory about
previous routes. We suggest that the centralities measures have to be modified
to incorporate the navigators' behavior, and present the intriguing effect of
navigators' greediness where removing some edges may actually enhance the
routing efficiency, which is reminiscent of Braess's paradox. In addition,
using samples of road structures in large cities around the world, it is shown
that the navigability measure we define reflects unique structural properties,
which are not easy to predict from other topological characteristics. In this
respect, we believe that our routing scheme significantly moves the routing
problem on networks one step closer to reality, incorporating the inevitable
incompleteness of navigators' information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2127</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2127</id><created>2011-11-09</created><authors><author><keyname>Potechin</keyname><forenames>Aaron</forenames></author></authors><title>Monotone switching networks for directed connectivity are strictly more
  powerful than certain-knowledge switching networks</title><categories>cs.CC</categories><comments>15 pages, 4 figures</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  L (Logarithmic space) versus NL (Non-deterministic logarithmic space) is one
of the great open problems in computational complexity theory. In the paper
&quot;Bounds on monotone switching networks for directed connectivity&quot;, we separated
monotone analogues of L and NL using a model called the switching network
model. In particular, by considering inputs consisting of just a path and
isolated vertices, we proved that any monotone switching network solving
directed connectivity on $N$ vertices must have size at least
$N^{\Omega(\lg(N))}$ and this bound is tight. If we could show a similar result
for general switching networks solving directed connectivity, then this would
prove that $L \neq NL$. However, proving lower bounds for general switching
networks solving directed connectivity requires proving stronger lower bounds
on monotone switching networks for directed connectivity. To work towards this
goal, we investigated a different set of inputs which we believed to be hard
for monotone switching networks to solve and attempted to prove similar lower
size bounds. Instead, we found that this set of inputs is actually easy for
monotone switching networks for directed connectivity to solve, yet if we
restrict ourselves to certain-knowledge switching networks, which are a simple
and intuitive subclass of monotone switching networks for directed
connectivity, then these inputs are indeed hard to solve. In this paper, we
give this set of inputs, demonstrate a &quot;weird&quot; polynomially-sized monotone
switching network for directed connectivity which solves this set of inputs,
and prove that no polynomially-sized certain-knowledge switching network can
solve this set of inputs, thus proving that monotone switching networks for
directed connectivity are strictly more powerful than certain-knowledge
switching networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2160</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2160</id><created>2011-11-09</created><authors><author><keyname>Lande</keyname><forenames>Sudhir B.</forenames></author><author><keyname>Helonde</keyname><forenames>J. B.</forenames></author><author><keyname>Pande</keyname><forenames>Rajesh</forenames></author><author><keyname>Pathak</keyname><forenames>S. S.</forenames></author></authors><title>Adaptive Subcarrier and Bit Allocation for Downlink OFDMA System with
  Proportional Fairness</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the adaptive subcarrier and bit allocation algorithm
for OFDMA systems. To minimize overall transmitted power, we propose a novel
adaptive subcarrier and bit allocation algorithm based on channel state
information (CSI) and quality state information (QSI). A suboptimal approach
that separately performs subcarrier allocation and bit loading is proposed. It
is shown that a near optimal solution is obtained by the proposed algorithm
which has low complexity compared to that of other conventional algorithm. We
will study the problem of finding an optimal sub-carrier and power allocation
strategy for downlink communication to multiple users in an OFDMA based
wireless system. Assuming knowledge of the instantaneous channel gains for all
users, we propose a multiuser OFDMA subcarrier, and bit allocation algorithm to
minimize the total transmit power. This is done by assigning each user a set of
subcarriers and by determining the number of bits and the transmit power level
for each subcarrier. The objective is to minimize the total transmitted power
over the entire network to satisfy the application layer and physical layer. We
formulate this problem as a constrained optimization problem and present
centralized algorithms. The simulation results will show that our approach
results in an efficient assignment of subcarriers and transmitter power levels
in terms of the energy required for transmitting each bit of information, to
address this need, we also present a bit loading algorithm for allocating
subcarriers and bits in order to satisfy the rate requirements of the links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2195</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2195</id><created>2011-11-09</created><updated>2012-06-26</updated><authors><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author></authors><title>Representative sets and irrelevant vertices: New tools for kernelization</title><categories>cs.DS</categories><comments>30 pages. To appear in FOCS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of a polynomial kernel for Odd Cycle Transversal was a
notorious open problem in parameterized complexity. Recently, this was settled
by the present authors (Kratsch and Wahlstr\&quot;om, SODA 2012), with a randomized
polynomial kernel for the problem, using matroid theory to encode flow
questions over a set of terminals in size polynomial in the number of
terminals.
  In the current work we further establish the usefulness of matroid theory to
kernelization by showing applications of a result on representative sets due to
Lov\'asz (Combinatorial Surveys 1977) and Marx (TCS 2009). We show how
representative sets can be used to give a polynomial kernel for the elusive
Almost 2-SAT problem. We further apply the representative sets tool to the
problem of finding irrelevant vertices in graph cut problems, i.e., vertices
which can be made undeletable without affecting the status of the problem. This
gives the first significant progress towards a polynomial kernel for the
Multiway Cut problem; in particular, we get a kernel of O(k^{s+1}) vertices for
Multiway Cut instances with at most s terminals. Both these kernelization
results have significant spin-off effects, producing the first polynomial
kernels for a range of related problems.
  More generally, the irrelevant vertex results have implications for covering
min-cuts in graphs. For a directed graph G=(V,E) and sets S, T \subseteq V, let
r be the size of a minimum (S,T)-vertex cut (which may intersect S and T). We
can find a set Z \subseteq V of size O(|S|*|T|*r) which contains a minimum
(A,B)-vertex cut for every A \subseteq S, B \subseteq T. Similarly, for an
undirected graph G=(V,E), a set of terminals X \subseteq V, and a constant s,
we can find a set Z\subseteq V of size O(|X|^{s+1}) which contains a minimum
multiway cut for any partition of X into at most s pairwise disjoint subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2208</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2208</id><created>2011-11-09</created><authors><author><keyname>Tuli</keyname><forenames>Ruchi</forenames></author><author><keyname>Kumar</keyname><forenames>Parveen</forenames></author></authors><title>Minimum Process Coordinated Checkpointing Scheme for Ad Hoc Networks</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless mobile ad hoc network (MANET) architecture is one consisting of
a set of mobile hosts capable of communicating with each other without the
assistance of base stations. This has made possible creating a mobile
distributed computing environment and has also brought several new challenges
in distributed protocol design. In this paper, we study a very fundamental
problem, the fault tolerance problem, in a MANET environment and propose a
minimum process coordinated checkpointing scheme. Since potential problems of
this new environment are insufficient power and limited storage capacity, the
proposed scheme tries to reduce the amount of information saved for recovery.
The MANET structure used in our algorithm is hierarchical based. The scheme is
based for Cluster Based Routing Protocol (CBRP) which belongs to a class of
Hierarchical Reactive routing protocols. The protocol proposed by us is
nonblocking coordinated checkpointing algorithm suitable for ad hoc
environments. It produces a consistent set of checkpoints; the algorithm makes
sure that only minimum number of nodes in the cluster are required to take
checkpoints; it uses very few control messages. Performance analysis shows that
our algorithm outperforms the existing related works and is a novel idea in the
field. Firstly, we describe an organization of the cluster. Then we propose a
minimum process coordinated checkpointing scheme for cluster based ad hoc
routing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2211</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2211</id><created>2011-11-09</created><authors><author><keyname>Rezgui</keyname><forenames>Salah Eddine</forenames></author><author><keyname>Benalla</keyname><forenames>Hocine</forenames></author></authors><title>High Performance Controllers for Speed and Position Induction Motor
  Drive using New Reaching Law</title><categories>cs.RO</categories><comments>16 pages, 6 figures, 22 references</comments><journal-ref>International Journal of Instrumentation and Control Systems
  (IJICS) Vol.1, No.2, October 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper present new approach in robust indirect rotor field oriented
(IRFOC) induction motor (IM) control. The introduction of new exponential
reaching law (ERL) based sliding mode control (SMC) improve significantly the
performances compared to the conventional SMC which are well known susceptible
to the annoying chattering phenomenon, so, the elimination of the chattering is
achieved while simplicity and high performance speed and position tracking are
maintained. Simulation results are given to discuss the performances of the
proposed control method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2217</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2217</id><created>2011-11-09</created><updated>2012-05-10</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Moderate-Deviations of Lossy Source Coding for Discrete and Gaussian
  Sources</title><categories>cs.IT math.IT</categories><comments>To be presented at ISIT 2012 in Cambridge, MA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the moderate-deviations (MD) setting for lossy source coding of
stationary memoryless sources. More specifically, we derive fundamental
compression limits of source codes whose rates are $R(D) \pm \epsilon_n$, where
$R(D)$ is the rate-distortion function and $\epsilon_n$ is a sequence that
dominates $\sqrt{1/n}$. This MD setting is complementary to the
large-deviations and central limit settings and was studied by Altug and Wagner
for the channel coding setting. We show, for finite alphabet and Gaussian
sources, that as in the central limit-type results, the so-called dispersion
for lossy source coding plays a fundamental role in the MD setting for the
lossy source coding problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2221</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2221</id><created>2011-11-09</created><authors><author><keyname>Dong</keyname><forenames>Weishan</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Tino</keyname><forenames>Peter</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>Scaling Up Estimation of Distribution Algorithms For Continuous
  Optimization</title><categories>cs.NE cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Estimation of Distribution Algorithms (EDA) were proposed, many
attempts have been made to improve EDAs' performance in the context of global
optimization. So far, the studies or applications of multivariate probabilistic
model based continuous EDAs are still restricted to rather low dimensional
problems (smaller than 100D). Traditional EDAs have difficulties in solving
higher dimensional problems because of the curse of dimensionality and their
rapidly increasing computational cost. However, scaling up continuous EDAs for
higher dimensional optimization is still necessary, which is supported by the
distinctive feature of EDAs: Because a probabilistic model is explicitly
estimated, from the learnt model one can discover useful properties or features
of the problem. Besides obtaining a good solution, understanding of the problem
structure can be of great benefit, especially for black box optimization. We
propose a novel EDA framework with Model Complexity Control (EDA-MCC) to scale
up EDAs. By using Weakly dependent variable Identification (WI) and Subspace
Modeling (SM), EDA-MCC shows significantly better performance than traditional
EDAs on high dimensional problems. Moreover, the computational cost and the
requirement of large population sizes can be reduced in EDA-MCC. In addition to
being able to find a good solution, EDA-MCC can also produce a useful problem
structure characterization. EDA-MCC is the first successful instance of
multivariate model based EDAs that can be effectively applied a general class
of up to 500D problems. It also outperforms some newly developed algorithms
designed specifically for large scale optimization. In order to understand the
strength and weakness of EDA-MCC, we have carried out extensive computational
studies of EDA-MCC. Our results have revealed when EDA-MCC is likely to
outperform others on what kind of benchmark functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2228</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2228</id><created>2011-11-09</created><authors><author><keyname>Pietracaprina</keyname><forenames>Andrea</forenames></author><author><keyname>Pucci</keyname><forenames>Geppino</forenames></author><author><keyname>Riondato</keyname><forenames>Matteo</forenames></author><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Space-Round Tradeoffs for MapReduce Computations</title><categories>cs.DS cs.DC</categories><journal-ref>Final version in Proc. of the 26th ACM international conference on
  Supercomputing, pages 235-244, 2012</journal-ref><doi>10.1145/2304576.2304607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores fundamental modeling and algorithmic issues arising in the
well-established MapReduce framework. First, we formally specify a
computational model for MapReduce which captures the functional flavor of the
paradigm by allowing for a flexible use of parallelism. Indeed, the model
diverges from a traditional processor-centric view by featuring parameters
which embody only global and local memory constraints, thus favoring a more
data-centric view. Second, we apply the model to the fundamental computation
task of matrix multiplication presenting upper and lower bounds for both dense
and sparse matrix multiplication, which highlight interesting tradeoffs between
space and round complexity. Finally, building on the matrix multiplication
results, we derive further space-round tradeoffs on matrix inversion and
matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2237</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2237</id><created>2011-11-06</created><authors><author><keyname>Legchekova</keyname><forenames>Elena</forenames></author><author><keyname>Titov</keyname><forenames>Oleg</forenames></author></authors><title>Choosing the best resource by method of mamdani</title><categories>cs.DC</categories><comments>Article in Russian</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A method for selecting the best service for the storage of information by
Mamdani.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2246</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2246</id><created>2011-11-09</created><authors><author><keyname>Malik</keyname><forenames>Salman</forenames></author><author><keyname>Jacquet</keyname><forenames>Philippe</forenames></author><author><keyname>Adjih</keyname><forenames>Cedric</forenames></author></authors><title>On the Throughput Capacity of Wireless Multi-hop Networks with ALOHA,
  Node Coloring and CSMA</title><categories>cs.NI</categories><comments>This work has been presented in the 4th IFIP Wireless-Days conference
  (WD 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We quantify the throughput capacity of wireless multi-hop networks with
several medium access schemes. We analyze pure ALOHA scheme where simultaneous
transmitters are dispatched according to a uniform Poisson distribution and
exclusion schemes where simultaneous transmitters are dispatched according to
an exclusion rule such as node coloring and carrier sense based schemes. We
consider both no-fading and standard Rayleigh fading channel models. Our
results show that, under no-fading, slotted ALOHA can achieve at least
one-third (or half under Rayleigh fading) of the throughput capacity of node
coloring scheme whereas carrier sense based scheme can achieve almost the same
throughput capacity as node coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2249</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2249</id><created>2011-10-31</created><authors><author><keyname>Xu</keyname><forenames>Lin</forenames></author><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>SATzilla: Portfolio-based Algorithm Selection for SAT</title><categories>cs.AI</categories><proxy>jair.org</proxy><journal-ref>Journal Of Artificial Intelligence Research, Volume 32, pages
  565-606, 2008</journal-ref><doi>10.1613/jair.2490</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been widely observed that there is no single &quot;dominant&quot; SAT solver;
instead, different solvers perform best on different instances. Rather than
following the traditional approach of choosing the best solver for a given
class of instances, we advocate making this decision online on a per-instance
basis. Building on previous work, we describe SATzilla, an automated approach
for constructing per-instance algorithm portfolios for SAT that use so-called
empirical hardness models to choose among their constituent solvers. This
approach takes as input a distribution of problem instances and a set of
component solvers, and constructs a portfolio optimizing a given objective
function (such as mean runtime, percent of instances solved, or score in a
competition). The excellent performance of SATzilla was independently verified
in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one
silver and one bronze medal. In this article, we go well beyond SATzilla07 by
making the portfolio construction scalable and completely automated, and
improving it by integrating local search solvers as candidate solvers, by
predicting performance score instead of runtime, and by using hierarchical
hardness models that take into account different types of SAT instances. We
demonstrate the effectiveness of these new techniques in extensive experimental
results on data sets including instances from the most recent SAT competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2251</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2251</id><created>2011-11-09</created><authors><author><keyname>Malik</keyname><forenames>Salman</forenames></author><author><keyname>Jacquet</keyname><forenames>Philippe</forenames></author></authors><title>On the Optimal Transmission Scheme to Maximize Local Capacity in
  Wireless Networks</title><categories>cs.NI</categories><comments>This work has been presented in the 4th IFIP Wireless-Days conference
  (WD 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal transmission scheme that maximizes the local capacity in
two-dimensional (2D) wireless networks. Local capacity is defined as the
average information rate received by a node randomly located in the network.
Using analysis based on analytical and numerical methods, we show that maximum
local capacity can be obtained if simultaneous emitters are positioned in a
grid pattern based on equilateral triangles. We also compare this maximum local
capacity with the local capacity of slotted ALOHA scheme and our results show
that slotted ALOHA can achieve at least half of the maximum local capacity in
wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2258</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2258</id><created>2011-11-09</created><authors><author><keyname>Neogi</keyname><forenames>Biswarup</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumyajit</forenames></author><author><keyname>Ghosal</keyname><forenames>Soumya</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author><author><keyname>Tibarewala</keyname><forenames>D. N.</forenames></author></authors><title>Design and Implementation of Prosthetic Arm using Gear Motor Control
  Technique with Appropriate Testing</title><categories>cs.RO cs.SY</categories><comments>5 Pages,13 Figures</comments><journal-ref>International Journal of Computer Applications in Engineering,
  Technology and Sciences(IJ-CA-ETS), Volume 3, Issue 1, Page 281-285, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any part of the human body replication procedure commences the prosthetic
control science. This paper highlights the hardware design technique of a
prosthetic arm with implementation of gear motor control aspect. The prosthetic
control arm movement has been demonstrated in this paper applying processor
programming and with the successful testing of the designed prosthetic model.
The architectural design of the prosthetic arm here has been replaced by
lighter material instead of heavy metal, as well as the traditional EMG
(electro myographic) signal has been replaced by the muscle strain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2259</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2259</id><created>2011-11-07</created><authors><author><keyname>Bandettini</keyname><forenames>Alberto</forenames></author><author><keyname>Luporini</keyname><forenames>Fabio</forenames></author><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>A Survey on Open Problems for Mobile Robots</title><categories>cs.RO cs.MA</categories><comments>28 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gathering mobile robots is a widely studied problem in robotic research. This
survey first introduces the related work, summarizing models and results. Then,
the focus shifts on the open problem of gathering fat robots. In this context,
&quot;fat&quot; means that the robot is not represented by a point in a bidimensional
space, but it has an extent. Moreover, it can be opaque in the sense that other
robots cannot &quot;see through&quot; it. All these issues lead to a redefinition of the
original problem and an extension of the CORDA model. For at most 4 robots an
algorithm is provided in the literature, but is gathering always possible for
n&gt;4 fat robots? Another open problem is considered: Boundary Patrolling by
mobile robots. A set of mobile robots with constraints only on speed and
visibility is working in a polygonal environment having boundary and possibly
obstacles. The robots have to perform a perpetual movement (possibly within the
environment) so that the maximum timespan in which a point of the boundary is
not being watched by any robot is minimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2262</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2262</id><created>2011-11-09</created><updated>2012-07-24</updated><authors><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Improved Bound for the Nystrom's Method and its Application to Kernel
  Classification</title><categories>cs.LG cs.NA</categories><doi>10.1109/TIT.2013.2271378</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop two approaches for analyzing the approximation error bound for the
Nystr\&quot;{o}m method, one based on the concentration inequality of integral
operator, and one based on the compressive sensing theory. We show that the
approximation error, measured in the spectral norm, can be improved from
$O(N/\sqrt{m})$ to $O(N/m^{1 - \rho})$ in the case of large eigengap, where $N$
is the total number of data points, $m$ is the number of sampled data points,
and $\rho \in (0, 1/2)$ is a positive constant that characterizes the eigengap.
When the eigenvalues of the kernel matrix follow a $p$-power law, our analysis
based on compressive sensing theory further improves the bound to $O(N/m^{p -
1})$ under an incoherence assumption, which explains why the Nystr\&quot;{o}m method
works well for kernel matrix with skewed eigenvalues. We present a kernel
classification approach based on the Nystr\&quot;{o}m method and derive its
generalization performance using the improved bound. We show that when the
eigenvalues of kernel matrix follow a $p$-power law, we can reduce the number
of support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p &gt;
1+\sqrt{2}$, without seriously sacrificing its generalization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2267</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2267</id><created>2011-11-09</created><authors><author><keyname>Kalise</keyname><forenames>Dante</forenames></author><author><keyname>Lie</keyname><forenames>Ivar</forenames></author></authors><title>Modelling and numerical approximation of a 2.5D set of equations for
  mesoscale atmospheric processes</title><categories>math.NA cs.NA physics.ao-ph</categories><doi>10.1016/j.jcp.2012.06.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The set of 3D inviscid primitive equations for the atmosphere is
dimensionally reduced by a Discontinuous Galerkin discretization in one
horizontal direction. The resulting model is a 2D system of balance laws where
with a source term depending on the layering procedure and the choice of
coupling fluxes, which is established in terms of upwind considerations. The
&quot;2.5D&quot; system is discretized via a WENO-TVD scheme based in a flux limiter
centered approach. We study four tests cases related to atmospheric phenomena
to analyze the physical validity of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2285</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2285</id><created>2011-11-09</created><updated>2012-01-11</updated><authors><author><keyname>Tembine</keyname><forenames>H.</forenames></author></authors><title>Large-scale games in large-scale systems</title><categories>cs.SY cs.GT math-ph math.DS math.MP math.OC</categories><comments>30 pages. Notes for the tutorial course on mean field stochastic
  games, March 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world problems modeled by stochastic games have huge state and/or
action spaces, leading to the well-known curse of dimensionality. The
complexity of the analysis of large-scale systems is dramatically reduced by
exploiting mean field limit and dynamical system viewpoints. Under regularity
assumptions and specific time-scaling techniques, the evolution of the mean
field limit can be expressed in terms of deterministic or stochastic equation
or inclusion (difference or differential). In this paper, we overview recent
advances of large-scale games in large-scale systems. We focus in particular on
population games, stochastic population games and mean field stochastic games.
Considering long-term payoffs, we characterize the mean field systems using
Bellman and Kolmogorov forward equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2301</identifier>
 <datestamp>2011-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2301</id><created>2011-11-09</created><authors><author><keyname>Augot</keyname><forenames>Daniel</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author><author><keyname>Barbier</keyname><forenames>Morgan</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author><author><keyname>Fontaine</keyname><forenames>Caroline</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>Ensuring message embedding in wet paper steganography</title><categories>cs.CR</categories><comments>IMACC 2011 (2011)</comments><proxy>ccsd</proxy><journal-ref>IMACC 2011 7089 (2011) 244-258</journal-ref><doi>10.1007/978-3-642-25516-8_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syndrome coding has been proposed by Crandall in 1998 as a method to
stealthily embed a message in a cover-medium through the use of bounded
decoding. In 2005, Fridrich et al. introduced wet paper codes to improve the
undetectability of the embedding by nabling the sender to lock some components
of the cover-data, according to the nature of the cover-medium and the message.
Unfortunately, almost all existing methods solving the bounded decoding
syndrome problem with or without locked components have a non-zero probability
to fail. In this paper, we introduce a randomized syndrome coding, which
guarantees the embedding success with probability one. We analyze the
parameters of this new scheme in the case of perfect codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2313</identifier>
 <datestamp>2012-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2313</id><created>2011-11-09</created><updated>2012-01-13</updated><authors><author><keyname>Delaplace</keyname><forenames>Franck</forenames></author><author><keyname>Klaudel</keyname><forenames>Hanna</forenames></author><author><keyname>Melliti</keyname><forenames>Tarek</forenames></author><author><keyname>Sen&#xe9;</keyname><forenames>Sylvain</forenames></author></authors><title>Modular organisation of interaction networks based on asymptotic
  dynamics</title><categories>cs.DM q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates questions related to the modularity in discrete
models of biological interaction networks. We develop a theoretical framework
based on the analysis of their asymptotic dynamics. More precisely, we exhibit
formal conditions under which agents of interaction networks can be grouped
into modules. As a main result, we show that the usual decomposition in
strongly connected components fulfils the conditions of being a modular
organisation. Furthermore, we point out that our framework enables a finer
analysis providing a decomposition in elementary modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2315</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2315</id><created>2011-11-09</created><authors><author><keyname>Malik</keyname><forenames>Salman</forenames></author><author><keyname>Jacquet</keyname><forenames>Philippe</forenames></author></authors><title>Optimizing Local Capacity of Wireless Ad Hoc Networks</title><categories>cs.NI</categories><comments>This work has been presented in the 4th IFIP Wireless and Mobile
  Networking conference (WMNC 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we evaluate local capacity of wireless ad hoc networks with
several medium access protocols and identify the most optimal protocol. We
define local capacity as the average information rate received by a receiver
randomly located in the network. We analyzed grid pattern protocols where
simultaneous transmitters are positioned in a regular grid pattern, pure ALOHA
protocols where simultaneous transmitters are dispatched according to a uniform
Poisson distribution and exclusion protocols where simultaneous transmitters
are dispatched according to an exclusion rule such as node coloring and carrier
sense protocols. Our analysis allows us to conjecture that local capacity is
optimal when simultaneous transmitters are positioned in a grid pattern based
on equilateral triangles and our results show that this optimal local capacity
is at most double the local capacity of simple ALOHA protocol. Our results also
show that node coloring and carrier sense protocols approach the optimal local
capacity by an almost negligible difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2384</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2384</id><created>2011-11-09</created><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author></authors><title>Complexity of Counting CSP with Complex Weights</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a complexity dichotomy theorem for the counting Constraint
Satisfaction Problem (#CSP in short) with complex weights. To this end, we give
three conditions for its tractability. Let F be any finite set of
complex-valued functions, then we prove that #CSP(F) is solvable in polynomial
time if all three conditions are satisfied; and is #P-hard otherwise.
  Our complexity dichotomy generalizes a long series of important results on
counting problems: (a) the problem of counting graph homomorphisms is the
special case when there is a single symmetric binary function in F; (b) the
problem of counting directed graph homomorphisms is the special case when there
is a single not-necessarily-symmetric binary function in F; and (c) the
standard form of #CSP is when all functions in F take values in {0,1}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2391</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2391</id><created>2011-11-09</created><authors><author><keyname>Vijayalakshmi</keyname><forenames>B.</forenames></author><author><keyname>Bharathi</keyname><forenames>V. Subbiah</forenames></author></authors><title>A Novel Approach to Texture classification using statistical feature</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture is an important spatial feature which plays a vital role in content
based image retrieval. The enormous growth of the internet and the wide use of
digital data have increased the need for both efficient image database creation
and retrieval procedure. This paper describes a new approach for texture
classification by combining statistical texture features of Local Binary
Pattern and Texture spectrum. Since most significant information of a texture
often appears in the high frequency channels, the features are extracted by the
computation of LBP and Texture Spectrum and Legendre Moments. Euclidean
distance is used for similarity measurement. The experimental result shows that
97.77% classification accuracy is obtained by the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2399</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2399</id><created>2011-11-10</created><authors><author><keyname>Nongmeikapam</keyname><forenames>Kishorjit</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Sivaji</forenames></author></authors><title>Genetic Algorithm (GA) in Feature Selection for CRF Based Manipuri
  Multiword Expression (MWE) Identification</title><categories>cs.CL cs.NE</categories><comments>14 pages, 6 figures, see
  http://airccse.org/journal/jcsit/1011csit05.pdf</comments><acm-class>I.2.7</acm-class><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 3, No 5, Oct 2011, pp 53-66</journal-ref><doi>10.5121/ijcsit.2011.3505</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the identification of Multiword Expressions (MWEs) in
Manipuri, a highly agglutinative Indian Language. Manipuri is listed in the
Eight Schedule of Indian Constitution. MWE plays an important role in the
applications of Natural Language Processing(NLP) like Machine Translation, Part
of Speech tagging, Information Retrieval, Question Answering etc. Feature
selection is an important factor in the recognition of Manipuri MWEs using
Conditional Random Field (CRF). The disadvantage of manual selection and
choosing of the appropriate features for running CRF motivates us to think of
Genetic Algorithm (GA). Using GA we are able to find the optimal features to
run the CRF. We have tried with fifty generations in feature selection along
with three fold cross validation as fitness function. This model demonstrated
the Recall (R) of 64.08%, Precision (P) of 86.84% and F-measure (F) of 73.74%,
showing an improvement over the CRF based Manipuri MWE identification without
GA application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2412</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2412</id><created>2011-11-10</created><authors><author><keyname>Dinesh</keyname><forenames>C.</forenames><affiliation>Mailam Engineering College</affiliation></author></authors><title>Secured Data Consistency and Storage Way in Untrusted Cloud using Server
  Management Algorithm</title><categories>cs.DC</categories><comments>6 pages,3 figures. I am the only author of this title and related
  information; International Journal of Computer Applications (0975 - 8887)
  Volume 31- No.6, October 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is very challenging part to keep safely all required data that are needed
in many applications for user in cloud. Storing our data in cloud may not be
fully trustworthy. Since client doesn't have copy of all stored data, he has to
depend on Cloud Service Provider. But dynamic data operations, Read-Solomon and
verification token construction methods don't tell us about total storage
capacity of server allocated space before and after the data addition in cloud.
So we have to introduce a new proposed system of efficient storage measurement
and space comparison algorithm with time management for measuring the total
allocated storage area before and after the data insertion in cloud. So by
using our proposed scheme, the value or weight of stored data before and after
is measured by client with specified time in cloud storage area with accuracy.
And here we also have proposed the multi-server restore point in server failure
condition. If there occurs any server failure, by using this scheme the data
can be recovered automatically in cloud server. Our proposed scheme efficiently
checks space for the in-outsourced data to maintain integrity. Here the TPA
necessarily doesn't have the delegation to audit user's data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2418</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2418</id><created>2011-11-10</created><authors><author><keyname>Dinesh</keyname><forenames>C.</forenames><affiliation>Mailam Engineering College</affiliation></author></authors><title>Data Integrity and Dynamic Storage Way in Cloud Computing</title><categories>cs.DC</categories><comments>1 figure,6 pages, i am the only author for this title and related
  information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is not an easy task to securely maintain all essential data where it has
the need in many applications for clients in cloud. To maintain our data in
cloud, it may not be fully trustworthy because client doesn't have copy of all
stored data. But any authors don't tell us data integrity through its user and
CSP level by comparison before and after the data update in cloud. So we have
to establish new proposed system for this using our data reading protocol
algorithm to check the integrity of data before and after the data insertion in
cloud. Here the security of data before and after is checked by client with the
help of CSP using our &quot;effective automatic data reading protocol from user as
well as cloud level into the cloud&quot; with truthfulness. Also we have proposed
the multi-server data comparison algorithm with the calculation of overall data
in each update before its outsourced level for server restore access point for
future data recovery from cloud data server. Our proposed scheme efficiently
checks integrity in efficient manner so that data integrity as well as security
can be maintained in all cases by considering drawbacks of existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2422</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2422</id><created>2011-11-10</created><authors><author><keyname>Karim</keyname><forenames>Mohammad Zaidul</forenames></author><author><keyname>Akter</keyname><forenames>Nargis</forenames></author></authors><title>Optimum Partition Parameter of Divide-and-Conquer Algorithm for Solving
  Closest-Pair Problem</title><categories>cs.CG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1010.5908</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 3, No 5, 2011, 211-219</journal-ref><doi>10.5121/ijcsit.2011.3519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Divide and Conquer is a well known algorithmic procedure for solving many
kinds of problem. In this procedure, the problem is partitioned into two parts
until the problem is trivially solvable. Finding the distance of the closest
pair is an interesting topic in computer science. With divide and conquer
algorithm we can solve closest pair problem. Here also the problem is
partitioned into two parts until the problem is trivially solvable. But it is
theoretically and practically observed that sometimes partitioning the problem
space into more than two parts can give better performances. In this paper, a
new proposal is given that dividing the problem space into (n) number of parts
can give better result while divide and conquer algorithm is used for solving
the closest pair of point's problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2425</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2425</id><created>2011-11-10</created><updated>2012-01-17</updated><authors><author><keyname>Meric</keyname><forenames>Hugo</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Arnal</keyname><forenames>Fabrice</forenames></author><author><keyname>Lesthievent</keyname><forenames>Guy</forenames></author><author><keyname>Boucheret</keyname><forenames>Marie-Laure</forenames></author></authors><title>Improving broadcast channel rate using hierarchical modulation</title><categories>cs.NI</categories><comments>5 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the design of a broadcast system where the aim is to maximise
the throughput. This task is usually challenging due to the channel
variability. Modern satellite communications systems such as DVB-SH and DVB-S2
mainly rely on time sharing strategy to optimize throughput. They consider
hierarchical modulation but only for unequal error protection or backward
compatibility purposes. We propose in this article to combine time sharing and
hierarchical modulation together and show how this scheme can improve the
performance in terms of available rate. We present the gain on a simple channel
modeling the broadcasting area of a satellite. Our work is applied to the
DVB-SH standard, which considers hierarchical modulation as an optional
feature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2430</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2430</id><created>2011-11-10</created><authors><author><keyname>Tebbi</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Attari</keyname><forenames>Mahmoud Ahmadian</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Achievable Rates for a Two-Relay Network with Relays-Transmitter
  Feedbacks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a relay network with two relays and two feedback links from the
relays to the sender. To obtain the achievability results, we use the
compress-and-forward and the decode-and-forward strategies to superimpose
facility and cooperation analogue to what proposed by Cover and El Gamal for a
relay channel. In addition to random binning, we use deterministic binning to
perform restricted decoding. We show how to use the feedback links for
cooperation between the sender and the relays to transmit the information which
is compressed in the sender and the relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2451</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2451</id><created>2011-11-10</created><updated>2014-09-13</updated><authors><author><keyname>&#xd6;z&#xe7;elikkale</keyname><forenames>Ay&#xe7;a</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author><author><keyname>Ozaktas</keyname><forenames>Haldun M.</forenames></author></authors><title>Unitary Precoding and Basis Dependency of MMSE Performance for Gaussian
  Erasure Channels</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the transmission of a Gaussian vector source over a
multi-dimensional Gaussian channel where a random or a fixed subset of the
channel outputs are erased. Within the setup where the only encoding operation
allowed is a linear unitary transformation on the source, we investigate the
MMSE performance, both in average, and also in terms of guarantees that hold
with high probability as a function of the system parameters. Under the
performance criterion of average MMSE, necessary conditions that should be
satisfied by the optimal unitary encoders are established and explicit
solutions for a class of settings are presented. For random sampling of signals
that have a low number of degrees of freedom, we present MMSE bounds that hold
with high probability. Our results illustrate how the spread of the eigenvalue
distribution and the unitary transformation contribute to these performance
guarantees. The performance of the discrete Fourier transform (DFT) is also
investigated. As a benchmark, we investigate the equidistant sampling of
circularly wide-sense stationary (c.w.s.s.) signals, and present the explicit
error expression that quantifies the effects of the sampling rate and the
eigenvalue distribution of the covariance matrix of the signal.
  These findings may be useful in understanding the geometric dependence of
signal uncertainty in a stochastic process. In particular, unlike information
theoretic measures such as entropy, we highlight the basis dependence of
uncertainty in a signal with another perspective. The unitary encoding space
restriction exhibits the most and least favorable signal bases for estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2456</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2456</id><created>2011-11-10</created><authors><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>Park</keyname><forenames>Jaeok</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Repeated Games With Intervention: Theory and Applications in
  Communications</title><categories>cs.IT cs.GT math.IT</categories><comments>42 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In communication systems where users share common resources, users' selfish
behavior usually results in suboptimal resource utilization. There have been
extensive works that model communication systems with selfish users as one-shot
games and propose incentive schemes to achieve Pareto optimal action profiles
as non-cooperative equilibria. However, in many communication systems, due to
strong negative externalities among users, the sets of feasible payoffs in
one-shot games are nonconvex. Thus, it is possible to expand the set of
feasible payoffs by having users choose convex combinations of different
payoffs. In this paper, we propose a repeated game model generalized by
intervention. First, we use repeated games to convexify the set of feasible
payoffs in one-shot games. Second, we combine conventional repeated games with
intervention, originally proposed for one-shot games, to achieve a larger set
of equilibrium payoffs and loosen requirements for users' patience to achieve
it. We study the problem of maximizing a welfare function defined on users'
equilibrium payoffs, subject to minimum payoff guarantees. Given the optimal
equilibrium payoff, we derive the minimum intervention capability required and
design corresponding equilibrium strategies. The proposed generalized repeated
game model applies to various communication systems, such as power control and
flow control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2470</identifier>
 <datestamp>2012-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2470</id><created>2011-11-10</created><updated>2012-08-07</updated><authors><author><keyname>Portillo</keyname><forenames>Ignacio Gomez</forenames></author></authors><title>Cooperation and its emergence in growing systems with cultural
  reproduction</title><categories>cs.GT physics.soc-ph</categories><comments>This paper has been withdrawn by the author due that exist other
  arXiv with the same paper with important corrections
  (http://arxiv.org/abs/1201.2197). Sorry, this has happened because of my
  inexperience in arXiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the emergence of cooperation in the framework of evolutionary game
theory. First we introduce the cooperation problem in a novel way that we
believe it have important consequences in how problem is addressed. Then we
present a minimal model for the emergence of cooperation in growing systems
with cultural reproduction where topological structure and the evolution of
strategies are decoupled instead a coevolution dynamic. We show that when
system grows, there exists cultural reproduction and a nonzero probability that
individuals take cooperation as first strategy; there are conditions to build
up a cooperative system with real topological structures for any natural
selection intensity. When the system is small cooperation is unstable but
become stable as soon as the system reaches an enough well defined topological
structure which size mainly depends on the intensity of natural selection. In
this way, we reduce the emergence of cooperation problem for systems with
cultural reproduction to justify a small initial cooperative structure, what we
call cooperative seed. Otherwise, given that the system grows principally as
cooperator whose cooperators inhabit the most linked parts of the system, the
condition required to cooperation prevails into the systems are drastically
reduced compared to those found in statics networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2477</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2477</id><created>2011-11-10</created><authors><author><keyname>Foucaud</keyname><forenames>Florent</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Laihonen</keyname><forenames>Tero</forenames><affiliation>IF</affiliation></author><author><keyname>Parreau</keyname><forenames>Aline</forenames><affiliation>IF</affiliation></author></authors><title>An improved lower bound for (1,&lt;=2)-identifying codes in the king grid</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><journal-ref>Advances in Mathematics of Communications 8(1):35-52, 2014</journal-ref><doi>10.3934/amc.2014.8.35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We call a subset $C$ of vertices of a graph $G$ a $(1,\leq \ell)$-identifying
code if for all subsets $X$ of vertices with size at most $\ell$, the sets
$\{c\in C |\exists u \in X, d(u,c)\leq 1\}$ are distinct. The concept of
identifying codes was introduced in 1998 by Karpovsky, Chakrabarty and Levitin.
Identifying codes have been studied in various grids. In particular, it has
been shown that there exists a $(1,\leq 2)$-identifying code in the king grid
with density 3/7 and that there are no such identifying codes with density
smaller than 5/12. Using a suitable frame and a discharging procedure, we
improve the lower bound by showing that any $(1,\leq 2)$-identifying code of
the king grid has density at least 47/111.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2480</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2480</id><created>2011-11-10</created><authors><author><keyname>Calvert</keyname><forenames>Wesley</forenames></author><author><keyname>Miller</keyname><forenames>Russell</forenames></author><author><keyname>Reimann</keyname><forenames>Jennifer Chubb</forenames></author></authors><title>The Distance Function on a Computable Graph</title><categories>math.LO cs.LO</categories><comments>submitted for publication 9 November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the techniques of computable model theory to the distance function
of a graph. This task leads us to adapt the definitions of several truth-table
reducibilities so that they apply to functions as well as to sets, and we prove
assorted theorems about the new reducibilities and about functions which have
nonincreasing computable approximations. Finally, we show that the spectrum of
the distance function can consist of an arbitrary single btt-degree which is
approximable from above, or of all such btt-degrees at once, or of the
bT-degrees of exactly those functions approximable from above in at most n
steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2503</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2503</id><created>2011-11-10</created><authors><author><keyname>Iliopoulos</keyname><forenames>D.</forenames></author><author><keyname>Adami</keyname><forenames>C.</forenames></author><author><keyname>Szor</keyname><forenames>P.</forenames></author></authors><title>Darwin inside the machines: Malware evolution and the consequences for
  computer security</title><categories>cs.CR cs.CY q-bio.PE</categories><comments>13 pages</comments><journal-ref>Proceedings of Virus Bulletin Conference 2008 (Ottawa), H. Martin
  ed., pp. 187-194</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in anti-malware technologies have steered the security
industry away from maintaining vast signature databases and into newer defence
technologies such as behaviour blocking, application whitelisting and others.
Most would agree that the reasoning behind this is to keep up with the arms
race established between malware writers and the security community almost
three decades ago. Still, malware writers have not as yet created new
paradigms. Indeed, malicious code development is still largely limited to code
pattern changes utilizing polymorphic and metamorphic engines, as well as
executable packer and wrapper technologies. Each new malware instance retains
the exact same core functionality as its ancestor and only alters the way it
looks. What if, instead, malware were able to change its function or behaviour
autonomously? What if, in the absence of human intervention, computer viruses
resembled biological viruses in their ability to adapt to new defence
technologies as soon as they came into effect? In this paper, we will provide
the theoretical proof behind malware implementation that closely models
Darwinian evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2514</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2514</id><created>2011-10-12</created><authors><author><keyname>Rahman</keyname><forenames>Muhammad Mahbubur</forenames></author><author><keyname>Alam</keyname><forenames>Arif Ul</forenames></author><author><keyname>Abdullah-Al-Mamun</keyname></author><author><keyname>Mursalin</keyname><forenames>Tamnun E</forenames></author></authors><title>A more appropriate Protein Classification using Data Mining</title><categories>cs.CE</categories><comments>11 pages, 15 figures, 7 tables. arXiv admin note: some text overlap
  with articles written by other authors,
  http://bioinformatics.oxfordjournals.org/content/21/15/3234.full ,
  http://www.oxfordjournals.org/nar/database/summary/616 ,
  http://www.jsbi.org/pdfs/journal1/GIW01/GIW01F14.pdf</comments><journal-ref>Journal of Theoretical and Applied Information Technology(JATIT),
  pp. 33-43, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in bioinformatics is a complex phenomenon as it overlaps two
knowledge domains, namely, biological and computer sciences. This paper has
tried to introduce an efficient data mining approach for classifying proteins
into some useful groups by representing them in hierarchy tree structure. There
are several techniques used to classify proteins but most of them had few
drawbacks on their grouping. Among them the most efficient grouping technique
is used by PSIMAP. Even though PSIMAP (Protein Structural Interactome Map)
technique was successful to incorporate most of the protein but it fails to
classify the scale free property proteins. Our technique overcomes this
drawback and successfully maps all the protein in different groups, including
the scale free property proteins failed to group by PSIMAP. Our approach
selects the six major attributes of protein: a) Structure comparison b)
Sequence Comparison c) Connectivity d) Cluster Index e) Interactivity f)
Taxonomic to group the protein from the databank by generating a hierarchal
tree structure. The proposed approach calculates the degree (probability) of
similarity of each protein newly entered in the system against of existing
proteins in the system by using probability theorem on each six properties of
proteins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2520</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2520</id><created>2011-11-10</created><authors><author><keyname>Feigenbaum</keyname><forenames>Joan</forenames></author><author><keyname>Johnson</keyname><forenames>Aaron</forenames></author><author><keyname>Syverson</keyname><forenames>Paul</forenames></author></authors><title>Probabilistic Analysis of Onion Routing in a Black-box Model</title><categories>cs.CR</categories><comments>Extended abstract appeared in Proceedings of the 2007 ACM Workshop on
  Privacy in Electronic Society (WPES 2007)</comments><acm-class>C.2.0; C.2.4; K.4.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform a probabilistic analysis of onion routing. The analysis is
presented in a black-box model of anonymous communication in the Universally
Composable framework that abstracts the essential properties of onion routing
in the presence of an active adversary that controls a portion of the network
and knows all a priori distributions on user choices of destination. Our
results quantify how much the adversary can gain in identifying users by
exploiting knowledge of their probabilistic behavior. In particular, we show
that, in the limit as the network gets large, a user u's anonymity is worst
either when the other users always choose the destination u is least likely to
visit or when the other users always choose the destination u chooses. This
worst-case anonymity with an adversary that controls a fraction b of the
routers is shown to be comparable to the best-case anonymity against an
adversary that controls a fraction \surdb.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2527</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2527</id><created>2011-11-10</created><authors><author><keyname>Sochi</keyname><forenames>Taha</forenames></author></authors><title>Testing the Connectivity of Networks</title><categories>cs.DS</categories><comments>21 pages, 7 figures</comments><doi>10.4304/jnw.9.2.239-243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we discuss general strategies and computer algorithms to test
the connectivity of unstructured networks which consist of a number of segments
connected through randomly distributed nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2530</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2530</id><created>2011-11-10</created><authors><author><keyname>Ramesh</keyname><forenames>C.</forenames></author><author><keyname>Rao</keyname><forenames>K. V. Chalapati</forenames></author><author><keyname>Govardhan</keyname><forenames>A.</forenames></author></authors><title>A semantically enriched web usage based recommendation model</title><categories>cs.DB</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 3, No 5, Oct 2011, 193-202</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of internet technologies, Web has become a huge
repository of information and keeps growing exponentially under no editorial
control. However the human capability to read, access and understand Web
content remains constant. This motivated researchers to provide Web
personalized online services such as Web recommendations to alleviate the
information overload problem and provide tailored Web experiences to the Web
users. Recent studies show that Web usage mining has emerged as a popular
approach in providing Web personalization. However conventional Web usage based
recommender systems are limited in their ability to use the domain knowledge of
the Web application. The focus is only on Web usage data. As a consequence the
quality of the discovered patterns is low. In this paper, we propose a novel
framework integrating semantic information in the Web usage mining process.
Sequential Pattern Mining technique is applied over the semantic space to
discover the frequent sequential patterns. The frequent navigational patterns
are extracted in the form of Ontology instances instead of Web page views and
the resultant semantic patterns are used for generating Web page
recommendations to the user. Experimental results shown are promising and
proved that incorporating semantic information into Web usage mining process
can provide us with more interesting patterns which consequently make the
recommendation system more functional, smarter and comprehensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2542</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2542</id><created>2011-11-10</created><authors><author><keyname>Balakrishnan</keyname><forenames>Bhargav</forenames></author></authors><title>Three Tier Encryption Algorithm For Secure File Transfer</title><categories>cs.CR</categories><comments>ICKD 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This encryption algorithm is mainly designed for having a secure file
transfer in the low privilege servers and as well as in a secured environment
too. This methodology will be implemented in the data center and other
important data transaction sectors of the organisation where the encoding
process of the software will be done by the database administrator or system
administrators and his trusted clients will have decoding process of the
software. This software will not be circulated to the unauthorised customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2581</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2581</id><created>2011-11-10</created><updated>2011-11-12</updated><authors><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Hybrid Approximate Message Passing with Applications to Structured
  Sparsity</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian and quadratic approximations of message passing algorithms on graphs
have attracted considerable recent attention due to their computational
simplicity, analytic tractability, and wide applicability in optimization and
statistical inference problems. This paper presents a systematic framework for
incorporating such approximate message passing (AMP) methods in general
graphical models. The key concept is a partition of dependencies of a general
graphical model into strong and weak edges, with the weak edges representing
interactions through aggregates of small, linearizable couplings of variables.
AMP approximations based on the Central Limit Theorem can be readily applied to
the weak edges and integrated with standard message passing updates on the
strong edges. The resulting algorithm, which we call hybrid generalized
approximate message passing (Hybrid-GAMP), can yield significantly simpler
implementations of sum-product and max-sum loopy belief propagation. By varying
the partition of strong and weak edges, a performance-complexity trade-off can
be achieved. Structured sparsity problems are studied as an example of this
general methodology where there is a natural partition of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2616</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2616</id><created>2011-11-10</created><authors><author><keyname>J&#xf8;rgensen</keyname><forenames>Jakob H.</forenames></author><author><keyname>Sidky</keyname><forenames>Emil Y.</forenames></author><author><keyname>Pan</keyname><forenames>Xiaochuan</forenames></author></authors><title>Ensuring convergence in total-variation-based reconstruction for
  accurate microcalcification imaging in breast X-ray CT</title><categories>physics.med-ph cs.CE math.OC</categories><comments>5 pages, 4 figures, extended version of conference paper for 2011
  IEEE Nuclear Science Symposium and Medical Imaging Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast X-ray CT imaging is being considered in screening as an extension to
mammography. As a large fraction of the population will be exposed to
radiation, low-dose imaging is essential. Iterative image reconstruction based
on solving an optimization problem, such as Total-Variation minimization, shows
potential for reconstruction from sparse-view data. For iterative methods it is
important to ensure convergence to an accurate solution, since important image
features, such as presence of microcalcifications indicating breast cancer, may
not be visible in a non-converged reconstruction, and this can have clinical
significance. To prevent excessively long computational times, which is a
practical concern for the large image arrays in CT, it is desirable to keep the
number of iterations low, while still ensuring a sufficiently accurate
reconstruction for the specific imaging task. This motivates the study of
accurate convergence criteria for iterative image reconstruction. In simulation
studies with a realistic breast phantom with microcalcifications we compare
different convergence criteria for reliable reconstruction. Our results show
that it can be challenging to ensure a sufficiently accurate microcalcification
reconstruction, when using standard convergence criteria. In particular, the
gray level of the small microcalcifications may not have converged long after
the background tissue is reconstructed uniformly. We propose the use of the
individual objective function gradient components to better monitor possible
regions of non-converged variables. For microcalcifications we find empirically
a large correlation between nonzero gradient components and non-converged
variables, which occur precisely within the microcalcifications. This supports
our claim that gradient components can be used to ensure convergence to a
sufficiently accurate reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2618</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2618</id><created>2011-11-10</created><updated>2012-05-17</updated><authors><author><keyname>Day</keyname><forenames>Brian P.</forenames></author><author><keyname>Margetts</keyname><forenames>Adam R.</forenames></author><author><keyname>Bliss</keyname><forenames>Daniel W.</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Full-Duplex MIMO Relaying: Achievable Rates under Limited Dynamic Range</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2012.2192925</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of full-duplex multiple-input
multiple-output (MIMO) relaying between multi-antenna source and destination
nodes. The principal difficulty in implementing such a system is that, due to
the limited attenuation between the relay's transmit and receive antenna
arrays, the relay's outgoing signal may overwhelm its limited-dynamic-range
input circuitry, making it difficult---if not impossible---to recover the
desired incoming signal. While explicitly modeling transmitter/receiver
dynamic-range limitations and channel estimation error, we derive tight upper
and lower bounds on the end-to-end achievable rate of decode-and-forward-based
full-duplex MIMO relay systems, and propose a transmission scheme based on
maximization of the lower bound. The maximization requires us to (numerically)
solve a nonconvex optimization problem, for which we detail a novel approach
based on bisection search and gradient projection. To gain insights into system
design tradeoffs, we also derive an analytic approximation to the achievable
rate and numerically demonstrate its accuracy. We then study the behavior of
the achievable rate as a function of signal-to-noise ratio,
interference-to-noise ratio, transmitter/receiver dynamic range, number of
antennas, and training length, using optimized half-duplex signaling as a
baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2619</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2619</id><created>2011-11-10</created><authors><author><keyname>Ruj</keyname><forenames>Sushmita</forenames></author><author><keyname>Nayak</keyname><forenames>Amiya</forenames></author><author><keyname>Stojmenovic</keyname><forenames>Ivan</forenames></author></authors><title>A Security Architecture for Data Aggregation and Access Control in Smart
  Grids</title><categories>cs.NI</categories><comments>12 Pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an integrated architecture for smart grids, that supports data
aggregation and access control. Data can be aggregated by home area network,
building area network and neighboring area network in such a way that the
privacy of customers is protected. We use homomorphic encryption technique to
achieve this. The consumer data that is collected is sent to the substations
where it is monitored by remote terminal units (RTU). The proposed access
control mechanism gives selective access to consumer data stored in data
repositories and used by different smart grid users. Users can be maintenance
units, utility centers, pricing estimator units or analyzing and prediction
groups. We solve this problem of access control using cryptographic technique
of attribute-based encryption. RTUs and users have attributes and cryptographic
keys distributed by several key distribution centers (KDC). RTUs send data
encrypted under a set of attributes. Users can decrypt information provided
they have valid attributes. The access control scheme is distributed in nature
and does not rely on a single KDC to distribute keys. Bobba \emph{et al.}
\cite{BKAA09} proposed an access control scheme, which relies on a centralized
KDC and is thus prone to single-point failure. The other requirement is that
the KDC has to be online, during data transfer which is not required in our
scheme. Our access control scheme is collusion resistant, meaning that users
cannot collude and gain access to data, when they are not authorized to access.
We theoretically analyze our schemes and show that the computation overheads
are low enough to be carried out in smart grids. To the best of our knowledge,
ours is the first work on smart grids, which integrates these two important
security components (privacy preserving data aggregation and access control)
and presents an overall security architecture in smart grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2621</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2621</id><created>2011-11-10</created><updated>2013-08-23</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Optimal Lower and Upper Bounds for Representing Sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence representations supporting queries $access$, $select$ and $rank$ are
at the core of many data structures. There is a considerable gap between the
various upper bounds and the few lower bounds known for such representations,
and how they relate to the space used. In this article we prove a strong lower
bound for $rank$, which holds for rather permissive assumptions on the space
used, and give matching upper bounds that require only a compressed
representation of the sequence. Within this compressed space, operations
$access$ and $select$ can be solved in constant or almost-constant time, which
is optimal for large alphabets. Our new upper bounds dominate all of the
previous work in the time/space map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2626</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2626</id><created>2011-11-10</created><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Oren</keyname><forenames>Sigal</forenames></author><author><keyname>Zohar</keyname><forenames>Aviv</forenames></author></authors><title>On Bitcoin and Red Balloons</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study scenarios in which the goal is to ensure that some information will
propagate through a large network of nodes. In these scenarios all nodes that
are aware of the information compete for the same prize, and thus have an
incentive not to propagate information. One example for such a scenario is the
2009 DARPA Network Challenge (finding red balloons). We give special attention
to a second domain, Bitcoin, a decentralized electronic currency system.
  Bitcoin, which has been getting a large amount of public attention over the
last year, represents a radical new approach to monetary systems which has
appeared in policy discussions and in the popular press. Its cryptographic
fundamentals have largely held up even as its usage has become increasingly
widespread. We find, however, that it exhibits a fundamental problem of a
different nature, based on how its incentives are structured. We propose a
modification to the protocol that can fix this problem.
  Bitcoin relies on a peer-to-peer network to track transactions that are
performed with the currency. For this purpose, every transaction a node learns
about should be transmitted to its neighbors in the network. As the protocol is
currently defined and implemented, it does not provide an incentive for nodes
to broadcast transactions they are aware of. In fact, it provides a strong
incentive not to do so. Our solution is to augment the protocol with a scheme
that rewards information propagation. We show that our proposed scheme succeeds
in setting the correct incentives, that it is Sybil-proof, and that it requires
only a small payment overhead, all this is achieved with iterated elimination
of dominated strategies. We provide lower bounds on the overhead that is
required to implement schemes with the stronger solution concept of Dominant
Strategies, indicating that such schemes might be impractical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2637</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2637</id><created>2011-11-10</created><updated>2012-03-22</updated><authors><author><keyname>Bouyuklieva</keyname><forenames>Stefka</forenames></author><author><keyname>Bouyukliev</keyname><forenames>Iliya</forenames></author><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>Some Extremal Self-Dual Codes and Unimodular Lattices in Dimension 40</title><categories>math.CO cs.IT math.IT math.NT</categories><comments>30 pages</comments><journal-ref>Finite Fields and Their Applications 21 (2013) 67-83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, binary extremal singly even self-dual codes of length 40 and
extremal odd unimodular lattices in dimension 40 are studied. We give a
classification of extremal singly even self-dual codes of length 40. We also
give a classification of extremal odd unimodular lattices in dimension 40 with
shadows having 80 vectors of norm 2 through their relationships with extremal
doubly even self-dual codes of length 40.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2640</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2640</id><created>2011-11-10</created><authors><author><keyname>He</keyname><forenames>Yuan Yuan</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Power Allocation for Outage Minimization in Cognitive Radio Networks
  with Limited Feedback</title><categories>cs.IT math.IT math.OC</categories><comments>22 pages, 7 Figures</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address an optimal transmit power allocation problem that minimizes the
outage probability of a secondary user (SU) who is allowed to coexist with a
primary user (PU) in a narrowband spectrum sharing cognitive radio network,
under a long term average transmit power constraint at the secondary
transmitter (SU-TX) and an average interference power constraint at the primary
receiver (PU-RX), with quantized channel state information (CSI) (including
both the channels from SU-TX to SU-RX, denoted as $g_1$ and the channel from
SU-TX to PU-RX, denoted as $g_0$) at the SU-TX. The optimal quantization
regions in the vector channel space is shown to have a 'stepwise' structure.
With this structure, the above outage minimization problem can be explicitly
formulated and solved by employing the Karush-Kuhn-Tucker (KKT) necessary
optimality conditions to obtain a locally optimal quantized power codebook. A
low-complexity near-optimal quantized power allocation algorithm is derived for
the case of large number of feedback bits. An explicit expression for the
asymptotic SU outage probability at high rate quantization (as the number of
feedback bits goes to infinity) is also provided, and is shown to approximate
the optimal outage behavior extremely well for large number of bits of feedback
via numerical simulations. Numerical results also illustrate that with 6 bits
of feedback, the derived algorithms provide SU outage performance very close to
that with full CSI at the SU-TX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2642</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2642</id><created>2011-11-10</created><updated>2016-01-31</updated><authors><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author></authors><title>On the rank functions of $\mathcal{H}$-matroids</title><categories>math.CO cs.DM</categories><comments>6 pages</comments><msc-class>05B35, 90C27</msc-class><journal-ref>Journal of Algebra Combinatorics Discrete Structures and
  Applications, Vol. 3, No. 1 (2016) 7-11</journal-ref><doi>10.13069/jacodesmath.26764</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of $\mathcal{H}$-matroids was introduced by U. Faigle and S.
Fujishige in 2009 as a general model for matroids and the greedy algorithm.
They gave a characterization of $\mathcal{H}$-matroids by the greedy algorithm.
In this note, we give a characterization of some $\mathcal{H}$-matroids by rank
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2651</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2651</id><created>2011-11-10</created><authors><author><keyname>Ng</keyname><forenames>Irene</forenames></author><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author></authors><title>Value, Variety and Viability: Designing For Co-creation in a Complex
  System of Direct and Indirect (goods) Service Value Proposition</title><categories>cs.SY</categories><comments>26 pages, 3 figures, 1 table, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While service-dominant logic proposes that all &quot;Goods are a distribution
mechanism for service provision&quot; (FP3), there is a need to understand when and
why a firm would utilise direct or indirect (goods) service provision, and the
interactions between them, to co-create value with the customer. Three
longitudinal case studies in B2B equipment-based 'complex service' systems were
analysed to gain an understanding of customers' co-creation activities to
achieve outcomes. We found the nature of value, degree of contextual variety
and the firm's legacy viability to be viability threats. To counter this, the
firm uses (a) Direct Service Provision for Scalability and Replicability, (b)
Indirect Service Provision for variety absorption and co-creating emotional
value and customer experience and (c) designing direct and indirect provision
for Scalability and Absorptive Resources of the customer. The co-creation of
complex multidimensional value could be delivered through different value
propositions of the firm. The research proposes a value-centric way of
understanding the interactions between direct and indirect service provision in
the design of the firm's value proposition and proposes a viable systems
approach towards reorganising the firm. The study provides a way for managers
to understand the effectiveness (rather than efficiency) of the firm in
co-creating value as a major issue in the design of complex socio-technical
systems. Goods are often designed within the domain of engineering and product
design, often placing human activity as a supporting role to the equipment.
Through an SDLogic lens, this study considers the design of both equipment and
human activity on an equal footing for value co-creation with the customer, and
it yielded interesting results on when direct provisioning (goods) should be
redesigned, considering all activities equally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2664</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2664</id><created>2011-11-11</created><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author></authors><title>A Collaborative Mechanism for Crowdsourcing Prediction Problems</title><categories>cs.LG cs.GT</categories><comments>Full version of the extended abstract which appeared in NIPS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning competitions such as the Netflix Prize have proven
reasonably successful as a method of &quot;crowdsourcing&quot; prediction tasks. But
these competitions have a number of weaknesses, particularly in the incentive
structure they create for the participants. We propose a new approach, called a
Crowdsourced Learning Mechanism, in which participants collaboratively &quot;learn&quot;
a hypothesis for a given prediction task. The approach draws heavily from the
concept of a prediction market, where traders bet on the likelihood of a future
event. In our framework, the mechanism continues to publish the current
hypothesis, and participants can modify this hypothesis by wagering on an
update. The critical incentive property is that a participant will profit an
amount that scales according to how much her update improves performance on a
released test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2669</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2669</id><created>2011-11-11</created><authors><author><keyname>Geeta</keyname><forenames>R. B.</forenames></author><author><keyname>Mamillapalli</keyname><forenames>Omkar</forenames></author><author><keyname>Totad</keyname><forenames>Shasikumar G.</forenames></author><author><keyname>D</keyname><forenames>Prasad Reddy P. V. G.</forenames></author></authors><title>A Novel Approach for Web Page Set Mining</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The one of the most time consuming steps for association rule mining is the
computation of the frequency of the occurrences of itemsets in the database.
The hash table index approach converts a transaction database to an hash index
tree by scanning the transaction database only once. Whenever user requests for
any Uniform Resource Locator (URL), the request entry is stored in the Log File
of the server. This paper presents the hash index table structure, a general
and dense structure which provides web page set extraction from Log File of
server. This hash table provides information about the original database. Web
Page set mining (WPs-Mine) provides a complete representation of the original
database. This approach works well for both sparse and dense data
distributions. Web page set mining supported by hash table index shows the
performance always comparable with and often better than algorithms accessing
data on flat files. Incremental update is feasible without reaccessing the
original transactional database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2678</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2678</id><created>2011-11-11</created><authors><author><keyname>Yu</keyname><forenames>Fang</forenames><affiliation>National Chengchi University, Taiwan</affiliation></author><author><keyname>Wang</keyname><forenames>Chao</forenames><affiliation>Virginia Tech, U.S.</affiliation></author></authors><title>Proceedings 13th International Workshop on Verification of
  Infinite-State Systems</title><categories>cs.LO cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011</journal-ref><doi>10.4204/EPTCS.73</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 13th International Workshop on
Verification of Infinite-State Systems (INFINITY 2011). The workshop was held
in Taipei, Taiwan on October 10, 2011, as a satellite event to the 9th
International Symposium on Automated Technology for Verification and Analysis
(ATVA). The INFINITY workshop aims at providing a forum for researchers who are
interested in the development of formal methods and algorithmic techniques for
the analysis of systems with infinitely many states, and their application in
automated verification of complex software and hardware systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2693</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2693</id><created>2011-11-11</created><updated>2012-04-09</updated><authors><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames></author><author><keyname>Nicolaou</keyname><forenames>Nicolas C.</forenames></author></authors><title>On the Practicality of Atomic MWMR Register Implementations</title><categories>cs.DC cs.DS</categories><comments>18 pages, 14 figures, 3 tables, Technical Report, Full Version of an
  Article appearing in the Proceedings of the 10th International Symposium on
  Parallel and Distributed Processing with Applications (ISPA 2012), Leganes,
  Madrid, July 2012</comments><report-no>UCY-CS-TR-11-08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-writer/multiple-reader (MWMR) atomic register implementations
provide precise consistency guarantees, in the asynchronous, crash-prone,
message passing environment. Fast MWMR atomic register implementations were
first introduced in Englert et al. 2009. Fastness is measured in terms of the
number of single round read and write operations that does not sacrifice
correctness. In Georgiou et al. 2011 was shown, however, that decreasing the
communication cost is not enough in these implementations. In particular,
considering that the performance is measured in terms of the latency of read
and write operations due to both (a) communication delays and (b)local
computation, they introduced two new algorithms that traded communication for
reducing computation. As computation is still part of the algorithms, someone
may wonder: What is the trade-off between communication and local computation
in real-time systems?
  In this work we conduct an experimental performance evaluation of four MWMR
atomic register implementations: SFW from Englert et al. 2009, APRX-SFW and
CWFR from Georgiou at al. 2011, and the generalization of the traditional
algorithm of Attiya et al. 1996 in the MWMR environment, which we call SIMPLE.
We implement and evaluate the algorithms on NS2, a single-processor simulator,
and on PlanetLab, a planetary-scale real-time network platform. Our comparison
provides an empirical answer to the above question and demonstrates the
practicality of atomic MWMR register implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2713</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2713</id><created>2011-11-11</created><authors><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>The asymptotic behavior of Grassmannian codes</title><categories>cs.DM</categories><comments>5 pages</comments><msc-class>94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The iterated Johnson bound is the best known upper bound on a size of an
error-correcting code in the Grassmannian $\mathcal{G}_q(n,k)$. The iterated
Sch\&quot;{o}nheim bound is the best known lower bound on the size of a covering
code in $\mathcal{G}_q(n,k)$. We use probabilistic methods to prove that both
bounds are asymptotically attained for fixed $k$ and fixed radius, as $n$
approaches infinity. We also determine the asymptotics of the size of the best
Grassmannian codes and covering codes when $n-k$ and the radius are fixed, as
$n$ approaches infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2744</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2744</id><created>2011-11-11</created><authors><author><keyname>Tapiador</keyname><forenames>Juan E.</forenames></author><author><keyname>Hernandez-Castro</keyname><forenames>Julio C.</forenames></author><author><keyname>Peris-Lopez</keyname><forenames>P.</forenames></author><author><keyname>Clark</keyname><forenames>John A.</forenames></author></authors><title>Cryptanalysis of Song's advanced smart card based password
  authentication protocol</title><categories>cs.CR</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Song \cite{Song10} proposed very recently a password-based authentication and
key establishment protocol using smart cards which attempts to solve some
weaknesses found in a previous scheme suggested by Xu, Zhu, and Feng
\cite{XZF09}. In this paper, we present attacks on the improved protocol,
showing that it fails to achieve the claimed security goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2750</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2750</id><created>2011-11-07</created><authors><author><keyname>M</keyname><forenames>Thirumaran.</forenames></author><author><keyname>P</keyname><forenames>Dhavachelvan.</forenames></author><author><keyname>Abarna</keyname><forenames>S.</forenames></author><author><keyname>P</keyname><forenames>Lakshmi.</forenames></author></authors><title>Finite State Machine Based Evaluation Model for Web Service Reliability
  Analysis</title><categories>cs.SE</categories><comments>13 pages,3 figures, WesT-2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days they are very much considering about the changes to be done at
shorter time since the reaction time needs are decreasing every moment.
Business Logic Evaluation Model (BLEM) are the proposed solution targeting
business logic automation and facilitating business experts to write
sophisticated business rules and complex calculations without costly custom
programming. BLEM is powerful enough to handle service manageability issues by
analyzing and evaluating the computability and traceability and other criteria
of modified business logic at run time. The web service and QOS grows
expensively based on the reliability of the service. Hence the service provider
of today things that reliability is the major factor and any problem in the
reliability of the service should overcome then and there in order to achieve
the expected level of reliability. In our paper we propose business logic
evaluation model for web service reliability analysis using Finite State
Machine (FSM) where FSM will be extended to analyze the reliability of composed
set of service i.e., services under composition, by analyzing reliability of
each participating service of composition with its functional work flow
process. FSM is exploited to measure the quality parameters. If any change
occurs in the business logic the FSM will automatically measure the
reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2760</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2760</id><created>2011-11-09</created><authors><author><keyname>Andr&#xe9;s</keyname><forenames>Miguel E.</forenames></author></authors><title>Quantitative Analysis of Information Leakage in Probabilistic and
  Nondeterministic Systems</title><categories>cs.CR</categories><comments>thesis, ISBN: 978-94-91211-74-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis addresses the foundational aspects of formal methods for
applications in security and in particular in anonymity. More concretely, we
develop frameworks for the specification of anonymity properties and propose
algorithms for their verification. Since in practice anonymity protocols always
leak some information, we focus on quantitative properties, which capture the
amount of information leaked by a protocol.
  The main contribution of this thesis is cpCTL, the first temporal logic that
allows for the specification and verification of conditional probabilities
(which are the key ingredient of most anonymity properties). In addition, we
have considered several prominent definitions of information-leakage and
developed the first algorithms allowing us to compute (and even approximate)
the information leakage of anonymity protocols according to these definitions.
We have also studied a well-known problem in the specification and analysis of
distributed anonymity protocols, namely full-information scheduling. To
overcome this problem, we have proposed an alternative notion of scheduling and
adjusted accordingly several anonymity properties from the literature. Our last
major contribution is a debugging technique that helps on the detection of
flaws in security protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2763</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2763</id><created>2011-11-08</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>N.</forenames></author><author><keyname>Balas</keyname><forenames>V. E.</forenames></author><author><keyname>Motoc</keyname><forenames>I. M.</forenames></author></authors><title>8-Valent Fuzzy Logic for Iris Recognition and Biometry</title><categories>cs.AI</categories><comments>6 pages, 2 figures, 5th IEEE Int. Symp. on Computational Intelligence
  and Intelligent Informatics (Floriana, Malta, September 15-17), ISBN:
  978-1-4577-1861-8 (electronic), 978-1-4577-1860-1 (print), 2011</comments><msc-class>03B52, 03B50, 03B80</msc-class><acm-class>F.4.1; I.2.4</acm-class><journal-ref>Proc. 5th IEEE Int. Symp. on Computational Intelligence and
  Intelligent Informatics, pp. 149-154, ISBN: 978-1-4577-1861-8 (electronic),
  978-1-4577-1860-1 (print), IEEE Press, 2011</journal-ref><doi>10.1109/ISCIII.2011.6069761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that maintaining logical consistency of an iris recognition
system is a matter of finding a suitable partitioning of the input space in
enrollable and unenrollable pairs by negotiating the user comfort and the
safety of the biometric system. In other words, consistent enrollment is
mandatory in order to preserve system consistency. A fuzzy 3-valued
disambiguated model of iris recognition is proposed and analyzed in terms of
completeness, consistency, user comfort and biometric safety. It is also shown
here that the fuzzy 3-valued model of iris recognition is hosted by an 8-valued
Boolean algebra of modulo 8 integers that represents the computational
formalization in which a biometric system (a software agent) can achieve the
artificial understanding of iris recognition in a logically consistent manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2768</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2768</id><created>2011-11-09</created><authors><author><keyname>Napoli</keyname><forenames>Margherita</forenames></author><author><keyname>Parente</keyname><forenames>Mimmo</forenames></author></authors><title>Graded CTL Model Checking for Test Generation</title><categories>cs.LO cs.FL</categories><comments>Symposium On Theory of Modeling and Simulation (DEVS/TMS'11)</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been a great attention from the scientific community
towards the use of the model-checking technique as a tool for test generation
in the simulation field. This paper aims to provide a useful mean to get more
insights along these lines. By applying recent results in the field of graded
temporal logics, we present a new efficient model-checking algorithm for
Hierarchical Finite State Machines (HSM), a well established symbolism long and
widely used for representing hierarchical models of discrete systems.
Performing model-checking against specifications expressed using graded
temporal logics has the peculiarity of returning more counterexamples within a
unique run. We think that this can greatly improve the efficacy of
automatically getting test cases. In particular we verify two different models
of HSM against branching time temporal properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2788</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2788</id><created>2011-11-11</created><authors><author><keyname>Kogler</keyname><forenames>Roman</forenames></author><author><keyname>South</keyname><forenames>David M.</forenames></author><author><keyname>Steder</keyname><forenames>Michael</forenames></author></authors><title>Data Preservation in High Energy Physics</title><categories>hep-ex cs.DL</categories><comments>8 pages, 6 figures, proceedings of ACAT 2011 poster</comments><report-no>ACAT2011</report-no><doi>10.1088/1742-6596/368/1/012026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data from high-energy physics experiments are collected with significant
financial and human effort and are mostly unique. However, until recently no
coherent strategy existed for data preservation and re-use, and many important
and complex data sets have simply been lost. While the current focus is on the
LHC at CERN, in the current period several important and unique experimental
programs at other facilities are coming to an end, including those at HERA,
b-factories and the Tevatron. To address this issue, an inter-experimental
study group on HEP data preservation and long-term analysis (DPHEP) was
convened at the end of 2008. The group now aims to publish a full and detailed
review of the present status of data preservation in high energy physics. This
contribution summarises the results of the DPHEP study group, describing the
challenges of data preservation in high energy physics and the group's first
conclusions and recommendations. The physics motivation for data preservation,
generic computing and preservation models, technological expectations and
governance aspects at local and international levels are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2824</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2824</id><created>2011-11-11</created><authors><author><keyname>Vaz</keyname><forenames>C.</forenames></author><author><keyname>Ferreira</keyname><forenames>C.</forenames></author></authors><title>Towards Automated Verification of Web Services</title><categories>cs.LO</categories><comments>Proceedings of the IADIS International Conference on WWW/Internet
  2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the use of model-checking software technology for the
verification of workflows and business processes behaviour based on web
services, namely the use of the SPIN model checker. Since the specification of
a business process behaviour based on web services can be decomposed into
patterns, it is proposed a translation of a well known collection of workflow
patterns into PROMELA, the input specification language of SPIN. The use of
this translation is illustrated with one business process example, which
demonstrates how its translation to a PROMELA model can be useful in the web
service specification and verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2825</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2825</id><created>2011-11-11</created><authors><author><keyname>Howard</keyname><forenames>Y.</forenames></author><author><keyname>Gruner</keyname><forenames>S.</forenames></author><author><keyname>Gravell</keyname><forenames>A.</forenames></author><author><keyname>Ferreira</keyname><forenames>C.</forenames></author><author><keyname>Augusto</keyname><forenames>J. C.</forenames></author></authors><title>Model-Based Trace-Checking</title><categories>cs.LO cs.SE</categories><journal-ref>UK Software Testing Research Workshop 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trace analysis can be a useful way to discover problems in a program under
test. Rather than writing a special purpose trace analysis tool, this paper
proposes that traces can usefully be analysed by checking them against a formal
model using a standard model-checker or else an animator for executable
specifications. These techniques are illustrated using a Travel Agent case
study implemented in J2EE. We added trace beans to this code that write trace
information to a database. The traces are then extracted and converted into a
form suitable for analysis by Spin, a popular model-checker, and Pro-B, a
model-checker and animator for the B notation. This illustrates the technique,
and also the fact that such a system can have a variety of models, in different
notations, that capture different features. These experiments have demonstrated
that model-based trace-checking is feasible. Future work is focussed on scaling
up the approach to larger systems by increasing the level of automation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2826</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2826</id><created>2011-11-11</created><authors><author><keyname>Gravell</keyname><forenames>A.</forenames></author><author><keyname>Howard</keyname><forenames>Y.</forenames></author><author><keyname>Augusto</keyname><forenames>J. C.</forenames></author><author><keyname>Ferreira</keyname><forenames>C.</forenames></author><author><keyname>Gruner</keyname><forenames>S.</forenames></author></authors><title>Concurrent Development of Model and Implementation</title><categories>cs.SE</categories><journal-ref>16th International Conference on Software and Systems Engineering
  and their Applications (ICSSEA 2003)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers how a formal mathematically-based model can be used in
support of evolutionary software development, and in particular how such a
model can be kept consistent with the implementation as it changes to meet new
requirements. A number of techniques are listed can make use of such a model to
enhance the development process, and also ways to keep model and implementation
consistent. The effectiveness of these techniques is investigated through two
case studies concerning the development of small e-business applications, a
travel agent and a mortgage broker. Some successes are reported, notably in the
use of rapid throwaway modelling to investigate design alternatives, and also
in the use of close team working and modelbased trace-checking to maintain
synchronisation between model and implementation throughout the development.
The main areas of weakness were seen to derive from deficiencies in tool
support. Recommendations are therefore made for future improvements to tools
supporting formal models which would, in principle, make this co-evolutionary
approach attractive to industrial software developers. It is claimed that in
fact tools already exist that provide the desired facilities, but these are not
necessarily production-quality, and do not all support the same notations, and
hence cannot be used together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2829</identifier>
 <datestamp>2012-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2829</id><created>2011-11-11</created><authors><author><keyname>da Silva</keyname><forenames>Roberto</forenames></author><author><keyname>Kalil</keyname><forenames>Fahad</forenames></author><author><keyname>Martinez</keyname><forenames>Alexandre Souto</forenames></author><author><keyname>de Oliveira</keyname><forenames>Jose Palazzo Moreira</forenames></author></authors><title>Universality in Bibliometrics</title><categories>physics.soc-ph cs.DL</categories><comments>To appear in Physica A (8 pages, 6 figures and 2 tables)</comments><journal-ref>Physica A 391 (2012) 2119-2128</journal-ref><doi>10.1016/j.physa.2011.11.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many discussions have enlarged the literature in Bibliometrics since the
Hirsh proposal, the so called $h$-index. Ranking papers according to their
citations, this index quantifies a researcher only by its greatest possible
number of papers that are cited at least $h$ times. A closed formula for
$h$-index distribution that can be applied for distinct databases is not yet
known. In fact, to obtain such distribution, the knowledge of citation
distribution of the authors and its specificities are required. Instead of
dealing with researchers randomly chosen, here we address different groups
based on distinct databases. The first group is composed by physicists and
biologists, with data extracted from Institute of Scientific Information (ISI).
The second group composed by computer scientists, which data were extracted
from Google-Scholar system. In this paper, we obtain a general formula for the
$h$-index probability density function (pdf) for groups of authors by using
generalized exponentials in the context of escort probability. Our analysis
includes the use of several statistical methods to estimate the necessary
parameters. Also an exhaustive comparison among the possible candidate
distributions are used to describe the way the citations are distributed among
authors. The $h$-index pdf should be used to classify groups of researchers
from a quantitative point of view, which is meaningfully interesting to
eliminate obscure qualitative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2837</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2837</id><created>2011-11-09</created><authors><author><keyname>Zhong</keyname><forenames>Peng</forenames></author><author><keyname>Haija</keyname><forenames>Ahmad Abu Al</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>On Compress-Forward without Wyner-Ziv Binning for Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noisy network coding is recently proposed for the general multi-source
network by Lim, Kim, El Gamal and Chung. This scheme builds on compress-forward
(CF) relaying but involves three new ideas, namely no Wyner-Ziv binning,
relaxed simultaneous decoding and message repetition. In this paper, using the
two-way relay channel as the underlining example, we analyze the impact of each
of these ideas on the achievable rate region of relay networks. First, CF
without binning but with joint decoding of both the message and compression
index can achieve a larger rate region than the original CF scheme for
multi-destination relay networks. With binning and successive decoding, the
compression rate at each relay is constrained by the weakest link from the
relay to a destination; but without binning, this constraint is relaxed.
Second, simultaneous decoding of all messages over all blocks without uniquely
decoding the compression indices can remove the constraints on compression rate
completely, but is still subject to the message block boundary effect. Third,
message repetition is necessary to overcome this boundary effect and achieve
the noisy network coding region for multi-source networks. The rate region is
enlarged with increasing repetition times. We also apply CF without binning
specifically to the one-way and two-way relay channels and analyze the rate
regions in detail. For the one-way relay channel, it achieves the same rate as
the original CF and noisy network coding but has only 1 block decoding delay.
For the two-way relay channel, we derive the explicit channel conditions in the
Gaussian and fading cases for CF without binning to achieve the same rate
region or sum rate as noisy network coding. These analyses may be appealing to
practical implementation because of the shorter encoding and decoding delay in
CF without binning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2852</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2852</id><created>2011-11-11</created><authors><author><keyname>Valduriez</keyname><forenames>Patrick</forenames><affiliation>INRIA Sophia Antipolis, LIRMM</affiliation></author></authors><title>Principles of Distributed Data Management in 2020?</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Int. Conf. on Databases and Expert Systems Applications (DEXA)
  6860 (2011) 1-11</journal-ref><doi>10.1007/978-3-642-23088-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advents of high-speed networks, fast commodity hardware, and the
web, distributed data sources have become ubiquitous. The third edition of the
\&quot;Ozsu-Valduriez textbook Principles of Distributed Database Systems [10]
reflects the evolution of distributed data management and distributed database
systems. In this new edition, the fundamental principles of distributed data
management could be still presented based on the three dimensions of earlier
editions: distribution, heterogeneity and autonomy of the data sources. In
retrospect, the focus on fundamental principles and generic techniques has been
useful not only to understand and teach the material, but also to enable an
infinite number of variations. The primary application of these generic
techniques has been obviously for distributed and parallel DBMS versions.
Today, to support the requirements of important data-intensive applications
(e.g. social networks, web data analytics, scientific applications, etc.), new
distributed data management techniques and systems (e.g. MapReduce, Hadoop,
SciDB, Peanut, Pig latin, etc.) are emerging and receiving much attention from
the research community. Although they do well in terms of
consistency/flexibility/performance trade-offs for specific applications, they
seem to be ad-hoc and might hurt data interoperability. The key questions I
discuss are: What are the fundamental principles behind the emerging solutions?
Is there any generic architectural model, to explain those principles? Do we
need new foundations to look at data distribution?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2885</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2885</id><created>2011-11-11</created><updated>2012-09-27</updated><authors><author><keyname>Dandekar</keyname><forenames>Pranav</forenames></author><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author></authors><title>Privacy Auctions for Recommender Systems</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a market for private data in which a data analyst publicly releases
a statistic over a database of private information. Individuals that own the
data incur a cost for their loss of privacy proportional to the differential
privacy guarantee given by the analyst at the time of the release. The analyst
incentivizes individuals by compensating them, giving rise to a \emph{privacy
auction}. Motivated by recommender systems, the statistic we consider is a
linear predictor function with publicly known weights. The statistic can be
viewed as a prediction of the unknown data of a new individual, based on the
data of individuals in the database. We formalize the trade-off between privacy
and accuracy in this setting, and show that a simple class of estimates
achieves an order-optimal trade-off. It thus suffices to focus on auction
mechanisms that output such estimates. We use this observation to design a
truthful, individually rational, proportional-purchase mechanism under a fixed
budget constraint. We show that our mechanism is 5-approximate in terms of
accuracy compared to the optimal mechanism, and that no truthful mechanism can
achieve a $2-\varepsilon$ approximation, for any $\varepsilon &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2888</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2888</id><created>2011-11-11</created><updated>2013-09-05</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Christin</keyname><forenames>Nicolas</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author></authors><title>Adaptive Regret Minimization in Bounded-Memory Games</title><categories>cs.GT</categories><comments>Full Version. GameSec 2013 (Invited Paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning algorithms that minimize regret provide strong guarantees in
situations that involve repeatedly making decisions in an uncertain
environment, e.g. a driver deciding what route to drive to work every day.
While regret minimization has been extensively studied in repeated games, we
study regret minimization for a richer class of games called bounded memory
games. In each round of a two-player bounded memory-m game, both players
simultaneously play an action, observe an outcome and receive a reward. The
reward may depend on the last m outcomes as well as the actions of the players
in the current round. The standard notion of regret for repeated games is no
longer suitable because actions and rewards can depend on the history of play.
To account for this generality, we introduce the notion of k-adaptive regret,
which compares the reward obtained by playing actions prescribed by the
algorithm against a hypothetical k-adaptive adversary with the reward obtained
by the best expert in hindsight against the same adversary. Roughly, a
hypothetical k-adaptive adversary adapts her strategy to the defender's actions
exactly as the real adversary would within each window of k rounds. Our
definition is parametrized by a set of experts, which can include both fixed
and adaptive defender strategies.
  We investigate the inherent complexity of and design algorithms for adaptive
regret minimization in bounded memory games of perfect and imperfect
information. We prove a hardness result showing that, with imperfect
information, any k-adaptive regret minimizing algorithm (with fixed strategies
as experts) must be inefficient unless NP=RP even when playing against an
oblivious adversary. In contrast, for bounded memory games of perfect and
imperfect information we present approximate 0-adaptive regret minimization
algorithms against an oblivious adversary running in time n^{O(1)}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2893</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2893</id><created>2011-11-11</created><authors><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Hartline</keyname><forenames>Jason D.</forenames></author><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author></authors><title>Optimal Crowdsourcing Contests</title><categories>cs.GT</categories><comments>The paper has 17 pages and 1 figure. It is to appear in the
  proceedings of ACM-SIAM Symposium on Discrete Algorithms 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design and approximation of optimal crowdsourcing contests.
Crowdsourcing contests can be modeled as all-pay auctions because entrants must
exert effort up-front to enter. Unlike all-pay auctions where a usual design
objective would be to maximize revenue, in crowdsourcing contests, the
principal only benefits from the submission with the highest quality. We give a
theory for optimal crowdsourcing contests that mirrors the theory of optimal
auction design: the optimal crowdsourcing contest is a virtual valuation
optimizer (the virtual valuation function depends on the distribution of
contestant skills and the number of contestants). We also compare crowdsourcing
contests with more conventional means of procurement. In this comparison,
crowdsourcing contests are relatively disadvantaged because the effort of
losing contestants is wasted. Nonetheless, we show that crowdsourcing contests
are 2-approximations to conventional methods for a large family of &quot;regular&quot;
distributions, and 4-approximations, otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2896</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2896</id><created>2011-11-11</created><authors><author><keyname>Chen</keyname><forenames>Ya-Hong</forenames></author><author><keyname>Pan</keyname><forenames>Rong-Ying</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao-Dong</forenames></author></authors><title>The Laplacian Spectra of Graphs and Complex Networks</title><categories>math.CO cs.SI physics.data-an physics.soc-ph</categories><comments>17 pages</comments><msc-class>05C50, 05C82, 05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is a brief survey of some recent new results and progress of the
Laplacian spectra of graphs and complex networks (in particular, random graph
and the small world network). The main contents contain the spectral radius of
the graph Laplacian for given a degree sequence, the Laplacian coefficients,
the algebraic connectivity and the graph doubly stochastic matrix, and the
spectra of random graphs and the small world networks. In addition, some
questions are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2904</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2904</id><created>2011-11-12</created><updated>2011-11-14</updated><authors><author><keyname>Ardon</keyname><forenames>Sebastien</forenames></author><author><keyname>Bagchi</keyname><forenames>Amitabha</forenames></author><author><keyname>Mahanti</keyname><forenames>Anirban</forenames></author><author><keyname>Ruhela</keyname><forenames>Amit</forenames></author><author><keyname>Seth</keyname><forenames>Aaditeshwar</forenames></author><author><keyname>Tripathy</keyname><forenames>Rudra M.</forenames></author><author><keyname>Triukose</keyname><forenames>Sipat</forenames></author></authors><title>Spatio-Temporal Analysis of Topic Popularity in Twitter</title><categories>cs.SI cs.CY</categories><comments>17 pages, 16 figures</comments><msc-class>82C20</msc-class><acm-class>H.5.4; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first comprehensive characterization of the diffusion of ideas
on Twitter, studying more than 4000 topics that include both popular and less
popular topics. On a data set containing approximately 10 million users and a
comprehensive scraping of all the tweets posted by these users between June
2009 and August 2009 (approximately 200 million tweets), we perform a rigorous
temporal and spatial analysis, investigating the time-evolving properties of
the subgraphs formed by the users discussing each topic. We focus on two
different notions of the spatial: the network topology formed by
follower-following links on Twitter, and the geospatial location of the users.
We investigate the effect of initiators on the popularity of topics and find
that users with a high number of followers have a strong impact on popularity.
We deduce that topics become popular when disjoint clusters of users discussing
them begin to merge and form one giant component that grows to cover a
significant fraction of the network. Our geospatial analysis shows that highly
popular topics are those that cross regional boundaries aggressively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2918</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2918</id><created>2011-11-12</created><updated>2012-09-12</updated><authors><author><keyname>Augustine</keyname><forenames>John</forenames></author><author><keyname>Das</keyname><forenames>Sandip</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Nandy</keyname><forenames>Subhas C.</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author><author><keyname>Sarvattomananda</keyname><forenames>Swami</forenames></author></authors><title>Localized Geometric Query Problems</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of geometric query problems are studied in this paper. We are
required to preprocess a set of geometric objects $P$ in the plane, so that for
any arbitrary query point $q$, the largest circle that contains $q$ but does
not contain any member of $P$, can be reported efficiently. The geometric sets
that we consider are point sets and boundaries of simple polygons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2931</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2931</id><created>2011-11-12</created><updated>2011-11-28</updated><authors><author><keyname>McDiarmid</keyname><forenames>Colin</forenames></author><author><keyname>Muller</keyname><forenames>Tobias</forenames></author></authors><title>Integer realizations of disk and segment graphs</title><categories>math.MG cs.CG math.CO</categories><comments>35 pages, 14 figures, corrected a typo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A disk graph is the intersection graph of disks in the plane, a unit disk
graph is the intersection graph of same radius disks in the plane, and a
segment graph is an intersection graph of line segments in the plane. It can be
seen that every disk graph can be realized by disks with centers on the integer
grid and with integer radii; and similarly every unit disk graph can be
realized by disks with centers on the integer grid and equal (integer) radius;
and every segment graph can be realized by segments whose endpoints lie on the
integer grid. Here we show that there exist disk graphs on $n$ vertices such
that in every realization by integer disks at least one coordinate or radius is
$2^{2^{\Omega(n)}}$ and on the other hand every disk graph can be realized by
disks with integer coordinates and radii that are at most $2^{2^{O(n)}}$; and
we show the analogous results for unit disk graphs and segment graphs. For
(unit) disk graphs this answers a question of Spinrad, and for segment graphs
this improves over a previous result by Kratochv\'{\i}l and Matou{\v{s}}ek.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2933</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2933</id><created>2011-11-12</created><authors><author><keyname>Phonphoem</keyname><forenames>Anan</forenames></author><author><keyname>Jansang</keyname><forenames>Aphirak</forenames></author></authors><title>A Simple Network Management Architecture for Supporting Network
  Administrator and QoS Requirements</title><categories>cs.NI</categories><comments>10 pages, 11 figures, The 15th international conference on Computer
  communication (ICCC 2002), Mumbai, India. August 11-14, 2002</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a simple network management architecture for supporting both
QoS requirements and organization network management policies is purposed. By
grouping the traffic flows according to the QoS requirements or certain network
management policies, the network resources are effectively controlled. The
purposed architecture is easy to deploy; the gateway is the only equipment that
needs installation, leaving the rest of the system untouched. The architecture
has not significantly degraded the overall system utilization when applying it
to the outgoing bound of the gateway. The architecture can also be implemented
on the wireless LAN at the access point because the architecture is designed in
such the way that it is independent to both the lower and upper protocol
layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2937</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2937</id><created>2011-11-12</created><authors><author><keyname>Gammaitoni</keyname><forenames>Luca</forenames></author></authors><title>Beating the Landauer's limit by trading energy with uncertainty</title><categories>cond-mat.mtrl-sci cs.ET</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the International Technology Roadmap for Semiconductors in the
next 10-15 years the limits imposed by the physics of switch operation will be
the major roadblock for future scaling of the CMOS technology. Among these
limits the most fundamental is represented by the so-called Shannon-von
Neumann-Landauer limit that sets a lower bound to the minimum heat dissipated
per bit erasing operation. Here we show that in a nanoscale switch, operated at
finite temperature T, this limit can be beaten by trading the dissipated energy
with the uncertainty in the distinguishability of switch logic states. We
establish a general relation between the minimum required energy and the
maximum error rate in the switch operation and briefly discuss the potential
applications in the design of future switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2942</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2942</id><created>2011-11-12</created><updated>2013-04-09</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kumar</keyname><forenames>Nirman</forenames></author></authors><title>Down the Rabbit Hole: Robust Proximity Search and Density Estimation in
  Sublinear Space</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a set of $n$ points in $\Re^d$, and parameters $k$ and $\eps$, we present
a data structure that answers $(1+\eps,k)$-\ANN queries in logarithmic time.
Surprisingly, the space used by the data-structure is $\Otilde (n /k)$; that
is, the space used is sublinear in the input size if $k$ is sufficiently large.
Our approach provides a novel way to summarize geometric data, such that
meaningful proximity queries on the data can be carried out using this sketch.
Using this, we provide a sublinear space data-structure that can estimate the
density of a point set under various measures, including:
  \begin{inparaenum}[(i)]
  \item sum of distances of $k$ closest points to the query point, and
  \item sum of squared distances of $k$ closest points to the query point.
  \end{inparaenum}
  Our approach generalizes to other distance based estimation of densities of
similar flavor. We also study the problem of approximating some of these
quantities when using sampling. In particular, we show that a sample of size
$\Otilde (n /k)$ is sufficient, in some restricted cases, to estimate the above
quantities. Remarkably, the sample size has only linear dependency on the
dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2944</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2944</id><created>2011-11-12</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Kitson</keyname><forenames>Stephen</forenames></author><author><keyname>Costello</keyname><forenames>Ben De Lacy</forenames></author><author><keyname>Matranga</keyname><forenames>Mario Ariosto</forenames></author><author><keyname>Younger</keyname><forenames>Daniel</forenames></author></authors><title>Computing with Liquid Crystal Fingers: Models of geometric and logical
  computation</title><categories>nlin.PS cs.ET</categories><comments>submitted Sept 2011</comments><journal-ref>Physical Review E 2011; 84: 061702</journal-ref><doi>10.1103/PhysRevE.84.061702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a voltage is applied across a thin layer of cholesteric liquid crystal,
fingers of cholesteric alignment can form and propagate in the layer. In
computer simulation, based on experimental laboratory results, we demonstrate
that these cholesteric fingers can solve selected problems of computational
geometry, logic and arithmetics. We show that branching fingers approximate a
planar Voronoi diagram, and non-branching fingers produce a convex subdivision
of concave polygons. We also provide a detailed blue-print and simulation of a
one-bit half-adder functioning on the principles of collision-based computing,
where the implementation is via collision of liquid crystal fingers with
obstacles and other fingers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2947</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2947</id><created>2011-11-12</created><authors><author><keyname>Dani</keyname><forenames>Varsha</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Olson</keyname><forenames>Anna</forenames></author></authors><title>Tight bounds on the threshold for permuted k-colorability</title><categories>math.CO cond-mat.stat-mech cs.CC math.PR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If each edge (u,v) of a graph G=(V,E) is decorated with a permutation
pi_{u,v} of k objects, we say that it has a permuted k-coloring if there is a
coloring sigma from V to {1,...,k} such that sigma(v) is different from
pi_{u,v}(sigma(u)) for all (u,v) in E. Based on arguments from statistical
physics, we conjecture that the threshold d_k for permuted k-colorability in
random graphs G(n,m=dn/2), where the permutations on the edges are uniformly
random, is equal to the threshold for standard graph k-colorability. The
additional symmetry provided by random permutations makes it easier to prove
bounds on d_k. By applying the second moment method with these additional
symmetries, and applying the first moment method to a random variable that
depends on the number of available colors at each vertex, we bound the
threshold within an additive constant. Specifically, we show that for any
constant epsilon &gt; 0, for sufficiently large k we have
  2 k ln k - ln k - 2 - epsilon &lt; d_k &lt; 2 k ln k - ln k - 1 + epsilon.
  In contrast, the best known bounds on d_k for standard k-colorability leave
an additive gap of about ln k between the upper and lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2948</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2948</id><created>2011-11-12</created><updated>2011-11-15</updated><authors><author><keyname>Domingues</keyname><forenames>Marcos A.</forenames></author><author><keyname>Jorge</keyname><forenames>Alipio Mario</forenames></author><author><keyname>Soares</keyname><forenames>Carlos</forenames></author></authors><title>Using Contextual Information as Virtual Items on Top-N Recommender
  Systems</title><categories>cs.LG cs.IR</categories><comments>Workshop on Context-Aware Recommender Systems (CARS'09) in
  conjunction with the 3rd ACM Conference on Recommender Systems (RecSys'09)</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, recommender systems for the Web deal with applications that
have two dimensions, users and items. Based on access logs that relate these
dimensions, a recommendation model can be built and used to identify a set of N
items that will be of interest to a certain user. In this paper we propose a
method to complement the information in the access logs with contextual
information without changing the recommendation algorithm. The method consists
in representing context as virtual items. We empirically test this method with
two top-N recommender systems, an item-based collaborative filtering technique
and association rules, on three data sets. The results show that our method is
able to take advantage of the context (new dimensions) when it is informative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2988</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2988</id><created>2011-11-12</created><authors><author><keyname>Baijal</keyname><forenames>Anant</forenames></author><author><keyname>Chauhan</keyname><forenames>Vikram Singh</forenames></author><author><keyname>Jayabarathi</keyname><forenames>T.</forenames></author></authors><title>Application of PSO, Artificial Bee Colony and Bacterial Foraging
  Optimization algorithms to economic load dispatch: An analysis</title><categories>cs.NE cs.AI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, 2011, 467-470</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper illustrates successful implementation of three evolutionary
algorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony
(ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load
dispatch problem (ELD). Power output of each generating unit and optimum fuel
cost obtained using all three algorithms have been compared. The results
obtained show that ABC and BFO algorithms converge to optimal fuel cost with
reduced computational time when compared to PSO for the two example problems
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2991</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2991</id><created>2011-11-13</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>Cyclotomic Constructions of Cyclic Codes with Length Being the Product
  of Two Primes</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are an interesting type of linear codes and have applications in
communication and storage systems due to their efficient encoding and decoding
algorithms. They have been studied for decades and a lot of progress has been
made. In this paper, three types of generalized cyclotomy of order two and
three classes of cyclic codes of length $n_1n_2$ and dimension $(n_1n_2+1)/2$
are presented and analysed, where $n_1$ and $n_2$ are two distinct primes.
Bounds on their minimum odd-like weight are also proved. The three
constructions produce the best cyclic codes in certain cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2993</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2993</id><created>2011-11-13</created><authors><author><keyname>Parhizkar</keyname><forenames>Behrang</forenames></author><author><keyname>Al-Modwahi</keyname><forenames>Ashraf Abbas M.</forenames></author><author><keyname>Lashkari</keyname><forenames>Arash Habibi</forenames></author><author><keyname>Bartaripou</keyname><forenames>Mohammad Mehdi</forenames></author><author><keyname>Babae</keyname><forenames>Hossein Reza</forenames></author></authors><title>A Survey on Web-based AR Applications</title><categories>cs.MM</categories><comments>International Journal of Computer Science Issues (IJCSI), Vol. 8,
  Issue 4, July 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increase of interest in Augmented Reality (AR), the potential uses
of AR are increasing also. It can benefit the user in various fields such as
education, business, medicine, and other. Augmented Reality supports the real
environment with synthetic environment to give more details and meaning to the
objects in the real word. AR refers to a situation in which the goal is to
supplement a user's perception of the real-world through the addition of
virtual objects. This paper is an attempt to make a survey of web-based
Augmented Reality applications and make a comparison among them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.2996</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.2996</id><created>2011-11-13</created><authors><author><keyname>Al-Howaide</keyname><forenames>Ala'a Z.</forenames></author><author><keyname>Doulat</keyname><forenames>Ahmad S.</forenames></author><author><keyname>Khamayseh</keyname><forenames>Yaser M.</forenames></author></authors><title>Performance Evaluation of Different Scheduling Algorithms in WiMAX</title><categories>cs.NI cs.PF</categories><comments>12 pages, 15 figures</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.1, No.5, October 2011, Pages 103-115</journal-ref><doi>10.5121/ijcsea.2011.1509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Worldwide Interoperability for Microwave Access (WiMAX) networks were
expected to be the main Broadband Wireless Access (BWA) technology that
provided several services such as data, voice, and video services including
different classes of Quality of Services (QoS), which in turn were defined by
IEEE 802.16 standard. Scheduling in WiMAX became one of the most challenging
issues, since it was responsible for distributing available resources of the
network among all users; this leaded to the demand of constructing and
designing high efficient scheduling algorithms in order to improve the network
utilization, to increase the network throughput, and to minimize the end-to-end
delay. In this study, we presenedt a simulation study to measure the
performance of several scheduling algorithms in WiMAX, which were Strict
Priority algorithm, Round-Robin (RR), Weighted Round Robin (WRR), Weighted Fair
Queuing (WFQ), Self-Clocked Fair (SCF), and Diff-Serv Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3000</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3000</id><created>2011-11-13</created><authors><author><keyname>H&#xfc;nniger</keyname><forenames>Martin</forenames></author></authors><title>Digital Manifolds and the Theorem of Jordan-Brouwer</title><categories>cs.CV math.GT</categories><comments>34 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an answer to the question given by T.Y.Kong in his article &quot;Can 3-D
Digital Topology be Based on Axiomatically Defined Digital Spaces?&quot; In this
article he asks the question, if so called &quot;good pairs&quot; of neighborhood
relations can be found on the set Z^n such that the existence of digital
manifolds of dimension n-1, that separate their complement in exactly two
connected sets, is guaranteed. To achieve this, we use a technique developed by
M. Khachan et.al. A set given in Z^n is translated into a simplicial complex
that can be used to study the topological properties of the original discrete
point-set. In this way, one is able to define the notion of a (n-1)-dimensional
digital manifold and prove the digital analog of the Jordan-Brouwer-Theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3001</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3001</id><created>2011-11-13</created><authors><author><keyname>Khan</keyname><forenames>Asif Irshad</forenames></author><author><keyname>Qurashi</keyname><forenames>Rizwan Jameel</forenames></author><author><keyname>Khan</keyname><forenames>Usman Ali</forenames></author></authors><title>A Comprehensive Study of Commonly Practiced Heavy and Light Weight
  Software Methodologies</title><categories>cs.SE</categories><comments>10 pages, ISSN (Online): 1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 2, 2011, 441-450</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software has been playing a key role in the development of modern society.
Software industry has an option to choose suitable methodology/process model
for its current needs to provide solutions to give problems. Though some
companies have their own customized methodology for developing their software
but majority agrees that software methodologies fall under two categories that
are heavyweight and lightweight. Heavyweight methodologies (Waterfall Model,
Spiral Model) are also known as the traditional methodologies, and their
focuses are detailed documentation, inclusive planning, and extroverted design.
Lightweight methodologies (XP, SCRUM) are, referred as agile methodologies.
Light weight methodologies focused mainly on short iterative cycles, and rely
on the knowledge within a team. The aim of this paper is to describe the
characteristics of popular heavyweight and lightweight methodologies that are
widely practiced in software industries. We have discussed the strengths and
weakness of the selected models. Further we have discussed the strengths and
weakness between the two opponent methodologies and some criteria is also
illustrated that help project managers for the selection of suitable model for
their projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3010</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3010</id><created>2011-11-13</created><authors><author><keyname>Tiwari</keyname><forenames>Ayu</forenames></author><author><keyname>Sanyal</keyname><forenames>Sudip</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Knapskog</keyname><forenames>Svein Johan</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Multi-Factor Security Protocol for Wireless Payment - Secure Web
  Authentication using Mobile Devices</title><categories>cs.CR</categories><comments>8 Pages, 3 Figures; IADIS International Conference on Applied
  Computing Proceedings of the IADIS International Conference on Applied
  Computing, Salamanca, Spain, 18-20 February 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous Web access authentication systems often use either the Web or the
Mobile channel individually to confirm the claimed identity of the remote user.
This paper proposes a new protocol using multifactor authentication system that
is both secure and highly usable. It uses a novel approach based on Transaction
Identification Code and SMS to enforce extra security level with the
traditional Login/password system. The system provides a highly secure
environment that is simple to use and deploy, that does not require any change
in infrastructure or protocol of wireless networks. This Protocol for Wireless
Payment is extended to provide two way authentications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3013</identifier>
 <datestamp>2012-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3013</id><created>2011-11-13</created><updated>2012-02-13</updated><authors><author><keyname>Alvim</keyname><forenames>M&#xe1;rio S.</forenames></author></authors><title>Formal approaches to information hiding: An analysis of interactive
  systems, statistical disclosure control, and refinement of specifications</title><categories>cs.CR</categories><comments>Manuscript of the PhD thesis of M\'ario S. Alvim. arXiv admin note:
  text overlap with arXiv:0705.3503 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis we consider the problem of information hiding in the scenarios
of interactive systems, statistical disclosure control, and refinement of
specifications. We apply quantitative approaches to information flow in the
first two cases, and we propose improvements for the usual solutions based on
process equivalences for the third case. In the first scenario we consider the
problem of defining the information leakage in interactive systems where
secrets and observables can alternate during the computation and influence each
other. We show that the information-theoretic approach which interprets such
systems as (simple) noisy channels is not valid. The principle can be recovered
if we consider channels with memory and feedback. We also propose the use of
directed information from input to output as the real measure of leakage in
interactive systems. In the second scenario we consider the problem of
statistical disclosure control, which concerns how to reveal accurate
statistics about a set of respondents while preserving the privacy of
individuals. We focus on the concept of differential privacy, a notion that has
become very popular in the database community. We show how to model the query
system in terms of an information-theoretic channel, and we compare the notion
of differential privacy with that of min-entropy leakage.In the third scenario
we address the problem of using process equivalences to characterize
information-hiding properties. We show that, in the presence of nondeterminism,
this approach may rely on the assumption that the scheduler &quot;works for the
benefit of the protocol&quot;, and this is often not a safe assumption. We present a
formalism in which we can specify admissible schedulers and, correspondingly,
safe versions of complete-trace equivalence and bisimulation, and we show that
safe equivalences can be used to establish information-hiding properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3022</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3022</id><created>2011-11-13</created><authors><author><keyname>Yang</keyname><forenames>Yiling</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Cao</keyname><forenames>Jiannong</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoxing</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>Design of a Sliding Window over Asynchronous Event Streams</title><categories>cs.DC</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of sensing and monitoring applications motivates adoption
of the event stream model of computation. Though sliding windows are widely
used to facilitate effective event stream processing, it is greatly challenged
when the event sources are distributed and asynchronous. To address this
challenge, we first show that the snapshots of the asynchronous event streams
within the sliding window form a convex distributive lattice (denoted by
Lat-Win). Then we propose an algorithm to maintain Lat-Win at runtime. The
Lat-Win maintenance algorithm is implemented and evaluated on the open-source
context-aware middleware we developed. The evaluation results first show the
necessity of adopting sliding windows over asynchronous event streams. Then
they show the performance of detecting specified predicates within Lat-Win,
even when faced with dynamic changes in the computing environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3025</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3025</id><created>2011-11-09</created><authors><author><keyname>Thomas</keyname><forenames>Andr&#xe9;</forenames><affiliation>CRAN</affiliation></author><author><keyname>Trentesaux</keyname><forenames>Damien</forenames></author><author><keyname>Valckenaers</keyname><forenames>Paul</forenames></author></authors><title>Intelligent Distributed Production Control</title><categories>cs.SY</categories><comments>Journal of Intelligent Manufacturing (2011) 7</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This editorial introduces the special issue of the Springer journal, Journal
of Intelligent Manufacturing, on intelligent distributed production control.
This special issue contains selected papers presented at the 13th IFAC
Symposium on Information Control Problems in Manufacturing - INCOM'2009
(Bakhtadze and Dolgui, 2009). The papers in this special issue were selected
because of their high quality and their specific way of addressing the variety
of issues dealing with intelligent distributed production control. Previous
global discussions about the state of the art in intelligent distributed
production control are provided, as well as exploratory guidelines for future
research in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3033</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3033</id><created>2011-11-13</created><updated>2012-12-02</updated><authors><author><keyname>Szalay-Beko</keyname><forenames>Mate</forenames></author><author><keyname>Palotai</keyname><forenames>Robin</forenames></author><author><keyname>Szappanos</keyname><forenames>Balazs</forenames></author><author><keyname>Kovacs</keyname><forenames>Istvan A.</forenames></author><author><keyname>Papp</keyname><forenames>Balazs</forenames></author><author><keyname>Csermely</keyname><forenames>Peter</forenames></author></authors><title>ModuLand plug-in for Cytoscape: determination of hierarchical layers of
  overlapping network modules and community centrality</title><categories>physics.comp-ph cond-mat.dis-nn cs.SI q-bio.MN</categories><comments>39 pages, 1 figure and a Supplement with 9 figures and 10 tables</comments><journal-ref>Bioinformatics (2012) 28: 2202-2204</journal-ref><doi>10.1093/bioinformatics/bts352</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summary: The ModuLand plug-in provides Cytoscape users an algorithm for
determining extensively overlapping network modules. Moreover, it identifies
several hierarchical layers of modules, where meta-nodes of the higher
hierarchical layer represent modules of the lower layer. The tool assigns
module cores, which predict the function of the whole module, and determines
key nodes bridging two or multiple modules. The plug-in has a detailed
JAVA-based graphical interface with various colouring options. The ModuLand
tool can run on Windows, Linux, or Mac OS. We demonstrate its use on protein
structure and metabolic networks. Availability: The plug-in and its user guide
can be downloaded freely from: http://www.linkgroup.hu/modules.php. Contact:
csermely.peter@med.semmelweis-univ.hu Supplementary information: Supplementary
information is available at Bioinformatics online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3041</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3041</id><created>2011-11-13</created><authors><author><keyname>Li</keyname><forenames>Shuang</forenames></author><author><keyname>Zheng</keyname><forenames>Zizhan</forenames></author><author><keyname>Ekici</keyname><forenames>Eylem</forenames></author><author><keyname>Shroff</keyname><forenames>Ness</forenames></author></authors><title>Maximizing System Throughput by Cooperative Sensing in Cognitive Radio
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio Networks allow unlicensed users to opportunistically access
the licensed spectrum without causing disruptive interference to the primary
users (PUs). One of the main challenges in CRNs is the ability to detect PU
transmissions. Recent works have suggested the use of secondary user (SU)
cooperation over individual sensing to improve sensing accuracy. In this paper,
we consider a CRN consisting of a single PU and multiple SUs to study the
problem of maximizing the total expected system throughput. We propose a
Bayesian decision rule based algorithm to solve the problem optimally with a
constant time complexity. To prioritize PU transmissions, we re-formulate the
throughput maximization problem by adding a constraint on the PU throughput.
The constrained optimization problem is shown to be NP-hard and solved via a
greedy algorithm with pseudo-polynomial time complexity that achieves strictly
greater than 1/2 of the optimal solution. We also investigate the case for
which a constraint is put on the sensing time overhead, which limits the number
of SUs that can participate in cooperative sensing. We reveal that the system
throughput is monotonic over the number of SUs chosen for sensing. We
illustrate the efficacy of the performance of our algorithms via a numerical
investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3048</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3048</id><created>2011-11-13</created><updated>2014-02-11</updated><authors><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Desai</keyname><forenames>Devendra</forenames></author></authors><title>On a Connection Between Small Set Expansions and Modularity Clustering
  in Social Networks</title><categories>cs.SI cs.CC physics.soc-ph</categories><comments>Information Processing Letters, 2014</comments><msc-class>68Q25, 68W25</msc-class><acm-class>F.2.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore a connection between two seemingly different
problems from two different domains: the small-set expansion problem studied in
unique games conjecture, and a popular community finding approach for social
networks known as the modularity clustering approach. We show that a
sub-exponential time algorithm for the small-set expansion problem leads to a
sub-exponential time constant factor approximation for some hard input
instances of the modularity clustering problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3056</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3056</id><created>2011-11-13</created><authors><author><keyname>Ramasubramanian</keyname><forenames>N.</forenames></author><author><keyname>V.</keyname><forenames>Srinivas V.</forenames></author><author><keyname>Gounden</keyname><forenames>N. Ammasai</forenames></author></authors><title>Performance of Cache Memory Subsystems for Multicore Architectures</title><categories>cs.AR</categories><comments>13 pages, 8 figures</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA), Vol. 1, No. 5, pp. 59-71, 2011</journal-ref><doi>10.5121/ijcsea.2011.1506</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Advancements in multi-core have created interest among many research groups
in finding out ways to harness the true power of processor cores. Recent
research suggests that on-board component such as cache memory plays a crucial
role in deciding the performance of multi-core systems. In this paper,
performance of cache memory is evaluated through the parameters such as cache
access time, miss rate and miss penalty. The influence of cache parameters over
execution time is also discussed. Results obtained from simulated studies of
multi-core environments with different instruction set architectures (ISA) like
ALPHA and X86 are produced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3063</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3063</id><created>2011-11-13</created><updated>2012-12-10</updated><authors><author><keyname>Angrishi</keyname><forenames>Kishore</forenames></author><author><keyname>Killat</keyname><forenames>Ulrich</forenames></author></authors><title>An Approach using Demisubmartingales for the Stochastic Analysis of
  Networks</title><categories>cs.NI</categories><comments>4 pages, 2 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic network calculus is the probabilistic version of the network
calculus, which uses envelopes to perform probabilistic analysis of queueing
networks. The accuracy of probabilistic end-to-end delay or backlog bounds
computed using network calculus has always been a concern. In this paper, we
propose novel end-to-end probabilistic bounds based on demisubmartingale
inequalities which improve the existing bounds for the tandem networks of
GI/GI/1 queues. In particular, we show that reasonably accurate bounds are
achieved by comparing the new bounds with the existing results for a network of
M/M/1 queues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3069</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3069</id><created>2011-11-13</created><authors><author><keyname>Satish</keyname><forenames>Laika</forenames><affiliation>Lecturer, Faculty of Computing and IT, King Abdul Aziz University, Rabigh, Saudi Arabia</affiliation></author><author><keyname>Halawani</keyname><forenames>Sami</forenames><affiliation>Dean, Faculty of Computing and IT, King Abdul Aziz University Rabigh, Saudi Arabia</affiliation></author></authors><title>A fusion algorithm for joins based on collections in Odra (Object
  Database for Rapid Application development)</title><categories>cs.DB</categories><comments>ISSN (Online): 1694-0814 http://www.IJCSI.org</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 2, 2011, 289-293</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the functionality of a currently under development
database programming methodology called ODRA (Object Database for Rapid
Application development) which works fully on the object oriented principles.
The database programming language is called SBQL (Stack based query language).
We discuss some concepts in ODRA for e.g. the working of ODRA, how ODRA runtime
environment operates, the interoperability of ODRA with .net and java .A view
of ODRA's working with web services and xml. Currently the stages under
development in ODRA are query optimization. So we present the prior work that
is done in ODRA related to Query optimization and we also present a new fusion
algorithm of how ODRA can deal with joins based on collections like set, lists,
and arrays for query optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3093</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3093</id><created>2011-11-13</created><updated>2012-10-26</updated><authors><author><keyname>Anashin</keyname><forenames>Vladimir</forenames></author><author><keyname>Khrennikov</keyname><forenames>Andrei</forenames></author><author><keyname>Yurova</keyname><forenames>Ekaterina</forenames></author></authors><title>T-functions revisited: New criteria for bijectivity/transitivity</title><categories>cs.CR math.DS</categories><msc-class>94A60, 11S82, 11T71</msc-class><journal-ref>Designs, Codes and Cryptography, Volume 71, Issue 3 (2014), Page
  383-407</journal-ref><doi>10.1007/s10623-012-9741-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents new criteria for bijectivity/transitivity of T-functions
and fast knapsack-like algorithm of evaluation of a T-function. Our approach is
based on non-Archimedean ergodic theory: Both the criteria and algorithm use
van der Put series to represent 1-Lipschitz $p$-adic functions and to study
measure-preservation/ergodicity of these.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3096</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3096</id><created>2011-11-14</created><authors><author><keyname>Gobjuka</keyname><forenames>Hassan</forenames></author><author><keyname>Ahmat</keyname><forenames>Kamal</forenames></author></authors><title>vFlow: A GUI-Based Tool for Building Batch Applications for Cloud
  Computing</title><categories>cs.SE cs.DC</categories><comments>IEEE INFOCOM 2011 Demo Session, 2 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce vFlow - A framework for rapid designing of batch
processing applications for Cloud Computing environment. vFlow batch processing
system extracts tasks from the vPlans diagrams, systematically captures the
dynamics in batch application management tasks, and translates them to Cloud
environment API, named vDocuments, that can be used to execute batch processing
applications. vDocuments do not only enable the complete execution of low-level
configuration management tasks, but also allow the construction of more
sophisticated tasks, while imposing additional reasoning logic to realize batch
application management objectives in Cloud environments. We present the design
of the vFlow framework and illustrate its utility by presenting the
implementation of several sophisticated operational tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3097</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3097</id><created>2011-11-14</created><updated>2012-04-07</updated><authors><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>The tile assembly model is intrinsically universal</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the abstract Tile Assembly Model (aTAM) of nanoscale
self-assembly is intrinsically universal. This means that there is a single
tile assembly system U that, with proper initialization, simulates any tile
assembly system T. The simulation is &quot;intrinsic&quot; in the sense that the
self-assembly process carried out by U is exactly that carried out by T, with
each tile of T represented by an m x m &quot;supertile&quot; of U. Our construction works
for the full aTAM at any temperature, and it faithfully simulates the
deterministic or nondeterministic behavior of each T.
  Our construction succeeds by solving an analog of the cell differentiation
problem in developmental biology: Each supertile of U, starting with those in
the seed assembly, carries the &quot;genome&quot; of the simulated system T. At each
location of a potential supertile in the self-assembly of U, a decision is made
whether and how to express this genome, i.e., whether to generate a supertile
and, if so, which tile of T it will represent. This decision must be achieved
using asynchronous communication under incomplete information, but it achieves
the correct global outcome(s).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3106</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3106</id><created>2011-11-14</created><authors><author><keyname>Peled</keyname><forenames>Doron</forenames><affiliation>Bar Ilan University</affiliation></author><author><keyname>Schewe</keyname><forenames>Sven</forenames><affiliation>University of Liverpool</affiliation></author></authors><title>Practical Distributed Control Synthesis</title><categories>cs.LO cs.SY</categories><comments>In Proceedings INFINITY 2011, arXiv:1111.2678</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011, pp. 2-17</journal-ref><doi>10.4204/EPTCS.73.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classic distributed control problems have an interesting dichotomy: they are
either trivial or undecidable. If we allow the controllers to fully
synchronize, then synthesis is trivial. In this case, controllers can
effectively act as a single controller with complete information, resulting in
a trivial control problem. But when we eliminate communication and restrict the
supervisors to locally available information, the problem becomes undecidable.
In this paper we argue in favor of a middle way. Communication is, in most
applications, expensive, and should hence be minimized. We therefore study a
solution that tries to communicate only scarcely and, while allowing
communication in order to make joint decision, favors local decisions over
joint decisions that require communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3107</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3107</id><created>2011-11-14</created><authors><author><keyname>Spelten</keyname><forenames>Alex</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Thomas</keyname><forenames>Wolfgang</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Winter</keyname><forenames>Sarah</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Trees over Infinite Structures and Path Logics with Synchronization</title><categories>cs.LO</categories><comments>In Proceedings INFINITY 2011, arXiv:1111.2678</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011, pp. 20-34</journal-ref><doi>10.4204/EPTCS.73.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide decidability and undecidability results on the model-checking
problem for infinite tree structures. These tree structures are built from
sequences of elements of infinite relational structures. More precisely, we
deal with the tree iteration of a relational structure M in the sense of
Shelah-Stupp. In contrast to classical results where model-checking is shown
decidable for MSO-logic, we show decidability of the tree model-checking
problem for logics that allow only path quantifiers and chain quantifiers
(where chains are subsets of paths), as they appear in branching time logics;
however, at the same time the tree is enriched by the equal-level relation
(which holds between vertices u, v if they are on the same tree level). We
separate cleanly the tree logic from the logic used for expressing properties
of the underlying structure M. We illustrate the scope of the decidability
results by showing that two slight extensions of the framework lead to
undecidability. In particular, this applies to the (stronger) tree iteration in
the sense of Muchnik-Walukiewicz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3108</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3108</id><created>2011-11-14</created><authors><author><keyname>Fribourg</keyname><forenames>Laurent</forenames><affiliation>Laboratoire Specification et Verification</affiliation></author><author><keyname>Revol</keyname><forenames>Bertrand</forenames><affiliation>SATIE</affiliation></author><author><keyname>Soulat</keyname><forenames>Romain</forenames><affiliation>Laboratoire Specification et Verification</affiliation></author></authors><title>Synthesis of Switching Rules for Ensuring Reachability Properties of
  Sampled Linear Systems</title><categories>cs.LO cs.SY</categories><comments>In Proceedings INFINITY 2011, arXiv:1111.2678</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011, pp. 35-48</journal-ref><doi>10.4204/EPTCS.73.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider here systems with piecewise linear dynamics that are periodically
sampled with a given period {\tau} . At each sampling time, the mode of the
system, i.e., the parameters of the linear dynamics, can be switched, according
to a switching rule. Such systems can be modelled as a special form of hybrid
automata, called &quot;switched systems&quot;, that are automata with an infinite real
state space. The problem is to find a switching rule that guarantees the system
to still be in a given area V at the next sampling time, and so on
indefinitely. In this paper, we will consider two approaches: the indirect one
that abstracts the system under the form of a finite discrete event system, and
the direct one that works on the continuous state space.
  Our methods rely on previous works, but we specialize them to a simplified
context (linearity, periodic switching instants, absence of control input),
which is motivated by the features of a focused case study: a DC-DC boost
converter built by electronics laboratory SATIE (ENS Cachan). Our enhanced
methods allow us to treat successfully this real-life example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3109</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3109</id><created>2011-11-14</created><authors><author><keyname>Ciaffaglione</keyname><forenames>Alberto</forenames><affiliation>University of Udine</affiliation></author></authors><title>A coinductive semantics of the Unlimited Register Machine</title><categories>cs.LO</categories><comments>In Proceedings INFINITY 2011, arXiv:1111.2678</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011, pp. 49-63</journal-ref><doi>10.4204/EPTCS.73.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exploit (co)inductive specifications and proofs to approach the evaluation
of low-level programs for the Unlimited Register Machine (URM) within the Coq
system, a proof assistant based on the Calculus of (Co)Inductive Constructions
type theory. Our formalization allows us to certify the implementation of
partial functions, thus it can be regarded as a first step towards the
development of a workbench for the formal analysis and verification of both
converging and diverging computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3110</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3110</id><created>2011-11-14</created><authors><author><keyname>Krause</keyname><forenames>Christian</forenames><affiliation>Hasso Plattner Institute</affiliation></author><author><keyname>Giese</keyname><forenames>Holger</forenames><affiliation>Hasso Plattner Institute</affiliation></author></authors><title>Model Checking Probabilistic Real-Time Properties for Service-Oriented
  Systems with Service Level Agreements</title><categories>cs.SE cs.LO cs.PF</categories><comments>In Proceedings INFINITY 2011, arXiv:1111.2678</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011, pp. 64-78</journal-ref><doi>10.4204/EPTCS.73.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The assurance of quality of service properties is an important aspect of
service-oriented software engineering. Notations for so-called service level
agreements (SLAs), such as the Web Service Level Agreement (WSLA) language,
provide a formal syntax to specify such assurances in terms of (legally
binding) contracts between a service provider and a customer. On the other
hand, formal methods for verification of probabilistic real-time behavior have
reached a level of expressiveness and efficiency which allows to apply them in
real-world scenarios. In this paper, we suggest to employ the recently
introduced model of Interval Probabilistic Timed Automata (IPTA) for formal
verification of QoS properties of service-oriented systems. Specifically, we
show that IPTA in contrast to Probabilistic Timed Automata (PTA) are able to
capture the guarantees specified in SLAs directly. A particular challenge in
the analysis of IPTA is the fact that their naive semantics usually yields an
infinite set of states and infinitely-branching transitions. However, using
symbolic representations, IPTA can be analyzed rather efficiently. We have
developed the first implementation of an IPTA model checker by extending the
PRISM tool and show that model checking IPTA is only slightly more expensive
than model checking comparable PTA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3111</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3111</id><created>2011-11-14</created><authors><author><keyname>Tomita</keyname><forenames>Takashi</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Hagihara</keyname><forenames>Shigeki</forenames><affiliation>Tokyo Institute of Technology</affiliation></author><author><keyname>Yonezaki</keyname><forenames>Naoki</forenames><affiliation>Tokyo Institute of Technology</affiliation></author></authors><title>A Probabilistic Temporal Logic with Frequency Operators and Its Model
  Checking</title><categories>cs.LO</categories><comments>In Proceedings INFINITY 2011, arXiv:1111.2678</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 73, 2011, pp. 79-93</journal-ref><doi>10.4204/EPTCS.73.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Computation Tree Logic (PCTL) and Continuous Stochastic Logic
(CSL) are often used to describe specifications of probabilistic properties for
discrete time and continuous time, respectively. In PCTL and CSL, the
possibility of executions satisfying some temporal properties can be
quantitatively represented by the probabilistic extension of the path
quantifiers in their basic Computation Tree Logic (CTL), however, path formulae
of them are expressed via the same operators in CTL. For this reason, both of
them cannot represent formulae with quantitative temporal properties, such as
those of the form &quot;some properties hold to more than 80% of time points (in a
certain bounded interval) on the path.&quot; In this paper, we introduce a new
temporal operator which expressed the notion of frequency of events, and define
probabilistic frequency temporal logic (PFTL) based on CTL\star. As a result,
we can easily represent the temporal properties of behavior in probabilistic
systems. However, it is difficult to develop a model checker for the full PFTL,
due to rich expressiveness. Accordingly, we develop a model-checking algorithm
for the CTL-like fragment of PFTL against finite-state Markov chains, and an
approximate model-checking algorithm for the bounded Linear Temporal Logic
(LTL) -like fragment of PFTL against countable-state Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3114</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3114</id><created>2011-11-14</created><updated>2011-12-21</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Diameter of Cayley graphs of permutation groups generated by
  transposition trees</title><categories>cs.DM cs.DS math.CO</categories><comments>This is an extension of arXiv:1106.5353</comments><journal-ref>Journal of Combinatorial Mathematics and Combinatorial Computing,
  vol. 84, pp. 29-40, February 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Gamma$ be a Cayley graph of the permutation group generated by a
transposition tree $T$ on $n$ vertices. In an oft-cited paper
\cite{Akers:Krishnamurthy:1989} (see also \cite{Hahn:Sabidussi:1997}), it is
shown that the diameter of the Cayley graph $\Gamma$ is bounded as
$$\diam(\Gamma) \le \max_{\pi \in S_n}{c(\pi)-n+\sum_{i=1}^n
\dist_T(i,\pi(i))},$$ where the maximization is over all permutations $\pi$,
$c(\pi)$ denotes the number of cycles in $\pi$, and $\dist_T$ is the distance
function in $T$. In this work, we first assess the performance (the sharpness
and strictness) of this upper bound. We show that the upper bound is sharp for
all trees of maximum diameter and also for all trees of minimum diameter, and
we exhibit some families of trees for which the bound is strict. We then show
that for every $n$, there exists a tree on $n$ vertices, such that the
difference between the upper bound and the true diameter value is at least
$n-4$.
  Observe that evaluating this upper bound requires on the order of $n!$ (times
a polynomial) computations. We provide an algorithm that obtains an estimate of
the diameter, but which requires only on the order of (polynomial in) $n$
computations; furthermore, the value obtained by our algorithm is less than or
equal to the previously known diameter upper bound. This result is possible
because our algorithm works directly with the transposition tree on $n$
vertices and does not require examining any of the permutations (only the proof
requires examining the permutations). For all families of trees examined so
far, the value $\beta$ computed by our algorithm happens to also be an upper
bound on the diameter, i.e.
  $$\diam(\Gamma) \le \beta \le \max_{\pi \in S_n}{c(\pi)-n+\sum_{i=1}^n
\dist_T(i,\pi(i))}.$$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3122</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3122</id><created>2011-11-14</created><authors><author><keyname>Eshkol</keyname><forenames>Iris</forenames><affiliation>LLL</affiliation></author><author><keyname>Maurel</keyname><forenames>D.</forenames><affiliation>LI</affiliation></author><author><keyname>Friburger</keyname><forenames>Nathalie</forenames><affiliation>LI</affiliation></author></authors><title>ESLO: from transcription to speakers' personal information annotation</title><categories>cs.CL</categories><comments>LREC2010, Malta (2010)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the preliminary works to put online a French oral corpus
and its transcription. This corpus is the Socio-Linguistic Survey in Orleans,
realized in 1968. First, we numerized the corpus, then we handwritten
transcribed it with the Transcriber software adding different tags about
speakers, time, noise, etc. Each document (audio file and XML file of the
transcription) was described by a set of metadata stored in an XML format to
allow an easy consultation. Second, we added different levels of annotations,
recognition of named entities and annotation of personal information about
speakers. This two annotation tasks used the CasSys system of transducer
cascades. We used and modified a first cascade to recognize named entities.
Then we built a second cascade to annote the designating entities, i.e.
information about the speaker. These second cascade parsed the named entity
annotated corpus. The objective is to locate information about the speaker and,
also, what kind of information can designate him/her. These two cascades was
evaluated with precision and recall measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3124</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3124</id><created>2011-11-14</created><authors><author><keyname>SaiToh</keyname><forenames>Akira</forenames></author></authors><title>A multiprecision matrix calculation library and its extension library
  for a matrix-product-state simulation of quantum computing</title><categories>cs.MS quant-ph</categories><comments>5 pages, 1 figure, technical report (a software overview)</comments><report-no>QIT2011-80</report-no><msc-class>97N80, 81-01</msc-class><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A C++ library, named ZKCM, has been developed for the purpose of
multiprecision matrix calculations, which is based on the GNU MP and MPFR
libraries. It is especially convenient for writing programs involving
tensor-product operations, tracing-out operations, and singular-value
decompositions. Its extension library, ZKCM_QC, for simulating quantum
computing has been developed using the time-dependent matrix-product-state
simulation method. This report gives a brief introduction to the libraries with
sample programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3125</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3125</id><created>2011-11-14</created><authors><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author></authors><title>Proving the power of postselection</title><categories>cs.CC quant-ph</categories><comments>26 pages. This is a heavily improved version of arXiv:1102.0666</comments><journal-ref>Fundamenta Informaticae, Vol. 123, No. 1, pp. 107-134, 2013</journal-ref><doi>10.3233/FI-2013-803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a widely believed, though unproven, conjecture that the capability of
postselection increases the language recognition power of both probabilistic
and quantum polynomial-time computers. It is also unknown whether
polynomial-time quantum machines with postselection are more powerful than
their probabilistic counterparts with the same resource restrictions. We
approach these problems by imposing additional constraints on the resources to
be used by the computer, and are able to prove for the first time that
postselection does augment the computational power of both classical and
quantum computers, and that quantum does outperform probabilistic in this
context, under simultaneous time and space bounds in a certain range. We also
look at postselected versions of space-bounded classes, as well as those
corresponding to error-free and one-sided error recognition, and provide
classical characterizations. It is shown that $\mathsf{NL}$ would equal
$\mathsf{RL}$ if the randomized machines had the postselection capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3127</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3127</id><created>2011-11-14</created><authors><author><keyname>Arratia</keyname><forenames>Argimiro</forenames></author><author><keyname>Caba&#xf1;a</keyname><forenames>Alejandra</forenames></author></authors><title>Tracing the temporal evolution of clusters in a financial stock market</title><categories>cs.CE math.ST q-fin.ST stat.TH</categories><comments>22 pages, 3 figures (submitted for publication)</comments><msc-class>62P05, 68R10</msc-class><doi>10.1007/s10614-012-9327-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a methodology for clustering financial time series of stocks'
returns, and a graphical set-up to quantify and visualise the evolution of
these clusters through time. The proposed graphical representation allows for
the application of well known algorithms for solving classical combinatorial
graph problems, which can be interpreted as problems relevant to portfolio
design and investment strategies. We illustrate this graph representation of
the evolution of clusters in time and its use on real data from the Madrid
Stock Exchange market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3152</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3152</id><created>2011-11-14</created><authors><author><keyname>Tolone</keyname><forenames>Elsa</forenames><affiliation>LIGM, FaMAF</affiliation></author><author><keyname>De La Clergerie</keyname><forenames>&#xc9;ric</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Benoit</keyname><forenames>Sagot</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>\'Evaluation de lexiques syntaxiques par leur int\'egartion dans
  l'analyseur syntaxiques FRMG</title><categories>cs.CL</categories><comments>30\`eme Colloque international sur le Lexique et la Grammaire
  (LGC'11), Nicosie : Chypre (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we evaluate various French lexica with the parser FRMG: the
Lefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,
the lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,
obtained by merging with DICOVALENCE and partial manual validation. For this,
all these lexica have been converted to the format of the Lefff, Alexina
format. The evaluation was made on the part of the EASy corpus used in the
first evaluation campaign Passage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3153</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3153</id><created>2011-11-14</created><authors><author><keyname>Ioannidou</keyname><forenames>Kyriaki</forenames><affiliation>LTTL</affiliation></author><author><keyname>Tolone</keyname><forenames>Elsa</forenames><affiliation>LIGM, FaMAF</affiliation></author></authors><title>Construction du lexique LGLex \`a partir des tables du Lexique-Grammaire
  des verbes du grec moderne</title><categories>cs.CL</categories><comments>30\`eme Colloque international sur le Lexique et la Grammaire
  (LGC'11), Nicosie : Chypre (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we summerize the work done on the resources of Modern Greek on
the Lexicon-Grammar of verbs. We detail the definitional features of each
table, and all changes made to the names of features to make them consistent.
Through the development of the table of classes, including all the features, we
have considered the conversion of tables in a syntactic lexicon: LGLex. The
lexicon, in plain text format or XML, is generated by the LGExtract tool
(Constant &amp; Tolone, 2010). This format is directly usable in applications of
Natural Language Processing (NLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3160</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3160</id><created>2011-11-14</created><authors><author><keyname>Kim</keyname><forenames>Taejoon</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>On the Spatial Degrees of Freedom of Multicell and Multiuser MIMO
  Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE TIT on Aug 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the converse and achievability for the degrees of freedom of the
multicellular multiple-input multiple-output (MIMO) multiple access channel
(MAC) with constant channel coefficients. We assume L&gt;1 homogeneous cells with
K&gt;0 users per cell where the users have M antennas and the base stations are
equipped with N antennas. The degrees of freedom outer bound for this L-cell
and K-user MIMO MAC is formulated. The characterized outer bound uses insight
from a limit on the total degrees of freedom for the L-cell heterogeneous MIMO
network. We also show through an example that a scheme selecting a transmitter
and performing partial message sharing outperforms a multiple distributed
transmission strategy in terms of the total degrees of freedom. Simple linear
schemes attaining the outer bound (i.e., those achieving the optimal degrees of
freedom) are explores for a few cases. The conditions for the required spatial
dimensions attaining the optimal degrees of freedom are characterized in terms
of K, L, and the number of transmit streams. The optimal degrees of freedom for
the two-cell MIMO MAC are examined by using transmit zero forcing and null
space interference alignment and subsequently, simple receive zero forcing is
shown to provide the optimal degrees of freedom for L&gt;1. Interestingly, it can
be shown that the developed linear schemes characterize the optimal degrees of
freedom with the minimum possible numbers of transmit and receive antennas when
assuming a single stream per user. By the uplink and downlink duality, the
degrees of freedom results in this paper are also applicable to the downlink.
In the downlink scenario, we study the degrees of freedom of L-cell MIMO
interference channel exploring multiuser diversity. Strong convergence modes of
the instantaneous degrees of freedom as the number of users increases are
characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3163</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3163</id><created>2011-11-14</created><updated>2011-11-15</updated><authors><author><keyname>Blasco</keyname><forenames>Francisco Lazaro</forenames></author><author><keyname>Rossetto</keyname><forenames>Francesco</forenames></author></authors><title>On the Derivation of Optimal Partial Successive Interference
  Cancellation</title><categories>cs.IT math.IT</categories><comments>IEEE GLOBECOM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The necessity of accurate channel estimation for Successive and Parallel
Interference Cancellation is well known. Iterative channel estimation and
channel decoding (for instance by means of the Expectation-Maximization
algorithm) is particularly important for these multiuser detection schemes in
the presence of time varying channels, where a high density of pilots is
necessary to track the channel. This paper designs a method to analytically
derive a weighting factor $\alpha$, necessary to improve the efficiency of
interference cancellation in the presence of poor channel estimates. Moreover,
this weighting factor effectively mitigates the presence of incorrect decisions
at the output of the channel decoder. The analysis provides insight into the
properties of such interference cancellation scheme and the proposed approach
significantly increases the effectiveness of Successive Interference
Cancellation under the presence of channel estimation errors, which leads to
gains of up to 3 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3165</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3165</id><created>2011-11-14</created><authors><author><keyname>Kumar</keyname><forenames>Pardeep</forenames></author><author><keyname>Sehgal</keyname><forenames>Vivek Kumar</forenames></author><author><keyname>Chauhan</keyname><forenames>Durg Singh</forenames></author><author><keyname>Gupta</keyname><forenames>P. K.</forenames></author><author><keyname>Diwakar</keyname><forenames>Manoj</forenames></author></authors><title>Effective Ways of Secure, Private and Trusted Cloud Computing</title><categories>cs.DC cs.CR</categories><comments>10 pages,6 figures; IJCSI International Journal of Computer Science
  Issues, May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is an Internet-based computing, where shared resources,
software and information, are provided to computers and devices on-demand. It
provides people the way to share distributed resources and services that belong
to different organization. Since cloud computing uses distributed resources in
open environment, thus it is important to provide the security and trust to
share the data for developing cloud computing applications. In this paper we
assess how can cloud providers earn their customers' trust and provide the
security, privacy and reliability, when a third party is processing sensitive
data in a remote machine located in various countries? A concept of utility
cloud has been represented to provide the various services to the users.
Emerging technologies can help address the challenges of Security, Privacy and
Trust in cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3166</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3166</id><created>2011-11-14</created><authors><author><keyname>Blasco</keyname><forenames>Francisco Lazaro</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author></authors><title>On the Concatenation of Non-Binary Random Linear Fountain Codes with
  Maximum Distance Separable Codes</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Communications 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel fountain coding scheme has been introduced. The scheme consists of a
parallel concatenation of a MDS block code with a LRFC code, both constructed
over the same field, $F_q$. The performance of the concatenated fountain coding
scheme has been analyzed through derivation of tight bounds on the probability
of decoding failure as a function of the overhead. It has been shown how the
concatenated scheme performs as well as LRFC codes in channels characterized by
high erasure probabilities, whereas they provide failure probabilities lower by
several orders of magnitude at moderate/low erasure probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3176</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3176</id><created>2011-11-14</created><authors><author><keyname>Smilkov</keyname><forenames>Daniel</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>The influence of the network topology on epidemic spreading</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 6 figures</comments><doi>10.1103/PhysRevE.85.016114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The influence of the network's structure on the dynamics of spreading
processes has been extensively studied in the last decade. Important results
that partially answer this question show a weak connection between the
macroscopic behavior of these processes and specific structural properties in
the network, such as the largest eigenvalue of a topology related matrix.
However, little is known about the direct influence of the network topology on
microscopic level, such as the influence of the (neighboring) network on the
probability of a particular node's infection. To answer this question, we
derive both an upper and a lower bound for the probability that a particular
node is infective in a susceptible-infective-susceptible model for two cases of
spreading processes: reactive and contact processes. The bounds are derived by
considering the $n-$hop neighborhood of the node; the bounds are tighter as one
uses a larger $n-$hop neighborhood to calculate them. Consequently, using local
information for different neighborhood sizes, we assess the extent to which the
topology influences the spreading process, thus providing also a strong
macroscopic connection between the former and the latter. Our findings are
complemented by numerical results for a real-world e-mail network. A very good
estimate for the infection density $\rho$ is obtained using only 2-hop
neighborhoods which account for 0.4% of the entire network topology on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3182</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3182</id><created>2011-11-14</created><authors><author><keyname>Veness</keyname><forenames>Joel</forenames></author><author><keyname>Ng</keyname><forenames>Kee Siong</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>Context Tree Switching</title><categories>cs.IT math.IT</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the Context Tree Switching technique, a modification of
Context Tree Weighting for the prediction of binary, stationary, n-Markov
sources. By modifying Context Tree Weighting's recursive weighting scheme, it
is possible to mix over a strictly larger class of models without increasing
the asymptotic time or space complexity of the original algorithm. We prove
that this generalization preserves the desirable theoretical properties of
Context Tree Weighting on stationary n-Markov sources, and show empirically
that this new technique leads to consistent improvements over Context Tree
Weighting as measured on the Calgary Corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3200</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3200</id><created>2011-11-14</created><authors><author><keyname>Matuz</keyname><forenames>Balazs</forenames></author><author><keyname>Blasco</keyname><forenames>Francisco Lazaro</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author></authors><title>On the Application of the Baum-Welch Algorithm for Modeling the Land
  Mobile Satellite Channel</title><categories>cs.IT math.IT</categories><comments>IEEE Globecom 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate channel models are of high importance for the design of upcoming
mobile satellite systems. Nowadays most of the models for the LMSC are based on
Markov chains and rely on measurement data, rather than on pure theoretical
considerations. A key problem lies in the determination of the model parameters
out of the observed data. In this work we face the issue of state
identification of the underlying Markov model whose model parameters are a
priori unknown. This can be seen as a HMM problem. For finding the ML estimates
of such model parameters the BW algorithm is adapted to the context of channel
modeling. Numerical results on test data sequences reveal the capabilities of
the proposed algorithm. Results on real measurement data are finally presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3204</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3204</id><created>2011-11-14</created><authors><author><keyname>Blasco</keyname><forenames>Francisco Lazaro</forenames></author><author><keyname>Rossetto</keyname><forenames>Francesco</forenames></author><author><keyname>Bauch</keyname><forenames>Gerhard</forenames></author></authors><title>Time Interference Alignment via Delay Offset for Long Delay Networks</title><categories>cs.IT math.IT</categories><comments>IEEE Globecom 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time Interference Alignment is a flavor of Interference Alignment that
increases the network capacity by suitably staggering the transmission delays
of the senders. In this work the analysis of the existing literature is
generalized and the focus is on the computation of the dof for networks with
randomly placed users in a n-dimensional Euclidean space. In the basic case
without coordination among the transmitters analytical expressions of the sum
dof can be derived. If the transmit delays are coordinated, in 20% of the cases
time Interference Alignment yields additional dof with respect to orthogonal
access schemes. The potential capacity improvements for satellite networks are
also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3240</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3240</id><created>2011-11-14</created><authors><author><keyname>Kazerooni</keyname><forenames>Abbas</forenames></author><author><keyname>Golmohammadi</keyname><forenames>Azarang</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Salt-and-Pepper Noise Removal Based on Sparse Signal Processing</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new method for Salt-and-Pepper noise removal from
images. Whereas most of the existing methods are based on Ordered Statistics
filters, our method is based on the growing theory of Sparse Signal Processing.
In other words, we convert the problem of denoising into a sparse signal
reconstruction problem which can be dealt with the corresponding techniques. As
a result, the output image of our method is preserved from the undesirable
opacity which is a disadvantage of most of the other methods. We also introduce
an efficient reconstruction algorithm which will be used in our method.
Simulation results indicate that our method outperforms the other best-known
methods both in term of PSNR and visual criterion. Furthermore, our method can
be easily used for reconstruction of missing samples in erasure channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3244</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3244</id><created>2011-11-14</created><updated>2013-06-25</updated><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>Faster fully compressed pattern matching by recompression</title><categories>cs.DS</categories><comments>Full version, submitted to a journal as is. Overall improvements over
  the previous version</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fully compressed pattern matching problem is studied. The
compression is represented by straight-line programs (SLPs), i.e. a
context-free grammars generating exactly one string; the term fully means that
both the pattern and the text are given in the compressed form. The problem is
approached using a recently developed technique of local recompression: the
SLPs are refactored, so that substrings of the pattern and text are encoded in
both SLPs in the same way. To this end, the SLPs are locally decompressed and
then recompressed in a uniform way.
  This technique yields an O((n+m)log M) algorithm for compressed pattern
matching, assuming that M fits in O(1) machine words, where n (m) is the size
of the compressed representation of the text (pattern, respectively), while M
is the size of the decompressed pattern. If only m+n fits in O(1) machine
words, the running time increases to O((n+m)log M log(n+m)). The previous best
algorithm due to Lifshits had O(n^2m) running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3268</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3268</id><created>2011-11-14</created><updated>2013-01-02</updated><authors><author><keyname>Durand</keyname><forenames>Fabien</forenames><affiliation>LAMFA</affiliation></author></authors><title>Decidability of the HD0L ultimate periodicity problem</title><categories>math.CO cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove the decidability of the HD0L ultimate periodicity
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3270</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3270</id><created>2011-11-14</created><authors><author><keyname>Kaytoue</keyname><forenames>Mehdi</forenames><affiliation>DCC - UFMG</affiliation></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames><affiliation>DCC - UFMG</affiliation></author><author><keyname>Macko</keyname><forenames>Juraj</forenames><affiliation>DCC - UFMG</affiliation></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><affiliation>DCC - UFMG</affiliation></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Mining Biclusters of Similar Values with Triadic Concept Analysis</title><categories>cs.DS cs.AI cs.DB</categories><comments>Concept Lattices and their Applications (CLA) (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biclustering numerical data became a popular data-mining task in the
beginning of 2000's, especially for analysing gene expression data. A bicluster
reflects a strong association between a subset of objects and a subset of
attributes in a numerical object/attribute data-table. So called biclusters of
similar values can be thought as maximal sub-tables with close values. Only few
methods address a complete, correct and non redundant enumeration of such
patterns, which is a well-known intractable problem, while no formal framework
exists. In this paper, we introduce important links between biclustering and
formal concept analysis. More specifically, we originally show that Triadic
Concept Analysis (TCA), provides a nice mathematical framework for
biclustering. Interestingly, existing algorithms of TCA, that usually apply on
binary data, can be used (directly or with slight modifications) after a
preprocessing step for extracting maximal biclusters of similar values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3271</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3271</id><created>2011-11-14</created><authors><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Miller</keyname><forenames>Scott A.</forenames></author><author><keyname>Adaska</keyname><forenames>Jason</forenames></author></authors><title>On Bellman's principle with inequality constraints</title><categories>math.OC cs.SY math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an example by Haviv (1996) of a constrained Markov decision
process that, in some sense, violates Bellman's principle. We resolve this
issue by showing how to preserve a form of Bellman's principle that accounts
for a change of constraint at states that are reachable from the initial state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3274</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3274</id><created>2011-11-14</created><authors><author><keyname>Safadi</keyname><forenames>Ebrahim B. Al</forenames></author><author><keyname>Naffouri</keyname><forenames>Tareq Y. Al</forenames></author></authors><title>Pilotless Recovery of Clipped OFDM Signals by Compressive Sensing over
  Reliable Data Carriers</title><categories>cs.IT math.IT</categories><comments>This short version was submitted Sep. 19, 2011 to ICC 2012. Long
  version submitted to IEEE Transactions on Communications November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel form of clipping mitigation in OFDM using
compressive sensing that completely avoids tone reservation and hence rate loss
for this purpose. The method builds on selecting the most reliable
perturbations from the constellation lattice upon decoding at the receiver, and
performs compressive sensing over these observations in order to completely
recover the temporally sparse nonlinear distortion. As such, the method
provides a unique practical solution to the problem of initial erroneous
decoding decisions in iterative ML methods, offering both the ability to
augment these techniques and to solely recover the distorted signal in one
shot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3275</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3275</id><created>2011-11-14</created><updated>2012-12-24</updated><authors><author><keyname>Yoshida</keyname><forenames>Beni</forenames></author></authors><title>Information storage capacity of discrete spin systems</title><categories>cs.IT cond-mat.str-el math-ph math.IT math.MP nlin.CG quant-ph</categories><comments>35 pages, 12 figures</comments><report-no>CTP-4326</report-no><journal-ref>Annals of Physics 338, 134 (2013)</journal-ref><doi>10.1016/j.aop.2013.07.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the limits imposed on information storage capacity of physical
systems is a problem of fundamental and practical importance which bridges
physics and information science. There is a well-known upper bound on the
amount of information that can be stored reliably in a given volume of discrete
spin systems which are supported by gapped local Hamiltonians. However, all the
previously known systems were far below this theoretical bound, and it remained
open whether there exists a gapped spin system that saturates this bound. Here,
we present a construction of spin systems which saturate this theoretical limit
asymptotically by borrowing an idea from fractal properties arising in the
Sierpinski triangle. Our construction provides not only the best classical
error-correcting code which is physically realizable as the energy ground space
of gapped frustration-free Hamiltonians, but also a new research avenue for
correlated spin phases with fractal spin configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3281</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3281</id><created>2011-11-14</created><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author><author><keyname>Milo</keyname><forenames>Thomas</forenames></author><author><keyname>Wisnovsky</keyname><forenames>Robert</forenames></author></authors><title>A prototype system for handwritten sub-word recognition: Toward
  Arabic-manuscript transliteration</title><categories>cs.CV cs.IR</categories><comments>8 pages, 7 figures, 6 tables</comments><doi>10.1109/ISSPA.2012.6310473</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prototype system for the transliteration of diacritics-less Arabic
manuscripts at the sub-word or part of Arabic word (PAW) level is developed.
The system is able to read sub-words of the input manuscript using a set of
skeleton-based features. A variation of the system is also developed which
reads archigraphemic Arabic manuscripts, which are dot-less, into
archigraphemes transliteration. In order to reduce the complexity of the
original highly multiclass problem of sub-word recognition, it is redefined
into a set of binary descriptor classifiers. The outputs of trained binary
classifiers are combined to generate the sequence of sub-word letters. SVMs are
used to learn the binary classifiers. Two specific Arabic databases have been
developed to train and test the system. One of them is a database of the Naskh
style. The initial results are promising. The systems could be trained on other
scripts found in Arabic manuscripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3282</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3282</id><created>2011-11-14</created><authors><author><keyname>Iv&#xe1;nyi</keyname><forenames>A.</forenames></author><author><keyname>Lucz</keyname><forenames>L.</forenames></author><author><keyname>M&#xf3;ri</keyname><forenames>T. F.</forenames></author><author><keyname>S&#xf3;t&#xe9;r</keyname><forenames>P.</forenames></author></authors><title>On Erd\H{o}s-Gallai and Havel-Hakimi algorithms</title><categories>cs.DM</categories><msc-class>05C85, 68R10</msc-class><acm-class>G.2.2</acm-class><journal-ref>Acta Univ. Sapientiae, Inform. 3, 2 (2011) 230--268</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Havel in 1955, Erd\H{o}s and Gallai in 1960, Hakimi in 1962, Ruskey, Cohen,
Eades and Scott in 1994, Barnes and Savage in 1997, Kohnert in 2004, Tripathi,
Venugopalan and West in 2010 proposed a method to decide, whether a sequence of
nonnegative integers can be the degree sequence of a simple graph. The running
time of their algorithms is $\Omega(n^2)$ in worst case. In this paper we
propose a new algorithm called EGL (Erd\H{o}s-Gallai Linear algorithm), whose
worst running time is $\Theta(n).$ As an application of this quick algorithm we
computed the number of the different degree sequences of simple graphs for $24,
...,29$ vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3288</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3288</id><created>2011-11-14</created><authors><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author></authors><title>Lower bounds for finding the maximum and minimum elements with k lies</title><categories>cs.DM</categories><msc-class>68P10</msc-class><acm-class>F.2.2</acm-class><journal-ref>Acta Univ. Sapirntiae, Inform. 3, 2 (2011) 224--229</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with the problem of finding the smallest and the
largest elements of a totally ordered set of size $n$ using pairwise
comparisons if $k$ of the comparisons might be erroneous where $k$ is a fixed
constant. We prove that at least $(k+1.5)n+\Theta(k)$ comparisons are needed in
the worst case thus disproving the conjecture that $(k+1+\epsilon)n$
comparisons are enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3297</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3297</id><created>2011-11-14</created><authors><author><keyname>J&#xe1;rai</keyname><forenames>A.</forenames></author><author><keyname>Vatai</keyname><forenames>E.</forenames></author></authors><title>Cache optimized linear sieve</title><categories>cs.DS</categories><msc-class>11Y11, 68W99</msc-class><acm-class>F.2.1</acm-class><journal-ref>Acta Univ. Sapientiae, Inform. 3,2 (2011) 205--223</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sieving is essential in different number theoretical algorithms. Sieving with
large primes violates locality of memory access, thus degrading performance.
Our suggestion on how to tackle this problem is to use cyclic data structures
in combination with in-place bucket-sort. We present our results on the
implementation of the sieve of Eratosthenes, using these ideas, which show that
this approach is more robust and less affected by slow memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3304</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3304</id><created>2011-11-09</created><updated>2012-03-01</updated><authors><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author><author><keyname>Cowburn</keyname><forenames>David</forenames></author></authors><title>Eigenvector Synchronization, Graph Rigidity and the Molecule Problem</title><categories>cs.CE cs.DS math.CO q-bio.QM</categories><comments>49 pages, 8 figures</comments><msc-class>15A18, 49M27, 90C06, 90C20, 90C22, 92-08, 92E10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph realization problem has received a great deal of attention in
recent years, due to its importance in applications such as wireless sensor
networks and structural biology. In this paper, we extend on previous work and
propose the 3D-ASAP algorithm, for the graph realization problem in
$\mathbb{R}^3$, given a sparse and noisy set of distance measurements. 3D-ASAP
is a divide and conquer, non-incremental and non-iterative algorithm, which
integrates local distance information into a global structure determination.
Our approach starts with identifying, for every node, a subgraph of its 1-hop
neighborhood graph, which can be accurately embedded in its own coordinate
system. In the noise-free case, the computed coordinates of the sensors in each
patch must agree with their global positioning up to some unknown rigid motion,
that is, up to translation, rotation and possibly reflection. In other words,
to every patch there corresponds an element of the Euclidean group Euc(3) of
rigid transformations in $\mathbb{R}^3$, and the goal is to estimate the group
elements that will properly align all the patches in a globally consistent way.
Furthermore, 3D-ASAP successfully incorporates information specific to the
molecule problem in structural biology, in particular information on known
substructures and their orientation. In addition, we also propose 3D-SP-ASAP, a
faster version of 3D-ASAP, which uses a spectral partitioning algorithm as a
preprocessing step for dividing the initial graph into smaller subgraphs. Our
extensive numerical simulations show that 3D-ASAP and 3D-SP-ASAP are very
robust to high levels of noise in the measured distances and to sparse
connectivity in the measurement graph, and compare favorably to similar
state-of-the art localization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3321</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3321</id><created>2011-11-14</created><updated>2014-03-20</updated><authors><author><keyname>D&#xed;az</keyname><forenames>Josep</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>Approximating Fixation Probabilities in the Generalized Moran Process</title><categories>cs.CC</categories><comments>updated to the final version, which appeared in Algorithmica</comments><msc-class>60J22</msc-class><acm-class>F.2.2; G.3</acm-class><journal-ref>Algorithmica May 2014, Volume 69, Issue 1, pp 78-91</journal-ref><doi>10.1007/s00453-012-9722-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Moran process, as generalized by Lieberman, Hauert and Nowak
(Nature, 433:312--316, 2005). A population resides on the vertices of a finite,
connected, undirected graph and, at each time step, an individual is chosen at
random with probability proportional to its assigned 'fitness' value. It
reproduces, placing a copy of itself on a neighbouring vertex chosen uniformly
at random, replacing the individual that was there. The initial population
consists of a single mutant of fitness $r&gt;0$ placed uniformly at random, with
every other vertex occupied by an individual of fitness 1. The main quantities
of interest are the probabilities that the descendants of the initial mutant
come to occupy the whole graph (fixation) and that they die out (extinction);
almost surely, these are the only possibilities. In general, exact computation
of these quantities by standard Markov chain techniques requires solving a
system of linear equations of size exponential in the order of the graph so is
not feasible. We show that, with high probability, the number of steps needed
to reach fixation or extinction is bounded by a polynomial in the number of
vertices in the graph. This bound allows us to construct fully polynomial
randomized approximation schemes (FPRAS) for the probability of fixation (when
$r\geq 1$) and of extinction (for all $r&gt;0$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3334</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3334</id><created>2011-11-14</created><authors><author><keyname>Singh</keyname><forenames>Abhishek Kr.</forenames></author><author><keyname>Giridhar</keyname><forenames>Bollibisai</forenames></author><author><keyname>Mandal</keyname><forenames>Partha Sarathi</forenames></author></authors><title>Fixing Data Anomalies with Prediction Based Algorithm in Wireless Sensor
  Networks</title><categories>cs.DC</categories><comments>6 pages, 8 figures, The paper has been accepted for presentation at
  7th IEEE Conference on Wireless Communication and Sensor Networks (WCSN-2011)
  December 05-09, 2011, Panna National Park, Madhya Pradesh, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data inconsistencies are present in the data collected over a large wireless
sensor network (WSN), usually deployed for any kind of monitoring applications.
Before passing this data to some WSN applications for decision making, it is
necessary to ensure that the data received are clean and accurate. In this
paper, we have used a statistical tool to examine the past data to fit in a
highly sophisticated prediction model i.e., ARIMA for a given sensor node and
with this, the model corrects the data using forecast value if any data anomaly
exists there. Another scheme is also proposed for detecting data anomaly at
sink among the aggregated data in the data are received from a particular
sensor node. The effectiveness of our methods are validated by data collected
over a real WSN application consisting of Crossbow IRIS Motes
\cite{Crossbow:2009}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3350</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3350</id><created>2011-11-14</created><updated>2012-02-14</updated><authors><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Orlandi</keyname><forenames>Claudio</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames></author></authors><title>Privacy-Aware Mechanism Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In traditional mechanism design, agents only care about the utility they
derive from the outcome of the mechanism. We look at a richer model where
agents also assign non-negative dis-utility to the information about their
private types leaked by the outcome of the mechanism.
  We present a new model for privacy-aware mechanism design, where we only
assume an upper bound on the agents' loss due to leakage, as opposed to
previous work where a full characterization of the loss was required.
  In this model, under a mild assumption on the distribution of how agents
value their privacy, we show a generic construction of privacy-aware mechanisms
and demonstrate its applicability to electronic polling and pricing of a
digital good.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3374</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3374</id><created>2011-11-14</created><updated>2012-11-27</updated><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author><author><keyname>Pignolet</keyname><forenames>Yvonne-Anne</forenames></author><author><keyname>Turkel</keyname><forenames>Itzik</forenames></author></authors><title>From Caesar to Twitter: An Axiomatic Approach to Elites of Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>under submission</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many societies there is an elite, a relatively small group of powerful
individuals that is well-connected and highly influential. Since the ancient
days of Julius Caesar's senate of Rome to the recent days of celebrities on
Twitter, the size of the elite is a result of conflicting social forces
competing to increase or decrease it.
  The main contribution of this paper is the answer to the question how large
the elite is at equilibrium. We take an axiomatic approach to solve this:
assuming that an elite exists and it is influential, stable and either minimal
or dense, we prove that its size must be $\Theta(\sqrt{m})$ (where $m$ is the
number of edges in the network).
  As an approximation for the elite, we then present an empirical study on nine
large real-world networks of the subgraph formed by the highest degree nodes,
also known as the rich-club. Our findings indicate that elite properties such
as disproportionate influence, stability and density of
$\Theta(\sqrt{m})$-rich-clubs are universal properties and should join a
growing list of common phenomena shared by social networks and complex systems
such as &quot;small world,&quot; power law degree distributions, high clustering, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3376</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3376</id><created>2011-11-14</created><authors><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Quinn</keyname><forenames>Christopher J.</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author></authors><title>Fingerprinting with Equiangular Tight Frames</title><categories>cs.IT cs.MM math.IT</categories><comments>10 pages, 6 figures, presented in part at ICASSP 2011 and SPIE 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital fingerprinting is a framework for marking media files, such as
images, music, or movies, with user-specific signatures to deter illegal
distribution. Multiple users can collude to produce a forgery that can
potentially overcome a fingerprinting system. This paper proposes an
equiangular tight frame fingerprint design which is robust to such collusion
attacks. We motivate this design by considering digital fingerprinting in terms
of compressed sensing. The attack is modeled as linear averaging of multiple
marked copies before adding a Gaussian noise vector. The content owner can then
determine guilt by exploiting correlation between each user's fingerprint and
the forged copy. The worst-case error probability of this detection scheme is
analyzed and bounded. Simulation results demonstrate the average-case
performance is similar to the performance of orthogonal and simplex fingerprint
designs, while accommodating several times as many users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3393</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3393</id><created>2011-11-14</created><authors><author><keyname>Travers</keyname><forenames>Nicholas F.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Infinite Excess Entropy Processes with Countable-State Generators</title><categories>cs.IT cond-mat.stat-mech math.IT math.PR nlin.CD</categories><comments>13 pages, 3 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/ieepcsg.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two examples of finite-alphabet, infinite excess entropy processes
generated by invariant hidden Markov models (HMMs) with countable state sets.
The first, simpler example is not ergodic, but the second is. It appears these
are the first constructions of processes of this type. Previous examples of
infinite excess entropy processes over finite alphabets admit only invariant
HMM presentations with uncountable state sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3395</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3395</id><created>2011-11-14</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author></authors><title>The Capacity of a Class of Multi-Way Relay Channels</title><categories>cs.IT math.IT</categories><journal-ref>Proceedings of the 12th IEEE International Conference on
  Communications Systems (ICCS 2010), Singapore, pp. 346--350, Nov. 17--19,
  2010</journal-ref><doi>10.1109/ICCS.2010.5686491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of a class of multi-way relay channels, where L users
communicate via a relay (at possibly different rates), is derived for the case
where the channel outputs are modular sums of the channel inputs and the
receiver noise. The cut-set upper bound to the capacity is shown to be
achievable. More specifically, the capacity is achieved using (i) rate
splitting, (ii) functional-decode-forward, and (iii) joint source-channel
coding. We note that while separate source-channel coding can achieve the
common-rate capacity, joint source-channel coding is used to achieve the
capacity for the general case where the users are transmitting at different
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3396</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3396</id><created>2011-11-14</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Functional-Decode-Forward for the General Discrete Memoryless Two-Way
  Relay Channel</title><categories>cs.IT math.IT</categories><journal-ref>Proceedings of the 12th IEEE International Conference on
  Communications Systems (ICCS 2010), Singapore, pp. 351--355, Nov. 17--19,
  2010</journal-ref><doi>10.1109/ICCS.2010.5686490</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the general discrete memoryless two-way relay channel, where two
users exchange messages via a relay, and propose two functional-decode-forward
coding strategies for this channel. Functional-decode-forward involves the
relay decoding a function of the users' messages rather than the individual
messages themselves. This function is then broadcast back to the users, which
can be used in conjunction with the user's own message to decode the other
user's message. Via a numerical example, we show that functional-decode-forward
with linear codes is capable of achieving strictly larger sum rates than those
achievable by other strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3398</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3398</id><created>2011-11-14</created><updated>2012-10-19</updated><authors><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Kacem</keyname><forenames>Fadi</forenames></author><author><keyname>Milis</keyname><forenames>Ioannis</forenames></author></authors><title>Speed scaling with power down scheduling for agreeable deadlines</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><doi>10.1016/j.suscom.2012.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of scheduling on a single processor a given set of n
jobs. Each job j has a workload w_j and a release time r_j. The processor can
vary its speed and hibernate to reduce energy consumption. In a schedule
minimizing overall consumed energy, it might be that some jobs complete
arbitrarily far from their release time. So in order to guarantee some quality
of service, we would like to impose a deadline d_j=r_j+F for every job j, where
F is a guarantee on the *flow time*. We provide an O(n^3) algorithm for the
more general case of *agreeable deadlines*, where jobs have release times and
deadlines and can be ordered such that for every i&lt;j, both r_i&lt;=r_j and
d_i&lt;=d_j.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3403</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3403</id><created>2011-11-14</created><authors><author><keyname>Bartoli</keyname><forenames>Daniele</forenames></author><author><keyname>Davydov</keyname><forenames>Alexander A.</forenames></author><author><keyname>Faina</keyname><forenames>Giorgio</forenames></author><author><keyname>Marcugini</keyname><forenames>Stefano</forenames></author><author><keyname>Pambianco</keyname><forenames>Fernanda</forenames></author></authors><title>Upper bounds on the smallest size of a complete arc in the plane PG(2,q)</title><categories>math.CO cs.IT math.IT</categories><comments>21 pages, 4 figures, 5 tables. arXiv admin note: substantial text
  overlap with arXiv:1011.3347</comments><msc-class>51E21, 51E22 (Primary) 94B05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New upper bounds on the smallest size t_{2}(2,q) of a complete arc in the
projective plane PG(2,q) are obtained for q &lt;= 9109. From these new bounds it
follows that for q &lt;= 2621 and q = 2659,2663,2683,2693,2753,2801, the relation
t_{2}(2,q) &lt; 4.5\sqrt{q} holds. Also, for q &lt;= 5399 and q =
5413,5417,5419,5441,5443,5471,5483,5501,5521, we have t_{2}(2,q) &lt; 4.8\sqrt{q}.
Finally, for q &lt;= 9067 it holds that t_{2}(2,q) &lt; 5\sqrt{q}. The new upper
bounds are obtained by finding new small complete arcs with the help of a
computer search using randomized greedy algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3412</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3412</id><created>2011-11-14</created><authors><author><keyname>Sun</keyname><forenames>Xiaojun</forenames></author><author><keyname>Zhao</keyname><forenames>Chunming</forenames></author></authors><title>Outage probability of selective decode and forward relaying with secrecy
  constraints</title><categories>cs.IT math.IT</categories><comments>3 papges,3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the outage probability of opportunistic relay selection in
decode-and-forward relaying with secrecy constraints. We derive the closed-form
expression for the outage probability. Based on the analytical result, the
asymptotic performance is then investigated. The accuracy of our performance
analysis is verified by the simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3420</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3420</id><created>2011-11-14</created><authors><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>Optimal Self-Dual Z4-Codes and a Unimodular Lattice in Dimension 41</title><categories>math.CO cs.IT math.IT math.NT</categories><comments>13 pages, Finite Fields and Their Applications (to appear)</comments><journal-ref>Finite Fields and Their Applications 18 (2012), 529-536</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For lengths up to 47 except 37, we determine the largest minimum Euclidean
weight among all Type I Z4-codes of that length. We also give the first example
of an optimal odd unimodular lattice in dimension 41 explicitly, which is
constructed from some Type I Z4-code of length 41.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3427</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3427</id><created>2011-11-14</created><updated>2014-02-27</updated><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author><author><keyname>Roozbehani</keyname><forenames>Mardavij</forenames></author></authors><title>Joint Spectral Radius and Path-Complete Graph Lyapunov Functions</title><categories>math.OC cs.SY</categories><comments>To appear in SIAM Journal on Control and Optimization. Version 2 has
  gone through two major rounds of revision. In particular, a section on the
  performance of our algorithm on application-motivated problems has been added
  and a more comprehensive literature review is presented</comments><journal-ref>SIAM J. Control and Optimization, Vol. 52, No. 1, pp. 687-717,
  2014</journal-ref><doi>10.1137/110855272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the framework of path-complete graph Lyapunov functions for
approximation of the joint spectral radius. The approach is based on the
analysis of the underlying switched system via inequalities imposed among
multiple Lyapunov functions associated to a labeled directed graph. Inspired by
concepts in automata theory and symbolic dynamics, we define a class of graphs
called path-complete graphs, and show that any such graph gives rise to a
method for proving stability of the switched system. This enables us to derive
several asymptotically tight hierarchies of semidefinite programming
relaxations that unify and generalize many existing techniques such as common
quadratic, common sum of squares, and maximum/minimum-of-quadratics Lyapunov
functions. We compare the quality of approximation obtained by certain classes
of path-complete graphs including a family of dual graphs and all path-complete
graphs with two nodes on an alphabet of two matrices. We provide approximation
guarantees for several families of path-complete graphs, such as the De Bruijn
graphs, establishing as a byproduct a constructive converse Lyapunov theorem
for maximum/minimum-of-quadratics Lyapunov functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3439</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3439</id><created>2011-11-15</created><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author><author><keyname>Okawa</keyname><forenames>Satoshi</forenames></author></authors><title>On context-free languages of scattered words</title><categories>cs.FL cs.DM</categories><msc-class>68Q42, 68Q45, 06A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that if a B\&quot;uchi context-free language (BCFL) consists of
scattered words, then there is an integer $n$, depending only on the language,
such that the Hausdorff rank of each word in the language is bounded by $n$.
Every BCFL is a M\&quot;uller context-free language (MCFL). In the first part of the
paper, we prove that an MCFL of scattered words is a BCFL iff the rank of every
word in the language is bounded by an integer depending only on the language.
  Then we establish operational characterizations of the BCFLs of well-ordered
and scattered words. We prove that a language is a BCFL consisting of
well-ordered words iff it can be generated from the singleton languages
containing the letters of the alphabet by substitution into ordinary
context-free languages and the $\omega$-power operation. We also establish a
corresponding result for BCFLs of scattered words and define expressions
denoting BCFLs of well-ordered and scattered words. In the final part of the
paper we give some applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3462</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3462</id><created>2011-11-15</created><authors><author><keyname>Tolone</keyname><forenames>Elsa</forenames><affiliation>LIGM, FaMAF</affiliation></author><author><keyname>Stavroula</keyname><forenames>Voyatzi</forenames><affiliation>LIGM</affiliation></author></authors><title>Extending the adverbial coverage of a NLP oriented resource for French</title><categories>cs.CL</categories><comments>Proceedings of the 5th International Joint Conference on Natural
  Language Processing (IJCNLP'11), Chiang Mai : Thailand (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a work on extending the adverbial entries of LGLex: a NLP
oriented syntactic resource for French. Adverbs were extracted from the
Lexicon-Grammar tables of both simple adverbs ending in -ment '-ly' (Molinier
and Levrier, 2000) and compound adverbs (Gross, 1986; 1990). This work relies
on the exploitation of fine-grained linguistic information provided in existing
resources. Various features are encoded in both LG tables and they haven't been
exploited yet. They describe the relations of deleting, permuting, intensifying
and paraphrasing that associate, on the one hand, the simple and compound
adverbs and, on the other hand, different types of compound adverbs. The
resulting syntactic resource is manually evaluated and freely available under
the LGPL-LR license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3477</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3477</id><created>2011-11-15</created><authors><author><keyname>Sun</keyname><forenames>Yuhua</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Zilong</forenames></author></authors><title>The cross-correlation distribution of a $p$-ary $m$-sequence of period
  $p^{2m}-1$ and its decimation by $\frac{(p^{m}+1)^{2}}{2(p^{e}+1)}$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $n=2m$, $m$ odd, $e|m$, and $p$ odd prime with $p\equiv1\ \mathrm{mod}\
4$. Let $d=\frac{(p^{m}+1)^{2}}{2(p^{e}+1)}$. In this paper, we study the
cross-correlation between a $p$-ary $m$-sequence $\{s_{t}\}$ of period
$p^{2m}-1$ and its decimation $\{s_{dt}\}$. Our result shows that the
cross-correlation function is six-valued and that it takes the values in
$\{-1,\ \pm p^{m}-1,\ \frac{1\pm p^{\frac{e}{2}}}{2}p^{m}-1,\ \frac{(1-
p^{e})}{2}p^{m}-1\}$. Also, the distribution of the cross-correlation is
completely determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3525</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3525</id><created>2011-11-15</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Chua</keyname><forenames>Leon</forenames></author></authors><title>Phenomenology of retained refractoriness: On semi-memristive discrete
  media</title><categories>nlin.CG cs.ET</categories><journal-ref>International Journal of Bifurcation and Chaos, Vol. 22, No. 11
  (2012) 1230036 (19 pages)</journal-ref><doi>10.1142/S0218127412300364</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two-dimensional cellular automata, each cell takes three states:
resting, excited and refractory. A resting cell excites if number of excited
neighbours lies in a certain interval (excitation interval). An excited cell
become refractory independently on states of its neighbours. A refractory cell
returns to a resting state only if the number of excited neighbours belong to
recovery interval. The model is an excitable cellular automaton abstraction of
a spatially extended semi-memristive medium where a cell's resting state
symbolises low-resistance and refractory state high-resistance. The medium is
semi-memristive because only transition from high- to low-resistance is
controlled by density of local excitation. We present phenomenological
classification of the automata behaviour for all possible excitation intervals
and recovery intervals. We describe eleven classes of cellular automata with
retained refractoriness based on criteria of space-filling ratio, morphological
and generative diversity, and types of travelling localisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3530</identifier>
 <datestamp>2011-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3530</id><created>2011-11-15</created><authors><author><keyname>Mahmood</keyname><forenames>Shah</forenames></author><author><keyname>Desmedt</keyname><forenames>Yvo</forenames></author></authors><title>Preliminary Analysis of Google+'s Privacy</title><categories>cs.SI cs.CR</categories><comments>CCS'11, October 17-21, 2011, Chicago, Illinois, USA</comments><acm-class>K.4.1; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a preliminary analysis of Google+ privacy. We
identified that Google+ shares photo metadata with users who can access the
photograph and discuss its potential impact on privacy. We also identified that
Google+ encourages the provision of other names including maiden name, which
may help criminals performing identity theft. We show that Facebook lists are a
superset of Google+ circles, both functionally and logically, even though
Google+ provides a better user interface. Finally we compare the use of
encryption and depth of privacy control in Google+ versus in Facebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3548</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3548</id><created>2011-11-15</created><authors><author><keyname>Soulignac</keyname><forenames>Francisco J.</forenames></author></authors><title>Fully dynamic recognition of proper circular-arc graphs</title><categories>cs.DS</categories><comments>60 pages, 15 figures</comments><msc-class>68R10</msc-class><doi>10.1007/s00453-013-9835-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fully dynamic algorithm for the recognition of proper
circular-arc (PCA) graphs. The allowed operations on the graph involve the
insertion and removal of vertices (together with its incident edges) or edges.
Edge operations cost O(log n) time, where n is the number of vertices of the
graph, while vertex operations cost O(log n + d) time, where d is the degree of
the modified vertex. We also show incremental and decremental algorithms that
work in O(1) time per inserted or removed edge. As part of our algorithm, fully
dynamic connectivity and co-connectivity algorithms that work in O(log n) time
per operation are obtained. Also, an O(\Delta) time algorithm for determining
if a PCA representation corresponds to a co-bipartite graph is provided, where
\Delta\ is the maximum among the degrees of the vertices. When the graph is
co-bipartite, a co-bipartition of each of its co-components is obtained within
the same amount of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3567</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3567</id><created>2011-11-15</created><updated>2012-11-13</updated><authors><author><keyname>Rebollo-Monedero</keyname><forenames>David</forenames></author><author><keyname>Parra-Arnau</keyname><forenames>Javier</forenames></author><author><keyname>Diaz</keyname><forenames>Claudia</forenames></author><author><keyname>Forn&#xe9;</keyname><forenames>Jordi</forenames></author></authors><title>On the Measurement of Privacy as an Attacker's Estimation Error</title><categories>cs.IT cs.CR math.IT</categories><comments>This paper has 18 pages and 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of privacy metrics have been proposed in the literature to
evaluate the level of protection offered by privacy enhancing-technologies.
Most of these metrics are specific to concrete systems and adversarial models,
and are difficult to generalize or translate to other contexts. Furthermore, a
better understanding of the relationships between the different privacy metrics
is needed to enable more grounded and systematic approach to measuring privacy,
as well as to assist systems designers in selecting the most appropriate metric
for a given application.
  In this work we propose a theoretical framework for privacy-preserving
systems, endowed with a general definition of privacy in terms of the
estimation error incurred by an attacker who aims to disclose the private
information that the system is designed to conceal. We show that our framework
permits interpreting and comparing a number of well-known metrics under a
common perspective. The arguments behind these interpretations are based on
fundamental results related to the theories of information, probability and
Bayes decision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3584</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3584</id><created>2011-11-15</created><updated>2013-04-08</updated><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo I.</forenames></author></authors><title>Computing a visibility polygon using few variables</title><categories>cs.CG cs.DS</categories><comments>11 pages. Full version of paper in Proceedings of ISAAC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several algorithms for computing the visibility polygon of a
simple polygon $P$ from a viewpoint inside the polygon, when the polygon
resides in read-only memory and only few working variables can be used. The
first algorithm uses a constant number of variables, and outputs the vertices
of the visibility polygon in $O(n\Rout)$ time, where $\Rout$ denotes the number
of reflex vertices of $P$ that are part of the output. The next two algorithms
use $O(\log \Rin)$ variables, and output the visibility polygon in $O(n\log
\Rin)$ randomized expected time or $O(n\log^2 \Rin)$ deterministic time, where
$\Rin$ is the number of reflex vertices of $P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3597</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3597</id><created>2011-11-15</created><updated>2013-01-25</updated><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author><author><keyname>Doumen</keyname><forenames>Jeroen</forenames></author><author><keyname>Roelse</keyname><forenames>Peter</forenames></author><author><keyname>Skoric</keyname><forenames>Boris</forenames></author><author><keyname>de Weger</keyname><forenames>Benne</forenames></author></authors><title>Dynamic Tardos Traitor Tracing Schemes</title><categories>cs.CR</categories><comments>13 pages, 5 figures</comments><msc-class>68P30, 94B60</msc-class><journal-ref>IEEE Transactions on Information Theory, vol. 59, no. 7, pp.
  4230-4242, 2013</journal-ref><doi>10.1109/TIT.2013.2251756</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct binary dynamic traitor tracing schemes, where the number of
watermark bits needed to trace and disconnect any coalition of pirates is
quadratic in the number of pirates, and logarithmic in the total number of
users and the error probability. Our results improve upon results of Tassa, and
our schemes have several other advantages, such as being able to generate all
codewords in advance, a simple accusation method, and flexibility when the
feedback from the pirate network is delayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3602</identifier>
 <datestamp>2011-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3602</id><created>2011-11-10</created><updated>2011-12-17</updated><authors><author><keyname>Elia</keyname><forenames>Michele</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author></authors><title>On the Rabin signature</title><categories>cs.CR cs.IT math.IT</categories><comments>General revision; new section on blind signatures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some Rabin signature schemes may be exposed to forgery; several variants are
here described to counter this vulnerability. Blind Rabin signatures are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3606</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3606</id><created>2011-11-15</created><updated>2015-02-12</updated><authors><author><keyname>Toussi</keyname><forenames>Hamid A.</forenames></author></authors><title>tym: Typed Matlab</title><categories>cs.PL cs.MS</categories><comments>Presented at University of Sistan and Baluchestan, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although, many scientists and engineers use Octave or MATLAB as their
preferred programming language, dynamic nature of these languages can lead to
slower running-time of programs written in these languages compared to programs
written in languages which are not as dynamic, like C, C++ and Fortran. In this
work we developed a translator for a new programming language (tym) which tries
to address performance issues, common in scientific programs, by adding new
constructs to a subset of Octave/MATLAB language. Our translator compiles
programs written in tym, to efficient C++ code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3616</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3616</id><created>2011-11-15</created><authors><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author><author><keyname>Moghadam</keyname><forenames>Nima N.</forenames></author></authors><title>An Experimental Investigation of SIMO, MIMO, Interference-Alignment (IA)
  and Coordinated Multi-Point (CoMP)</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present experimental implementations of interference
alignment (IA) and coordinated multi-point transmission (CoMP). We provide
results for a system with three base-stations and three mobile-stations all
having two antennas. We further employ OFDM modulation, with high-order
constellations, and measure many positions both line-of-sight and
non-line-of-sight under interference limited conditions. We find the CoMP
system to perform better than IA at the cost of a higher back-haul capacity
requirement. During the measurements we also logged the channel estimates for
off-line processing. We use these channel estimates to calculate the
performance under ideal conditions. The performance estimates obtained this way
is substantially higher than what is actually observed in the end-to-end
transmissions---in particular in the CoMP case where the theoretical
performance is very high. We find the reason for this discrepancy to be the
impact of dirty-RF effects such as phase-noise and non-linearities. We are able
to model the dirty-RF effects to some extent. These models can be used to
simulate more complex systems and still account for the dirty-RF effects (e.g.,
systems with tens of mobiles and base-stations). Both IA and CoMP perform
better than reference implementations of single-user SIMO and MIMO in our
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3618</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3618</id><created>2011-11-15</created><authors><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author></authors><title>Linking to Data - Effect on Citation Rates in Astronomy</title><categories>cs.DL astro-ph.IM</categories><comments>4 pages, 3 figures, will appear proceedings of ADASS XXI</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Is there a difference in citation rates between articles that were published
with links to data and articles that were not? Besides being interesting from a
purely academic point of view, this question is also highly relevant for the
process of furthering science. Data sharing not only helps the process of
verification of claims, but also the discovery of new findings in archival
data. However, linking to data still is a far cry away from being a &quot;practice&quot;,
especially where it comes to authors providing these links during the writing
and submission process. You need to have both a willingness and a publication
mechanism in order to create such a practice. Showing that articles with links
to data get higher citation rates might increase the willingness of scientists
to take the extra steps of linking data sources to their publications. In this
presentation we will show this is indeed the case: articles with links to data
result in higher citation rates than articles without such links. The ADS is
funded by NASA Grant NNX09AB39G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3620</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3620</id><created>2011-11-15</created><updated>2012-10-01</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author><author><keyname>Mansfield</keyname><forenames>Shane</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author><author><keyname>Barbosa</keyname><forenames>Rui Soares</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author></authors><title>The Cohomology of Non-Locality and Contextuality</title><categories>quant-ph cs.LO math.CT</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 95, 2012, pp. 1-14</journal-ref><doi>10.4204/EPTCS.95.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper with Adam Brandenburger, we used sheaf theory to analyze
the structure of non-locality and contextuality. Moreover, on the basis of this
formulation, we showed that the phenomena of non-locality and contextuality can
be characterized precisely in terms of obstructions to the existence of global
sections.
  Our aim in the present work is to build on these results, and to use the
powerful tools of sheaf cohomology to study the structure of non-locality and
contextuality. We use the Cech cohomology on an abelian presheaf derived from
the support of a probabilistic model, viewed as a compatible family of
distributions, in order to define a cohomological obstruction for the family as
a certain cohomology class. This class vanishes if the family has a global
section. Thus the non-vanishing of the obstruction provides a sufficient (but
not necessary) condition for the model to be contextual.
  We show that for a number of salient examples, including PR boxes, GHZ
states, the Peres-Mermin magic square, and the 18-vector configuration due to
Cabello et al. giving a proof of the Kochen-Specker theorem in four dimensions,
the obstruction does not vanish, thus yielding cohomological witnesses for
contextuality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3645</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3645</id><created>2011-11-15</created><updated>2015-10-15</updated><authors><author><keyname>Savov</keyname><forenames>Ivan</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Classical codes for quantum broadcast channels</title><categories>quant-ph cs.IT math.IT</categories><comments>v4: 20 pages, final version to appear in IEEE Transactions on
  Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 12, pages
  1-12, December 2015</journal-ref><doi>10.1109/TIT.2015.2485998</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two approaches for transmitting classical information over quantum
broadcast channels. The first technique is a quantum generalization of the
superposition coding scheme for the classical broadcast channel. We use a
quantum simultaneous nonunique decoder and obtain a proof of the rate region
stated in [Yard et al., IEEE Trans. Inf. Theory 57 (10), 2011]. Our second
result is a quantum generalization of the Marton coding scheme. The error
analysis for the quantum Marton region makes use of ideas in our earlier work
and an idea recently presented by Radhakrishnan et al. in arXiv:1410.3248. Both
results exploit recent advances in quantum simultaneous decoding developed in
the context of quantum interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3652</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3652</id><created>2011-11-15</created><updated>2012-05-02</updated><authors><author><keyname>Ferretti</keyname><forenames>Luca</forenames></author><author><keyname>Cortelezzi</keyname><forenames>Michele</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Marmorini</keyname><forenames>Giacomo</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Features and heterogeneities in growing network models</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI q-bio.MN</categories><comments>16 pages, 6 figures, revtex</comments><doi>10.1103/PhysRevE.85.066110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex networks from the World-Wide-Web to biological networks are
growing taking into account the heterogeneous features of the nodes. The
feature of a node might be a discrete quantity such as a classification of a
URL document as personal page, thematic website, news, blog, search engine,
social network, ect. or the classification of a gene in a functional module.
Moreover the feature of a node can be a continuous variable such as the
position of a node in the embedding space. In order to account for these
properties, in this paper we provide a generalization of growing network models
with preferential attachment that includes the effect of heterogeneous features
of the nodes. The main effect of heterogeneity is the emergence of an
&quot;effective fitness&quot; for each class of nodes, determining the rate at which
nodes acquire new links. The degree distribution exhibits a multiscaling
behaviour analogous to the the fitness model. This property is robust with
respect to variations in the model, as long as links are assigned through
effective preferential attachment. Beyond the degree distribution, in this
paper we give a full characterization of the other relevant properties of the
model. We evaluate the clustering coefficient and show that it disappears for
large network size, a property shared with the Barab\'asi-Albert model.
Negative degree correlations are also present in the studied class of models,
along with non-trivial mixing patterns among features. We therefore conclude
that both small clustering coefficients and disassortative mixing are outcomes
of the preferential attachment mechanism in general growing networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3659</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3659</id><created>2011-11-15</created><updated>2012-11-26</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Living in Living Cities</title><categories>nlin.AO cs.SI physics.soc-ph</categories><comments>40 pages, 4 figures, overview paper</comments><report-no>C3 Report 2011.09</report-no><journal-ref>Artificial Life, 19(3 &amp; 4):401-420. 2013</journal-ref><doi>10.1162/ARTL_a_00112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an overview of current and potential applications of
living technology to some urban problems. Living technology can be described as
technology that exhibits the core features of living systems. These features
can be useful to solve dynamic problems. In particular, urban problems
concerning mobility, logistics, telecommunications, governance, safety,
sustainability, and society and culture are presented, while solutions
involving living technology are reviewed. A methodology for developing living
technology is mentioned, while supraoptimal public transportation systems are
used as a case study to illustrate the benefits of urban living technology.
Finally, the usefulness of describing cities as living systems is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3663</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3663</id><created>2011-11-15</created><authors><author><keyname>P&#x103;tca&#x15f;</keyname><forenames>C.</forenames></author></authors><title>The debts' clearing problem: a new approach</title><categories>cs.DS</categories><msc-class>05C85</msc-class><acm-class>G.2.2</acm-class><journal-ref>Acta Univ. Sapientia Inform. 3,2 (2011) 192--204</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The debts' clearing problem is about clearing all the debts in a group of $n$
entities (e.g. persons, companies) using a minimal number of money transaction
operations. In our previous works we studied the problem, gave a dynamic
programming solution solving it and proved that it is NP-hard. In this paper we
adapt the problem to dynamic graphs and give a data structure to solve it.
Based on this data structure we develop a new algorithm, that improves our
previous one for the static version of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3668</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3668</id><created>2011-11-15</created><authors><author><keyname>Herendi</keyname><forenames>T.</forenames></author><author><keyname>Major</keyname><forenames>R.</forenames></author></authors><title>Modular exponentiation of matrices on FPGA-s</title><categories>cs.DS</categories><msc-class>65F60, 11Y55</msc-class><acm-class>B.2.4</acm-class><journal-ref>Acta Univ. Sapientiae, Inform. 3, 2 (2011) 172--191</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an efficient FPGA implementation for the exponentiation of large
matrices. The research is related to an algorithm for constructing uniformly
distributed linear recurring sequences. The design utilizes the special
properties of both the FPGA and the used matrices to achieve a very significant
speedup compared to traditional architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3670</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3670</id><created>2011-11-15</created><authors><author><keyname>Farkas</keyname><forenames>G.</forenames></author><author><keyname>Kall&#xf3;s</keyname><forenames>G.</forenames></author><author><keyname>Kiss</keyname><forenames>G.</forenames></author></authors><title>Large primes in generalized Pascal triangles</title><categories>cs.DS</categories><msc-class>05A10, 11Y11</msc-class><acm-class>G.4</acm-class><journal-ref>Acta Univ. Sapientiae, Inform. 3, 2 (2011) 158--171</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, after presenting the results of the generalization of Pascal
triangle (using powers of base numbers), we examine some properties of the
112-based triangle, most of all regarding to prime numbers. Additionally, an
effective implementation of ECPP method is presented which enables Magma
computer algebra system to prove the primality of numbers with more than 1000
decimal digits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3673</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3673</id><created>2011-11-15</created><authors><author><keyname>Pataki</keyname><forenames>N.</forenames></author></authors><title>C++ Standard Template Library by template specialized containers</title><categories>cs.PL</categories><msc-class>68N19</msc-class><acm-class>D.3.2</acm-class><journal-ref>Acta Univ. Sapientiae, Inform. 3,2 (2011) 141--157</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The C++ Standard Template Library is the flagship example for libraries based
on the generic programming paradigm. The usage of this library is intended to
minimize the number of classical C/C++ errors, but does not warrant bug-free
programs. Furthermore, many new kinds of errors may arise from the inaccurate
use of the generic programming paradigm, like dereferencing invalid iterators
or misunderstanding remove-like algorithms. In this paper we present some
typical scenarios that may cause runtime or portability problems. We emit
warnings and errors while these risky constructs are used. We also present a
general approach to emit &quot;customized&quot; warnings. We support the so-called
&quot;believe-me marks&quot; to disable warnings. We present another typical usage of our
technique, when classes become deprecated during the software lifecycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3689</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3689</id><created>2011-11-15</created><authors><author><keyname>Sarma</keyname><forenames>Anish Das</forenames></author><author><keyname>Jain</keyname><forenames>Ankur</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Bohannon</keyname><forenames>Philip</forenames></author></authors><title>CBLOCK: An Automatic Blocking Mechanism for Large-Scale De-duplication
  Tasks</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  De-duplication---identification of distinct records referring to the same
real-world entity---is a well-known challenge in data integration. Since very
large datasets prohibit the comparison of every pair of records, {\em blocking}
has been identified as a technique of dividing the dataset for pairwise
comparisons, thereby trading off {\em recall} of identified duplicates for {\em
efficiency}. Traditional de-duplication tasks, while challenging, typically
involved a fixed schema such as Census data or medical records. However, with
the presence of large, diverse sets of structured data on the web and the need
to organize it effectively on content portals, de-duplication systems need to
scale in a new dimension to handle a large number of schemas, tasks and data
sets, while handling ever larger problem sizes. In addition, when working in a
map-reduce framework it is important that canopy formation be implemented as a
{\em hash function}, making the canopy design problem more challenging. We
present CBLOCK, a system that addresses these challenges. CBLOCK learns hash
functions automatically from attribute domains and a labeled dataset consisting
of duplicates. Subsequently, CBLOCK expresses blocking functions using a
hierarchical tree structure composed of atomic hash functions. The application
may guide the automated blocking process based on architectural constraints,
such as by specifying a maximum size of each block (based on memory
requirements), impose disjointness of blocks (in a grid environment), or
specify a particular objective function trading off recall for efficiency. As a
post-processing step to automatically generated blocks, CBLOCK {\em rolls-up}
smaller blocks to increase recall. We present experimental results on two
large-scale de-duplication datasets at Yahoo!---consisting of over 140K movies
and 40K restaurants respectively---and demonstrate the utility of CBLOCK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3690</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3690</id><created>2011-11-15</created><authors><author><keyname>Chevaleyre</keyname><forenames>Yann</forenames></author><author><keyname>Lang</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Maudet</keyname><forenames>Nicolas</forenames></author><author><keyname>Monnot</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>New Candidates Welcome! Possible Winners with respect to the Addition of
  New Candidates</title><categories>cs.AI</categories><comments>34 pages</comments><acm-class>I.2.11</acm-class><journal-ref>Mathematical Social Sciences 64(1), 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In voting contexts, some new candidates may show up in the course of the
process. In this case, we may want to determine which of the initial candidates
are possible winners, given that a fixed number $k$ of new candidates will be
added. We give a computational study of this problem, focusing on scoring
rules, and we provide a formal comparison with related problems such as control
via adding candidates or cloning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3696</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3696</id><created>2011-11-15</created><updated>2011-11-23</updated><authors><author><keyname>Truhachev</keyname><forenames>Dmitri</forenames></author></authors><title>Achieving AWGN Channel Capacity with Sparse Graph Modulation and &quot;In the
  Air&quot; Coupling</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication over a multiple access channel is considered. Each user
modulates his signal as a superposition of redundant data streams where
interconnection of data bits can be represented by means of a sparse graph. The
receiver observes a signal resulting from the coupling of the sparse modulation
graphs. Iterative interference cancellation decoding is analyzed. It is proved
that spatial graph coupling allows to achieve the AWGN channel capacity with
equal power transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3711</identifier>
 <datestamp>2011-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3711</id><created>2011-11-15</created><updated>2011-12-16</updated><authors><author><keyname>Azgin</keyname><forenames>Aytac</forenames></author><author><keyname>Altunbasak</keyname><forenames>Yucel</forenames></author></authors><title>Channel Reordering with Time-shifted Streams to Improve Channel Change
  Latency in IPTV Networks</title><categories>cs.NI cs.MM</categories><comments>Compared to the previous version, corrected a few typos and
  simplified two figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In IPTV networks, channel change latency is considered as a major obstacle in
achieving broadcast-level quality video delivery. Because of the bandwidth
limitations observed at the client side, users typically have access to a
limited number of channels. As a result, channel change requests oftentimes
need to go through the network, thereby leading to significant delays. In this
paper, we address this problem by proposing a resource-efficient time-shifted
channel reordering mechanism to minimize the channel change latency. The
proposed framework exploits the differing key-frame delivery times for the
adjacent sessions to dynamically arrange the switching order during the surfing
periods. The simulation results show that, with the proposed framework, more
than 50% improvement can be achieved in channel change latency without
introducing any overhead in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3715</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3715</id><created>2011-11-15</created><authors><author><keyname>Huang</keyname><forenames>Wenqi</forenames></author><author><keyname>Ye</keyname><forenames>Tao</forenames></author><author><keyname>Chen</keyname><forenames>Duanbing</forenames></author></authors><title>Corner Occupying Theorem for the Two-dimensional Integral Rectangle
  Packing Problem</title><categories>cs.DM math.CO math.OC</categories><comments>11 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1107.4463</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves a corner occupying theorem for the two-dimensional integral
rectangle packing problem, stating that if it is possible to orthogonally place
n arbitrarily given integral rectangles into an integral rectangular container
without overlapping, then we can achieve a feasible packing by successively
placing an integral rectangle onto a bottom-left corner in the container. Based
on this theorem, we might develop efficient heuristic algorithms for solving
the integral rectangle packing problem. In fact, as a vague conjecture, this
theorem has been implicitly mentioned with different appearances by many people
for a long time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3728</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3728</id><created>2011-11-16</created><updated>2012-04-13</updated><authors><author><keyname>Joseph</keyname><forenames>Vinay</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author></authors><title>Variability Aware Network Utility Maximization</title><categories>cs.SY cs.NI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Utility Maximization (NUM) provides the key conceptual framework to
study resource allocation amongst a collection of users/entities across
disciplines as diverse as economics, law and engineering. In network
engineering, this framework has been particularly insightful towards
understanding how Internet protocols allocate bandwidth, and motivated diverse
research on distributed mechanisms to maximize network utility while
incorporating new relevant constraints, on energy/power, storage, stability,
etc., for systems ranging from communication networks to the smart-grid.
However when the available resources and/or users' utilities vary over time, a
user's allocations will tend to vary, which in turn may have a detrimental
impact on the users' utility or quality of experience. This paper introduces a
generalized NUM framework which explicitly incorporates the detrimental impact
of temporal variability in a user's allocated rewards. It explicitly
incorporates tradeoffs amongst the mean and variability in users' allocations.
We propose an online algorithm to realize variance-sensitive NUM, which, under
stationary ergodic assumptions, is shown to be asymptotically optimal, i.e.,
achieves a time-average equal to that of an offline algorithm with knowledge of
the future variability in the system. This substantially extends work on NUM to
an interesting class of relevant problems where users/entities are sensitive to
temporal variability in their service or allocated rewards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3735</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3735</id><created>2011-11-16</created><authors><author><keyname>Synnaeve</keyname><forenames>Gabriel</forenames><affiliation>LIG, LPPA</affiliation></author><author><keyname>Bessi&#xe8;re</keyname><forenames>Pierre</forenames><affiliation>LIG, LPPA</affiliation></author></authors><title>A Bayesian Model for Plan Recognition in RTS Games applied to StarCraft</title><categories>cs.LG cs.AI</categories><comments>7 pages; Artificial Intelligence and Interactive Digital
  Entertainment Conference (AIIDE 2011), Palo Alto : \'Etats-Unis (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of keyhole (unobtrusive) plan recognition is central to adaptive
game AI. &quot;Tech trees&quot; or &quot;build trees&quot; are the core of real-time strategy (RTS)
game strategic (long term) planning. This paper presents a generic and simple
Bayesian model for RTS build tree prediction from noisy observations, which
parameters are learned from replays (game logs). This unsupervised machine
learning approach involves minimal work for the game developers as it leverage
players' data (com- mon in RTS). We applied it to StarCraft1 and showed that it
yields high quality and robust predictions, that can feed an adaptive AI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3739</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3739</id><created>2011-11-16</created><authors><author><keyname>Tibabishev</keyname><forenames>V. N.</forenames></author></authors><title>The function space to describe the dynamics of linear systems</title><categories>cs.SY</categories><comments>13 pages</comments><msc-class>93A10, 93C05, 93C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually, the dynamics of linear time-invariant systems described by an
integral operator of convolution type, which is defined in the Hilbert space of
Lebesgue square integrable functions on the whole line. Such a description
leads to contradictions. It is shown that the transition to the Hilbert space
of almost periodic functions leads to the elimination of the detected
inconsistencies. Multiple signals and interference with discrete spectrum are
systems of sets. The properties of these systems lead to a new more effective
method to combat noise in this space. The method used to identify the
differential equations for the airbus. Baseline data were obtained during
automatic landing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3752</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3752</id><created>2011-11-16</created><updated>2012-06-08</updated><authors><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Single-User Beamforming in Large-Scale MISO Systems with Per-Antenna
  Constant-Envelope Constraints: The Doughnut Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large antenna arrays at the base station (BS) has recently been shown to
achieve remarkable intra-cell interference suppression at low complexity.
However, building large arrays in practice, would require the use of
power-efficient RF amplifiers, which generally have poor linearity
characteristics and hence would require the use of input signals with a very
small peak-to-average power ratio (PAPR). In this paper, we consider the
single-user Multiple-Input Single-Output (MISO) downlink channel for the case
where the BS antennas are constrained to transmit signals having constant
envelope (CE). We show that, with per-antenna CE transmission the effective
channel seen by the receiver is a SISO AWGN channel with its input constrained
to lie in a doughnut-shaped region. For single-path direct-line-of-sight (DLOS)
and general i.i.d. fading channels, analysis of the effective doughnut channel
shows that under a per-antenna CE input constraint, i) compared to an
average-only total transmit power constrained MISO channel, the extra total
transmit power required to achieve a desired information rate is small and
bounded, ii) with N base station antennas an O(N) array power gain is
achievable, and iii) for a desired information rate, using power-efficient
amplifiers with CE inputs would require significantly less total transmit power
when compared to using highly linear (power-inefficient) amplifiers with high
PAPR inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3753</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3753</id><created>2011-11-16</created><authors><author><keyname>Goyal</keyname><forenames>Vipul</forenames></author><author><keyname>Kumar</keyname><forenames>Virendra</forenames></author><author><keyname>Singh</keyname><forenames>Mayank</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>CompChall: Addressing Password Guessing Attacks</title><categories>cs.CR</categories><comments>6 Pages, 1 Figure; International Conference on Information
  Technology: Coding and Computing, 2005. ITCC 2005</comments><doi>10.1109/ITCC.2005.107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though passwords are the most convenient means of authentication, they
bring along themselves the threat of dictionary attacks. Dictionary attacks may
be of two kinds: online and offline. While offline dictionary attacks are
possible only if the adversary is able to collect data for a successful
protocol execution by eavesdropping on the communication channel and can be
successfully countered using public key cryptography, online dictionary attacks
can be performed by anyone and there is no satisfactory solution to counter
them. This paper presents a new authentication protocol which is called
CompChall (computational challenge). The proposed protocol uses only one way
hash functions as the building blocks and attempts to eliminate online
dictionary attacks by implementing a challenge-response system. This
challenge-response system is designed in a fashion that it does not pose any
difficulty to a genuine user but is time consuming and computationally
intensive for an adversary trying to launch a large number of login requests
per unit time as in the case of an online dictionary attack. The protocol is
stateless and thus less vulnerable to DoS (Denial of Service) attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3758</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3758</id><created>2011-11-16</created><authors><author><keyname>Das</keyname><forenames>Soumyendu</forenames></author><author><keyname>Das</keyname><forenames>Subhendu</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Bijoy</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Steganography and Steganalysis: Different Approaches</title><categories>cs.CR</categories><comments>11 Pages, 5 Figures; International Journal of Computers, Information
  Technology and Engineering (IJCITAE), Vol. 2, No 1, June, 2008, Serial
  Publications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is the technique of hiding confidential information within any
media. Steganography is often confused with cryptography because the two are
similar in the way that they both are used to protect confidential information.
The difference between the two is in the appearance in the processed output;
the output of steganography operation is not apparently visible but in
cryptography the output is scrambled so that it can draw attention. Steganlysis
is process to detect of presence of steganography. In this article we have
tried to elucidate the different approaches towards implementation of
steganography using 'multimedia' file (text, static image, audio and video) and
Network IP datagram as cover. Also some methods of steganalysis will be
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3784</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3784</id><created>2011-11-16</created><authors><author><keyname>Buddelmeijer</keyname><forenames>Hugo</forenames></author><author><keyname>Boxhoorn</keyname><forenames>Danny</forenames></author><author><keyname>Valentijn</keyname><forenames>Edwin A.</forenames></author></authors><title>Automatic Optimized Discovery, Creation and Processing of Astronomical
  Catalogs</title><categories>astro-ph.IM cs.DB</categories><comments>Accepted for publication in topical issue of Experimental Astronomy
  on Astro-WISE information system</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the design of a novel way of handling astronomical catalogs in
Astro-WISE in order to achieve the scalability required for the data produced
by large scale surveys. A high level of automation and abstraction is achieved
in order to facilitate interoperation with visualization software for
interactive exploration. At the same time flexibility in processing is enhanced
and data is shared implicitly between scientists.
  This is accomplished by using a data model that primarily stores how catalogs
are derived; the contents of the catalogs are only created when necessary and
stored only when beneficial for performance. Discovery of existing catalogs and
creation of new catalogs is done through the same process by directly
requesting the final set of sources (astronomical objects) and attributes
(physical properties) that is required, for example from within visualization
software.
  New catalogs are automatically created to provide attributes of sources for
which no suitable existing catalogs can be found. These catalogs are defined to
contain the new attributes on the largest set of sources the calculation of the
attributes is applicable to, facilitating reuse for future data requests.
Subsequently, only those parts of the catalogs that are required for the
requested end product are actually processed, ensuring scalability.
  The presented mechanisms primarily determine which catalogs are created and
what data has to be processed and stored: the actual processing and storage
itself is left to existing functionality of the underlying information system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3805</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3805</id><created>2011-11-16</created><authors><author><keyname>Dupuy</keyname><forenames>Florian</forenames></author><author><keyname>Loubaton</keyname><forenames>Philippe</forenames></author></authors><title>Diversity of the MMSE receiver in flat fading and frequency selective
  MIMO channels at fixed rate</title><categories>cs.IT math.IT</categories><comments>Extended version of Asilomar 2011 conference paper, 17 pages, 2
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, the evaluation of the diversity of the MIMO MMSE
receiver is addressed for finite rates in both flat fading channels and
frequency selective fading channels with cyclic prefix. It has been observed
recently that in contrast with the other MIMO receivers, the MMSE receiver has
a diversity depending on the aimed finite rate, and that for sufficiently low
rates the MMSE receiver reaches the full diversity - that is, the diversity of
the ML receiver. This behavior has so far only been partially explained. The
purpose of this paper is to provide complete proofs for flat fading MIMO
channels, and to improve the partial existing results in frequency selective
MIMO channels with cyclic prefix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3806</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3806</id><created>2011-11-16</created><authors><author><keyname>Saarinen</keyname><forenames>Aki</forenames></author><author><keyname>Siekkinen</keyname><forenames>Matti</forenames></author><author><keyname>Xiao</keyname><forenames>Yu</forenames></author><author><keyname>Nurminen</keyname><forenames>Jukka K.</forenames></author><author><keyname>Kemppainen</keyname><forenames>Matti</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author></authors><title>Offloadable Apps using SmartDiet: Towards an analysis toolkit for mobile
  application developers</title><categories>cs.DC cs.NI cs.SE</categories><comments>7 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offloading work to cloud is one of the proposed solutions for increasing the
battery life of mobile devices. Most prior research has focused on
computation-intensive applications, even though such applications are not the
most popular ones. In this paper, we first study the feasibility of
method-level offloading in network-intensive applications, using an open source
Twitter client as an example. Our key observation is that implementing
offloading transparently to the developer is difficult: various constraints
heavily limit the offloading possibilities, and estimation of the potential
benefit is challenging. We then propose a toolkit, SmartDiet, to assist mobile
application developers in creating code which is suitable for energy-efficient
offloading. SmartDiet provides fine-grained offloading constraint
identification and energy usage analysis for Android applications. In addition
to outlining the overall functionality of the toolkit, we study some of its key
mechanisms and identify the remaining challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3818</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3818</id><created>2011-11-16</created><authors><author><keyname>H&#xfc;nniger</keyname><forenames>Martin</forenames></author></authors><title>Good Pairs of Adjacency Relations in Arbitrary Dimensions</title><categories>cs.CV</categories><comments>29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this text we show, that the notion of a &quot;good pair&quot; that was introduced in
the paper &quot;Digital Manifolds and the Theorem of Jordan-Brouwer&quot; has actually
known models. We will show, how to choose cubical adjacencies, the
generalizations of the well known 4- and 8-neighborhood to arbitrary
dimensions, in order to find good pairs. Furthermore, we give another proof for
the well known fact that the Khalimsky-topology implies good pairs. The outcome
is consistent with the known theory as presented by T.Y. Kong, A. Rosenfeld,
G.T. Herman and M. Khachan et.al and gives new insights in higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3820</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3820</id><created>2011-11-16</created><updated>2012-03-29</updated><authors><author><keyname>Bocharova</keyname><forenames>Irina E.</forenames></author><author><keyname>Hug</keyname><forenames>Florian</forenames></author><author><keyname>Johannesson</keyname><forenames>Rolf</forenames></author><author><keyname>Kudryashov</keyname><forenames>Boris D.</forenames></author></authors><title>A Closed Form Expression for the Exact Bit Error Probability for Viterbi
  Decoding of Convolutional Codes</title><categories>cs.IT math.IT</categories><comments>9 pages, 9 figures, submitted to IEEE Transactions on Information
  Theory in November 2011, revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1995, Best et al. published a formula for the exact bit error probability
for Viterbi decoding of the rate R=1/2, memory m=1 (2-state) convolutional
encoder with generator matrix G(D)=(1 1+D) when used to communicate over the
binary symmetric channel. Their formula was later extended to the rate R=1/2,
memory m=2 (4-state) convolutional encoder with generator matrix G(D)=(1+D^2
1+D+D^2) by Lentmaier et al.
  In this paper, a different approach to derive the exact bit error probability
is described. A general recurrent matrix equation, connecting the average
information weight at the current and previous states of a trellis section of
the Viterbi decoder, is derived and solved. The general solution of this matrix
equation yields a closed form expression for the exact bit error probability.
As special cases, the expressions obtained by Best et al. for the 2-state
encoder and by Lentmaier et al. for a 4-state encoder are obtained. The closed
form expression derived in this paper is evaluated for various realizations of
encoders, including rate R=1/2 and R=2/3 encoders, of as many as 16 states.
  Moreover, it is shown that it is straightforward to extend the approach to
communication over the quantized additive white Gaussian noise channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3824</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3824</id><created>2011-11-16</created><updated>2012-03-22</updated><authors><author><keyname>Elias</keyname><forenames>Marek</forenames></author><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author></authors><title>Higher-order Erdos--Szekeres theorems</title><categories>cs.CG math.CO</categories><comments>Contains a counter example of Gunter Rote which gives a reply for the
  problem number 5 in the previous versions of this paper</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Let P=(p_1,p_2,...,p_N) be a sequence of points in the plane, where
p_i=(x_i,y_i) and x_1&lt;x_2&lt;...&lt;x_N. A famous 1935 Erdos--Szekeres theorem
asserts that every such P contains a monotone subsequence S of $\sqrt N$
points. Another, equally famous theorem from the same paper implies that every
such P contains a convex or concave subsequence of $\Omega(\log N)$ points.
  Monotonicity is a property determined by pairs of points, and convexity
concerns triples of points. We propose a generalization making both of these
theorems members of an infinite family of Ramsey-type results. First we define
a (k+1)-tuple $K\subseteq P$ to be positive if it lies on the graph of a
function whose kth derivative is everywhere nonnegative, and similarly for a
negative (k+1)-tuple. Then we say that $S\subseteq P$ is kth-order monotone if
its (k+1)-tuples are all positive or all negative.
  We investigate quantitative bound for the corresponding Ramsey-type result
(i.e., how large kth-order monotone subsequence can be guaranteed in every
N-point P). We obtain an $\Omega(\log^{(k-1)}N)$ lower bound ((k-1)-times
iterated logarithm). This is based on a quantitative Ramsey-type theorem for
what we call transitive colorings of the complete (k+1)-uniform hypergraph; it
also provides a unified view of the two classical Erdos--Szekeres results
mentioned above.
  For k=3, we construct a geometric example providing an $O(\log\log N)$ upper
bound, tight up to a multiplicative constant. As a consequence, we obtain
similar upper bounds for a Ramsey-type theorem for order-type homogeneous
subsets in R^3, as well as for a Ramsey-type theorem for hyperplanes in R^4
recently used by Dujmovic and Langerman.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3837</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3837</id><created>2011-11-16</created><updated>2012-04-26</updated><authors><author><keyname>Xi</keyname><forenames>Zhengjun</forenames></author><author><keyname>Lu</keyname><forenames>Xiao-Ming</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoguang</forenames></author><author><keyname>Li</keyname><forenames>Yongming</forenames></author></authors><title>Necessary and sufficient condition for saturating the upper bound of
  quantum discord</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 1figures, version accepted Phys.Rev.A, 85, 032109 (2012)</comments><doi>10.1103/PhysRevA.85.032109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the upper bound of quantum discord given by the von Neumann
entropy of the measured subsystem. Using the Koashi-Winter relation, we obtain
a trade-off between the amount of classical correlation and quantum discord in
the tripartite pure states. The difference between the quantum discord and its
upper bound is interpreted as a measure on the classical correlative capacity.
Further, we give the explicit characterization of the quantum states saturating
the upper bound of quantum discord, through the equality condition for the
Araki-Lieb inequality. We also demonstrate that the saturating of the upper
bound of quantum discord precludes any further correlation between the measured
subsystem and the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3846</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3846</id><created>2011-11-16</created><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>No Free Lunch versus Occam's Razor in Supervised Learning</title><categories>cs.LG cs.IT math.IT</categories><comments>16 LaTeX pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The No Free Lunch theorems are often used to argue that domain specific
knowledge is required to design successful algorithms. We use algorithmic
information theory to argue the case for a universal bias allowing an algorithm
to succeed in all interesting problem domains. Additionally, we give a new
algorithm for off-line classification, inspired by Solomonoff induction, with
good performance on all structured problems under reasonable assumptions. This
includes a proof of the efficacy of the well-known heuristic of randomly
selecting training data in the hope of reducing misclassification rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3854</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3854</id><created>2011-11-16</created><authors><author><keyname>Wood</keyname><forenames>Ian</forenames></author><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>(Non-)Equivalence of Universal Priors</title><categories>cs.IT math.IT</categories><comments>10 LaTeX pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ray Solomonoff invented the notion of universal induction featuring an aptly
termed &quot;universal&quot; prior probability function over all possible computable
environments. The essential property of this prior was its ability to dominate
all other such priors. Later, Levin introduced another construction --- a
mixture of all possible priors or `universal mixture'. These priors are well
known to be equivalent up to multiplicative constants. Here, we seek to clarify
further the relationships between these three characterisations of a universal
prior (Solomonoff's, universal mixtures, and universally dominant priors). We
see that the the constructions of Solomonoff and Levin define an identical
class of priors, while the class of universally dominant priors is strictly
larger. We provide some characterisation of the discrepancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3859</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3859</id><created>2011-11-10</created><updated>2012-01-07</updated><authors><author><keyname>Wen</keyname><forenames>H.</forenames></author><author><keyname>Kish</keyname><forenames>L. B.</forenames></author><author><keyname>Klappenecker</keyname><forenames>A.</forenames></author><author><keyname>Peper</keyname><forenames>F.</forenames></author></authors><title>New noise-based logic representations to avoid some problems with time
  complexity</title><categories>cs.OH physics.gen-ph</categories><comments>submitted for publication</comments><journal-ref>Fluctuation and Noise Letters 11 (2012) 1250003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instantaneous noise-based logic can avoid time-averaging, which implies
significant potential for low-power parallel operations in
beyond-Moore-law-chips. However, the universe (uniform superposition) will be
zero with high probability (non-zero with exponentially low probability) in the
random-telegraph-wave representation thus the operations with the universe
would require exponential time-complexity. To fix this deficiency, we modify
the amplitudes of the signals of the L and H states and achieve an exponential
speedup compared to the old situation. Another improvement concerns the
identification of a single product (hyperspace) state. We introduce a time
shifted noise-based logic, which is constructed by shifting each reference
signal with a small time delay. This modification implies an exponential
speedup of single hyperspace vector identification compared to the former case
and it requires the same, O(N) complexity as in quantum computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3866</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3866</id><created>2011-11-16</created><authors><author><keyname>Vazquez</keyname><forenames>Emmanuel</forenames></author><author><keyname>Bect</keyname><forenames>Julien</forenames></author></authors><title>Sequential search based on kriging: convergence analysis of some
  algorithms</title><categories>math.ST cs.LG math.OC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\FF$ be a set of real-valued functions on a set $\XX$ and let $S:\FF \to
\GG$ be an arbitrary mapping. We consider the problem of making inference about
$S(f)$, with $f\in\FF$ unknown, from a finite set of pointwise evaluations of
$f$. We are mainly interested in the problems of approximation and
optimization. In this article, we make a brief review of results concerning
average error bounds of Bayesian search methods that use a random process prior
about $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3879</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3879</id><created>2011-11-16</created><updated>2012-04-17</updated><authors><author><keyname>Bekkai</keyname><forenames>Siham</forenames></author></authors><title>Minimum degree, independence number and pseudo [2,b]-factors in graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudo [2,b]-factor of a graph G is a spanning subgraph in which each
component C on at least three vertices is a [2,b]-graph. The main contibution
of this paper, is to give an upper bound to the number of components that are
edges or vertices in a pseudo [2,b]-factor of a graph G. This bound is sharp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3919</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3919</id><created>2011-11-16</created><updated>2012-05-21</updated><authors><author><keyname>Teng</keyname><forenames>Chun-Yuen</forenames></author><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Adamic</keyname><forenames>Lada A.</forenames></author></authors><title>Recipe recommendation using ingredient networks</title><categories>cs.SI physics.soc-ph</categories><comments>V2 fixes typos. V3 omits erroneous result regarding type of cooking
  method (mechanical, heat, chemical) and average recipe rating; Proc. 4th
  International Conference on Web Science (WebSci'12), 2012</comments><acm-class>H.2.8</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The recording and sharing of cooking recipes, a human activity dating back
thousands of years, naturally became an early and prominent social use of the
web. The resulting online recipe collections are repositories of ingredient
combinations and cooking methods whose large-scale and variety yield
interesting insights about both the fundamentals of cooking and user
preferences. At the level of an individual ingredient we measure whether it
tends to be essential or can be dropped or added, and whether its quantity can
be modified. We also construct two types of networks to capture the
relationships between ingredients. The complement network captures which
ingredients tend to co-occur frequently, and is composed of two large
communities: one savory, the other sweet. The substitute network, derived from
user-generated suggestions for modifications, can be decomposed into many
communities of functionally equivalent ingredients, and captures users'
preference for healthier variants of a recipe. Our experiments reveal that
recipe ratings can be well predicted with features derived from combinations of
ingredient networks and nutrition information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3925</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3925</id><created>2011-11-16</created><authors><author><keyname>Zhao</keyname><forenames>Pengkai</forenames></author><author><keyname>Shen</keyname><forenames>Cong</forenames></author></authors><title>A Low-Delay Low-Complexity EKF Design for Joint Channel and CFO
  Estimation in Multi-User Cognitive Communications</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Parameter estimation in cognitive communications can be formulated as a
multi-user estimation problem, which is solvable under maximum likelihood
solution but involves high computational complexity. This paper presents a
time-sharing and interference mitigation based EKF (Extended Kalman Filter)
design for joint CFO (carrier frequency offset) and channel estimation at
multiple cognitive users. The key objective is to realize low implementation
complexity by decomposing highdimensional parameters into multiple separate
low-dimensional estimation problems, which can be solved in a time-shared
manner via pipelining operation. We first present a basic EKF design that
estimates the parameters from one TX user to one RX antenna. Then such basic
design is time-shared and reused to estimate parameters from multiple TX users
to multiple RX antennas. Meanwhile, we use interference mitigation module to
cancel the co-channel interference at each RX sample. In addition, we further
propose adaptive noise variance tracking module to improve the estimation
performance. The proposed design enjoys low delay and low buffer size (because
of its online real-time processing), as well as low implementation complexity
(because of time-sharing and pipeling design). Its estimation performance is
verified to be close to Cramer-Rao bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3933</identifier>
 <datestamp>2011-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3933</id><created>2011-11-16</created><authors><author><keyname>Kuniavsky</keyname><forenames>Sergey</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames></author></authors><title>Equilibrium and Potential in Coalitional Congestion Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of congestion games is widely used to analyze games related to
traffic and communication. A central property of these games is that they are
potential games and hence posses a pure Nash equilibrium. In reality it is
often the case that some players cooperatively decide on their joint action in
order to maximize the coalition's total utility. This is by modeled by
Coalitional Congestion Games. Typical settings include truck drivers who work
for the same shipping company, or routers that belong to the same ISP. The
formation of coalitions will typically imply that the resulting coalitional
congestion game will no longer posses a pure Nash equilibrium. In this paper we
provide conditions under which such games are potential games and posses a pure
Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3934</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3934</id><created>2011-11-16</created><updated>2012-05-12</updated><authors><author><keyname>Hibbard</keyname><forenames>Bill</forenames></author></authors><title>Model-based Utility Functions</title><categories>cs.AI</categories><comments>24 pages, extensive revisions</comments><journal-ref>Journal of Artificial General Intelligence 3(1) 1-24, 2012</journal-ref><doi>10.2478/v10229-011-0013-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orseau and Ring, as well as Dewey, have recently described problems,
including self-delusion, with the behavior of agents using various definitions
of utility functions. An agent's utility function is defined in terms of the
agent's history of interactions with its environment. This paper argues, via
two examples, that the behavior problems can be avoided by formulating the
utility function in two steps: 1) inferring a model of the environment from
interactions, and 2) computing utility as a function of the environment model.
Basing a utility function on a model that the agent must learn implies that the
utility function must initially be expressed in terms of specifications to be
matched to structures in the learned model. These specifications constitute
prior assumptions about the environment so this approach will not work with
arbitrary environments. But the approach should work for agents designed by
humans to act in the physical world. The paper also addresses the issue of
self-modifying agents and shows that if provided with the possibility to modify
their utility functions agents will not choose to do so, under some usual
assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3966</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3966</id><created>2011-11-16</created><updated>2011-12-06</updated><authors><author><keyname>Wu</keyname><forenames>Zhuohua</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Partial Decode-Forward Binning Schemes for the Causal Cognitive Relay
  Channels</title><categories>cs.IT math.IT</categories><comments>revised introduction and comparison, submitted to IEEE Trans. on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal cognitive relay channel (CRC) has two sender-receiver pairs, in
which the second sender obtains information from the first sender causally and
assists the transmission of both senders. In this paper, we study both the
full- and half-duplex modes. In each mode, we propose two new coding schemes
built successively upon one another to illustrate the impact of different
coding techniques. The first scheme called partial decode-forward binning
(PDF-binning) combines the ideas of partial decode-forward relaying and
Gelfand-Pinsker binning. The second scheme called Han-Kobayashi partial
decode-forward binning (HK-PDF-binning) combines PDF-binning with Han-Kobayashi
coding by further splitting rates and applying superposition coding,
conditional binning and relaxed joint decoding.
  In both schemes, the second sender decodes a part of the message from the
first sender, then uses Gelfand-Pinsker binning technique to bin against the
decoded codeword, but in such a way that allows both state nullifying and
forwarding. For the Gaussian channels, this PDF-binning essentializes to a
correlation between the transmit signal and the binning state, which
encompasses the traditional dirty-paper-coding binning as a special case when
this correlation factor is zero. We also provide the closed-form optimal
binning parameter for each scheme.
  The 2-phase half-duplex schemes are adapted from the full-duplex ones by
removing block Markov encoding, sending different message parts in different
phases and applying joint decoding across both phases. Analysis shows that the
HK-PDF-binning scheme in both modes encompasses the Han-Kobayashi rate region
and achieves both the partial decode-forward relaying rate for the first sender
and interference-free rate for the second sender. Furthermore, this scheme
outperforms all existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3969</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3969</id><created>2011-11-16</created><updated>2011-11-18</updated><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Le&#xf3;n</keyname><forenames>Alejandro J.</forenames></author></authors><title>The Object Projection Feature Estimation Problem in Unsupervised
  Markerless 3D Motion Tracking</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D motion tracking is a critical task in many computer vision applications.
Existing 3D motion tracking techniques require either a great amount of
knowledge on the target object or specific hardware. These requirements
discourage the wide spread of commercial applications based on 3D motion
tracking. 3D motion tracking systems that require no knowledge on the target
object and run on a single low-budget camera require estimations of the object
projection features (namely, area and position). In this paper, we define the
object projection feature estimation problem and we present a novel 3D motion
tracking system that needs no knowledge on the target object and that only
requires a single low-budget camera, as installed in most computers and
smartphones. Our system estimates, in real time, the three-dimensional position
of a non-modeled unmarked object that may be non-rigid, non-convex, partially
occluded, self occluded, or motion blurred, given that it is opaque, evenly
colored, and enough contrasting with the background in each frame. Our system
is also able to determine the most relevant object to track in the screen. Our
3D motion tracking system does not impose hard constraints, therefore it allows
a market-wide implementation of applications that use 3D motion tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3970</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3970</id><created>2011-11-16</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cubero</keyname><forenames>Juan-Carlos</forenames></author></authors><title>A Tool for Model-Based Language Specification</title><categories>cs.SE cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal languages let us define the textual representation of data with
precision. Formal grammars, typically in the form of BNF-like productions,
describe the language syntax, which is then annotated for syntax-directed
translation and completed with semantic actions. When, apart from the textual
representation of data, an explicit representation of the corresponding data
structure is required, the language designer has to devise the mapping between
the suitable data model and its proper language specification, and then develop
the conversion procedure from the parse tree to the data model instance.
Unfortunately, whenever the format of the textual representation has to be
modified, changes have to propagated throughout the entire language processor
tool chain. These updates are time-consuming, tedious, and error-prone.
Besides, in case different applications use the same language, several copies
of the same language specification have to be maintained. In this paper, we
introduce a model-based parser generator that decouples language specification
from language processing, hence avoiding many of the problems caused by
grammar-driven parsers and parser generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3971</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3971</id><created>2011-11-15</created><authors><author><keyname>Suslo</keyname><forenames>Tomasz</forenames></author></authors><title>The Numerical Generalized Least-Squares Estimator of an Unknown Constant
  Mean of Random Field</title><categories>cs.NA</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We constraint on computer the best linear unbiased generalized statistics of
random field for the best linear unbiased generalized statistics of an unknown
constant mean of random field and derive the numerical generalized
least-squares estimator of an unknown constant mean of random field. We derive
the third constraint of spatial statistics and show that the classic
generalized least-squares estimator of an unknown constant mean of the field is
only an asymptotic disjunction of the numerical one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3983</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3983</id><created>2011-11-16</created><authors><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Goodman</keyname><forenames>Alyssa</forenames></author><author><keyname>Muench</keyname><forenames>August</forenames></author></authors><title>The ADS All-Sky Survey</title><categories>astro-ph.IM cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ADS All-Sky Survey (ADSASS) is an ongoing effort aimed at turning the
NASA Astrophysics Data System (ADS), widely known for its unrivaled value as a
literature resource for astronomers, into a data resource. The ADS is not a
data repository per se, but it implicitly contains valuable holdings of
astronomical data, in the form of images, tables and object references
contained within articles. The objective of the ADSASS effort is to extract
these data and make them discoverable and available through existing data
viewers. The resulting ADSASS data layer promises to greatly enhance workflows
and enable new research by tying astronomical literature and data assets into
one resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.3996</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.3996</id><created>2011-11-16</created><updated>2013-01-10</updated><authors><author><keyname>Kov&#xe1;&#x10d;</keyname><forenames>Jakub</forenames></author></authors><title>Complexity of the path avoiding forbidden pairs problem revisited</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G = (V, E) be a directed acyclic graph with two distinguished vertices s,
t and let F be a set of forbidden pairs of vertices. We say that a path in G is
safe, if it contains at most one vertex from each pair {u, v} in F. Given G and
F, the path avoiding forbidden pairs (PAFP) problem is to find a safe s-t path
in G. We systematically study the complexity of different special cases of the
PAFP problem defined according to the mutual positions of forbidden pairs. Fix
one topological ordering of vertices; we say that pairs {u, v} and {x, y} are
disjoint, if u, v &lt; x, y, nested, if u &lt; x, y &lt; v, and halving, if u &lt; x &lt; v &lt;
y. The PAFP problem is known to be NP-hard in general or if no two pairs are
disjoint; we prove that it remains NP-hard even when no two forbidden pairs are
nested. On the other hand, if no two pairs are halving, the problem is known to
be solvable in cubic time. We simplify and improve this result by showing an
O(M(n)) time algorithm, where M(n) is the time to multiply two n \times n
boolean matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4034</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4034</id><created>2011-11-17</created><authors><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Agarwal</keyname><forenames>N. K.</forenames></author></authors><title>Proposed Cellular Network for Indian Conditions for Enhancement of
  Spectral Density and Reduction of Power Consumption &amp; RF Pollution</title><categories>cs.CY</categories><comments>5 pages, 7 figures and IEEE international conference; IEEE
  international conference, ICCCT-2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the exponentially increasing demand for wireless communications the
capacity of current cellular systems will soon become incapable of handling the
growing traffic. Since radio frequencies are diminishing natural resources,
there seems to be a fundamental barrier to further capacity increase. The
solution can be found by using smart antenna systems. Smart or adaptive antenna
arrays consist of an array of antenna elements with signal processing
capability that optimizes the radiation and reception of a desired signal,
dynamically. Smart antenna can place nulls in the direction of interferers via
adapting adaptive updating of weights linked to each antenna element. They thus
cancel out most of the cochannel interference resulting in better quality of
reception and lower dropped calls. Smart antenna can also track the user within
a cell via direction of arrival algorithms. This paper focuses on about the
smart antenna in hierarchical cell clustering (overlay-underlay) with demand
based frequency allocation techniques in cellular mobile radio networks in
INDIA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4036</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4036</id><created>2011-11-17</created><authors><author><keyname>Chakraborty</keyname><forenames>Tamal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Atri</forenames></author><author><keyname>Misra</keyname><forenames>Iti Saha</forenames></author><author><keyname>Sanyal</keyname><forenames>Salil Kumar</forenames></author></authors><title>VoIP Call Optimization in Diverse Network Scenarios Using Learning Based
  State-Space Search Technique</title><categories>cs.NI</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  3, No. 5, 2011, 211-228</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A VoIP based call has stringent QoS requirements with respect to delay,
jitter, loss, MOS and R-Factor. Various QoS mechanisms implemented to satisfy
these requirements must be adaptive under diverse network scenarios and applied
in proper sequence, otherwise they may conflict with each other. The objective
of this paper is to address the problem of adaptive QoS maintenance and
sequential execution of available QoS implementation mechanisms with respect to
VoIP under varying network conditions. In this paper, we generalize this
problem as state-space problem and solve it. Firstly, we map the problem of QoS
optimization into state-space domain and apply incremental heuristic search. We
implement the proposed algorithm under various network and user scenarios in a
VoIP test-bed for QoS enhancement. Then learning strategy is implemented for
refinement of knowledge base to improve the performance of call quality over
time. Finally, we discuss the advantages and uniqueness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4045</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4045</id><created>2011-11-17</created><authors><author><keyname>Rebollo-Monedero</keyname><forenames>David</forenames></author><author><keyname>Parra-Arnau</keyname><forenames>Javier</forenames></author><author><keyname>Forn&#xe9;</keyname><forenames>Jordi</forenames></author></authors><title>An Information-Theoretic Privacy Criterion for Query Forgery in
  Information Retrieval</title><categories>cs.IT cs.CR math.IT</categories><comments>This paper has 15 pages and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, we presented a novel information-theoretic privacy
criterion for query forgery in the domain of information retrieval. Our
criterion measured privacy risk as a divergence between the user's and the
population's query distribution, and contemplated the entropy of the user's
distribution as a particular case. In this work, we make a twofold
contribution. First, we thoroughly interpret and justify the privacy metric
proposed in our previous work, elaborating on the intimate connection between
the celebrated method of entropy maximization and the use of entropies and
divergences as measures of privacy. Secondly, we attempt to bridge the gap
between the privacy and the information-theoretic communities by substantially
adapting some technicalities of our original work to reach a wider audience,
not intimately familiar with information theory and the method of types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4052</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4052</id><created>2011-11-17</created><authors><author><keyname>Thai</keyname><forenames>Le Hoang</forenames></author><author><keyname>Nguyen</keyname><forenames>Nguyen Do Thai</forenames></author><author><keyname>Hai</keyname><forenames>Tran Son</forenames></author></authors><title>A Facial Expression Classification System Integrating Canny, Principal
  Component Analysis and Artificial Neural Network</title><categories>cs.CV</categories><comments>6 pages, 10 figures, International Journal of Machine Learning and
  Computing, Vol. 1, No. 4, October 2011, ISSN (Online): 2010-3700,
  http://www.ijmlc.org/</comments><journal-ref>International Journal of Machine Learning and Computing, Vol. 1,
  No. 4, 2011, 388-393</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial Expression Classification is an interesting research problem in recent
years. There are a lot of methods to solve this problem. In this research, we
propose a novel approach using Canny, Principal Component Analysis (PCA) and
Artificial Neural Network. Firstly, in preprocessing phase, we use Canny for
local region detection of facial images. Then each of local region's features
will be presented based on Principal Component Analysis (PCA). Finally, using
Artificial Neural Network (ANN)applies for Facial Expression Classification. We
apply our proposal method (Canny_PCA_ANN) for recognition of six basic facial
expressions on JAFFE database consisting 213 images posed by 10 Japanese female
models. The experimental result shows the feasibility of our proposal method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4083</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4083</id><created>2011-11-17</created><authors><author><keyname>Berthier</keyname><forenames>Denis</forenames><affiliation>DSI</affiliation></author></authors><title>Unbiased Statistics of a CSP - A Controlled-Bias Generator</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Innovations in Computing Sciences and Software Engineering, (2010)
  165-170</journal-ref><doi>10.1007/978-90-481-3660-5_28</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that estimating the complexity (mean and distribution) of the
instances of a fixed size Constraint Satisfaction Problem (CSP) can be very
hard. We deal with the main two aspects of the problem: defining a measure of
complexity and generating random unbiased instances. For the first problem, we
rely on a general framework and a measure of complexity we presented at
CISSE08. For the generation problem, we restrict our analysis to the Sudoku
example and we provide a solution that also explains why it is so difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4090</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4090</id><created>2011-11-17</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Aniruddha</forenames></author><author><keyname>Banerjee</keyname><forenames>Arnab</forenames></author><author><keyname>Bose</keyname><forenames>Dipayan</forenames></author><author><keyname>Saha</keyname><forenames>Himadri Nath</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Debika</forenames></author></authors><title>Different types of attacks in Mobile ADHOC Network</title><categories>cs.NI</categories><comments>11 Pages, 8 Figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Security in mobile AD HOC network is a big challenge as it has no centralized
authority which can supervise the individual nodes operating in the network.
The attacks can come from both inside the network and from the outside. We are
trying to classify the existing attacks into two broad categories: DATA traffic
attacks and CONTROL traffic attacks. We will also be discussing the presently
proposed methods of mitigating those attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4121</identifier>
 <datestamp>2012-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4121</id><created>2011-11-17</created><updated>2012-02-02</updated><authors><author><keyname>Zwirn</keyname><forenames>Herve</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author></authors><title>Unpredictability and Computational Irreducibility</title><categories>cs.CC</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore several concepts for analyzing the intuitive notion of
computational irreducibility and we propose a robust formal definition, first
in the field of cellular automata and then in the general field of any
computable function f from N to N. We prove that, through a robust definition
of what means &quot;to be unable to compute the nth step without having to follow
the same path than simulating the automaton or the function&quot;, this implies
genuinely, as intuitively expected, that if the behavior of an object is
computationally irreducible, no computation of its nth state can be faster than
the simulation itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4136</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4136</id><created>2011-11-16</created><authors><author><keyname>Gr&#xfc;n</keyname><forenames>Christine</forenames></author></authors><title>A probabilistic-numerical approximation for an obstacle problem arising
  in game theory</title><categories>cs.GT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a two-player zero-sum stochastic differential game in which
one of the players has more information on the game than his opponent. We show
how to construct numerical schemes for the value function of this game, which
is given by the solution of a quasilinear partial differential equation with
obstacle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4144</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4144</id><created>2011-11-17</created><updated>2013-10-17</updated><authors><author><keyname>Krishnamoorthy</keyname><forenames>Aravindh</forenames></author><author><keyname>Menon</keyname><forenames>Deepak</forenames></author></authors><title>Matrix Inversion Using Cholesky Decomposition</title><categories>cs.MS</categories><comments>3pp with minor changes, pre-print; accepted for publication 2013 IEEE
  Signal Processing: Algorithms, Architectures, Arrangements, and Applications
  (SPA) conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method for matrix inversion based on Cholesky
decomposition with reduced number of operations by avoiding computation of
intermediate results; further, we use fixed point simulations to compare the
numerical accuracy of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4174</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4174</id><created>2011-11-17</created><updated>2012-10-06</updated><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Universal Strongly Secure Network Coding with Dependent and Non-Uniform
  Messages</title><categories>cs.IT cs.CR cs.NI math.IT</categories><comments>8 pages, 1 figure, IEEEtrans.cls. Ver 3 emphasizes the difference
  between the universal secure network coding and the secret sharing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the random linear precoder at the source node as a secure network
coding. We prove that it is strongly secure in the sense of Harada and Yamamoto
and universal secure in the sense of Silva and Kschischang, while allowing
arbitrary small but nonzero mutual information to the eavesdropper. Our
security proof allows statistically dependent and non-uniform multiple secret
messages, while all previous constructions of weakly or strongly secure network
coding assumed independent and uniform messages, which are difficult to be
ensured in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4181</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4181</id><created>2011-11-17</created><updated>2012-03-06</updated><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Rivero</keyname><forenames>Alejandro</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Locating privileged spreaders on an Online Social Network</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 4 figures</comments><doi>10.1103/PhysRevE.85.066123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media have provided plentiful evidence of their capacity for
information diffusion. Fads and rumors, but also social unrest and riots travel
fast and affect large fractions of the population participating in online
social networks (OSNs). This has spurred much research regarding the mechanisms
that underlie social contagion, and also who (if any) can unleash system-wide
information dissemination. Access to real data, both regarding topology --the
network of friendships-- and dynamics --the actual way in which OSNs users
interact--, is crucial to decipher how the former facilitates the latter's
success, understood as efficiency in information spreading. With the
quantitative analysis that stems from complex network theory, we discuss who
(and why) has privileged spreading capabilities when it comes to information
diffusion. This is done considering the evolution of an episode of political
protest which took place in Spain, spanning one month in 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4189</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4189</id><created>2011-11-17</created><authors><author><keyname>Georgakopoulos</keyname><forenames>Agelos</forenames></author><author><keyname>Winkler</keyname><forenames>Peter</forenames></author></authors><title>Two-Color Babylon</title><categories>math.CO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the game of Babylon when played with chips of two colors, giving a
winning strategy for the second player in all previously unsolved cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4232</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4232</id><created>2011-11-17</created><authors><author><keyname>Sorudeykin</keyname><forenames>Kirill A.</forenames></author></authors><title>A Model of Spatial Thinking for Computational Intelligence</title><categories>cs.AI</categories><comments>8 pages, 5 figures; IEEE East-West Design &amp; Test Symposium, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trying to be effective (no matter who exactly and in what field) a person
face the problem which inevitably destroys all our attempts to easily get to a
desired goal. The problem is the existence of some insuperable barriers for our
mind, anotherwords barriers for principles of thinking. They are our clue and
main reason for research. Here we investigate these barriers and their features
exposing the nature of mental process. We start from special structures which
reflect the ways to define relations between objects. Then we came to realizing
about what is the material our mind uses to build thoughts, to make
conclusions, to understand, to form reasoning, etc. This can be called a mental
dynamics. After this the nature of mental barriers on the required level of
abstraction as well as the ways to pass through them became clear. We begin to
understand why thinking flows in such a way, with such specifics and with such
limitations we can observe in reality. This can help us to be more optimal. At
the final step we start to understand, what ma-thematical models can be applied
to such a picture. We start to express our thoughts in a language of
mathematics, developing an apparatus for our Spatial Theory of Mind, suitable
to represent processes and infrastructure of thinking. We use abstract algebra
and stay invariant in relation to the nature of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4244</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4244</id><created>2011-11-17</created><authors><author><keyname>Parvaresh</keyname><forenames>Farzad</forenames></author><author><keyname>Etkin</keyname><forenames>Raul</forenames></author></authors><title>Efficient Capacity Computation and Power Optimization for Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity or approximations to capacity of various single-source
single-destination relay network models has been characterized in terms of the
cut-set upper bound. In principle, a direct computation of this bound requires
evaluating the cut capacity over exponentially many cuts. We show that the
minimum cut capacity of a relay network under some special assumptions can be
cast as a minimization of a submodular function, and as a result, can be
computed efficiently. We use this result to show that the capacity, or an
approximation to the capacity within a constant gap for the Gaussian, wireless
erasure, and Avestimehr-Diggavi-Tse deterministic relay network models can be
computed in polynomial time. We present some empirical results showing that
computing constant-gap approximations to the capacity of Gaussian relay
networks with around 300 nodes can be done in order of minutes.
  For Gaussian networks, cut-set capacities are also functions of the powers
assigned to the nodes. We consider a family of power optimization problems and
show that they can be solved in polynomial time. In particular, we show that
the minimization of the sum of powers assigned to the nodes subject to a
minimum rate constraint (measured in terms of cut-set bounds) can be computed
in polynomial time. We propose an heuristic algorithm to solve this problem and
measure its performance through simulations on random Gaussian networks. We
observe that in the optimal allocations most of the power is assigned to a
small subset of relays, which suggests that network simplification may be
possible without excessive performance degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4246</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4246</id><created>2011-11-17</created><authors><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author><author><keyname>Gelman</keyname><forenames>Andrew</forenames></author></authors><title>The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian
  Monte Carlo</title><categories>stat.CO cs.LG</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm
that avoids the random walk behavior and sensitivity to correlated parameters
that plague many MCMC methods by taking a series of steps informed by
first-order gradient information. These features allow it to converge to
high-dimensional target distributions much more quickly than simpler methods
such as random walk Metropolis or Gibbs sampling. However, HMC's performance is
highly sensitive to two user-specified parameters: a step size {\epsilon} and a
desired number of steps L. In particular, if L is too small then the algorithm
exhibits undesirable random walk behavior, while if L is too large the
algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an
extension to HMC that eliminates the need to set a number of steps L. NUTS uses
a recursive algorithm to build a set of likely candidate points that spans a
wide swath of the target distribution, stopping automatically when it starts to
double back and retrace its steps. Empirically, NUTS perform at least as
efficiently as and sometimes more efficiently than a well tuned standard HMC
method, without requiring user intervention or costly tuning runs. We also
derive a method for adapting the step size parameter {\epsilon} on the fly
based on primal-dual averaging. NUTS can thus be used with no hand-tuning at
all. NUTS is also suitable for applications such as BUGS-style automatic
inference engines that require efficient &quot;turnkey&quot; sampling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4267</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4267</id><created>2011-11-17</created><authors><author><keyname>Rodriguez-Toro</keyname><forenames>Victor A.</forenames></author><author><keyname>Garzon</keyname><forenames>Jaime E.</forenames></author><author><keyname>Lopez</keyname><forenames>Jesus A.</forenames></author></authors><title>Control Neuronal por Modelo Inverso de un Servosistema Usando Algoritmos
  de Aprendizaje Levenberg-Marquardt y Bayesiano</title><categories>cs.AI cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the experimental results of the neural network
control of a servo-system in order to control its speed. The control strategy
is implemented by using an inverse-model control based on Artificial Neural
Networks (ANNs). The network training was performed using two learning
algorithms: Levenberg-Marquardt and Bayesian regularization. We evaluate the
generalization capability for each method according to both the correct
operation of the controller to follow the reference signal, and the control
efforts developed by the ANN-based controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4278</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4278</id><created>2011-11-17</created><updated>2012-01-31</updated><authors><author><keyname>Huang</keyname><forenames>Haiping</forenames></author><author><keyname>Zhou</keyname><forenames>Haijun</forenames></author></authors><title>Counting solutions from finite samplings</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT physics.bio-ph</categories><comments>9 pages, 4 figures and 1 table, further discussions added</comments><journal-ref>Phys. Rev. E 85, 026118 (2012)</journal-ref><doi>10.1103/PhysRevE.85.026118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the solution counting problem within the framework of inverse
Ising problem and use fast belief propagation equations to estimate the entropy
whose value provides an estimate on the true one. We test this idea on both
diluted models (random 2-SAT and 3-SAT problems) and fully-connected model
(binary perceptron), and show that when the constraint density is small, this
estimate can be very close to the true value. The information stored by the
salamander retina under the natural movie stimuli can also be estimated and our
result is consistent with that obtained by Monte Carlo method. Of particular
significance is sizes of other metastable states for this real neuronal network
are predicted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4279</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4279</id><created>2011-11-17</created><authors><author><keyname>Roy</keyname><forenames>Sourya</forenames></author><author><keyname>Clemons</keyname><forenames>Tyler</forenames></author><author><keyname>Faisal</keyname><forenames>S M</forenames></author><author><keyname>Liu</keyname><forenames>Ke</forenames></author><author><keyname>Hardavellas</keyname><forenames>Nikos</forenames></author><author><keyname>Parthasarathy</keyname><forenames>Srinivasan</forenames></author></authors><title>Elastic Fidelity: Trading-off Computational Accuracy for Energy
  Reduction</title><categories>cs.AR</categories><report-no>Northwestern University NWU-EECS-11-02, February 2011</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power dissipation and energy consumption have become one of the most
important problems in the design of processors today. This is especially true
in power-constrained environments, such as embedded and mobile computing. While
lowering the operational voltage can reduce power consumption, there are limits
imposed at design time, beyond which hardware components experience faulty
operation. Moreover, the decrease in feature size has led to higher
susceptibility to process variations, leading to reliability issues and
lowering yield. However, not all computations and all data in a workload need
to maintain 100% fidelity. In this paper, we explore the idea of employing
functional or storage units that let go the conservative guardbands imposed on
the design to guarantee reliable execution. Rather, these units exhibit Elastic
Fidelity, by judiciously lowering the voltage to trade-off reliable execution
for power consumption based on the error guarantees required by the executing
code. By estimating the accuracy required by each computational segment of a
workload, and steering each computation to different functional and storage
units, Elastic Fidelity Computing obtains power and energy savings while
reaching the reliability targets required by each computational segment. Our
preliminary results indicate that even with conservative estimates, Elastic
Fidelity can reduce the power and energy consumption of a processor by 11-13%
when executing applications involving human perception that are typically
included in modern mobile platforms, such as audio, image, and video decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4280</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4280</id><created>2011-11-17</created><authors><author><keyname>Rodriguez-Toro</keyname><forenames>Victor A.</forenames></author><author><keyname>Noguera-Leon</keyname><forenames>Fabio</forenames></author><author><keyname>Velasco-Medina</keyname><forenames>Jaime</forenames></author></authors><title>Dise\~no de una Arquitectura para la Solucion de la Ecuacion de
  Schroedinger usando el Metodo de Numerov</title><categories>physics.comp-ph cs.NA quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a first approach in order to design an optimal
architecture to implement the Numerov method, which solves the time-independent
Schroedinger equation (TISE) for one dimension. The design and simulation have
been performed by using 64-bits floating-point megafunctions available in
Quartus II (Version 9.0). The verification of these results was done by using
Matlab. According to these results, it is possible to extend this design to
parallel structures, which would be able to calculate several TISE solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4287</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4287</id><created>2011-11-18</created><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author></authors><title>Parametric Estimation of the Ultimate Size of Hypercomputers</title><categories>cs.PF cs.AR</categories><comments>6 pages, 2 figures, published in Proc. 6th World Multiconference on
  Systemics, Cybernetics and Informatics, 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of the emerging petaflops-scale supercomputers of the nearest
future (hypercomputers) will be governed not only by the clock frequency of the
processing nodes or by the width of the system bus, but also by such factors as
the overall power consumption and the geometric size. In this paper, we study
the influence of such parameters on one of the most important characteristics
of a general purpose computer - on the degree of multithreading that must be
present in an application to make the use of the hypercomputer justifiable. Our
major finding is that for the class of applications with purely random memory
access patterns &quot;super-fast computing&quot; and &quot;high-performance computing&quot; are
essentially synonyms for &quot;massively-parallel computing.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4289</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4289</id><created>2011-11-18</created><updated>2011-11-21</updated><authors><author><keyname>Fisher</keyname><forenames>Axman</forenames></author></authors><title>Interfacial Numerical Dispersion and New Conformal FDTD Method</title><categories>physics.comp-ph cs.CE cs.NA</categories><comments>9 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This article shows the interfacial relation in electrodynamics shall be
corrected in discrete grid form which can be seen as certain numerical
dispersion beyond the usual bulk type. Furthermore we construct a lossy
conductor model to illustrate how to simulate more general materials other than
traditional PEC or simple dielectrics, by a new conformal FDTD method which
main considers the effects of penetrative depth and the distribution of free
bulk electric charge and current.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4290</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4290</id><created>2011-11-18</created><authors><author><keyname>Dhandra</keyname><forenames>B. V.</forenames></author><author><keyname>Benne</keyname><forenames>R. G.</forenames></author><author><keyname>Hangarge</keyname><forenames>Mallikarjun</forenames></author></authors><title>A Single Euler Number Feature for Multi-font Multi-size Kannada Numeral
  Recognition</title><categories>cs.CV</categories><comments>4 pages, 1 figure, 5 tables, &quot;Recent Trends in Information
  Technology(RTIT-2009)&quot;</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper a novel approach is proposed based on single Euler number
feature which is free from thinning and size normalization for multi-font and
multi-size Kannada numeral recognition system. A nearest neighbor
classification is used for classification of Kannada numerals by considering
the Euclidian distance. A total 1500 numeral images with different font sizes
between (10..84) are tested for algorithm efficiency and the overall the
classification accuracy is found to be 99.00% .The said method is thinning
free, fast, and showed encouraging results on varying font styles and sizes of
Kannada numerals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4291</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4291</id><created>2011-11-18</created><authors><author><keyname>Dhandra</keyname><forenames>B. V.</forenames></author><author><keyname>Benne</keyname><forenames>R. G.</forenames></author><author><keyname>Hangarge</keyname><forenames>Mallikarjun</forenames></author></authors><title>Multi-font Multi-size Kannada Numeral Recognition Based on Structural
  Features</title><categories>cs.CV</categories><comments>5 pages, 5 figures, 4 tables,&quot;Emerging Trends in Information
  Technology(eIT-2007), India&quot;</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper a fast and novel method is proposed for multi-font multi-size
Kannada numeral recognition which is thinning free and without size
normalization approach. The different structural feature are used for numeral
recognition namely, directional density of pixels in four directions, water
reservoirs, maximum profile distances, and fill hole density are used for the
recognition of Kannada numerals. A Euclidian minimum distance criterion is used
to find minimum distances and K-nearest neighbor classifier is used to classify
the Kannada numerals by varying the size of numeral image from 16 to 50 font
sizes for the 20 different font styles from NUDI and BARAHA popular word
processing Kannada software. The total 1150 numeral images are tested and the
overall accuracy of classification is found to be 100%. The average time taken
by this method is 0.1476 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4297</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4297</id><created>2011-11-18</created><authors><author><keyname>Chen</keyname><forenames>Cheng</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author><author><keyname>Srinivasan</keyname><forenames>Venkatesh</forenames></author><author><keyname>Zhang</keyname><forenames>Xudong</forenames></author></authors><title>Battling the Internet Water Army: Detection of Hidden Paid Posters</title><categories>cs.SI</categories><comments>10 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a systematic study to help distinguish a special group of online
users, called hidden paid posters, or termed &quot;Internet water army&quot; in China,
from the legitimate ones. On the Internet, the paid posters represent a new
type of online job opportunity. They get paid for posting comments and new
threads or articles on different online communities and websites for some
hidden purposes, e.g., to influence the opinion of other people towards certain
social events or business markets. Though an interesting strategy in business
marketing, paid posters may create a significant negative effect on the online
communities, since the information from paid posters is usually not
trustworthy. When two competitive companies hire paid posters to post fake news
or negative comments about each other, normal online users may feel overwhelmed
and find it difficult to put any trust in the information they acquire from the
Internet. In this paper, we thoroughly investigate the behavioral pattern of
online paid posters based on real-world trace data. We design and validate a
new detection mechanism, using both non-semantic analysis and semantic
analysis, to identify potential online paid posters. Our test results with
real-world datasets show a very promising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4299</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4299</id><created>2011-11-18</created><updated>2011-12-14</updated><authors><author><keyname>Mastrolilli</keyname><forenames>Monaldo</forenames></author></authors><title>The Feedback Arc Set Problem with Triangle Inequality is a Vertex Cover
  Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the (precedence constrained) Minimum Feedback Arc Set problem
with triangle inequalities on the weights, which finds important applications
in problems of ranking with inconsistent information. We present a surprising
structural insight showing that the problem is a special case of the minimum
vertex cover in hypergraphs with edges of size at most 3. This result leads to
combinatorial approximation algorithms for the problem and opens the road to
studying the problem as a vertex cover problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4300</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4300</id><created>2011-11-18</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Weil</keyname><forenames>Pascal</forenames><affiliation>LaBRI</affiliation></author></authors><title>Star-Free Languages are Church-Rosser Congruential</title><categories>cs.FL</categories><proxy>ccsd</proxy><journal-ref>Theoretical Computer Science 454 (2012) 129-135</journal-ref><doi>10.1016/j.tcs.2012.01.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of Church-Rosser congruential languages has been introduced by
McNaughton, Narendran, and Otto in 1988. A language L is Church-Rosser
congruential (belongs to CRCL), if there is a finite, confluent, and
length-reducing semi-Thue system S such that L is a finite union of congruence
classes modulo S. To date, it is still open whether every regular language is
in CRCL. In this paper, we show that every star-free language is in CRCL. In
fact, we prove a stronger statement: For every star-free language L there
exists a finite, confluent, and subword-reducing semi-Thue system S such that
the total number of congruence classes modulo S is finite and such that L is a
union of congruence classes modulo S. The construction turns out to be
effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4301</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4301</id><created>2011-11-18</created><authors><author><keyname>Bogdanov</keyname><forenames>Andrej</forenames></author><author><keyname>Lee</keyname><forenames>Chin Ho</forenames></author></authors><title>Homomorphic encryption from codes</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new homomorphic encryption scheme based on the hardness of
decoding under independent random noise from certain affine families of codes.
Unlike in previous lattice-based homomorphic encryption schemes, where the
message is hidden in the noisy part of the ciphertext, our scheme carries the
message in the affine part of the transformation and applies noise only to
achieve security. Our scheme can tolerate noise of arbitrary magnitude, as long
as the noise vector has sufficiently small hamming weight (and its entries are
independent).
  Our design achieves &quot;proto-homomorphic&quot; properties in an elementary manner:
message addition and multiplication are emulated by pointwise addition and
multiplication of the ciphertext vectors. Moreover, the extremely simple nature
of our decryption makes the scheme easily amenable to bootstrapping. However,
some complications are caused by the inherent presence of noticeable encryption
error. Our main technical contribution is the development of two new techniques
for handling this error in the homomorphic evaluation process.
  We also provide a definitional framework for homomorphic encryption that may
be useful elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4316</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4316</id><created>2011-11-18</created><authors><author><keyname>Fionda</keyname><forenames>Valeria</forenames></author><author><keyname>Gutierrez</keyname><forenames>Claudio</forenames></author><author><keyname>Pirr&#xf3;</keyname><forenames>Giuseppe</forenames></author></authors><title>Semantic Navigation on the Web of Data: Specification of Routes, Web
  Fragments and Actions</title><categories>cs.NI cs.CL</categories><comments>10 pages - first submission</comments><acm-class>H.3.3; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive semantic data sources linked in the Web of Data give new meaning
to old features like navigation; introduce new challenges like semantic
specification of Web fragments; and make it possible to specify actions relying
on semantic data. In this paper we introduce a declarative language to face
these challenges. Based on navigational features, it is designed to specify
fragments of the Web of Data and actions to be performed based on these data.
We implement it in a centralized fashion, and show its power and performance.
Finally, we explore the same ideas in a distributed setting, showing their
feasibility, potentialities and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4335</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4335</id><created>2011-11-18</created><authors><author><keyname>Karjee</keyname><forenames>Jyotirmoy</forenames></author><author><keyname>Jamadagni</keyname><forenames>H. S</forenames></author></authors><title>Energy Aware Node Selection for Cluster-based Data Accuracy Estimation
  in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The main objective of this paper is to reduce the number of sensor nodes by
estimating a trade off between data accuracy and energy consumption for
selecting nodes in probabilistic approach in distributed networks. Design
Procedure/Approach: Observed data are highly correlated among sensor nodes in
the spatial domain due to deployment of high density of sensor nodes. These
sensor nodes form non-overlapping distributed clusters due to high data
correlation among them. We develop a probabilistic model for each distributed
cluster to perform data accuracy and energy consumption model in the network.
Finally we find a trade off between data accuracy and energy consumption model
to select an optimal number of sensor nodes in each distributed cluster. We
also compare the performance for our data accuracy estimation model with
information accuracy model for each distributed cluster in the network.
Practical Implementation: Measuring temperature in physical environment and
measuring moisture content in agricultural field. Inventive /Novel Idea:
Optimal node selection in probabilistic approach using the trade of between
data accuracy and energy consumption in cluster-based distributed network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4339</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4339</id><created>2011-11-18</created><updated>2013-11-27</updated><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Li</keyname><forenames>Angsheng</forenames></author></authors><title>Kolmogorov complexity and computably enumerable sets</title><categories>math.LO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computably enumerable sets in terms of the: (a) Kolmogorov
complexity of their initial segments; (b) Kolmogorov complexity of finite
programs when they are used as oracles. We present an extended discussion of
the existing research on this topic, along with recent developments and open
problems. Besides this survey, our main original result is the following
characterization of the computably enumerable sets with trivial initial segment
prefix-free complexity. A computably enumerable set $A$ is $K$-trivial if and
only if the family of sets with complexity bounded by the complexity of $A$ is
uniformly computable from the halting problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4343</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4343</id><created>2011-11-18</created><authors><author><keyname>Ostapov</keyname><forenames>Yuriy</forenames></author></authors><title>Question Answering in a Natural Language Understanding System Based on
  Object-Oriented Semantics</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms of question answering in a computer system oriented on input and
logical processing of text information are presented. A knowledge domain under
consideration is social behavior of a person. A database of the system includes
an internal representation of natural language sentences and supplemental
information. The answer {\it Yes} or {\it No} is formed for a general question.
A special question containing an interrogative word or group of interrogative
words permits to find a subject, object, place, time, cause, purpose and way of
action or event. Answer generation is based on identification algorithms of
persons, organizations, machines, things, places, and times. Proposed
algorithms of question answering can be realized in information systems closely
connected with text processing (criminology, operation of business, medicine,
document systems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4345</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4345</id><created>2011-11-18</created><updated>2012-03-17</updated><authors><author><keyname>Liu</keyname><forenames>Yulong</forenames></author><author><keyname>Mi</keyname><forenames>Tiebin</forenames></author><author><keyname>Li</keyname><forenames>Shidong</forenames></author></authors><title>Compressed Sensing with General Frames via Optimal-dual-based
  $\ell_1$-analysis</title><categories>cs.IT math.IT</categories><comments>34 pages, 8 figures. To appear in IEEE Transactions on Information
  Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 58, no. 7, pp.
  4201-4214, July, 2012</journal-ref><doi>10.1109/TIT.2012.2191612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing with sparse frame representations is seen to have much
greater range of practical applications than that with orthonormal bases. In
such settings, one approach to recover the signal is known as
$\ell_1$-analysis. We expand in this article the performance analysis of this
approach by providing a weaker recovery condition than existing results in the
literature. Our analysis is also broadly based on general frames and
alternative dual frames (as analysis operators). As one application to such a
general-dual-based approach and performance analysis, an optimal-dual-based
technique is proposed to demonstrate the effectiveness of using alternative
dual frames as analysis operators. An iterative algorithm is outlined for
solving the optimal-dual-based $\ell_1$-analysis problem. The effectiveness of
the proposed method and algorithm is demonstrated through several experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4350</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4350</id><created>2011-11-18</created><updated>2011-12-12</updated><authors><author><keyname>Iosifidis</keyname><forenames>George</forenames></author><author><keyname>Chorppath</keyname><forenames>Anil Kumar</forenames></author><author><keyname>Alpcan</keyname><forenames>Tansu</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Iordanis</forenames></author></authors><title>Incentive Mechanisms for Hierarchical Spectrum Markets</title><categories>cs.NI cs.GT cs.SY</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study spectrum allocation mechanisms in hierarchical
multi-layer markets which are expected to proliferate in the near future based
on the current spectrum policy reform proposals. We consider a setting where a
state agency sells spectrum channels to Primary Operators (POs) who
subsequently resell them to Secondary Operators (SOs) through auctions. We show
that these hierarchical markets do not result in a socially efficient spectrum
allocation which is aimed by the agency, due to lack of coordination among the
entities in different layers and the inherently selfish revenue-maximizing
strategy of POs. In order to reconcile these opposing objectives, we propose an
incentive mechanism which aligns the strategy and the actions of the POs with
the objective of the agency, and thus leads to system performance improvement
in terms of social welfare. This pricing-based scheme constitutes a method for
hierarchical market regulation. A basic component of the proposed incentive
mechanism is a novel auction scheme which enables POs to allocate their
spectrum by balancing their derived revenue and the welfare of the SOs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4362</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4362</id><created>2011-11-18</created><authors><author><keyname>Leroux</keyname><forenames>Camille</forenames></author><author><keyname>Raymond</keyname><forenames>Alexandre J.</forenames></author><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Tal</keyname><forenames>Ido</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Hardware Implementation of Successive Cancellation Decoders for Polar
  Codes</title><categories>cs.AR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently-discovered polar codes are seen as a major breakthrough in
coding theory; they provably achieve the theoretical capacity of discrete
memoryless channels using the low complexity successive cancellation (SC)
decoding algorithm. Motivated by recent developments in polar coding theory, we
propose a family of efficient hardware implementations for SC polar decoders.
We show that such decoders can be implemented with O(n) processing elements,
O(n) memory elements, and can provide a constant throughput for a given target
clock frequency. Furthermore, we show that SC decoding can be implemented in
the logarithm domain, thereby eliminating costly multiplication and division
operations and reducing the complexity of each processing element greatly. We
also present a detailed architecture for an SC decoder and provide logic
synthesis results confirming the linear growth in complexity of the decoder as
the code length increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4372</identifier>
 <datestamp>2012-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4372</id><created>2011-11-18</created><updated>2012-02-15</updated><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>An additivity theorem for plain Kolmogorov complexity</title><categories>cs.CC</categories><comments>4 pages</comments><msc-class>68Q30</msc-class><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the formula C(a,b) = K(a|C(a,b)) + C(b|a,C(a,b)) + O(1) that
expresses the plain complexity of a pair in terms of prefix and plain
conditional complexities of its components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4382</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4382</id><created>2011-11-18</created><authors><author><keyname>Dinh</keyname><forenames>Hang</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>Quantum Fourier sampling, Code Equivalence, and the quantum security of
  the McEliece and Sidelnikov cryptosystems</title><categories>cs.CC cs.CR quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Code Equivalence problem is that of determining whether two given linear
codes are equivalent to each other up to a permutation of the coordinates. This
problem has a direct reduction to a nonabelian hidden subgroup problem (HSP),
suggesting a possible quantum algorithm analogous to Shor's algorithms for
factoring or discrete log. However, we recently showed that in many cases of
interest---including Goppa codes---solving this case of the HSP requires rich,
entangled measurements. Thus, solving these cases of Code Equivalence via
Fourier sampling appears to be out of reach of current families of quantum
algorithms.
  Code equivalence is directly related to the security of McEliece-type
cryptosystems in the case where the private code is known to the adversary.
However, for many codes the support splitting algorithm of Sendrier provides a
classical attack in this case. We revisit the claims of our previous article in
the light of these classical attacks, and discuss the particular case of the
Sidelnikov cryptosystem, which is based on Reed-Muller codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4385</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4385</id><created>2011-11-18</created><updated>2014-06-08</updated><authors><author><keyname>Spieler</keyname><forenames>David</forenames><affiliation>Saarland University</affiliation></author><author><keyname>Hahn</keyname><forenames>Ernst Moritz</forenames><affiliation>State Key Laboratory of Computer Science</affiliation></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames><affiliation>State Key Laboratory of Computer Science</affiliation></author></authors><title>Model Checking CSL for Markov Population Models</title><categories>cs.NA cs.LO</categories><comments>In Proceedings QAPL 2014, arXiv:1406.1567</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 154, 2014, pp. 93-107</journal-ref><doi>10.4204/EPTCS.154.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov population models (MPMs) are a widely used modelling formalism in the
area of computational biology and related areas. The semantics of a MPM is an
infinite-state continuous-time Markov chain. In this paper, we use the
established continuous stochastic logic (CSL) to express properties of Markov
population models. This allows us to express important measures of biological
systems, such as probabilistic reachability, survivability, oscillations,
switching times between attractor regions, and various others. Because of the
infinite state space, available analysis techniques only apply to a very
restricted subset of CSL properties. We present a full algorithm for model
checking CSL for MPMs, and provide experimental evidence showing that our
method is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4395</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4395</id><created>2011-11-18</created><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Valenzuela</keyname><forenames>Daniel</forenames></author></authors><title>Practical Top-K Document Retrieval in Reduced Space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supporting top-k document retrieval queries on general text databases, that
is, finding the k documents where a given pattern occurs most frequently, has
become a topic of interest with practical applications. While the problem has
been solved in optimal time and linear space, the actual space usage is a
serious concern. In this paper we study various reduced-space structures that
support top-k retrieval and propose new alternatives. Our experimental results
show that our novel algorithms and data structures dominate almost all the
space/time tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4407</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4407</id><created>2011-11-16</created><authors><author><keyname>Van Gorp</keyname><forenames>Pieter</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Mazanek</keyname><forenames>Steffen</forenames><affiliation>University of York</affiliation></author><author><keyname>Rose</keyname><forenames>Louis</forenames><affiliation>University of York</affiliation></author></authors><title>Proceedings Fifth Transformation Tool Contest</title><categories>cs.SE cs.PL</categories><comments>EPTCS 74, 2011</comments><proxy>EPTCS</proxy><acm-class>D3.2;D3.4</acm-class><doi>10.4204/EPTCS.74</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the Transformation Tool Contest (TTC) series is to compare the
expressiveness, the usability and the performance of graph and model
transformation tools along a number of selected case studies. Participants want
to learn about the pros and cons of each tool considering different
applications. A deeper understanding of the relative merits of different tool
features will help to further improve graph and model transformation tools and
to indicate open problems.
  TTC 2011 involved 25 offline case study solutions: 12 solutions to the Hello
World case, 2 solutions to the GMF Model Migration case, 5 solutions to the
Compiler Optimization case, and 7 solutions to the Reengineering (i.e., Program
Understanding) case. This volume contains the submissions that have passed an
additional (post-workshop) reviewing round.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4442</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4442</id><created>2011-11-18</created><authors><author><keyname>Dainiak</keyname><forenames>Alex</forenames></author></authors><title>Inverse problems for the number of maximal independent sets</title><categories>math.CO cs.DM</categories><comments>1 figure, 8 pages</comments><msc-class>05C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following inverse graph-theoretic problem: how many vertices
should a graph have given that it has a specified value of some parameter. We
obtain asymptotic for the minimal number of vertices of the graph with the
given number $n$ of maximal independent sets for a class of natural numbers
that can be represented as concatenation of periodic binary words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4450</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4450</id><created>2011-11-18</created><authors><author><keyname>Persson</keyname><forenames>Simeon</forenames></author><author><keyname>V&#xe1;rnai</keyname><forenames>Kristian</forenames></author></authors><title>Full Restoration of Visual Encrypted Color Images</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While strictly black and white images have been the basis for visual
cryptography, there has been a lack of an easily implemented format for colour
images. This paper establishes a simple, yet secure way of implementing visual
cryptography with colour, assuming a binary data representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4460</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4460</id><created>2011-11-18</created><authors><author><keyname>Jiang</keyname><forenames>Chong</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author></authors><title>Parametrized Stochastic Multi-armed Bandits with Binary Rewards</title><categories>cs.LG</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of multi-armed bandits with a large,
possibly infinite number of correlated arms. We assume that the arms have
Bernoulli distributed rewards, independent across time, where the probabilities
of success are parametrized by known attribute vectors for each arm, as well as
an unknown preference vector, each of dimension $n$. For this model, we seek an
algorithm with a total regret that is sub-linear in time and independent of the
number of arms. We present such an algorithm, which we call the Two-Phase
Algorithm, and analyze its performance. We show upper bounds on the total
regret which applies uniformly in time, for both the finite and infinite arm
cases. The asymptotics of the finite arm bound show that for any $f \in
\omega(\log(T))$, the total regret can be made to be $O(n \cdot f(T))$. In the
infinite arm case, the total regret is $O(\sqrt{n^3 T})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4470</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4470</id><created>2011-11-18</created><updated>2015-07-14</updated><authors><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Efficient Regression in Metric Spaces via Approximate Lipschitz
  Extension</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for performing efficient regression in general metric
spaces. Roughly speaking, our regressor predicts the value at a new point by
computing a Lipschitz extension --- the smoothest function consistent with the
observed data --- after performing structural risk minimization to avoid
overfitting. We obtain finite-sample risk bounds with minimal structural and
noise assumptions, and a natural speed-precision tradeoff. The offline
(learning) and online (prediction) stages can be solved by convex programming,
but this naive approach has runtime complexity $O(n^3)$, which is prohibitive
for large datasets. We design instead a regression algorithm whose speed and
generalization performance depend on the intrinsic dimension of the data, to
which the algorithm adapts. While our main innovation is algorithmic, the
statistical results may also be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4499</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4499</id><created>2011-11-18</created><authors><author><keyname>Kafaie</keyname><forenames>Somayeh</forenames></author><author><keyname>Kashefi</keyname><forenames>Omid</forenames></author><author><keyname>Sharifi</keyname><forenames>Mohsen</forenames></author></authors><title>A Low-Energy Fast Cyber Foraging Mechanism for Mobile Devices</title><categories>cs.DC</categories><comments>12 pages, 7 figures, International Journal of Wireless &amp; Mobile
  Networks (IJWMN)</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  3, No. 5, October 2011</journal-ref><doi>10.5121/ijwmn.2011.3516</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever increasing demands for using resource-constrained mobile devices for
running more resource intensive applications nowadays has initiated the
development of cyber foraging solutions that offload parts or whole
computational intensive tasks to more powerful surrogate stationary computers
and run them on behalf of mobile devices as required. The choice of proper mix
of mobile devices and surrogates has remained an unresolved challenge though.
In this paper, we propose a new decision-making mechanism for cyber foraging
systems to select the best locations to run an application, based on context
metrics such as the specifications of surrogates, the specifications of mobile
devices, application specification, and communication network specification.
Experimental results show faster response time and lower energy consumption of
benched applications compared to when applications run wholly on mobile devices
and when applications are offloaded to surrogates blindly for execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4500</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4500</id><created>2011-11-18</created><updated>2012-12-14</updated><authors><author><keyname>Travers</keyname><forenames>Nicholas F.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Equivalence of History and Generator Epsilon-Machines</title><categories>math.PR cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML</categories><comments>23 pages, 5 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/hgem.htm; Expanded literature
  review, additional examples and figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epsilon-machines are minimal, unifilar presentations of stationary stochastic
processes. They were originally defined in the history machine sense, as hidden
Markov models whose states are the equivalence classes of infinite pasts with
the same probability distribution over futures. In analyzing synchronization,
though, an alternative generator definition was given: unifilar, edge-emitting
hidden Markov models with probabilistically distinct states. The key difference
is that history epsilon-machines are defined by a process, whereas generator
epsilon-machines define a process. We show here that these two definitions are
equivalent in the finite-state case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4503</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4503</id><created>2011-11-18</created><authors><author><keyname>Ugander</keyname><forenames>Johan</forenames></author><author><keyname>Karrer</keyname><forenames>Brian</forenames></author><author><keyname>Backstrom</keyname><forenames>Lars</forenames></author><author><keyname>Marlow</keyname><forenames>Cameron</forenames></author></authors><title>The Anatomy of the Facebook Social Graph</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of the social graph of active Facebook users, the
largest social network ever analyzed. We compute numerous features of the graph
including the number of users and friendships, the degree distribution, path
lengths, clustering, and mixing patterns. Our results center around three main
observations. First, we characterize the global structure of the graph,
determining that the social network is nearly fully connected, with 99.91% of
individuals belonging to a single large connected component, and we confirm the
&quot;six degrees of separation&quot; phenomenon on a global scale. Second, by studying
the average local clustering coefficient and degeneracy of graph neighborhoods,
we show that while the Facebook graph as a whole is clearly sparse, the graph
neighborhoods of users contain surprisingly dense structure. Third, we
characterize the assortativity patterns present in the graph by studying the
basic demographic and network properties of users. We observe clear degree
assortativity and characterize the extent to which &quot;your friends have more
friends than you&quot;. Furthermore, we observe a strong effect of age on friendship
preferences as well as a globally modular community structure driven by
nationality, but we do not find any strong gender homophily. We compare our
results with those from smaller social networks and find mostly, but not
entirely, agreement on common structural network characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4504</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4504</id><created>2011-11-18</created><authors><author><keyname>Szajowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Multi-variate Quickest Detection of Significant Change Process</title><categories>cs.GT math.OC math.PR math.ST stat.TH</categories><comments>11 pages</comments><msc-class>60G40, 90C39, 90D12, 62C25, 62H99, 62L15, 68G99</msc-class><acm-class>B.5.3; G.3; H.3.4; H.5</acm-class><journal-ref>GameSec 2011, LNCS 7037, pp. 56-66, 2011</journal-ref><doi>10.1007/978-3-642-25280-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with a mathematical model of a surveillance system based on a
net of sensors. The signals acquired by each node of the net are Markovian
process, have two different transition probabilities, which depends on the
presence or absence of an intruder nearby. The detection of the transition
probability change at one node should be confirmed by a detection of similar
change at some other sensors. Based on a simple game the model of a fusion
center is then constructed. The aggregate function defined on the net is the
background of the definition of a non-cooperative stopping game which is a
model of the multivariate disorder detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4511</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4511</id><created>2011-11-18</created><authors><author><keyname>Verma</keyname><forenames>Kshitiz</forenames></author><author><keyname>Rizzo</keyname><forenames>Gianluca</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Rum&#xed;n</keyname><forenames>Rub&#xe9;n Cuevas</forenames></author><author><keyname>Azcorra</keyname><forenames>Arturo</forenames></author></authors><title>Greening File Distribution: Centralized or Distributed?</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite file-distribution applications are responsible for a major portion of
the current Internet traffic, so far little effort has been dedicated to study
file distribution from the point of view of energy efficiency. In this paper,
we present a first approach at the problem of energy efficiency for file
distribution. Specifically, we first demonstrate that the general problem of
minimizing energy consumption in file distribution in heterogeneous settings is
NP-hard. For homogeneous settings, we derive tight lower bounds on energy
consumption, and we design a family of algorithms that achieve these bounds.
Our results prove that collaborative p2p schemes achieve up to 50% energy
savings with respect to the best available centralized file distribution
scheme. Through simulation, we demonstrate that in more realistic cases (e.g.,
considering network congestion, and link variability across hosts) we validate
this observation, since our collaborative algorithms always achieve significant
energy savings with respect to the power consumption of centralized file
distribution systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4533</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4533</id><created>2011-11-18</created><updated>2013-02-19</updated><authors><author><keyname>Pamies-Juarez</keyname><forenames>Lluis</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>In-Network Redundancy Generation for Opportunistic Speedup of Backup</title><categories>cs.DC cs.IT cs.NI math.IT</categories><journal-ref>Future Generation Computer Systems (Volume 29, Issue 6, August
  2013, Pages 1353-1362)</journal-ref><doi>10.1016/j.future.2013.02.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erasure coding is a storage-efficient alternative to replication for
achieving reliable data backup in distributed storage systems. During the
storage process, traditional erasure codes require a unique source node to
create and upload all the redundant data to the different storage nodes.
However, such a source node may have limited communication and computation
capabilities, which constrain the storage process throughput. Moreover, the
source node and the different storage nodes might not be able to send and
receive data simultaneously -- e.g., nodes might be busy in a datacenter
setting, or simply be offline in a peer-to-peer setting -- which can further
threaten the efficacy of the overall storage process. In this paper we propose
an &quot;in-network&quot; redundancy generation process which distributes the data
insertion load among the source and storage nodes by allowing the storage nodes
to generate new redundant data by exchanging partial information among
themselves, improving the throughput of the storage process. The process is
carried out asynchronously, utilizing spare bandwidth and computing resources
from the storage nodes. The proposed approach leverages on the local
repairability property of newly proposed erasure codes tailor made for the
needs of distributed storage systems. We analytically show that the performance
of this technique relies on an efficient usage of the spare node resources, and
we derive a set of scheduling algorithms to maximize the same. We
experimentally show, using availability traces from real peer-to-peer
applications as well as Google data center availability and workload traces,
that our algorithms can, depending on the environment characteristics, increase
the throughput of the storage process significantly (up to 90% in data centers,
and 60% in peer-to-peer settings) with respect to the classical naive data
insertion approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4541</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4541</id><created>2011-11-19</created><updated>2012-02-28</updated><authors><author><keyname>Khoa</keyname><forenames>Nguyen Lu Dang</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author></authors><title>Large Scale Spectral Clustering Using Approximate Commute Time Embedding</title><categories>cs.LG</categories><doi>10.1007/978-3-642-33492-4_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is a novel clustering method which can detect complex
shapes of data clusters. However, it requires the eigen decomposition of the
graph Laplacian matrix, which is proportion to $O(n^3)$ and thus is not
suitable for large scale systems. Recently, many methods have been proposed to
accelerate the computational time of spectral clustering. These approximate
methods usually involve sampling techniques by which a lot information of the
original data may be lost. In this work, we propose a fast and accurate
spectral clustering approach using an approximate commute time embedding, which
is similar to the spectral embedding. The method does not require using any
sampling technique and computing any eigenvector at all. Instead it uses random
projection and a linear time solver to find the approximate embedding. The
experiments in several synthetic and real datasets show that the proposed
approach has better clustering quality and is faster than the state-of-the-art
approximate spectral clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4545</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4545</id><created>2011-11-19</created><authors><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Vasudevan</keyname><forenames>Rangarajan A.</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Paprzycki</keyname><forenames>Marcin</forenames></author></authors><title>Grid Security and Integration with Minimal Performance Degradation</title><categories>cs.CR</categories><comments>8 Pages, 1 Figure</comments><journal-ref>Journal of Digital Information Management, Vol. 2, Issue 2,
  September, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational grids are believed to be the ultimate framework to meet the
growing computational needs of the scientific community. Here, the processing
power of geographically distributed resources working under different
ownerships, having their own access policy, cost structure and the likes, is
logically coupled to make them perform as a unified resource. The continuous
increase of availability of high-bandwidth communication as well as powerful
computers built of low-cost components further enhance chances of computational
grids becoming a reality. However, the question of grid security remains one of
the important open research issues. Here, we present some novel ideas about how
to implement grid security, without appreciable performance degradation in
grids. A suitable alternative to the computationally expensive encryption is
suggested, which uses a key for message authentication. Methods of secure
transfer and exchange of the required key(s) are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4552</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4552</id><created>2011-11-19</created><updated>2011-11-30</updated><authors><author><keyname>Noual</keyname><forenames>Mathilde</forenames></author><author><keyname>Regnault</keyname><forenames>Damien</forenames></author><author><keyname>Sen&#xe9;</keyname><forenames>Sylvain</forenames></author></authors><title>Non-monotony and Boolean automata networks</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at setting the keystone of a prospective theoretical study on
the role of non-monotone interactions in biological regulation networks.
Focusing on discrete models of these networks, namely, Boolean automata
networks, we propose to analyse the contribution of non-monotony to the
diversity and complexity in their dynamical behaviours. More precisely, in this
paper, we start by detailing some motivations, both mathematical and
biological, for our interest in non-monotony, and we discuss how it may account
for phenomena that cannot be produced by monotony only. Then, to build some
understanding in this direction, we propose some preliminary results on the
dynamical behaviour of some specific non-monotone Boolean automata networks
called XOR circulant networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4555</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4555</id><created>2011-11-19</created><updated>2012-04-15</updated><authors><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author></authors><title>Large Deviations Performance of Consensus+Innovations Distributed
  Detection with Non-Gaussian Observations</title><categories>cs.IT math.IT</categories><comments>30 pages, journal, submitted Nov 17, 2011; revised Apr 3, 2012</comments><doi>10.1109/TSP.2012.2210885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the large deviations asymptotic performance (error exponent) of
consensus+innovations distributed detection over random networks with generic
(non-Gaussian) sensor observations. At each time instant, sensors 1) combine
theirs with the decision variables of their neighbors (consensus) and 2)
assimilate their new observations (innovations). This paper shows for general
non-Gaussian distributions that consensus+innovations distributed detection
exhibits a phase transition behavior with respect to the network degree of
connectivity. Above a threshold, distributed is as good as centralized, with
the same optimal asymptotic detection performance, but, below the threshold,
distributed detection is suboptimal with respect to centralized detection. We
determine this threshold and quantify the performance loss below threshold.
Finally, we show the dependence of the threshold and performance on the
distribution of the observations: distributed detectors over the same random
network, but with different observations' distributions, for example, Gaussian,
Laplace, or quantized, may have different asymptotic performance, even when the
corresponding centralized detectors have the same asymptotic performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4570</identifier>
 <datestamp>2012-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4570</id><created>2011-11-19</created><updated>2012-01-05</updated><authors><author><keyname>Backstrom</keyname><forenames>Lars</forenames></author><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Rosa</keyname><forenames>Marco</forenames></author><author><keyname>Ugander</keyname><forenames>Johan</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Four Degrees of Separation</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frigyes Karinthy, in his 1929 short story &quot;L\'aancszemek&quot; (&quot;Chains&quot;)
suggested that any two persons are distanced by at most six friendship links.
(The exact wording of the story is slightly ambiguous: &quot;He bet us that, using
no more than five individuals, one of whom is a personal acquaintance, he could
contact the selected individual [...]&quot;. It is not completely clear whether the
selected individual is part of the five, so this could actually allude to
distance five or six in the language of graph theory, but the &quot;six degrees of
separation&quot; phrase stuck after John Guare's 1990 eponymous play. Following
Milgram's definition and Guare's interpretation, we will assume that &quot;degrees
of separation&quot; is the same as &quot;distance minus one&quot;, where &quot;distance&quot; is the
usual path length-the number of arcs in the path.) Stanley Milgram in his
famous experiment challenged people to route postcards to a fixed recipient by
passing them only through direct acquaintances. The average number of
intermediaries on the path of the postcards lay between 4.4 and 5.7, depending
on the sample of people chosen.
  We report the results of the first world-scale social-network graph-distance
computations, using the entire Facebook network of active users (\approx721
million users, \approx69 billion friendship links). The average distance we
observe is 4.74, corresponding to 3.74 intermediaries or &quot;degrees of
separation&quot;, showing that the world is even smaller than we expected, and
prompting the title of this paper. More generally, we study the distance
distribution of Facebook and of some interesting geographic subgraphs, looking
also at their evolution over time.
  The networks we are able to explore are almost two orders of magnitude larger
than those analysed in the previous literature. We report detailed statistical
metadata showing that our measurements (which rely on probabilistic algorithms)
are very accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4572</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4572</id><created>2011-11-19</created><updated>2013-04-18</updated><authors><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author></authors><title>On the mean square error of randomized averaging algorithms</title><categories>math.OC cs.SI cs.SY</categories><comments>11 pages. to appear as a journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper regards randomized discrete-time consensus systems that preserve
the average &quot;on average&quot;. As a main result, we provide an upper bound on the
mean square deviation of the consensus value from the initial average. Then, we
apply our result to systems where few or weakly correlated interactions take
place: these assumptions cover several algorithms proposed in the literature.
For such systems we show that, when the network size grows, the deviation tends
to zero, and the speed of this decay is not slower than the inverse of the
size. Our results are based on a new approach, which is unrelated to the
convergence properties of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4575</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4575</id><created>2011-11-19</created><authors><author><keyname>Anzabi-Nezhad</keyname><forenames>Nima S.</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author><author><keyname>Kakhki</keyname><forenames>Mohammad Molavi</forenames></author></authors><title>Information Theoretic Exemplification of the Impact of
  Transmitter-Receiver Cognition on the Channel Capacity</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study, information theoretically, the impact of transmitter
and or receiver cognition on the channel capacity. The cognition can be
described by state information, dependent on the channel noise and or input.
Specifically, as a new idea, we consider the receiver cognition as a state
information dependent on the noise and we derive a capacity theorem based on
the Gaussian version of the Cover-Chiang capacity theorem for two-sided state
information channel. As intuitively expected, the receiver cognition increases
the channel capacity and our theorem shows this increase quantitatively. Also,
our capacity theorem includes the famous Costa theorem as its special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4580</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4580</id><created>2011-11-19</created><authors><author><keyname>Khan</keyname><forenames>Usman A.</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Networked estimation under information constraints</title><categories>cs.IT cs.MA cs.SI math.IT</categories><comments>Submitted to IEEE TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study estimation of potentially unstable linear dynamical
systems when the observations are distributed over a network. We are interested
in scenarios when the information exchange among the agents is restricted. In
particular, we consider that each agent can exchange information with its
neighbors only once per dynamical system evolution-step. Existing work with
similar information-constraints is restricted to static parameter estimation,
whereas, the work on dynamical systems assumes large number of information
exchange iterations between every two consecutive system evolution steps.
  We show that when the agent communication network is sparely-connected, the
sparsity of the network plays a key role in the stability and performance of
the underlying estimation algorithm. To this end, we introduce the notion of
\emph{Network Tracing Capacity} (NTC), which is defined as the largest two-norm
of the system matrix that can be estimated with bounded error. Extending this
to fully-connected networks or infinite information exchanges (per dynamical
system evolution-step), we note that the NTC is infinite, i.e., any dynamical
system can be estimated with bounded error. In short, the NTC characterizes the
estimation capability of a sparse network by relating it to the evolution of
the underlying dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4582</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4582</id><created>2011-11-19</created><authors><author><keyname>Busic</keyname><forenames>Ana</forenames></author><author><keyname>Fates</keyname><forenames>Nazim</forenames></author><author><keyname>Mairesse</keyname><forenames>Jean</forenames></author><author><keyname>Marcovici</keyname><forenames>Irene</forenames></author></authors><title>Density classification on infinite lattices and trees</title><categories>math.PR cs.DM nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an infinite graph with nodes initially labeled by independent
Bernoulli random variables of parameter p. We address the density
classification problem, that is, we want to design a (probabilistic or
deterministic) cellular automaton or a finite-range interacting particle system
that evolves on this graph and decides whether p is smaller or larger than 1/2.
Precisely, the trajectories should converge to the uniform configuration with
only 0's if p&lt;1/2, and only 1's if p&gt;1/2. We present solutions to that problem
on the d-dimensional lattice, for any d&gt;1, and on the regular infinite trees.
For Z, we propose some candidates that we back up with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4596</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4596</id><created>2011-11-19</created><updated>2012-07-30</updated><authors><author><keyname>Ayach</keyname><forenames>Omar El</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Grassmannian Differential Limited Feedback for Interference Alignment</title><categories>cs.IT math.IT</categories><comments>30 pages, submitted to IEEE Transactions on Signal Processing,
  November 2011</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 60, no. 12, pp.
  6481-6494, December 2012</journal-ref><doi>10.1109/TSP.2012.2218238</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel state information (CSI) in the interference channel can be used to
precode, align, and reduce the dimension of interference at the receivers, to
achieve the channel's maximum multiplexing gain, through what is known as
interference alignment. Most interference alignment algorithms require
knowledge of all the interfering channels to compute the alignment precoders.
CSI, considered available at the receivers, can be shared with the transmitters
via limited feedback. When alignment is done by coding over frequency
extensions in a single antenna system, the required CSI lies on the
Grassmannian manifold and its structure can be exploited in feedback.
Unfortunately, the number of channels to be shared grows with the square of the
number of users, creating too much overhead with conventional feedback methods.
This paper proposes Grassmannian differential feedback to reduce feedback
overhead by exploiting both the channel's temporal correlation and Grassmannian
structure. The performance of the proposed algorithm is characterized both
analytically and numerically as a function of channel length, mobility, and the
number of feedback bits. The main conclusions are that the proposed feedback
strategy allows interference alignment to perform well over a wide range of
Doppler spreads, and to approach perfect CSI performance in slowly varying
channels. Numerical results highlight the trade-off between the frequency of
feedback and the accuracy of individual feedback updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4600</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4600</id><created>2011-11-19</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author><author><keyname>F&#xfc;gger</keyname><forenames>Matthias</forenames></author><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author></authors><title>On the Transience of Linear Max-Plus Dynamical Systems</title><categories>cs.DM</categories><comments>35 pages, 5 figures</comments><msc-class>05C38</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the transients of linear max-plus dynamical systems. For that, we
consider for each irreducible max-plus matrix A, the weighted graph G(A) such
that A is the adjacency matrix of G(A). Based on a novel graph-theoretic
counterpart to the number-theoretic Brauer's theorem, we propose two new
methods for the construction of arbitrarily long paths in G(A) with maximal
weight. That leads to two new upper bounds on the transient of a linear
max-plus system which both improve on the bounds previously given by Even and
Rajsbaum (STOC 1990, Theory of Computing Systems 1997), by Bouillard and Gaujal
(Research Report 2000), and by Soto y Koelemeijer (PhD Thesis 2003), and are,
in general, incomparable with Hartmann and Arguelles' bound (Mathematics of
Operations Research 1999). With our approach, we also show how to improve the
latter bound by a factor of two.
  A significant benefit of our bounds is that each of them turns out to be
linear in the size of the system in various classes of linear max-plus system
whereas the bounds previously given are all at least quadratic. Our second
result concerns the relationship between matrix and system transients: We prove
that the transient of an NxN matrix A is, up to some constant, equal to the
transient of an A-linear system with an initial vector whose norm is quadratic
in N. Finally, we study the applicability of our results to the well-known Full
Reversal algorithm whose behavior can be described as a min-plus linear system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4601</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4601</id><created>2011-11-19</created><updated>2013-12-05</updated><authors><author><keyname>Kaslovsky</keyname><forenames>Daniel N.</forenames></author><author><keyname>Meyer</keyname><forenames>Francois G.</forenames></author></authors><title>Non-Asymptotic Analysis of Tangent Space Perturbation</title><categories>physics.data-an cs.NA stat.ML</categories><comments>53 pages. Revised manuscript with new content addressing application
  of results to real data sets</comments><msc-class>62H25, 15A42, 60B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing an efficient parameterization of a large, noisy data set of
points lying close to a smooth manifold in high dimension remains a fundamental
problem. One approach consists in recovering a local parameterization using the
local tangent plane. Principal component analysis (PCA) is often the tool of
choice, as it returns an optimal basis in the case of noise-free samples from a
linear subspace. To process noisy data samples from a nonlinear manifold, PCA
must be applied locally, at a scale small enough such that the manifold is
approximately linear, but at a scale large enough such that structure may be
discerned from noise. Using eigenspace perturbation theory and non-asymptotic
random matrix theory, we study the stability of the subspace estimated by PCA
as a function of scale, and bound (with high probability) the angle it forms
with the true tangent space. By adaptively selecting the scale that minimizes
this bound, our analysis reveals an appropriate scale for local tangent plane
recovery. We also introduce a geometric uncertainty principle quantifying the
limits of noise-curvature perturbation for stable recovery. With the purpose of
providing perturbation bounds that can be used in practice, we propose plug-in
estimates that make it possible to directly apply the theoretical results to
real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4610</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4610</id><created>2011-11-20</created><authors><author><keyname>Mandel</keyname><forenames>Jan</forenames></author><author><keyname>Beezley</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Kochanski</keyname><forenames>Adam K.</forenames></author><author><keyname>Kondratenko</keyname><forenames>Volodymyr Y.</forenames></author><author><keyname>Zhang</keyname><forenames>Lin</forenames></author><author><keyname>Anderson</keyname><forenames>Erik</forenames></author><author><keyname>Daniels</keyname><forenames>Joel</forenames><suffix>II</suffix></author><author><keyname>Silva</keyname><forenames>Claudio T.</forenames></author><author><keyname>Johnson</keyname><forenames>Christopher R.</forenames></author></authors><title>A wildland fire modeling and visualization environment</title><categories>physics.ao-ph cs.CE</categories><comments>12 pages; Ninth Symposium on Fire and Forest Meteorology, Palm
  Springs, CA, October 2011</comments><report-no>UCD CCM Report 305</report-no><msc-class>86-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an overview of a modeling environment, consisting of a coupled
atmosphere-wildfire model, utilities for visualization, data processing, and
diagnostics, open source software repositories, and a community wiki. The fire
model, called SFIRE, is based on a fire-spread model, implemented by the
level-set method, and it is coupled with the Weather Research Forecasting (WRF)
model. A version with a subset of the features is distributed with WRF 3.3 as
WRF-Fire. In each time step, the fire module takes the wind as input and
returns the latent and sensible heat fluxes. The software architecture uses WRF
parallel infrastructure for massively parallel computing. Recent features of
the code include interpolation from an ideal logarithmic wind profile for
nonhomogeneous fuels and ignition from a fire perimeter with an atmosphere and
fire spin-up. Real runs use online sources for fuel maps, fine-scale
topography, and meteorological data, and can run faster than real time.
Visualization pathways allow generating images and animations in many packages,
including VisTrails, VAPOR, MayaVi, and Paraview, as well as output to Google
Earth. The environment is available from openwfm.org. New diagnostic variables
were added to the code recently, including a new kind of fireline intensity,
which takes into account also the speed of burning, unlike Byram's fireline
intensity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4611</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4611</id><created>2011-11-20</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author><author><keyname>Gabbay</keyname><forenames>Murdoch</forenames></author></authors><title>From nominal sets binding to functions and lambda-abstraction:
  connecting the logic of permutation models with the logic of functions</title><categories>cs.LO</categories><msc-class>03B70 (Primary) 68Q55 (Secondary)</msc-class><acm-class>F.3.0; F.3.2</acm-class><doi>10.1016/j.tcs.2012.06.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permissive-Nominal Logic (PNL) extends first-order predicate logic with
term-formers that can bind names in their arguments. It takes a semantics in
(permissive-)nominal sets. In PNL, the forall-quantifier or lambda-binder are
just term-formers satisfying axioms, and their denotation is functions on
nominal atoms-abstraction.
  Then we have higher-order logic (HOL) and its models in ordinary (i.e.
Zermelo-Fraenkel) sets; the denotation of forall or lambda is functions on full
or partial function spaces.
  This raises the following question: how are these two models of binding
connected? What translation is possible between PNL and HOL, and between
nominal sets and functions?
  We exhibit a translation of PNL into HOL, and from models of PNL to certain
models of HOL. It is natural, but also partial: we translate a restricted
subsystem of full PNL to HOL. The extra part which does not translate is the
symmetry properties of nominal sets with respect to permutations. To use a
little nominal jargon: we can translate names and binding, but not their
nominal equivariance properties. This seems reasonable since HOL---and ordinary
sets---are not equivariant.
  Thus viewed through this translation, PNL and HOL and their models do
different things, but they enjoy non-trivial and rich subsystems which are
isomorphic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4619</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4619</id><created>2011-11-20</created><authors><author><keyname>Ram</keyname><forenames>Idan</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Cohen</keyname><forenames>Israel</forenames></author></authors><title>Redundant Wavelets on Graphs and High Dimensional Data Clouds</title><categories>cs.CV</categories><comments>4 pages, 4 figures, 1 table, submitted to IEEE Signal Processing
  Letters</comments><doi>10.1109/LSP.2012.2190983</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new redundant wavelet transform applicable to
scalar functions defined on high dimensional coordinates, weighted graphs and
networks. The proposed transform utilizes the distances between the given data
points. We modify the filter-bank decomposition scheme of the redundant wavelet
transform by adding in each decomposition level linear operators that reorder
the approximation coefficients. These reordering operators are derived by
organizing the tree-node features so as to shorten the path that passes through
these points. We explore the use of the proposed transform to image denoising,
and show that it achieves denoising results that are close to those obtained
with the BM3D algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4624</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4624</id><created>2011-11-20</created><updated>2012-01-02</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author></authors><title>Sensing Matrix Setting Schemes for Cognitive Networks and Their
  Performance Analysis</title><categories>cs.PF cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powerful spectrum decision schemes enable cognitive radios (CRs) to find
transmission opportunities in spectral resources allocated exclusively to the
primary users. One of the key effecting factor on the CR network throughput is
the spectrum sensing sequence used by each secondary user. In this paper,
secondary users' throughput maximization through finding an appropriate sensing
matrix (SM) is investigated. To this end, first the average throughput of the
CR network is evaluated for a given SM. Then, an optimization problem based on
the maximization of the network throughput is formulated in order to find the
optimal SM. As the optimum solution is very complicated, to avoid its major
challenges, three novel sub optimum solutions for finding an appropriate SM are
proposed for various cases including perfect and non-perfect sensing. Despite
of having less computational complexities as well as lower consumed energies,
the proposed solutions perform quite well compared to the optimum solution (the
optimum SM). The structure and performance of the proposed SM setting schemes
are discussed in detail and a set of illustrative simulation results is
presented to validate their efficiencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4626</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4626</id><created>2011-11-20</created><updated>2013-02-28</updated><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author><author><keyname>Mueller</keyname><forenames>Ralf R.</forenames></author><author><keyname>Vehkaperae</keyname><forenames>Mikko</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author></authors><title>On an Achievable Rate of Large Rayleigh Block-Fading MIMO Channels with
  No CSI</title><categories>cs.IT math.IT</categories><comments>re-submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training-based transmission over Rayleigh block-fading multiple-input
multiple-output (MIMO) channels is investigated. As a training method a
combination of a pilot-assisted scheme and a biased signaling scheme is
considered. The achievable rates of successive decoding (SD) receivers based on
the linear minimum mean-squared error (LMMSE) channel estimation are analyzed
in the large-system limit, by using the replica method under the assumption of
replica symmetry. It is shown that negligible pilot information is best in
terms of the achievable rates of the SD receivers in the large-system limit.
The obtained analytical formulas of the achievable rates can improve the
existing lower bound on the capacity of the MIMO channel with no channel state
information (CSI), derived by Hassibi and Hochwald, for all signal-to-noise
ratios (SNRs). The comparison between the obtained bound and a high SNR
approximation of the channel capacity, derived by Zheng and Tse, implies that
the high SNR approximation is unreliable unless quite high SNR is considered.
Energy efficiency in the low SNR regime is also investigated in terms of the
power per information bit required for reliable communication. The required
minimum power is shown to be achieved at a positive rate for the SD receiver
with no CSI, whereas it is achieved in the zero-rate limit for the case of
perfect CSI available at the receiver. Moreover, numerical simulations imply
that the presented large-system analysis can provide a good approximation for
not so large systems. The results in this paper imply that SD schemes can
provide a significant performance gain in the low-to-moderate SNR regimes,
compared to conventional receivers based on one-shot channel estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4635</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4635</id><created>2011-11-20</created><authors><author><keyname>Shi</keyname><forenames>Tao</forenames></author><author><keyname>Anashin</keyname><forenames>Vladimir</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>Linear Relation on General Ergodic T-Function</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find linear (as well as quadratic) relations in a very large class of
T-functions. The relations may be used in analysis of T-function-based stream
ciphers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4639</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4639</id><created>2011-11-20</created><authors><author><keyname>Lahti</keyname><forenames>Leo</forenames></author><author><keyname>Sch&#xe4;fer</keyname><forenames>Martin</forenames></author><author><keyname>Klein</keyname><forenames>Hans-Ulrich</forenames></author><author><keyname>Bicciato</keyname><forenames>Silvio</forenames></author><author><keyname>Dugas</keyname><forenames>Martin</forenames></author></authors><title>Cancer gene prioritization by integrative analysis of mRNA expression
  and DNA copy number data: a comparative review</title><categories>cs.CE q-bio.GN stat.AP stat.ME</categories><comments>PDF file including supplementary material. 9 pages. Preprint</comments><doi>10.1093/bib/bbs005</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A variety of genome-wide profiling techniques are available to probe
complementary aspects of genome structure and function. Integrative analysis of
heterogeneous data sources can reveal higher-level interactions that cannot be
detected based on individual observations. A standard integration task in
cancer studies is to identify altered genomic regions that induce changes in
the expression of the associated genes based on joint analysis of genome-wide
gene expression and copy number profiling measurements. In this review, we
provide a comparison among various modeling procedures for integrating
genome-wide profiling data of gene copy number and transcriptional alterations
and highlight common approaches to genomic data integration. A transparent
benchmarking procedure is introduced to quantitatively compare the cancer gene
prioritization performance of the alternative methods. The benchmarking
algorithms and data sets are available at http://intcomp.r-forge.r-project.org
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4645</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4645</id><created>2011-11-20</created><authors><author><keyname>Altshuler</keyname><forenames>Yaniv</forenames></author><author><keyname>Aharony</keyname><forenames>Nadav</forenames></author><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Incremental Learning with Accuracy Prediction of Social and Individual
  Properties from Mobile-Phone Data</title><categories>cs.SI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phones are quickly becoming the primary source for social, behavioral,
and environmental sensing and data collection. Today's smartphones are equipped
with increasingly more sensors and accessible data types that enable the
collection of literally dozens of signals related to the phone, its user, and
its environment. A great deal of research effort in academia and industry is
put into mining this raw data for higher level sense-making, such as
understanding user context, inferring social networks, learning individual
features, predicting outcomes, and so on. In this work we investigate the
properties of learning and inference of real world data collected via mobile
phones over time. In particular, we look at the dynamic learning process over
time, and how the ability to predict individual parameters and social links is
incrementally enhanced with the accumulation of additional data. To do this, we
use the Friends and Family dataset, which contains rich data signals gathered
from the smartphones of 140 adult members of a young-family residential
community for over a year, and is one of the most comprehensive mobile phone
datasets gathered in academia to date. We develop several models that predict
social and individual properties from sensed mobile phone data, including
detection of life-partners, ethnicity, and whether a person is a student or
not. Then, for this set of diverse learning tasks, we investigate how the
prediction accuracy evolves over time, as new data is collected. Finally, based
on gained insights, we propose a method for advance prediction of the maximal
learning accuracy possible for the learning task at hand, based on an initial
set of measurements. This has practical implications, like informing the design
of mobile data collection campaigns, or evaluating analysis strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4646</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4646</id><created>2011-11-20</created><updated>2012-08-13</updated><authors><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Davenport</keyname><forenames>Mark</forenames></author></authors><title>On the Fundamental Limits of Adaptive Sensing</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we can sequentially acquire arbitrary linear measurements of an
n-dimensional vector x resulting in the linear model y = Ax + z, where z
represents measurement noise. If the signal is known to be sparse, one would
expect the following folk theorem to be true: choosing an adaptive strategy
which cleverly selects the next row of A based on what has been previously
observed should do far better than a nonadaptive strategy which sets the rows
of A ahead of time, thus not trying to learn anything about the signal in
between observations. This paper shows that the folk theorem is false. We prove
that the advantages offered by clever adaptive strategies and sophisticated
estimation procedures---no matter how intractable---over classical compressed
acquisition/recovery schemes are, in general, minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4649</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4649</id><created>2011-11-20</created><updated>2013-08-23</updated><authors><author><keyname>Chandrasekaran</keyname><forenames>Karthekeyan</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Integer Feasibility of Random Polytopes</title><categories>cs.DS math.PR</categories><acm-class>G.2.0; F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study integer programming instances over polytopes P(A,b)={x:Ax&lt;=b} where
the constraint matrix A is random, i.e., its entries are i.i.d. Gaussian or,
more generally, its rows are i.i.d. from a spherically symmetric distribution.
The radius of the largest inscribed ball is closely related to the existence of
integer points in the polytope. We show that for m=2^O(sqrt{n}), there exist
constants c_0 &lt; c_1 such that with high probability, random polytopes are
integer feasible if the radius of the largest ball contained in the polytope is
at least c_1sqrt{log(m/n)}; and integer infeasible if the largest ball
contained in the polytope is centered at (1/2,...,1/2) and has radius at most
c_0sqrt{log(m/n)}. Thus, random polytopes transition from having no integer
points to being integer feasible within a constant factor increase in the
radius of the largest inscribed ball. We show integer feasibility via a
randomized polynomial-time algorithm for finding an integer point in the
polytope.
  Our main tool is a simple new connection between integer feasibility and
linear discrepancy. We extend a recent algorithm for finding low-discrepancy
solutions (Lovett-Meka, FOCS '12) to give a constructive upper bound on the
linear discrepancy of random matrices. By our connection between discrepancy
and integer feasibility, this upper bound on linear discrepancy translates to
the radius lower bound that guarantees integer feasibility of random polytopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4650</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4650</id><created>2011-11-20</created><authors><author><keyname>Altshuler</keyname><forenames>Yaniv</forenames></author><author><keyname>Pan</keyname><forenames>Wei</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Trends Prediction Using Social Diffusion Models</title><categories>cs.SI physics.soc-ph</categories><comments>6 Pages + Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of the ability of predict trends in social media has been
growing rapidly in the past few years with the growing dominance of social
media in our everyday's life. Whereas many works focus on the detection of
anomalies in networks, there exist little theoretical work on the prediction of
the likelihood of anomalous network pattern to globally spread and become
&quot;trends&quot;. In this work we present an analytic model the social diffusion
dynamics of spreading network patterns. Our proposed method is based on
information diffusion models, and is capable of predicting future trends based
on the analysis of past social interactions between the community's members. We
present an analytic lower bound for the probability that emerging trends would
successful spread through the network. We demonstrate our model using two
comprehensive social datasets - the &quot;Friends and Family&quot; experiment that was
held in MIT for over a year, where the complete activity of 140 users was
analyzed, and a financial dataset containing the complete activities of over
1.5 million members of the &quot;eToro&quot; social trading community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4654</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4654</id><created>2011-11-20</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>A self-portrait of young Leonardo</title><categories>cs.CV</categories><comments>Image processing, digital restoration, Leonardo da Vinci</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most famous drawings by Leonardo da Vinci is a self-portrait in
red chalk, where he looks quite old. In fact, there is a sketch in one of his
notebooks, partially covered by written notes, that can be a self-portrait of
the artist when he was young. The use of image processing, to remove the
handwritten text and improve the image, allows a comparison of the two
portraits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4662</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4662</id><created>2011-11-20</created><updated>2015-05-11</updated><authors><author><keyname>Male</keyname><forenames>Camille</forenames></author></authors><title>The distributions of traffics and their free product</title><categories>math.PR cs.DM math.CO math.OA</categories><comments>Correction of a mistake in Proposition 4.4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffics are defined as elements of Voiculescu's non commutative spaces
(called non commutative random variables), for which we specify more structure.
We define a new notion of free product in that context. It is weaker than
Voiculescu's free product and encodes the independence of complex random
variables. This free product models the limits of independent random matrices
invariant by conjugation by permutation matrices. We generalize known theorems
of asymptotic freeness (for Wigner, unitary Haar, uniform permutation and
deterministic matrices) and present examples of random matrices that converges
in non commutative law and are not asymptotically free in the sense of
Voiculescu.
  Our approach provides some additional applications. Firstly, the convergence
in distribution of traffics is related to two notions of convergence of graphs,
namely the weak local convergence of Benjamini and Schramm and the convergence
of graphons of Lovasz. These connections give descriptions of the limiting
eigenvalue distributions of large graphs with uniformly bounded degree and
random matrices with variance profile.
  Moreover, we prove a new central limit theorems for the normalized sum of non
commutative random variables. It interpolates Voiculescu's and de
Moivre-Laplace central limit theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4676</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4676</id><created>2011-11-20</created><authors><author><keyname>Pickin</keyname><forenames>Andrew</forenames></author></authors><title>Facial Asymmetry and Emotional Expression</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report is about facial asymmetry, its connection to emotional
expression, and methods of measuring facial asymmetry in videos of faces. The
research was motivated by two factors: firstly, there was a real opportunity to
develop a novel measure of asymmetry that required minimal human involvement
and that improved on earlier measures in the literature; and secondly, the
study of the relationship between facial asymmetry and emotional expression is
both interesting in its own right, and important because it can inform
neuropsychological theory and answer open questions concerning emotional
processing in the brain. The two aims of the research were: first, to develop
an automatic frame-by-frame measure of facial asymmetry in videos of faces that
improved on previous measures; and second, to use the measure to analyse the
relationship between facial asymmetry and emotional expression, and connect our
findings with previous research of the relationship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4692</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4692</id><created>2011-11-20</created><authors><author><keyname>Croll</keyname><forenames>Grenville J.</forenames></author></authors><title>An Insight into Spreadsheet User Behaviour through an Analysis of
  EuSpRIG Website Statistics</title><categories>cs.CY</categories><comments>10 Pages, 5 Tables, 2 Colour Figures, ISBN 978-0-9566256-9-4</comments><journal-ref>Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2011 129-139</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The European Spreadsheet Risks Interest Group (EuSpRIG) has maintained a
website almost since its inception in 2000. We present here longitudinal and
cross-sectional statistics from the website log in order to shed some light
upon end-user activity in the EuSpRIG domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4724</identifier>
 <datestamp>2012-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4724</id><created>2011-11-20</created><updated>2012-02-01</updated><authors><author><keyname>Lee</keyname><forenames>Kyunghan</forenames></author><author><keyname>Kim</keyname><forenames>Yoora</forenames></author><author><keyname>Chong</keyname><forenames>Song</forenames></author><author><keyname>Rhee</keyname><forenames>Injong</forenames></author><author><keyname>Yi</keyname><forenames>Yung</forenames></author><author><keyname>Shroff</keyname><forenames>Ness. B.</forenames></author></authors><title>On the Critical Delays of Mobile Networks under L\'{e}vy Walks and
  L\'{e}vy Flights</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-capacity tradeoffs for mobile networks have been analyzed through a
number of research work. However, L\'{e}vy mobility known to closely capture
human movement patterns has not been adopted in such work. Understanding the
delay-capacity tradeoff for a network with L\'{e}vy mobility can provide
important insights into understanding the performance of real mobile networks
governed by human mobility. This paper analytically derives an important point
in the delay-capacity tradeoff for L\'{e}vy mobility, known as the critical
delay. The critical delay is the minimum delay required to achieve greater
throughput than what conventional static networks can possibly achieve (i.e.,
$O(1/\sqrt{n})$ per node in a network with $n$ nodes). The L\'{e}vy mobility
includes L\'{e}vy flight and L\'{e}vy walk whose step size distributions
parametrized by $\alpha \in (0,2]$ are both heavy-tailed while their times
taken for the same step size are different. Our proposed technique involves (i)
analyzing the joint spatio-temporal probability density function of a
time-varying location of a node for L\'{e}vy flight and (ii) characterizing an
embedded Markov process in L\'{e}vy walk which is a semi-Markov process. The
results indicate that in L\'{e}vy walk, there is a phase transition such that
for $\alpha \in (0,1)$, the critical delay is always $\Theta (n^{1/2})$ and for
$\alpha \in [1,2]$ it is $\Theta(n^{\frac{\alpha}{2}})$. In contrast, L\'{e}vy
flight has the critical delay $\Theta(n^{\frac{\alpha}{2}})$ for
$\alpha\in(0,2]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4729</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4729</id><created>2011-11-20</created><updated>2012-12-01</updated><authors><author><keyname>Li</keyname><forenames>Yanhua</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Yajun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhi-Li</forenames></author></authors><title>Influence Diffusion Dynamics and Influence Maximization in Social
  Networks with Friend and Foe Relationships</title><categories>cs.SI cs.DM physics.soc-ph</categories><comments>27 pages, 23 figures, A short version appears in proceedings of the
  6th ACM International Conference on Web Search and Data Mining (WSDM 2013)</comments><acm-class>E.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diffusion and influence maximization in large-scale online social
networks (OSNs) have been extensively studied, because of their impacts on
enabling effective online viral marketing. Existing studies focus on social
networks with only friendship relations, whereas the foe or enemy relations
that commonly exist in many OSNs, e.g., Epinions and Slashdot, are completely
ignored. In this paper, we make the first attempt to investigate the influence
diffusion and influence maximization in OSNs with both friend and foe
relations, which are modeled using positive and negative edges on signed
networks. In particular, we extend the classic voter model to signed networks
and analyze the dynamics of influence diffusion of two opposite opinions. We
first provide systematic characterization of both short-term and long-term
dynamics of influence diffusion in this model, and illustrate that the steady
state behaviors of the dynamics depend on three types of graph structures,
which we refer to as balanced graphs, anti-balanced graphs, and strictly
unbalanced graphs. We then apply our results to solve the influence
maximization problem and develop efficient algorithms to select initial seeds
of one opinion that maximize either its short-term influence coverage or
long-term steady state influence coverage. Extensive simulation results on both
synthetic and real-world networks, such as Epinions and Slashdot, confirm our
theoretical analysis on influence diffusion dynamics, and demonstrate the
efficacy of our influence maximization algorithm over other heuristic
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4736</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4736</id><created>2011-11-21</created><authors><author><keyname>Herrmannsdoerfer</keyname><forenames>Markus</forenames><affiliation>Institut f&#xfc;r Informatik, Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>GMF: A Model Migration Case for the Transformation Tool Contest</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 1-5</journal-ref><doi>10.4204/EPTCS.74.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a real-life evolution taken from the Graphical Modeling Framework, we
invite submissions to explore ways in which model transformation and migration
tools can be used to migrate models in response to metamodel adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4737</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4737</id><created>2011-11-21</created><authors><author><keyname>Buchwald</keyname><forenames>Sebastian</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Jakumeit</keyname><forenames>Edgar</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Compiler Optimization: A Case for the Transformation Tool Contest</title><categories>cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 6-16</journal-ref><doi>10.4204/EPTCS.74.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimizing compiler consists of a front end parsing a textual programming
language into an intermediate representation (IR), a middle end performing
optimizations on the IR, and a back end lowering the IR to a target
representation (TR) built of operations supported by the target hardware. In
modern compiler construction graph-based IRs are employed. Optimization and
lowering tasks can then be implemented with graph transformation rules. This
case provides two compiler tasks to evaluate the participating tools regarding
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4738</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4738</id><created>2011-11-21</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Program Understanding: A Reengineering Case for the Transformation Tool
  Contest</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 17-21</journal-ref><doi>10.4204/EPTCS.74.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Software Reengineering, one of the central artifacts is the source code of
the legacy system in question. In fact, in most cases it is the only definitive
artifact, because over the time the code has diverged from the original
architecture and design documents. The first task of any reengineering project
is to gather an understanding of the system's architecture. Therefore, a common
approach is to use parsers to translate the source code into a model conforming
to the abstract syntax of the programming language the system is implemented in
which can then be subject to querying. Despite querying, transformations can be
used to generate more abstract views on the system's architecture. This
transformation case deals with the creation of a state machine model out of a
Java syntax graph. It is derived from a task that originates from a real
reengineering project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4739</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4739</id><created>2011-11-21</created><authors><author><keyname>Mazanek</keyname><forenames>Steffen</forenames></author></authors><title>HelloWorld! An Instructive Case for the Transformation Tool Contest</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 22-26</journal-ref><doi>10.4204/EPTCS.74.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This case comprises several primitive tasks that can be solved straight away
with most transformation tools. The aim is to cover the most important kinds of
primitive operations on models, i.e. create, read, update and delete (CRUD). To
this end, tasks such as a constant transformation, a model-to-text
transformation, a very basic migration transformation or diverse simple queries
or in-place operations on graphs have to be solved.
  The motivation for this case is that the results expectedly will be very
instructive for beginners. Also, it is really hard to compare transformation
languages along complex cases, because the complexity of the respective case
might hide the basic language concepts and constructs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4740</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4740</id><created>2011-11-21</created><authors><author><keyname>Herrmannsdoerfer</keyname><forenames>Markus</forenames><affiliation>Institut f&#xfc;r Informatik, Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>Solving the TTC 2011 Model Migration Case with Edapt</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 27-35</journal-ref><doi>10.4204/EPTCS.74.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an overview of the Edapt solution to the GMF model migration
case of the Transformation Tool Contest 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4741</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4741</id><created>2011-11-21</created><authors><author><keyname>Lano</keyname><forenames>K.</forenames><affiliation>Department of Informatics, King's College London</affiliation></author><author><keyname>Kolahdouz-Rahimi</keyname><forenames>S.</forenames><affiliation>Department of Informatics, King's College London</affiliation></author></authors><title>Solving the TTC 2011 Model Migration Case with UML-RSDS</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 36-41</journal-ref><doi>10.4204/EPTCS.74.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply the UML-RSDS notation and tools to the GMF model
migration case study and explain how to use the UML-RSDS tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4742</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4742</id><created>2011-11-21</created><authors><author><keyname>Buchwald</keyname><forenames>Sebastian</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Jakumeit</keyname><forenames>Edgar</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Solving the TTC 2011 Compiler Optimization Case with GrGen.NET</title><categories>cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 42-53</journal-ref><doi>10.4204/EPTCS.74.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of the Compiler Optimization Case is to perform local
optimizations and instruction selection on the graph-based intermediate
representation of a compiler. The case is designed to compare participating
tools regarding their performance. We tackle this task employing the general
purpose graph rewrite system GrGen.NET (www.grgen.net).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4743</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4743</id><created>2011-11-21</created><authors><author><keyname>Li</keyname><forenames>Dan</forenames><affiliation>University of Macau</affiliation></author><author><keyname>Li</keyname><forenames>Xiaoshan</forenames><affiliation>University of Macau</affiliation></author><author><keyname>Stolz</keyname><forenames>Volker</forenames><affiliation>University of Oslo and UNU-IIST</affiliation></author></authors><title>Solving the TTC 2011 Compiler Optimization Case with QVTR-XSLT</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 54-69</journal-ref><doi>10.4204/EPTCS.74.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper we present our solution for the Compiler Optimization
case study of the Transformation Tool Contest (TTC) 2011 using the QVTR-XSLT
tool. The tool supports editing and execution of the graphical notation of QVT
Relations language
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4744</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4744</id><created>2011-11-21</created><authors><author><keyname>Lepper</keyname><forenames>Markus</forenames></author><author><keyname>Widemann</keyname><forenames>Baltasar Tranc&#xf3;n y</forenames></author></authors><title>Solving the TTC 2011 Compiler Optimization Task with metatools</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><acm-class>D.1.1;D.1.2;D.2.7;E.1;F.4.2;I.2.2</acm-class><journal-ref>EPTCS 74, 2011, pp. 70-115</journal-ref><doi>10.4204/EPTCS.74.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The authors' &quot;metatools&quot; are a collection of tools for generic programming.
This includes generating Java sources from mathematically well-founded
specifications, as well as the creation of strictly typed document object
models for XML encoded texts. In this context, almost every computer-internal
structure is treated as a &quot;model&quot;, and every computation is a kind of model
transformation.
  This concept differs significantly from &quot;classical model transformation&quot;
executed by specialized tools and languages. Therefore it seemed promising to
the organizers of the TTC 2011, as well as to the authors, to apply metatools
to one of the challenges, namely to the &quot;compiler optimization task&quot;. This is a
report on the resulting experiences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4745</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4745</id><created>2011-11-21</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Solving the TTC 2011 Compiler Optimization Case with GReTL</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 116-125</journal-ref><doi>10.4204/EPTCS.74.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the GReTL solution of the TTC 2011 Compiler Optimization
case. The submitted solution covers both the constant folding task and the
instruction selection task. The verifier for checking the validity of the graph
is also implemented, and some additional test graphs are provided as requested
by the extension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4746</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4746</id><created>2011-11-21</created><authors><author><keyname>Rensink</keyname><forenames>Arend</forenames><affiliation>University of Twente, NL</affiliation></author><author><keyname>Zambon</keyname><forenames>Eduardo</forenames><affiliation>University of Twente, NL</affiliation></author></authors><title>Solving the TTC 2011 Compiler Optimization Case with GROOVE</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 126-130</journal-ref><doi>10.4204/EPTCS.74.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a partial solution to the Compiler Optimization case
study using GROOVE. We explain how the input graphs provided with the case
study were adapted into a GROOVE representation and we describe an initial
solution for Task 1. This solution allows us to automatically reproduce the
steps of the constant folding example given in the case description. We did not
solve Task 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4747</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4747</id><created>2011-11-21</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Solving the TTC 2011 Reengineering Case with GReTL</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 131-135</journal-ref><doi>10.4204/EPTCS.74.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the GReTL reference solution of the TTC 2011
Reengineering case. Given a Java syntax graph, a simple state machine model has
to be extracted. The submitted solution covers both the core task and the two
extension tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4748</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4748</id><created>2011-11-21</created><authors><author><keyname>Hegedus</keyname><forenames>&#xc1;bel</forenames><affiliation>BME</affiliation></author><author><keyname>Ujhelyi</keyname><forenames>Zolt&#xe1;n</forenames><affiliation>BME</affiliation></author><author><keyname>Bergmann</keyname><forenames>G&#xe1;bor</forenames><affiliation>BME</affiliation></author></authors><title>Solving the TTC 2011 Reengineering Case with VIATRA2</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 136-148</journal-ref><doi>10.4204/EPTCS.74.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current paper presents a solution of the Program Understanding: A
Reengineering Case for the Transformation Tool Contest using the VIATRA2 model
transformation tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4749</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4749</id><created>2011-11-21</created><authors><author><keyname>Herrmannsdoerfer</keyname><forenames>Markus</forenames><affiliation>Institut f&#xfc;r Informatik, Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>Solving the TTC 2011 Reengineering Case with Edapt</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 149-158</journal-ref><doi>10.4204/EPTCS.74.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an overview of the Edapt solution to the reengineering case
of the Transformation Tool Contest 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4750</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4750</id><created>2011-11-21</created><authors><author><keyname>Sostaks</keyname><forenames>Agris</forenames></author><author><keyname>Kalnina</keyname><forenames>Elina</forenames></author><author><keyname>Kalnins</keyname><forenames>Audris</forenames></author><author><keyname>Celms</keyname><forenames>Edgars</forenames></author><author><keyname>Iraids</keyname><forenames>Janis</forenames></author></authors><title>Solving the TTC 2011 Reengineering Case with MOLA and Higher-Order
  Transformations</title><categories>cs.PL cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 159-167</journal-ref><doi>10.4204/EPTCS.74.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reengineering Case of the Transformation Tool Contest 2011 deals with
automatic extraction of state machine from Java source code. The transformation
task involves complex, non-local matching of model elements. This paper
contains the solution of the task using model transformation language MOLA. The
MOLA solution uses higher-order transformations (HOT-s) to generate a part of
the required MOLA program. The described HOT approach allows creating reusable,
complex model transformation libraries for generic tasks without modifying an
implementation of a model transformation language. Thus model transformation
users who are not the developers of the language can achieve the desired
functionality more easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4751</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4751</id><created>2011-11-21</created><authors><author><keyname>Jakumeit</keyname><forenames>Edgar</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Buchwald</keyname><forenames>Sebastian</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Solving the TTC 2011 Reengineering Case with GrGen.NET</title><categories>cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 168-180</journal-ref><doi>10.4204/EPTCS.74.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of the Reengineering Case is to extract a state machine model
out of the abstract syntax graph of a Java program. The extracted state machine
offers a reduced view on the full program graph and thus helps to understand
the program regarding the question of interest. We tackle this task employing
the general purpose graph rewrite system GrGen.NET (www.grgen.net).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4752</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4752</id><created>2011-11-21</created><authors><author><keyname>Jurack</keyname><forenames>Stefan</forenames><affiliation>Universit&#xe4;t Marburg, Germany</affiliation></author><author><keyname>Tietje</keyname><forenames>Johannes</forenames><affiliation>Technische Hochschule Mittelhessen, Giessen, Germany</affiliation></author></authors><title>Solving the TTC 2011 Reengineering Case with Henshin</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 181-203</journal-ref><doi>10.4204/EPTCS.74.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Henshin solution to the Model Transformations for
Program Understanding case study as part of the Transformation Tool Contest
2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4753</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4753</id><created>2011-11-21</created><authors><author><keyname>Herrmannsdoerfer</keyname><forenames>Markus</forenames><affiliation>Institut f&#xfc;r Informatik, Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>Saying Hello World with Edapt - A Solution to the TTC 2011 Instructive
  Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 204-214</journal-ref><doi>10.4204/EPTCS.74.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an overview of the Edapt solution to the hello world case of
the Transformation Tool Contest 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4754</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4754</id><created>2011-11-21</created><authors><author><keyname>Ghamarian</keyname><forenames>Amir Hossein</forenames></author><author><keyname>de Mol</keyname><forenames>Maarten</forenames></author><author><keyname>Rensink</keyname><forenames>Arend</forenames></author><author><keyname>Zambon</keyname><forenames>Eduardo</forenames></author></authors><title>Saying Hello World with GROOVE - A Solution to the TTC 2011 Instructive
  Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 215-222</journal-ref><doi>10.4204/EPTCS.74.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a solution to the Hello World case study of TTC 2011
using GROOVE. We provide and explain the grammar that we used to solve the case
study. Every requested question of the case study was solved by a single rule
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4755</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4755</id><created>2011-11-21</created><authors><author><keyname>Kalnina</keyname><forenames>Elina</forenames><affiliation>Institute of Mathematics and Computer Science, University of Latvia</affiliation></author><author><keyname>Kalnins</keyname><forenames>Audris</forenames><affiliation>Institute of Mathematics and Computer Science, University of Latvia</affiliation></author><author><keyname>Sostaks</keyname><forenames>Agris</forenames><affiliation>Institute of Mathematics and Computer Science, University of Latvia</affiliation></author><author><keyname>Iraids</keyname><forenames>Janis</forenames><affiliation>Institute of Mathematics and Computer Science, University of Latvia</affiliation></author><author><keyname>Celms</keyname><forenames>Edgars</forenames><affiliation>Institute of Mathematics and Computer Science, University of Latvia</affiliation></author></authors><title>Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive
  Case</title><categories>cs.PL cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 237-252</journal-ref><doi>10.4204/EPTCS.74.21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the solution of Hello World transformations in MOLA
transformation language. Transformations implementing the task are relatively
straightforward and easily inferable from the task specification. The required
additional steps related to model import and export are also described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4756</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4756</id><created>2011-11-21</created><authors><author><keyname>Jurack</keyname><forenames>Stefan</forenames><affiliation>Universit&#xe4;t Marburg, Germany</affiliation></author><author><keyname>Tietje</keyname><forenames>Johannes</forenames><affiliation>Technische Hochschule Mittelhessen, Giessen, Germany</affiliation></author></authors><title>Saying Hello World with Henshin - A Solution to the TTC 2011 Instructive
  Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 253-280</journal-ref><doi>10.4204/EPTCS.74.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an overview of the Henshin solution to the Hello World case
study of the Transformation Tool Contest 2011, intended to show basic language
concepts and constructs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4757</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4757</id><created>2011-11-21</created><authors><author><keyname>Buchwald</keyname><forenames>Sebastian</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Jakumeit</keyname><forenames>Edgar</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Saying Hello World with GrGen.NET - A Solution to the TTC 2011
  Instructive Case</title><categories>cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 281-294</journal-ref><doi>10.4204/EPTCS.74.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the graph transformation tool GrGen.NET (www.grgen.net) by
solving the Hello World Case of the Transformation Tool Contest 2011 which
consists of a collection of small transformation tasks; for each task a section
is given explaining our implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4758</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4758</id><created>2011-11-21</created><authors><author><keyname>Hegedus</keyname><forenames>&#xc1;bel</forenames><affiliation>BME</affiliation></author><author><keyname>Ujhelyi</keyname><forenames>Zolt&#xe1;n</forenames><affiliation>BME</affiliation></author><author><keyname>Bergmann</keyname><forenames>G&#xe1;bor</forenames><affiliation>BME</affiliation></author></authors><title>Saying Hello World with VIATRA2 - A Solution to the TTC 2011 Instructive
  Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 302-324</journal-ref><doi>10.4204/EPTCS.74.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a solution of the Hello World! An Instructive Case for the
Transformation Tool Contest using the VIATRA2 model transformation tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4761</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4761</id><created>2011-11-21</created><authors><author><keyname>Li</keyname><forenames>Dan</forenames><affiliation>University of Macau</affiliation></author><author><keyname>Li</keyname><forenames>Xiaoshan</forenames><affiliation>University of Macau</affiliation></author><author><keyname>Stolz</keyname><forenames>Volker</forenames><affiliation>University of Oslo &amp; UNU-IIST</affiliation></author></authors><title>Saying HelloWorld with QVTR-XSLT - A Solution to the TTC 2011
  Instructive Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 223-236</journal-ref><doi>10.4204/EPTCS.74.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper we present our solution for the Hello World case study of
the Transformation Tool Contest (TTC) 2011 using the QVTR-XSLT tool. The tool
supports editing and execution of the graphical notation of QVT Relations
language. The case study consists of a set of simple transformation tasks which
covers the basic functions required for a transformation language, such as
creating, reading/querying, updating and deleting of model elements. We design
a transformation for each of the tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4762</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4762</id><created>2011-11-21</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Saying Hello World with GReTL - A Solution to the TTC 2011 Instructive
  Case</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 295-301</journal-ref><doi>10.4204/EPTCS.74.24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the GReTL solution of the TTC 2011 Hello World case. The
submitted solution covers all tasks including the optional ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4763</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4763</id><created>2011-11-21</created><authors><author><keyname>Lano</keyname><forenames>K.</forenames><affiliation>Department of Informatics, King's College London</affiliation></author><author><keyname>Kolahdouz-Rahimi</keyname><forenames>S.</forenames><affiliation>Department of Informatics, King's College London</affiliation></author></authors><title>Saying Hello World with UML-RSDS - A Solution to the 2011 Instructive
  Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 325-331</journal-ref><doi>10.4204/EPTCS.74.26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply the UML-RSDS notation and tools to the &quot;Hello World&quot;
case studies and explain the underlying development process for this model
transformation approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4764</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4764</id><created>2011-11-21</created><authors><author><keyname>Rose</keyname><forenames>Louis M.</forenames><affiliation>Department of Computer Science, The University of York</affiliation></author><author><keyname>Garc&#xed;a-Dom&#xed;nguez</keyname><forenames>Antonio</forenames><affiliation>Department of Computer Languages and Systems, University of C&#xe1;diz</affiliation></author><author><keyname>Williams</keyname><forenames>James R.</forenames><affiliation>Department of Computer Science, The University of York</affiliation></author><author><keyname>Kolovos</keyname><forenames>Dimitrios S.</forenames><affiliation>Department of Computer Science, The University of York</affiliation></author><author><keyname>Paige</keyname><forenames>Richard F.</forenames><affiliation>Department of Computer Science, The University of York</affiliation></author><author><keyname>Polack</keyname><forenames>Fiona A. C.</forenames><affiliation>Department of Computer Science, The University of York</affiliation></author></authors><title>Saying Hello World with Epsilon - A Solution to the 2011 Instructive
  Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2011, arXiv:1111.4407</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 74, 2011, pp. 332-339</journal-ref><doi>10.4204/EPTCS.74.27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epsilon is an extensible platform of integrated and task-specific languages
for model management. With solutions to the 2011 TTC Hello World case, this
paper demonstrates some of the key features of the Epsilon Object Language (an
extension and reworking of OCL), which is at the core of Epsilon. In addition,
the paper introduces several of the task-specific languages provided by Epsilon
including the Epsilon Generation Language (for model-to-text transformation),
the Epsilon Validation Language (for model validation) and Epsilon Flock (for
model migration).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4766</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4766</id><created>2011-11-21</created><updated>2015-03-01</updated><authors><author><keyname>Busch</keyname><forenames>Costas</forenames></author><author><keyname>Dutta</keyname><forenames>Chinmoy</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author><author><keyname>Rajaraman</keyname><forenames>Rajmohan</forenames></author><author><keyname>Srinivasagopalan</keyname><forenames>Srivathsan</forenames></author></authors><title>On Strong Graph Partitions and Universal Steiner Trees</title><categories>cs.DS</categories><msc-class>05C85, 68W25</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing universal Steiner trees for undirected
graphs. Given a graph $G$ and a root node $r$, we seek a single spanning tree
$T$ of minimum {\em stretch}, where the stretch of $T$ is defined to be the
maximum ratio, over all terminal sets $X$, of the cost of the minimal sub-tree
$T_X$ of $T$ that connects $X$ to $r$ to the cost of an optimal Steiner tree
connecting $X$ to $r$ in $G$. Universal Steiner trees (USTs) are important for
data aggregation problems where computing the Steiner tree from scratch for
every input instance of terminals is costly, as for example in low energy
sensor network applications.
  We provide a polynomial time \ust\ construction for general graphs with
$2^{O(\sqrt{\log n})}$-stretch. We also give a polynomial time
$\polylog(n)$-stretch construction for minor-free graphs. One basic building
block of our algorithms is a hierarchy of graph partitions, each of which
guarantees small strong diameter for each cluster and bounded neighbourhood
intersections for each node. We show close connections between the problems of
constructing USTs and building such graph partitions. Our construction of
partition hierarchies for general graphs is based on an iterative cluster
merging procedure, while the one for minor-free graphs is based on a separator
theorem for such graphs and the solution to a cluster aggregation problem that
may be of independent interest even for general graphs. To our knowledge, this
is the first subpolynomial-stretch ($o(n^\epsilon)$ for any $\epsilon &gt; 0$) UST
construction for general graphs, and the first polylogarithmic-stretch UST
construction for minor-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4768</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4768</id><created>2011-11-21</created><authors><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Capacity of Multiple Unicast in Wireless Networks: A Polymatroidal
  Approach</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical result in undirected wireline networks is the near optimality of
routing (flow) for multiple-unicast traffic (multiple sources communicating
independent messages to multiple destinations): the min cut upper bound is
within a logarithmic factor of the number of sources of the max flow. In this
paper we &quot;extend&quot; the wireline result to the wireless context.
  Our main result is the approximate optimality of a simple layering principle:
{\em local physical-layer schemes combined with global routing}. We use the
{\em reciprocity} of the wireless channel critically in this result. Our formal
result is in the context of channel models for which &quot;good&quot; local schemes, that
achieve the cut-set bound, exist (such as Gaussian MAC and broadcast channels,
broadcast erasure networks, fast fading Gaussian networks).
  Layered architectures, common in the engineering-design of wireless networks,
can have near-optimal performance if the {\em locality} over which
physical-layer schemes should operate is carefully designed. Feedback is shown
to play a critical role in enabling the separation between the physical and the
network layers. The key technical idea is the modeling of a wireless network by
an undirected &quot;polymatroidal&quot; network, for which we establish a max-flow
min-cut approximation theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4785</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4785</id><created>2011-11-21</created><authors><author><keyname>Muller</keyname><forenames>Christian L.</forenames></author><author><keyname>Ramaswamy</keyname><forenames>Rajesh</forenames></author><author><keyname>Sbalzarini</keyname><forenames>Ivo F.</forenames></author></authors><title>Global parameter identification of stochastic reaction networks from
  single trajectories</title><categories>q-bio.MN cs.CE</categories><comments>Article in print as a book chapter in Springer's &quot;Advances in Systems
  Biology&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of inferring the unknown parameters of a stochastic
biochemical network model from a single measured time-course of the
concentration of some of the involved species. Such measurements are available,
e.g., from live-cell fluorescence microscopy in image-based systems biology. In
addition, fluctuation time-courses from, e.g., fluorescence correlation
spectroscopy provide additional information about the system dynamics that can
be used to more robustly infer parameters than when considering only mean
concentrations. Estimating model parameters from a single experimental
trajectory enables single-cell measurements and quantification of cell--cell
variability. We propose a novel combination of an adaptive Monte Carlo sampler,
called Gaussian Adaptation, and efficient exact stochastic simulation
algorithms that allows parameter identification from single stochastic
trajectories. We benchmark the proposed method on a linear and a non-linear
reaction network at steady state and during transient phases. In addition, we
demonstrate that the present method also provides an ellipsoidal volume
estimate of the viable part of parameter space and is able to estimate the
physical volume of the compartment in which the observed reactions take place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4795</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4795</id><created>2011-11-21</created><updated>2012-03-19</updated><authors><author><keyname>Jung</keyname><forenames>Kyomin</forenames></author><author><keyname>Heo</keyname><forenames>Wooram</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>IRIE: Scalable and Robust Influence Maximization in Social Networks</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization is the problem of selecting top $k$ seed nodes in a
social network to maximize their influence coverage under certain influence
diffusion models. In this paper, we propose a novel algorithm IRIE that
integrates a new message passing based influence ranking (IR), and influence
estimation (IE) methods for influence maximization in both the independent
cascade (IC) model and its extension IC-N that incorporates negative opinion
propagations. Through extensive experiments, we demonstrate that IRIE matches
the influence coverage of other algorithms while scales much better than all
other algorithms. Moreover IRIE is more robust and stable than other algorithms
both in running time and memory usage for various density of networks and
cascade size. It runs up to two orders of magnitude faster than other
state-of-the-art algorithms such as PMIA for large networks with tens of
millions of nodes and edges, while using only a fraction of memory comparing
with PMIA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4800</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4800</id><created>2011-11-21</created><authors><author><keyname>Mukherjee</keyname><forenames>Aroop</forenames><affiliation>Vehere Interactive, Calcutta - India</affiliation></author><author><keyname>Kanrar</keyname><forenames>Soumen</forenames><affiliation>Vehere Interactive, Calcutta - India</affiliation></author></authors><title>Enhancement of Image Resolution by Binarization</title><categories>cs.CV cs.MM</categories><comments>5 pages, 8 figures</comments><journal-ref>International Journal of Computer Applications 10(10):15-19, 2010</journal-ref><doi>10.5120/1519-1942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is one of the principal approaches of image processing.
The choice of the most appropriate Binarization algorithm for each case proved
to be a very interesting procedure itself. In this paper, we have done the
comparison study between the various algorithms based on Binarization
algorithms and propose a methodologies for the validation of Binarization
algorithms. In this work we have developed two novel algorithms to determine
threshold values for the pixels value of the gray scale image. The performance
estimation of the algorithm utilizes test images with, the evaluation metrics
for Binarization of textual and synthetic images. We have achieved better
resolution of the image by using the Binarization method of optimum
thresholding techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4802</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4802</id><created>2011-11-21</created><authors><author><keyname>Benassi</keyname><forenames>Romain</forenames></author><author><keyname>Bect</keyname><forenames>Julien</forenames></author><author><keyname>Vazquez</keyname><forenames>Emmanuel</forenames></author></authors><title>Bayesian optimization using sequential Monte Carlo</title><categories>math.OC cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimizing a real-valued continuous function $f$
using a Bayesian approach, where the evaluations of $f$ are chosen sequentially
by combining prior information about $f$, which is described by a random
process model, and past evaluation results. The main difficulty with this
approach is to be able to compute the posterior distributions of quantities of
interest which are used to choose evaluation points. In this article, we decide
to use a Sequential Monte Carlo (SMC) approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4807</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4807</id><created>2011-11-21</created><updated>2012-03-03</updated><authors><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Banerjee</keyname><forenames>Abhik</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Becker</keyname><forenames>Monique</forenames></author><author><keyname>Yeo</keyname><forenames>Chai Kiat</forenames></author><author><keyname>Lee</keyname><forenames>Bu Sung</forenames></author></authors><title>Achieving Small World Properties using Bio-Inspired Techniques in
  Wireless Networks</title><categories>cs.NI</categories><comments>Accepted for publication: Special Issue on Security and Performance
  of Networks and Clouds (The Computer Journal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is highly desirable and challenging for a wireless ad hoc network to have
self-organization properties in order to achieve network wide characteristics.
Studies have shown that Small World properties, primarily low average path
length and high clustering coefficient, are desired properties for networks in
general. However, due to the spatial nature of the wireless networks, achieving
small world properties remains highly challenging. Studies also show that,
wireless ad hoc networks with small world properties show a degree distribution
that lies between geometric and power law. In this paper, we show that in a
wireless ad hoc network with non-uniform node density with only local
information, we can significantly reduce the average path length and retain the
clustering coefficient. To achieve our goal, our algorithm first identifies
logical regions using Lateral Inhibition technique, then identifies the nodes
that beamform and finally the beam properties using Flocking. We use Lateral
Inhibition and Flocking because they enable us to use local state information
as opposed to other techniques. We support our work with simulation results and
analysis, which show that a reduction of up to 40% can be achieved for a
high-density network. We also show the effect of hopcount used to create
regions on average path length, clustering coefficient and connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4819</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4819</id><created>2011-11-21</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author><author><keyname>Siraj</keyname><forenames>Mohammad</forenames></author></authors><title>Enhanced Antenna Position Implementation Over Vehicular Ad Hoc Network
  (VNET) In 3D Space</title><categories>cs.NI</categories><comments>12 pages, 8 figures; ISSN: 0975-3834 [Online]; 0975-4679 [Print]</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol-2,
  No -1, 2010, 164-175</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technology related to networking moves wired connection to wireless
connection.The basic problem concern in the wireless domain, random packet loss
for the end to end connection. In this paper we show the performance and the
impact of the packet loss and delay, by the bit error rate throughput etc with
respect to the real world scenario vehicular ad hoc network in 3-dimension
space (VANET in 3D). Over the years software development has responded to the
increasing growth of wireless connectivity in developing network enabled
software. In this paper we consider the real world physical problem in three
dimensional wireless domain and map the problem to analytical problem . In this
paper we simulate that analytic problem with respect to real world scenario by
using enhanced antenna position system (EAPS) mounted over the mobile node in
3D space. In this paper we convert the real world problem into lab oriented
problem by using the EAPS -system and shown the performance in wireless domain
in 3 dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4825</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4825</id><created>2011-11-21</created><updated>2012-06-06</updated><authors><author><keyname>Montijano</keyname><forenames>Eduardo</forenames></author><author><keyname>Montijano</keyname><forenames>Juan I.</forenames></author><author><keyname>Sagues</keyname><forenames>Carlos</forenames></author></authors><title>Chebyshev Polynomials in Distributed Consensus Applications</title><categories>cs.SY cs.DC cs.MA</categories><comments>Preprint submitted for its publication in a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the use of Chebyshev polynomials in distributed
consensus applications. We study the properties of these polynomials to propose
a distributed algorithm that reaches the consensus in a fast way. The algorithm
is expressed in the form of a linear iteration and, at each step, the agents
only require to transmit their current state to their neighbors. The difference
with respect to previous approaches is that the update rule used by the network
is based on the second order difference equation that describes the Chebyshev
polynomials of first kind. As a consequence, we show that our algorithm
achieves the consensus using far less iterations than other approaches. We
characterize the main properties of the algorithm for both, fixed and switching
communication topologies. The main contribution of the paper is the study of
the properties of the Chebyshev polynomials in distributed consensus
applications, proposing an algorithm that increases the convergence rate with
respect to existing approaches. Theoretical results, as well as experiments
with synthetic data, show the benefits using our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4831</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4831</id><created>2011-11-21</created><authors><author><keyname>Karimi</keyname><forenames>N.</forenames></author></authors><title>Analytical calculation of optimal POVM for unambiguous discrimination of
  quantum states using KKT method</title><categories>cs.IT math.IT quant-ph</categories><comments>18 pages. arXiv admin note: substantial text overlap with
  arXiv:0708.2323</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the present paper, an exact analytic solution for the optimal unambiguous
state discrimination (OPUSD) problem involving an arbitrary number of pure
linearly independent quantum states with real and complex inner product is
presented. Using semidefinite programming and Karush-Kuhn-Tucker convex
optimization method, we derive an analytical formula which shows the relation
between optimal solution of unambiguous state discrimination problem and an
arbitrary number of pure linearly independent quantum states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4840</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4840</id><created>2011-11-21</created><updated>2012-04-11</updated><authors><author><keyname>Montijano</keyname><forenames>Eduardo</forenames></author><author><keyname>Aragues</keyname><forenames>Rosario</forenames></author><author><keyname>Sagues</keyname><forenames>Carlos</forenames></author></authors><title>Distributed Multi-view Matching in Networks with Limited Communications</title><categories>cs.CV cs.MA cs.RO</categories><comments>This paper has been withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of distributed matching of features in networks with
vision systems. Every camera in the network has limited communication
capabilities and can only exchange local matches with its neighbors. We propose
a distributed algorithm that takes these local matches and computes global
correspondences by a proper propagation in the network. When the algorithm
finishes, each camera knows the global correspondences between its features and
the features of all the cameras in the network. The presence of spurious
introduced by the local matcher may produce inconsistent global
correspondences, which are association paths between features from the same
camera. The contributions of this work are the propagation of the local matches
and the detection and resolution of these inconsistencies by deleting local
matches. Our resolution algorithm considers the quality of each local match,
when this information is provided by the local matcher. We formally prove that
after executing the algorithm, the network finishes with a global data
association free of inconsistencies. We provide a fully decentralized solution
to the problem which does not rely on any particular communication topology.
Simulations and experimental results with real images show the performance of
the method considering different features, matching functions and scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4841</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4841</id><created>2011-11-21</created><authors><author><keyname>Font-Clos</keyname><forenames>Francesc</forenames></author><author><keyname>Massucci</keyname><forenames>Francesco Alessandro</forenames></author><author><keyname>Castillo</keyname><forenames>Isaac P&#xe9;rez</forenames></author></authors><title>A weighted message-passing algorithm to estimate volume-related
  properties of random polytopes</title><categories>cs.DS cond-mat.dis-nn math.MG</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we introduce a novel message-passing algorithm for a class of
problems which can be mathematically understood as estimating volume-related
properties of random polytopes. Unlike the usual approach consisting in
approximating the real-valued cavity marginal distributions by a few
parameters, we propose a weighted message-passing algorithm to deal with the
entire function. Various alternatives of how to implement our approach are
discussed and numerical results for random polytopes are compared with results
using the Hit-and-Run algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4852</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4852</id><created>2011-11-21</created><authors><author><keyname>Watanabe</keyname><forenames>Hayafumi</forenames></author><author><keyname>Takayasu</keyname><forenames>Hideki</forenames></author><author><keyname>Takayasu</keyname><forenames>Misako</forenames></author></authors><title>Biased diffusion on Japanese inter-firm trading network: Estimation of
  sales from network structure</title><categories>q-fin.GN cs.SI physics.soc-ph</categories><journal-ref>New J. Phys. 14 (2012) 043034</journal-ref><doi>10.1088/1367-2630/14/4/043034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To investigate the actual phenomena of transport on a complex network, we
analysed empirical data for an inter-firm trading network, which consists of
about one million Japanese firms and the sales of these firms (a sale
corresponds to the total in-flow into a node). First, we analysed the
relationships between sales and sales of nearest neighbourhoods from which we
obtain a simple linear relationship between sales and the weighted sum of sales
of nearest neighbourhoods (i.e., customers). In addition, we introduce a simple
money transport model that is coherent with this empirical observation. In this
model, a firm (i.e., customer) distributes money to its out-edges (suppliers)
proportionally to the in-degree of destinations. From intensive numerical
simulations, we find that the steady flows derived from these models can
approximately reproduce the distribution of sales of actual firms. The sales of
individual firms deduced from the money-transport model are shown to be
proportional, on an average, to the real sales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4872</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4872</id><created>2011-11-21</created><authors><author><keyname>Kalinich</keyname><forenames>Adam O.</forenames></author></authors><title>Flipping the Winner of a Poset Game</title><categories>cs.GT</categories><doi>10.1016/j.ipl.2011.09.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially-ordered set games, also called poset games, are a class of
two-player combinatorial games. The playing field consists of a set of
elements, some of which are greater than other elements. Two players take turns
removing an element and all elements greater than it, and whoever takes the
last element wins. Examples of poset games include Nim and Chomp. We
investigate the complexity of computing which player of a poset game has a
winning strategy. We give an inductive procedure that modifies poset games to
change the nim- value which informally captures the winning strategies in the
game. For a generic poset game G, we describe an efficient method for
constructing a game not G such that the first player has a winning strategy if
and only if the second player has a winning strategy on G. This solves the
long-standing problem of whether this construction can be done efficiently.
This construction also allows us to reduce the class of Boolean formulas to
poset games, establishing a lower bound on the complexity of poset games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4877</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4877</id><created>2011-11-21</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Sha</keyname><forenames>Qian</forenames></author><author><keyname>Fan</keyname><forenames>Xiao</forenames></author></authors><title>Adleman-Manders-Miller Root Extraction Method Revisited</title><categories>cs.SC math.NT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1977, Adleman, Manders and Miller had briefly described how to extend
their square root extraction method to the general $r$th root extraction over
finite fields, but not shown enough details. Actually, there is a dramatic
difference between the square root extraction and the general $r$th root
extraction because one has to solve discrete logarithms for $r$th root
extraction. In this paper, we clarify their method and analyze its complexity.
Our heuristic presentation is helpful to grasp the method entirely and deeply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4886</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4886</id><created>2011-11-21</created><updated>2011-11-24</updated><authors><author><keyname>Mahantesh</keyname><forenames>S. M. Vijay</forenames></author><author><keyname>Iyengar</keyname><forenames>Sudarshan</forenames></author><author><keyname>Vijesh</keyname><forenames>M.</forenames></author><author><keyname>Nayak</keyname><forenames>Shruthi</forenames></author><author><keyname>Shenoy</keyname><forenames>Nikitha</forenames></author></authors><title>Prediction Of Arrival Of Nodes In A Scale Free Network</title><categories>cs.SI cs.NI physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the networks observed in real life obey power-law degree
distribution. It is hypothesized that the emergence of such a degree
distribution is due to preferential attachment of the nodes. Barabasi-Albert
model is a generative procedure that uses preferential attachment based on
degree and one can use this model to generate networks with power-law degree
distribution. In this model, the network is assumed to grow one node every time
step. After the evolution of such a network, it is impossible for one to
predict the exact order of node arrivals. We present in this article, a novel
strategy to partially predict the order of node arrivals in such an evolved
network. We show that our proposed method outperforms other centrality measure
based approaches. We bin the nodes and predict the order of node arrivals
between the bins with an accuracy of above 80%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4898</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4898</id><created>2011-11-21</created><authors><author><keyname>M.</keyname><forenames>Vijesh</forenames></author><author><keyname>Iyengar</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mahantesh</keyname><forenames>Vijay</forenames></author><author><keyname>Ramesh</keyname><forenames>Amitash</forenames></author><author><keyname>Madhavan</keyname><forenames>Veni</forenames></author></authors><title>A Navigation Algorithm Inspired by Human Navigation</title><categories>cs.SI physics.soc-ph</categories><comments>Human Navigation, Path Concatenation, Hotspots, Center Strategic
  Paths, Approximation Algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human navigation has been a topic of interest in spatial cognition from the
past few decades. It has been experimentally observed that humans accomplish
the task of way-finding a destination in an unknown environment by recognizing
landmarks. Investigations using network analytic techniques reveal that humans,
when asked to way-find their destination, learn the top ranked nodes of a
network. In this paper we report a study simulating the strategy used by humans
to recognize the centers of a network. We show that the paths obtained from our
simulation has the same properties as the paths obtained in human based
experiment. The simulation thus performed leads to a novel way of path-finding
in a network. We discuss the performance of our method and compare it with the
existing techniques to find a path between a pair of nodes in a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4930</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4930</id><created>2011-11-21</created><updated>2012-01-31</updated><authors><author><keyname>Ghosh</keyname><forenames>Arka</forenames></author></authors><title>Comparative study of Financial Time Series Prediction by Artificial
  Neural Network with Gradient Descent Learning</title><categories>cs.NE cs.AI</categories><journal-ref>International Journal Of Scientific &amp; Engineering Research
  ISSN-2229-5518 Volume 3 Issue 1 January2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial forecasting is an example of a signal processing problem which is
challenging due to Small sample sizes, high noise, non-stationarity, and
non-linearity,but fast forecasting of stock market price is very important for
strategic business planning.Present study is aimed to develop a comparative
predictive model with Feedforward Multilayer Artificial Neural Network &amp;
Recurrent Time Delay Neural Network for the Financial Timeseries
Prediction.This study is developed with the help of historical stockprice
dataset made available by GoogleFinance.To develop this prediction model
Backpropagation method with Gradient Descent learning has been
implemented.Finally the Neural Net, learned with said algorithm is found to be
skillful predictor for non-stationary noisy Financial Timeseries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4937</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4937</id><created>2011-11-21</created><updated>2011-11-22</updated><authors><author><keyname>He</keyname><forenames>Bryan Dawei</forenames></author></authors><title>A Simple Optimal Binary Representation of Mosaic Floorplans and Baxter
  Permutations</title><categories>cs.DS cs.DM math.CO</categories><comments>11 pages, 7 figures</comments><msc-class>05A05, 68R99, 68U07, 68W35, 94B99</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A &quot;floorplan&quot; is a rectangle subdivided into smaller rectangular sections by
horizontal and vertical line segments. Each section in the floorplan is called
a &quot;block&quot;. Two floorplans are considered equivalent if and only if there is a
one-to-one correspondence between the blocks in the two floorplans such that
the relative position relationship of the blocks in one floorplan is the same
as the relative position relationship of the corresponding blocks in another
floorplan. The objects of &quot;Mosaic floorplans&quot; are the same as floorplans, but
an alternative definition of equivalence is used. Two mosaic floorplans are
considered equivalent if and only if they can be converted to each other by
sliding the line segments that divide the blocks.
  Mosaic floorplans are widely used in VLSI circuit design. An important
problem in this area is to find short binary string representations of the set
of n-block mosaic floorplans. The best known representation is the
&quot;Quarter-State Sequence&quot; which uses 4n bits. This paper introduces a simple
binary representation of n-block mosaic floorplan using 3n-3 bits. It has been
shown that any binary representation of n-block mosaic floorplans must use at
least (3n-o(n)) bits. Therefore, the representation presented in this paper is
optimal (up to an additive lower order term).
  &quot;Baxter permutations&quot; are a set of permutations defined by prohibited
subsequences. Baxter permutations have been shown to have one-to-one
correspondences to many interesting objects in the so-called &quot;Baxter
combinatorial family&quot;. In particular, there exists a simple one-to-one
correspondence between mosaic floorplans and Baxter permutations. As a result,
the methods introduced in this paper also lead to an optimal binary
representation of Baxter permutations and all objects in the Baxter
combinatorial family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4949</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4949</id><created>2011-11-21</created><updated>2015-07-03</updated><authors><author><keyname>Mondal</keyname><forenames>Nabarun</forenames></author><author><keyname>Ghosh</keyname><forenames>Partha P.</forenames></author></authors><title>Universal Computation Is 'Almost Surely' Chaotic</title><categories>cs.CC math.DS math.NT nlin.CD</categories><comments>24 pages, 0 figures, paper accepted at &quot;Scientific Online&quot; :
  http://www.scipublish.com/journals/AM/papers/1506</comments><msc-class>03D10, 65P20, 68Q05, 68Q87</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixed point iterations are known to generate chaos, for some values in their
parameter range. It is an established fact that Turing Machines are fixed point
iterations. However, as these Machines operate in integer space, the standard
notions of a chaotic system is not readily applicable for them. Changing the
state space of Turing Machines from integer to rational space, the condition
for chaotic dynamics can be suitably established, as presented in the current
paper. Further it is deduced that, given random input, computation performed by
a Universal Turing Machine would be 'almost surely' chaotic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.4969</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.4969</id><created>2011-11-21</created><authors><author><keyname>Jiang</keyname><forenames>Chunxiao</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>K. J. Ray</forenames></author><author><keyname>Ren</keyname><forenames>Yong</forenames></author></authors><title>Renewal-Theoretical Dynamic Spectrum Access in Cognitive Radio Networks
  with Unknown Primary Behavior</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum access in cognitive radio networks can greatly improve the
spectrum utilization efficiency. Nevertheless, interference may be introduced
to the Primary User (PU) when the Secondary Users (SUs) dynamically utilize the
PU's licensed channels. If the SUs can be synchronous with the PU's time slots,
the interference is mainly due to their imperfect spectrum sensing of the
primary channel. However, if the SUs have no knowledge about the PU's exact
communication mechanism, additional interference may occur. In this paper, we
propose a dynamic spectrum access protocol for the SUs confronting with unknown
primary behavior and study the interference caused by their dynamic access.
Through analyzing the SUs' dynamic behavior in the primary channel which is
modeled as an ON-OFF process, we prove that the SUs' communication behavior is
a renewal process. Based on the Renewal Theory, we quantify the interference
caused by the SUs and derive the corresponding close-form expressions. With the
interference analysis, we study how to optimize the SUs' performance under the
constraints of the PU's communication quality of service (QoS) and the
secondary network's stability. Finally, simulation results are shown to verify
the effectiveness of our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5002</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5002</id><created>2011-11-21</created><authors><author><keyname>Colver</keyname><forenames>David</forenames></author></authors><title>Drivers of the Cost of Spreadsheet Audit</title><categories>cs.SE</categories><comments>8 Pages, 2 Tables; Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A review of 75 formal audit assignments shows that the effort taken to
identify defects in financial models taken from the domain of limited recourse
(project) finance is uncorrelated with common measures of the physical
characteristics of the spreadsheets concerned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5003</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5003</id><created>2011-11-21</created><updated>2012-04-24</updated><authors><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>Construction of Almost Disjunct Matrices for Group Testing</title><categories>cs.IT cs.DM math.IT</categories><comments>15 Pages</comments><journal-ref>Algorithms and Computation, Lecture Notes in Computer Science
  Volume 7676, 2012, pp 649-658</journal-ref><doi>10.1007/978-3-642-35261-4_67</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a \emph{group testing} scheme, a set of tests is designed to identify a
small number $t$ of defective items among a large set (of size $N$) of items.
In the non-adaptive scenario the set of tests has to be designed in one-shot.
In this setting, designing a testing scheme is equivalent to the construction
of a \emph{disjunct matrix}, an $M \times N$ matrix where the union of supports
of any $t$ columns does not contain the support of any other column. In
principle, one wants to have such a matrix with minimum possible number $M$ of
rows (tests). One of the main ways of constructing disjunct matrices relies on
\emph{constant weight error-correcting codes} and their \emph{minimum
distance}. In this paper, we consider a relaxed definition of a disjunct matrix
known as \emph{almost disjunct matrix}. This concept is also studied under the
name of \emph{weakly separated design} in the literature. The relaxed
definition allows one to come up with group testing schemes where a
close-to-one fraction of all possible sets of defective items are identifiable.
Our main contribution is twofold. First, we go beyond the minimum distance
analysis and connect the \emph{average distance} of a constant weight code to
the parameters of an almost disjunct matrix constructed from it. Our second
contribution is to explicitly construct almost disjunct matrices based on our
average distance analysis, that have much smaller number of rows than any
previous explicit construction of disjunct matrices. The parameters of our
construction can be varied to cover a large range of relations for $t$ and $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5007</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5007</id><created>2011-11-21</created><authors><author><keyname>Wu</keyname><forenames>Nancy</forenames></author></authors><title>Leveraging User Profile and Behaviour to Design Practical Spreadsheet
  Controls for the Finance Function</title><categories>cs.SE</categories><comments>8 Pages, 4 Tables; Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing that the use of spreadsheets within finance will likely not
subside in the near future, this paper discusses a major barrier that is
preventing more organizations from adopting enterprise spreadsheet management
programs. But even without a corporate mandated effort to improve spreadsheet
controls, finance functions can still take simple yet effective steps to start
managing the risk of errors in key spreadsheets by strategically selecting
controls that complement existing user practice
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5038</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5038</id><created>2011-11-21</created><updated>2011-11-28</updated><authors><author><keyname>Okubo</keyname><forenames>Fumiya</forenames></author><author><keyname>Kobayashi</keyname><forenames>Satoshi</forenames></author><author><keyname>Yokomori</keyname><forenames>Takashi</forenames></author></authors><title>Reaction Automata</title><categories>cs.FL</categories><comments>19 pages, 6 figures</comments><report-no>EMTR-11-02</report-no><msc-class>68Q45 (Primary) 68Q05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reaction systems are a formal model that has been introduced to investigate
the interactive behaviors of biochemical reactions. Based on the formal
framework of reaction systems, we propose new computing models called reaction
automata that feature (string) language acceptors with multiset manipulation as
a computing mechanism, and show that reaction automata are computationally
Turing universal. Further, some subclasses of reaction automata with space
complexity are investigated and their language classes are compared to the ones
in the Chomsky hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5046</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5046</id><created>2011-11-21</created><updated>2012-05-16</updated><authors><author><keyname>Yilmaz</keyname><forenames>Yasin</forenames></author><author><keyname>Moustakides</keyname><forenames>George</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Cooperative Sequential Spectrum Sensing Based on Level-triggered
  Sampling</title><categories>stat.AP cs.IT math.IT</categories><doi>10.1109/TSP.2012.2202657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for cooperative spectrum sensing in cognitive
radio networks, that is based on a novel class of non-uniform samplers, called
the event-triggered samplers, and sequential detection. In the proposed scheme,
each secondary user computes its local sensing decision statistic based on its
own channel output; and whenever such decision statistic crosses certain
predefined threshold values, the secondary user will send one (or several) bit
of information to the fusion center. The fusion center asynchronously receives
the bits from different secondary users and updates the global sensing decision
statistic to perform a sequential probability ratio test (SPRT), to reach a
sensing decision. We provide an asymptotic analysis for the above scheme, and
under different conditions, we compare it against the cooperative sensing
scheme that is based on traditional uniform sampling and sequential detection.
Simulation results show that the proposed scheme, using even 1 bit, can
outperform its uniform sampling counterpart that uses infinite number of bits
under changing target error probabilities, SNR values, and number of SUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5062</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5062</id><created>2011-11-21</created><updated>2013-09-19</updated><authors><author><keyname>Blundo</keyname><forenames>Carlo</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author></authors><title>EsPRESSo: Efficient Privacy-Preserving Evaluation of Sample Set
  Similarity</title><categories>cs.CR</categories><comments>A preliminary version of this paper was published in the Proceedings
  of the 7th ESORICS International Workshop on Digital Privacy Management (DPM
  2012). This is the full version, appearing in the Journal of Computer
  Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic information is increasingly often shared among entities without
complete mutual trust. To address related security and privacy issues, a few
cryptographic techniques have emerged that support privacy-preserving
information sharing and retrieval. One interesting open problem in this context
involves two parties that need to assess the similarity of their datasets, but
are reluctant to disclose their actual content. This paper presents an
efficient and provably-secure construction supporting the privacy-preserving
evaluation of sample set similarity, where similarity is measured as the
Jaccard index. We present two protocols: the first securely computes the
(Jaccard) similarity of two sets, and the second approximates it, using MinHash
techniques, with lower complexities. We show that our novel protocols are
attractive in many compelling applications, including document/multimedia
similarity, biometric authentication, and genetic tests. In the process, we
demonstrate that our constructions are appreciably more efficient than prior
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5080</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5080</id><created>2011-11-21</created><updated>2013-03-21</updated><authors><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Zhu</keyname><forenames>Haojin</forenames></author><author><keyname>Gao</keyname><forenames>Zhaoyu</forenames></author><author><keyname>Guan</keyname><forenames>Xinping</forenames></author><author><keyname>Xing</keyname><forenames>Kai</forenames></author></authors><title>YouSense: Mitigating Entropy Selfishness in Distributed Collaborative
  Spectrum Sensing</title><categories>cs.NI</categories><comments>at INFOCOM'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative spectrum sensing has been recognized as a promising approach to
improve the sensing performance via exploiting the spatial diversity of the
secondary users. In this study, a new selfishness issue is identified, that
selfish users sense no spectrum in collaborative sensing. For easier
presentation, it's denoted as entropy selfishness. This selfish behavior is
difficult to distinguish, making existing detection based incentive schemes
fail to work. To thwart entropy selfishness in distributed collaborative
sensing, we propose YouSense, a One-Time Pad (OTP) based incentive design that
could naturally isolate entropy selfish users from the honest users without
selfish node detection. The basic idea of YouSense is to construct a trapdoor
one-time pad for each sensing report by combining the original report and a
random key. Such a one-time pad based encryption could prevent entropy selfish
users from accessing the original sensing report while enabling the honest
users to recover the report. Different from traditional cryptography based OTP
which requires the key delivery, YouSense allows an honest user to recover the
pad (or key) by exploiting a unique characteristic of collaborative sensing
that different secondary users share some common observations on the same radio
spectrum. We further extend YouSense to improve the recovery successful rate by
reducing the cardinality of set of the possible pads. By extensive USRP based
experiments, we show that YouSense can successfully thwart entropy selfishness
with low system overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5092</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5092</id><created>2011-11-22</created><updated>2014-01-18</updated><authors><author><keyname>Hur</keyname><forenames>Youngmi</forenames></author><author><keyname>Zheng</keyname><forenames>Fang</forenames></author></authors><title>Coset Sum: an alternative to the tensor product in wavelet construction</title><categories>math.NA cs.IT math.IT</categories><comments>Version published in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Trans. Inform. Theory, Vol. 59 (2013) 3554-3571</journal-ref><doi>10.1109/TIT.2013.2244165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multivariate biorthogonal wavelet system can be obtained from a pair of
multivariate biorthogonal refinement masks in Multiresolution Analysis setup.
Some multivariate refinement masks may be decomposed into lower dimensional
refinement masks. Tensor product is a popular way to construct a decomposable
multivariate refinement mask from lower dimensional refinement masks.
  We present an alternative method, which we call coset sum, for constructing
multivariate refinement masks from univariate refinement masks. The coset sum
shares many essential features of the tensor product that make it attractive in
practice: (1) it preserves the biorthogonality of univariate refinement masks,
(2) it preserves the accuracy number of the univariate refinement mask, and (3)
the wavelet system associated with it has fast algorithms for computing and
inverting the wavelet coefficients. The coset sum can even provide a wavelet
system with faster algorithms in certain cases than the tensor product. These
features of the coset sum suggest that it is worthwhile to develop and practice
alternative methods to the tensor product for constructing multivariate wavelet
systems. Some experimental results using 2-D images are presented to illustrate
our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5108</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5108</id><created>2011-11-22</created><authors><author><keyname>Nagaraj</keyname><forenames>Sriram</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>A Theory for Optical flow-based Transport on Image Manifolds</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An image articulation manifold (IAM) is the collection of images formed when
an object is articulated in front of a camera. IAMs arise in a variety of image
processing and computer vision applications, where they provide a natural
low-dimensional embedding of the collection of high-dimensional images. To date
IAMs have been studied as embedded submanifolds of Euclidean spaces.
Unfortunately, their promise has not been realized in practice, because real
world imagery typically contains sharp edges that render an IAM
non-differentiable and hence non-isometric to the low-dimensional parameter
space under the Euclidean metric. As a result, the standard tools from
differential geometry, in particular using linear tangent spaces to transport
along the IAM, have limited utility. In this paper, we explore a nonlinear
transport operator for IAMs based on the optical flow between images and
develop new analytical tools reminiscent of those from differential geometry
using the idea of optical flow manifolds (OFMs). We define a new metric for
IAMs that satisfies certain local isometry conditions, and we show how to use
this metric to develop a new tools such as flow fields on IAMs, parallel flow
fields, parallel transport, as well as a intuitive notion of curvature. The
space of optical flow fields along a path of constant curvature has a natural
multi-scale structure via a monoid structure on the space of all flow fields
along a path. We also develop lower bounds on approximation errors while
approximating non-parallel flow fields by parallel flow fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5123</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5123</id><created>2011-11-22</created><authors><author><keyname>Heen</keyname><forenames>Olivier</forenames></author><author><keyname>Merrer</keyname><forenames>Erwan Le</forenames></author><author><keyname>Neumann</keyname><forenames>Christoph</forenames></author><author><keyname>Onno</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Pretty Private Group Management</title><categories>cs.DC cs.SI</categories><journal-ref>Symposium on Reliable Distributed Systems (2012) 191-200</journal-ref><doi>10.1109/SRDS.2012.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group management is a fundamental building block of today's Internet
applications. Mailing lists, chat systems, collaborative document edition but
also online social networks such as Facebook and Twitter use group management
systems. In many cases, group security is required in the sense that access to
data is restricted to group members only. Some applications also require
privacy by keeping group members anonymous and unlinkable. Group management
systems routinely rely on a central authority that manages and controls the
infrastructure and data of the system. Personal user data related to groups
then becomes de facto accessible to the central authority. In this paper, we
propose a completely distributed approach for group management based on
distributed hash tables. As there is no enrollment to a central authority, the
created groups can be leveraged by various applications. Following this
paradigm we describe a protocol for such a system. We consider security and
privacy issues inherently introduced by removing the central authority and
provide a formal validation of security properties of the system using AVISPA.
We demonstrate the feasibility of this protocol by implementing a prototype
running on top of Vuze's DHT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5133</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5133</id><created>2011-11-22</created><updated>2012-03-15</updated><authors><author><keyname>Bauer</keyname><forenames>Andreas</forenames><affiliation>ANU</affiliation></author><author><keyname>Falcone</keyname><forenames>Yli&#xe8;s</forenames><affiliation>U of Grenoble</affiliation></author></authors><title>Decentralised LTL Monitoring</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users wanting to monitor distributed or component-based systems often
perceive them as monolithic systems which, seen from the outside, exhibit a
uniform behaviour as opposed to many components displaying many local
behaviours that together constitute the system's global behaviour. This level
of abstraction is often reasonable, hiding implementation details from users
who may want to specify the system's global behaviour in terms of an LTL
formula. However, the problem that arises then is how such a specification can
actually be monitored in a distributed system that has no central data
collection point, where all the components' local behaviours are observable. In
this case, the LTL specification needs to be decomposed into sub-formulae
which, in turn, need to be distributed amongst the components' locally attached
monitors, each of which sees only a distinct part of the global behaviour. The
main contribution of this paper is an algorithm for distributing and monitoring
LTL formulae, such that satisfac- tion or violation of specifications can be
detected by local monitors alone. We present an implementation and show that
our algorithm introduces only a minimum delay in detecting
satisfaction/violation of a specification. Moreover, our practical results show
that the communication overhead introduced by the local monitors is
considerably lower than the number of messages that would need to be sent to a
central data collection point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5135</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5135</id><created>2011-11-22</created><authors><author><keyname>Nithyanandam</keyname><forenames>S.</forenames></author><author><keyname>Gayathri</keyname><forenames>K. S.</forenames></author><author><keyname>Priyadarshini</keyname><forenames>P. L. K.</forenames></author></authors><title>A New IRIS Normalization Process For Recognition System With
  Cryptographic Techniques</title><categories>cs.CV cs.CR</categories><comments>7 Pages,16 Figures; ISSN (Online): 1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, 2011, 342-348</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric technologies are the foundation of personal identification systems.
It provides an identification based on a unique feature possessed by the
individual. This paper provides a walkthrough for image acquisition,
segmentation, normalization, feature extraction and matching based on the Human
Iris imaging. A Canny Edge Detection scheme and a Circular Hough Transform, is
used to detect the iris boundaries in the eye's digital image. The extracted
IRIS region was normalized by using Image Registration technique. A phase
correlation base method is used for this iris image registration purpose. The
features of the iris region is encoded by convolving the normalized iris region
with 2D Gabor filter. Hamming distance measurement is used to compare the
quantized vectors and authenticate the users. To improve the security,
Reed-Solomon technique is employed directly to encrypt and decrypt the data.
Experimental results show that our system is quite effective and provides
encouraging performance. Keywords: Biometric, Iris Recognition, Phase
correlation, cryptography, Reed-Solomon
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5170</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5170</id><created>2011-11-22</created><authors><author><keyname>Diertens</keyname><forenames>Bob</forenames></author></authors><title>Concurrent Models for Function Execution</title><categories>cs.SE</categories><report-no>TCS1101</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive an abstract computational model from a sequential computational
model that is generally used for function execution. This abstract
computational model allows for the concurrent execution of functions. We
discuss concurrent models for function execution as implementations from the
abstract computational model. We give an example of a particular concurrent
function construct that can be implemented on a concurrent machine model using
multi-threading. The result is a framework of computational models at different
levels of abstraction that can be used in further development of concurrent
computational models that deal with the problems inherent with concurrency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5172</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5172</id><created>2011-11-22</created><authors><author><keyname>Diertens</keyname><forenames>Bob</forenames></author></authors><title>Communicating Concurrent Functions</title><categories>cs.SE</categories><report-no>TCS1102</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we extend the framework of execution of concurrent functions
on different abstract levels from previous work with communication between the
concurrent functions. We classify the communications and identify problems that
can occur with these communications. We present solutions for the problems
based on encapsulation and abstraction to obtain correct behaviours. The result
is that communication on a low level of abstraction in the form of shared
memory and message passing is dealt with on an higher level of abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5189</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5189</id><created>2011-11-22</created><authors><author><keyname>Tan</keyname><forenames>Evan</forenames></author><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>A Frame Rate Optimization Framework For Improving Continuity In Video
  Streaming</title><categories>cs.NI cs.MM</categories><acm-class>C.2.0; H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to reduce the prebuffering requirements, while maintaining
continuity, for video streaming. Current approaches do this by making use of
adaptive media playout (AMP) to reduce the playout rate. However, this
introduces playout distortion to the viewers and increases the viewing latency.
We approach this by proposing a frame rate optimization framework that adjusts
both the encoder frame generation rate and the decoder playout frame rate.
Firstly, we model this problem as the joint adjustment of the encoder frame
generation interval and the decoder playout frame interval. This model is used
with a discontinuity penalty virtual buffer to track the accumulated difference
between the receiving frame interval and the playout frame interval. We then
apply Lyapunov optimization to the model to systematically derive a pair of
decoupled optimization policies. We show that the occupancy of the
discontinuity penalty virtual buffer is correlated to the video discontinuity
and that this framework produces a very low playout distortion in addition to a
significant reduction in the prebuffering requirements compared to existing
approaches. Secondly, we introduced a delay constraint into the framework by
using a delay accumulator virtual buffer. Simulation results show that the the
delay constrained framework provides a superior tradeoff between the video
quality and the delay introduced compared to the existing approach. Finally, we
analyzed the impact of delayed feedback between the receiver and the sender on
the optimization policies. We show that the delayed feedbacks have a minimal
impact on the optimization policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5200</identifier>
 <datestamp>2012-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5200</id><created>2011-11-22</created><updated>2012-01-19</updated><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Mitra</keyname><forenames>Pradipta</forenames></author></authors><title>Wireless Capacity and Admission Control in Cognitive Radio</title><categories>cs.DS cs.NI</categories><comments>to appear in INFOCOM 2012, 16 pages, 4 fgures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give algorithms with constant-factor performance guarantees for several
capacity and throughput problems in the SINR model. The algorithms are all
based on a novel LP formulation for capacity problems. First, we give a new
constant-factor approximation algorithm for selecting the maximum subset of
links that can be scheduled simultaneously, under any non-decreasing and
sublinear power assignment. For the case of uniform power, we extend this to
the case of variable QoS requirements and link-dependent noise terms. Second,
we approximate a problem related to cognitive radio: find a maximum set of
links that can be simultaneously scheduled without affecting a given set of
previously assigned links. Finally, we obtain constant-factor approximation of
weighted capacity under linear power assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5207</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5207</id><created>2011-11-22</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Robot Companions: Technology for Humans</title><categories>cs.RO</categories><comments>Jeremy Pitt (Edt.) This Pervasive Day: The Potential and Perils of
  Pervasive Computing, Imperial College Press, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creation of devices and mechanisms which help people has a long history.
Their inventors always targeted practical goals such as irrigation, harvesting,
devices for construction sites, measurement, and, last but not least, military
tasks for different mechanical and later mechatronic systems. Development of
such assisting mechanisms counts back to Greek engineering, came through Middle
Ages and led finally in XIX and XX centuries to autonomous devices, which we
call today &quot;Robots&quot;. This chapter provides overview of several robotic
technologies, introduces bio-/chemo- hybrid and collective systems and discuss
their applications in service areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5219</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5219</id><created>2011-11-22</created><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Awareness and Self-Awareness for Multi-Robot Organisms</title><categories>cs.RO</categories><comments>submitted to &quot;Awareness Magazine&quot;, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Awareness and self-awareness are two different notions related to knowing the
environment and itself. In a general context, the mechanism of self-awareness
belongs to a class of co-called &quot;self-issues&quot; (self-* or self-star):
self-adaptation, self-repairing, self-replication, self-development or
self-recovery. The self-* issues are connected in many ways to adaptability and
evolvability, to the emergence of behavior and to the controllability of
long-term developmental processes. Self-* are either natural properties of
several systems, such as self-assembling of molecular networks, or may emerge
as a result of homeostatic regulation. Different computational processes,
leading to a global optimization, increasing scalability and reliability of
collective systems, create such a homeostatic regulation. Moreover, conditions
of ecological survival, imposed on such systems, lead to a discrimination
between &quot;self&quot; and &quot;non-self&quot; as well as to the emergence of different
self-phenomena. There are many profound challenges, such as understanding these
mechanisms, or long-term predictability, which have a considerable impact on
research in the area of artificial intelligence and intelligent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5220</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5220</id><created>2011-11-22</created><updated>2011-12-03</updated><authors><author><keyname>Grossi</keyname><forenames>Roberto</forenames></author><author><keyname>Ottaviano</keyname><forenames>Giuseppe</forenames></author></authors><title>Fast Compressed Tries through Path Decompositions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tries are popular data structures for storing a set of strings, where common
prefixes are represented by common root-to-node paths. Over fifty years of
usage have produced many variants and implementations to overcome some of their
limitations. We explore new succinct representations of path-decomposed tries
and experimentally evaluate the corresponding reduction in space usage and
memory latency, comparing with the state of the art. We study two cases of
applications: (1) a compressed dictionary for (compressed) strings, and (2) a
monotone minimal perfect hash for strings that preserves their lexicographic
order.
  For (1), we obtain data structures that outperform other state-of-the-art
compressed dictionaries in space efficiency, while obtaining predictable query
times that are competitive with data structures preferred by the practitioners.
In (2), our tries perform several times faster than other trie-based monotone
perfect hash functions, while occupying nearly the same space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5223</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5223</id><created>2011-11-22</created><updated>2012-01-18</updated><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames><affiliation>LIX, Ecole Polytechnique, and CEA LIST</affiliation></author><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames><affiliation>INRIA Saclay, and Ecole Polytechnique</affiliation></author><author><keyname>Goubault</keyname><forenames>Eric</forenames><affiliation>CEA LIST</affiliation></author></authors><title>Coupling policy iteration with semi-definite relaxation to compute
  accurate numerical invariants in static analysis</title><categories>cs.LO math.OC</categories><comments>32 pages, 9 figures, preliminary version appeared in ESOP (European
  Symposium On Programming) 2010; LMCS 8 (1:1) 2012</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (January
  19, 2012) lmcs:687</journal-ref><doi>10.2168/LMCS-8(1:01)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new domain for finding precise numerical invariants of
programs by abstract interpretation. This domain, which consists of level sets
of non-linear functions, generalizes the domain of linear &quot;templates&quot;
introduced by Manna, Sankaranarayanan, and Sipma. In the case of quadratic
templates, we use Shor's semi-definite relaxation to derive computable yet
precise abstractions of semantic functionals, and we show that the abstract
fixpoint equation can be solved accurately by coupling policy iteration and
semi-definite programming. We demonstrate the interest of our approach on a
series of examples (filters, integration schemes) including a degenerate one
(symplectic scheme).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5228</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5228</id><created>2011-11-19</created><updated>2011-11-24</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel A.</forenames></author><author><keyname>Khandani</keyname><forenames>Amir E.</forenames></author><author><keyname>Lo</keyname><forenames>Andrew W.</forenames></author></authors><title>Privacy-Preserving Methods for Sharing Financial Risk Exposures</title><categories>q-fin.RM cs.CE cs.CR q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike other industries in which intellectual property is patentable, the
financial industry relies on trade secrecy to protect its business processes
and methods, which can obscure critical financial risk exposures from
regulators and the public. We develop methods for sharing and aggregating such
risk exposures that protect the privacy of all parties involved and without the
need for a trusted third party. Our approach employs secure multi-party
computation techniques from cryptography in which multiple parties are able to
compute joint functions without revealing their individual inputs. In our
framework, individual financial institutions evaluate a protocol on their
proprietary data which cannot be inverted, leading to secure computations of
real-valued statistics such a concentration indexes, pairwise correlations, and
other single- and multi-point statistics. The proposed protocols are
computationally tractable on realistic sample sizes. Potential financial
applications include: the construction of privacy-preserving real-time indexes
of bank capital and leverage ratios; the monitoring of delegated portfolio
investments; financial audits; and the publication of new indexes of
proprietary trading strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5239</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5239</id><created>2011-11-22</created><updated>2011-11-28</updated><authors><author><keyname>Shuman</keyname><forenames>David I.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Distributed Signal Processing via Chebyshev Polynomial Approximation</title><categories>cs.DC</categories><comments>14 pages, 6 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unions of graph multiplier operators are an important class of linear
operators for processing signals defined on graphs. We present a novel method
to efficiently distribute the application of these operators. The proposed
method features approximations of the graph multipliers by shifted Chebyshev
polynomials, whose recurrence relations make them readily amenable to
distributed computation. We demonstrate how the proposed method can be applied
to distributed processing tasks such as smoothing, denoising, inverse
filtering, and semi-supervised classification, and show that the communication
requirements of the method scale gracefully with the size of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5241</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5241</id><created>2011-11-22</created><authors><author><keyname>Taneja</keyname><forenames>Inder Jeet</forenames></author></authors><title>Refinement of Gini-Means Inequalities and Connections with Divergence
  Measures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1938, Gini studied a mean having two parameters. Later, many authors
studied properties of this mean. It contains as particular cases the famous
means such as harmonic, geometric, arithmetic, etc. Also it contains, the power
mean of order r and Lehmer mean as particular cases. In this paper we have
considered inequalities arising due to Gini-Mean and Heron's mean, and improved
them based on the results recently studied by the author (Taneja, 2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5251</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5251</id><created>2011-11-22</created><authors><author><keyname>Fortuna</keyname><forenames>Miguel A.</forenames></author><author><keyname>Bonachela</keyname><forenames>Juan A.</forenames></author><author><keyname>Levin</keyname><forenames>Simon A.</forenames></author></authors><title>Evolution of a Modular Software Network</title><categories>cs.OS cs.SE</categories><comments>To appear in PNAS</comments><doi>10.1073/pnas.1115960108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Evolution behaves like a tinkerer&quot; (Francois Jacob, Science, 1977). Software
systems provide a unique opportunity to understand biological processes using
concepts from network theory. The Debian GNU/Linux operating system allows us
to explore the evolution of a complex network in a novel way. The modular
design detected during its growth is based on the reuse of existing code in
order to minimize costs during programming. The increase of modularity
experienced by the system over time has not counterbalanced the increase in
incompatibilities between software packages within modules. This negative
effect is far from being a failure of design. A random process of package
installation shows that the higher the modularity the larger the fraction of
packages working properly in a local computer. The decrease in the relative
number of conflicts between packages from different modules avoids a failure in
the functionality of one package spreading throughout the entire system. Some
potential analogies with the evolutionary and ecological processes determining
the structure of ecological networks of interacting species are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5272</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5272</id><created>2011-11-22</created><updated>2012-07-02</updated><authors><author><keyname>Ziniel</keyname><forenames>Justin</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Efficient High-Dimensional Inference in the Multiple Measurement Vector
  Problem</title><categories>cs.IT math.IT</categories><comments>28 pages, 9 figures</comments><journal-ref>IEEE Trans. Signal Processing, Vol. 61, No. 2, pp. 340-354, Jan.
  2013</journal-ref><doi>10.1109/TSP.2012.2222382</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a Bayesian approximate message passing algorithm is proposed
for solving the multiple measurement vector (MMV) problem in compressive
sensing, in which a collection of sparse signal vectors that share a common
support are recovered from undersampled noisy measurements. The algorithm,
AMP-MMV, is capable of exploiting temporal correlations in the amplitudes of
non-zero coefficients, and provides soft estimates of the signal vectors as
well as the underlying support. Central to the proposed approach is an
extension of recently developed approximate message passing techniques to the
amplitude-correlated MMV setting. Aided by these techniques, AMP-MMV offers a
computational complexity that is linear in all problem dimensions. In order to
allow for automatic parameter tuning, an expectation-maximization algorithm
that complements AMP-MMV is described. Finally, a detailed numerical study
demonstrates the power of the proposed approach and its particular suitability
for application to high-dimensional problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5279</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5279</id><created>2011-10-31</created><authors><author><keyname>Dossena</keyname><forenames>Marida</forenames></author></authors><title>Coverage Related Issues in Networks</title><categories>cs.NI</categories><comments>14 pages; 8 figures; Computer Networks 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks consisting of great number of cheap and tiny sensor
nodes which are used for military environment controlling, natural events
recording, traffic monitoring, robot navigation, and etc. Such a networks
encounter with various types of challenges like energy consumption, routing,
coverage, reliability. The most significant types of these problems are
coverage that originated from the nodes energy consumption constrained. In
order to dominate this problem different kinds of methods has been presented
where the majority of them based on theoretical methods and used unbalanced and
calculated distributions. In all of the proposed methods a large numbers of
nodes are used. In this paper our attempt is based on using a few numbers of
sensors in order to cover the vast area of environment. We proposed an
algorithm that divides the desired environment to several areas and in each of
these areas by using the genetic algorithm improve the coverage. The proposed
method is simulated in MATLAB software and the obtained results are compared
with the existing algorithms. Results show that the presented algorithm has a
substantial coverage in compare with its previous counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5280</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5280</id><created>2011-11-22</created><updated>2013-11-19</updated><authors><author><keyname>Bonnabel</keyname><forenames>Silvere</forenames></author></authors><title>Stochastic gradient descent on Riemannian manifolds</title><categories>math.OC cs.LG stat.ML</categories><comments>A slightly shorter version has been published in IEEE Transactions
  Automatic Control</comments><journal-ref>IEEE Transactions on Automatic Control, Vol 58 (9), pages 2217 -
  2229, Sept 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent is a simple approach to find the local minima of
a cost function whose evaluations are corrupted by noise. In this paper, we
develop a procedure extending stochastic gradient descent algorithms to the
case where the function is defined on a Riemannian manifold. We prove that, as
in the Euclidian case, the gradient descent algorithm converges to a critical
point of the cost function. The algorithm has numerous potential applications,
and is illustrated here by four examples. In particular a novel gossip
algorithm on the set of covariance matrices is derived and tested numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5285</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5285</id><created>2011-11-08</created><authors><author><keyname>George</keyname><forenames>Laurent</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Lotte</keyname><forenames>Fabien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Abad</keyname><forenames>Raquel Viciana</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>L&#xe9;cuyer</keyname><forenames>Anatole</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Using Scalp Electrical Biosignals to Control an Object by Concentration
  and Relaxation Tasks: Design and Evaluation</title><categories>cs.OH</categories><comments>International Conference of the IEEE EMBS (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the use of electrical biosignals measured on scalp
and corresponding to mental relaxation and concentration tasks in order to
control an object in a video game. To evaluate the requirements of such a
system in terms of sensors and signal processing we compare two designs. The
first one uses only one scalp electroencephalographic (EEG) electrode and the
power in the alpha frequency band. The second one uses sixteen scalp EEG
electrodes and machine learning methods. The role of muscular activity is also
evaluated using five electrodes positioned on the face and the neck. Results
show that the first design enabled 70% of the participants to successfully
control the game, whereas 100% of the participants managed to do it with the
second design based on machine learning. Subjective questionnaires confirm
these results: users globally felt to have control in both designs, with an
increased feeling of control in the second one. Offline analysis of face and
neck muscle activity shows that this activity could also be used to distinguish
between relaxation and concentration tasks. Results suggest that the
combination of muscular and brain activity could improve performance of this
kind of system. They also suggest that muscular activity has probably been
recorded by EEG electrodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5287</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5287</id><created>2011-11-22</created><authors><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>On the Gaussian Z-Interference Channel with Processing Energy Cost</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, presented in Asilomar Conference, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers a Gaussian interference channel with processing energy
cost, which explicitly takes into account the energy expended for processing
when each transmitter is on. With processing overhead, bursty transmission at
each transmitter generally becomes more advantageous. Assuming on-off states do
not carry information, for a two-user Z-interference channel, the new regime of
very strong interference is identified and shown to be enlarged compared with
the conventional one. With the interfered receiver listening when its own
transmitter is silent, for a wide range of cross-link power gains, one can
either achieve or get close to the interference-free upper bound on sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5288</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5288</id><created>2011-11-11</created><authors><author><keyname>Badr</keyname><forenames>Elmir</forenames></author><author><keyname>Bouchaib</keyname><forenames>Bounabat</forenames></author></authors><title>A Novel Approach for Periodic Assessment of Business Process
  Interoperability</title><categories>cs.OH</categories><comments>9 pages, 8 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, July 2011. ISSN (Online): 1694-0814, www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business collaboration networks provide collaborative organizations a
favorable context for automated business process interoperability. This paper
aims to present a novel approach for assessing interoperability of process
driven services by considering the three main aspects of interoperation:
potentiality, compatibility and operational performance. It presents also a
software tool that supports the proposed assessment method. In addition to its
capacity to track and control the evolution of interoperation degree in time,
the proposed tool measures the required effort to reach a planned degree of
interoperability. Public accounting of financial authority is given as an
illustrative case study of interoperability monitoring in public collaboration
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5293</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5293</id><created>2011-11-13</created><authors><author><keyname>Dwivedi</keyname><forenames>Sanjay K.</forenames></author><author><keyname>Sukhadeve</keyname><forenames>Pramod P.</forenames></author></authors><title>Rule based Part of speech Tagger for Homoeopathy Clinical realm</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A tagger is a mandatory segment of most text scrutiny systems, as it
consigned a s yntax class (e.g., noun, verb, adjective, and adverb) to every
word in a sentence. In this paper, we present a simple part of speech tagger
for homoeopathy clinical language. This paper reports about the anticipated
part of speech tagger for homoeopathy clinical language. It exploit standard
pattern for evaluating sentences, untagged clinical corpus of 20085 words is
used, from which we had selected 125 sentences (2322 tokens). The problem of
tagging in natural language processing is to find a way to tag every word in a
text as a meticulous part of speech. The basic idea is to apply a set of rules
on clinical sentences and on each word, Accuracy is the leading factor in
evaluating any POS tagger so the accuracy of proposed tagger is also conversed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5296</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5296</id><created>2011-11-20</created><updated>2011-12-29</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Abdi</keyname><forenames>Younes</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author></authors><title>Analytical and Learning-Based Spectrum Sensing Time Optimization in
  Cognitive Radio Systems</title><categories>cs.NI cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powerful spectrum sensing schemes enable cognitive radios (CRs) to find
transmission opportunities in spectral resources allocated exclusively to the
primary users. In this paper, maximizing the average throughput of a secondary
user by optimizing its spectrum sensing time is formulated assuming that a
prior knowledge of the presence and absence probabilities of the primary users
is available. The energy consumed for finding a transmission opportunity is
evaluated and a discussion on the impact of the number of the primary users on
the secondary user throughput and consumed energy is presented. In order to
avoid the challenges associated with the analytical method, as a second
solution, a systematic neural network-based sensing time optimization approach
is also proposed in this paper. The proposed adaptive scheme is able to find
the optimum value of the channel sensing time without any prior knowledge or
assumption about the wireless environment. The structure, performance, and
cooperation of the artificial neural networks used in the proposed method are
disclosed in detail and a set of illustrative simulation results is presented
to validate the analytical results as well as the performance of the proposed
learning-based optimization scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5305</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5305</id><created>2011-11-22</created><updated>2013-10-04</updated><authors><author><keyname>Yousefi</keyname><forenames>Arman</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>On a Linear Program for Minimum-Weight Triangulation</title><categories>cs.CG cs.DS</categories><comments>To appear in SICOMP. Extended abstract appeared in SODA 2012</comments><msc-class>68W25, 90C05</msc-class><acm-class>G.1.6; I.3.5</acm-class><journal-ref>SIAM Journal on Computing 43(1):25-51(2014)</journal-ref><doi>10.1137/120887928</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum-weight triangulation (MWT) is NP-hard. It has a polynomial-time
constant-factor approximation algorithm, and a variety of effective polynomial-
time heuristics that, for many instances, can find the exact MWT. Linear
programs (LPs) for MWT are well-studied, but previously no connection was known
between any LP and any approximation algorithm or heuristic for MWT. Here we
show the first such connections: for an LP formulation due to Dantzig et al.
(1985): (i) the integrality gap is bounded by a constant; (ii) given any
instance, if the aforementioned heuristics find the MWT, then so does the LP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5312</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5312</id><created>2011-11-22</created><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author></authors><title>Representations and Ensemble Methods for Dynamic Relational
  Classification</title><categories>cs.AI cs.SI physics.soc-ph stat.ML</categories><msc-class>68T01</msc-class><acm-class>I.2.6; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal networks are ubiquitous and evolve over time by the addition,
deletion, and changing of links, nodes, and attributes. Although many
relational datasets contain temporal information, the majority of existing
techniques in relational learning focus on static snapshots and ignore the
temporal dynamics. We propose a framework for discovering temporal
representations of relational data to increase the accuracy of statistical
relational learning algorithms. The temporal relational representations serve
as a basis for classification, ensembles, and pattern mining in evolving
domains. The framework includes (1) selecting the time-varying relational
components (links, attributes, nodes), (2) selecting the temporal granularity,
(3) predicting the temporal influence of each time-varying relational
component, and (4) choosing the weighted relational classifier. Additionally,
we propose temporal ensemble methods that exploit the temporal-dimension of
relational data. These ensembles outperform traditional and more sophisticated
relational ensembles while avoiding the issue of learning the most optimal
representation. Finally, the space of temporal-relational models are evaluated
using a sample of classifiers. In all cases, the proposed temporal-relational
classifiers outperform competing models that ignore the temporal information.
The results demonstrate the capability and necessity of the temporal-relational
representations for classification, ensembles, and for mining temporal
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5340</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5340</id><created>2011-11-22</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>On the Expected Complexity of Random Convex Hulls</title><categories>cs.CG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we present several results on the expected complexity of a
convex hull of $n$ points chosen uniformly and independently from a convex
shape.
  (i) We show that the expected number of vertices of the convex hull of $n$
points, chosen uniformly and independently from a disk is $O(n^{1/3})$, and
$O(k \log{n})$ for the case a convex polygon with $k$ sides. Those results are
well known (see \cite{rs-udkhv-63,r-slcdn-70,ps-cgi-85}), but we believe that
the elementary proof given here are simpler and more intuitive.
  (ii) Let $\D$ be a set of directions in the plane, we define a generalized
notion of convexity induced by $\D$, which extends both rectilinear convexity
and standard convexity.
  We prove that the expected complexity of the $\D$-convex hull of a set of $n$
points, chosen uniformly and independently from a disk, is $O(n^{1/3} +
\sqrt{n\alpha(\D)})$, where $\alpha(\D)$ is the largest angle between two
consecutive vectors in $\D$. This result extends the known bounds for the cases
of rectilinear and standard convexity.
  (iii) Let $\B$ be an axis parallel hypercube in $\Re^d$. We prove that the
expected number of points on the boundary of the quadrant hull of a set $S$ of
$n$ points, chosen uniformly and independently from $\B$ is $O(\log^{d-1}n)$.
Quadrant hull of a set of points is an extension of rectilinear convexity to
higher dimensions. In particular, this number is larger than the number of
maxima in $S$, and is also larger than the number of points of $S$ that are
vertices of the convex hull of $S$.
  Those bounds are known \cite{bkst-anmsv-78}, but we believe the new proof is
simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5348</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5348</id><created>2011-11-22</created><authors><author><keyname>Noon</keyname><forenames>Abbas</forenames></author><author><keyname>Kalakech</keyname><forenames>Ali</forenames></author><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author></authors><title>A New Round Robin Based Scheduling Algorithm for Operating Systems:
  Dynamic Quantum Using the Mean Average</title><categories>cs.OS</categories><comments>6 pages, 4 figures</comments><msc-class>68N25</msc-class><acm-class>B.1.5</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 1, 2011, 224-229</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Round Robin, considered as the most widely adopted CPU scheduling algorithm,
undergoes severe problems directly related to quantum size. If time quantum
chosen is too large, the response time of the processes is considered too high.
On the other hand, if this quantum is too small, it increases the overhead of
the CPU. In this paper, we propose a new algorithm, called AN, based on a new
approach called dynamic-time-quantum; the idea of this approach is to make the
operating systems adjusts the time quantum according to the burst time of the
set of waiting processes in the ready queue. Based on the simulations and
experiments, we show that the new proposed algorithm solves the fixed time
quantum problem and increases the performance of Round Robin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5357</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5357</id><created>2011-11-22</created><authors><author><keyname>Gruber</keyname><forenames>Hermann</forenames></author></authors><title>Digraph Complexity Measures and Applications in Formal Language Theory</title><categories>cs.DM cs.DS cs.FL</categories><comments>19 pages, 1 figure</comments><msc-class>05C20</msc-class><acm-class>G.2.2; F.2.2; F.4.3</acm-class><journal-ref>Discrete Mathematics &amp; Theoretical Computer Science,
  14(2):189-204, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate structural complexity measures on digraphs, in particular the
cycle rank. This concept is intimately related to a classical topic in formal
language theory, namely the star height of regular languages. We explore this
connection, and obtain several new algorithmic insights regarding both cycle
rank and star height. Among other results, we show that computing the cycle
rank is NP-complete, even for sparse digraphs of maximum outdegree 2.
Notwithstanding, we provide both a polynomial-time approximation algorithm and
an exponential-time exact algorithm for this problem. The former algorithm
yields an O((log n)^(3/2))- approximation in polynomial time, whereas the
latter yields the optimum solution, and runs in time and space O*(1.9129^n) on
digraphs of maximum outdegree at most two. Regarding the star height problem,
we identify a subclass of the regular languages for which we can precisely
determine the computational complexity of the star height problem. Namely, the
star height problem for bideterministic languages is NP-complete, and this
holds already for binary alphabets. Then we translate the algorithmic results
concerning cycle rank to the bideterministic star height problem, thus giving a
polynomial-time approximation as well as a reasonably fast exact exponential
algorithm for bideterministic star height.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5358</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5358</id><created>2011-11-22</created><updated>2012-09-05</updated><authors><author><keyname>Anand</keyname><forenames>Abhishek</forenames></author><author><keyname>Koppula</keyname><forenames>Hema Swetha</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Contextually Guided Semantic Labeling and Search for 3D Point Clouds</title><categories>cs.RO cs.AI cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1106.5551</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RGB-D cameras, which give an RGB image to- gether with depths, are becoming
increasingly popular for robotic perception. In this paper, we address the task
of detecting commonly found objects in the 3D point cloud of indoor scenes
obtained from such cameras. Our method uses a graphical model that captures
various features and contextual relations, including the local visual
appearance and shape cues, object co-occurence relationships and geometric
relationships. With a large number of object classes and relations, the model's
parsimony becomes important and we address that by using multiple types of edge
potentials. We train the model using a maximum-margin learning approach. In our
experiments over a total of 52 3D scenes of homes and offices (composed from
about 550 views), we get a performance of 84.06% and 73.38% in labeling office
and home scenes respectively for 17 object classes each. We also present a
method for a robot to search for an object using the learned model and the
contextual information available from the current labelings of the scene. We
applied this algorithm successfully on a mobile robot for the task of finding
12 object classes in 10 different offices and achieved a precision of 97.56%
with 78.43% recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5377</identifier>
 <datestamp>2011-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5377</id><created>2011-11-22</created><updated>2011-12-15</updated><authors><author><keyname>Jahid</keyname><forenames>Sonia</forenames></author><author><keyname>Nilizadeh</keyname><forenames>Shirin</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author><author><keyname>Kapadia</keyname><forenames>Apu</forenames></author></authors><title>DECENT: A Decentralized Architecture for Enforcing Privacy in Online
  Social Networks</title><categories>cs.CR cs.NI cs.SI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multitude of privacy breaches, both accidental and malicious, have prompted
users to distrust centralized providers of online social networks (OSNs) and
investigate decentralized solutions. We examine the design of a fully
decentralized (peer-to-peer) OSN, with a special focus on privacy and security.
In particular, we wish to protect the confidentiality, integrity, and
availability of user content and the privacy of user relationships. We propose
DECENT, an architecture for OSNs that uses a distributed hash table to store
user data, and features cryptographic protections for confidentiality and
integrity, as well as support for flexible attribute policies and fast
revocation. DECENT ensures that neither data nor social relationships are
visible to unauthorized users and provides availability through replication and
authentication of updates. We evaluate DECENT through simulation and
experiments on the PlanetLab network and show that DECENT is able to replicate
the main functionality of current centralized OSNs with manageable overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5382</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5382</id><created>2011-11-22</created><authors><author><keyname>Ercsey-Ravasz</keyname><forenames>Maria</forenames></author><author><keyname>Lichtenwalter</keyname><forenames>Ryan</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zoltan</forenames></author></authors><title>Range-limited Centrality Measures in Complex Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.DS cs.SI</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we present a range-limited approach to centrality measures in both
non-weighted and weighted directed complex networks. We introduce an efficient
method that generates for every node and every edge its betweenness centrality
based on shortest paths of lengths not longer than $\ell = 1,...,L$ in case of
non-weighted networks, and for weighted networks the corresponding quantities
based on minimum weight paths with path weights not larger than $w_{\ell}=\ell
\Delta$, $\ell=1,2...,L=R/\Delta$. These measures provide a systematic
description on the positioning importance of a node (edge) with respect to its
network neighborhoods 1-step out, 2-steps out, etc. up to including the whole
network. We show that range-limited centralities obey universal scaling laws
for large non-weighted networks. As the computation of traditional centrality
measures is costly, this scaling behavior can be exploited to efficiently
estimate centralities of nodes and edges for all ranges, including the
traditional ones. The scaling behavior can also be exploited to show that the
ranking top-list of nodes (edges) based on their range-limited centralities
quickly freezes as function of the range, and hence the diameter-range top-list
can be efficiently predicted. We also show how to estimate the typical largest
node-to-node distance for a network of $N$ nodes, exploiting the aforementioned
scaling behavior. These observations are illustrated on model networks and on a
large social network inferred from cell-phone trace logs ($\sim 5.5\times 10^6$
nodes and $\sim 2.7\times 10^7$ edges). Finally, we apply these concepts to
efficiently detect the vulnerability backbone of a network (defined as the
smallest percolating cluster of the highest betweenness nodes and edges) and
illustrate the importance of weight-based centrality measures in weighted
networks in detecting such backbones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5386</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5386</id><created>2011-11-22</created><authors><author><keyname>Chan</keyname><forenames>Ho-Leung</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Lee</keyname><forenames>Lap-Kei</forenames></author><author><keyname>Pan</keyname><forenames>Jiangwei</forenames></author><author><keyname>Ting</keyname><forenames>Hing-Fung</forenames></author><author><keyname>Zhang</keyname><forenames>Qin</forenames></author></authors><title>Edit Distance to Monotonicity in Sliding Windows</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a stream of items each associated with a numerical value, its edit
distance to monotonicity is the minimum number of items to remove so that the
remaining items are non-decreasing with respect to the numerical value. The
space complexity of estimating the edit distance to monotonicity of a data
stream is becoming well-understood over the past few years. Motivated by
applications on network quality monitoring, we extend the study to estimating
the edit distance to monotonicity of a sliding window covering the $w$ most
recent items in the stream for any $w \ge 1$. We give a deterministic algorithm
which can return an estimate within a factor of $(4+\eps)$ using
$O(\frac{1}{\eps^2} \log^2(\eps w))$ space.
  We also extend the study in two directions. First, we consider a stream where
each item is associated with a value from a partial ordered set. We give a
randomized $(4+\epsilon)$-approximate algorithm using $O(\frac{1}{\epsilon^2}
\log \epsilon^2 w \log w)$ space. Second, we consider an out-of-order stream
where each item is associated with a creation time and a numerical value, and
items may be out of order with respect to their creation times. The goal is to
estimate the edit distance to monotonicity with respect to the numerical value
of items arranged in the order of creation times. We show that any randomized
constant-approximate algorithm requires linear space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5391</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5391</id><created>2011-11-22</created><authors><author><keyname>Dong</keyname><forenames>Qiang</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Xiaofan</forenames></author></authors><title>Hamiltonian Connectivity of Twisted Hypercube-Like Networks under the
  Large Fault Model</title><categories>cs.DC</categories><comments>22 pages, 26 figures, Submitted to IEEE TPDS</comments><msc-class>68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twisted hypercube-like networks (THLNs) are an important class of
interconnection networks for parallel computing systems, which include most
popular variants of the hypercubes, such as crossed cubes, M\&quot;obius cubes,
twisted cubes and locally twisted cubes. This paper deals with the
fault-tolerant hamiltonian connectivity of THLNs under the large fault model.
Let $G$ be an $n$-dimensional THLN and $F \subseteq V(G)\bigcup E(G)$, where $n
\geq 7$ and $|F| \leq 2n - 10$. We prove that for any two nodes $u,v \in V(G -
F)$ satisfying a simple necessary condition on neighbors of $u$ and $v$, there
exists a hamiltonian or near-hamiltonian path between $u$ and $v$ in $G-F$. The
result extends further the fault-tolerant graph embedding capability of THLNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5395</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5395</id><created>2011-11-22</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>A graph theoretical Gauss-Bonnet-Chern Theorem</title><categories>math.DG cs.CG cs.DM</categories><comments>26 pages, 70 figures</comments><msc-class>05C10, 57M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a discrete Gauss-Bonnet-Chern theorem which states where summing the
curvature over all vertices of a finite graph G=(V,E) gives the Euler
characteristic of G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5412</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5412</id><created>2011-11-23</created><authors><author><keyname>Feder</keyname><forenames>Elie</forenames></author><author><keyname>Garber</keyname><forenames>David</forenames></author></authors><title>On the Orchard crossing number of prisms, ladders and other related
  graphs</title><categories>math.CO cs.CG cs.DM</categories><comments>17 pages, 14 figures; submitted</comments><msc-class>05C62, 68R10 (Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the Orchard crossing number of some families of graphs
which are based on cycles. These include disjoint cycles, cycles which share a
vertex and cycles which share an edge. Specifically, we focus on the prism and
ladder graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5414</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5414</id><created>2011-11-23</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Randomized Speedup of the Bellman-Ford Algorithm</title><categories>cs.DS</categories><comments>12 Pages, 6 Figures, ANALCO 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a variant of the Bellman-Ford algorithm for single-source
shortest paths in graphs with negative edges but no negative cycles that
randomly permutes the vertices and uses this randomized order to process the
vertices within each pass of the algorithm. The modification reduces the
worst-case expected number of relaxation steps of the algorithm, compared to
the previously-best variant by Yen (1970), by a factor of 2/3 with high
probability. We also use our high probability bound to add negative cycle
detection to the randomized algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5417</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5417</id><created>2011-11-23</created><authors><author><keyname>Hu</keyname><forenames>Haibo</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofan</forenames></author></authors><title>How people make friends in social networking sites - A microscopic
  perspective</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 12 figures, 2 tables</comments><doi>10.1016/j.physa.2011.10.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the detailed growth of a social networking site with full temporal
information by examining the creation process of each friendship relation that
can collectively lead to the macroscopic properties of the network. We first
study the reciprocal behavior of users, and find that link requests are quickly
responded to and that the distribution of reciprocation intervals decays in an
exponential form. The degrees of inviters/accepters are slightly negatively
correlative with reciprocation time. In addition, the temporal feature of the
online community shows that the distributions of intervals of user behaviors,
such as sending or accepting link requests, follow a power law with a universal
exponent, and peaks emerge for intervals of an integral day. We finally study
the preferential selection and linking phenomena of the social networking site
and find that, for the former, a linear preference holds for preferential
sending and reception, and for the latter, a linear preference also holds for
preferential acceptance, creation, and attachment. Based on the linearly
preferential linking, we put forward an analyzable network model which can
reproduce the degree distribution of the network. The research framework
presented in the paper could provide a potential insight into how the
micro-motives of users lead to the global structure of online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5425</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5425</id><created>2011-11-23</created><authors><author><keyname>Wolf</keyname><forenames>Michael M.</forenames></author><author><keyname>Cubitt</keyname><forenames>Toby S.</forenames></author><author><keyname>Perez-Garcia</keyname><forenames>David</forenames></author></authors><title>Are problems in Quantum Information Theory (un)decidable?</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note is intended to foster a discussion about the extent to which
typical problems arising in quantum information theory are algorithmically
decidable (in principle rather than in practice). Various problems in the
context of entanglement theory and quantum channels turn out to be decidable
via quantifier elimination as long as they admit a compact formulation without
quantification over integers. For many asymptotically defined properties which
have to hold for all or for one integer N, however, effective procedures seem
to be difficult if not impossible to find. We review some of the main tools for
(dis)proving decidability and apply them to problems in quantum information
theory. We find that questions like &quot;can we overcome fidelity 1/2 w.r.t. a
two-qubit singlet state?&quot; easily become undecidable. A closer look at such
questions might rule out some of the &quot;single-letter&quot; formulas sought in quantum
information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5442</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5442</id><created>2011-11-23</created><updated>2012-08-27</updated><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Schmied</keyname><forenames>Richard</forenames></author></authors><title>Improved Lower Bounds for the Shortest Superstring and Related Problems</title><categories>cs.CC cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximation hardness of the Shortest Superstring, the Maximal
Compression and the Maximum Asymmetric Traveling Salesperson (MAX-ATSP)
problem. We introduce a new reduction method that produces strongly restricted
instances of the Shortest Superstring problem, in which the maximal orbit size
is eight (with no character appearing more than eight times) and all given
strings having length four. Based on this reduction method, we are able to
improve the best up to now known approximation lower bound for the Shortest
Superstring problem and the Maximal Compression problem by an order of
magnitude. The results imply also an improved approximation lower bound for the
MAX-ATSP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5454</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5454</id><created>2011-11-23</created><authors><author><keyname>Rooksby</keyname><forenames>John</forenames></author><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author></authors><title>The Management and Use of Social Network Sites in a Government
  Department</title><categories>cs.SI cs.SE</categories><comments>Accepted for publication in JCSCW (The Journal of Computer Supported
  Cooperative Work)</comments><journal-ref>Computer Supported Cooperative Work 21(4-5): 397-415 (2012)</journal-ref><doi>10.1007/s10606-011-9150-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we report findings from a study of social network site use in a
UK Government department. We have investigated this from a managerial,
organisational perspective. We found at the study site that there are already
several social network technologies in use, and that these: misalign with and
problematize organisational boundaries; blur boundaries between working and
social lives; present differing opportunities for control; have different
visibilities; have overlapping functionality with each other and with other
information technologies; that they evolve and change over time; and that their
uptake is conditioned by existing infrastructure and availability. We find the
organisational complexity that social technologies are often hoped to cut
across is, in reality, something that shapes their uptake and use. We argue the
idea of a single, central social network site for supporting cooperative work
within an organisation will hit the same problems as any effort of
centralisation in organisations. We argue that while there is still plenty of
scope for design and innovation in this area, an important challenge now is in
supporting organisations in managing what can best be referred to as a social
network site 'ecosystem'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5458</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5458</id><created>2011-11-23</created><updated>2012-12-06</updated><authors><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Zhu</keyname><forenames>Xuding</forenames></author></authors><title>Towards on-line Ohba's conjecture</title><categories>math.CO cs.DM</categories><comments>new abstract and introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The on-line choice number of a graph is a variation of the choice number
defined through a two person game. It is at least as large as the choice number
for all graphs and is strictly larger for some graphs. In particular, there are
graphs $G$ with $|V(G)| = 2 \chi(G)+1$ whose on-line choice numbers are larger
than their chromatic numbers, in contrast to a recently confirmed conjecture of
Ohba that every graph $G$ with $|V(G)| \le 2 \chi(G)+1$ has its choice number
equal its chromatic number. Nevertheless, an on-line version of Ohba conjecture
was proposed in [P. Huang, T. Wong and X. Zhu, Application of polynomial method
to on-line colouring of graphs, European J. Combin., 2011]: Every graph $G$
with $|V(G)| \le 2 \chi(G)$ has its on-line choice number equal its chromatic
number. This paper confirms the on-line version of Ohba conjecture for graphs
$G$ with independence number at most 3. We also study list colouring of
complete multipartite graphs $K_{3\star k}$ with all parts of size 3. We prove
that the on-line choice number of $K_{3 \star k}$ is at most $3/2k$, and
present an alternate proof of Kierstead's result that its choice number is
$\lceil (4k-1)/3 \rceil$. For general graphs $G$, we prove that if $|V(G)| \le
\chi(G)+\sqrt{\chi(G)}$ then its on-line choice number equals chromatic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5467</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5467</id><created>2011-11-23</created><authors><author><keyname>Carpi</keyname><forenames>Arturo</forenames></author><author><keyname>D'Alessandro</keyname><forenames>Flavio</forenames></author></authors><title>Independent sets of words and the synchronization problem</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synchronization problem is investigated for the class of locally strongly
transitive automata introduced in a previous work of the authors. Some
extensions of this problem related to the notions of stable set and word of
minimal rank of an automaton are studied. An application to synchronizing
colorings of aperiodic graphs with a Hamiltonian path is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5472</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5472</id><created>2011-11-23</created><updated>2012-11-13</updated><authors><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Chong</keyname><forenames>Stephen</forenames></author><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author><author><keyname>Moran</keyname><forenames>Tal</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Truthful Mechanisms for Agents that Value Privacy</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has constructed economic mechanisms that are both truthful and
differentially private. In these mechanisms, privacy is treated separately from
the truthfulness; it is not incorporated in players' utility functions (and
doing so has been shown to lead to non-truthfulness in some cases). In this
work, we propose a new, general way of modelling privacy in players' utility
functions. Specifically, we only assume that if an outcome $o$ has the property
that any report of player $i$ would have led to $o$ with approximately the same
probability, then $o$ has small privacy cost to player $i$. We give three
mechanisms that are truthful with respect to our modelling of privacy: for an
election between two candidates, for a discrete version of the facility
location problem, and for a general social choice problem with discrete
utilities (via a VCG-like mechanism). As the number $n$ of players increases,
the social welfare achieved by our mechanisms approaches optimal (as a fraction
of $n$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5473</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5473</id><created>2011-11-23</created><updated>2012-06-11</updated><authors><author><keyname>Rothvo&#xdf;</keyname><forenames>Thomas</forenames></author></authors><title>Directed Steiner Tree and the Lasserre Hierarchy</title><categories>cs.DS</categories><comments>23 pages, 1 figure</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal for the Directed Steiner Tree problem is to find a minimum cost tree
in a directed graph G=(V,E) that connects all terminals X to a given root r. It
is well known that modulo a logarithmic factor it suffices to consider acyclic
graphs where the nodes are arranged in L &lt;= log |X| levels. Unfortunately the
natural LP formulation has a |X|^(1/2) integrality gap already for 5 levels. We
show that for every L, the O(L)-round Lasserre Strengthening of this LP has
integrality gap O(L log |X|). This provides a polynomial time
|X|^{epsilon}-approximation and a O(log^3 |X|) approximation in O(n^{log |X|)
time, matching the best known approximation guarantee obtained by a greedy
algorithm of Charikar et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5479</identifier>
 <datestamp>2012-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5479</id><created>2011-11-23</created><updated>2012-08-07</updated><authors><author><keyname>Mazumder</keyname><forenames>Rahul</forenames></author><author><keyname>Hastie</keyname><forenames>Trevor</forenames></author></authors><title>The Graphical Lasso: New Insights and Alternatives</title><categories>stat.ML cs.LG</categories><comments>This is a revised version of our previous manuscript with the same
  name ArXiv id: http://arxiv.org/abs/1111.5479</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graphical lasso \citep{FHT2007a} is an algorithm for learning the
structure in an undirected Gaussian graphical model, using $\ell_1$
regularization to control the number of zeros in the precision matrix
${\B\Theta}={\B\Sigma}^{-1}$ \citep{BGA2008,yuan_lin_07}. The {\texttt R}
package \GL\ \citep{FHT2007a} is popular, fast, and allows one to efficiently
build a path of models for different values of the tuning parameter.
Convergence of \GL\ can be tricky; the converged precision matrix might not be
the inverse of the estimated covariance, and occasionally it fails to converge
with warm starts. In this paper we explain this behavior, and propose new
algorithms that appear to outperform \GL.
  By studying the &quot;normal equations&quot; we see that, \GL\ is solving the {\em
dual} of the graphical lasso penalized likelihood, by block coordinate ascent;
a result which can also be found in \cite{BGA2008}.
  In this dual, the target of estimation is $\B\Sigma$, the covariance matrix,
rather than the precision matrix $\B\Theta$. We propose similar primal
algorithms \PGL\ and \DPGL, that also operate by block-coordinate descent,
where $\B\Theta$ is the optimization target. We study all of these algorithms,
and in particular different approaches to solving their coordinate
sub-problems. We conclude that \DPGL\ is superior from several points of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5483</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5483</id><created>2011-11-23</created><updated>2013-09-30</updated><authors><author><keyname>Quax</keyname><forenames>Rick</forenames></author><author><keyname>Apolloni</keyname><forenames>Andrea</forenames></author><author><keyname>Sloot</keyname><forenames>Peter M. A.</forenames></author></authors><title>The diminishing role of hubs in dynamical processes on complex networks</title><categories>cs.IT math.IT nlin.AO physics.soc-ph</categories><comments>Published version</comments><acm-class>F.1.1</acm-class><journal-ref>Journal of the Royal Society Interface, vol. 10, nr 88 2013. ISSN:
  1742-5662</journal-ref><doi>10.1098/rsif.2013.0568</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is notoriously difficult to predict the behaviour of a complex
self-organizing system, where the interactions among dynamical units form a
heterogeneous topology. Even if the dynamics of each microscopic unit is known,
a real understanding of their contributions to the macroscopic system behaviour
is still lacking. Here we develop information-theoretical methods to
distinguish the contribution of each individual unit to the collective
out-of-equilibrium dynamics. We show that for a system of units connected by a
network of interaction potentials with an arbitrary degree distribution, highly
connected units have less impact on the system dynamics as compared to
intermediately connected units. In an equilibrium setting, the hubs are often
found to dictate the long-term behaviour. However, we find both analytically
and experimentally that the instantaneous states of these units have a
short-lasting effect on the state trajectory of the entire system. We present
qualitative evidence of this phenomenon from empirical findings about a social
network of product recommendations, a protein-protein interaction network, and
a neural network, suggesting that it might indeed be a widespread property in
nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5484</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5484</id><created>2011-11-23</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author><author><keyname>Kl&#xf8;ve</keyname><forenames>Torleiv</forenames></author></authors><title>A class of punctured simplex codes which are proper for error detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary linear [n,k] codes that are proper for error detection are known for
many combinations of n and k. For the remaining combinations, existence of
proper codes is conjectured. In this paper, a particular class of [n,k] codes
is studied in detail. In particular, it is shown that these codes are proper
for many combinations of n and k which were previously unsettled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5485</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5485</id><created>2011-11-23</created><updated>2012-08-30</updated><authors><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Membership(s) and compliance(s) with class-based graphs</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 4 figures</comments><journal-ref>Information Processing Letters. 112 (2012) 849-855</journal-ref><doi>10.1016/j.ipl.2012.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides the need for a better understanding of networks, there is a need for
prescriptive models and tools to specify requirements concerning networks and
their associated graph representations. We propose class-based graphs as a
means to specify requirements concerning object-based graphs. Various variants
of membership are proposed as special relations between class-based and
object-based graphs at the local level, while various variants of compliance
are proposed at the global level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5493</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5493</id><created>2011-11-23</created><updated>2012-07-12</updated><authors><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>A Formalization of Social Requirements for Human Interactions with
  Service Protocols</title><categories>cs.SI cs.SE</categories><comments>36 pages, 9 figures</comments><doi>10.1016/j.ins.2013.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaboration models and tools aim at improving the efficiency and
effectiveness of human interactions. Although social relations among
collaborators have been identified as having a strong influence on
collaboration, they are still insufficiently taken into account in current
collaboration models and tools. In this paper, the concept of service protocols
is proposed as a model for human interactions supporting social requirements,
i.e., sets of constraints on the relations among interacting humans. Service
protocols have been proposed as an answer to the need for models for human
interactions in which not only the potential sequences of activities are
specified-as in process models-but also the constraints on the relations among
collaborators. Service protocols are based on two main ideas: first, service
protocols are rooted in the service-oriented architecture (SOA): each service
protocol contains a service-oriented summary which provides a representation of
the activities of an associated process model in SOA terms. Second, a
class-based graph-referred to as a service network schema-restricts the set of
potential service elements that may participate in the service protocol by
defining constraints on nodes and constraints on arcs, i.e., social
requirements. Another major contribution to the modelling of human interactions
is a unified approach organized around the concept of service, understood in a
broad sense with services being not only Web services, but also provided by
humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5500</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5500</id><created>2011-11-15</created><authors><author><keyname>Sabino</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Rodrigues</keyname><forenames>Armanda</forenames></author></authors><title>Understanding the Role of Cooperation in Emergency Plan Construction</title><categories>cs.HC</categories><comments>5 pages, 1 figure, 1 table, The 8th International Conference on
  Information Systems for Crisis Response and Management -- ISCRAM 2011, held
  in Lisbon, Portugal, May 8-11, 2011</comments><acm-class>H.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a proposal for information organization for
computer supported cooperative work, while working with spatial information. It
is focused on emergency response plan construction, and the requirements
extracted from that task. At the centre of our proposal is the analysis of the
structure of the cooperative workspace. We argue that the internal information
representation should follow a spatial approach, tying the structure used to
manage users with the structure used to manage information, suggesting the use
of different spaces to represent the information. The gain we expect from this
approach is the improved capacity to extract information on how people are
cooperating and their relationship with the information they are working with.
The ideas are introduced while focusing on real life emergency planning
activities, where we discuss the current shortcomings of the cooperation
strategies in use and propose a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5502</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5502</id><created>2011-11-23</created><authors><author><keyname>Paszkiewicz</keyname><forenames>Zbigniew</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Modelling Competences for Partner Selection in Service-Oriented Virtual
  Organization Breeding Environments</title><categories>cs.SE cs.SI</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of globalization and dynamic markets, collaboration among
organizations is a condition sine qua non for organizations, especially small
and medium enterprises, to remain competitive. Virtual organizations have been
proposed as an organizational structure adapted to collaboration among
organizations. The concept of Virtual Organization Breeding Environment (VOBE)
has been proposed as a means to support the creation and operation of virtual
organizations. With the rise of the service-oriented architecture (SOA), the
concept of service-oriented VOBE (SOVOBE) has been proposed as a VOBE
systematically organized around the concept of services. In the context of
SOVOBEs, novel competence models supporting both service orientation and
collaboration among organizations have to be developed to support efficiently
partner selection, a key aspect of VO creation. In this paper, such a
competence model is presented. Our competence model consists of a competence
description model, a competence verification method, and a competence search
method. The competence description model is an information model to describe
organizations, their competences, and services they provides. The competence
verification method enables the verification of the reliance and relevance of
competence descriptions. The competence search method allows a VO planner to
select appropriate partners based on VO specifications, encompassing competence
requirements. Finally, implementation concerns based on the development of the
prototype ErGo system are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5518</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5518</id><created>2011-11-23</created><authors><author><keyname>Ismail</keyname><forenames>Anis</forenames></author><author><keyname>Quafafou</keyname><forenames>Mohamed</forenames></author><author><keyname>Nachouki</keyname><forenames>Gilles</forenames></author><author><keyname>Hajjar</keyname><forenames>Mohammad</forenames></author></authors><title>Efficient Super-Peer-Based Queries Routing: Simulation and Evaluation</title><categories>cs.DB cs.NI</categories><comments>Journal of Emerging Technologies in Web Intelligence, Vol 3, No 3
  (2011), 206-216, Aug 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of
internet traffic. P2P systems have emerged as a popular way to share huge
volumes of data. Requirements for widely distributed information systems
supporting virtual organizations have given rise to a new category of P2P
systems called schema- based. In such systems each peer is a database
management system in itself, ex-posing its own schema. A fundamental problem
that confronts peer-to-peer applications is the efficient location of the node
that stores a desired data item. In such settings, the main objective is the
efficient search across peer databases by processing each incoming query
without overly consuming bandwidth. The usability of these systems depends on
effective techniques to find and retrieve data; however, efficient and
effective routing of content- based queries is an emerging problem in P2P
networks. In this paper, we propose an architecture, based on super-peers, and
we focus on query routing. Our approach considers that super-Peers having
similar interests are grouped together for an efficient query routing method.
In such groups, called Knowledge-Super-Peers (KSP), super-peers submit queries
that are often processed by members of this group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5528</identifier>
 <datestamp>2012-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5528</id><created>2011-11-23</created><updated>2012-08-02</updated><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Benoit</keyname><forenames>Anne</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author></authors><title>Energy-aware scheduling under reliability and makespan constraints</title><categories>cs.DS cs.DC</categories><comments>22 pages. A 10 pages version should appear in HiPC'12</comments><report-no>Inria Research Report 7757</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a task graph mapped on a set of homogeneous processors. We aim at
minimizing the energy consumption while enforcing two constraints: a prescribed
bound on the execution time (or makespan), and a reliability threshold. Dynamic
voltage and frequency scaling (DVFS) is an approach frequently used to reduce
the energy consumption of a schedule, but slowing down the execution of a task
to save energy is decreasing the reliability of the execution. In this work, to
improve the reliability of a schedule while reducing the energy consumption, we
allow for the re-execution of some tasks. We assess the complexity of the
tri-criteria scheduling problem (makespan, reliability, energy) of deciding
which task to re-execute, and at which speed each execution of a task should be
done, with two different speed models: either processors can have arbitrary
speeds (continuous model), or a processor can run at a finite number of
different speeds and change its speed during a computation (VDD model). We
propose several novel tri-criteria scheduling heuristics under the continuous
speed model, and we evaluate them through a set of simulations. The two best
heuristics turn out to be very efficient and complementary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5534</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5534</id><created>2011-11-23</created><authors><author><keyname>Gallos</keyname><forenames>Lazaros K.</forenames></author><author><keyname>Rybski</keyname><forenames>Diego</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>How people interact in evolving online affiliation networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 8 figures</comments><journal-ref>Phys. Rev. X 2, 031014 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of human interactions is of central importance for understanding
the behavior of individuals, groups and societies. Here, we observe the
formation and evolution of networks by monitoring the addition of all new links
and we analyze quantitatively the tendencies used to create ties in these
evolving online affiliation networks. We first show that an accurate estimation
of these probabilistic tendencies can only be achieved by following the time
evolution of the network. For example, actions that are attributed to the usual
friend of a friend mechanism through a static snapshot of the network are
overestimated by a factor of two. A detailed analysis of the dynamic network
evolution shows that half of those triangles were generated through other
mechanisms, in spite of the characteristic static pattern. We start by
characterizing every single link when the tie was established in the network.
This allows us to describe the probabilistic tendencies of tie formation and
extract sociological conclusions as follows. The tendencies to add new links
differ significantly from what we would expect if they were not affected by the
individuals' structural position in the network, i.e., from random link
formation. We also find significant differences in behavioral traits among
individuals according to their degree of activity, gender, age, popularity and
other attributes. For instance, in the particular datasets analyzed here, we
find that women reciprocate connections three times as much as men and this
difference increases with age. Men tend to connect with the most popular people
more often than women across all ages. On the other hand, triangular ties
tendencies are similar and independent of gender. Our findings can be useful to
build models of realistic social network structures and discover the underlying
laws that govern establishment of ties in evolving social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5548</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5548</id><created>2011-11-23</created><authors><author><keyname>Tasi&#x107;</keyname><forenames>Milan B.</forenames></author><author><keyname>Stanimirovi&#x107;</keyname><forenames>Predrag S.</forenames></author><author><keyname>Pepi&#x107;</keyname><forenames>Selver H.</forenames></author></authors><title>Computation of generalized inverses using Php/MySql environment</title><categories>cs.DB cs.DS math.NA</categories><comments>International Journal of Computer Mathematics, Volume 88, Issue 11,
  2011</comments><msc-class>15A09, 68P15, 68N15</msc-class><doi>10.1080/00207160.2010.541453</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The main aim of this paper is to develop a client/server-based model for
computing the weighted Moore-Penrose inverse using the partitioning method as
well as for storage of generated results. The web application is developed in
the PHP/MySQL environment. The source code is open and free for testing by
using a web browser. Influence of different matrix representations and storage
systems on the computational time is investigated. The CPU time for searching
the previously stored pseudo-inverses is compared with the CPU time spent for
new computation of the same inverses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5572</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5572</id><created>2011-11-23</created><authors><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author><author><keyname>Bolosky</keyname><forenames>William J.</forenames></author><author><keyname>Curtis</keyname><forenames>Kristal</forenames></author><author><keyname>Fox</keyname><forenames>Armando</forenames></author><author><keyname>Patterson</keyname><forenames>David</forenames></author><author><keyname>Shenker</keyname><forenames>Scott</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author><author><keyname>Karp</keyname><forenames>Richard M.</forenames></author><author><keyname>Sittler</keyname><forenames>Taylor</forenames></author></authors><title>Faster and More Accurate Sequence Alignment with SNAP</title><categories>cs.DS q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Scalable Nucleotide Alignment Program (SNAP), a new short and
long read aligner that is both more accurate (i.e., aligns more reads with
fewer errors) and 10-100x faster than state-of-the-art tools such as BWA.
Unlike recent aligners based on the Burrows-Wheeler transform, SNAP uses a
simple hash index of short seed sequences from the genome, similar to BLAST's.
However, SNAP greatly reduces the number and cost of local alignment checks
performed through several measures: it uses longer seeds to reduce the false
positive locations considered, leverages larger memory capacities to speed
index lookup, and excludes most candidate locations without fully computing
their edit distance to the read. The result is an algorithm that scales well
for reads from one hundred to thousands of bases long and provides a rich error
model that can match classes of mutations (e.g., longer indels) that today's
fast aligners ignore. We calculate that SNAP can align a dataset with 30x
coverage of a human genome in less than an hour for a cost of $2 on Amazon EC2,
with higher accuracy than BWA. Finally, we describe ongoing work to further
improve SNAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5593</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5593</id><created>2011-11-23</created><authors><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Agile Professional Virtual Community Inheritance via Adaptation of
  Social Protocols</title><categories>cs.CY cs.HC</categories><comments>15 pages, 3 figures</comments><journal-ref>International Journal of Services and Operations Management 2010 -
  Vol. 6, No.3 pp. 362 - 376</journal-ref><doi>10.1504/IJSOM.2010.031959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support for human-to-human interactions over a network is still insufficient,
particularly for professional virtual communities (PVC). Among other
limitations, adaptation and learning-by-experience capabilities of humans are
not taken into account in existing models for collaboration processes in PVC.
This paper presents a model for adaptive human collaboration. A key element of
this model is the use of negotiation for adaptation of social protocols
modelling processes. A second contribution is the proposition of various
adaptation propagation strategies as means for continuous management of the PVC
inheritance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5595</identifier>
 <datestamp>2011-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5595</id><created>2011-11-23</created><updated>2011-12-22</updated><authors><author><keyname>Gonzalez-Bailon</keyname><forenames>Sandra</forenames></author><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Rivero</keyname><forenames>Alejandro</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>The Dynamics of Protest Recruitment through an Online Network</title><categories>physics.soc-ph cs.SI</categories><comments>Main text: 20 pages, 4 figures; Supplementary Information: 17 pages,
  5 figures, 3 tables</comments><journal-ref>S. Gonz\'alez-Bail\'on, J. Borge-Holthoefer, A. Rivero and Y.
  Moreno. The Dynamics of Protest Recruitment through an Online Network.
  Scientific Reports, 1, 197 (2011)</journal-ref><doi>10.1038/srep00197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent wave of mobilizations in the Arab world and across Western
countries has generated much discussion on how digital media is connected to
the diffusion of protests. We examine that connection using data from the surge
of mobilizations that took place in Spain in May 2011. We study recruitment
patterns in the Twitter network and find evidence of social influence and
complex contagion. We identify the network position of early participants (i.e.
the leaders of the recruitment process) and of the users who acted as seeds of
message cascades (i.e. the spreaders of information). We find that early
participants cannot be characterized by a typical topological position but
spreaders tend to me more central to the network. These findings shed light on
the connection between online networks, social contagion, and collective
dynamics, and offer an empirical test to the recruitment mechanisms theorized
in formal models of collective action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5596</identifier>
 <datestamp>2011-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5596</id><created>2011-11-23</created><authors><author><keyname>Winter</keyname><forenames>Frank</forenames></author></authors><title>Accelerating QDP++/Chroma on GPUs</title><categories>hep-lat cs.DC</categories><comments>7 pages, 4 figures; Talk given at the XXIX International Symposium on
  Lattice Field Theory - Lattice 2011, Lake Tahoe, California, USA</comments><report-no>Edinburgh 2011/35</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensions to the C++ implementation of the QCD Data Parallel Interface are
provided enabling acceleration of expression evaluation on NVIDIA GPUs. Single
expressions are off-loaded to the device memory and execution domain leveraging
the Portable Expression Template Engine and using Just-in-Time compilation
techniques. Memory management is automated by a software implementation of a
cache controlling the GPU's memory. Interoperability with existing Krylov space
solvers is demonstrated and special attention is paid on 'Chroma readiness'.
Non-kernel routines in lattice QCD calculations typically not subject of
hand-tuned optimisations are accelerated which can reduce the effects otherwise
suffered from Amdahl's Law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5612</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5612</id><created>2011-11-23</created><authors><author><keyname>Thirumalai</keyname><forenames>Vijayaraghavan</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Distributed Representation of Geometrically Correlated Images with
  Compressed Linear Measurements</title><categories>cs.CV cs.MM</categories><doi>10.1109/TIP.2012.2188035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of distributed coding of images whose
correlation is driven by the motion of objects or positioning of the vision
sensors. It concentrates on the problem where images are encoded with
compressed linear measurements. We propose a geometry-based correlation model
in order to describe the common information in pairs of images. We assume that
the constitutive components of natural images can be captured by visual
features that undergo local transformations (e.g., translation) in different
images. We first identify prominent visual features by computing a sparse
approximation of a reference image with a dictionary of geometric basis
functions. We then pose a regularized optimization problem to estimate the
corresponding features in correlated images given by quantized linear
measurements. The estimated features have to comply with the compressed
information and to represent consistent transformation between images. The
correlation model is given by the relative geometric transformations between
corresponding features. We then propose an efficient joint decoding algorithm
that estimates the compressed images such that they stay consistent with both
the quantized measurements and the correlation model. Experimental results show
that the proposed algorithm effectively estimates the correlation between
images in multi-view datasets. In addition, the proposed algorithm provides
effective decoding performance that compares advantageously to independent
coding solutions as well as state-of-the-art distributed coding schemes based
on disparity learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5639</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5639</id><created>2011-11-23</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>Smaili</keyname><forenames>Mohamad</forenames></author><author><keyname>Kassem</keyname><forenames>Hussam</forenames></author><author><keyname>Hayek</keyname><forenames>Hassan</forenames></author></authors><title>A New Technique to Backup and Restore DBMS using XML and .NET
  Technologies</title><categories>cs.DB</categories><journal-ref>International Journal on Computer Science and Engineering Vol. 02,
  No. 04, 2010, 1092-1102</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we proposed a new technique for backing up and restoring
different Database Management Systems (DBMS). The technique is enabling to
backup and restore a part of or the whole database using a unified interface
using ASP.NET and XML technologies. It presents a Web Solution allowing the
administrators to do their jobs from everywhere, locally or remotely. To show
the importance of our solution, we have taken two case studies, oracle 11g and
SQL Server 2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5640</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5640</id><created>2011-11-23</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author></authors><title>A New Proposed Technique to Improve Software Regression Testing Cost</title><categories>cs.SE</categories><journal-ref>International Journal of Security and Its Applications Vol. 5 No.
  3, July, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we describe the regression test process to test and verify
the changes made on software. A developed technique use the automation test
based on decision tree and test selection process in order to reduce the
testing cost is given. The developed technique is applied to a practical case
and the result show its improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5641</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5641</id><created>2011-11-23</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>Smaili</keyname><forenames>Mohamad</forenames></author></authors><title>An Improvement of RC4 Cipher Using Vigenere Cipher</title><categories>cs.CR</categories><journal-ref>International Journal of Computational Intelligence and
  Information Security Vo. 1 No. 3, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a new algorithm to improve the security of RC4. Given
that RC4 cipher is widely used in the wireless communication and has some
weaknesses in the security of RC4 cipher, our idea is based on the combination
of the RC4 and the poly alphabetic cipher Vigen\`ere to give a new and more
secure algorithm which we called VRC4. In this technique the plain text is
encrypted using the classic RC4 cipher then re-encrypt the resulted cipher text
using Vigen\`ere cipher to be a more secure cipher text. For simplicity, we
have implemented our new algorithm in Java Script taking into consideration two
factors: improvement of the security and the time complexity. To show the
performance of the new algorithm, we have used the well known network cracking
software KisMac.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5648</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5648</id><created>2011-11-23</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Falsification and future performance</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We information-theoretically reformulate two measures of capacity from
statistical learning theory: empirical VC-entropy and empirical Rademacher
complexity. We show these capacity measures count the number of hypotheses
about a dataset that a learning algorithm falsifies when it finds the
classifier in its repertoire minimizing empirical risk. It then follows from
that the future performance of predictors on unseen data is controlled in part
by how many hypotheses the learner falsifies. As a corollary we show that
empirical VC-entropy quantifies the message length of the true hypothesis in
the optimal code of a particular probability distribution, the so-called actual
repertoire.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5652</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5652</id><created>2011-11-23</created><updated>2012-02-15</updated><authors><author><keyname>Fuchs</keyname><forenames>Alexander</forenames><affiliation>The University of Iowa</affiliation></author><author><keyname>Goel</keyname><forenames>Amit</forenames><affiliation>Intel Corporation</affiliation></author><author><keyname>Grundy</keyname><forenames>Jim</forenames><affiliation>Intel Corporation</affiliation></author><author><keyname>Krsti&#x107;</keyname><forenames>Sava</forenames><affiliation>Intel Corporation</affiliation></author><author><keyname>Tinelli</keyname><forenames>Cesare</forenames><affiliation>The University of Iowa</affiliation></author></authors><title>Ground interpolation for the theory of equality</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.2.4, F.3.1, F.4.1, I.2.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (February
  16, 2012) lmcs:709</journal-ref><doi>10.2168/LMCS-8(1:6)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theory interpolation has found several successful applications in model
checking. We present a novel method for computing interpolants for ground
formulas in the theory of equality. The method produces interpolants from
colored congruence graphs representing derivations in that theory. These graphs
can be produced by conventional congruence closure algorithms in a
straightforward manner. By working with graphs, rather than at the level of
individual proof steps, we are able to derive interpolants that are pleasingly
simple (conjunctions of Horn clauses) and smaller than those generated by other
tools. Our interpolation method can be seen as a theory-specific implementation
of a cooperative interpolation game between two provers. We present a generic
version of the interpolation game, parametrized by the theory T, and define a
general method to extract runs of the game from proofs in T and then generate
interpolants from these runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5653</identifier>
 <datestamp>2012-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5653</id><created>2011-11-23</created><authors><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Borse</keyname><forenames>Rushikesh</forenames></author><author><keyname>Shah</keyname><forenames>Ronak</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Subhasis</forenames></author></authors><title>Estimation of the Embedding Capacity in Pixel-pair based Watermarking
  Schemes</title><categories>cs.CR cs.MM</categories><comments>This manuscript is submitted to Transactions of Image Processing, on
  september 5th 2011</comments><acm-class>I.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of the Embedding capacity is an important problem specifically in
reversible multi-pass watermarking and is required for analysis before any
image can be watermarked. In this paper, we propose an efficient method for
estimating the embedding capacity of a given cover image under multi-pass
embedding, without actually embedding the watermark. We demonstrate this for a
class of reversible watermarking schemes which operate on a disjoint group of
pixels, specifically for pixel pairs. The proposed algorithm iteratively
updates the co-occurrence matrix at every stage, to estimate the multi-pass
embedding capacity, and is much more efficient vis-a-vis actual watermarking.
We also suggest an extremely efficient, pre-computable tree based
implementation which is conceptually similar to the co-occurrence based method,
but provides the estimates in a single iteration, requiring a complexity akin
to that of single pass capacity estimation. We also provide bounds on the
embedding capacity. We finally show how our method can be easily used on a
number of watermarking algorithms and specifically evaluate the performance of
our algorithms on the benchmark watermarking schemes of Tian [11] and Coltuc
[6].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5654</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5654</id><created>2011-11-23</created><updated>2012-05-18</updated><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Wilson</keyname><forenames>Christo</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaohan</forenames></author><author><keyname>Zhu</keyname><forenames>Yibo</forenames></author><author><keyname>Mohanlal</keyname><forenames>Manish</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Serf and Turf: Crowdturfing for Fun and Profit</title><categories>cs.SI cs.CR</categories><comments>Proceedings of WWW 2012 Conference, 10 pages, 23 figures, 4 tables</comments><acm-class>H.3.5; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular Internet services in recent years have shown that remarkable things
can be achieved by harnessing the power of the masses using crowd-sourcing
systems. However, crowd-sourcing systems can also pose a real challenge to
existing security mechanisms deployed to protect Internet services. Many of
these techniques make the assumption that malicious activity is generated
automatically by machines, and perform poorly or fail if users can be organized
to perform malicious tasks using crowd-sourcing systems. Through measurements,
we have found surprising evidence showing that not only do malicious
crowd-sourcing systems exist, but they are rapidly growing in both user base
and total revenue. In this paper, we describe a significant effort to study and
understand these &quot;crowdturfing&quot; systems in today's Internet. We use detailed
crawls to extract data about the size and operational structure of these
crowdturfing systems. We analyze details of campaigns offered and performed in
these sites, and evaluate their end-to-end effectiveness by running active,
non-malicious campaigns of our own. Finally, we study and compare the source of
workers on crowdturfing sites in different countries. Our results suggest that
campaigns on these systems are highly effective at reaching users, and their
continuing growth poses a concrete threat to online communities such as social
networks, both in the US and elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5656</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5656</id><created>2011-11-23</created><authors><author><keyname>Valtr</keyname><forenames>Pavel</forenames></author></authors><title>On empty pentagons and hexagons in planar point sets</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give improved lower bounds on the minimum number of $k$-holes (empty
convex $k$-gons) in a set of $n$ points in general position in the plane, for
$k=5,6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5668</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5668</id><created>2011-11-23</created><authors><author><keyname>Truhachev</keyname><forenames>Dmitri</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Costello,</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Connecting Spatially Coupled LDPC Code Chains</title><categories>cs.IT cs.DM math.IT</categories><comments>Submitted to International Conference on Communications (ICC) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes constructed from connected spatially coupled low-density parity-check
code (SC-LDPCC) chains are proposed and analyzed. It is demonstrated that
connecting coupled chains results in improved iterative decoding performance.
The constructed protograph ensembles have better iterative decoding thresholds
compared to an individual SC-LDPCC chain and require less computational
complexity per bit when operating in the near-threshold region. In addition, it
is shown that the proposed constructions are asymptotically good in terms of
minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5679</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5679</id><created>2011-11-23</created><authors><author><keyname>Duan</keyname><forenames>Fabing</forenames></author><author><keyname>Chapeau-Blondeau</keyname><forenames>Francois</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Fisher information as a performance metric for locally optimum
  processing</title><categories>physics.data-an cs.IT math.IT</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a known weak signal in additive white noise, the asymptotic performance
of a locally optimum processor (LOP) is shown to be given by the Fisher
information (FI) of a standardized even probability density function (PDF) of
noise in three cases: (i) the maximum signal-to-noise ratio (SNR) gain for a
periodic signal; (ii) the optimal asymptotic relative efficiency (ARE) for
signal detection; (iii) the best cross-correlation gain (CG) for signal
transmission. The minimal FI is unity, corresponding to a Gaussian PDF, whereas
the FI is certainly larger than unity for any non-Gaussian PDFs. In the sense
of a realizable LOP, it is found that the dichotomous noise PDF possesses an
infinite FI for known weak signals perfectly processed by the corresponding
LOP. The significance of FI lies in that it provides a upper bound for the
performance of locally optimum processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5682</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5682</id><created>2011-11-24</created><authors><author><keyname>Fernando</keyname><forenames>Nirmal</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Flip-OFDM for Optical Wireless Communications</title><categories>cs.IT math.IT</categories><comments>published in IEEE Information Theory Workshop, Paraty Brazil, Sept
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two uniploar OFDM techniques for optical wireless communications:
asymmetric clipped optical OFDM (ACO-OFDM) and Flip-OFDM. Both techniques can
be used to compensate multipath distortion effects in optical wireless
channels. However, ACO-OFDM has been widely studied in the literature, while
the performance of Flip-OFDM has never been investigated. In this paper, we
conduct the performance analysis of Flip-OFDM and propose additional
modification to the original scheme in order to compare the performance of both
techniques. Finally, it is shown by simulation that both techniques have the
same performance but different hardware complexities. In particular, for slow
fading channels, Flip-OFDM offers 50% saving in hardware complexity over
ACO-OFDM at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5684</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5684</id><created>2011-11-24</created><updated>2013-02-11</updated><authors><author><keyname>Venditti</keyname><forenames>Michelina</forenames></author><author><keyname>Reale</keyname><forenames>Emanuela</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The Disclosure of University Research for Third Parties: A Non-Market
  Perspective on an Italian University</title><categories>cs.CY cs.DL</categories><comments>Science and Public Policy (in press; 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nations, universities, and regional governments commit resources to promote
the dissemination of scientific and technical knowledge. One focuses on
knowledge-based innovations and the economic function of the university in
terms of technology transfer, intellectual property,
university-industry-government relations, etc. Faculties other than engineering
or applied sciences, however, may not be able to recognize opportunities in
this &quot;linear model&quot; of technology transfer. We elaborate a non-market
perspective on the third mission in terms of disclosure of the knowledge and
areas of expertise available for disclosure to other audiences at a provincial
university. The use of ICT can enhance communication between actors on the
supply and demand sides. Using an idea originally developed in the context of
the Dutch science shops, the university staff was questionnaired about keywords
and areas of expertise with the specific purpose of disclosing this information
to audiences other than academic colleagues. The results were brought online in
a thesaurus-like structure that enables users to access the university at the
level of individual email address. This model stimulates variation on both the
supply and demand side of the innovation process, and strengthens the
accessibility and embeddedness of the knowledge base in a regional economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5687</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5687</id><created>2011-11-24</created><authors><author><keyname>Ducatel</keyname><forenames>Baptiste</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Kaytoue</keyname><forenames>Mehdi</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Marcuola</keyname><forenames>Florent</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Szathmary</keyname><forenames>Laszlo</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Coron : Plate-forme d'extraction de connaissances dans les bases de
  donn\'ees</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>17\`eme conf\'erence en Reconnaissance des Formes et Intelligence
  Artificielle (2010) 883-884</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5689</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5689</id><created>2011-11-24</created><authors><author><keyname>Kaytoue</keyname><forenames>Mehdi</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Revisiting Numerical Pattern Mining with Formal Concept Analysis</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Joint Conference on Artificial Intelligence (IJCAI)
  (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of mining numerical data in the
framework of Formal Concept Analysis. The usual way is to use a scaling
procedure --transforming numerical attributes into binary ones-- leading either
to a loss of information or of efficiency, in particular w.r.t. the volume of
extracted patterns. By contrast, we propose to directly work on numerical data
in a more precise and efficient way, and we prove it. For that, the notions of
closed patterns, generators and equivalent classes are revisited in the
numerical context. Moreover, two original algorithms are proposed and used in
an evaluation involving real-world data, showing the predominance of the
present approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5690</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5690</id><created>2011-11-24</created><authors><author><keyname>Kaytoue</keyname><forenames>Mehdi</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Marcuola</keyname><forenames>Florent</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Szathmary</keyname><forenames>Laszlo</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Villerd</keyname><forenames>Jean</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>The Coron System</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>8th International Conference on Formal Concept Analsis (ICFCA)
  (2010) 55--58</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coron is a domain and platform independent, multi-purposed data mining
toolkit, which incorporates not only a rich collection of data mining
algorithms, but also allows a number of auxiliary operations. To the best of
our knowledge, a data mining toolkit designed specifically for itemset
extraction and association rule generation like Coron does not exist elsewhere.
Coron also provides support for preparing and filtering data, and for
interpreting the extracted units of knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5710</identifier>
 <datestamp>2012-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5710</id><created>2011-11-24</created><authors><author><keyname>Benaim</keyname><forenames>Michel</forenames></author><author><keyname>Boudec</keyname><forenames>Jean-Yves Le</forenames></author></authors><title>On Mean Field Convergence and Stationary Regime</title><categories>math.DS cs.PF cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume that a family of stochastic processes on some Polish space $E$
converges to a deterministic process; the convergence is in distribution (hence
in probability) at every fixed point in time. This assumption holds for a large
family of processes, among which many mean field interaction models and is
weaker than previously assumed. We show that any limit point of an invariant
probability of the stochastic process is an invariant probability of the
deterministic process. The results are valid in discrete and in continuous
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5720</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5720</id><created>2011-11-24</created><authors><author><keyname>Konstantinidis</keyname><forenames>Andreas</forenames></author><author><keyname>Haralambous</keyname><forenames>Haris</forenames></author><author><keyname>Agapitos</keyname><forenames>Alexandros</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Harris</forenames></author></authors><title>A GP-MOEA/D Approach for Modelling Total Electron Content over Cyprus</title><categories>cs.AI cs.NE</categories><journal-ref>A. Konstantinidis, H. Haralambous, A. Agapitos and H.
  Papadopoulos. A GP-MOEA/D Approach for Modelling Total Electron Content over
  Cyprus. Engineering Intelligent Systems 18(3-4): 193-203. CRL Publishing,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vertical Total Electron Content (vTEC) is an ionospheric characteristic used
to derive the signal delay imposed by the ionosphere on near-vertical
trans-ionospheric links. The major aim of this paper is to design a prediction
model based on the main factors that influence the variability of this
parameter on a diurnal, seasonal and long-term time-scale. The model should be
accurate and general (comprehensive) enough for efficiently approximating the
high variations of vTEC. However, good approximation and generalization are
conflicting objectives. For this reason a Genetic Programming (GP) with
Multi-objective Evolutionary Algorithm based on Decomposition characteristics
(GP-MOEA/D) is designed and proposed for modeling vTEC over Cyprus.
Experimental results show that the Multi-Objective GP-model, considering real
vTEC measurements obtained over a period of 11 years, has produced a good
approximation of the modeled parameter and can be implemented as a local model
to account for the ionospheric imposed error in positioning. Particulary, the
GP-MOEA/D approach performs better than a Single Objective Optimization GP, a
GP with Non-dominated Sorting Genetic Algorithm-II (NSGA-II) characteristics
and the previously proposed Neural Network-based approach in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5721</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5721</id><created>2011-11-24</created><authors><author><keyname>Paszkiewicz</keyname><forenames>Zbigniew</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>MAPSS, a Multi-Aspect Partner and Service Selection Method</title><categories>cs.SE</categories><comments>10 pages, 1 figure</comments><doi>10.1007/978-3-642-15961-9_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Service-Oriented Virtual Organization Breeding Environments (SOVOBEs),
services performed by people, organizations and information systems are
composed in potentially complex business processes performed by a set of
partners. In a SOVOBE, the success of a virtual organization depends largely on
the partner and service selection process, which determines the composition of
services performed by the VO partners. In this paper requirements for a partner
and service selection method for SOVOBEs are defined and a novel Multi-Aspect
Partner and Service Selection method, MAPSS, is presented. The MAPSS method
allows a VO planner to select appropriate services and partners based on their
competences and their relations with other services/partners. The MAPSS method
relies on a genetic algorithm to select the most appropriate set of partners
and services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5733</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5733</id><created>2011-11-24</created><authors><author><keyname>&#x15a;wierzowicz</keyname><forenames>Jan</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Social Service Brokerage based on UDDI and Social Requirements</title><categories>cs.SE cs.SI</categories><comments>8 pages, 3 figures</comments><doi>10.1007/978-3-642-15961-9_51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The choice of a suitable service provider is an important issue often
overlooked in existing architectures. Current systems focus mostly on the
service itself, paying little (if at all) attention to the service provider. In
the Service Oriented Architecture (SOA), Universal Description, Discovery and
Integration (UDDI) registries have been proposed as a way to publish and find
information about available services. These registries have been criticized for
not being completely trustworthy. In this paper, an enhancement of existing
mechanisms for finding services is proposed. The concept of Social Service
Broker addressing both service and social requirements is proposed. While UDDI
registries still provide information about available services, methods from
Social Network Analysis are proposed as a way to evaluate and rank the services
proposed by a UDDI registry in social terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5735</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5735</id><created>2011-11-24</created><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author></authors><title>Efficient Joint Network-Source Coding for Multiple Terminals with Side
  Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of source coding in networks with multiple receiving
terminals, each having access to some kind of side information. In this case,
standard coding techniques are either prohibitively complex to decode, or
require network-source coding separation, resulting in sub-optimal transmission
rates. To alleviate this problem, we offer a joint network-source coding scheme
based on matrix sparsification at the code design phase, which allows the
terminals to use an efficient decoding procedure (syndrome decoding using
LDPC), despite the network coding throughout the network. Via a novel relation
between matrix sparsification and rate-distortion theory, we give lower and
upper bounds on the best achievable sparsification performance. These bounds
allow us to analyze our scheme, and, in particular, show that in the limit
where all receivers have comparable side information (in terms of conditional
entropy), or, equivalently, have weak side information, a vanishing density can
be achieved. As a result, efficient decoding is possible at all terminals
simultaneously. Simulation results motivate the use of this scheme at
non-limiting rates as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5737</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5737</id><created>2011-11-24</created><authors><author><keyname>Cellary</keyname><forenames>Wojciech</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Agile and Pro-Active Public Administration as a Collaborative Networked
  Organization</title><categories>cs.CY</categories><comments>6 pages, 2 figures</comments><acm-class>J.1; K.4.3</acm-class><doi>10.1145/1930321.1930324</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In highly competitive, globalized economies and societies of always-on-line
people intensively using the Internet and mobile phones, public administrations
have to adapt to new challenges. Enterprises and citizens expect public
administrations to be agile and pro-active to foster development. A way to
achieve agility and pro-activity is application of a model of Collaborative
Network Organizations in its two forms: Virtual Organizations (VO) and Virtual
Organization Breeding Environments (VOBE). In the paper, advantages are shown
of public administration playing a role of a Virtual Organization customer on
the one hand, and a Virtual Organization member on the other hand. It is also
shown how public administration playing a role of a Virtual Organization
Breeding Environment may improve its agility and promote advanced technologies
and management methods among local organizations. It is argued in the paper
that public administration should provide a Virtual Organization Breeding
Environment as a part of public services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5741</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5741</id><created>2011-11-24</created><authors><author><keyname>Paszkiewicz</keyname><forenames>Zbigniew</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Reference Model for Performance Management in Service-Oriented Virtual
  Organization Breeding Environments</title><categories>cs.OH</categories><comments>10 pages</comments><doi>10.1007/978-3-642-04568-4_52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance management (PM) is a key function of virtual organization (VO)
management. A large set of PM indicators has been proposed and evaluated within
the context of virtual breeding environments (VBEs). However, it is currently
difficult to describe and select suitable PM indicators because of the lack of
a common vocabulary and taxonomies of PM indicators. Therefore, there is a need
for a framework unifying concepts in the domain of VO PM. In this paper, a
reference model for VO PM is presented in the context of service-oriented VBEs.
In the proposed reference model, both a set of terms that could be used to
describe key performance indicators, and a set of taxonomies reflecting various
aspects of PM are proposed. The proposed reference model is a first attempt and
a work in progress that should not be supposed exhaustive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5743</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5743</id><created>2011-11-24</created><authors><author><keyname>Markstr&#xf6;m</keyname><forenames>Klas</forenames></author></authors><title>Two questions of Erd\H{o}s on hypergraphs above the Tur{\'a}n threshold}</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For ordinary graphs it is known that any graph $G$ with more edges than the
Tur{\'a}n number of $K_s$ must contain several copies of $K_s$, and a copy of
$K_{s+1}^-$, the complete graph on $s+1$ vertices with one missing edge.
Erd\H{o}s asked if the same result is true for $K^3_s$, the complete 3-uniform
hypergraph on $s$ vertices.
  In this note we show that for small values of $n$, the number of vertices in
$G$, the answer is negative for $s=4$. For the second property, that of
containing a ${K^3_{s+1}}^-$, we show that for $s=4$ the answer is negative for
all large $n$ as well, by proving that the Tur{\'a}n density of ${K^3_5}^-$ is
greater than that of $K^3_4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5750</identifier>
 <datestamp>2011-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5750</id><created>2011-11-24</created><updated>2011-12-05</updated><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author></authors><title>Effects of mass media on opinion spreading in the Sznajd sociophysics
  model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 8 figures, to appear in Physica A</comments><journal-ref>Physica A 391, 1729 (2012)</journal-ref><doi>10.1016/j.physa.2011.11.038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the influence of mass media in the dynamics of the
two-dimensional Sznajd model. This influence acts as an external field, and it
is introduced in the model by means of a probability $p$ of the agents to
follow the media opinion. We performed Monte Carlo simulations on square
lattices with different sizes, and our numerical results suggest a change on
the critical behavior of the model, with the absence of the usual phase
transition for $p&gt;\sim 0.18$. Another effect of the probability $p$ is to
decrease the average relaxation times $\tau$, that are log-normally
distributed, as in the standard model. In addition, the $\tau$ values depend on
the lattice size $L$ in a power-law form, $\tau\sim L^{\alpha}$, where the
power-law exponent depends on the probability $p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5765</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5765</id><created>2011-11-24</created><authors><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Social Protocols for Agile Virtual Teams</title><categories>cs.OH</categories><comments>10 pages, 1 figure</comments><doi>10.1007/978-3-642-04568-4_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite many works on collaborative networked organizations (CNOs), CSCW,
groupware, workflow systems and social networks, computer support for virtual
teams is still insufficient, especially support for agility, i.e. the
capability of virtual team members to rapidly and cost efficiently adapt the
way they interact to changes. In this paper, requirements for computer support
for agile virtual teams are presented. Next, an extension of the concept of
social protocol is proposed as a novel model supporting agile interactions
within virtual teams. The extended concept of social protocol consists of an
extended social network and a workflow model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5767</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5767</id><created>2011-11-24</created><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Morisset</keyname><forenames>Charles</forenames></author></authors><title>PTaCL: A Language for Attribute-Based Access Control in Open Systems</title><categories>cs.CR cs.LO</categories><comments>26 pages, submitted to Principles of Security and Trust (POST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many languages and algebras have been proposed in recent years for the
specification of authorization policies. For some proposals, such as XACML, the
main motivation is to address real-world requirements, typically by providing a
complex policy language with somewhat informal evaluation methods; others try
to provide a greater degree of formality (particularly with respect to policy
evaluation) but support far fewer features. In short, there are very few
proposals that combine a rich set of language features with a well-defined
semantics, and even fewer that do this for authorization policies for
attribute-based access control in open environments. In this paper, we
decompose the problem of policy specification into two distinct sub-languages:
the policy target language (PTL) for target specification, which determines
when a policy should be evaluated; and the policy composition language (PCL)
for building more complex policies from existing ones. We define syntax and
semantics for two such languages and demonstrate that they can be both simple
and expressive. PTaCL, the language obtained by combining the features of these
two sub-languages, supports the specification of a wide range of policies.
However, the power of PTaCL means that it is possible to define policies that
could produce unexpected results. We provide an analysis of how PTL should be
restricted and how policies written in PCL should be evaluated to minimize the
likelihood of undesirable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5773</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5773</id><created>2011-11-24</created><authors><author><keyname>&#x15a;wierzowicz</keyname><forenames>Jan</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Social Requirements for Virtual Organization Breeding Environments</title><categories>cs.SI</categories><comments>10 pages, 2 figures</comments><doi>10.1007/978-3-642-04568-4_63</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The creation of Virtual Breeding Environments (VBE) is a topic which has
received too little attention: in most former works, the existence of the VBE
is either assumed, or is considered as the result of the voluntary,
participatory gathering of a set of candidate companies. In this paper, the
creation of a VBE by a third authority is considered: chambers of commerce, as
organizations whose goal is to promote and facilitate business interests and
activity in the community, could be good candidates for exogenous VBE creators.
During VBE planning, there is a need to specify social requirements for the
VBE. In this paper, SNA metrics are proposed as a way for a VBE planner to
express social requirements for a VBE to be created. Additionally, a set of
social requirements for VO planners, VO brokers, and VBE members are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5775</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5775</id><created>2011-11-24</created><updated>2011-12-06</updated><authors><author><keyname>Hesselink</keyname><forenames>Wim H.</forenames></author></authors><title>Partial mutual exclusion for infinitely many processes</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial mutual exclusion is the drinking philosophers problem for complete
graphs. It is the problem that a process may enter a critical section CS of its
code only when some finite set nbh of other processes are not in their critical
sections. For each execution of CS, the set nbh can be given by the
environment. We present a starvation free solution of this problem in a setting
with infinitely many processes, each with finite memory, that communicate by
asynchronous messages. The solution has the property of first-come
first-served, in so far as this can be guaranteed by asynchronous messages. For
every execution of CS and every process in nbh, between three and six messages
are needed. The correctness of the solution is argued with invariants and
temporal logic. It has been verified with the proof assistant PVS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5778</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5778</id><created>2011-11-24</created><authors><author><keyname>Paszkiewicz</keyname><forenames>Zbigniew</forenames></author><author><keyname>Picard</keyname><forenames>Willy</forenames></author></authors><title>Modeling Virtual Organization Architecture with the Virtual Organization
  Breeding Methodology</title><categories>cs.OH</categories><comments>9 pages, 4 figures</comments><doi>10.1007/978-3-642-04568-4_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Enterprise Architecture Modeling (EAM) methodologies become more and
more popular, an EAM methodology tailored to the needs of virtual organizations
(VO) is still to be developed. Among the most popular EAM methodologies, TOGAF
has been chosen as the basis for a new EAM methodology taking into account
characteristics of VOs presented in this paper. In this new methodology,
referred as Virtual Organization Breeding Methodology (VOBM), concepts
developed within the ECOLEAD project, e.g. the concept of Virtual Breeding
Environment (VBE) or the VO creation schema, serve as fundamental elements for
development of VOBM. VOBM is a generic methodology that should be adapted to a
given VBE. VOBM defines the structure of VBE and VO architectures in a
service-oriented environment, as well as an architecture development method for
virtual organizations (ADM4VO). Finally, a preliminary set of tools and methods
for VOBM is given in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5792</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5792</id><created>2011-11-24</created><updated>2012-10-25</updated><authors><author><keyname>Seoane</keyname><forenames>Lu&#xed;s F.</forenames></author><author><keyname>Ruttor</keyname><forenames>Andreas</forenames></author></authors><title>Successful attack on permutation-parity-machine-based neural
  cryptography</title><categories>cs.CR cond-mat.dis-nn</categories><comments>4 pages, 2 figures; abstract changed, note about chaos cryptography
  added, typos corrected</comments><journal-ref>Phys. Rev. E 85, 025101(R) (2012)</journal-ref><doi>10.1103/PhysRevE.85.025101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm is presented which implements a probabilistic attack on the
key-exchange protocol based on permutation parity machines. Instead of
imitating the synchronization of the communicating partners, the strategy
consists of a Monte Carlo method to sample the space of possible weights during
inner rounds and an analytic approach to convey the extracted information from
one outer round to the next one. The results show that the protocol under
attack fails to synchronize faster than an eavesdropper using this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5799</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5799</id><created>2011-11-24</created><updated>2013-06-27</updated><authors><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Spatial Throughput of Mobile Ad Hoc Networks Powered by Energy
  Harvesting</title><categories>cs.IT math.IT</categories><comments>This paper has been presented in part at Asilomar Conf. on Signals,
  Systems, and Computers 2011 and at IEEE Intl. Conf. on Communications (ICC)
  2013. The full version will appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing mobiles to harvest ambient energy such as kinetic activities or
electromagnetic radiation will enable wireless networks to be self sustaining
besides alleviating global warming. In this paper, the spatial throughput of a
mobile ad hoc network powered by energy harvesting is analyzed using a
stochastic-geometry model. In this model, transmitters are distributed as a
Poisson point process and energy arrives at each transmitter randomly with a
uniform average rate called the energy arrival rate; upon harvesting sufficient
energy, each transmitter transmits with fixed power to an intended receiver
under an outage-probability constraint for a target
signal-to-interference-and-noise ratio. It is assumed that transmitters store
energy in batteries with infinite capacity. By applying the random-walk theory,
the probability that a transmitter transmits, called the transmission
probability, is proved to be equal to one if the energy-arrival rate exceeds
transmission power or otherwise is equal to their ratio. This result and tools
from stochastic geometry are applied to maximize the network throughput for a
given energy-arrival rate by optimizing transmission power. The maximum network
throughput is shown to be proportional to the optimal transmission probability,
which is equal to one if the transmitter density is below a derived function of
the energy-arrival rate or otherwise is smaller than one and solves a given
polynomial equation. Last, the limits of the maximum network throughput are
obtained for the extreme cases of high energy-arrival rates and dense networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5807</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5807</id><created>2011-11-24</created><authors><author><keyname>Krasinski</keyname><forenames>Tadeusz</forenames></author><author><keyname>Sakowski</keyname><forenames>Sebastian</forenames></author><author><keyname>Poplawski</keyname><forenames>Tomasz</forenames></author></authors><title>Autonomous push-down automaton built on DNA</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a biomolecular implementation of the push-down
automaton (one of theoretical models of computing device with unbounded memory)
using DNA molecules. The idea of this improved implementation was inspired by
Cavaliere et al. (2005).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5810</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5810</id><created>2011-11-24</created><authors><author><keyname>Bulakci</keyname><forenames>&#xd6;mer</forenames></author><author><keyname>Saleh</keyname><forenames>Abdallah Bou</forenames></author><author><keyname>Redana</keyname><forenames>Simone</forenames></author><author><keyname>Raaf</keyname><forenames>Bernhard</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author></authors><title>Enhancing LTE-Advanced Relay Deployments via Relay Cell Extension</title><categories>cs.NI</categories><comments>This work was presented at the 15th International OFDM-Workshop
  (InOWo 10), Hamburg, Germany, 1. - 2. September 2010. 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relaying is a promising enhancement to current radio access networks. Relay
enhanced LTE-Advanced networks are expected to fulfill the demanding coverage
and capacity requirements in a cost-efficient way. However, due to low transmit
power, the coverage areas of the relay nodes will be small. Therefore, the
performance of relay deployments may be limited by load imbalances. In this
study, we present a practical solution for this problem by introducing a bias
to cell selection and handover decisions along with a reduction in eNB transmit
power. This method results in an extension of the relay cells and an
appropriate load balance can then be achieved. Moreover, it is shown that a
proper power control setting is necessary in the uplink and that power control
optimization can further enhance the system performance. Comprehensive system
level simulations confirm that the proposed solution yields significant user
throughput gains both in the uplink and the downlink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5848</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5848</id><created>2011-11-24</created><authors><author><keyname>Manch&#xf3;n</keyname><forenames>Carles Navarro</forenames></author><author><keyname>Kirkelund</keyname><forenames>Gunvor E.</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Christensen</keyname><forenames>Lars P. B.</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard H.</forenames></author></authors><title>Receiver Architectures for MIMO-OFDM Based on a Combined VMP-SP
  Algorithm</title><categories>stat.ML cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative information processing, either based on heuristics or analytical
frameworks, has been shown to be a very powerful tool for the design of
efficient, yet feasible, wireless receiver architectures. Within this context,
algorithms performing message-passing on a probabilistic graph, such as the
sum-product (SP) and variational message passing (VMP) algorithms, have become
increasingly popular.
  In this contribution, we apply a combined VMP-SP message-passing technique to
the design of receivers for MIMO-ODFM systems. The message-passing equations of
the combined scheme can be obtained from the equations of the stationary points
of a constrained region-based free energy approximation. When applied to a
MIMO-OFDM probabilistic model, we obtain a generic receiver architecture
performing iterative channel weight and noise precision estimation,
equalization and data decoding. We show that this generic scheme can be
particularized to a variety of different receiver structures, ranging from
high-performance iterative structures to low complexity receivers. This allows
for a flexible design of the signal processing specially tailored for the
requirements of each specific application. The numerical assessment of our
solutions, based on Monte Carlo simulations, corroborates the high performance
of the proposed algorithms and their superiority to heuristic approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5867</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5867</id><created>2011-11-24</created><authors><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Narayan</keyname><forenames>Manjari</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Suboptimality of Nonlocal Means for Images with Sharp Edges</title><categories>math.ST cs.CV cs.IT math.IT stat.TH</categories><comments>33 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct an asymptotic risk analysis of the nonlocal means image denoising
algorithm for the Horizon class of images that are piecewise constant with a
sharp edge discontinuity. We prove that the mean square risk of an optimally
tuned nonlocal means algorithm decays according to $n^{-1}\log^{1/2+\epsilon}
n$, for an $n$-pixel image with $\epsilon&gt;0$. This decay rate is an improvement
over some of the predecessors of this algorithm, including the linear
convolution filter, median filter, and the SUSAN filter, each of which provides
a rate of only $n^{-2/3}$. It is also within a logarithmic factor from
optimally tuned wavelet thresholding. However, it is still substantially lower
than the the optimal minimax rate of $n^{-4/3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5872</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5872</id><created>2011-11-24</created><authors><author><keyname>Mahmoody-Ghaidary</keyname><forenames>Ahmad</forenames></author><author><keyname>Chauve</keyname><forenames>Cedric</forenames></author><author><keyname>Stacho</keyname><forenames>Ladislav</forenames></author></authors><title>Tractability results for the Double-Cut-and-Join circular median problem</title><categories>cs.DM cs.DS</categories><comments>12 pages, 6 figures, submitted</comments><msc-class>68R10, 90C27</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The circular median problem in the Double-Cut-and-Join (DCJ) distance asks to
find, for three given genomes, a fourth circular genome that minimizes the sum
of the mutual distances with the three other ones. This problem has been shown
to be NP-complete. We show here that, if the number of vertices of degree 3 in
the breakpoint graph of the three input genomes is fixed, then the problem is
tractable
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5880</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5880</id><created>2011-11-24</created><authors><author><keyname>Zhang</keyname><forenames>Fumin</forenames></author><author><keyname>Shi</keyname><forenames>Zhenwu</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Shayok</forenames></author></authors><title>Robustness Analysis for Battery Supported Cyber-Physical Systems</title><categories>cs.ET cs.OS cs.SY</categories><comments>This paper has been accepted by ACM Transactions in Embedded
  Computing Systems (TECS) in October, 2011</comments><acm-class>C.3; D.4.1; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes a novel analytical approach to quantify robustness of
scheduling and battery management for battery supported cyber-physical systems.
A dynamic schedulability test is introduced to determine whether tasks are
schedulable within a finite time window. The test is used to measure robustness
of a real-time scheduling algorithm by evaluating the strength of computing
time perturbations that break schedulability at runtime. Robustness of battery
management is quantified analytically by an adaptive threshold on the state of
charge. The adaptive threshold significantly reduces the false alarm rate for
battery management algorithms to decide when a battery needs to be replaced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5884</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5884</id><created>2011-11-24</created><authors><author><keyname>Ben-Sasson</keyname><forenames>Eli</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Zewi</keyname><forenames>Noga</forenames></author></authors><title>An additive combinatorics approach to the log-rank conjecture in
  communication complexity</title><categories>cs.CC math.CO</categories><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a $\{0,1\}$-valued matrix $M$ let $\rm{CC}(M)$ denote the deterministic
communication complexity of the boolean function associated with $M$. The
log-rank conjecture of Lov\'{a}sz and Saks [FOCS 1988] states that $\rm{CC}(M)
\leq \log^c(\rm{rank}(M))$ for some absolute constant $c$ where $\rm{rank}(M)$
denotes the rank of $M$ over the field of real numbers. We show that
$\rm{CC}(M)\leq c \cdot \rm{rank}(M)/\log \rm{rank}(M)$ for some absolute
constant $c$, assuming a well-known conjecture from additive combinatorics
known as the Polynomial Freiman-Ruzsa (PFR) conjecture.
  Our proof is based on the study of the &quot;approximate duality conjecture&quot; which
was recently suggested by Ben-Sasson and Zewi [STOC 2011] and studied there in
connection to the PFR conjecture. First we improve the bounds on approximate
duality assuming the PFR conjecture. Then we use the approximate duality
conjecture (with improved bounds) to get the aforementioned upper bound on the
communication complexity of low-rank martices, where this part uses the
methodology suggested by Nisan and Wigderson [Combinatorica 1995].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5885</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5885</id><created>2011-11-24</created><updated>2012-05-09</updated><authors><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author></authors><title>Type inference in mathematics</title><categories>cs.LO</categories><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the theory of programming languages, type inference is the process of
inferring the type of an expression automatically, often making use of
information from the context in which the expression appears. Such mechanisms
turn out to be extremely useful in the practice of interactive theorem proving,
whereby users interact with a computational proof assistant to construct formal
axiomatic derivations of mathematical theorems. This article explains some of
the mechanisms for type inference used by the Mathematical Components project,
which is working towards a verification of the Feit-Thompson theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5892</identifier>
 <datestamp>2012-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5892</id><created>2011-11-25</created><updated>2012-01-30</updated><authors><author><keyname>Sher</keyname><forenames>Gene I.</forenames></author></authors><title>Evolving Chart Pattern Sensitive Neural Network Based Forex Trading
  Agents</title><categories>cs.NE cs.CE</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though machine learning has been applied to the foreign exchange market for
algorithmic trading for quiet some time now, and neural networks(NN) have been
shown to yield positive results, in most modern approaches the NN systems are
optimized through traditional methods like the backpropagation algorithm for
example, and their input signals are price lists, and lists composed of other
technical indicator elements. The aim of this paper is twofold: the
presentation and testing of the application of topology and weight evolving
artificial neural network (TWEANN) systems to automated currency trading, and
to demonstrate the performance when using Forex chart images as input to
geometrical regularity aware indirectly encoded neural network systems,
enabling them to use the patterns &amp; trends within, when trading. This paper
presents the benchmark results of NN based automated currency trading systems
evolved using TWEANNs, and compares the performance and generalization
capabilities of these direct encoded NNs which use the standard sliding-window
based price vector inputs, and the indirect (substrate) encoded NNs which use
charts as input. The TWEANN algorithm I will use in this paper to evolve these
currency trading agents is the memetic algorithm based TWEANN system called
Deus Ex Neural Network (DXNN) platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5893</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5893</id><created>2011-11-25</created><updated>2014-06-16</updated><authors><author><keyname>Inkulu</keyname><forenames>Rajasekhar</forenames></author><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author></authors><title>ANN queries: covering Voronoi diagram with hyperboxes</title><categories>cs.CG cs.DS</categories><comments>This paper has been withdrawn by the authors. Not interesting to have
  it around</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $S$ of $n$ points in $d$-dimensional Euclidean metric space $X$
and a small positive real number $\epsilon$, we present an algorithm to
preprocess $S$ and answer queries that require finding a set $S' \subseteq S$
of $\epsilon$-approximate nearest neighbors (ANNs) to a given query point $q
\in X$. The following are the characteristics of points belonging to set $S'$:
  - $\forall s \in S'$, $\exists$ a point $p \in X$ such that $|pq| \le
\epsilon$ and the nearest neighbor of $p$ is $s$, and
  - $\exists$ a $s' \in S'$ such that $s'$ is a nearest neighbor of $q$.
  During the preprocessing phase, from the Voronoi diagram of $S$ we construct
a set of box trees of size $O(4^d\frac{V}{\delta}(\frac{\pi}{\epsilon})^{d-1})$
which facilitate in querying ANNs of any input query point in $O(\frac{1}{d}lg
\frac{V}{\delta} + (\frac{\pi}{\epsilon})^{d-1})$ time. Here $\delta$ equals to
$(\frac{\epsilon}{2\sqrt{d}})^d$, and $V$ is the volume of a large bounding box
that contains all the points of set $S$. The average case cardinality of $S'$
is shown to rely on $S$ and $\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5897</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5897</id><created>2011-11-25</created><authors><author><keyname>Pesenson</keyname><forenames>Isaac</forenames></author></authors><title>Variational Splines and Paley--Wiener Spaces on Combinatorial Graphs</title><categories>cs.IT math.IT</categories><journal-ref>Constr Approx (2009) 29: 1--21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Notions of interpolating variational splines and Paley-Wiener spaces are
introduced on a combinatorial graph G. Both of these definitions explore
existence of a combinatorial Laplace operator onG. The existence and uniqueness
of interpolating variational splines on a graph is shown. As an application of
variational splines, the paper presents a reconstruction algorithm of
Paley-Wiener functions on graphs from their uniqueness sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5899</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5899</id><created>2011-11-25</created><authors><author><keyname>Pesenson</keyname><forenames>Isaac Z.</forenames></author><author><keyname>Pesenson</keyname><forenames>Meyer Z.</forenames></author></authors><title>Sampling, Filtering and Sparse Approximations on Combinatorial Graphs</title><categories>cs.IT math.FA math.IT</categories><journal-ref>J Fourier Anal Appl, 2010. DOI 10.1007/s00041-009-9116-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address sampling and approximation of functions on
combinatorial graphs. We develop filtering on graphs by using Schr\&quot;odinger's
group of operators generated by combinatorial Laplace operator. Then we
construct a sampling theory by proving Poincare and Plancherel-Polya-type
inequalities for functions on graphs. These results lead to a theory of sparse
approximations on graphs and have potential applications to filtering,
denoising, data dimension reduction, image processing, image compression,
computer graphics, visualization and learning theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5900</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5900</id><created>2011-11-25</created><authors><author><keyname>Pesenson</keyname><forenames>Isaac Z.</forenames></author><author><keyname>Geller</keyname><forenames>Daryl</forenames></author></authors><title>Cubature formulas and discrete fourier transform on compact manifolds</title><categories>math.FA cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1002.3841 and
  arXiv:1104.0632</comments><journal-ref>Dev. Math., 28, Springer, New York, 2013, 431-453</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the paper is to describe essentially optimal cubature formulas on
compact Riemannian manifolds which are exact on spaces of band- limited
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5901</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5901</id><created>2011-11-25</created><updated>2011-12-12</updated><authors><author><keyname>Schwentick</keyname><forenames>Thomas</forenames><affiliation>Technische Universit&#xe4;t Dortmund</affiliation></author><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames><affiliation>Goethe-Universit&#xe4;t Frankfurt am Main</affiliation></author></authors><title>A note on the expressive power of linear orders</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 4 (December
  14, 2011) lmcs:1008</journal-ref><doi>10.2168/LMCS-7(4:7)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article shows that there exist two particular linear orders such that
first-order logic with these two linear orders has the same expressive power as
first-order logic with the Bit-predicate FO(Bit). As a corollary we obtain that
there also exists a built-in permutation such that first-order logic with a
linear order and this permutation is as expressive as FO(Bit).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5930</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5930</id><created>2011-11-25</created><authors><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Juneja</keyname><forenames>Dimple</forenames></author><author><keyname>Sharma</keyname><forenames>A. K.</forenames></author></authors><title>Agent Development Toolkits</title><categories>cs.MA</categories><comments>Contains Seven pages, 2 Figures and One table</comments><journal-ref>International Journal of Advancements in Technology,vol. 2, No.1,
  January 2011, http://ijict.org</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Development of agents as well as their wide usage requires good underlying
infrastructure. Literature indicates scarcity of agent development tools in
initial years of research which limited the exploitation of this beneficial
technology. However, today a wide variety of tools are available, for
developing robust infrastructure. This technical note provides a deep overview
of such tools and contrasts features provided by them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5950</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5950</id><created>2011-11-25</created><updated>2013-05-10</updated><authors><author><keyname>Banelli</keyname><forenames>Paolo</forenames></author></authors><title>Non-Linear Transformations of Gaussians and Gaussian-Mixtures with
  implications on Estimation and Information Theory</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>26 pages, 4 figures (8 sub-figures), submitted to IEEE Trans. on
  Information Theory 20th April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the statistical properties of non-linear
transformations (NLT) of random variables, in order to establish useful tools
for estimation and information theory. Specifically, the paper focuses on
linear regression analysis of the NLT output and derives sufficient general
conditions to establish when the input-output regression coefficient is equal
to the \emph{partial} regression coefficient of the output with respect to a
(additive) part of the input. A special case is represented by zero-mean
Gaussian inputs, obtained as the sum of other zero-mean Gaussian random
variables. The paper shows how this property can be generalized to the
regression coefficient of non-linear transformations of Gaussian-mixtures. Due
to its generality, and the wide use of Gaussians and Gaussian-mixtures to
statistically model several phenomena, this theoretical framework can find
applications in multiple disciplines, such as communication, estimation, and
information theory, when part of the nonlinear transformation input is the
quantity of interest and the other part is the noise. In particular, the paper
shows how the said properties can be exploited to simplify closed-form
computation of the signal-to-noise ratio (SNR), the estimation mean-squared
error (MSE), and bounds on the mutual information in additive non-Gaussian
(possibly non-linear) channels, also establishing relationships among them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5979</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5979</id><created>2011-11-25</created><authors><author><keyname>Knauer</keyname><forenames>Christian</forenames></author><author><keyname>Werner</keyname><forenames>Daniel</forenames></author></authors><title>Erd\H{o}s-Szekeres and Testing Weak epsilon-Nets are NP-hard in 3
  dimensions - and what now?</title><categories>cs.CG cs.CC</categories><comments>6 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the computational versions of the Erd\H os-Szekeres theorem and
related problems in 3 dimensions. We show that, in constrast to the planar
case, no polynomial time algorithm exists for determining the largest (empty)
convex subset (unless P=NP) among a set of points, by proving that the
corresponding decision problem is NP-hard. This answers a question by Dobkin,
Edelsbrunner and Overmars from 1990.
  As a corollary, we derive a similar result for the closely related problem of
testing weak epsilon-nets in R^3. Answering a question by Chazelle et al. from
1995, our reduction shows that the problem is co-NP-hard.
  This is work in progress - we are still trying to find a smart approximation
algorithm for the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.5986</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.5986</id><created>2011-11-25</created><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author></authors><title>The Clique Problem in Ray Intersection Graphs</title><categories>cs.CG cs.CC</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ray intersection graphs are intersection graphs of rays, or halflines, in the
plane. We show that any planar graph has an even subdivision whose complement
is a ray intersection graph. The construction can be done in polynomial time
and implies that finding a maximum clique in a segment intersection graph is
NP-hard. This solves a 21-year old open problem posed by Kratochv\'il and
Ne\v{s}et\v{r}il.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6026</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6026</id><created>2011-11-25</created><updated>2013-11-13</updated><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author><author><keyname>Riis</keyname><forenames>Soren</forenames></author></authors><title>Memoryless computation: new results, constructions, and extensions</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are interested in memoryless computation, a modern paradigm
to compute functions which generalises the famous XOR swap algorithm to
exchange the contents of two variables without using a buffer. This uses a
combinatorial framework for procedural programming languages, where programs
are only allowed to update one variable at a time. We first consider programs
which do not have any memory. We prove that any function of $n$ variables can
be computed this way in only $4n-3$ variable updates. We then derive the exact
number of instructions required to compute any manipulation of variables. This
shows that combining variables, instead of simply moving them around, not only
allows for memoryless programs, but also yields shorter programs. Second, we
show that allowing programs to use memory is also incorporated in the
memoryless computation framework. We then quantify the gains obtained by using
memory: this leads to shorter programs and allows us to use only binary
instructions, which is not sufficient in general when no memory is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6030</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6030</id><created>2011-11-25</created><updated>2011-12-06</updated><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>An image processing of a Raphael's portrait of Leonardo</title><categories>cs.CV</categories><comments>Image processing. Portrait. Self-portrait. Leonardo da Vinci.
  Raphael. Raffaello Sanzio Images revised using a high-quality image of
  Raphael's Plato</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In one of his paintings, the School of Athens, Raphael is depicting Leonardo
da Vinci as the philosopher Plato. Some image processing tools can help us in
comparing this portrait with two Leonardo's portraits, considered as
self-portraits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6052</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6052</id><created>2011-11-25</created><updated>2012-03-01</updated><authors><author><keyname>Fehr</keyname><forenames>Serge</forenames></author><author><keyname>Gelles</keyname><forenames>Ran</forenames></author><author><keyname>Schaffner</keyname><forenames>Christian</forenames></author></authors><title>Security and Composability of Randomness Expansion from Bell
  Inequalities</title><categories>quant-ph cs.CR</categories><comments>12 pages, v3: significant changes: security is proven against
  adversaries holding only classical side information</comments><doi>10.1103/PhysRevA.87.012335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nonlocal behavior of quantum mechanics can be used to generate guaranteed
fresh randomness from an untrusted device that consists of two nonsignalling
components; since the generation process requires some initial fresh randomness
to act as a catalyst, one also speaks of randomness expansion. Colbeck and Kent
proposed the first method for generating randomness from untrusted devices,
however, without providing a rigorous analysis. This was addressed subsequently
by Pironio et al. [Nature 464 (2010)], who aimed at deriving a lower bound on
the min-entropy of the data extracted from an untrusted device, based only on
the observed non-local behavior of the device. Although that article succeeded
in developing important tools towards the acquired goal, it failed in putting
the tools together in a rigorous and correct way, and the given formal claim on
the guaranteed amount of min-entropy needs to be revisited. In this paper we
show how to combine the tools provided by Pironio et al., as to obtain a
meaningful and correct lower bound on the min-entropy of the data produced by
an untrusted device, based on the observed non-local behavior of the device.
Our main result confirms the essence of the improperly formulated claims of
Pironio et al., and puts them on solid ground. We also address the question of
composability and show that different untrusted devices can be composed in an
alternating manner under the assumption that they are not entangled. This
enables for superpolynomial randomness expansion based on two untrusted yet
unentangled devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6053</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6053</id><created>2011-11-25</created><authors><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author></authors><title>Testing the fairness of citation indicators for comparison across
  scientific domains: the case of fractional citation counts</title><categories>physics.soc-ph cs.DL</categories><comments>8 pages, 6 figures, 1 table</comments><journal-ref>J. Informetr. 6, 121-130 (2012)</journal-ref><doi>10.1016/j.joi.2011.09.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation numbers are extensively used for assessing the quality of scientific
research. The use of raw citation counts is generally misleading, especially
when applied to cross-disciplinary comparisons, since the average number of
citations received is strongly dependent on the scientific discipline of
reference of the paper. Measuring and eliminating biases in citation patterns
is crucial for a fair use of citation numbers. Several numerical indicators
have been introduced with this aim, but so far a specific statistical test for
estimating the fairness of these numerical indicators has not been developed.
Here we present a statistical method aimed at estimating the effectiveness of
numerical indicators in the suppression of citation biases. The method is
simple to implement and can be easily generalized to various scenarios. As a
practical example we test, in a controlled case, the fairness of fractional
citation count, which has been recently proposed as a tool for cross-discipline
comparison. We show that this indicator is not able to remove biases in
citation patterns and performs much worse than the rescaling of citation counts
with average values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6074</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6074</id><created>2011-11-25</created><authors><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author><author><keyname>Ahnert</keyname><forenames>Sebastian E.</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author></authors><title>Flavor network and the principles of food pairing</title><categories>physics.soc-ph cs.SI</categories><comments>39 pages, 15 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cultural diversity of culinary practice, as illustrated by the variety of
regional cuisines, raises the question of whether there are any general
patterns that determine the ingredient combinations used in food today or
principles that transcend individual tastes and recipes. We introduce a flavor
network that captures the flavor compounds shared by culinary ingredients.
Western cuisines show a tendency to use ingredient pairs that share many flavor
compounds, supporting the so-called food pairing hypothesis. By contrast, East
Asian cuisines tend to avoid compound sharing ingredients. Given the increasing
availability of information on food preparation, our data-driven investigation
opens new avenues towards a systematic understanding of culinary practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6082</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6082</id><created>2011-11-25</created><updated>2012-09-27</updated><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author></authors><title>Trading Regret for Efficiency: Online Convex Optimization with Long Term
  Constraints</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a framework for solving constrained online convex
optimization problem. Our motivation stems from the observation that most
algorithms proposed for online convex optimization require a projection onto
the convex set $\mathcal{K}$ from which the decisions are made. While for
simple shapes (e.g. Euclidean ball) the projection is straightforward, for
arbitrary complex sets this is the main computational challenge and may be
inefficient in practice. In this paper, we consider an alternative online
convex optimization problem. Instead of requiring decisions belong to
$\mathcal{K}$ for all rounds, we only require that the constraints which define
the set $\mathcal{K}$ be satisfied in the long run. We show that our framework
can be utilized to solve a relaxed version of online learning with side
constraints addressed in \cite{DBLP:conf/colt/MannorT06} and
\cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an online
convex-concave optimization problem, we propose an efficient algorithm which
achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and
$\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then we
modify the algorithm in order to guarantee that the constraints are satisfied
in the long run. This gain is achieved at the price of getting
$\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based on
the Mirror Prox method \citep{nemirovski-2005-prox} to solve variational
inequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound for
both regret and the violation of constraints when the domain $\K$ can be
described by a finite number of linear constraints. Finally, we extend the
result to the setting where we only have partial access to the convex set
$\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the same
bounds in expectation as our first algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6084</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6084</id><created>2011-11-25</created><authors><author><keyname>Bonifati</keyname><forenames>Angela</forenames></author><author><keyname>Summa</keyname><forenames>Gianvito</forenames></author><author><keyname>Pacitti</keyname><forenames>Esther</forenames></author><author><keyname>Draidi</keyname><forenames>Fady</forenames></author></authors><title>Semantic Query Reformulation in Social PDMS</title><categories>cs.DB cs.SI</categories><comments>29 pages, 8 figures, query rewriting in PDMS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider social peer-to-peer data management systems (PDMS), where each
peer maintains both semantic mappings between its schema and some
acquaintances, and social links with peer friends. In this context,
reformulating a query from a peer's schema into other peer's schemas is a hard
problem, as it may generate as many rewritings as the set of mappings from that
peer to the outside and transitively on, by eventually traversing the entire
network. However, not all the obtained rewritings are relevant to a given
query. In this paper, we address this problem by inspecting semantic mappings
and social links to find only relevant rewritings. We propose a new notion of
'relevance' of a query with respect to a mapping, and, based on this notion, a
new semantic query reformulation approach for social PDMS, which achieves great
accuracy and flexibility. To find rapidly the most interesting mappings, we
combine several techniques: (i) social links are expressed as FOAF (Friend of a
Friend) links to characterize peer's friendship and compact mapping summaries
are used to obtain mapping descriptions; (ii) local semantic views are special
views that contain information about external mappings; and (iii) gossiping
techniques improve the search of relevant mappings. Our experimental
evaluation, based on a prototype on top of PeerSim and a simulated network
demonstrate that our solution yields greater recall, compared to traditional
query translation approaches proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6087</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6087</id><created>2011-11-25</created><authors><author><keyname>Almeida</keyname><forenames>Paulo S&#xe9;rgio</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author><author><keyname>Cunha</keyname><forenames>Alcino</forenames></author></authors><title>Fast Distributed Computation of Distances in Networks</title><categories>cs.DC cs.NI cs.SI</categories><comments>12 pages</comments><journal-ref>IEEE 51st Annual Conference on Decision and Control (2012),
  5215-5220</journal-ref><doi>10.1109/CDC.2012.6426872</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a distributed algorithm to simultaneously compute the
diameter, radius and node eccentricity in all nodes of a synchronous network.
Such topological information may be useful as input to configure other
algorithms. Previous approaches have been modular, progressing in sequential
phases using building blocks such as BFS tree construction, thus incurring
longer executions than strictly required. We present an algorithm that, by
timely propagation of available estimations, achieves a faster convergence to
the correct values. We show local criteria for detecting convergence in each
node. The algorithm avoids the creation of BFS trees and simply manipulates
sets of node ids and hop counts. For the worst scenario of variable start
times, each node i with eccentricity ecc(i) can compute: the node eccentricity
in diam(G)+ecc(i)+2 rounds; the diameter in 2*diam(G)+ecc(i)+2 rounds; and the
radius in diam(G)+ecc(i)+2*radius(G) rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6115</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6115</id><created>2011-11-25</created><authors><author><keyname>Nishikawa</keyname><forenames>Takashi</forenames></author><author><keyname>Motter</keyname><forenames>Adilson E.</forenames></author></authors><title>Discovering Network Structure Beyond Communities</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI nlin.AO</categories><comments>Software implementing the method described in the paper is available
  at http://purl.oclc.org/net/find_structural_groups and is accompanied by a
  demo video available at
  http://www.nature.com/srep/2011/111109/srep00151/extref/srep00151-s2.mov</comments><journal-ref>T. Nishikawa and A.E. Motter, Scientific Reports 1, 151 (2011)</journal-ref><doi>10.1038/srep00151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To understand the formation, evolution, and function of complex systems, it
is crucial to understand the internal organization of their interaction
networks. Partly due to the impossibility of visualizing large complex
networks, resolving network structure remains a challenging problem. Here we
overcome this difficulty by combining the visual pattern recognition ability of
humans with the high processing speed of computers to develop an exploratory
method for discovering groups of nodes characterized by common network
properties, including but not limited to communities of densely connected
nodes. Without any prior information about the nature of the groups, the method
simultaneously identifies the number of groups, the group assignment, and the
properties that define these groups. The results of applying our method to real
networks suggest the possibility that most group structures lurk undiscovered
in the fast-growing inventory of social, biological, and technological networks
of scientific interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6116</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6116</id><created>2011-11-25</created><authors><author><keyname>Gray</keyname><forenames>Norman</forenames></author><author><keyname>Mann</keyname><forenames>Robert G</forenames></author><author><keyname>Morris</keyname><forenames>Dave</forenames></author><author><keyname>Holliman</keyname><forenames>Mark</forenames></author><author><keyname>Noddle</keyname><forenames>Keith</forenames></author></authors><title>AstroDAbis: Annotations and Cross-Matches for Remote Catalogues</title><categories>astro-ph.IM cs.DL cs.IR</categories><comments>4 pages, 1 figure, to appear in Proceedings of ADASS XXI, Paris, 2011</comments><acm-class>J.2; H.2.8; H.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomers are good at sharing data, but poorer at sharing knowledge.
  Almost all astronomical data ends up in open archives, and access to these is
being simplified by the development of the global Virtual Observatory (VO).
This is a great advance, but the fundamental problem remains that these
archives contain only basic observational data, whereas all the astrophysical
interpretation of that data -- which source is a quasar, which a low-mass star,
and which an image artefact -- is contained in journal papers, with very little
linkage back from the literature to the original data archives. It is therefore
currently impossible for an astronomer to pose a query like &quot;give me all
sources in this data archive that have been identified as quasars&quot; and this
limits the effective exploitation of these archives, as the user of an archive
has no direct means of taking advantage of the knowledge derived by its
previous users.
  The AstroDAbis service aims to address this, in a prototype service enabling
astronomers to record annotations and cross-identifications in the AstroDAbis
service, annotating objects in other catalogues. We have deployed two
interfaces to the annotations, namely one astronomy-specific one using the TAP
protocol}, and a second exploiting generic Linked Open Data (LOD) and RDF
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6117</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6117</id><created>2011-11-25</created><authors><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Principles of Solomonoff Induction and AIXI</title><categories>cs.AI</categories><comments>14 LaTeX pages</comments><journal-ref>Proc. Solomonoff 85th Memorial Conference (SOL 2011) pages 386-398</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify principles characterizing Solomonoff Induction by demands on an
agent's external behaviour. Key concepts are rationality, computability,
indifference and time consistency. Furthermore, we discuss extensions to the
full AI case to derive AIXI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6121</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6121</id><created>2011-11-25</created><authors><author><keyname>Mu&#xf1;oz</keyname><forenames>Roberto</forenames></author><author><keyname>Barr&#xed;a</keyname><forenames>Marta</forenames></author><author><keyname>Rusu</keyname><forenames>Cristian</forenames></author></authors><title>Virtual Worlds as a Support to Engineering Teaching</title><categories>cs.CY</categories><comments>XIII Chilean Congress on Higher Education in Computer Science,
  CCESC'2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual Worlds (VWs) are an emerging technology used by a growing number of
educational institutions around the world. It is an environment, a way of
learning and an educational tool that allows different levels of online
interaction. In the course &quot;Programming I&quot;, of the career Informatics
Engineering at Universidad de Valpara\'iso, we conducted a pilot experience
with the VW of Second Life, in order to evaluate the potential of using VWs in
the teaching practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6149</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6149</id><created>2011-11-26</created><authors><author><keyname>Murthy</keyname><forenames>G. Rama</forenames></author></authors><title>Optimal Organizational Hierarchies: Source Coding: Disaster Relief</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ulticasting is an important communication paradigm for enabling the
dissemination of information selectively. This paper considers the problem of
optimal secure multicasting in a communication network captured through a graph
(optimal is in an interesting sense) and provides a doubly optimal solution
using results from source coding. It is realized that the solution leads to
optimal design (in a well defined optimality sense) of organizational
hierarchies captured through a graph. In this effort two novel concepts :
prefix free path, graph entropy are introduced. Some results of graph entropy
are provided. Also some results on Kraft inequality are discussed. As an
application Hierarchical Hybrid Communication Network is utilized as a model of
structured Mobile Adhoc network for utility in Disaster Management. Several new
research problems that naturally emanate from this research are summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6156</identifier>
 <datestamp>2012-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6156</id><created>2011-11-26</created><updated>2012-08-09</updated><authors><author><keyname>Kuniavsky</keyname><forenames>Sergey</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames></author></authors><title>Greediness and Equilibrium in Congestion Games</title><categories>cs.GT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rosenthal (1973) introduced the class of congestion games and proved that
they always possess a Nash equilibrium in pure strategies. Fotakis et al.
(2005) introduce the notion of a greedy strategy tuple, where players
sequentially and irrevocably choose a strategy that is a best response to the
choice of strategies by former players. Whereas the former solution concept is
driven by strong assumptions on the rationality of the players and the common
knowledge thereof, the latter assumes very little rationality on the players'
behavior. From Fotakis \cite{fotakis10} it follows that for Tree Representable
congestion Games greedy behavior leads to a NE. In this paper we obtain
necessary and sufficient conditions for the equivalence of these two solution
concepts. Such equivalence enhances the viability of these concepts as
realistic outcomes of the environment. The conditions for such equivalence to
emerge for monotone symmetric games is that the strategy set has a tree-form,
or equivalently is a `extension-parallel graph'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6174</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6174</id><created>2011-11-26</created><authors><author><keyname>Bickel</keyname><forenames>David R.</forenames></author></authors><title>Resolving conflicts between statistical methods by probability
  combination: Application to empirical Bayes analyses of genomic data</title><categories>stat.ME cs.IT math.IT math.ST q-bio.QM stat.TH</categories><journal-ref>D. R. Bickel, Game-theoretic probability combination with
  applications to resolving conflicts between statistical methods,
  International Journal of Approximate Reasoning 53, 880-891 (2012)</journal-ref><doi>10.1016/j.ijar.2012.04.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the typical analysis of a data set, a single method is selected for
statistical reporting even when equally applicable methods yield very different
results. Examples of equally applicable methods can correspond to those of
different ancillary statistics in frequentist inference and of different prior
distributions in Bayesian inference. More broadly, choices are made between
parametric and nonparametric methods and between frequentist and Bayesian
methods.
  Rather than choosing a single method, it can be safer, in a game-theoretic
sense, to combine those that are equally appropriate in light of the available
information. Since methods of combining subjectively assessed probability
distributions are not objective enough for that purpose, this paper introduces
a method of distribution combination that does not require any assignment of
distribution weights. It does so by formalizing a hedging strategy in terms of
a game between three players: nature, a statistician combining distributions,
and a statistician refusing to combine distributions. The optimal move of the
first statistician reduces to the solution of a simpler problem of selecting an
estimating distribution that minimizes the Kullback-Leibler loss maximized over
the plausible distributions to be combined. The resulting combined distribution
is a linear combination of the most extreme of the distributions to be combined
that are scientifically plausible. The optimal weights are close enough to each
other that no extreme distribution dominates the others.
  The new methodology is illustrated by combining conflicting empirical Bayes
methodologies in the context of gene expression data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6188</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6188</id><created>2011-11-26</created><updated>2013-03-20</updated><authors><author><keyname>Lin</keyname><forenames>Fu</forenames></author><author><keyname>Fardad</keyname><forenames>Makan</forenames></author><author><keyname>Jovanovi&#x107;</keyname><forenames>Mihailo R.</forenames></author></authors><title>Design of Optimal Sparse Feedback Gains via the Alternating Direction
  Method of Multipliers</title><categories>math.OC cs.SY</categories><comments>To appear in IEEE Trans. Automat. Control</comments><journal-ref>IEEE Trans. Automat. Control (2013), vol. 58, no. 9, pp. 2426-2431</journal-ref><doi>10.1109/TAC.2013.2257618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design sparse and block sparse feedback gains that minimize the variance
amplification (i.e., the $H_2$ norm) of distributed systems. Our approach
consists of two steps. First, we identify sparsity patterns of feedback gains
by incorporating sparsity-promoting penalty functions into the optimal control
problem, where the added terms penalize the number of communication links in
the distributed controller. Second, we optimize feedback gains subject to
structural constraints determined by the identified sparsity patterns. In the
first step, the sparsity structure of feedback gains is identified using the
alternating direction method of multipliers, which is a powerful algorithm
well-suited to large optimization problems. This method alternates between
promoting the sparsity of the controller and optimizing the closed-loop
performance, which allows us to exploit the structure of the corresponding
objective functions. In particular, we take advantage of the separability of
the sparsity-promoting penalty functions to decompose the minimization problem
into sub-problems that can be solved analytically. Several examples are
provided to illustrate the effectiveness of the developed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6191</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6191</id><created>2011-11-26</created><authors><author><keyname>Bringmann</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Nijssen</keyname><forenames>Siegfried</forenames></author><author><keyname>Zimmermann</keyname><forenames>Albrecht</forenames></author></authors><title>Pattern-Based Classification: A Unifying Perspective</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of patterns in predictive models is a topic that has received a lot
of attention in recent years. Pattern mining can help to obtain models for
structured domains, such as graphs and sequences, and has been proposed as a
means to obtain more accurate and more interpretable models. Despite the large
amount of publications devoted to this topic, we believe however that an
overview of what has been accomplished in this area is missing. This paper
presents our perspective on this evolving area. We identify the principles of
pattern mining that are important when mining patterns for models and provide
an overview of pattern-based classification methods. We categorize these
methods along the following dimensions: (1) whether they post-process a
pre-computed set of patterns or iteratively execute pattern mining algorithms;
(2) whether they select patterns model-independently or whether the pattern
selection is guided by a model. We summarize the results that have been
obtained for each of these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6201</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6201</id><created>2011-11-26</created><updated>2013-02-24</updated><authors><author><keyname>Kao</keyname><forenames>Yi-Hao</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Learning a Factor Model via Regularized PCA</title><categories>cs.LG stat.ML</categories><journal-ref>Machine Learning, Volume 91, Number 3, pp. 279-303 (2013)</journal-ref><doi>10.1007/s10994-013-5345-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a linear factor model. We propose a
regularized form of principal component analysis (PCA) and demonstrate through
experiments with synthetic and real data the superiority of resulting estimates
to those produced by pre-existing factor analysis approaches. We also establish
theoretical results that explain how our algorithm corrects the biases induced
by conventional approaches. An important feature of our algorithm is that its
computational requirements are similar to those of PCA, which enjoys wide use
in large part due to its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6214</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6214</id><created>2011-11-26</created><authors><author><keyname>Ibrahimi</keyname><forenames>Morteza</forenames></author><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Kanoria</keyname><forenames>Yashodhan</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Robust Max-Product Belief Propagation</title><categories>cs.CE cs.LG math.OC</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of optimizing a graph-structured objective function
under \emph{adversarial} uncertainty. This problem can be modeled as a
two-persons zero-sum game between an Engineer and Nature. The Engineer controls
a subset of the variables (nodes in the graph), and tries to assign their
values to maximize an objective function. Nature controls the complementary
subset of variables and tries to minimize the same objective. This setting
encompasses estimation and optimization problems under model uncertainty, and
strategic problems with a graph structure. Von Neumann's minimax theorem
guarantees the existence of a (minimax) pair of randomized strategies that
provide optimal robustness for each player against its adversary.
  We prove several structural properties of this strategy pair in the case of
graph-structured payoff function. In particular, the randomized minimax
strategies (distributions over variable assignments) can be chosen in such a
way to satisfy the Markov property with respect to the graph. This
significantly reduces the problem dimensionality. Finally we introduce a
message passing algorithm to solve this minimax problem. The algorithm
generalizes max-product belief propagation to this new domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6218</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6218</id><created>2011-11-26</created><authors><author><keyname>Sampath</keyname><forenames>Amritha</forenames></author><author><keyname>C</keyname><forenames>Tripti.</forenames></author><author><keyname>Thampi</keyname><forenames>Sabu M.</forenames></author></authors><title>An ACO Algorithm for Effective Cluster Head Selection</title><categories>cs.NI cs.DC</categories><comments>7 pages, 5 figures, International Journal of Advances in Information
  Technology (JAIT); ISSN: 1798-2340; Academy Publishers, Finland</comments><journal-ref>International Journal of Advances in Information Technology
  (JAIT), Vol. 2, No. 1, February 2011, pp. 50-56</journal-ref><doi>10.4304/jait.2.1.50-56</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an effective algorithm for selecting cluster heads in
mobile ad hoc networks using ant colony optimization. A cluster in an ad hoc
network consists of a cluster head and cluster members which are at one hop
away from the cluster head. The cluster head allocates the resources to its
cluster members. Clustering in MANET is done to reduce the communication
overhead and thereby increase the network performance. A MANET can have many
clusters in it. This paper presents an algorithm which is a combination of the
four main clustering schemes- the ID based clustering, connectivity based,
probability based and the weighted approach. An Ant colony optimization based
approach is used to minimize the number of clusters in MANET. This can also be
considered as a minimum dominating set problem in graph theory. The algorithm
considers various parameters like the number of nodes, the transmission range
etc. Experimental results show that the proposed algorithm is an effective
methodology for finding out the minimum number of cluster heads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6223</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6223</id><created>2011-11-26</created><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Garcia</keyname><forenames>Alfredo</forenames></author><author><keyname>Garzas</keyname><forenames>J. Joaquin Escudero</forenames></author><author><keyname>Garcia-Armada</keyname><forenames>Ana</forenames></author></authors><title>Lower Bounds Optimization for Coordinated Linear Transmission Beamformer
  Design in Multicell Network Downlink</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the coordinated downlink beamforming problem in a cellular
network with the base stations (BSs) equipped with multiple antennas, and with
each user equipped with a single antenna. The BSs cooperate in sharing their
local interference information, and they aim at maximizing the sum rate of the
users in the network. A set of new lower bounds (one bound for each BS) of the
non-convex sum rate is identified. These bounds facilitate the development of a
set of algorithms that allow the BSs to update their beams by optimizing their
respective lower bounds. We show that when there is a single user per-BS, the
lower bound maximization problem can be solved exactly with rank-1 solutions.
In this case, the overall sum rate maximization problem can be solved to a KKT
point. Numerical results show that the proposed algorithms achieve high system
throughput with reduced backhaul information exchange among the BSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6224</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6224</id><created>2011-11-26</created><authors><author><keyname>Hwang</keyname><forenames>Hsien-Kuei</forenames></author><author><keyname>Tsai</keyname><forenames>Tsung-Hsi</forenames></author><author><keyname>Chen</keyname><forenames>Wei-Mei</forenames></author></authors><title>Threshold phenomena in k-dominant skylines of random samples</title><categories>cs.DS</categories><comments>38 pages, 4 figures</comments><msc-class>60C05, 68P15, 60F20, 68Q25, 82B26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skylines emerged as a useful notion in database queries for selecting
representative groups in multivariate data samples for further decision making,
multi-objective optimization or data processing, and the $k$-dominant skylines
were naturally introduced to resolve the abundance of skylines when the
dimensionality grows or when the coordinates are negatively correlated. We
prove in this paper that the expected number of $k$-dominant skylines is
asymptotically zero for large samples when $1\le k\le d-1$ under two reasonable
(continuous) probability assumptions of the input points, $d$ being the
(finite) dimensionality, in contrast to the asymptotic unboundedness when
$k=d$. In addition to such an asymptotic zero-infinity property, we also
establish a sharp threshold phenomenon for the expected ($d-1$)-dominant
skylines when the dimensionality is allowed to grow with $n$. Several related
issues such as the dominant cycle structures and numerical aspects, are also
briefly studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6237</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6237</id><created>2011-11-27</created><updated>2012-05-01</updated><authors><author><keyname>Han</keyname><forenames>Xuebing</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Gang</forenames></author></authors><title>Fast Algorithms for Sparse Recovery with Perturbed Dictionary</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we account for approaches of sparse recovery from large
underdetermined linear models with perturbation present in both the
measurements and the dictionary matrix. Existing methods have high computation
and low efficiency. The total least-squares (TLS) criterion has well-documented
merits in solving linear regression problems while FOCal Underdetermined System
Solver (FOCUSS) has low-computation complexity in sparse recovery. Based on TLS
and FOCUSS methods, the present paper develops more fast and robust algorithms,
TLS-FOCUSS and SD-FOCUSS. TLS-FOCUSS algorithm is not only near-optimum but
also fast in solving TLS optimization problems under sparsity constraints, and
thus fit for large scale computation. In order to reduce the complexity of
algorithm further, another suboptimal algorithm named D-FOCUSS is devised.
SD-FOCUSS can be applied in MMV (multiple-measurement-vectors) TLS model, which
fills the gap of solving linear regression problems under sparsity constraints.
The convergence of TLS-FOCUSS algorithm and SD-FOCUSS algorithm is established
with mathematical proof. The simulations illustrate the advantage of TLS-FOCUSS
and SD-FOCUSS in accuracy and stability, compared with other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6244</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6244</id><created>2011-11-27</created><authors><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Tzachar</keyname><forenames>Nir</forenames></author></authors><title>Efficient and Universal Corruption Resilient Fountain Codes</title><categories>cs.IT cs.CR math.IT</categories><comments>23 pages, 5 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new family of fountain codes which overcome
adversarial errors. That is, we consider the possibility that some portion of
the arriving packets of a rateless erasure code are corrupted in an
undetectable fashion. In practice, the corrupted packets may be attributed to a
portion of the communication paths which are controlled by an adversary or to a
portion of the sources that are malicious.
  The presented codes resemble and extend LT and Raptor codes. Yet, their
benefits over existing coding schemes are manifold. First, to overcome the
corrupted packets, our codes use information theoretic techniques, rather than
cryptographic primitives. Thus, no secret channel between the senders and the
receivers is required. Second, the encoders in the suggested scheme are
oblivious to the strength of the adversary, yet perform as if its strength was
known in advance. Third, the sparse structure of the codes facilitates
efficient decoding. Finally, the codes easily fit a decentralized scenario with
several sources, when no communication between the sources is allowed.
  We present both exhaustive as well as efficient decoding rules. Beyond the
obvious use as a rateless codes, our codes have important applications in
distributed computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6265</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6265</id><created>2011-11-27</created><authors><author><keyname>Lawto</keyname><forenames>Julien</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Gauvain</keyname><forenames>Jean-Luc</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Lamel</keyname><forenames>Lori</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Grefenstete</keyname><forenames>Gregory</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gravier</keyname><forenames>Guillaume</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Despres</keyname><forenames>Julien</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Guinaudeau</keyname><forenames>Camille</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>S&#xe9;billot</keyname><forenames>Pascale</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>A Scalable Video Search Engine Based on Audio Content Indexing and Topic
  Segmentation</title><categories>cs.MM</categories><comments>NEM Summit, Torino : Italy (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important class of online videos is that of news broadcasts. Most news
organisations provide near-immediate access to topical news broadcasts over the
Internet, through RSS streams or podcasts. Until lately, technology has not
made it possible for a user to automatically go to the smaller parts, within a
longer broadcast, that might interest them. Recent advances in both speech
recognition systems and natural language processing have led to a number of
robust tools that allow us to provide users with quicker, more focussed access
to relevant segments of one or more news broadcast videos. Here we present our
new interface for browsing or searching news broadcasts (video/audio) that
exploits these new language processing tools to (i) provide immediate access to
topical passages within news broadcasts, (ii) browse news broadcasts by events
as well as by people, places and organisations, (iii) perform cross lingual
search of news broadcasts, (iv) search for news through a map interface, (v)
browse news by trending topics, and (vi) see automatically-generated textual
clues for news segments, before listening. Our publicly searchable demonstrator
currently indexes daily broadcast news content from 50 sources in English,
French, Chinese, Arabic, Spanish, Dutch and Russian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6276</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6276</id><created>2011-11-27</created><updated>2011-11-28</updated><authors><author><keyname>Kolev</keyname><forenames>Vasil</forenames></author></authors><title>Compressed sensing of astronomical images: orthogonal wavelets domains</title><categories>cs.CV astro-ph.IM physics.data-an</categories><comments>8 pages, The ACM proceedings of CompSysTech 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple approach for orthogonal wavelets in compressed sensing (CS)
applications is presented. We compare efficient algorithm for different
orthogonal wavelet measurement matrices in CS for image processing from scanned
photographic plates (SPP). Some important characteristics were obtained for
astronomical image processing of SPP. The best orthogonal wavelet choice for
measurement matrix construction in CS for image compression of images of SPP is
given. The image quality measure for linear and nonlinear image compression
method is defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6278</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6278</id><created>2011-11-27</created><updated>2012-03-08</updated><authors><author><keyname>Neves</keyname><forenames>Jorge</forenames></author><author><keyname>Pinto</keyname><forenames>Maria Vaz</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Vanishing ideals over graphs and even cycles</title><categories>math.AC cs.IT math.AG math.CO math.IT</categories><msc-class>13P25, 14G50, 94B27</msc-class><journal-ref>Comm. Algebra 43 (2015), no. 3, 1050--1075</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let X be an algebraic toric set in a projective space over a finite field. We
study the vanishing ideal, I(X), of X and show some useful degree bounds for a
minimal set of generators of I(X). We give an explicit description of a set of
generators of I(X), when X is the algebraic toric set associated to an even
cycle or to a connected bipartite graph with pairwise disjoint even cycles. In
this case, a fomula for the regularity of I(X) is given. We show an upper bound
for this invariant, when X is associated to a (not necessarily connected)
bipartite graph. The upper bound is sharp if the graph is connected. We are
able to show a formula for the length of the parameterized linear code
associated with any graph, in terms of the number of bipartite and
non-bipartite components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6285</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6285</id><created>2011-11-27</created><updated>2011-12-11</updated><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author><author><keyname>Legendre</keyname><forenames>Pierre</forenames></author></authors><title>Ward's Hierarchical Clustering Method: Clustering Criterion and
  Agglomerative Algorithm</title><categories>stat.ML cs.CV stat.AP</categories><comments>20 pages, 21 citations, 4 figures</comments><msc-class>62H30, 91C20</msc-class><acm-class>G.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ward error sum of squares hierarchical clustering method has been very
widely used since its first description by Ward in a 1963 publication. It has
also been generalized in various ways. However there are different
interpretations in the literature and there are different implementations of
the Ward agglomerative algorithm in commonly used software systems, including
differing expressions of the agglomerative criterion. Our survey work and case
studies will be useful for all those involved in developing software for data
analysis using Ward's hierarchical clustering method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6289</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6289</id><created>2011-11-27</created><updated>2013-03-10</updated><authors><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author><author><keyname>Lu</keyname><forenames>Hsiao-feng</forenames></author><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author></authors><title>Inverse Determinant Sums and Connections Between Fading Channel
  Information Theory and Algebra</title><categories>cs.IT math.IT math.NT math.RA</categories><comments>A draft version of the paper submitted to Transactions on Information
  Theory. This is a completely rewritten version of the previous one. Minor
  fixes and some clarification added to Section 4 C</comments><acm-class>H.1.1</acm-class><journal-ref>IEEE Transactions on Information Theory, vol 59, pp. 6060 - 6082,
  September 2013</journal-ref><doi>10.1109/TIT.2013.2266396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concentrates on the study of inverse determinant sums, which arise
from the union bound on the error probability, as a tool for designing and
analyzing algebraic space-time block codes.
  A general framework to study these sums is established, and the connection
between asymptotic growth of inverse determinant sums and the
diversity-multiplexing gain trade-off is investigated. It is proven that the
growth of the inverse determinant sum of a division algebra-based space-time
code is completely determined by the growth of the unit group. This reduces the
inverse determinant sum analysis to studying certain asymptotic integrals in
Lie groups.
  Using recent methods from ergodic theory, a complete classification of the
inverse determinant sums of the most well known algebraic space-time codes is
provided. The approach reveals an interesting and tight relation between
diversity-multiplexing gain trade-off and point counting in Lie groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6321</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6321</id><created>2011-11-27</created><authors><author><keyname>Lozev</keyname><forenames>Kamen</forenames></author></authors><title>Shape and Trajectory Tracking of Moving Obstacles</title><categories>cs.GR cs.DS math.OC</categories><comments>22 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents new methods and algorithms for tracking the shape and
trajectory of moving reflecting obstacles with broken rays, or rays reflecting
at an obstacle. While in tomography the focus of the reconstruction method is
to recover the velocity structure of the domain, the shape and trajectory
reconstruction procedure directly finds the shape and trajectory of the
obstacle. The physical signal carrier for this innovative method are ultrasonic
beams. When the speed of sound is constant, the rays are straight line segments
and the shape and trajectory of moving objects will be reconstructed with
methods based on the travel time equation and ellipsoid geometry. For variable
speed of sound, we start with the eikonal equation and a system of differential
equations that has its origins in acoustics and seismology. In this case, the
rays are curves that are not necessarily straight line segments and we develop
algorithms for shape and trajectory tracking based on the numerical solution of
these equations. We present methods and algorithms for shape and trajectory
tracking of moving obstacles with reflected rays when the location of the
receiver of the reflected ray is not known in advance. The shape and trajectory
tracking method is very efficient because it is not necessary for the reflected
signal to traverse the whole domain or the same path back to the transmitter.
It could be received close to the point of reflection or far away from the
transmitter. This optimizes the energy spent by transmitters for tracking the
object, reduces signal attenuation and improves image resolution. It is a safe
and secure method. We also present algorithms for tracking the shape and
trajectory of absorbing obstacles. The new methods and algorithms for shape and
trajectory tracking enable new applications and an application to one-hop
Internet routing is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6324</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6324</id><created>2011-11-27</created><authors><author><keyname>Turk</keyname><forenames>Ata</forenames></author><author><keyname>Aykanat</keyname><forenames>Cevdet</forenames></author><author><keyname>Demirci</keyname><forenames>G. Vehbi</forenames></author><author><keyname>von Alfthan</keyname><forenames>Sebastian</forenames></author><author><keyname>Honkonen</keyname><forenames>Ilja</forenames></author></authors><title>Improving the Load Balancing Performance of Vlasiator</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This whitepaper describes the load-balancing performance issues that are
observed and tackled during the petascaling of the Vlasiator codes. Vlasiator
is a Vlasov-hybrid simulation code developed in Finnish Meteorological
Institute (FMI). Vlasiator models the communications associated with the
spatial grid operated on as a hypergraph and partitions the grid using the
parallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning
framework. The result of partitioning determines the distribution of grid cells
to processors. It is observed that the partitioning phase takes a substantial
percentage of the overall computation time. Alternative
(graph-partitioning-based) schemes that perform almost as well as the
hypergraph partitioning scheme and that require less preprocessing overhead and
better balance are proposed and investigated. A comparison in terms of effect
on running time, preprocessing overhead and load-balancing quality of Zoltan's
PHG, ParMeTiS, and PT-SCOTCH are presented. Test results on J\&quot;uelich
BlueGene/P cluster are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6334</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6334</id><created>2011-11-27</created><authors><author><keyname>McKilliam</keyname><forenames>Robby</forenames></author><author><keyname>Subramanian</keyname><forenames>Ramanan</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author><author><keyname>Clarkson</keyname><forenames>I. Vaughan L.</forenames></author></authors><title>On the error performance of the $A_n$ lattices</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the root lattice $A_n$ and derive explicit formulae for the
moments of its Voronoi cell. We then show that these formulae enable accurate
prediction of the error probability of lattice codes constructed from $A_n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6337</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6337</id><created>2011-11-27</created><updated>2012-06-13</updated><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>Regret Bound by Variation for Online Convex Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In citep{Hazan-2008-extract}, the authors showed that the regret of online
linear optimization can be bounded by the total variation of the cost vectors.
In this paper, we extend this result to general online convex optimization. We
first analyze the limitations of the algorithm in \citep{Hazan-2008-extract}
when applied it to online convex optimization. We then present two algorithms
for online convex optimization whose regrets are bounded by the variation of
cost functions. We finally consider the bandit setting, and present a
randomized algorithm for online bandit convex optimization with a
variation-based regret bound. We show that the regret bound for online bandit
convex optimization is optimal when the variation of cost functions is
independent of the number of trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6349</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6349</id><created>2011-11-28</created><authors><author><keyname>Sayed</keyname><forenames>Awny</forenames></author></authors><title>XML Information Retrieval Systems: A Survey</title><categories>cs.IR</categories><comments>10 pages, 25 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continuous growth in the XML information repositories has been matched by
increasing efforts in development of XML retrieval systems, in large parts
aiming at supporting content-oriented XML retrieval. These systems exploit the
available structural information, as market up in XML documents, in order to
return documents components- the so called XML elements-instead of the
complement documents in repose to the user query. In this paper, we provide an
overview of the different XML information retrieval systems and classify them
according to their storage and query evaluation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6372</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6372</id><created>2011-11-28</created><updated>2012-05-05</updated><authors><author><keyname>Taneja</keyname><forenames>Inder Jeet</forenames></author></authors><title>Nested Inequalities Among Divergence Measures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have considered a single inequality having 11 known
divergence measures. This inequality include measures like:
Jeffryes-Kullback-Leiber J-divergence, Jensen-Shannon divergence (Burbea-Rao,
1982), arithmetic-geometric mean divergence (Taneja, 1995), Hellinger
discrimination, symmetric chi-square divergence, triangular discrimination,
etc. All these measures are well-known in the literature on Information theory
and Statistics. This sequence of 11 measures also include measures due to Kumar
and Johnson (2005) and Jain and Srivastava (2007). Three measures arising due
to some mean divergences also appears in this inequality. Based on non-negative
differences arising due to this single inequality of 11 measures, we have put
more than 40 divergence measures in nested or sequential form. Idea of reverse
inequalities is also introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6374</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6374</id><created>2011-11-28</created><updated>2012-06-17</updated><authors><author><keyname>Aliaga</keyname><forenames>Jos&#xe9; I.</forenames><affiliation>Depto. de Ingenier&#xed;a y Ciencia de Computadores, Universidad Jaume I</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>RWTH-Aachen University</affiliation></author><author><keyname>Davidovi&#x107;</keyname><forenames>Davor</forenames><affiliation>Institut Ruder Boskov&#xed;c, Centarza Informatiku i Racunarstvo</affiliation></author><author><keyname>Di Napoli</keyname><forenames>Edoardo</forenames><affiliation>JSC, Forschungszentrum J&#xfc;lich</affiliation></author><author><keyname>Igual</keyname><forenames>Francisco D.</forenames><affiliation>Depto. de Ingenier&#xed;a y Ciencia de Computadores, Universidad Jaume I</affiliation></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames><affiliation>Depto. de Ingenier&#xed;a y Ciencia de Computadores, Universidad Jaume I</affiliation></author></authors><title>Solving Dense Generalized Eigenproblems on Multi-threaded Architectures</title><categories>cs.PF cond-mat.mtrl-sci cs.DC cs.MS</categories><comments>5 tables and 4 figures. In press by Applied Mathematics and
  Computation. Accepted version</comments><doi>10.1016/j.amc.2012.05.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare two approaches to compute a portion of the spectrum of dense
symmetric definite generalized eigenproblems: one is based on the reduction to
tridiagonal form, and the other on the Krylov-subspace iteration. Two
large-scale applications, arising in molecular dynamics and material science,
are employed to investigate the contributions of the application, architecture,
and parallelism of the method to the performance of the solvers. The
experimental results on a state-of-the-art 8-core platform, equipped with a
graphics processing unit (GPU), reveal that in real applications, iterative
Krylov-subspace methods can be a competitive approach also for the solution of
dense problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6387</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6387</id><created>2011-11-28</created><authors><author><keyname>Kassimi</keyname><forenames>My Abdellah</forenames></author><author><keyname>beqqali</keyname><forenames>Omar El</forenames></author></authors><title>3D Model Retrieval Based on Semantic and Shape Indexes</title><categories>cs.IR cs.AI cs.CV</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 8, Issue
  3, May 2011</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The size of 3D models used on the web or stored in databases is becoming
increasingly high. Then, an efficient method that allows users to find similar
3D objects for a given 3D model query has become necessary. Keywords and the
geometry of a 3D model cannot meet the needs of users' retrieval because they
do not include the semantic information. In this paper, a new method has been
proposed to 3D models retrieval using semantic concepts combined with shape
indexes. To obtain these concepts, we use the machine learning methods to label
3D models by k-means algorithm in measures and shape indexes space. Moreover,
semantic concepts have been organized and represented by ontology language OWL
and spatial relationships are used to disambiguate among models of similar
appearance. The SPARQL query language has been used to question the information
displayed in this language and to compute the similarity between two 3D models.
We interpret our results using the Princeton Shape Benchmark Database and the
results show the performance of the proposed new approach to retrieval 3D
models. Keywords: 3D Model, 3D retrieval, measures, shape indexes, semantic,
ontology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6401</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6401</id><created>2011-11-28</created><authors><author><keyname>Elmaghraoui</keyname><forenames>Hajar</forenames></author><author><keyname>Zaoui</keyname><forenames>Imane</forenames></author><author><keyname>Chiadmi</keyname><forenames>Dalila</forenames></author><author><keyname>Benhlima</keyname><forenames>Laila</forenames></author></authors><title>Graph based E-Government web service composition</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, e-government has emerged as a government policy to improve the
quality and efficiency of public administrations. By exploiting the potential
of new information and communication technologies, government agencies are
providing a wide spectrum of online services. These services are composed of
several web services that comply with well defined processes. One of the big
challenges is the need to optimize the composition of the elementary web
services. In this paper, we present a solution for optimizing the computation
effort in web service composition. Our method is based on Graph Theory. We
model the semantic relationship between the involved web services through a
directed graph. Then, we compute all shortest paths using for the first time,
an extended version of the Floyd-Warshall algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6414</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6414</id><created>2011-11-28</created><authors><author><keyname>Goff</keyname><forenames>St&#xe9;phane Y. Le</forenames></author></authors><title>Capacity-Approaching Signal Constellations for the Additive Exponential
  Noise Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new family of signal constellations, called log constellations,
that can be used to design near-capacity coded modulation schemes over additive
exponential noise (AEN) channels. Log constellations are designed by
geometrically approximating the input distribution that maximizes the AEN
channel capacity. The mutual information achievable over AEN channels with both
coded modulation (CM) and bit-interleaved coded modulation (BICM) approaches is
evaluated for various signal sets. In the case of CM, the proposed log
constellations outperform, sometimes by over half a decibel, the best existing
signal sets available from the literature, and can display error performance
within only 0.12 dB of the AEN channel capacity. In the context of BICM, log
constellations do not offer significant performance advantages over the best
existing constellations. As the potential performance degradation resulting
from the use of BICM instead of CM is larger than 1 dB, BICM may however not be
a suitable design approach over AEN channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6431</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6431</id><created>2011-11-28</created><authors><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Trachtenberg</keyname><forenames>Ari</forenames></author></authors><title>Unique decodability of bigram counts by finite automata</title><categories>cs.FL</categories><comments>12 pages</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of deciding whether a given string is uniquely
decodable from its bigram counts by means of a finite automaton. An efficient
algorithm for constructing a polynomial-size nondeterministic finite automaton
that decides unique decodability is given. Conversely, we show that the minimum
deterministic finite automaton for deciding unique decodability has at least
exponentially many states in alphabet size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6453</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6453</id><created>2011-11-28</created><updated>2013-10-08</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Learning with Submodular Functions: A Convex Optimization Perspective</title><categories>cs.LG math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular functions are relevant to machine learning for at least two
reasons: (1) some problems may be expressed directly as the optimization of
submodular functions and (2) the lovasz extension of submodular functions
provides a useful set of regularization functions for supervised and
unsupervised learning. In this monograph, we present the theory of submodular
functions from a convex analysis perspective, presenting tight links between
certain polyhedra, combinatorial optimization and convex optimization problems.
In particular, we show how submodular function minimization is equivalent to
solving a wide variety of convex optimization problems. This allows the
derivation of new efficient algorithms for approximate and exact submodular
function minimization with theoretical guarantees and good practical
performance. By listing many examples of submodular functions, we review
various applications to machine learning, such as clustering, experimental
design, sensor placement, graphical model structure learning or subset
selection, as well as a family of structured sparsity-inducing norms that can
be derived and used from submodular functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6465</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6465</id><created>2011-11-28</created><authors><author><keyname>Vriend</keyname><forenames>Willem-Jan</forenames></author><author><keyname>Valentijn</keyname><forenames>Edwin A.</forenames></author><author><keyname>Belikov</keyname><forenames>Andrey</forenames></author><author><keyname>Kleijn</keyname><forenames>Gijs A. Verdoes</forenames></author></authors><title>Astro-WISE Information System</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, Proc. of ADASS XXI, ASP Conference Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astro-WISE is a scientific information system for the data processing of
optical images. In this paper we review main features of Astro-WISE and
describe the current status of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6473</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6473</id><created>2011-11-28</created><authors><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>Salakoski</keyname><forenames>Tapio</forenames></author><author><keyname>Stock</keyname><forenames>Michiel</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author></authors><title>A kernel-based framework for learning graded relations from data</title><categories>stat.ML cs.LG</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><doi>10.1109/TFUZZ.2012.2194151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by a large number of potential applications in areas like
bioinformatics, information retrieval and social network analysis, the problem
setting of inferring relations between pairs of data objects has recently been
investigated quite intensively in the machine learning community. To this end,
current approaches typically consider datasets containing crisp relations, so
that standard classification methods can be adopted. However, relations between
objects like similarities and preferences are often expressed in a graded
manner in real-world applications. A general kernel-based framework for
learning relations from data is introduced here. It extends existing approaches
because both crisp and graded relations are considered, and it unifies existing
approaches because different types of graded relations can be modeled,
including symmetric and reciprocal relations. This framework establishes
important links between recent developments in fuzzy set theory and machine
learning. Its usefulness is demonstrated through various experiments on
synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6502</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6502</id><created>2011-11-28</created><authors><author><keyname>Erkal</keyname><forenames>Hakan</forenames></author><author><keyname>Ozcelik</keyname><forenames>F. Mehmet</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author></authors><title>Optimal Offline Broadcast Scheduling with an Energy Harvesting
  Transmitter</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an energy harvesting transmitter broadcasting data to two
receivers. Energy and data arrivals are assumed to occur at arbitrary but known
instants. The goal is to minimize the total transmission time of the packets
arriving within a certain time window, using the energy that becomes available
during this time. An achievable rate region with structural properties
satisfied by the two-user AWGN BC capacity region is assumed. Structural
properties of power and rate allocation in an optimal policy are established,
as well as the uniqueness of the optimal policy under the condition that all
the data of the &quot;weaker&quot; user are available at the beginning. An iterative
algorithm, DuOpt, based on block coordinate descent that achieves the same
structural properties as the optimal is described. Investigating the ways to
have the optimal schedule of two consecutive epochs in terms of energy
efficiency and minimum transmission duration, it has been shown that DuOpt
achieves best performance under the same special condition of uniqueness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6519</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6519</id><created>2011-11-28</created><authors><author><keyname>Lingas</keyname><forenames>Andrzej</forenames></author><author><keyname>Sledneu</keyname><forenames>Dzmitry</forenames></author></authors><title>A Combinatorial Algorithm for All-Pairs Shortest Paths in Directed
  Vertex-Weighted Graphs with Applications to Disc Graphs</title><categories>cs.DS</categories><doi>10.1007/978-3-642-27660-6_31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing all-pairs shortest paths in a directed
graph with real weights assigned to vertices.
  For an $n\times n$ 0-1 matrix $C,$ let $K_{C}$ be the complete weighted graph
on the rows of $C$ where the weight of an edge between two rows is equal to
their Hamming distance. Let $MWT(C)$ be the weight of a minimum weight spanning
tree of $K_{C}.$
  We show that the all-pairs shortest path problem for a directed graph $G$ on
$n$ vertices with nonnegative real weights and adjacency matrix $A_G$ can be
solved by a combinatorial randomized algorithm in time
$$\widetilde{O}(n^{2}\sqrt {n + \min\{MWT(A_G), MWT(A_G^t)\}})$$
  As a corollary, we conclude that the transitive closure of a directed graph
$G$ can be computed by a combinatorial randomized algorithm in the
aforementioned time. $\widetilde{O}(n^{2}\sqrt {n + \min\{MWT(A_G),
MWT(A_G^t)\}})$
  We also conclude that the all-pairs shortest path problem for uniform disk
graphs, with nonnegative real vertex weights, induced by point sets of bounded
density within a unit square can be solved in time $\widetilde{O}(n^{2.75})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6539</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6539</id><created>2011-11-28</created><authors><author><keyname>sookhak</keyname><forenames>Mehdi</forenames></author><author><keyname>Karimi</keyname><forenames>Ramin</forenames></author><author><keyname>Haghparast</keyname><forenames>Mahboobeh</forenames></author><author><keyname>ISnin</keyname><forenames>Ismail Fauzi</forenames></author></authors><title>Secure Geographic Routing Protocols: Issues and Approaches</title><categories>cs.NI</categories><comments>8 pages</comments><journal-ref>International Journal of Computer Science Issues 8(4): 382-389
  (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the years, routing protocols in wireless sensor networks (WSN) have been
substantially investigated by researches. Most state-of-the-art surveys have
focused on reviewing of wireless sensor network .In this paper we review the
existing secure geographic routing protocols for wireless sensor network (WSN)
and also provide a qualitative comparison of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6549</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6549</id><created>2011-11-28</created><authors><author><keyname>Albrecht</keyname><forenames>Martin R.</forenames></author><author><keyname>Bard</keyname><forenames>Gregory V.</forenames></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames></author></authors><title>Efficient Dense Gaussian Elimination over the Finite Field with Two
  Elements</title><categories>cs.MS math.AC</categories><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we describe an efficient implementation of a hierarchy of
algorithms for Gaussian elimination upon dense matrices over the field with two
elements. We discuss both well-known and new algorithms as well as our
implementations in the M4RI library, which has been adopted into Sage. The
focus of our discussion is a block iterative algorithm for PLE decomposition
which is inspired by the M4RI algorithm. The implementation presented in this
work provides considerable performance gains in practice when compared to the
previously fastest implementation. We provide performance figures on x86_64
CPUs to demonstrate the alacrity of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6552</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6552</id><created>2011-11-28</created><updated>2014-01-24</updated><authors><author><keyname>Bouasker</keyname><forenames>Souad</forenames></author><author><keyname>Hamrouni</keyname><forenames>Tarek</forenames></author><author><keyname>Yahia</keyname><forenames>Sadok Ben</forenames></author></authors><title>Nouvelle repr\'esentation concise exacte des motifs corr\'el\'es rares :
  Application \`a la d\'etection d'intrusions</title><categories>cs.DB</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlated rare pattern mining is an interesting issue in Data mining. In
this respect, the set of correlated rare patterns w.r.t. to the bond
correlation measure was studied in a recent work, in which the RCPR concise
exact representation of the set of correlated rare patterns was proposed.
However, none algorithm was proposed in order to mine this representation and
none experiment was carried out to evaluate it. In this paper, we introduce the
new RcprMiner algorithm allowing an efficient extraction of RCPR. We also
present the IsRCP algorithm allowing the query of the RCPR representation in
addition to the RCPRegeneration algorithm allowing the regeneration of the
whole set RCP of rare correlated patterns starting from this representation.
The carried out experiments highlight interesting compactness rates offered by
RCPR. The effectiveness of the proposed classification method, based on generic
rare correlated association rules derived from RCPR, has also been proved in
the context of intrusion detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6553</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6553</id><created>2011-11-28</created><authors><author><keyname>P&#xf6;schko</keyname><forenames>Jan</forenames></author></authors><title>Exploring Twitter Hashtags</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter messages often contain so-called hashtags to denote keywords related
to them. Using a dataset of 29 million messages, I explore relations among
these hashtags with respect to co-occurrences. Furthermore, I present an
attempt to classify hashtags into five intuitive classes, using a
machine-learning approach. The overall outcome is an interactive Web
application to explore Twitter hashtags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6561</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6561</id><created>2011-11-28</created><authors><author><keyname>An</keyname><forenames>Hyung-Chan</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>A Diameter-Revealing Proof of the Bondy-Lov\'asz Lemma</title><categories>cs.DM</categories><comments>5 pages, 1 figures</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strengthened version of a lemma due to Bondy and Lov\'asz. This
lemma establishes the connectivity of a certain graph whose nodes correpond to
the spanning trees of a 2-vertex-connected graph, and implies the k=2 case of
the Gy\H{o}ri-Lov\'asz Theorem on partitioning of k-vertex-connected graphs.
Our strengthened version constructively proves an asymptotically tight O(|V|^2)
bound on the worst-case diameter of this graph of spanning trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6563</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6563</id><created>2011-11-28</created><authors><author><keyname>Sangari</keyname><forenames>Arash</forenames></author><author><keyname>Mirkia</keyname><forenames>Hasti</forenames></author><author><keyname>Assadi</keyname><forenames>Amir H.</forenames></author></authors><title>Perception of Motion and Architectural Form: Computational Relationships
  between Optical Flow and Perspective</title><categories>q-bio.NC cs.NE</categories><comments>10 pages, 13 figures, submitted and accepted in DoCEIS'2012
  Conference: http://www.uninova.pt/doceis/doceis12/home/home.php</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perceptual geometry refers to the interdisciplinary research whose objectives
focuses on study of geometry from the perspective of visual perception, and in
turn, applies such geometric findings to the ecological study of vision.
Perceptual geometry attempts to answer fundamental questions in perception of
form and representation of space through synthesis of cognitive and biological
theories of visual perception with geometric theories of the physical world.
Perception of form, space and motion are among fundamental problems in vision
science. In cognitive and computational models of human perception, the
theories for modeling motion are treated separately from models for perception
of form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6583</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6583</id><created>2011-11-27</created><updated>2012-05-12</updated><authors><author><keyname>Ketcheson</keyname><forenames>David I.</forenames></author><author><keyname>Mandli</keyname><forenames>Kyle T.</forenames></author><author><keyname>Ahmadia</keyname><forenames>Aron</forenames></author><author><keyname>Alghamdi</keyname><forenames>Amal</forenames></author><author><keyname>Quezada</keyname><forenames>Manuel</forenames></author><author><keyname>Parsani</keyname><forenames>Matteo</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Emmett</keyname><forenames>Matthew</forenames></author></authors><title>PyClaw: Accessible, Extensible, Scalable Tools for Wave Propagation
  Problems</title><categories>math.NA cs.DC cs.MS physics.comp-ph</categories><journal-ref>SISC 34(4):C210-C231 (2012)</journal-ref><doi>10.1137/110856976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of scientific software involves tradeoffs between ease of use,
generality, and performance. We describe the design of a general hyperbolic PDE
solver that can be operated with the convenience of MATLAB yet achieves
efficiency near that of hand-coded Fortran and scales to the largest
supercomputers. This is achieved by using Python for most of the code while
employing automatically-wrapped Fortran kernels for computationally intensive
routines, and using Python bindings to interface with a parallel computing
library and other numerical packages. The software described here is PyClaw, a
Python-based structured grid solver for general systems of hyperbolic PDEs
\cite{pyclaw}. PyClaw provides a powerful and intuitive interface to the
algorithms of the existing Fortran codes Clawpack and SharpClaw, simplifying
code development and use while providing massive parallelism and scalable
solvers via the PETSc library. The package is further augmented by use of
PyWENO for generation of efficient high-order weighted essentially
non-oscillatory reconstruction code. The simplicity, capability, and
performance of this approach are demonstrated through application to example
problems in shallow water flow, compressible flow and elasticity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6616</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6616</id><created>2011-11-28</created><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Macpherson</keyname><forenames>Dugald</forenames></author><author><keyname>Thapper</keyname><forenames>Johan</forenames></author></authors><title>Constraint Satisfaction Tractability from Semi-lattice Operations on
  Infinite Sets</title><categories>cs.CC cs.DM math.LO</categories><comments>20 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A famous result by Jeavons, Cohen, and Gyssens shows that every constraint
satisfaction problem (CSP) where the constraints are preserved by a
semi-lattice operation can be solved in polynomial time. This is one of the
basic facts for the so-called universal-algebraic approach to a systematic
theory of tractability and hardness in finite domain constraint satisfaction.
  Not surprisingly, the theorem of Jeavons et al. fails for arbitrary infinite
domain CSPs. Many CSPs of practical interest, though, and in particular those
CSPs that are motivated by qualitative reasoning calculi from Artificial
Intelligence, can be formulated with constraint languages that are rather
well-behaved from a model-theoretic point of view. In particular, the
automorphism group of these constraint languages tends to be large in the sense
that the number of orbits of n-subsets of the automorphism group is bounded by
some function in n.
  In this paper we present a generalization of the theorem by Jeavons et al. to
infinite domain CSPs where the number of orbits of n-subsets grows
sub-exponentially in n, and prove that preservation under a semi-lattice
operation for such CSPs implies polynomial-time tractability. Unlike the result
of Jeavons et al., this includes many CSPs that cannot be solved by Datalog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6631</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6631</id><created>2011-11-28</created><updated>2012-10-10</updated><authors><author><keyname>Sangari</keyname><forenames>Arash</forenames></author><author><keyname>Ardalan</keyname><forenames>Adel</forenames></author><author><keyname>Lambe</keyname><forenames>Larry</forenames></author><author><keyname>Eghbalnia</keyname><forenames>Hamid</forenames></author><author><keyname>Assadi</keyname><forenames>Amir H.</forenames></author></authors><title>Mathematical Analysis and Computational Integration of Massive
  Heterogeneous Data from the Human Retina</title><categories>q-bio.QM cs.IR math.SP</categories><comments>9 pages, 3 figures, submitted and accepted in Damor2012 conference:
  http://www.uninova.pt/damor2012/index.php?page=authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern epidemiology integrates knowledge from heterogeneous collections of
data consisting of numerical, descriptive and imaging. Large-scale
epidemiological studies use sophisticated statistical analysis, mathematical
models using differential equations and versatile analytic tools that handle
numerical data. In contrast, knowledge extraction from images and descriptive
information in the form of text and diagrams remain a challenge for most
fields, in particular, for diseases of the eye. In this article we provide a
roadmap towards extraction of knowledge from text and images with focus on
forthcoming applications to epidemiological investigation of retinal diseases,
especially from existing massive heterogeneous collections of data distributed
around the globe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6640</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6640</id><created>2011-11-28</created><authors><author><keyname>Hand</keyname><forenames>Scott</forenames></author></authors><title>A Markov Random Field Topic Space Model for Document Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel statistical approach to intelligent document
retrieval. It seeks to offer a more structured and extensible mathematical
approach to the term generalization done in the popular Latent Semantic
Analysis (LSA) approach to document indexing. A Markov Random Field (MRF) is
presented that captures relationships between terms and documents as
probabilistic dependence assumptions between random variables. From there, it
uses the MRF-Gibbs equivalence to derive joint probabilities as well as local
probabilities for document variables. A parameter learning method is proposed
that utilizes rank reduction with singular value decomposition in a matter
similar to LSA to reduce dimensionality of document-term relationships to that
of a latent topic space. Experimental results confirm the ability of this
approach to effectively and efficiently retrieve documents from substantial
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6661</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6661</id><created>2011-11-28</created><authors><author><keyname>Hassan</keyname><forenames>A. H.</forenames></author><author><keyname>Fluke</keyname><forenames>C. J.</forenames></author><author><keyname>Barnes</keyname><forenames>D. G.</forenames></author></authors><title>Unleashing the Power of Distributed CPU/GPU Architectures: Massive
  Astronomical Data Analysis and Visualization case study</title><categories>astro-ph.IM cs.DC</categories><comments>4 Pages, 1 figures, To appear in the proceedings of ADASS XXI, ed.
  P.Ballester and D.Egret, ASP Conf. Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Upcoming and future astronomy research facilities will systematically
generate terabyte-sized data sets moving astronomy into the Petascale data era.
While such facilities will provide astronomers with unprecedented levels of
accuracy and coverage, the increases in dataset size and dimensionality will
pose serious computational challenges for many current astronomy data analysis
and visualization tools. With such data sizes, even simple data analysis tasks
(e.g. calculating a histogram or computing data minimum/maximum) may not be
achievable without access to a supercomputing facility.
  To effectively handle such dataset sizes, which exceed today's single machine
memory and processing limits, we present a framework that exploits the
distributed power of GPUs and many-core CPUs, with a goal of providing data
analysis and visualizing tasks as a service for astronomers. By mixing shared
and distributed memory architectures, our framework effectively utilizes the
underlying hardware infrastructure handling both batched and real-time data
analysis and visualization tasks. Offering such functionality as a service in a
&quot;software as a service&quot; manner will reduce the total cost of ownership, provide
an easy to use tool to the wider astronomical community, and enable a more
optimized utilization of the underlying hardware infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6664</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6664</id><created>2011-11-28</created><updated>2014-03-30</updated><authors><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Kwon</keyname><forenames>Seokbeop</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Generalized Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Signal Process., vol. 60, no. 12, pp. 6202-6216, Dec.
  2012</journal-ref><doi>10.1109/TSP.2012.2218810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a greedy algorithm to recover sparse signals from compressed measurements,
orthogonal matching pursuit (OMP) algorithm has received much attention in
recent years. In this paper, we introduce an extension of the OMP for pursuing
efficiency in reconstructing sparse signals. Our approach, henceforth referred
to as generalized OMP (gOMP), is literally a generalization of the OMP in the
sense that multiple $N$ indices are identified per iteration. Owing to the
selection of multiple ''correct'' indices, the gOMP algorithm is finished with
much smaller number of iterations when compared to the OMP. We show that the
gOMP can perfectly reconstruct any $K$-sparse signals ($K &gt; 1$), provided that
the sensing matrix satisfies the RIP with $\delta_{NK} &lt;
\frac{\sqrt{N}}{\sqrt{K} + 3 \sqrt{N}}$. We also demonstrate by empirical
simulations that the gOMP has excellent recovery performance comparable to
$\ell_1$-minimization technique with fast processing speed and competitive
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6677</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6677</id><created>2011-11-28</created><authors><author><keyname>Fang</keyname><forenames>Chengfang</forenames></author><author><keyname>Chang</keyname><forenames>Ee-Chien</forenames></author></authors><title>Publishing Location Dataset Differential Privately with Isotonic
  Regression</title><categories>cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of publishing location datasets, in particular 2D
spatial pointsets, in a differentially private manner. Many existing mechanisms
focus on frequency counts of the points in some a priori partition of the
domain that is difficult to determine. We propose an approach that adds noise
directly to the point, or to a group of neighboring points. Our approach is
based on the observation that, the sensitivity of sorting, as a function on
sets of real numbers, can be bounded. Together with isotonic regression, the
dataset can be accurately reconstructed. To extend the mechanism to higher
dimension, we employ locality preserving function to map the dataset to a
bounded interval. Although there are fundamental limits on the performance of
locality preserving functions, fortunately, our problem only requires distance
preservation in the &quot;easier&quot; direction, and the well-known Hilbert
space-filling curve suffices to provide high accuracy. The publishing process
is simple from the publisher's point of view: the publisher just needs to map
the data, sort them, group them, add Laplace noise and publish the dataset. The
only parameter to determine is the group size which can be chosen based on
predicted generalization errors. Empirical study shows that the published
dataset can also exploited to answer other queries, for example, range query
and median query, accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6682</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6682</id><created>2011-11-28</created><updated>2013-02-25</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A General Robust Linear Transceiver Design for Multi-Hop
  Amplify-and-Forward MIMO Relaying Systems</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures, Accepted by IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, linear transceiver design for multi-hop amplify-and-forward
(AF) multiple-input multiple-out (MIMO) relaying systems with Gaussian
distributed channel estimation errors is investigated. Commonly used
transceiver design criteria including weighted mean-square-error (MSE)
minimization, capacity maximization, worst-MSE/MAX-MSE minimization and
weighted sum-rate maximization, are considered and unified into a single
matrix-variate optimization problem. A general robust design algorithm is
proposed to solve the unified problem. Specifically, by exploiting majorization
theory and properties of matrix-variate functions, the optimal structure of the
robust transceiver is derived when either the covariance matrix of channel
estimation errors seen from the transmitter side or the corresponding
covariance matrix seen from the receiver side is proportional to an identity
matrix. Based on the optimal structure, the original transceiver design
problems are reduced to much simpler problems with only scalar variables whose
solutions are readily obtained by iterative water-filling algorithm. A number
of existing transceiver design algorithms are found to be special cases of the
proposed solution. The differences between our work and the existing related
work are also discussed in detail. The performance advantages of the proposed
robust designs are demonstrated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6685</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6685</id><created>2011-11-28</created><authors><author><keyname>Chiang</keyname><forenames>Chun-Ying</forenames></author><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Li</keyname><forenames>Bo-Jr</forenames></author><author><keyname>Wu</keyname><forenames>Jiaojiao</forenames></author><author><keyname>Yeh</keyname><forenames>Hong-Gwa</forenames></author></authors><title>Some Results on the Target Set Selection Problem</title><categories>math.CO cs.CC cs.DM cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a fundamental problem in the area of viral
marketing, called T{\scriptsize ARGET} S{\scriptsize ET} S{\scriptsize
ELECTION} problem. We study the problem when the underlying graph is a
block-cactus graph, a chordal graph or a Hamming graph. We show that if $G$ is
a block-cactus graph, then the T{\scriptsize ARGET} S{\scriptsize ET}
S{\scriptsize ELECTION} problem can be solved in linear time, which generalizes
Chen's result \cite{chen2009} for trees, and the time complexity is much better
than the algorithm in \cite{treewidth} (for bounded treewidth graphs) when
restricted to block-cactus graphs. We show that if the underlying graph $G$ is
a chordal graph with thresholds $\theta(v)\leq 2$ for each vertex $v$ in $G$,
then the problem can be solved in linear time. For a Hamming graph $G$ having
thresholds $\theta(v)=2$ for each vertex $v$ of $G$, we precisely determine an
optimal target set $S$ for $(G,\theta)$. These results partially answer an open
problem raised by Dreyer and Roberts \cite{Dreyer2009}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6689</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6689</id><created>2011-11-28</created><authors><author><keyname>Khabbazian</keyname><forenames>Majid</forenames></author><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author><author><keyname>Haghnegahdar</keyname><forenames>Alireza</forenames></author></authors><title>Bounding Interference in Wireless Ad Hoc Networks with Nodes in Random
  Position</title><categories>cs.CG cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interference at a wireless node s can be modelled by the number of
wireless nodes whose transmission ranges cover s. Given a set of positions for
wireless nodes, the interference minimization problem is to assign a
transmission radius (equivalently, a power level) to each node such that the
resulting communication graph is connected, while minimizing the maximum
interference. We consider the model introduced by von Rickenback et al. (2005),
in which each transmission range is represented by a ball and edges in the
communication graph are symmetric. The problem is NP-complete in two dimensions
(Buchin 2008) and no polynomial-time approximation algorithm is known.
Furthermore, even in one dimension (the highway model), the problem's
complexity is unknown and the maximum interference of a set of n wireless nodes
can be as high as Theta(sqrt(n)) (von Rickenback et al. 2005). In this paper we
show how to solve the problem efficiently in settings typical for wireless ad
hoc networks. In particular, we show that if node positions are represented by
a set P of n points selected uniformly and independently at random over a
d-dimensional rectangular region, for any fixed d, then the topology given by
the closure of the Euclidean minimum spanning tree of P has maximum
interference O(log n) with high probability. We extend this bound to a general
class of communication graphs over a broad set of probability distributions.
Next we present a local algorithm that constructs a graph from this class; this
is the first local algorithm to provide an upper bound on the expected maximum
interference. Finally, we discuss an empirical evaluation of our algorithm with
a suite of simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6691</identifier>
 <datestamp>2012-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6691</id><created>2011-11-28</created><updated>2012-01-04</updated><authors><author><keyname>Sunny</keyname><forenames>Albert</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author><author><keyname>Sahasrabudhe</keyname><forenames>Nachiket</forenames></author></authors><title>Approximate Aggregate Utility Maximization in Multi-Hop Wireless
  Networks using Distributed Greedy Scheduling</title><categories>cs.NI</categories><comments>Because this paper has been submitted at a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of greedy scheduling in multihop
wireless networks, where the objective is aggregate utility maximization.
Following standard approaches, we consider the dual of the original
optimization problem. The dual can be solved optimally, only with the knowledge
of the maximal independent sets in the network. But computation of maximal
independent sets is known to be NP-hard. Motivated by this, we propose a
distributed greedy heuristic to address the problem of link scheduling. We
evaluate the effect of the distributed greedy heuristic on aggregate utility
maximization in detail, for the case of an arbitrary graph. We provide some
insights into the factors affecting aggregate utility maximization in a
network, by providing bounds on the same. We give simulation results for the
approximate aggregate utility maximization achieved under distributed
implementation of the greedy heuristic and find them close to the maximum
aggregate utility obtained using optimal scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6695</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6695</id><created>2011-11-29</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author><author><keyname>Khoshnevis</keyname><forenames>Behrouz</forenames></author></authors><title>Optimal Shape-Gain Quantization for Multiuser MIMO Systems with Linear
  Precoding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the optimal bit allocation for shape-gain vector
quantization of wireless channels in multiuser (MU) multiple-input
multiple-output (MIMO) downlink systems based on linear precoding. Our design
minimizes the mean squared-error between the original and quantized channels
through optimal bit allocation across shape (direction) and gain (magnitude)
for a fixed feedback overhead per user. This is shown to significantly reduce
the quantization error, which in turn, decreases the MU interference. This
paper makes three main contributions: first, we focus on channel gain
quantization and derive the quantization distortion, based on a Euclidean
distance measure, corresponding to singular values of a MIMO channel. Second,
we show that the Euclidean distance-based distortion of a unit norm complex
channel, due to shape quantization, is proportional to \frac{2^{-2Bs}}{2M-1},
where, Bs is the number of shape quantization bits and M is the number of
transmit antennas. Finally, we show that for channels in complex space and
allowing for a large feedback overhead, the number of direction quantization
bits should be approximately (2M - 1) times the number of channel magnitude
quantization bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6698</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6698</id><created>2011-11-29</created><updated>2011-12-02</updated><authors><author><keyname>Hajiaghayi</keyname><forenames>Mohammad Taghi</forenames></author><author><keyname>Li</keyname><forenames>Shi</forenames></author></authors><title>On the Integrality Gap of the Directed-Component Relaxation for Steiner
  Tree</title><categories>cs.DS</categories><comments>This paper has been withdrawn due to a crucial error in Lemma 4. In
  Lemma 4, additional property for $\{Y'_j\}$ is needed to guarantee that the
  family of distributions exists. However, we can not guarantee the condition
  for every iteration</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that the integrality gap of the $k$-Directed-Component-
Relaxation($k$-DCR) LP for the Steiner tree problem, introduced by Byrka,
Grandoni, Rothvob and Sanita (STOC 2010), is at most $\ln(4)&lt;1.39$. The proof
is constructive: we can efficiently find a Steiner tree whose cost is at most
$\ln(4)$ times the cost of the optimal fractional $k$-restricted Steiner tree
given by the $k$-DCR LP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6713</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6713</id><created>2011-11-29</created><authors><author><keyname>Tolba</keyname><forenames>Ahmed</forenames></author><author><keyname>Eladawi</keyname><forenames>Nabila</forenames></author><author><keyname>Elmogy</keyname><forenames>Mohammed</forenames></author></authors><title>An Enhanced Indexing And Ranking Technique On The Semantic Web</title><categories>cs.AI</categories><comments>8 pages, 7 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 3, 2011, 118-125</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the fast growth of the Internet, more and more information is available
on the Web. The Semantic Web has many features which cannot be handled by using
the traditional search engines. It extracts metadata for each discovered Web
documents in RDF or OWL formats, and computes relations between documents. We
proposed a hybrid indexing and ranking technique for the Semantic Web which
finds relevant documents and computes the similarity among a set of documents.
First, it returns with the most related document from the repository of
Semantic Web Documents (SWDs) by using a modified version of the ObjectRank
technique. Then, it creates a sub-graph for the most related SWDs. Finally, It
returns the hubs and authorities of these document by using the HITS algorithm.
Our technique increases the quality of the results and decreases the execution
time of processing the user's query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6727</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6727</id><created>2011-11-29</created><authors><author><keyname>Bamatraf</keyname><forenames>Abdullah</forenames></author><author><keyname>Ibrahim</keyname><forenames>Rosziati</forenames></author><author><keyname>Salleh</keyname><forenames>Mohd. Najib Mohd.</forenames></author></authors><title>A New Digital Watermarking Algorithm Using Combination of Least
  Significant Bit (LSB) and Inverse Bit</title><categories>cs.MM</categories><comments>8 pages, 6 figures and 4 tables; Journal of Computing, Volume 3,
  Issue 4, April 2011, ISSN 2151-9617</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we introduce a new digital watermarking algorithm using least
significant bit (LSB). LSB is used because of its little effect on the image.
This new algorithm is using LSB by inversing the binary values of the watermark
text and shifting the watermark according to the odd or even number of pixel
coordinates of image before embedding the watermark. The proposed algorithm is
flexible depending on the length of the watermark text. If the length of the
watermark text is more than ((MxN)/8)-2 the proposed algorithm will also embed
the extra of the watermark text in the second LSB. We compare our proposed
algorithm with the 1-LSB algorithm and Lee's algorithm using Peak
signal-to-noise ratio (PSNR). This new algorithm improved its quality of the
watermarked image. We also attack the watermarked image by using cropping and
adding noise and we got good results as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6730</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6730</id><created>2011-11-29</created><authors><author><keyname>Farid</keyname><forenames>Humaira</forenames></author><author><keyname>Azam</keyname><forenames>Farooque</forenames></author><author><keyname>Iqbal</keyname><forenames>M. Aqeel</forenames></author></authors><title>Minimizing the Risk of Architectural Decay by using Architecture-Centric
  Evolution Process</title><categories>cs.SE</categories><comments>12 Pages 8 Figures Journal Paper; International Journal of Computer
  Science, Engineering and Applications (IJCSEA) Vol.1, No.5, October 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software systems endure many noteworthy changes throughout their life-cycle
in order to follow the evolution of the problem domains. Generally, the
software system architecture cannot follow the rapid evolution of a problem
domain which results in the discrepancies between the implemented and designed
architecture. Software architecture illustrates a system's structure and global
properties and consequently determines not only how the system should be
constructed but also leads its evolution. Architecture plays an important role
to ensure that a system satisfies its business and mission goals during
implementation and evolution. However, the capabilities of the designed
architecture may possibly be lost when the implementation does not conform to
the designed architecture. Such a loss of consistency causes the risk of
architectural decay. The architectural decay can be avoided if architectural
changes are made as early as possible. The paper presents the Process Model for
Architecture-Centric Evolution which improves the quality of software systems
through maintaining consistency between designed architecture and
implementation. It also increases architecture awareness of developers which
assists in minimizing the risk of architectural decay. In the proposed approach
consistency checks are performed before and after the change implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6735</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6735</id><created>2011-11-29</created><authors><author><keyname>K</keyname><forenames>Hareesh.</forenames></author><author><keyname>H</keyname><forenames>Manjaiah D.</forenames></author></authors><title>Peer-to-Peer Live Streaming and Video On Demand Design Issues and its
  Challenges</title><categories>cs.NI cs.MM</categories><comments>International Journal of Peer to Peer Networks (IJP2P), Vol.2, No.4,
  October 2011</comments><doi>10.5121/ijp2p.2011.2401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer Live streaming and Video on Demand is the most popular media
applications over the Internet in recent years. These systems reduce the load
on the server and provide a scalable content distribution. A new paradigm of
P2P network collaborates to build large distributed video applications on
existing networks .But, the problem of designing the system are at par with the
P2P media streaming, live and Video on demand systems. Hence a comprehensive
design comparison is needed to build such kind of system architecture.
Therefore, in this paper we elaborately studied the traditional approaches for
P2P streaming architectures, and its critical design issues, as well as
practicable challenges. Thus, our studies in this paper clearly point the
tangible design issues and its challenges, and other intangible issues for
providing P2P VoD services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6745</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6745</id><created>2011-11-29</created><updated>2013-01-28</updated><authors><author><keyname>Feldmann</keyname><forenames>Andreas Emil</forenames></author></authors><title>Fast Balanced Partitioning is Hard, Even on Grids and Trees</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two kinds of approximation algorithms exist for the k-BALANCED PARTITIONING
problem: those that are fast but compute unsatisfying approximation ratios, and
those that guarantee high quality ratios but are slow. In this paper we prove
that this tradeoff between runtime and solution quality is necessary. For the
problem a minimum number of edges in a graph need to be found that, when cut,
partition the vertices into k equal-sized sets. We develop a reduction
framework which identifies some necessary conditions on the considered graph
class in order to prove the hardness of the problem. We focus on two
combinatorially simple but very different classes, namely trees and solid grid
graphs. The latter are finite connected subgraphs of the infinite 2D grid
without holes. First we use the framework to show that for solid grid graphs it
is NP-hard to approximate the optimum number of cut edges within any satisfying
ratio. Then we consider solutions in which the sets may deviate from being
equal-sized. Our framework is used on grids and trees to prove that no fully
polynomial time algorithm exists that computes solutions in which the sets are
arbitrarily close to equal-sized. This is true even if the number of edges cut
is allowed to increase the more stringent the limit on the set sizes is. These
are the first bicriteria inapproximability results for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6756</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6756</id><created>2011-11-29</created><authors><author><keyname>Baghdadi</keyname><forenames>Riyadh</forenames></author><author><keyname>Cohen</keyname><forenames>Albert</forenames></author><author><keyname>Bastoul</keyname><forenames>Cedric</forenames></author><author><keyname>Pouchet</keyname><forenames>Louis-Noel</forenames></author><author><keyname>Rauchwerger</keyname><forenames>Lawrence</forenames></author></authors><title>The Potential of Synergistic Static, Dynamic and Speculative Loop Nest
  Optimizations for Automatic Parallelization</title><categories>cs.DC cs.PF cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in automatic parallelization of loop-centric programs started with
static analysis, then broadened its arsenal to include dynamic
inspection-execution and speculative execution, the best results involving
hybrid static-dynamic schemes. Beyond the detection of parallelism in a
sequential program, scalable parallelization on many-core processors involves
hard and interesting parallelism adaptation and mapping challenges. These
challenges include tailoring data locality to the memory hierarchy, structuring
independent tasks hierarchically to exploit multiple levels of parallelism,
tuning the synchronization grain, balancing the execution load, decoupling the
execution into thread-level pipelines, and leveraging heterogeneous hardware
with specialized accelerators. The polyhedral framework allows to model,
construct and apply very complex loop nest transformations addressing most of
the parallelism adaptation and mapping challenges. But apart from
hardware-specific, back-end oriented transformations (if-conversion, trace
scheduling, value prediction), loop nest optimization has essentially ignored
dynamic and speculative techniques. Research in polyhedral compilation recently
reached a significant milestone towards the support of dynamic, data-dependent
control flow. This opens a large avenue for blending dynamic analyses and
speculative techniques with advanced loop nest optimizations. Selecting
real-world examples from SPEC benchmarks and numerical kernels, we make a case
for the design of synergistic static, dynamic and speculative loop
transformation techniques. We also sketch the embedding of dynamic information,
including speculative assumptions, in the heart of affine transformation search
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6757</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6757</id><created>2011-11-29</created><authors><author><keyname>Awad</keyname><forenames>Hussain A. H.</forenames></author><author><keyname>Battah</keyname><forenames>Fadi M.</forenames></author></authors><title>Enhancing Information Systems Security in Educational Organizations in
  KSA through proposing security model</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that technology utilization is not restricted for one sector
than the other anymore, Educational organizations share many parts of their
information systems with commercial organizations. In this paper we will try to
identify the main characteristics of information systems in educational
organizations, then we will propose a model of two parts to enhance the
information systems security, the first part of the model will handle the
policy and laws of the information system, the second part will provide a
technical approach on how to audit and subsequently maintain the security of
information system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6771</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6771</id><created>2011-11-29</created><authors><author><keyname>Salih</keyname><forenames>Nadir K.</forenames></author><author><keyname>Zang</keyname><forenames>Tianyi</forenames></author><author><keyname>Viju</keyname><forenames>PG. K.</forenames></author><author><keyname>Mohamed</keyname><forenames>Abdelmotalib A.</forenames></author></authors><title>Autonomic Management for Multi-agent Systems</title><categories>cs.MA</categories><comments>4page,2figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Autonomic computing is a computing system that can manage itself by
self-configuration, self-healing, self-optimizing and self-protection.
Researchers have been emphasizing the strong role that multi agent systems can
play progressively towards the design and implementation of complex autonomic
systems. The important of autonomic computing is to create computing systems
capable of managing themselves to a far greater extent than they do today. With
the nature of autonomy, reactivity, sociality and pro-activity, software agents
are promising to make autonomic computing system a reality. This paper mixed
multi-agent system with autonomic feature that completely hides its complexity
from users/services. Mentioned Java Application Development Framework as
platform example of this environment, could applied to web services as front
end to users. With multi agent support it also provides adaptability,
intelligence, collaboration, goal oriented interactions, flexibility, mobility
and persistence in software systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6790</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6790</id><created>2011-11-29</created><authors><author><keyname>Nguyen</keyname><forenames>Sao Mai</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Baranes</keyname><forenames>Adrien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Constraining the Size Growth of the Task Space with Socially Guided
  Intrinsic Motivation using Demonstrations</title><categories>cs.AI</categories><comments>JCAI Workshop on Agents Learning Interactively from Human Teachers
  (ALIHT), Barcelona : Spain (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm for learning a highly redundant inverse
model in continuous and non-preset environments. Our Socially Guided Intrinsic
Motivation by Demonstrations (SGIM-D) algorithm combines the advantages of both
social learning and intrinsic motivation, to specialise in a wide range of
skills, while lessening its dependence on the teacher. SGIM-D is evaluated on a
fishing skill learning experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6792</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6792</id><created>2011-11-29</created><authors><author><keyname>Buddelmeijer</keyname><forenames>Hugo</forenames></author><author><keyname>Williams</keyname><forenames>O. Rees</forenames></author><author><keyname>McFarland</keyname><forenames>John P.</forenames></author><author><keyname>Belikov</keyname><forenames>Andrey</forenames></author></authors><title>Astro-WISE processing of wide-field images and other data</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, Procedings of ADASS XXI, ASP Conference Series</comments><journal-ref>Astronomical Data Analysis Software and Systems XXI, volume 461,
  year 2012, page 881</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astro-WISE is the Astronomical Wide-field Imaging System for Europe. It is a
scientific information system which consists of hardware and software federated
over about a dozen institutes throughout Europe. It has been developed to
exploit the ever increasing avalanche of data produced by astronomical surveys
and data intensive scientific experiments in general.
  The demo explains the architecture of the Astro-WISE information system and
shows the use of Astro-WISE interfaces. Wide-field astronomical images are
derived from the raw image to the final catalog according to the user's
request. The demo is based on the standard Astro-WISE guided tour, which can be
accessed from the Astro-WISE website.
  The typical Astro-WISE data processing chain is shown, which can be used for
data handling for a variety of different instruments, currently 14, including
OmegaCAM, MegaCam, WFI, WFC, ACS/HST, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6793</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6793</id><created>2011-11-29</created><authors><author><keyname>Buddelmeijer</keyname><forenames>Hugo</forenames></author><author><keyname>Valentijn</keyname><forenames>Edwin A.</forenames></author></authors><title>Query Driven Visualization</title><categories>astro-ph.IM cs.SE</categories><comments>4 pages, Procedings ADASS XXI, ASP Conference Series</comments><journal-ref>Astronomical Data Analysis Software and Systems XXI, volume 461,
  year 2012, page 521</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The request driven way of deriving data in Astro-WISE is extended to a query
driven way of visualization. This allows scientists to focus on the science
they want to perform, because all administration of their data is automated.
This can be done over an abstraction layer that enhances control and
flexibility for the scientist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6804</identifier>
 <datestamp>2012-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6804</id><created>2011-11-29</created><updated>2012-02-02</updated><authors><author><keyname>Abbasi</keyname><forenames>Alireza</forenames></author><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Betweenness Centrality as a Driver of Preferential Attachment in the
  Evolution of Research Collaboration Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Journal of Informetrics (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze whether preferential attachment in scientific coauthorship
networks is different for authors with different forms of centrality. Using a
complete database for the scientific specialty of research about &quot;steel
structures,&quot; we show that betweenness centrality of an existing node is a
significantly better predictor of preferential attachment by new entrants than
degree or closeness centrality. During the growth of a network, preferential
attachment shifts from (local) degree centrality to betweenness centrality as a
global measure. An interpretation is that supervisors of PhD projects and
postdocs broker between new entrants and the already existing network, and thus
become focal to preferential attachment. Because of this mediation, scholarly
networks can be expected to develop differently from networks which are
predicated on preferential attachment to nodes with high degree centrality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6807</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6807</id><created>2011-11-29</created><authors><author><keyname>Bobkov</keyname><forenames>Sergey G.</forenames></author><author><keyname>Madiman</keyname><forenames>Mokshay M.</forenames></author></authors><title>On the problem of reversibility of the entropy power inequality</title><categories>math.FA cs.IT math.IT math.PR</categories><comments>13 pages</comments><journal-ref>&quot;Limit Theorems in Probability, Statistics and Number Theory (in
  honor of Friedrich G\&quot;otze)&quot;, P. Eichelsbacher et al. (ed.), Springer
  Proceedings in Mathematics and Statistics 42, pp. 61-74, Springer-Verlag,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As was shown recently by the authors, the entropy power inequality can be
reversed for independent summands with sufficiently concave densities, when the
distributions of the summands are put in a special position. In this note it is
proved that reversibility is impossible over the whole class of convex
probability distributions. Related phenomena for identically distributed
summands are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6808</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6808</id><created>2011-11-29</created><authors><author><keyname>Komondoor</keyname><forenames>Raghavan</forenames></author><author><keyname>Lakshmi</keyname><forenames>K. Vasanta</forenames></author><author><keyname>Seetharam</keyname><forenames>Deva P.</forenames></author><author><keyname>Balodia</keyname><forenames>Sudha</forenames></author></authors><title>Packet flow analysis in IP networks via abstract interpretation</title><categories>cs.NI</categories><comments>8 pages</comments><acm-class>C.2.5; C.2.6; D.2.4; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static analysis (aka offline analysis) of a model of an IP network is useful
for understanding, debugging, and verifying packet flow properties of the
network. There have been static analysis approaches proposed in the literature
for networks based on model checking as well as graph reachability. Abstract
interpretation is a method that has typically been applied to static analysis
of programs. We propose a new, abstract-interpretation based approach for
analysis of networks. We formalize our approach, mention its correctness
guarantee, and demonstrate its flexibility in addressing multiple
network-analysis problems that have been previously solved via tailor-made
approaches. Finally, we investigate an application of our analysis to a novel
problem -- inferring a high-level policy for the network -- which has been
addressed in the past only in the restricted single-router setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6822</identifier>
 <datestamp>2012-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6822</id><created>2011-11-29</created><updated>2012-07-10</updated><authors><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Optimal Phase Transitions in Compressed Sensing</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>to appear in IEEE Transactions of Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing deals with efficient recovery of analog signals from
linear encodings. This paper presents a statistical study of compressed sensing
by modeling the input signal as an i.i.d. process with known distribution.
Three classes of encoders are considered, namely optimal nonlinear, optimal
linear and random linear encoders. Focusing on optimal decoders, we investigate
the fundamental tradeoff between measurement rate and reconstruction fidelity
gauged by error probability and noise sensitivity in the absence and presence
of measurement noise, respectively. The optimal phase transition threshold is
determined as a functional of the input distribution and compared to suboptimal
thresholds achieved by popular reconstruction algorithms. In particular, we
show that Gaussian sensing matrices incur no penalty on the phase transition
threshold with respect to optimal nonlinear encoding. Our results also provide
a rigorous justification of previous results based on replica heuristics in the
weak-noise regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6825</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6825</id><created>2011-11-29</created><authors><author><keyname>Amirshahi</keyname><forenames>Alireza</forenames></author><author><keyname>Fathi</keyname><forenames>Mahmood</forenames></author><author><keyname>Romoozi</keyname><forenames>Morteza</forenames></author><author><keyname>Assarian</keyname><forenames>Mohammad</forenames></author></authors><title>A Fuzzy Realistic Mobility Model For Ad hoc Networks</title><categories>cs.AI cs.NI</categories><journal-ref>International journal of computer science Issues,Vol. 8,Issue 5,No
  3, 2011, 42-50</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Realistic mobility models can demonstrate more precise evaluation results
because their parameters are closer to the reality. In this paper a realistic
Fuzzy Mobility Model has been proposed. This model has rules which is
changeable depending on nodes and environment conditions. This model is more
complete and precise than the other mobility models and this is the advantage
of this model. After simulation, it was found out that not only considering
nodes movement as being imprecise (fuzzy) has a positive effects on most of ad
hoc network parameters, but also, more importantly as they are closer to the
real world condition, they can have a more positive effect on the
implementation of ad hoc network protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6828</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6828</id><created>2011-11-29</created><updated>2012-12-11</updated><authors><author><keyname>Banelli</keyname><forenames>Paolo</forenames></author></authors><title>Bayesian Estimation of a Gaussian source in Middleton's Class-A
  Impulsive Noise</title><categories>cs.IT math.IT stat.AP</categories><comments>30 pages, 13 figures, part of this work has been submitted to IEEE
  Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on minimum mean square error (MMSE) Bayesian estimation for
a Gaussian source impaired by additive Middleton's Class-A impulsive noise. In
addition to the optimal Bayesian estimator, the paper considers also the
soft-limiter and the blanker, which are two popular suboptimal estimators
characterized by very low complexity. The MMSE-optimum thresholds for such
suboptimal estimators are obtained by practical iterative algorithms with fast
convergence. The paper derives also the optimal thresholds according to a
maximum-SNR (MSNR) criterion, and establishes connections with the MMSE
criterion. Furthermore, closed form analytic expressions are derived for the
MSE and the SNR of all the suboptimal estimators, which perfectly match
simulation results. Noteworthy, these results can be applied to characterize
the receiving performance of any multicarrier system impaired by a
Gaussian-mixture noise, such as asymmetric digital subscriber lines (ADSL) and
power-line communications (PLC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6836</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6836</id><created>2011-11-29</created><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author><author><keyname>Iyer</keyname><forenames>Radha R.</forenames></author></authors><title>The regular number of a graph</title><categories>cs.DM math.CO</categories><journal-ref>Journal of Discrete Mathematical Sciences and Cryptography, vol.
  15, no. 2-3, pp. 149-157, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a simple undirected graph. The regular number of $G$ is defined to
be the minimum number of subsets into which the edge set of $G$ can be
partitioned so that the subgraph induced by each subset is regular. In this
work, we obtain the regular number of some families of graphs and discuss some
general bounds on this parameter. Also, some of the lower or upper bounds
proved in \cite{Kulli:Janakiram:Iyer:2001} are shown here to hold with
equality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6842</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6842</id><created>2011-11-29</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Fast Private Data Release Algorithms for Sparse Queries</title><categories>cs.DS cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of accurately answering large classes of statistical
queries while preserving differential privacy. Previous approaches to this
problem have either been very general but have not had run-time polynomial in
the size of the database, have applied only to very limited classes of queries,
or have relaxed the notion of worst-case error guarantees. In this paper we
consider the large class of sparse queries, which take non-zero values on only
polynomially many universe elements. We give efficient query release algorithms
for this class, in both the interactive and the non-interactive setting. Our
algorithms also achieve better accuracy bounds than previous general techniques
do when applied to sparse queries: our bounds are independent of the universe
size. In fact, even the runtime of our interactive mechanism is independent of
the universe size, and so can be implemented in the &quot;infinite universe&quot; model
in which no finite universe need be specified by the data curator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6843</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6843</id><created>2011-11-29</created><updated>2012-08-23</updated><authors><author><keyname>Paradowski</keyname><forenames>Micha&#x142; B.</forenames></author><author><keyname>Jonak</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Understanding the Social Cascading of Geekspeak and the Upshots for
  Social Cognitive Systems</title><categories>cs.AI cs.MA</categories><comments>M.B. Paradowski, {\L}. Jonak (2012) Understanding the social
  cascading of geekspeak and the upshots for social cognitive systems. In: A.
  Galton, Z. Wood (Eds) Understanding and Modelling Collective Phenomena.
  UBham, 27-32</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Barring swarm robotics, a substantial share of current machine-human and
machine-machine learning and interaction mechanisms are being developed and fed
by results of agent-based computer simulations, game-theoretic models, or
robotic experiments based on a dyadic communication pattern. Yet, in real life,
humans no less frequently communicate in groups, and gain knowledge and take
decisions basing on information cumulatively gleaned from more than one single
source. These properties should be taken into consideration in the design of
autonomous artificial cognitive systems construed to interact with learn from
more than one contact or 'neighbour'. To this end, significant practical import
can be gleaned from research applying strict science methodology to human and
social phenomena, e.g. to discovery of realistic creativity potential spans, or
the 'exposure thresholds' after which new information could be accepted by a
cognitive agent. The results will be presented of a project analysing the
social propagation of neologisms in a microblogging service. From local,
low-level interactions and information flows between agents inventing and
imitating discrete lexemes we aim to describe the processes of the emergence of
more global systemic order and dynamics, using the latest methods of complexity
science. Whether in order to mimic them, or to 'enhance' them, parameters
gleaned from complexity science approaches to humans' social and humanistic
behaviour should subsequently be incorporated as points of reference in the
field of robotics and human-machine interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6849</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6849</id><created>2011-11-27</created><authors><author><keyname>Gros</keyname><forenames>Claudius</forenames></author><author><keyname>Kaczor</keyname><forenames>Gregor</forenames></author><author><keyname>Markovic</keyname><forenames>Dimitrije</forenames></author></authors><title>Neuropsychological constraints to human data production on a global
  scale</title><categories>cs.SI cs.CC</categories><comments>to be published in: European Physical Journal B</comments><journal-ref>European Physical Journal B, 85: 28 (2012)</journal-ref><doi>10.1140/epjb/e2011-20581-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which are the factors underlying human information production on a global
level? In order to gain an insight into this question we study a corpus of
252-633 Million publicly available data files on the Internet corresponding to
an overall storage volume of 284-675 Terabytes. Analyzing the file size
distribution for several distinct data types we find indications that the
neuropsychological capacity of the human brain to process and record
information may constitute the dominant limiting factor for the overall growth
of globally stored information, with real-world economic constraints having
only a negligible influence. This supposition draws support from the
observation that the files size distributions follow a power law for data
without a time component, like images, and a log-normal distribution for
multimedia files, for which time is a defining qualia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6857</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6857</id><created>2011-11-28</created><updated>2012-08-29</updated><authors><author><keyname>Timme</keyname><forenames>Nicholas</forenames></author><author><keyname>Alford</keyname><forenames>Wesley</forenames></author><author><keyname>Flecker</keyname><forenames>Benjamin</forenames></author><author><keyname>Beggs</keyname><forenames>John M.</forenames></author></authors><title>Multivariate information measures: an experimentalist's perspective</title><categories>cs.IT cs.LG math.IT physics.data-an stat.AP</categories><comments>Manuscript (15 pages, 3 figures, 8 tables)</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory is widely accepted as a powerful tool for analyzing
complex systems and it has been applied in many disciplines. Recently, some
central components of information theory - multivariate information measures -
have found expanded use in the study of several phenomena. These information
measures differ in subtle yet significant ways. Here, we will review the
information theory behind each measure, as well as examine the differences
between these measures by applying them to several simple model systems. In
addition to these systems, we will illustrate the usefulness of the information
measures by analyzing neural spiking data from a dissociated culture through
early stages of its development. We hope that this work will aid other
researchers as they seek the best multivariate information measure for their
specific research goals and system. Finally, we have made software available
online which allows the user to calculate all of the information measures
discussed within this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6858</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6858</id><created>2011-11-29</created><authors><author><keyname>Bekenn</keyname><forenames>Bill</forenames></author><author><keyname>Hooper</keyname><forenames>Ray</forenames></author></authors><title>Workbook Structure Analysis - &quot;Coping with the Imperfect&quot;</title><categories>cs.SE</categories><comments>12 Pages, 7 Figures; Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Paper summarises the operation of software developed for the analysis of
workbook structure. This comprises: the identification of layout in terms of
filled areas formed into &quot;Stripes&quot;, the identification of all the Formula
Blocks/Cells and the identification of Data Blocks/Cells referenced by those
formulas. This development forms part of our FormulaDataSleuth toolset. It is
essential for the initial &quot;Watching&quot; of an existing workbook and enables the
workbook to be subsequently managed and protected from damage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6860</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6860</id><created>2011-11-29</created><updated>2012-04-13</updated><authors><author><keyname>Diana</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Lochin</keyname><forenames>Emmanuel</forenames></author></authors><title>Modelling the Delay Distribution of Binary Spray and Wait Routing
  Protocol</title><categories>cs.NI</categories><comments>Accepted for publication. AOC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a stochastic model to obtain the end-to-end delay law
between two nodes of a Delay Tolerant Network (DTN). We focus on the commonly
used Binary Spray and Wait (BSW) routing protocol and propose a model that can
be applied to homogeneous or heterogeneous networks (i.e. when the
inter-contact law parameter takes one or several values). To the best of our
knowledge, this is the first model allowing to estimate the delay distribution
of Binary Spray and Wait DTN protocol in heterogeneous networks. We first
detail the model and propose a set of simulations to validate the theoretical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6866</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6866</id><created>2011-11-29</created><authors><author><keyname>McDaid</keyname><forenames>Kevin</forenames></author><author><keyname>MacRuairi</keyname><forenames>Ronan</forenames></author><author><keyname>Clynch</keyname><forenames>Neil</forenames></author><author><keyname>Logue</keyname><forenames>Kevin</forenames></author><author><keyname>Clancy</keyname><forenames>Cian</forenames></author><author><keyname>Hayes</keyname><forenames>Shane</forenames></author></authors><title>Spreadsheets in Financial Departments: An Automated Analysis of 65,000
  Spreadsheets using the Luminous Technology</title><categories>cs.SE</categories><comments>14 Pages, 6 Tables, 4 Colour Figures; Proc. European Spreadsheet
  Risks Int. Grp. (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheet technology is a cornerstone of IT systems in most organisations.
It is often the glue that binds more structured transaction-based systems
together. Financial operations are a case in point where spreadsheets fill the
gaps left by dedicated accounting systems, particularly covering reporting and
business process operations. However, little is understood as to the nature of
spreadsheet usage in organisations and the contents and structure of these
spreadsheets as they relate to key business functions with few, if any,
comprehensive analyses of spreadsheet repositories in real organisations. As
such this paper represents an important attempt at profiling real and
substantial spreadsheet repositories.
  Using the Luminous technology an analysis of 65,000 spreadsheets for the
financial departments of both a government and a private commercial
organisation was conducted. This provides an important insight into the nature
and structure of these spreadsheets, the links between them, the existence and
nature of macros and the level of repetitive processes performed through the
spreadsheets. Furthermore it highlights the organisational dependence on
spreadsheets and the range and number of spreadsheets dealt with by individuals
on a daily basis. In so doing, this paper prompts important questions that can
frame future research in the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6870</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6870</id><created>2011-11-29</created><authors><author><keyname>Guthrie</keyname><forenames>Gordon</forenames></author><author><keyname>McCrory</keyname><forenames>Stephen</forenames></author></authors><title>Beyond The Desktop Spreadsheet</title><categories>cs.SE</categories><comments>11 Pages; Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2011
  ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypernumbers is a new commercial web-based spreadsheet. It addresses several
risk factors in deploying spreadsheets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6872</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6872</id><created>2011-11-29</created><authors><author><keyname>McKeever</keyname><forenames>Ruth</forenames></author><author><keyname>McDaid</keyname><forenames>Kevin</forenames></author></authors><title>Effect of Range Naming Conventions on Reliability and Development Time
  for Simple Spreadsheet Formulas</title><categories>cs.SE</categories><comments>13 Pages, 3 Tables; Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practitioners often argue that range names make spreadsheets easier to
understand and use, akin to the role of good variable names in traditional
programming languages, yet there is no supporting scientific evidence. The
authors previously published experiments that disproved this theory in relation
to debugging, and now turn their focus to development. This paper presents the
results of two iterations of a new experiment, which measure the effect of
range names on the correctness of, and the time it takes to develop, simple
summation formulas. Our findings, supported by statistically significant
results, show that formulas developed by non-experts using range names are more
likely to contain errors and take longer to develop. Taking these findings with
the findings from previous experiments, we conclude that range names do not
improve the quality of spreadsheets developed by novice and intermediate users.
This paper is important in that it finds that the choice of naming convention
can have a significant impact on novice and intermediate users' performance in
formula development, with less structured naming conventions resulting in
poorer performance by users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6878</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6878</id><created>2011-11-29</created><authors><author><keyname>Kulesz</keyname><forenames>Daniel</forenames></author></authors><title>From Good Practices to Effective Policies for Preventing Errors in
  Spreadsheets</title><categories>cs.SE</categories><comments>11 Pages, 3 Colour Figures; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to the enormous flexibility they provide, spreadsheets are considered
a priceless blessing by many end-users. Many spreadsheets, however, contain
errors which can lead to severe consequences in some cases. To manage these
risks, quality managers in companies are often asked to develop appropriate
policies for preventing spreadsheet errors. Good policies should specify rules
which are based on &quot;known-good&quot; practices. While there are many proposals for
such practices in literature written by practitioners and researchers, they are
often not consistent with each other. Therefore no general agreement has been
reached yet and no science-based &quot;golden rules&quot; have been published. This paper
proposes an expert-based, retrospective approach to the identification of good
practices for spreadsheets. It is based on an evaluation loop that
cross-validates the findings of human domain experts against rules implemented
in a semi-automated spreadsheet workbench, taking into account the context in
which the spreadsheets are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6883</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6883</id><created>2011-11-29</created><authors><author><keyname>Moguillansky</keyname><forenames>Mart&#xed;n O.</forenames></author><author><keyname>Rotstein</keyname><forenames>Nicol&#xe1;s D.</forenames></author><author><keyname>Falappa</keyname><forenames>Marcelo A.</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>Alejandro J.</forenames></author><author><keyname>Simari</keyname><forenames>Guillermo R.</forenames></author></authors><title>Dynamics of Knowledge in DeLP through Argument Theory Change</title><categories>cs.AI cs.LO</categories><comments>61 pages To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is devoted to the study of methods to change defeasible logic
programs (de.l.p.s) which are the knowledge bases used by the Defeasible Logic
Programming (DeLP) interpreter. DeLP is an argumentation formalism that allows
to reason over potentially inconsistent de.l.p.s. Argument Theory Change (ATC)
studies certain aspects of belief revision in order to make them suitable for
abstract argumentation systems. In this article, abstract arguments are
rendered concrete by using the particular rule-based defeasible logic adopted
by DeLP. The objective of our proposal is to define prioritized argument
revision operators \`a la ATC for de.l.p.s, in such a way that the newly
inserted argument ends up undefeated after the revision, thus warranting its
conclusion. In order to ensure this warrant, the de.l.p. has to be changed in
concordance with a minimal change principle. To this end, we discuss different
minimal change criteria that could be adopted. Finally, an algorithm is
presented, implementing the argument revision operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6884</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6884</id><created>2011-11-29</created><authors><author><keyname>Baglietto</keyname><forenames>Pierpaolo</forenames></author><author><keyname>Fornasa</keyname><forenames>Martino</forenames></author><author><keyname>Mangiante</keyname><forenames>Simone</forenames></author><author><keyname>Maresca</keyname><forenames>Massimo</forenames></author><author><keyname>Parodi</keyname><forenames>Andrea</forenames></author><author><keyname>Stecca</keyname><forenames>Michele</forenames></author></authors><title>A Platform for Spreadsheet Composition</title><categories>cs.SE</categories><comments>12 Pages, 7 Colour Figures; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A huge amount of data is everyday managed in large organizations in many
critical business sectors with the support of spreadsheet applications. The
process of elaborating spreadsheet data is often performed in a distributed,
collaborative way, where many actors enter data belonging to their local
business domain to contribute to a global business view. The manual fusion of
such data may lead to errors in copy-paste operations, loss of alignment and
coherency due to multiple spreadsheet copies in circulation, as well as loss of
data due to broken cross-spreadsheet links. In this paper we describe a
methodology, based on a Spreadsheet Composition Platform, which greatly reduces
these risks. The proposed platform seamlessly integrates the distributed
spreadsheet elaboration, supports the commonly known spreadsheet tools for data
processing and helps organizations to adopt a more controlled and secure
environment for data fusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6887</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6887</id><created>2011-11-29</created><authors><author><keyname>Coster</keyname><forenames>Nancy</forenames></author><author><keyname>Leon</keyname><forenames>Linda</forenames></author><author><keyname>Kalbers</keyname><forenames>Lawrence</forenames></author><author><keyname>Abraham</keyname><forenames>Dolphy</forenames></author></authors><title>Controls over Spreadsheets for Financial Reporting in Practice</title><categories>cs.CY</categories><comments>13Pages, 4 Tables, 1 Colour Figure; Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Past studies show that only a small percent of organizations implement and
enforce formal rules or informal guidelines for the designing, testing,
documenting, using, modifying, sharing and archiving of spreadsheet models. Due
to lack of such policies, there has been little research on how companies can
effectively govern spreadsheets throughout their life cycle. This paper
describes a survey involving 38 participants from the United States,
representing companies that were working on compliance with the Sarbanes-Oxley
Act of 2002 (SOX) as it relates to spreadsheets for financial reporting. The
findings of this survey describe specific controls organizations have
implemented to manage spreadsheets for financial reporting throughout the
spreadsheet's lifecycle. Our findings indicate that there are problems in all
stages of a spreadsheet's life cycle and suggest several important areas for
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6895</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6895</id><created>2011-11-29</created><authors><author><keyname>Hermans</keyname><forenames>Felienne</forenames></author><author><keyname>Pinzger</keyname><forenames>Martin</forenames></author><author><keyname>van Deursen</keyname><forenames>Arie</forenames></author></authors><title>Breviz: Visualizing Spreadsheets using Dataflow Diagrams</title><categories>cs.SE</categories><comments>9 Pages, 5 Colour Figures; Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets are used extensively in industry, often for business critical
purposes. In previous work we have analyzed the information needs of
spreadsheet professionals and addressed their need for support with the
transition of a spreadsheet to a colleague with the generation of data flow
diagrams. In this paper we describe the application of these data flow diagrams
for the purpose of understanding a spreadsheet with three example cases. We
furthermore suggest an additional application of the data flow diagrams: the
assessment of the quality of the spreadsheet's design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6900</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6900</id><created>2011-11-29</created><authors><author><keyname>Albrecht</keyname><forenames>Martin R.</forenames></author></authors><title>The M4RIE library for dense linear algebra over small fields with even
  characteristic</title><categories>cs.MS</categories><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present the M4RIE library which implements efficient
algorithms for linear algebra with dense matrices over GF(2^e) for 2 &lt;= 2 &lt;=
10. As the name of the library indicates, it makes heavy use of the M4RI
library both directly (i.e., by calling it) and indirectly (i.e., by using its
concepts). We provide an open-source GPLv2+ C library for efficient linear
algebra over GF(2^e) for e small. In this library we implemented an idea due to
Bradshaw and Boothby which reduces matrix multiplication over GF(p^k) to a
series of matrix multiplications over GF(p). Furthermore, we propose a caching
technique - Newton-John tables - to avoid finite field multiplications which is
inspired by Kronrod's method (&quot;M4RM&quot;) for matrix multiplication over GF(2).
Using these two techniques we provide asymptotically fast triangular solving
with matrices (TRSM) and PLE-based Gaussian elimination. As a result, we are
able to significantly improve upon the state of the art in dense linear algebra
over GF(2^e) with 2 &lt;= e &lt;= 10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6902</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6902</id><created>2011-11-29</created><authors><author><keyname>Correia</keyname><forenames>Jos&#xe9; Pedro</forenames></author><author><keyname>Ferreira</keyname><forenames>Miguel A.</forenames></author></authors><title>Requirements for Automated Assessment of Spreadsheet Maintainability</title><categories>cs.SE</categories><comments>7 Pages, 1 Figure; Proc. European Spreadsheet Risks Int. Grp.
  (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of spreadsheets is widespread. Be it in business, finance,
engineering or other areas, spreadsheets are created for their flexibility and
ease to quickly model a problem. Very often they evolve from simple prototypes
to implementations of crucial business logic. Spreadsheets that play a crucial
role in an organization will naturally have a long lifespan and will be
maintained and evolved by several people. Therefore, it is important not only
to look at their reliability, i.e., how well is the intended functionality
implemented, but also at their maintainability, i.e., how easy it is to
diagnose a spreadsheet for deficiencies and modify it without degrading its
quality. In this position paper we argue for the need to create a model to
estimate the maintainability of a spreadsheet based on (automated) measurement.
We propose to do so by applying a structured methodology that has already shown
its value in the estimation of maintainability of software products. We also
argue for the creation of a curated, community-contributed repository of
spreadsheets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6907</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6907</id><created>2011-11-29</created><authors><author><keyname>Grossman</keyname><forenames>Thomas A.</forenames></author><author><keyname>Mehrotra</keyname><forenames>Vijay</forenames></author><author><keyname>Sander</keyname><forenames>Johncharles</forenames></author></authors><title>Towards Evaluating the Quality of a Spreadsheet: The Case of the
  Analytical Spreadsheet Model</title><categories>cs.SE</categories><comments>Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2011 ISBN
  978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the challenge of creating guidelines to evaluate the quality of a
spreadsheet model. We suggest four principles. First, state the domain-the
spreadsheets to which the guidelines apply. Second, distinguish between the
process by which a spreadsheet is constructed from the resulting spreadsheet
artifact. Third, guidelines should be written in terms of the artifact,
independent of the process. Fourth, the meaning of &quot;quality&quot; must be defined.
We illustrate these principles with an example. We define the domain of
&quot;analytical spreadsheet models&quot;, which are used in business, finance,
engineering, and science. We propose for discussion a framework and terminology
for evaluating the quality of analytical spreadsheet models. This framework
categorizes and generalizes the findings of previous work on the more narrow
domain of financial spreadsheet models. We suggest that the ultimate goal is a
set of guidelines for an evaluator, and a checklist for a developer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6909</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6909</id><created>2011-11-29</created><authors><author><keyname>Przasnyski</keyname><forenames>Zbigniew</forenames></author><author><keyname>Leon</keyname><forenames>Linda</forenames></author><author><keyname>Seal</keyname><forenames>Kala Chand</forenames></author></authors><title>In Search of a Taxonomy for Classifying Qualitative Spreadsheet Errors</title><categories>cs.SE</categories><comments>Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2011 ISBN
  978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most organizations use large and complex spreadsheets that are embedded in
their mission-critical processes and are used for decision-making purposes.
Identification of the various types of errors that can be present in these
spreadsheets is, therefore, an important control that organizations can use to
govern their spreadsheets. In this paper, we propose a taxonomy for
categorizing qualitative errors in spreadsheet models that offers a framework
for evaluating the readiness of a spreadsheet model before it is released for
use by others in the organization. The classification was developed based on
types of qualitative errors identified in the literature and errors committed
by end-users in developing a spreadsheet model for Panko's (1996) &quot;Wall
problem&quot;. Closer inspection of the errors reveals four logical groupings of the
errors creating four categories of qualitative errors. The usability and
limitations of the proposed taxonomy and areas for future extension are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6911</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6911</id><created>2011-11-29</created><authors><author><keyname>Omogbadegun</keyname><forenames>Zacchaeus</forenames></author><author><keyname>Uwadia</keyname><forenames>Charles</forenames></author><author><keyname>Ayo</keyname><forenames>Charles</forenames></author><author><keyname>Mbarika</keyname><forenames>Victor</forenames></author><author><keyname>Omoregbe</keyname><forenames>Nicholas</forenames></author><author><keyname>Otofia</keyname><forenames>Efe</forenames></author><author><keyname>Chieze</keyname><forenames>Frank</forenames></author></authors><title>Multimedia-based Medicinal Plants Sustainability Management System</title><categories>cs.OH</categories><comments>12 pages, 9 figures, 6 tables, 37 references</comments><acm-class>J.3</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 3, September 2011; ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medicinal plants are increasingly recognized worldwide as an alternative
source of efficacious and inexpensive medications to synthetic
chemo-therapeutic compound. Rapid declining wild stocks of medicinal plants
accompanied by adulteration and species substitutions reduce their efficacy,
quality and safety. Consequently, the low accessibility to and
non-affordability of orthodox medicine costs by rural dwellers to be healthy
and economically productive further threaten their life expectancy. Finding
comprehensive information on medicinal plants of conservation concern at a
global level has been difficult. This has created a gap between computing
technologies' promises and expectations in the healing process under
complementary and alternative medicine. This paper presents the design and
implementation of a Multimedia-based Medicinal Plants Sustainability Management
System addressing these concerns. Medicinal plants' details for designing the
system were collected through semi-structured interviews and databases. Unified
Modelling Language, Microsoft-Visual-Studio.Net, C#3.0,
Microsoft-Jet-Engine4.0, MySQL, Loquendo Multilingual Text-to-Speech Software,
YouTube, and VLC Media Player were used. Keywords: Complementary and
Alternative Medicine, conservation, extinction, medicinal plant, multimedia,
phytoconstituents, rural dwellers
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6917</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6917</id><created>2011-11-29</created><authors><author><keyname>Preeti</keyname><forenames>K. S.</forenames></author><author><keyname>Singh</keyname><forenames>Vijit</forenames></author><author><keyname>Bhatia</keyname><forenames>Sushant</forenames></author><author><keyname>Singh</keyname><forenames>Ekansh Preet</forenames></author><author><keyname>Gupta</keyname><forenames>Manu Sheel</forenames></author></authors><title>Spreadsheet on Cloud -- Framework for Learning and Health Management
  System</title><categories>cs.SE</categories><comments>13 Pages, 8 Colour Figures; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2011 ISBN 978-0-9566256-9-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing has caused a paradigm shift in the world of computing.
Several use case scenarios have been floating around the programming world in
relation to this. Applications such as Spreadsheets have the capability to use
the Cloud framework to create complex web based applications. In our effort to
do the same, we have proposed a Spreadsheet on the cloud as the framework for
building new web applications, which will be useful in various scenarios,
specifically a School administration system and governance scenarios, such as
Health and Administration. This paper is a manifestation of this work, and
contains some use cases and architectures which can be used to realize these
scenarios in the most efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6922</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6922</id><created>2011-11-29</created><updated>2012-03-13</updated><authors><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Hardness of Mastermind</title><categories>cs.GT</categories><comments>12 pages; Sixth International Conference on FUN WITH ALGORITHMS, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mastermind is a popular board game released in 1971, where a codemaker
chooses a secret pattern of colored pegs, and a codebreaker has to guess it in
several trials. After each attempt, the codebreaker gets a response from the
codemaker containing some information on the number of correctly guessed pegs.
The search space is thus reduced at each turn, and the game continues until the
codebreaker is able to find the correct code, or runs out of trials.
  In this paper we study several variations of #MSP, the problem of computing
the size of the search space resulting from a given (possibly fictitious)
sequence of guesses and responses. Our main contribution is a proof of the
#P-completeness of #MSP under parsimonious reductions, which settles an open
problem posed by Stuckman and Zhang in 2005, concerning the complexity of
deciding if the secret code is uniquely determined by the previous guesses and
responses. Similarly, #MSP stays #P-complete under Turing reductions even with
the promise that the search space has at least k elements, for any constant k.
(In a regular game of Mastermind, k=1.)
  All our hardness results hold even in the most restrictive setting, in which
there are only two available peg colors, and also if the codemaker's responses
contain less information, for instance like in the so-called single-count
(black peg) Mastermind variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6923</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6923</id><created>2011-11-29</created><authors><author><keyname>Soni</keyname><forenames>Akshay</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author></authors><title>Efficient Adaptive Compressive Sensing Using Sparse Hierarchical Learned
  Dictionaries</title><categories>stat.ML cs.CV cs.IT math.IT math.PR stat.AP</categories><comments>5 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent breakthrough results in compressed sensing (CS) have established that
many high dimensional objects can be accurately recovered from a relatively
small number of non- adaptive linear projection observations, provided that the
objects possess a sparse representation in some basis. Subsequent efforts have
shown that the performance of CS can be improved by exploiting the structure in
the location of the non-zero signal coefficients (structured sparsity) or using
some form of online measurement focusing (adaptivity) in the sensing process.
In this paper we examine a powerful hybrid of these two techniques. First, we
describe a simple adaptive sensing procedure and show that it is a provably
effective method for acquiring sparse signals that exhibit structured sparsity
characterized by tree-based coefficient dependencies. Next, employing
techniques from sparse hierarchical dictionary learning, we show that
representations exhibiting the appropriate form of structured sparsity can be
learned from collections of training data. The combination of these techniques
results in an effective and efficient adaptive compressive acquisition
procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6925</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6925</id><created>2011-11-29</created><authors><author><keyname>Zhou</keyname><forenames>Yang</forenames></author></authors><title>Structure Learning of Probabilistic Graphical Models: A Comprehensive
  Survey</title><categories>stat.ML cs.LG</categories><comments>survey on structure learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic graphical models combine the graph theory and probability
theory to give a multivariate statistical modeling. They provide a unified
description of uncertainty using probability and complexity using the graphical
model. Especially, graphical models provide the following several useful
properties:
  - Graphical models provide a simple and intuitive interpretation of the
structures of probabilistic models. On the other hand, they can be used to
design and motivate new models.
  - Graphical models provide additional insights into the properties of the
model, including the conditional independence properties.
  - Complex computations which are required to perform inference and learning
in sophisticated models can be expressed in terms of graphical manipulations,
in which the underlying mathematical expressions are carried along implicitly.
  The graphical models have been applied to a large number of fields, including
bioinformatics, social science, control theory, image processing, marketing
analysis, among others. However, structure learning for graphical models
remains an open challenge, since one must cope with a combinatorial search over
the space of all possible structures.
  In this paper, we present a comprehensive survey of the existing structure
learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6934</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6934</id><created>2011-11-29</created><authors><author><keyname>Kalmukov</keyname><forenames>Yordan</forenames></author></authors><title>Architecture of a Conference Management System Providing Advanced Paper
  Assignment Features</title><categories>cs.DL</categories><comments>Published by Foundation of Computer Science, New York, USA</comments><journal-ref>International Journal of Computer Applications 34(3):51-59, 2011</journal-ref><doi>10.5120/4083-5888</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an architecture and assignment management model of a
conference management system that performs a precise and accurate automatic
assignment of reviewers to papers. The system relies on taxonomy of keywords to
describe papers and reviewers' competences. The implied hierarchical structure
of the taxonomy provides important additional information - the semantic
relationships between the separate keywords. It allows similarity measures to
take into account not only the number of exactly matching keywords between a
paper and a reviewer, but in case of non-matching ones to calculate how
semantically close they are. Reviewers are allowed to bid on the papers they
would like to (or not like to) review and to explicitly state conflicts of
interest (CoI) with papers. An automatic CoI detection is checking for
additional conflicts based on institutional affiliation, co-authorship (within
the local database) and previous co-authorship in the past (within the major
bibliographic indexes and digital libraries). The algorithm for automatic
assignment takes into account all - selected keywords, reviewers' bids and
conflicts of interest and tries to find the most accurate assignment while
maintaining load balancing among reviewers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6937</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6937</id><created>2011-11-29</created><updated>2013-02-22</updated><authors><author><keyname>Riondato</keyname><forenames>Matteo</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Efficient Discovery of Association Rules and Frequent Itemsets through
  Sampling with Tight Performance Guarantees</title><categories>cs.DS cs.DB cs.LG</categories><comments>19 pages, 7 figures. A shorter version of this paper appeared in the
  proceedings of ECML PKDD 2012</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tasks of extracting (top-$K$) Frequent Itemsets (FI's) and Association
Rules (AR's) are fundamental primitives in data mining and database
applications. Exact algorithms for these problems exist and are widely used,
but their running time is hindered by the need of scanning the entire dataset,
possibly multiple times. High quality approximations of FI's and AR's are
sufficient for most practical uses, and a number of recent works explored the
application of sampling for fast discovery of approximate solutions to the
problems. However, these works do not provide satisfactory performance
guarantees on the quality of the approximation, due to the difficulty of
bounding the probability of under- or over-sampling any one of an unknown
number of frequent itemsets. In this work we circumvent this issue by applying
the statistical concept of \emph{Vapnik-Chervonenkis (VC) dimension} to develop
a novel technique for providing tight bounds on the sample size that guarantees
approximation within user-specified parameters. Our technique applies both to
absolute and to relative approximations of (top-$K$) FI's and AR's. The
resulting sample size is linearly dependent on the VC-dimension of a range
space associated with the dataset to be mined. The main theoretical
contribution of this work is a proof that the VC-dimension of this range space
is upper bounded by an easy-to-compute characteristic quantity of the dataset
which we call \emph{d-index}, and is the maximum integer $d$ such that the
dataset contains at least $d$ transactions of length at least $d$ such that no
one of them is a superset of or equal to another. We show that this bound is
strict for a large class of datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6954</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6954</id><created>2011-11-29</created><authors><author><keyname>M&#x142;ynarski</keyname><forenames>Kajetan</forenames></author></authors><title>Reflexivity and the diagonal argument in proofs of limitative theorems</title><categories>cs.LO cs.CC</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses limitations of reflexive and diagonal arguments as
methods of proof of limitative theorems (e.g. G\&quot;odel's theorem on
Entscheidungsproblem, Turing's halting problem or Chaitin-G\&quot;odel's theorem).
The fact, that a formal system contains a sentence, which introduces
reflexitivity, does not imply, that the same system does not contain a sentence
or a proof procedure which solves this problem. Second basic method of proof -
diagonal argument (i.e. showing non-eqiunumerosity of a program set with the
set of real numbers) does not exclude existance of a single program, capable of
computing all real numbers. In this work, we suggest an algorithm generating
real numbers (arbitrary, infinite in the limit, binary strings), and we
speculate it's meaning for theoretical computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6983</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6983</id><created>2011-11-29</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Aggregation of Composite Solutions: strategies, models, examples</title><categories>cs.SE cs.AI math.OC</categories><comments>72 pages, 116 figures, 35 tables</comments><msc-class>68T20, 90C217, 90C59</msc-class><acm-class>D.2; H.1.1; H.4.0; E.1; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses aggregation issues for composite (modular) solutions. A
systemic view point is suggested for various aggregation problems. Several
solution structures are considered: sets, set morphologies, trees, etc. Mainly,
the aggregation approach is targeted to set morphologies. The aggregation
problems are based on basic structures as substructure, superstructure,
median/consensus, and extended median/consensus. In the last case, preliminary
structure is built (e.g., substructure, median/consensus) and addition of
solution elements is considered while taking into account profit of the
additional elements and total resource constraint. Four aggregation strategies
are examined: (i) extension strategy (designing a substructure of initial
solutions as &quot;system kernel&quot; and extension of the substructure by additional
elements); (ii) compression strategy (designing a superstructure of initial
solutions and deletion of some its elements); (iii) combined strategy; and (iv)
new design strategy to build a new solution over an extended domain of solution
elements. Numerical real-world examples (e.g., telemetry system, communication
protocol, student plan, security system, Web-based information system,
investment, educational courses) illustrate the suggested aggregation approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.6990</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.6990</id><created>2011-11-29</created><updated>2012-09-19</updated><authors><author><keyname>Fox</keyname><forenames>Kyle</forenames></author></authors><title>Shortest Non-trivial Cycles in Directed and Undirected Surface Graphs</title><categories>cs.CG cs.DM cs.DS</categories><comments>Accepted to SODA 2013. Updated for reviewer comments, to include new
  results for undirected graphs, and to include new title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a graph embedded on a surface of genus g with b boundary cycles. We
describe algorithms to compute multiple types of non-trivial cycles in G, using
different techniques depending on whether or not G is an undirected graph. If G
is undirected, then we give an algorithm to compute a shortest non-separating
cycle in 2^O(g) n log log n time. Similar algorithms are given to compute a
shortest non-contractible or non-null-homologous cycle in 2^O(g+b) n log log n
time. Our algorithms for undirected G combine an algorithm of Kutz with known
techniques for efficiently enumerating homotopy classes of curves that may be
shortest non-trivial cycles.
  Our main technical contributions in this work arise from assuming G is a
directed graph with possibly asymmetric edge weights. For this case, we give an
algorithm to compute a shortest non-contractible cycle in G in O((g^3 + g b)n
log n) time. In order to achieve this time bound, we use a restriction of the
infinite cyclic cover that may be useful in other contexts. We also describe an
algorithm to compute a shortest non-null-homologous cycle in G in O((g^2 + g
b)n log n) time, extending a known algorithm of Erickson to compute a shortest
non-separating cycle. In both the undirected and directed cases, our algorithms
improve the best time bounds known for many values of g and b.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7013</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7013</id><created>2011-11-29</created><updated>2013-05-29</updated><authors><author><keyname>Kakhbod</keyname><forenames>Ali</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Correction to &quot;An Efficient Game Form for Multi-rate Multicast Service
  Provisioning&quot;</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A correction to the specification of the mechanism proposed in &quot;An Efficient
Game Form for Multi-rate Multicast Service Provisioning&quot; is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7025</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7025</id><created>2011-11-29</created><authors><author><keyname>Georgievski</keyname><forenames>Il&#x10d;e</forenames></author><author><keyname>Lazovik</keyname><forenames>Alexander</forenames></author><author><keyname>Aiello</keyname><forenames>Marco</forenames></author></authors><title>Task Interaction in an HTN Planner</title><categories>cs.AI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical Task Network (HTN) planning uses task decomposition to plan for
an executable sequence of actions as a solution to a problem. In order to
reason effectively, an HTN planner needs expressive domain knowledge. For
instance, a simplified HTN planning system such as JSHOP2 uses such
expressivity and avoids some task interactions due to the increased complexity
of the planning process. We address the possibility of simplifying the domain
representation needed for an HTN planner to find good solutions, especially in
real-world domains describing home and building automation environments. We
extend the JSHOP2 planner to reason about task interaction that happens when
task's effects are already achieved by other tasks. The planner then prunes
some of the redundant searches that can occur due to the planning process's
interleaving nature. We evaluate the original and our improved planner on two
benchmark domains. We show that our planner behaves better by using simplified
domain knowledge and outperforms JSHOP2 in a number of relevant cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7033</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7033</id><created>2011-11-29</created><authors><author><keyname>De Wilde</keyname><forenames>Philippe</forenames></author><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author></authors><title>Stability of Evolving Multi-Agent Systems</title><categories>cs.MA cs.NE</categories><comments>9 pages, 5 figures, journal</comments><journal-ref>Systems, Man and Cybernetics - Part B, 41(4):1149-1157, 2011</journal-ref><doi>10.1109/TSMCB.2011.2110642</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Multi-Agent System is a distributed system where the agents or nodes
perform complex functions that cannot be written down in analytic form.
Multi-Agent Systems are highly connected, and the information they contain is
mostly stored in the connections. When agents update their state, they take
into account the state of the other agents, and they have access to those
states via the connections. There is also external, user-generated input into
the Multi-Agent System. As so much information is stored in the connections,
agents are often memory-less. This memory-less property, together with the
randomness of the external input, has allowed us to model Multi-Agent Systems
using Markov chains. In this paper, we look at Multi-Agent Systems that evolve,
i.e. the number of agents varies according to the fitness of the individual
agents. We extend our Markov chain model, and define stability. This is the
start of a methodology to control Multi-Agent Systems. We then build upon this
to construct an entropy-based definition for the degree of instability (entropy
of the limit probabilities), which we used to perform a stability analysis. We
then investigated the stability of evolving agent populations through
simulation, and show that the results are consistent with the original
definition of stability in non-evolving Multi-Agent Systems, proposed by Chli
and De Wilde. This paper forms the theoretical basis for the construction of
Digital Business Ecosystems, and applications have been reported elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7051</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7051</id><created>2011-11-30</created><authors><author><keyname>Pal</keyname><forenames>Arup Kumar</forenames></author><author><keyname>Biswas</keyname><forenames>G. P.</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>S.</forenames></author></authors><title>Design of Image Cryptosystem by Simultaneous VQ-Compression and
  Shuffling of Codebook and Index Matrix</title><categories>cs.CR</categories><journal-ref>The International journal of Multimedia &amp; Its Applications (IJMA),
  Vol.1, No.1, November 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of Internet usage although increases exponentially, it is
incapable of providing the security for exchange of confidential data between
the users. As a result, several cryptosystems for encryption of data and images
have been developed for secured transmission over Internet. In this work, a
scheme for Image encryption/decryption based on Vector Quantization (VQ) has
been proposed that concurrently encodes the images for compression and shuffles
the codebook and the index matrix using pseudorandom sequences for encryption.
The processing time of the proposed scheme is much less than the other
cryptosystems, because it does not use any traditional cryptographic
operations, and instead it performs swapping between the contents of the
codebook with respect to a random sequence, which resulted an indirect
shuffling of the contents of the index matrix. It may be noted that the
security of the proposed cryptosystem depends on the generation and the
exchange of the random sequences used. Since the generation of truly random
sequences are not practically feasible, we simulate the proposed scheme using
MATLAB, where its operators like rand(method, seed), randperm(n) has been used
to generate pseudorandom sequences and it has been seen that the proposed
cryptosystem shows the expected performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7055</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7055</id><created>2011-11-30</created><updated>2013-10-01</updated><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author></authors><title>Enumerating fundamental normal surfaces: Algorithms, experiments and
  invariants</title><categories>math.GT cs.CG</categories><comments>17 pages, 5 figures; v2: Stronger experimental focus, restrict
  attention to primal &amp; dual algorithms only, larger and more detailed
  experiments, more new crosscap numbers</comments><journal-ref>ALENEX 2014: Proceedings of the Sixteenth Workshop on Algorithm
  Engineering and Experiments, SIAM, 2013, pp. 112-124</journal-ref><doi>10.1137/1.9781611973198.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational knot theory and 3-manifold topology have seen significant
breakthroughs in recent years, despite the fact that many key algorithms have
complexity bounds that are exponential or greater. In this setting,
experimentation is essential for understanding the limits of practicality, as
well as for gauging the relative merits of competing algorithms.
  In this paper we focus on normal surface theory, a key tool that appears
throughout low-dimensional topology. Stepping beyond the well-studied problem
of computing vertex normal surfaces (essentially extreme rays of a polyhedral
cone), we turn our attention to the more complex task of computing fundamental
normal surfaces (essentially an integral basis for such a cone). We develop,
implement and experimentally compare a primal and a dual algorithm, both of
which combine domain-specific techniques with classical Hilbert basis
algorithms. Our experiments indicate that we can solve extremely large problems
that were once though intractable. As a practical application of our
techniques, we fill gaps from the KnotInfo database by computing 398
previously-unknown crosscap numbers of knots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7064</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7064</id><created>2011-11-30</created><updated>2012-04-10</updated><authors><author><keyname>Li</keyname><forenames>Liang</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Correlation Decay up to Uniqueness in Spin Systems</title><categories>cs.DS</categories><comments>27 pages, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a complete characterization of the two-state anti-ferromagnetic spin
systems which are of strong spatial mixing on general graphs. We show that a
two-state anti-ferromagnetic spin system is of strong spatial mixing on all
graphs of maximum degree at most \Delta if and only if the system has a unique
Gibbs measure on infinite regular trees of degree up to \Delta, where \Delta
can be either bounded or unbounded. As a consequence, there exists an FPTAS for
the partition function of a two-state anti-ferromagnetic spin system on graphs
of maximum degree at most \Delta when the uniqueness condition is satisfied on
infinite regular trees of degree up to \Delta. In particular, an FPTAS exists
for arbitrary graphs if the uniqueness is satisfied on all infinite regular
trees. This covers as special cases all previous algorithmic results for
two-state anti-ferromagnetic systems on general-structure graphs.
  Combining with the FPRAS for two-state ferromagnetic spin systems of
Jerrum-Sinclair and Goldberg-Jerrum-Paterson, and the very recent hardness
results of Sly-Sun and independently of Galanis-Stefankovic-Vigoda, this gives
a complete classification, except at the phase transition boundary, of the
approximability of all two-state spin systems, on either degree-bounded
families of graphs or family of all graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7069</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7069</id><created>2011-11-30</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Huang</keyname><forenames>Anpeng</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Differential Modulation for Bi-directional Relaying with Analog Network
  Coding</title><categories>cs.IT math.IT</categories><comments>19 pages, 7 figures</comments><journal-ref>IEEE Transactions on Signal Processing, Vol. 58, No. 7, pp. 3933 -
  3938, Jul. 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an analog network coding scheme with differential
modulation (ANC-DM) using amplify-and-forward protocol for bidirectional relay
networks when neither the source nodes nor the relay knows the channel state
information (CSI). The performance of the proposed ANC-DM scheme is analyzed
and a simple asymptotic bit error rate (BER) expression is derived. The
analytical results are verified through simulations. It is shown that the BER
performance of the proposed differential scheme is about 3 dB away from that of
the coherent detection scheme. To improve the system performance, the optimum
power allocation between the sources and the relay is determined based on the
simplified BER. Simulation results indicate that the proposed differential
scheme with optimum power allocation yields 1-2 dB performance improvement over
an equal power allocation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7076</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7076</id><created>2011-11-30</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author></authors><title>Relay Selection for Two-way Relaying with Amplify-and-Forward Protocols</title><categories>cs.IT math.IT</categories><comments>19 pages, 6 figures</comments><journal-ref>IEEE Transactions on Vehicle Technology, vol. 60, no. 4, pp.
  1954-1959, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a relay selection amplify-and-forward (RS-AF)
protocol in general bi-directional relay networks with two sources and $N$
relays. In the proposed scheme, the two sources first transmit to all the
relays simultaneously, and then a single relay with a minimum sum symbol error
rate (SER) will be selected to broadcast the received signals back to both
sources. To facilitate the selection process, we propose a simple sub-optimal
Min-Max criterion for relay selection, where a single relay which minimizes the
maximum SER of two source nodes will be selected. Simulation results show that
the proposed Min-Max selection has almost the same performance as the optimal
selection with lower complexity. We also present a simple asymptotic SER
expression and make comparison with the conventional all-participate
amplify-and-forward (AP-AF) relaying scheme. The analytical results are
verified through simulations. To improve the system performance, optimum power
allocation (OPA) between the sources and the relay is determined based on the
asymptotic SER. Simulation results indicate that the proposed RS-AF scheme with
OPA yields considerable performance improvement over an equal power allocation
(EPA) scheme, specially with large number of relay nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7078</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7078</id><created>2011-11-30</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Guo</keyname><forenames>Hong</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Joint Relay Selection and Analog Network Coding using Differential
  Modulation in Two-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>20 pages, 7 figures</comments><journal-ref>IEEE Transactions on Vehicle Technology, Vol. 59, No. 6, pp. 2932
  - 2939, Jul. 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a general bi-directional relay network with two
sources and N relays when neither the source nodes nor the relays know the
channel state information (CSI). A joint relay selection and analog network
coding using differential modulation (RS-ANC-DM) is proposed. In the proposed
scheme, the two sources employ differential modulations and transmit the
differential modulated symbols to all relays at the same time. The signals
received at the relay is a superposition of two transmitted symbols, which we
call the analog network coded symbols. Then a single relay which has minimum
sum SER is selected out of N relays to forward the ANC signals to both sources.
To facilitate the selection process, in this paper we also propose a simple
sub-optimal Min-Max criterion for relay selection, where a single relay which
minimizes the maximum SER of two source nodes is selected. Simulation results
show that the proposed Min-Max selection has almost the same performance as the
optimal selection, but is much simpler. The performance of the proposed
RS-ANC-DM scheme is analyzed, and a simple asymptotic SER expression is
derived. The analytical results are verified through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7088</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7088</id><created>2011-11-30</created><updated>2012-04-04</updated><authors><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author><author><keyname>Shen</keyname><forenames>Hao</forenames></author></authors><title>Uniqueness Analysis of Non-Unitary Matrix Joint Diagonalization</title><categories>cs.IT math.IT</categories><comments>23 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Matrix Joint Diagonalization (MJD) is a powerful approach for solving the
Blind Source Separation (BSS) problem. It relies on the construction of
matrices which are diagonalized by the unknown demixing matrix. Their joint
diagonalizer serves as a correct estimate of this demixing matrix only if it is
uniquely determined. Thus, a critical question is under what conditions a joint
diagonalizer is unique. In the present work we fully answer this question about
the identifiability of MJD based BSS approaches and provide a general result on
uniqueness conditions of matrix joint diagonalization. It unifies all existing
results which exploit the concepts of non-circularity, non-stationarity,
non-whiteness, and non-Gaussianity. As a corollary, we propose a solution for
complex BSS, which can be formulated in a closed form in terms of an eigenvalue
and a singular value decomposition of two matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7094</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7094</id><created>2011-11-30</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Multi-Gateway Cooperation in Multibeam Satellite Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multibeam systems with hundreds of beams have been recently deployed in order
to provide higher capacities by employing fractional frequency reuse.
Furthermore, employing full frequency reuse and precoding over multiple beams
has shown great throughput potential in literature. However, feeding all this
data from a single gateway is not feasible based on the current frequency
allocations. In this context, we investigate a range of scenarios involving
beam clusters where each cluster is managed by a single gateway. More
specifically, the following cases are considered for handling intercluster
interference: a) conventional frequency colouring, b) joint processing within
cluster, c) partial CSI sharing among clusters, d) partial CSI and data sharing
among clusters. CSI sharing does not provide considerable performance gains
with respect to b) but combined with data sharing offers roughly a 40%
improvement over a) and a 15% over b).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7100</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7100</id><created>2011-11-30</created><updated>2012-07-09</updated><authors><author><keyname>Gardner</keyname><forenames>Richard J.</forenames></author><author><keyname>Gronchi</keyname><forenames>Paolo</forenames></author><author><keyname>Theobald</keyname><forenames>Thorsten</forenames></author></authors><title>Determining a rotation of a tetrahedron from a projection</title><categories>math.MG cs.CG cs.CV</categories><comments>16 pages, 3 figures; minor revision based on reviewers' comments</comments><msc-class>52B10 68U05, 68W30 (Primary) 05E18 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following problem, arising from medical imaging, is addressed: Suppose
that $T$ is a known tetrahedron in $\R^3$ with centroid at the origin. Also
known is the orthogonal projection $U$ of the vertices of the image $\phi T$ of
$T$ under an unknown rotation $\phi$ about the origin. Under what circumstances
can $\phi$ be determined from $T$ and $U$?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7101</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7101</id><created>2011-11-30</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongshan</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Non-cooperative Feedback Rate Control Game for Channel State Information
  in Wireless Networks</title><categories>cs.NI cs.GT</categories><comments>26 pages, 10 figures; IEEE Journal on Selected Areas in
  Communications, special issue on Game Theory in Wireless Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been well recognized that channel state information (CSI) feedback is
of great importance for dowlink transmissions of closed-loop wireless networks.
However, the existing work typically researched the CSI feedback problem for
each individual mobile station (MS), and thus, cannot efficiently model the
interactions among self-interested mobile users in the network level. To this
end, in this paper, we propose an alternative approach to investigate the CSI
feedback rate control problem in the analytical setting of a game theoretic
framework, in which a multiple-antenna base station (BS) communicates with a
number of co-channel MSs through linear precoder. Specifically, we first
present a non-cooperative feedback-rate control game (NFC), in which each MS
selects the feedback rate to maximize its performance in a distributed way. To
improve efficiency from a social optimum point of view, we then introduce
pricing, called the non-cooperative feedback-rate control game with price
(NFCP). The game utility is defined as the performance gain by CSI feedback
minus the price as a linear function of the CSI feedback rate. The existence of
the Nash equilibrium of such games is investigated, and two types of feedback
protocols (FDMA and CSMA) are studied. Simulation results show that by
adjusting the pricing factor, the distributed NFCP game results in close
optimal performance compared with that of the centralized scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7104</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7104</id><created>2011-11-30</created><authors><author><keyname>Zhang</keyname><forenames>L.</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Ma</keyname><forenames>M.</forenames></author><author><keyname>Jiao</keyname><forenames>B.</forenames></author></authors><title>On the Minimum Differential Feedback for Time-Correlated MIMO Rayleigh
  Block-Fading Channels</title><categories>cs.IT math.IT</categories><comments>20 pages, 5 figures; IEEE Transactions on Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a general multiple input multiple output (MIMO)
system with channel state information (CSI) feedback over time-correlated
Rayleigh block-fading channels. Specifically, we first derive the closed-form
expression of the minimum differential feedback rate to achieve the maximum
erdodic capacity in the presence of channel estimation errors and quantization
distortion at the receiver. With the feedback-channel transmission rate
constraint, in the periodic feedback system, we further investigate the
relationship of the ergodic capacity and the differential feedback interval,
and we find by theoretical analysis that there exists an optimal differential
feedback interval to maximize ergodic capacity. Finally, analytical results are
verified through simulations in a practical periodic differential feedback
system using Lloyd's quantization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7108</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7108</id><created>2011-11-30</created><authors><author><keyname>Chen</keyname><forenames>Jingchao</forenames></author><author><keyname>Zhang</keyname><forenames>Rongqing</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Joint Relay and Jammer Selection for Secure Two-Way Relay Networks</title><categories>cs.IT math.IT</categories><comments>25 pages, 7 figures; IEEE Transactions on Information Forensics and
  Security, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate joint relay and jammer selection in two-way
cooperative networks, consisting of two sources, a number of intermediate
nodes, and one eavesdropper, with the constraints of physical layer security.
Specifically, the proposed algorithms select two or three intermediate nodes to
enhance security against the malicious eavesdropper. The first selected node
operates in the conventional relay mode and assists the sources to deliver
their data to the corresponding destinations using an amplify-and-forward
protocol. The second and third nodes are used in different communication phases
as jammers in order to create intentional interference upon the eavesdropper
node. Firstly, we find that in a topology where the intermediate nodes are
randomly and sparsely distributed, the proposed schemes with cooperative
jamming outperform the conventional non-jamming schemes within a certain
transmitted power regime. We also find that, in the scenario in which the
intermediate nodes gather as a close cluster, the jamming schemes may be less
effective than their non-jamming counterparts. Therefore, we introduce a hybrid
scheme to switch between jamming and non-jamming modes. Simulation results
validate our theoretical analysis and show that the hybrid switching scheme
further improves the secrecy rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7148</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7148</id><created>2011-11-30</created><updated>2011-12-01</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>A Cook's Tour of the Finitary Non-Well-Founded Sets</title><categories>cs.LO math.LO quant-ph</categories><comments>This paper is a write-up of a lecture originally given in 1988. It
  appeared in the Festschrift for Dov Gabbay, We Will Show Them: Essays in
  honour of Dov Gabbay, edited by Sergei Artemov, Howard Barringer, Artur
  d'Avila Garcez, Luis C. Lamb and John Woods, College Publications, Vol. 1,
  1-18, 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give multiple descriptions of a topological universe of finitary sets,
which can be seen as a natural limit completion of the hereditarily finite
sets. This universe is characterized as a metric completion of the hereditarily
finite sets; as a Stone space arising as the solution of a functorial
fixed-point equation involving the Vietoris construction; as the Stone dual of
the free modal algebra; and as the subspace of maximal elements of a domain
equation involving the Plotkin (or convex) powerdomain. These results
illustrate the methods developed in the author's 'Domain theory in logical
form', and related literature, and have been taken up in recent work on
topological coalgebras.
  The set-theoretic universe of finitary sets also supports an interesting form
of set theory. It contains non-well founded sets and a universal set; and is
closed under positive versions of the usual axioms of set theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7154</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7154</id><created>2011-11-30</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>A Structural Approach to Reversible Computation</title><categories>cs.LO math.LO quant-ph</categories><comments>30 pages, appeared in Theoretical Computer Science</comments><journal-ref>Theoretical Computer Science, Volume 347, Issue 3, 1 December
  2005, Pages 441-464</journal-ref><doi>10.1016/j.tcs.2005.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversibility is a key issue in the interface between computation and
physics, and of growing importance as miniaturization progresses towards its
physical limits. Most foundational work on reversible computing to date has
focussed on simulations of low-level machine models. By contrast, we develop a
more structural approach. We show how high-level functional programs can be
mapped compositionally (i.e. in a syntax-directed fashion) into a simple kind
of automata which are immediately seen to be reversible. The size of the
automaton is linear in the size of the functional term. In mathematical terms,
we are building a concrete model of functional computation. This construction
stems directly from ideas arising in Geometry of Interaction and Linear
Logic---but can be understood without any knowledge of these topics. In fact,
it serves as an excellent introduction to them. At the same time, an
interesting logical delineation between reversible and irreversible forms of
computation emerges from our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7159</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7159</id><created>2011-11-30</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Sequentiality vs. Concurrency in Games and Logic</title><categories>cs.LO math.LO quant-ph</categories><comments>35 pages, appeared in Mathematical Structures in Computer Science</comments><journal-ref>Mathematical Structures in Computer Science, Volume 13 Issue 4,
  pages 531-565, August 2003</journal-ref><doi>10.1017/S0960129503003980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connections between the sequentiality/concurrency distinction and the
semantics of proofs are investigated, with particular reference to games and
Linear Logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7164</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7164</id><created>2011-11-30</created><authors><author><keyname>Suchanek</keyname><forenames>Fabian M.</forenames></author><author><keyname>Abiteboul</keyname><forenames>Serge</forenames></author><author><keyname>Senellart</keyname><forenames>Pierre</forenames></author></authors><title>PARIS: Probabilistic Alignment of Relations, Instances, and Schema</title><categories>cs.DB</categories><comments>VLDB2012. arXiv admin note: substantial text overlap with
  arXiv:1105.5516</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  157-168 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges that the Semantic Web faces is the integration of
a growing number of independently designed ontologies. In this work, we present
PARIS, an approach for the automatic alignment of ontologies. PARIS aligns not
only instances, but also relations and classes. Alignments at the instance
level cross-fertilize with alignments at the schema level. Thereby, our system
provides a truly holistic solution to the problem of ontology alignment. The
heart of the approach is probabilistic, i.e., we measure degrees of matchings
based on probability estimates. This allows PARIS to run without any parameter
tuning. We demonstrate the efficiency of the algorithm and its precision
through extensive experiments. In particular, we obtain a precision of around
90% in experiments with some of the world's largest ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7165</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7165</id><created>2011-11-30</created><authors><author><keyname>Ranu</keyname><forenames>Sayan</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author></authors><title>Answering Top-k Queries Over a Mixture of Attractive and Repulsive
  Dimensions</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  169-180 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formulate a top-k query that compares objects in a database
to a user-provided query object on a novel scoring function. The proposed
scoring function combines the idea of attractive and repulsive dimensions into
a general framework to overcome the weakness of traditional distance or
similarity measures. We study the properties of the proposed class of scoring
functions and develop efficient and scalable index structures that index the
isolines of the function. We demonstrate various scenarios where the query
finds application. Empirical evaluation demonstrates a performance gain of one
to two orders of magnitude on querying time over existing state-of-the-art
top-k techniques. Further, a qualitative analysis is performed on a real
dataset to highlight the potential of the proposed query in discovering hidden
data characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7166</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7166</id><created>2011-11-30</created><authors><author><keyname>Armbrust</keyname><forenames>Michael</forenames></author><author><keyname>Curtis</keyname><forenames>Kristal</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Fox</keyname><forenames>Armando</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Patterson</keyname><forenames>David A.</forenames></author></authors><title>PIQL: Success-Tolerant Query Processing in the Cloud</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  181-192 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Newly-released web applications often succumb to a &quot;Success Disaster,&quot; where
overloaded database machines and resulting high response times destroy a
previously good user experience. Unfortunately, the data independence provided
by a traditional relational database system, while useful for agile
development, only exacerbates the problem by hiding potentially expensive
queries under simple declarative expressions. As a result, developers of these
applications are increasingly abandoning relational databases in favor of
imperative code written against distributed key/value stores, losing the many
benefits of data independence in the process. Instead, we propose PIQL, a
declarative language that also provides scale independence by calculating an
upper bound on the number of key/value store operations that will be performed
for any query. Coupled with a service level objective (SLO) compliance
prediction model and PIQL's scalable database architecture, these bounds make
it easy for developers to write success-tolerant applications that support an
arbitrarily large number of users while still providing acceptable performance.
In this paper, we present the PIQL query processing system and evaluate its
scale independence on hundreds of machines using two benchmarks, TPC-W and
SCADr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7167</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7167</id><created>2011-11-30</created><authors><author><keyname>Zhao</keyname><forenames>Peixiang</forenames></author><author><keyname>Aggarwal</keyname><forenames>Charu C.</forenames></author><author><keyname>Wang</keyname><forenames>Min</forenames></author></authors><title>gSketch: On Query Estimation in Graph Streams</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  193-204 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many dynamic applications are built upon large network infrastructures, such
as social networks, communication networks, biological networks and the Web.
Such applications create data that can be naturally modeled as graph streams,
in which edges of the underlying graph are received and updated sequentially in
a form of a stream. It is often necessary and important to summarize the
behavior of graph streams in order to enable effective query processing.
However, the sheer size and dynamic nature of graph streams present an enormous
challenge to existing graph management techniques. In this paper, we propose a
new graph sketch method, gSketch, which combines well studied synopses for
traditional data streams with a sketch partitioning technique, to estimate and
optimize the responses to basic queries on graph streams. We consider two
different scenarios for query estimation: (1) A graph stream sample is
available; (2) Both a graph stream sample and a query workload sample are
available. Algorithms for different scenarios are designed respectively by
partitioning a global sketch to a group of localized sketches in order to
optimize the query estimation accuracy. We perform extensive experimental
studies on both real and synthetic data sets and demonstrate the power and
robustness of gSketch in comparison with the state-of-the-art global sketch
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7168</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7168</id><created>2011-11-30</created><authors><author><keyname>Ruttenberg</keyname><forenames>Brian E.</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author></authors><title>Indexing the Earth Mover's Distance Using Normal Distributions</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  205-216 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Querying uncertain data sets (represented as probability distributions)
presents many challenges due to the large amount of data involved and the
difficulties comparing uncertainty between distributions. The Earth Mover's
Distance (EMD) has increasingly been employed to compare uncertain data due to
its ability to effectively capture the differences between two distributions.
Computing the EMD entails finding a solution to the transportation problem,
which is computationally intensive. In this paper, we propose a new lower bound
to the EMD and an index structure to significantly improve the performance of
EMD based K-nearest neighbor (K-NN) queries on uncertain databases. We propose
a new lower bound to the EMD that approximates the EMD on a projection vector.
Each distribution is projected onto a vector and approximated by a normal
distribution, as well as an accompanying error term. We then represent each
normal as a point in a Hough transformed space. We then use the concept of
stochastic dominance to implement an efficient index structure in the
transformed space. We show that our method significantly decreases K-NN query
time on uncertain databases. The index structure also scales well with database
cardinality. It is well suited for heterogeneous data sets, helping to keep EMD
based queries tractable as uncertain data sets become larger and more complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7169</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7169</id><created>2011-11-30</created><authors><author><keyname>Fakas</keyname><forenames>Georgios J.</forenames></author><author><keyname>Cai</keyname><forenames>Zhi</forenames></author><author><keyname>Mamoulis</keyname><forenames>Nikos</forenames></author></authors><title>Size-l Object Summaries for Relational Keyword Search</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  229-240 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A previously proposed keyword search paradigm produces, as a query result, a
ranked list of Object Summaries (OSs). An OS is a tree structure of related
tuples that summarizes all data held in a relational database about a
particular Data Subject (DS). However, some of these OSs are very large in size
and therefore unfriendly to users that initially prefer synoptic information
before proceeding to more comprehensive information about a particular DS. In
this paper, we investigate the effective and efficient retrieval of concise and
informative OSs. We argue that a good size-l OS should be a stand-alone and
meaningful synopsis of the most important information about the particular DS.
More precisely, we define a size-l OS as a partial OS composed of l important
tuples. We propose three algorithms for the efficient generation of size-l OSs
(in addition to the optimal approach which requires exponential time).
Experimental evaluation on DBLP and TPC-H databases verifies the effectiveness
and efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7170</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7170</id><created>2011-11-30</created><authors><author><keyname>Fang</keyname><forenames>Lujun</forenames></author><author><keyname>Sarma</keyname><forenames>Anish Das</forenames></author><author><keyname>Yu</keyname><forenames>Cong</forenames></author><author><keyname>Bohannon</keyname><forenames>Philip</forenames></author></authors><title>REX: Explaining Relationships between Entity Pairs</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  241-252 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge bases of entities and relations (either constructed manually or
automatically) are behind many real world search engines, including those at
Yahoo!, Microsoft, and Google. Those knowledge bases can be viewed as graphs
with nodes representing entities and edges representing (primary)
relationships, and various studies have been conducted on how to leverage them
to answer entity seeking queries. Meanwhile, in a complementary direction,
analyses over the query logs have enabled researchers to identify entity pairs
that are statistically correlated. Such entity relationships are then presented
to search users through the &quot;related searches&quot; feature in modern search
engines. However, entity relationships thus discovered can often be &quot;puzzling&quot;
to the users because why the entities are connected is often indescribable. In
this paper, we propose a novel problem called &quot;entity relationship
explanation&quot;, which seeks to explain why a pair of entities are connected, and
solve this challenging problem by integrating the above two complementary
approaches, i.e., we leverage the knowledge base to &quot;explain&quot; the connections
discovered between entity pairs. More specifically, we present REX, a system
that takes a pair of entities in a given knowledge base as input and
efficiently identifies a ranked list of relationship explanations. We formally
define relationship explanations and analyze their desirable properties.
Furthermore, we design and implement algorithms to efficiently enumerate and
rank all relationship explanations based on multiple measures of
&quot;interestingness.&quot; We perform extensive experiments over real web-scale data
gathered from DBpedia and a commercial search engine, demonstrating the
efficiency and scalability of REX. We also perform user studies to corroborate
the effectiveness of explanations generated by REX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7171</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7171</id><created>2011-11-30</created><authors><author><keyname>Li</keyname><forenames>Guoliang</forenames></author><author><keyname>Deng</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Jiannan</forenames></author><author><keyname>Feng</keyname><forenames>Jianhua</forenames></author></authors><title>PASS-JOIN: A Partition-based Method for Similarity Joins</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  253-264 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an essential operation in data cleaning, the similarity join has attracted
considerable attention from the database community. In this paper, we study
string similarity joins with edit-distance constraints, which find similar
string pairs from two large sets of strings whose edit distance is within a
given threshold. Existing algorithms are efficient either for short strings or
for long strings, and there is no algorithm that can efficiently and adaptively
support both short strings and long strings. To address this problem, we
propose a partition-based method called Pass-Join. Pass-Join partitions a
string into a set of segments and creates inverted indices for the segments.
Then for each string, Pass-Join selects some of its substrings and uses the
selected substrings to find candidate pairs using the inverted indices. We
devise efficient techniques to select the substrings and prove that our method
can minimize the number of selected substrings. We develop novel pruning
techniques to efficiently verify the candidate pairs. Experimental results show
that our algorithms are efficient for both short strings and long strings, and
outperform state-of-the-art methods on real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7190</identifier>
 <datestamp>2012-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7190</id><created>2011-11-29</created><updated>2012-07-20</updated><authors><author><keyname>Paradowski</keyname><forenames>Micha&#x142; B.</forenames></author></authors><title>Developing Embodied Multisensory Dialogue Agents</title><categories>cs.AI cs.CL</categories><comments>(2012) Developing embodied multisensory dialogue agents. In: R.
  Rzepka, M. Ptaszy\'nski, P. Dybala (Eds) Linguistic and Cognitive Approaches
  To Dialogue Agents. AISB, 6-14</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A few decades of work in the AI field have focused efforts on developing a
new generation of systems which can acquire knowledge via interaction with the
world. Yet, until very recently, most such attempts were underpinned by
research which predominantly regarded linguistic phenomena as separated from
the brain and body. This could lead one into believing that to emulate
linguistic behaviour, it suffices to develop 'software' operating on abstract
representations that will work on any computational machine. This picture is
inaccurate for several reasons, which are elucidated in this paper and extend
beyond sensorimotor and semantic resonance. Beginning with a review of
research, I list several heterogeneous arguments against disembodied language,
in an attempt to draw conclusions for developing embodied multisensory agents
which communicate verbally and non-verbally with their environment. Without
taking into account both the architecture of the human brain, and embodiment,
it is unrealistic to replicate accurately the processes which take place during
language acquisition, comprehension, production, or during non-linguistic
actions. While robots are far from isomorphic with humans, they could benefit
from strengthened associative connections in the optimization of their
processes and their reactivity and sensitivity to environmental stimuli, and in
situated human-machine interaction. The concept of multisensory integration
should be extended to cover linguistic input and the complementary information
combined from temporally coincident sensory impressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7209</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7209</id><created>2011-11-30</created><authors><author><keyname>Wei</keyname><forenames>Chuan-Sheng</forenames></author><author><keyname>Chen</keyname><forenames>Sheng-Gwo</forenames></author><author><keyname>Huang</keyname><forenames>Tone-Yau</forenames></author><author><keyname>Ong</keyname><forenames>Yao Lin</forenames></author></authors><title>A secure solution on hierarchical access control</title><categories>cs.CR</categories><comments>16pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical access control is an important and traditional problem in
information security. In 2001, Wu et.al. proposed an elegant solution for
hierarchical access control by the secure-filter. Jeng and Wang presented an
improvement of Wu et. al.'s method by the ECC cryptosystem. However,
secure-filter method is insecure in dynaminc access control. Lie, Hsu and
Tripathy, Paul pointed out some secure leaks on the secure-filter and presented
some improvements to eliminate these secure flaws. In this paper, we revise the
secure-filter in Jeng-Wang method and propose another secure solutions in
hierarchical access control problem. CA is a super security class (user) in our
proposed method and the secure-filter of $u_i$ in our solutions is a polynomial
of degree $n_i+1$ in $\mathbb{Z}_p^*$,
$f_i(x)=(x-h_i)(x-a_1)...(x-a_{n_i})+L_{l_i}(K_i)$. Although the degree of our
secure-filter is larger than others solutions, our solution is secure and
efficient in dynamics access control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7217</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7217</id><created>2011-11-30</created><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Adeyeye</keyname><forenames>John O.</forenames></author><author><keyname>Laubenbacher</keyname><forenames>Reinhard</forenames></author></authors><title>Nested Canalyzing Functions And Their Average Sensitivities</title><categories>cs.DM</categories><msc-class>05A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain complete characterization for nested canalyzing
functions (NCFs) by obtaining its unique algebraic normal form (polynomial
form). We introduce a new concept, LAYER NUMBER for NCF. Based on this, we
obtain explicit formulas for the the following important parameters: 1) Number
of all the nested canalyzing functions, 2) Number of all the NCFs with given
LAYER NUMBER, 3) Hamming weight of any NCF, 4) The activity number of any
variable of any NCF, 5) The average sensitivity of any NCF. Based on these
formulas, we show the activity number is greater for those variables in out
layer and equal in the same layer. We show the average sensitivity attains
minimal value when the NCF has only one layer. We also prove the average
sensitivity for any NCF (No matter how many variables it has) is between 0 and
2. Hence, theoretically, we show why NCF is stable since a random Boolean
function has average sensitivity $\frac{n}{2}$. Finally we conjecture that the
NCF attain the maximal average sensitivity if it has the maximal LAYER NUMBER
$n-1$. Hence, we guess the uniform upper bound for the average sensitivity of
any NCF can be reduced to 4/3 which is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7219</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7219</id><created>2011-11-30</created><authors><author><keyname>Paquot</keyname><forenames>Yvan</forenames></author><author><keyname>Duport</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Smerieri</keyname><forenames>Anteo</forenames></author><author><keyname>Dambre</keyname><forenames>Joni</forenames></author><author><keyname>Schrauwen</keyname><forenames>Benjamin</forenames></author><author><keyname>Haelterman</keyname><forenames>Marc</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author></authors><title>Optoelectronic Reservoir Computing</title><categories>cs.ET cs.LG cs.NE nlin.CD physics.optics</categories><comments>Contains main paper and two Supplementary Materials</comments><journal-ref>Scientific Reports 2, Article number: 287, (2012)</journal-ref><doi>10.1038/srep00287</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir computing is a recently introduced, highly efficient bio-inspired
approach for processing time dependent data. The basic scheme of reservoir
computing consists of a non linear recurrent dynamical system coupled to a
single input layer and a single output layer. Within these constraints many
implementations are possible. Here we report an opto-electronic implementation
of reservoir computing based on a recently proposed architecture consisting of
a single non linear node and a delay line. Our implementation is sufficiently
fast for real time information processing. We illustrate its performance on
tasks of practical importance such as nonlinear channel equalization and speech
recognition, and obtain results comparable to state of the art digital
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7221</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7221</id><created>2011-11-30</created><authors><author><keyname>Shah</keyname><forenames>Parikshit</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo</forenames></author></authors><title>An Optimal Controller Architecture for Poset-Causal Systems</title><categories>math.OC cs.SY</categories><comments>32 pages, 9 figures, submitted to IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel and natural architecture for decentralized control that is
applicable whenever the underlying system has the structure of a partially
ordered set (poset). This controller architecture is based on the concept of
Moebius inversion for posets, and enjoys simple and appealing separation
properties, since the closed-loop dynamics can be analyzed in terms of
decoupled subsystems. The controller structure provides rich and interesting
connections between concepts from order theory such as Moebius inversion and
control-theoretic concepts such as state prediction, correction, and
separability. In addition, using our earlier results on H_2-optimal
decentralized control for arbitrary posets, we prove that the H_2-optimal
controller in fact possesses the proposed structure, thereby establishing the
optimality of the new controller architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7222</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7222</id><created>2011-11-30</created><authors><author><keyname>Ibidapo</keyname></author><author><keyname>Akinyemi</keyname><forenames>O.</forenames></author><author><keyname>Omogbadegun</keyname><forenames>Zaccheous O.</forenames></author><author><keyname>Oyelami</keyname><forenames>Olufemi M.</forenames></author></authors><title>Towards Designing a Biometric Measure for Enhancing ATM Security in
  Nigeria E-Banking System</title><categories>cs.CR</categories><comments>6 pages, 7 figures, and 7 references;
  http://www.ijens.org/109106-3535%20IJECS-IJENS.pdf,
  http://www.ijens.org/IJECS%20Vol%2010%20Issue%2006.html</comments><acm-class>K.4.4</acm-class><journal-ref>International Journal of Electrical &amp; Computer Sciences
  IJECS-IJENS Vol: 10 No: 06, December 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security measures at banks can play a critical, contributory role in
preventing attacks on customers. These measures are of paramount importance
when considering vulnerabilities and causation in civil litigation. Banks must
meet certain standards in order to ensure a safe and secure banking environment
for their customers. This paper focuses on vulnerabilities and the increasing
wave of criminal activities occurring at Automated Teller Machines (ATMs) where
quick cash is the prime target for criminals rather than at banks themselves. A
biometric measure as a means of enhancing the security has emerged from the
discourse. Keywords-Security, ATM, Biometric, Crime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7224</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7224</id><created>2011-11-30</created><authors><author><keyname>Qumsiyeh</keyname><forenames>Rani</forenames></author><author><keyname>Pera</keyname><forenames>Maria S.</forenames></author><author><keyname>Ng</keyname><forenames>Yiu-Kai</forenames></author></authors><title>Generating Exact- and Ranked Partially-Matched Answers to Questions in
  Advertisements</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.
  217-228 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking advantage of the Web, many advertisements (ads for short) websites,
which aspire to increase client's transactions and thus profits, offer
searching tools which allow users to (i) post keyword queries to capture their
information needs or (ii) invoke form-based interfaces to create queries by
selecting search options, such as a price range, filled-in entries, check
boxes, or drop-down menus. These search mechanisms, however, are inadequate,
since they cannot be used to specify a natural-language query with rich
syntactic and semantic content, which can only be handled by a question
answering (QA) system. Furthermore, existing ads websites are incapable of
evaluating arbitrary Boolean queries or retrieving partiallymatched answers
that might be of interest to the user whenever a user's search yields only a
few or no results at all. In solving these problems, we present a QA system for
ads, called CQAds, which (i) allows users to post a natural-language question Q
for retrieving relevant ads, if they exist, (ii) identifies ads as answers that
partially-match the requested information expressed in Q, if insufficient or no
answers to Q can be retrieved, which are ordered using a similarity-ranking
approach, and (iii) analyzes incomplete or ambiguous questions to perform the
&quot;best guess&quot; in retrieving answers that &quot;best match&quot; the selection criteria
specified in Q. CQAds is also equipped with a Boolean model to evaluate Boolean
operators that are either explicitly or implicitly specified in Q, i.e., with
or without Boolean operators specified by the users, respectively. CQAds is
easy to use, scalable to all ads domains, and more powerful than search tools
provided by existing ads websites, since its query-processing strategy
retrieves relevant ads of higher quality and quantity. We have verified the
accuracy of CQAds in retrieving ads on eight ads domains and compared
it...[truncated].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7226</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7226</id><created>2011-11-25</created><authors><author><keyname>Lu</keyname><forenames>Xiangyang</forenames></author><author><keyname>Hu</keyname><forenames>Jin</forenames></author><author><keyname>Tao</keyname><forenames>Ran</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author></authors><title>Controlling Communication Field of Complex Networks by Transformation
  Method</title><categories>cs.NI</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling the global statuses of a network by its local dynamic parameters
is an important issue, and it is difficult to obtain the direct solution for.
The transformation method, which is originally used to control physical field
by designing material parameters, is proposed to obtain the necessary local
dynamic parameters when the global statuses of a network system are prescribed
in a space. The feasibility of this transformation method is demonstrated and
verified by two examples (a communication field cloak and a communication field
bender) in the network system. It is shown that the global system state can be
controlled by adjusting the local nodes dynamics with the transformation
method. Simulation results also show that the transformation method provides a
powerful, intuitive and direct way for the global statuses controlling of
network systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7258</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7258</id><created>2011-11-30</created><authors><author><keyname>Ravi</keyname><forenames>Nirlakalla</forenames></author><author><keyname>Satish</keyname><forenames>A.</forenames></author><author><keyname>Prasad</keyname><forenames>T. Jayachandra</forenames></author><author><keyname>Rao</keyname><forenames>T. Subba</forenames></author></authors><title>A New Design for Array Multiplier with Trade off in Power and Area</title><categories>cs.AR</categories><comments>5 pages, 6 figures; IJCSI International Journal of Computer Science
  Issues, Vol. 8, Issue 3, No. 2, May 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a low power and low area array multiplier with carry save adder
is proposed. The proposed adder eliminates the final addition stage of the
multiplier than the conventional parallel array multiplier. The conventional
and proposed multiplier both are synthesized with 16-T full adder. Among
Transmission Gate, Transmission Function Adder, 14-T, 16-T full adder shows
energy efficiency. In the proposed 4x4 multiplier to add carry bits with out
using Ripple Carry Adder (RCA) in the final stage, the carries given to the
input of the next left column input. Due to this the proposed multiplier shows
56 less transistor count, then cause trade off in power and area. The proposed
multiplier has shown 13.91% less power, 34.09% more speed and 59.91% less
energy consumption for TSMC 0.18nm technology at a supply voltage 2.0V than the
conventional multiplier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7265</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7265</id><created>2011-11-30</created><authors><author><keyname>Leszek</keyname><forenames>Szczecinski</forenames></author></authors><title>Linear Correction of Mismatched L-values in BICM receivers</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyze the problem of linear correction of the reliability
metrics (L-values) in BICM receivers. We want to find the correction factors
that minimize the probability of error of a maximum likelihood decoder that
uses the corrected L-values. To this end, we use the efficient approximation of
the pairwise error probability in the domain of the cumulant generating
functions (CGF) of the L-values and conclude that the optimal correction
factors are equal to the twice of the saddlepoint of the CGF. We provide a
simple numerical example of transmission in the presence of interference where
we demonstrate a notable improvement attainable with the proposed method. The
proposed method is compared with the one based on the maximization of
generalized mutual information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7271</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7271</id><created>2011-11-30</created><authors><author><keyname>Nava</keyname><forenames>Rodrigo</forenames></author><author><keyname>Crist&#xf3;bal</keyname><forenames>Gabriel</forenames></author><author><keyname>Escalante-Ram&#xed;rez</keyname><forenames>Boris</forenames></author></authors><title>Invariant texture analysis through Local Binary Patterns</title><categories>cs.CV</categories><comments>7 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many image processing applications, such as segmentation and
classification, the selection of robust features descriptors is crucial to
improve the discrimination capabilities in real world scenarios. In particular,
it is well known that image textures constitute power visual cues for feature
extraction and classification. In the past few years the local binary pattern
(LBP) approach, a texture descriptor method proposed by Ojala et al., has
gained increased acceptance due to its computational simplicity and more
importantly for encoding a powerful signature for describing textures. However,
the original algorithm presents some limitations such as noise sensitivity and
its lack of rotational invariance which have led to many proposals or
extensions in order to overcome such limitations. In this paper we performed a
quantitative study of the Ojala's original LBP proposal together with other
recently proposed LBP extensions in the presence of rotational, illumination
and noisy changes. In the experiments we have considered two different
databases: Brodatz and CUReT for different sizes of LBP masks. Experimental
results demonstrated the effectiveness and robustness of the described texture
descriptors for images that are subjected to geometric or radiometric changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7277</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7277</id><created>2011-11-30</created><authors><author><keyname>Hall</keyname><forenames>Rob</forenames></author><author><keyname>Nardi</keyname><forenames>Yuval</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen</forenames></author></authors><title>Achieving Both Valid and Secure Logistic Regression Analysis on
  Aggregated Data from Different Private Sources</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preserving the privacy of individual databases when carrying out statistical
calculations has a long history in statistics and had been the focus of much
recent attention in machine learning In this paper, we present a protocol for
computing logistic regression when the data are held by separate parties
without actually combining information sources by exploiting results from the
literature on multi-party secure computation. We provide only the final result
of the calculation compared with other methods that share intermediate values
and thus present an opportunity for compromise of values in the combined
database. Our paper has two themes: (1) the development of a secure protocol
for computing the logistic parameters, and a demonstration of its performances
in practice, and (2) and amended protocol that speeds up the computation of the
logistic function. We illustrate the nature of the calculations and their
accuracy using an extract of data from the Current Population Survey divided
between two parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7280</identifier>
 <datestamp>2011-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7280</id><created>2011-11-30</created><updated>2011-12-13</updated><authors><author><keyname>Goemans</keyname><forenames>Michel X.</forenames></author><author><keyname>Olver</keyname><forenames>Neil</forenames></author><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>Matroids and Integrality Gaps for Hypergraphic Steiner Tree Relaxations</title><categories>cs.DM cs.DS</categories><comments>Corrects an issue at the end of Section 3. Various other minor
  improvements to the exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until recently, LP relaxations have played a limited role in the design of
approximation algorithms for the Steiner tree problem. In 2010, Byrka et al.
presented a ln(4)+epsilon approximation based on a hypergraphic LP relaxation,
but surprisingly, their analysis does not provide a matching bound on the
integrality gap.
  We take a fresh look at hypergraphic LP relaxations for the Steiner tree
problem - one that heavily exploits methods and results from the theory of
matroids and submodular functions - which leads to stronger integrality gaps,
faster algorithms, and a variety of structural insights of independent
interest. More precisely, we present a deterministic ln(4)+epsilon
approximation that compares against the LP value and therefore proves a
matching ln(4) upper bound on the integrality gap.
  Similarly to Byrka et al., we iteratively fix one component and update the LP
solution. However, whereas they solve an LP at every iteration after
contracting a component, we show how feasibility can be maintained by a greedy
procedure on a well-chosen matroid. Apart from avoiding the expensive step of
solving a hypergraphic LP at each iteration, our algorithm can be analyzed
using a simple potential function. This gives an easy means to determine
stronger approximation guarantees and integrality gaps when considering
restricted graph topologies. In particular, this readily leads to a 73/60 bound
on the integrality gap for quasi-bipartite graphs.
  For the case of quasi-bipartite graphs, we present a simple algorithm to
transform an optimal solution to the bidirected cut relaxation to an optimal
solution of the hypergraphic relaxation, leading to a fast 73/60 approximation
for quasi-bipartite graphs. Furthermore, we show how the separation problem of
the hypergraphic relaxation can be solved by computing maximum flows, providing
a fast independence oracle for our matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7284</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7284</id><created>2011-11-30</created><updated>2013-04-30</updated><authors><author><keyname>Munemasa</keyname><forenames>Akihiro</forenames></author><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author><author><keyname>Taniguchi</keyname><forenames>Tetsuji</forenames></author></authors><title>Fat Hoffman graphs with smallest eigenvalue at least $-1-\tau$</title><categories>math.CO cs.DM</categories><comments>19 pages, 10 figures</comments><msc-class>05C50, 05C75</msc-class><journal-ref>Ars Mathematica Contemporanea 7 (2014) 247-262</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that all fat Hoffman graphs with smallest eigenvalue
at least -1-\tau, where \tau is the golden ratio, can be described by a finite
set of fat (-1-\tau)-irreducible Hoffman graphs. In the terminology of Woo and
Neumaier, we mean that every fat Hoffman graph with smallest eigenvalue at
least -1-\tau is an H-line graph, where H is the set of isomorphism classes of
maximal fat (-1-\tau)-irreducible Hoffman graphs. It turns out that there are
37 fat (-1-\tau)-irreducible Hoffman graphs, up to isomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7295</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7295</id><created>2011-11-30</created><updated>2011-12-02</updated><authors><author><keyname>Viswanathan</keyname><forenames>Raajay</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Laxman</keyname><forenames>Srivatsan</forenames></author><author><keyname>Arasu</keyname><forenames>Arvind</forenames></author></authors><title>A Learning Framework for Self-Tuning Histograms</title><categories>cs.DB cs.LG</categories><comments>Submitted to VLDB-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of estimating self-tuning histograms
using query workloads. To this end, we propose a general learning theoretic
formulation. Specifically, we use query feedback from a workload as training
data to estimate a histogram with a small memory footprint that minimizes the
expected error on future queries. Our formulation provides a framework in which
different approaches can be studied and developed. We first study the simple
class of equi-width histograms and present a learning algorithm, EquiHist, that
is competitive in many settings. We also provide formal guarantees for
equi-width histograms that highlight scenarios in which equi-width histograms
can be expected to succeed or fail. We then go beyond equi-width histograms and
present a novel learning algorithm, SpHist, for estimating general histograms.
Here we use Haar wavelets to reduce the problem of learning histograms to that
of learning a sparse vector. Both algorithms have multiple advantages over
existing methods: 1) simple and scalable extensions to multi-dimensional data,
2) scalability with number of histogram buckets and size of query feedback, 3)
natural extensions to incorporate new feedback and handle database updates. We
demonstrate these advantages over the current state-of-the-art, ISOMER, through
detailed experiments on real and synthetic data. In particular, we show that
SpHist obtains up to 50% less error than ISOMER on real-world multi-dimensional
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7297</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7297</id><created>2011-11-30</created><authors><author><keyname>Fernique</keyname><forenames>Thomas</forenames></author><author><keyname>Regnault</keyname><forenames>Damien</forenames></author></authors><title>Stochastic Flips on Dimer Tilings</title><categories>math.PR cond-mat.stat-mech cs.DM</categories><comments>13 pages, 10 figures; DMTCS Proceedings, 21st International Meeting
  on Probabilistic, Combinatorial, and Asymptotic Methods in the Analysis of
  Algorithms (AofA'10), 2010</comments><msc-class>60J10, 60C05, 52C23, 37A25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a Markov process inspired by the problem of
quasicrystal growth. It acts over dimer tilings of the triangular grid by
randomly performing local transformations, called {\em flips}, which do not
increase the number of identical adjacent tiles (this number can be thought as
the tiling energy). Fixed-points of such a process play the role of
quasicrystals. We are here interested in the worst-case expected number of
flips to converge towards a fixed-point. Numerical experiments suggest a bound
quadratic in the number n of tiles of the tiling. We prove a O(n^2.5) upper
bound and discuss the gap between this bound and the previous one. We also
briefly discuss the average-case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1111.7299</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1111.7299</id><created>2011-11-30</created><updated>2012-04-26</updated><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>Les crashs sont rationnels</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we show by using notions of equilibrium in infinite sequential games,
crashes or financial escalations are rational for economic or environmental
agents, who have a vision of an infinite world. This contradicts a picture of a
self-regulating, wise and pacific economic world. In other words, in this
context, equilibrium is not synonymous of stability. We try to draw, from this
statement, methodological consequences and new ways of thinking, especially in
economic game theory. Among those new paths, coinduction is the basis of our
reasoning in infinite games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0031</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0031</id><created>2011-11-30</created><authors><author><keyname>Gleich</keyname><forenames>David</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Neighborhoods are good communities</title><categories>cs.SI cs.DM cs.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communities of a social network are sets of vertices with more
connections inside the set than outside. We theoretically demonstrate that two
commonly observed properties of social networks, heavy-tailed degree
distributions and large clustering coefficients, imply the existence of vertex
neighborhoods (also known as egonets) that are themselves good communities. We
evaluate these neighborhood communities on a range of graphs. What we find is
that the neighborhood communities often exhibit conductance scores that are as
good as the Fiedler cut. Also, the conductance of neighborhood communities
shows similar behavior as the network community profile computed with a
personalized PageRank community detection method. The latter requires sweeping
over a great many starting vertices, which can be expensive. By using a small
and easy-to-compute set of neighborhood communities as seeds for these PageRank
communities, however, we find communities that precisely capture the behavior
of the network community profile when seeded everywhere in the graph, and at a
significant reduction in total work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0032</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0032</id><created>2011-11-30</created><authors><author><keyname>Kembellec</keyname><forenames>G&#xe9;rald</forenames></author><author><keyname>Saleh</keyname><forenames>Imad</forenames></author><author><keyname>Sauvaget</keyname><forenames>Catherine</forenames></author></authors><title>A model of Cross Language Retrieval for IT domain papers through a map
  of ACM Computing Classification System</title><categories>cs.DL cs.HC cs.IR</categories><comments>7 pages, 3 figures, 1 table</comments><acm-class>H.3.7</acm-class><doi>10.1109/MMCS.2009.5256709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a concept model, and the associated tool to help
advanced learners to find adapted bibliography. The purpose is the use of an IT
representation as educational research software for newcomers in research. We
use an ontology based on the ACM's Computing Classification System in order to
find scientific articles directly related to the new researcher's domain
without any formal request. An ontology translation in French is automatically
proposed and can be based on Web 2.0 enhanced by a community of users. A
visualization and navigation model is proposed to make it more accessible and
examples are given to show the interface of our tool: Ontology Navigator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0038</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0038</id><created>2011-11-30</created><authors><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>Information Theoretic Authentication and Secrecy Codes in the Splitting
  Model</title><categories>cs.CR cs.IT math.IT</categories><comments>4 pages (double-column); to appear in Proc. 2012 International Zurich
  Seminar on Communications (IZS 2012, Zurich)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the splitting model, information theoretic authentication codes allow
non-deterministic encoding, that is, several messages can be used to
communicate a particular plaintext. Certain applications require that the
aspect of secrecy should hold simultaneously. Ogata-Kurosawa-Stinson-Saido
(2004) have constructed optimal splitting authentication codes achieving
perfect secrecy for the special case when the number of keys equals the number
of messages. In this paper, we establish a construction method for optimal
splitting authentication codes with perfect secrecy in the more general case
when the number of keys may differ from the number of messages. To the best
knowledge, this is the first result of this type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0045</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0045</id><created>2011-11-30</created><updated>2012-03-19</updated><authors><author><keyname>Stojmirovi&#x107;</keyname><forenames>Aleksandar</forenames></author><author><keyname>Bliskovsky</keyname><forenames>Alexander</forenames></author><author><keyname>Yu</keyname><forenames>Yi-Kuo</forenames></author></authors><title>CytoITMprobe: a network information flow plugin for Cytoscape</title><categories>q-bio.QM cs.DB q-bio.MN</categories><comments>16 pages, 6 figures. Version 2</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  To provide the Cytoscape users the possibility of integrating ITM Probe into
their workflows, we developed CytoITMprobe, a new Cytoscape plugin.
CytoITMprobe maintains all the desirable features of ITM Probe and adds
additional flexibility not achievable through its web service version. It
provides access to ITM Probe either through a web server or locally. The input,
consisting of a Cytoscape network, together with the desired origins and/or
destinations of information and a dissipation coefficient, is specified through
a query form. The results are shown as a subnetwork of significant nodes and
several summary tables. Users can control the composition and appearance of the
subnetwork and interchange their ITM Probe results with other software tools
through tab-delimited files.
  The main strength of CytoITMprobe is its flexibility. It allows the user to
specify as input any Cytoscape network, rather than being restricted to the
pre-compiled protein-protein interaction networks available through the ITM
Probe web service. Users may supply their own edge weights and
directionalities. Consequently, as opposed to ITM Probe web service,
CytoITMprobe can be applied to many other domains of network-based research
beyond protein-networks. It also enables seamless integration of ITM Probe
results with other Cytoscape plugins having complementary functionality for
data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0049</identifier>
 <datestamp>2012-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0049</id><created>2011-11-30</created><authors><author><keyname>Ben-Naim</keyname><forenames>E.</forenames></author><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author></authors><title>Popularity-Driven Networking</title><categories>cond-mat.stat-mech cs.SI math.PR physics.soc-ph</categories><comments>5 pages, 2 figures</comments><journal-ref>EPL 97, 48003 (2012)</journal-ref><doi>10.1209/0295-5075/97/48003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the growth of connectivity in a network. In our model,
starting with a set of disjoint nodes, links are added sequentially. Each link
connects two nodes, and the connection rate governing this random process is
proportional to the degrees of the two nodes. Interestingly, this network
exhibits two abrupt transitions, both occurring at finite times. The first is a
percolation transition in which a giant component, containing a finite fraction
of all nodes, is born. The second is a condensation transition in which the
entire system condenses into a single, fully connected, component. We derive
the size distribution of connected components as well as the degree
distribution, which is purely exponential throughout the evolution.
Furthermore, we present a criterion for the emergence of sudden condensation
for general homogeneous connection rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0052</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0052</id><created>2011-11-30</created><updated>2013-12-02</updated><authors><author><keyname>Mashagba</keyname><forenames>Eman Al</forenames></author><author><keyname>Mashagba</keyname><forenames>Feras Al</forenames></author><author><keyname>Nassar</keyname><forenames>Mohammad Othman</forenames></author></authors><title>Query Optimization Using Genetic Algorithms in the Vector Space Model</title><categories>cs.IR</categories><comments>7 pages; ISSN (online): 1694-0814 This paper has been withdrawn by
  the author due to a crucial errors in table: 2,3,4,5,6 and in the results</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), Volume
  8, Issue 5, pp 450-457, September 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In information retrieval research; Genetic Algorithms (GA) can be used to
find global solutions in many difficult problems. This study used different
similarity measures (Dice, Inner Product) in the VSM, for each similarity
measure we compared ten different GA approaches based on different fitness
functions, different mutations and different crossover strategies to find the
best strategy and fitness function that can be used when the data collection is
the Arabic language. Our results shows that the GA approach which uses
one-point crossover operator, point mutation and Inner Product similarity as a
fitness function is the best IR system in VSM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0054</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0054</id><created>2011-11-30</created><updated>2013-12-02</updated><authors><author><keyname>Nassar</keyname><forenames>Mohammad Othman</forenames></author><author><keyname>Mashagba</keyname><forenames>Feras Al</forenames></author><author><keyname>Mashagba</keyname><forenames>Eman Al</forenames></author></authors><title>Improving the User Query for the Boolean Model Using Genetic Algorithms</title><categories>cs.IR</categories><comments>4 pages; ISSN (online): 1694-0814 this paper has been withdrawn by
  the author due to a crucial errors in table 1 and 2. other errors are related
  to the final results</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), Volume
  8, Issue 5, pp 66-70, September 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Use of genetic algorithms in the Information retrieval (IR) area,
especially in optimizing a user query in Arabic data collections is presented
in this paper. Very little research has been carried out on Arabic text
collections. Boolean model have been used in this research. To optimize the
query using GA we used different fitness functions, different mutation
strategies to find which is the best strategy and fitness function that can be
used with Boolean model when the data collection is the Arabic language. Our
results show that the best GA strategy for the Boolean model is the GA (M2,
Precision) method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0057</identifier>
 <datestamp>2011-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0057</id><created>2011-11-30</created><updated>2011-12-13</updated><authors><author><keyname>Fernando</keyname><forenames>Nirmal</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Flip-OFDM for Unipolar Communication Systems</title><categories>cs.IT math.IT</categories><comments>19 pages, 8 pages (re-uploaded with corrected Fig 2a)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unipolar communications systems can transmit information using only real and
positive signals. This includes a variety of physical channels ranging from
optical (fiber or free-space), to RF wireless using amplitude modulation with
non-coherent reception, to baseband single wire communications. Unipolar OFDM
techniques enable to efficiently compensate frequency selective distortion in
the unipolar communication systems. One of the leading examples of unipolar
OFDM is asymmetric clipped optical OFDM (ACO-OFDM) originally proposed for
optical communications. Flip-OFDM is an alternative approach that was proposed
in a patent, but its performance and full potentials have never been
investigated in the literature. In this paper, we first compare Flip-OFDM and
ACO-OFDM, and show that both techniques have the same performance but different
complexities (Flip-OFDM offers 50% saving). We then propose a new detection
scheme, which enables to reduce the noise at the Flip-OFDM receiver by almost
3dB. The analytical performance of the noise filtering schemes is supported by
the simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0059</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0059</id><created>2011-11-30</created><authors><author><keyname>McCann</keyname><forenames>Sancho</forenames></author><author><keyname>Lowe</keyname><forenames>David G.</forenames></author></authors><title>Local Naive Bayes Nearest Neighbor for Image Classification</title><categories>cs.CV</categories><report-no>TR-2011-11</report-no><acm-class>I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Local Naive Bayes Nearest Neighbor, an improvement to the NBNN
image classification algorithm that increases classification accuracy and
improves its ability to scale to large numbers of object classes. The key
observation is that only the classes represented in the local neighborhood of a
descriptor contribute significantly and reliably to their posterior probability
estimates. Instead of maintaining a separate search structure for each class,
we merge all of the reference data together into one search structure, allowing
quick identification of a descriptor's local neighborhood. We show an increase
in classification accuracy when we ignore adjustments to the more distant
classes and show that the run time grows with the log of the number of classes
rather than linearly in the number of classes as did the original. This gives a
100 times speed-up over the original method on the Caltech 256 dataset. We also
provide the first head-to-head comparison of NBNN against spatial pyramid
methods using a common set of input features. We show that local NBNN
outperforms all previous NBNN based methods and the original spatial pyramid
model. However, we find that local NBNN, while competitive with, does not beat
state-of-the-art spatial pyramid methods that use local soft assignment and
max-pooling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0061</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0061</id><created>2011-11-30</created><authors><author><keyname>Shadbakht</keyname><forenames>Sormeh</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>On the Entropy Region of Gaussian Random Variables</title><categories>cs.IT math.IT</categories><comments>39 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given n (discrete or continuous) random variables X_i, the
(2^n-1)-dimensional vector obtained by evaluating the joint entropy of all
non-empty subsets of {X_1,...,X_n} is called an entropic vector. Determining
the region of entropic vectors is an important open problem with many
applications in information theory. Recently, it has been shown that the
entropy regions for discrete and continuous random variables, though different,
can be determined from one another. An important class of continuous random
variables are those that are vector-valued and jointly Gaussian. In this paper
we give a full characterization of the convex cone of the entropy region of
three jointly Gaussian vector-valued random variables and prove that it is the
same as the convex cone of three scalar-valued Gaussian random variables and
further that it yields the entire entropy region of 3 arbitrary random
variables. We further determine the actual entropy region of 3 vector-valued
jointly Gaussian random variables through a conjecture. For n&gt;=4 number of
random variables, we point out a set of 2^n-1-n(n+1)/2 minimal necessary and
sufficient conditions that 2^n-1 numbers must satisfy in order to correspond to
the entropy vector of n scalar jointly Gaussian random variables. This improves
on a result of Holtz and Sturmfels which gave a nonminimal set of conditions.
These constraints are related to Cayley's hyperdeterminant and hence with an
eye towards characterizing the entropy region of jointly Gaussian random
variables, we also present some new results in this area. We obtain a new
(determinant) formula for the 2*2*2 hyperdeterminant and we also give a new
(transparent) proof of the fact that the principal minors of an n*n symmetric
matrix satisfy the 2*2*...*2 (up to n times) hyperdeterminant relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0062</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0062</id><created>2011-11-30</created><updated>2012-05-06</updated><authors><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Qi</keyname><forenames>Yanfeng</forenames></author><author><keyname>Xu</keyname><forenames>Maozhi</forenames></author><author><keyname>Wang</keyname><forenames>Baocheng</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>A new class of hyper-bent Boolean functions in binomial forms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bent functions, which are maximally nonlinear Boolean functions with even
numbers of variables and whose Hamming distance to the set of all affine
functions equals $2^{n-1}\pm 2^{\frac{n}{2}-1}$, were introduced by Rothaus in
1976 when he considered problems in combinatorics. Bent functions have been
extensively studied due to their applications in cryptography, such as S-box,
block cipher and stream cipher. Further, they have been applied to coding
theory, spread spectrum and combinatorial design. Hyper-bent functions, as a
special class of bent functions, were introduced by Youssef and Gong in 2001,
which have stronger properties and rarer elements. Many research focus on the
construction of bent and hyper-bent functions. In this paper, we consider
functions defined over $\mathbb{F}_{2^n}$ by
$f_{a,b}:=\mathrm{Tr}_{1}^{n}(ax^{(2^m-1)})+\mathrm{Tr}_{1}^{4}(bx^{\frac{2^n-1}{5}})$,
where $n=2m$, $m\equiv 2\pmod 4$, $a\in \mathbb{F}_{2^m}$ and
$b\in\mathbb{F}_{16}$. When $a\in \mathbb{F}_{2^m}$ and $(b+1)(b^4+b+1)=0$,
with the help of Kloosterman sums and the factorization of $x^5+x+a^{-1}$, we
present a characterization of hyper-bentness of $f_{a,b}$. Further, we use
generalized Ramanujan-Nagell equations to characterize hyper-bent functions of
$f_{a,b}$ in the case $a\in\mathbb{F}_{2^{\frac{m}{2}}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0067</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0067</id><created>2011-11-30</created><authors><author><keyname>Seaman</keyname><forenames>Rob</forenames></author></authors><title>An Inventory of UTC Dependencies for IRAF</title><categories>astro-ph.IM cs.SE</categories><comments>Contribution to the Colloquium on Decoupling Civil Timekeeping from
  Earth Rotation: http://futureofutc.org, 9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Image Reduction and Analysis Facility is a scientific image processing
package widely used throughout the astronomical community. IRAF has been
developed and distributed by the National Optical Astronomy Observatory in
Tucson, Arizona since the early 1980's. Other observatories and projects have
written many dozens of layered external application packages. More than ten
thousand journal articles acknowledge the use of IRAF and thousands of
professional astronomers rely on it. As with many other classes of astronomical
software, IRAF depends on Universal Time (UT) in many modules throughout its
codebase. The author was the Y2K lead for IRAF in the late 1990's. A
conservative underestimate of the initial inventory of UTC &quot;hits&quot; in IRAF
(e.g., from search terms like &quot;UT&quot;, &quot;GMT&quot; and &quot;MJD&quot;) contains several times as
many files as the corresponding Y2K (&quot;millennium bug&quot;) inventory did in the
1990's. We will discuss dependencies of IRAF upon Coordinated Universal Time,
and implications of these for the broader astronomical community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0071</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0071</id><created>2011-11-30</created><updated>2012-03-13</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Robustly Stable Signal Recovery in Compressed Sensing with Structured
  Matrix Perturbation</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 60, no. 9, pp.
  4658--4671, 2012</journal-ref><doi>10.1109/TSP.2012.2201152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sparse signal recovery in the standard compressed sensing (CS) problem
requires that the sensing matrix be known a priori. Such an ideal assumption
may not be met in practical applications where various errors and fluctuations
exist in the sensing instruments. This paper considers the problem of
compressed sensing subject to a structured perturbation in the sensing matrix.
Under mild conditions, it is shown that a sparse signal can be recovered by
$\ell_1$ minimization and the recovery error is at most proportional to the
measurement noise level, which is similar to the standard CS result. In the
special noise free case, the recovery is exact provided that the signal is
sufficiently sparse with respect to the perturbation level. The formulated
structured sensing matrix perturbation is applicable to the direction of
arrival estimation problem, so has practical relevance. Algorithms are proposed
to implement the $\ell_1$ minimization problem and numerical simulations are
carried out to verify the result obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0076</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0076</id><created>2011-11-30</created><updated>2013-08-01</updated><authors><author><keyname>Della Penna</keyname><forenames>Nicolas</forenames></author><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author></authors><title>Bandit Market Makers</title><categories>q-fin.TR cs.GT stat.ML</categories><comments>A previous version of this work appeared in the NIPS 2011 Workshop on
  Computational Social Science and the Wisdom of the Crowds</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce a modular framework for market making. It combines cost-function
based automated market makers with bandit algorithms. We obtain worst-case
profits guarantee's relative to the best in hindsight within a class of natural
&quot;overround&quot; cost functions . This combination allow us to have
distribution-free guarantees on the regret of profits while preserving the
bounded worst-case losses and computational tractability over combinatorial
spaces of the cost function based approach. We present simulation results to
better understand the practical behaviour of market makers from the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0077</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0077</id><created>2011-11-30</created><authors><author><keyname>Hu</keyname><forenames>Ke</forenames></author><author><keyname>Tang</keyname><forenames>Yi</forenames></author></authors><title>Immunization for complex network based on the effective degree of vertex</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 5 figures</comments><doi>10.1142/S021797921250052X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic idea of many effective immunization strategies is first to rank the
importance of vertices according to the degrees of vertices and then remove the
vertices from highest importance to lowest until the network becomes
disconnected. Here we define the effective degrees of vertex, i.e., the number
of its connections linking to un-immunized nodes in current network during the
immunization procedure, to rank the importance of vertex, and modify these
strategies by using the effective degrees of vertices. Simulations on both the
scale-free network models with various degree correlations and two real
networks have revealed that the immunization strategies based on the effective
degrees are often more effective than those based on the degrees in the initial
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0088</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0088</id><created>2011-12-01</created><updated>2012-07-23</updated><authors><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author><author><keyname>Bapat</keyname><forenames>R. B.</forenames></author><author><keyname>Balaji</keyname><forenames>R.</forenames></author></authors><title>Simple expressions for the long walk distance</title><categories>math.CO cs.DM math.MG math.RA</categories><comments>7 pages. Accepted for publication in Linear Algebra and Its
  Applications</comments><msc-class>05C12, 15B48, 05C50</msc-class><doi>10.1016/j.laa.2012.07.033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The walk distances in graphs are defined as the result of appropriate
transformations of the $\sum_{k=0}^\infty(tA)^k$ proximity measures, where $A$
is the weighted adjacency matrix of a connected weighted graph and $t$ is a
sufficiently small positive parameter. The walk distances are graph-geodetic,
moreover, they converge to the shortest path distance and to the so-called long
walk distance as the parameter $t$ approaches its limiting values. In this
paper, simple expressions for the long walk distance are obtained. They involve
the generalized inverse, minors, and inverses of submatrices of the symmetric
irreducible singular M-matrix ${\cal L}=\rho I-A,$ where $\rho$ is the Perron
root of $A.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0097</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0097</id><created>2011-12-01</created><authors><author><keyname>Mouradian</keyname><forenames>Alexandre</forenames><affiliation>CITI Insa Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Aug&#xe9;-Blum</keyname><forenames>Isabelle</forenames><affiliation>CITI Insa Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>1-D Coordinate Based on Local Information for MAC and Routing Issues in
  WSNs</title><categories>cs.NI</categories><comments>(2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More and more critical Wireless Sensor Networks (WSNs) applications are
emerging. Those applications need reliability and respect of time constraints.
The underlying mechanisms such as MAC and routing must handle such
requirements. Our approach to the time constraint problem is to bound the
hop-count between a node and the sink and the time it takes to do a hop so the
end-to-end delay can be bounded and the communications are thus real-time. For
reliability purpose we propose to select forwarder nodes depending on how they
are connected in the direction of the sink. In order to be able to do so we
need a coordinate (or a metric) that gives information on hop-count, that
allows to strongly differentiate nodes and gives information on the
connectivity of each node keeping in mind the intrinsic constraints of WSWs
such as energy consumption, autonomy, etc. Due to the efficiency and
scalability of greedy routing in WSNs and the financial cost of GPS chips,
Virtual Coordinate Systems (VCSs) for WSNs have been proposed. A category of
VCSs is based on the hop-count from the sink, this scheme leads to many nodes
having the same coordinate. The main advantage of this system is that the hops
number of a packet from a source to the sink is known. Nevertheless, it does
not allow to differentiate the nodes with the same hop-count. In this report we
propose a novel hop-count-based VCS which aims at classifying the nodes having
the same hop-count depending on their connectivity and at differentiating nodes
in a 2-hop neighborhood. Those properties make the coordinates, which also can
be viewed as a local identifier, a very powerful metric which can be used in
WSNs mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0101</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0101</id><created>2011-12-01</created><authors><author><keyname>Liu</keyname><forenames>Keqin</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Dynamic Intrusion Detection in Resource-Constrained Cyber Networks</title><categories>cs.SY math.DS math.OC</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a large-scale cyber network with N components (e.g., paths,
servers, subnets). Each component is either in a healthy state (0) or an
abnormal state (1). Due to random intrusions, the state of each component
transits from 0 to 1 over time according to certain stochastic process. At each
time, a subset of K (K &lt; N) components are checked and those observed in
abnormal states are fixed. The objective is to design the optimal scheduling
for intrusion detection such that the long-term network cost incurred by all
abnormal components is minimized. We formulate the problem as a special class
of Restless Multi-Armed Bandit (RMAB) process. A general RMAB suffers from the
curse of dimensionality (PSPACE-hard) and numerical methods are often
inapplicable. We show that, for this class of RMAB, Whittle index exists and
can be obtained in closed form, leading to a low-complexity implementation of
Whittle index policy with a strong performance. For homogeneous components,
Whittle index policy is shown to have a simple structure that does not require
any prior knowledge on the intrusion processes. Based on this structure,
Whittle index policy is further shown to be optimal over a finite time horizon
with an arbitrary length. Beyond intrusion detection, these results also find
applications in queuing networks with finite-size buffers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0126</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0126</id><created>2011-12-01</created><authors><author><keyname>Behrens</keyname><forenames>S.</forenames></author><author><keyname>Nicaud</keyname><forenames>C.</forenames></author><author><keyname>Nicodeme</keyname><forenames>P.</forenames></author></authors><title>An automaton approach for waiting times in DNA evolution</title><categories>cs.DM cs.CE cs.FL q-bio.PE</categories><comments>8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent article, Behrens and Vingron (JCB 17, 12, 2010) compute waiting
times for k-mers to appear during DNA evolution under the assumption that the
considered k-mers do not occur in the initial DNA sequence, an issue arising
when studying the evolution of regulatory DNA sequences with regard to
transcription factor (TF) binding site emergence. The mathematical analysis
underlying their computation assumes that occurrences of words under interest
do not overlap. We relax here this assumption by use of an automata approach.
In an alphabet of size 4 like the DNA alphabet, most words have no or a low
autocorrelation; therefore, globally, our results confirm those of Behrens and
Vingron. The outcome is quite different when considering highly autocorrelated
k-mers; in this case, the autocorrelation pushes down the probability of
occurrence of these k-mers at generation 1 and, consequently, increases the
waiting time for apparition of these k-mers up to 40%. An analysis of existing
TF binding sites unveils a significant proportion of k-mers exhibiting
autocorrelation. Thus, our computations based on automata greatly improve the
accuracy of predicting waiting times for the emergence of TF binding sites to
appear during DNA evolution. We do the computation in the Bernoulli or M0
model; computations in the M1 model, a Markov model of order 1, are more costly
in terms of time and memory but should produce similar results. While Behrens
and Vingron considered specifically promoters of length 1000, we extend the
results to promoters of any size; we exhibit the property that the probability
that a k-mer occurs at generation time 1 while being absent at time 0 behaves
linearly with respect to the length of the promoter, which induces a hyperbolic
behaviour of the waiting time of any k-mer with respect to the length of the
promoter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0136</identifier>
 <datestamp>2012-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0136</id><created>2011-12-01</created><updated>2012-11-01</updated><authors><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Sampling High-Dimensional Bandlimited Fields on Low-Dimensional
  Manifolds</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, Nov 2011;
  revised July 2012; accepted Oct 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the task of sampling and reconstructing a bandlimited spatial field
in $\Re^2$ using moving sensors that take measurements along their path. It is
inexpensive to increase the sampling rate along the paths of the sensors but
more expensive to increase the total distance traveled by the sensors per unit
area, which we call the \emph{path density}. In this paper we introduce the
problem of designing sensor trajectories that are minimal in path density
subject to the condition that the measurements of the field on these
trajectories admit perfect reconstruction of bandlimited fields. We study
various possible designs of sampling trajectories. Generalizing some ideas from
the classical theory of sampling on lattices, we obtain necessary and
sufficient conditions on the trajectories for perfect reconstruction. We show
that a single set of equispaced parallel lines has the lowest path density from
certain restricted classes of trajectories that admit perfect reconstruction.
  We then generalize some of our results to higher dimensions. We first obtain
results on designing sampling trajectories in higher dimensional fields.
Further, interpreting trajectories as 1-dimensional manifolds, we extend some
of our ideas to higher dimensional sampling manifolds. We formulate the problem
of designing $\kappa$-dimensional sampling manifolds for $d$-dimensional
spatial fields that are minimal in \emph{manifold density}, a natural
generalization of the path density. We show that our results on sampling
trajectories for fields in $\Re^2$ can be generalized to analogous results on
$d-1$-dimensional sampling manifolds for $d$-dimensional spatial fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0147</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0147</id><created>2011-12-01</created><authors><author><keyname>Belavkin</keyname><forenames>Viacheslav P.</forenames></author><author><keyname>Brown</keyname><forenames>Matthew F.</forenames></author></authors><title>Q-Adapted Quantum Stochastic Integrals and Differentials in Fock Scale</title><categories>math-ph cs.IT math.IT math.MP math.QA quant-ph</categories><comments>A similar version is due to appear in the proceedings of the 13th
  workshop: non-commutative harmonic analysis. 18 pages</comments><msc-class>81S25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we first introduce the Fock-Guichardet formalism for the
quantum stochastic integration, then the four fundamental processes of the
dynamics are introduced in the canonical basis as the operator-valued measures
of the QS integration over a space-time. Then rigorous analysis of the QS
integrals is carried out, and continuity of the QS derivative is proved.
Finally, Q-adapted dynamics is discussed, including Bosonic Q=1, Fermionic
Q=-1, and monotone Q=0 quantum dynamics. These may be of particular interest to
quantum field theory, quantum open systems, and quantum theory of stochastic
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0168</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0168</id><created>2011-12-01</created><authors><author><keyname>Othman</keyname><forenames>Achraf</forenames></author><author><keyname>Jemni</keyname><forenames>Mohamed</forenames></author></authors><title>Statistical Sign Language Machine Translation: from English written text
  to American Sign Language Gloss</title><categories>cs.CL</categories><comments>9 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 3, 2011, 65-73</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This works aims to design a statistical machine translation from English text
to American Sign Language (ASL). The system is based on Moses tool with some
modifications and the results are synthesized through a 3D avatar for
interpretation. First, we translate the input text to gloss, a written form of
ASL. Second, we pass the output to the WebSign Plug-in to play the sign.
Contributions of this work are the use of a new couple of language English/ASL
and an improvement of statistical machine translation based on string matching
thanks to Jaro-distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0184</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0184</id><created>2011-12-01</created><updated>2014-04-10</updated><authors><author><keyname>Konrad</keyname><forenames>Christian</forenames></author><author><keyname>Magniez</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author></authors><title>Maximum Matching in Semi-Streaming with Few Passes</title><categories>cs.DS</categories><comments>Algorithms for general graphs have been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the semi-streaming model, an algorithm receives a stream of edges of a
graph in arbitrary order and uses a memory of size $O(n \mbox{ polylog } n)$,
where $n$ is the number of vertices of a graph. In this work, we present
semi-streaming algorithms that perform one or two passes over the input stream
for maximum matching with no restrictions on the input graph, and for the
important special case of bipartite graphs that we refer to as maximum
bipartite matching (MBM). The Greedy matching algorithm performs one pass over
the input and outputs a $1/2$ approximation. Whether there is a better one-pass
algorithm has been an open question since the appearance of the first paper on
streaming algorithms for matching problems in 2005 [Feigenbaum et al., SODA
2005]. We make the following progress on this problem:
  In the one-pass setting, we show that there is a deterministic semi-streaming
algorithm for MBM with expected approximation factor $1/2+0.005$, assuming that
edges arrive one by one in (uniform) random order. We extend this algorithm to
general graphs, and we obtain a $1/2+0.003$ approximation.
  In the two-pass setting, we do not require the random arrival order
assumption (the edge stream is in arbitrary order). We present a simple
randomized two-pass semi-streaming algorithm for MBM with expected
approximation factor $1/2 + 0.019$. Furthermore, we discuss a more involved
deterministic two-pass semi-streaming algorithm for MBM with approximation
factor $1/2 + 0.019$ and a generalization of this algorithm to general graphs
with approximation factor $1/2 + 0.0071$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0195</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0195</id><created>2011-12-01</created><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>Ma</keyname><forenames>shaodan</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author></authors><title>Cooperative Beamforming for Dual-Hop Amplify-and-Forward Multi-Antenna
  Relaying Cellular Networks</title><categories>cs.IT math.IT</categories><comments>24 Pages, 8 figures, Submitted to Wireless Communications April 2010;
  Wireless Communications April 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, linear beamforming design for amplify-and-forward relaying
cellular networks is considered, in which base station, relay station and
mobile terminals are all equipped with multiple antennas. The design is based
on minimum mean-square-error criterion, and both uplink and downlink scenarios
are considered. It is found that the downlink and uplink beamforming design
problems are in the same form, and iterative algorithms with the same structure
can be used to solve the design problems. For the specific cases of fully
loaded or overloaded uplink systems, a novel algorithm is derived and its
relationships with several existing beamforming design algorithms for
conventional MIMO or multiuser systems are revealed. Simulation results are
presented to demonstrate the performance advantage of the proposed design
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0204</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0204</id><created>2011-12-01</created><authors><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author><author><keyname>Sadedin</keyname><forenames>Suzanne</forenames></author><author><keyname>De Wilde</keyname><forenames>Philippe</forenames></author></authors><title>Digital Ecosystems: Ecosystem-Oriented Architectures</title><categories>cs.NI cs.MA cs.NE</categories><comments>39 pages, 26 figures, journal</comments><doi>10.1007/s11047-011-9254-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We view Digital Ecosystems to be the digital counterparts of biological
ecosystems. Here, we are concerned with the creation of these Digital
Ecosystems, exploiting the self-organising properties of biological ecosystems
to evolve high-level software applications. Therefore, we created the Digital
Ecosystem, a novel optimisation technique inspired by biological ecosystems,
where the optimisation works at two levels: a first optimisation, migration of
agents which are distributed in a decentralised peer-to-peer network, operating
continuously in time; this process feeds a second optimisation based on
evolutionary computing that operates locally on single peers and is aimed at
finding solutions to satisfy locally relevant constraints. The Digital
Ecosystem was then measured experimentally through simulations, with measures
originating from theoretical ecology, evaluating its likeness to biological
ecosystems. This included its responsiveness to requests for applications from
the user base, as a measure of the ecological succession (ecosystem maturity).
Overall, we have advanced the understanding of Digital Ecosystems, creating
Ecosystem-Oriented Architectures where the word ecosystem is more than just a
metaphor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0210</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0210</id><created>2011-11-29</created><authors><author><keyname>Wawrzyniak</keyname><forenames>Karol</forenames></author><author><keyname>Wislicki</keyname><forenames>Wojciech</forenames></author></authors><title>Mesoscopic approach to minority games in herd regime</title><categories>nlin.AO cs.MA math.DS q-fin.TR stat.AP</categories><comments>arXiv admin note: substantial text overlap with arXiv:0907.3231</comments><doi>10.1016/j.physa.2011.11.041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study minority games in efficient regime. By incorporating the utility
function and aggregating agents with similar strategies we develop an effective
mesoscale notion of state of the game. Using this approach, the game can be
represented as a Markov process with substantially reduced number of states
with explicitly computable probabilities. For any payoff, the finiteness of the
number of states is proved. Interesting features of an extensive random
variable, called aggregated demand, viz. its strong inhomogeneity and presence
of patterns in time, can be easily interpreted. Using Markov theory and
quenched disorder approach, we can explain important macroscopic
characteristics of the game: behavior of variance per capita and predictability
of the aggregated demand. We prove that in case of linear payoff many
attractors in the state space are possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0213</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0213</id><created>2011-12-01</created><authors><author><keyname>Gr&#xfc;ning</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sporea</keyname><forenames>Ioana</forenames></author></authors><title>Supervised Learning of Logical Operations in Layered Spiking Neural
  Networks with Spike Train Encoding</title><categories>cs.NE q-bio.NC</categories><comments>15 pages, 4 figures</comments><journal-ref>Neural Processing Letters October 2012, Volume 36, Issue 2, pp
  117-134</journal-ref><doi>10.1007/s11063-012-9225-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Few algorithms for supervised training of spiking neural networks exist that
can deal with patterns of multiple spikes, and their computational properties
are largely unexplored. We demonstrate in a set of simulations that the ReSuMe
learning algorithm can be successfully applied to layered neural networks.
Input and output patterns are encoded as spike trains of multiple precisely
timed spikes, and the network learns to transform the input trains into target
output trains. This is done by combining the ReSuMe learning algorithm with
multiplicative scaling of the connections of downstream neurons.
  We show in particular that layered networks with one hidden layer can learn
the basic logical operations, including Exclusive-Or, while networks without
hidden layer cannot, mirroring an analogous result for layered networks of rate
neurons.
  While supervised learning in spiking neural networks is not yet fit for
technical purposes, exploring computational properties of spiking neural
networks advances our understanding of how computations can be done with spike
trains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0215</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0215</id><created>2011-12-01</created><authors><author><keyname>Griesmayer</keyname><forenames>Andreas</forenames></author><author><keyname>Liu</keyname><forenames>Zhiming</forenames></author><author><keyname>Morisset</keyname><forenames>Charles</forenames></author><author><keyname>Wang</keyname><forenames>Shuling</forenames></author></authors><title>A Framework for Automated and Certified Refinement Steps</title><categories>cs.SE</categories><comments>24 pages, submitted to Innovations in Systems and Software
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The refinement calculus provides a methodology for transforming an abstract
specification into a concrete implementation, by following a succession of
refinement rules. These rules have been mechanized in theorem-provers, thus
providing a formal and rigorous way to prove that a given program refines
another one. In a previous work, we have extended this mechanization for
object-oriented programs, where the memory is represented as a graph, and we
have integrated our approach within the rCOS tool, a model-driven software
development tool providing a refinement language. Hence, for any refinement
step, the tool automatically generates the corresponding proof obligations and
the user can manually discharge them, using a provided library of refinement
lemmas. In this work, we propose an approach to automate the search of possible
refinement rules from a program to another, using the rewriting tool Maude.
Each refinement rule in Maude is associated with the corresponding lemma in
Isabelle, thus allowing the tool to automatically generate the Isabelle proof
when a refinement rule can be automatically found. The user can add a new
refinement rule by providing the corresponding Maude rule and Isabelle lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0217</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0217</id><created>2011-12-01</created><authors><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author><author><keyname>Sprecher</keyname><forenames>Markus</forenames></author></authors><title>A Polynomial-Time Algorithm for the Tridiagonal and Hessenberg P-Matrix
  Linear Complementarity Problem</title><categories>math.OC cs.CC</categories><comments>4 pages, 3 figures</comments><msc-class>65K05, 90C33</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial-time dynamic programming algorithm for solving the
linear complementarity problem with tridiagonal or, more generally, Hessenberg
P-matrices. We briefly review three known tractable matrix classes and show
that none of them contains all tridiagonal P-matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0221</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0221</id><created>2011-12-01</created><updated>2013-06-17</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames><affiliation>University of Liverpool</affiliation></author><author><keyname>Schewe</keyname><forenames>Sven</forenames><affiliation>University of Liverpool</affiliation></author></authors><title>Time and Parallelizability Results for Parity Games with Bounded Tree
  and DAG Width</title><categories>cs.GT</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (June 18,
  2013) lmcs:791</journal-ref><doi>10.2168/LMCS-9(2:6)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parity games are a much researched class of games in NP intersect CoNP that
are not known to be in P. Consequently, researchers have considered specialised
algorithms for the case where certain graph parameters are small. In this
paper, we study parity games on graphs with bounded treewidth, and graphs with
bounded DAG width. We show that parity games with bounded DAG width can be
solved in O(n^(k+3) k^(k + 2) (d + 1)^(3k + 2)) time, where n, k, and d are the
size, treewidth, and number of priorities in the parity game. This is an
improvement over the previous best algorithm, given by Berwanger et al., which
runs in n^O(k^2) time. We also show that, if a tree decomposition is provided,
then parity games with bounded treewidth can be solved in O(n k^(k + 5) (d +
1)^(3k + 5)) time. This improves over previous best algorithm, given by
Obdrzalek, which runs in O(n d^(2(k+1)^2)) time. Our techniques can also be
adapted to show that the problem of solving parity games with bounded treewidth
lies in the complexity class NC^2, which is the class of problems that can be
efficiently parallelized. This is in stark contrast to the general parity game
problem, which is known to be P-hard, and thus unlikely to be contained in NC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0241</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0241</id><created>2011-12-01</created><authors><author><keyname>Liu</keyname><forenames>Weiping</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Zhou</keyname><forenames>Yanbo</forenames></author></authors><title>Degree heterogeneity in spatial networks with total cost constraint</title><categories>physics.soc-ph cs.SI stat.CO</categories><comments>4 pages, 4 figures</comments><journal-ref>Europhysics Letter 98, 28003 (2012)</journal-ref><doi>10.1209/0295-5075/98/28003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, In [Phys. Rev. Lett. 104, 018701 (2010)] the authors studied a
spatial network which is constructed from a regular lattice by adding
long-range edges (shortcuts) with probability $P_{ij}\sim r_{ij}^{-\alpha}$,
where $r_{ij}$ is the Manhattan length of the long-range edges. The total
length of the additional edges is subject to a cost constraint ($\sum r=C$).
These networks have fixed optimal exponent $\alpha$ for transportation
(measured by the average shortest-path length). However, we observe that the
degree in such spatial networks is homogenously distributed, which is far
different from real networks such as airline systems. In this paper, we propose
a method to introduce degree heterogeneity in spatial networks with total cost
constraint. Results show that with degree heterogeneity the optimal exponent
shifts to a smaller value and the average shortest-path length can further
decrease. Moreover, we consider the synchronization on the spatial networks and
related results are discussed. Our new model may better reproduce the features
of many real transportation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0245</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0245</id><created>2011-12-01</created><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Simultaneous PQ-Ordering with Applications to Constrained Embedding
  Problems</title><categories>cs.DS cs.DM</categories><comments>46 pages, 15 figures</comments><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define and study the new problem Simultaneous PQ-Ordering.
Its input consists of a set of PQ-trees, which represent sets of circular
orders of their leaves, together with a set of child-parent relations between
these PQ-trees, such that the leaves of the child form a subset of the leaves
of the parent. Simultaneous PQ-Ordering asks whether orders of the leaves of
each of the trees can be chosen simultaneously, that is, for every child-parent
relation the order chosen for the parent is an extension of the order chosen
for the child. We show that Simultaneous PQ-Ordering is NP-complete in general
and that it is efficiently solvable for a special subset of instances, the
2-fixed instances. We then show that several constrained embedding problems can
be formulated as such 2-fixed instances.
  In particular, we obtain a linear-time algorithm for Partially PQ-Constrained
Planarity for biconnected graphs, a common generalization of two recently
considered embedding problems, and a quadratic-time algorithm for Simultaneous
Embedding with Fixed Edges for biconnected graphs with a connected
intersection; formerly only the much more restricted case that the intersection
is biconnected was known to be efficiently solvable. Both results can be
extended to the case where the input graphs are not necessarily biconnected but
have the property that each cutvertex is contained in at most two non-trivial
blocks. This includes for example the case where both graphs have maximum
degree 5. Moreover, we give an optimal linear-time algorithm for recognition of
simultaneous interval graphs, improving upon a recent O(n^2 log n)-time
algorithm due to Jampani and Lubiw and show that this can be used to also solve
the problem of extending partial interval representations of graphs with n
vertices and m edges in time O(n + m), improving a recent result of Klav\'ik et
al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0253</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0253</id><created>2011-12-01</created><updated>2011-12-03</updated><authors><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author></authors><title>Singularities and global stability of decentralized formations in the
  plane</title><categories>math.OC cs.SY</categories><comments>33 pages, 11 figures, submitted for publication. Replaces and updates
  the second part of arXiv:1101.2421</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formation control is concerned with the design of control laws that stabilize
agents at given distances from each other, with the constraint that an agent's
dynamics can depend only on a subset of other agents. When the information flow
graph of the system, which encodes this dependency, is acyclic, simple control
laws are known to globally stabilize the system, save for a set of measure zero
of initial conditions. The situation has proven to be more complex when the
graph contains cycles; in fact, with the exception of the cyclic formation with
three agents, which is stabilized with laws similar to the ones of the acyclic
case, very little is known about formations with cycles. Moreover, all of the
control laws used in the acyclic case fail at stabilizing more complex cyclic
formations. In this paper, we explain why this is the case and show that a
large class of planar formations with cycles cannot be globally stabilized,
even up to sets of measure zero of initial conditions. The approach rests on
relating the information flow to singularities in the dynamics of formations.
These singularities are in turn shown to make the existence of stable
configurations that do not satisfy the prescribed edge lengths generic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0262</identifier>
 <datestamp>2011-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0262</id><created>2011-12-01</created><authors><author><keyname>Flomenbom</keyname><forenames>Ophir</forenames></author></authors><title>Fairness in society</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages + 29 pages supplementary information. Submitted, November
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models that explain the economical and political realities of nowadays
societies should help all the world's citizens. Yet, the last four years showed
that the current models are missing. Here we develop a dynamical
society-deciders model showing that the long lasting economical stress can be
solved when increasing fairness in nations. fairness is computed for each
nation using indicators from economy and politics. Rather than austerity versus
spending, the dynamical model suggests that solving crises in western societies
is possible with regulations that reduce the stability of the deciders, while
shifting wealth in the direction of the people. This shall increase the
dynamics among socio-economic classes, further increasing fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0278</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0278</id><created>2011-12-01</created><updated>2012-01-01</updated><authors><author><keyname>Bu</keyname><forenames>Tian-Ming</forenames></author><author><keyname>Yuan</keyname><forenames>Chen</forenames></author><author><keyname>Zhang</keyname><forenames>Peng</forenames></author></authors><title>Computing on Binary Strings</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in Computer Science can be abstracted to the following
question: given a set of objects and rules respectively, which new objects can
be produced? In the paper, we consider a succinct version of the question:
given a set of binary strings and several operations like conjunction and
disjunction, which new binary strings can be generated? Although it is a
fundamental problem, to the best of our knowledge, the problem hasn't been
studied yet. In this paper, an O(m^2n) algorithm is presented to determine
whether a string s is representable by a set W, where n is the number of
strings in W and each string has the same length m. However, looking for the
minimum subset from a set to represent a given string is shown to be NP-hard.
Also, finding the smallest subset from a set to represent each string in the
original set is NP-hard. We establishes inapproximability results and
approximation algorithms for them. In addition, we prove that counting the
number of strings representable is #P-complete. We then explore how the
problems change when the operator negation is available. For example, if the
operator negation can be used, the number is some power of 2. This difference
maybe help us understand the problem more profoundly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0296</identifier>
 <datestamp>2012-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0296</id><created>2011-12-01</created><updated>2012-01-23</updated><authors><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>AWGN Channel under Time-Varying Amplitude Constraints with Causal
  Information at the Transmitter</title><categories>cs.IT cs.NI math.IT</categories><comments>Published in Asilomar Conference on Signals, Systems and Computers,
  November 2011; Fig. 4 revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical AWGN channel where the channel input is constrained
to an amplitude constraint that stochastically varies at each channel use,
independent of the message. This is an abstraction of an energy harvesting
transmitter where the code symbol energy at each channel use is determined by
an exogenous energy arrival process and there is no battery for energy storage.
At each channel use, an independent realization of the amplitude constraint
process is observed by the transmitter causally. This scenario is a
state-dependent channel with perfect causal state information at the
transmitter. We derive the capacity of this channel using Shannon's coding
scheme with causal state information. We prove that the code symbols must be
selected from a finite set in the capacity achieving scheme, as in the case of
Smith. We numerically study the binary on-off energy arrivals where the
amplitude constraint is either zero or a non-zero constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0311</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0311</id><created>2011-11-30</created><updated>2012-11-30</updated><authors><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Narayan</keyname><forenames>Manjari</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Anisotropic Nonlocal Means Denoising</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>Accepted for publication in Applied and Computational Harmonic
  Analysis (ACHA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has recently been proved that the popular nonlocal means (NLM) denoising
algorithm does not optimally denoise images with sharp edges. Its weakness lies
in the isotropic nature of the neighborhoods it uses to set its smoothing
weights. In response, in this paper we introduce several theoretical and
practical anisotropic nonlocal means (ANLM) algorithms and prove that they are
near minimax optimal for edge-dominated images from the Horizon class. On
real-world test images, an ANLM algorithm that adapts to the underlying image
gradients outperforms NLM by a significant margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0329</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0329</id><created>2011-12-01</created><authors><author><keyname>Bocchino</keyname><forenames>F.</forenames></author><author><keyname>Lopez-Santiago</keyname><forenames>J.</forenames></author><author><keyname>Albacete-Colombo</keyname><forenames>F.</forenames></author><author><keyname>Bucciantini</keyname><forenames>N.</forenames></author></authors><title>YouASTRO: a web-based bibliography management system with distributed
  comments and rating features for SAO/NASA ADS papers</title><categories>astro-ph.IM astro-ph.GA cs.DL</categories><comments>4 pages, 1 figure, proceedings of the ADASS conference, Paris,
  November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a working prototype of YouASTRO (www.youastro.org), a web-based
BibTeX-compliant reference management software (RMS) for astrophysical papers
in the SAO/NASA ADS database. It also includes as a main feature the concept of
distributed paper comments and ratings. In these paper, we introduce the main
characteristics of the web application, and we will briefly discuss what could
be the advantages and drawbacks of such a system being widespread adopted by
the astrophysical community for its scientific literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0343</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0343</id><created>2011-12-01</created><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Orsi</keyname><forenames>Giorgio</forenames></author><author><keyname>Pieris</keyname><forenames>Andreas</forenames></author></authors><title>Ontological Queries: Rewriting and Optimization (Extended Version)</title><categories>cs.DB cs.LO</categories><comments>Extended version of &quot;Ontological Queries: Rewriting and Optimization&quot;
  presented at ICDE 2011</comments><msc-class>68P15</msc-class><acm-class>H.2.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Ontological queries are evaluated against an ontology rather than directly on
a database. The evaluation and optimization of such queries is an intriguing
new problem for database research.
  In this paper we discuss two important aspects of this problem: query
rewriting and query optimization. Query rewriting consists of the compilation
of an ontological query into an equivalent query against the underlying
relational database. The focus here is on soundness and completeness. We review
previous results and present a new rewriting algorithm for rather general types
of ontological constraints.
  In particular, we show how a conjunctive query against an ontology can be
compiled into a union of conjunctive queries against the underlying database.
Ontological query optimization, in this context, attempts to improve this
process so to produce possibly small and cost-effective UCQ rewritings for an
input query. We review existing optimization methods, and propose an effective
new method that works for linear Datalog+/-, a class of Datalog-based rules
that encompasses well-known description logics of the DL-Lite family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0347</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0347</id><created>2011-12-01</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Domain Theory and the Logic of Observable Properties</title><categories>cs.LO</categories><comments>235 pages. Ph.D thesis, 1988, Queen Mary College, University of
  London</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mathematical framework of Stone duality is used to synthesize a number of
hitherto separate developments in Theoretical Computer Science: - Domain
Theory, the mathematical theory of computation introduced by Scott as a
foundation for denotational semantics. - The theory of concurrency and systems
behaviour developed by Milner, Hennessy et al. based on operational semantics.
- Logics of programs.
  Stone duality provides a junction between semantics (spaces of points =
denotations of computational processes) and logics (lattices of properties of
processes). Moreover, the underlying logic is geometric, which can be
computationally interpreted as the logic of observable properties---i.e.
properties which can be determined to hold of a process on the basis of a
finite amount of information about its execution.
  These ideas lead to the following programme:
  1. A metalanguage is introduced, comprising
  - types = universes of discourse for various computational situations.
  - terms = programs = syntactic intensions for models or points.
  2. A standard denotational interpretation of the metalanguage is given,
assigning domains to types and domain elements to terms.
  3. The metalanguage is also given a {\em logical} interpretation, in which
types are interpreted as propositional theories and terms are interpreted via a
program logic, which axiomatizes the properties they satisfy.
  4. The two interpretations are related by showing that they are Stone duals
of each other. Hence, semantics and logic are guaranteed to be in harmony with
each other, and in fact each determines the other up to isomorphism.
  This opens the way to a whole range of applications. Given a denotational
description of a computational situation in our meta-language, we can turn the
handle to obtain a logic for that situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0348</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0348</id><created>2011-12-01</created><authors><author><keyname>Halabian</keyname><forenames>Hassan</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author><author><keyname>Lung</keyname><forenames>Chung-Horng</forenames></author></authors><title>Explicit Characterization of Stability Region for Stationary Multi-Queue
  Multi-Server Systems</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>35 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the network stability region (capacity region)
of multi-queue multi-server (MQMS) queueing systems with stationary channel
distribution and stationary arrival processes. The stability region is
specified by a finite set of linear inequalities. We first show that the
stability region is a polytope characterized by the finite set of its facet
defining hyperplanes. We explicitly determine the coefficients of the linear
inequalities describing the facet defining hyperplanes of the stability region
polytope. We further derive the necessary and sufficient conditions for the
stability of the system for general arrival processes with finite first and
second moments. For the case of stationary arrival processes, the derived
conditions characterize the system stability region. Furthermore, we obtain an
upper bound for the average queueing delay of Maximum Weight (MW) server
allocation policy which has been shown in the literature to be a throughput
optimal policy for MQMS systems. Using a similar approach, we can characterize
the stability region for a fluid model MQMS system. However, the stability
region of the fluid model system is described by an infinite number of linear
inequalities since in this case the stability region is a convex surface. We
present an example where we show that in some cases depending on the channel
distribution, the stability region can be characterized by a finite set of
non-linear inequalities instead of an infinite number of linear inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0371</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0371</id><created>2011-12-01</created><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Wang</keyname><forenames>Zhiying</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Zigzag Codes: MDS Array Codes with Optimal Rebuilding</title><categories>cs.IT math.IT</categories><comments>23 pages, 5 figures, submitted to IEEE transactions on information
  theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MDS array codes are widely used in storage systems to protect data against
erasures. We address the \emph{rebuilding ratio} problem, namely, in the case
of erasures, what is the fraction of the remaining information that needs to be
accessed in order to rebuild \emph{exactly} the lost information? It is clear
that when the number of erasures equals the maximum number of erasures that an
MDS code can correct then the rebuilding ratio is 1 (access all the remaining
information). However, the interesting and more practical case is when the
number of erasures is smaller than the erasure correcting capability of the
code. For example, consider an MDS code that can correct two erasures: What is
the smallest amount of information that one needs to access in order to correct
a single erasure? Previous work showed that the rebuilding ratio is bounded
between 1/2 and 3/4, however, the exact value was left as an open problem. In
this paper, we solve this open problem and prove that for the case of a single
erasure with a 2-erasure correcting code, the rebuilding ratio is 1/2. In
general, we construct a new family of $r$-erasure correcting MDS array codes
that has optimal rebuilding ratio of $\frac{e}{r}$ in the case of $e$ erasures,
$1 \le e \le r$. Our array codes have efficient encoding and decoding
algorithms (for the case $r=2$ they use a finite field of size 3) and an
optimal update property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0383</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0383</id><created>2011-12-01</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author><author><keyname>Feng</keyname><forenames>Rongquan</forenames></author><author><keyname>Zhang</keyname><forenames>Aixian</forenames></author></authors><title>Bounds on and Constructions of Unit Time-Phase Signal Sets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital signals are complex-valued functions on $\Z_n$. Signal sets with
certain properties are required in various communication systems. Traditional
signal sets consider only the time distortion during transmission. Recently,
signal sets against both the time and phase distortion have been studied, and
are called {\em time-phase} signal sets. Several constructions of time-phase
signal sets are available in the literature. There are a number of bounds on
time signal sets (also called codebooks). They are automatically bounds on
time-phase signal sets, but are bad bounds. The first objective of this paper
is to develop better bounds on time-phase signal sets from known bounds on time
signal sets. The second objective of this paper is to construct two series of
time-phase signal sets, one of which is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0384</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0384</id><created>2011-12-01</created><authors><author><keyname>Dutta</keyname><forenames>Chinmoy</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Rajaraman</keyname><forenames>Rajmohan</forenames></author><author><keyname>Sun</keyname><forenames>Zhifeng</forenames></author></authors><title>Information Spreading in Dynamic Networks</title><categories>cs.DC</categories><comments>18 pages</comments><msc-class>68Q85, 68Q25, 68M12, 68M14</msc-class><acm-class>G.2.2; G.2.3; C.2.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of information spreading (also known as
gossip) in dynamic networks. In gossip, or more generally, $k$-gossip, there
are $k$ pieces of information (or tokens) that are initially present in some
nodes and the problem is to disseminate the $k$ tokens to all nodes. The goal
is to accomplish the task in as few rounds of distributed computation as
possible. The problem is especially challenging in dynamic networks where the
network topology can change from round to round and can be controlled by an
on-line adversary.
  The focus of this paper is on the power of token-forwarding algorithms, which
do not manipulate tokens in any way other than storing and forwarding them. We
first consider a worst-case adversarial model first studied by Kuhn, Lynch, and
Oshman~\cite{kuhn+lo:dynamic} in which the communication links for each round
are chosen by an adversary, and nodes do not know who their neighbors for the
current round are before they broadcast their messages. Our main result is an
$\Omega(nk/\log n)$ lower bound on the number of rounds needed for any
deterministic token-forwarding algorithm to solve $k$-gossip. This resolves an
open problem raised in~\cite{kuhn+lo:dynamic}, improving their lower bound of
$\Omega(n \log k)$, and matching their upper bound of $O(nk)$ to within a
logarithmic factor.
  We next show that token-forwarding algorithms can achieve subquadratic time
in the offline version of the problem where the adversary has to commit all the
topology changes in advance at the beginning of the computation, and present
two polynomial-time offline token-forwarding algorithms. Our results are a step
towards understanding the power and limitation of token-forwarding algorithms
in dynamic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0391</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0391</id><created>2011-12-02</created><updated>2011-12-06</updated><authors><author><keyname>Nguyen</keyname><forenames>Nam H.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Robust Lasso with missing and grossly corrupted observations</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>19 pages, 3 figures. Partial of this work is presented at NIPS 2011
  conference in Granda, Spain, December 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of accurately recovering a sparse vector
$\beta^{\star}$ from highly corrupted linear measurements $y = X \beta^{\star}
+ e^{\star} + w$ where $e^{\star}$ is a sparse error vector whose nonzero
entries may be unbounded and $w$ is a bounded noise. We propose a so-called
extended Lasso optimization which takes into consideration sparse prior
information of both $\beta^{\star}$ and $e^{\star}$. Our first result shows
that the extended Lasso can faithfully recover both the regression as well as
the corruption vector. Our analysis relies on the notion of extended restricted
eigenvalue for the design matrix $X$. Our second set of results applies to a
general class of Gaussian design matrix $X$ with i.i.d rows $\oper N(0,
\Sigma)$, for which we can establish a surprising result: the extended Lasso
can recover exact signed supports of both $\beta^{\star}$ and $e^{\star}$ from
only $\Omega(k \log p \log n)$ observations, even when the fraction of
corruption is arbitrarily close to one. Our analysis also shows that this
amount of observations required to achieve exact signed support is indeed
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0393</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0393</id><created>2011-12-02</created><authors><author><keyname>Bahanfar</keyname><forenames>Saeid</forenames></author><author><keyname>Darougaran</keyname><forenames>Ladan</forenames></author><author><keyname>Kousha</keyname><forenames>Helia</forenames></author><author><keyname>Babaie</keyname><forenames>Shahram</forenames></author></authors><title>Reliable Communication in Wireless Body Area Sensor Network for Health
  Monitoring</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now days, interests in the application of Wireless Body Area Network (WBAN)
have grown considerably. A number of tiny wireless sensors, strategically
placed on the human body, create a wireless body area network that can monitor
various vital signs, providing real-time feedback to the user and medical
personnel. This communication needs to be energy efficient and highly reliable
while keeping delays low. In this paper we present hardware and software
architecture for BAN and also we offer reliable communication and data
aggregation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0396</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0396</id><created>2011-12-02</created><authors><author><keyname>Thant</keyname><forenames>Win Win</forenames></author><author><keyname>Htwe</keyname><forenames>Tin Myat</forenames></author><author><keyname>Thein</keyname><forenames>Ni Lar</forenames></author></authors><title>Grammatical Relations of Myanmar Sentences Augmented by
  Transformation-Based Learning of Function Tagging</title><categories>cs.CL</categories><comments>10 pages, 15 figures, 11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe function tagging using Transformation Based
Learning (TBL) for Myanmar that is a method of extensions to the previous
statistics-based function tagger. Contextual and lexical rules (developed using
TBL) were critical in achieving good results. First, we describe a method for
expressing lexical relations in function tagging that statistical function
tagging are currently unable to express. Function tagging is the preprocessing
step to show grammatical relations of the sentences. Then we use the context
free grammar technique to clarify the grammatical relations in Myanmar
sentences or to output the parse trees. The grammatical relations are the
functional structure of a language. They rely very much on the function tag of
the tokens. We augment the grammatical relations of Myanmar sentences with
transformation-based learning of function tagging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0404</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0404</id><created>2011-12-02</created><updated>2012-02-06</updated><authors><author><keyname>Agaev</keyname><forenames>Rafig</forenames></author><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author></authors><title>A Cyclic Representation of Discrete Coordination Procedures</title><categories>cs.MA cs.DM cs.SY math.OC</categories><comments>6 pages, 1 figure</comments><msc-class>93A14, 15B51, 05C50, 05C05</msc-class><journal-ref>Automation and Remote Control, 2012, vol.73, No.1, p.161-166</journal-ref><doi>10.1134/S0005117912010134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any discrete opinion pooling procedure with positive weights can
be asymptotically approximated by DeGroot's procedure whose communication
digraph is a Hamiltonian cycle with loops. In this cycle, the weight of each
arc (which is not a loop) is inversely proportional to the influence of the
agent the arc leads to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0416</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0416</id><created>2011-12-02</created><updated>2012-06-20</updated><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author></authors><title>Publish-Subscribe Systems via Gossip: a Study based on Complex Networks</title><categories>cs.DC cs.NI</categories><comments>To appear in: Proc. of the 4th International Workshop on Simplifying
  Complex Networks for Pratictioners (SIMPLEX 2012) - World Wide Web Conference
  (WWW 2012), ACM, Lyon (France), April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the adoption of unstructured P2P overlay networks to
build publish-subscribe systems. We consider a very simple distributed
communication protocol, based on gossip and on the local knowledge each node
has about subscriptions made by its neighbours. In particular, upon reception
(or generation) of a novel event, a node sends it to those neighbours whose
subscriptions match that event. Moreover, the node gossips the event to its
&quot;non-interested&quot; neighbours, so that the event can be spread through the
overlay. A mathematical analysis is provided to estimate the number of nodes
receiving the event, based on the network topology, the amount of subscribers
and the gossip probability. These outcomes are compared to those obtained via
simulation. Results show even when the amount of subscribers represents a very
small (yet non-negligible) portion of network nodes, by tuning the gossip
probability the event can percolate through the overlay. Hence, the use of
unstructured networks. coupled with simple dissemination protocols, represents
a viable approach to build peer-to-peer publish-subscribe applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0427</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0427</id><created>2011-12-02</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>A Generalized Kahn Principle for Abstract Asynchronous Networks</title><categories>cs.LO math.CT quant-ph</categories><comments>25 pages. Published in the Proceedings of the Symposium on
  Mathematical Foundations of Programming Language Semantics, Springer Lecture
  Notes in Computer Science vol. 442, pp. 1--21</comments><doi>10.1007/BFb0040252</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our general motivation is to answer the question: &quot;What is a model of
concurrent computation?&quot;. As a preliminary exercise, we study dataflow
networks. We develop a very general notion of model for asynchronous networks.
The &quot;Kahn Principle&quot;, which states that a network built from functional nodes
is the least fixpoint of a system of equations associated with the network, has
become a benchmark for the formal study of dataflow networks. We formulate a
generalized version of the Kahn Principle, which applies to a large class of
non-deterministic systems, in the setting of abstract asynchronous networks;
and prove that the Kahn Principle holds under certain natural assumptions on
the model. We also show that a class of models, which represent networks that
compute over arbitrary event structures, generalizing dataflow networks which
compute over streams, satisfy these assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0442</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0442</id><created>2011-12-02</created><updated>2012-02-27</updated><authors><author><keyname>Lu</keyname><forenames>Tan</forenames><affiliation>Department of Information Engineering, The Chinese University of Hong Kong</affiliation></author><author><keyname>Chen</keyname><forenames>Minghua</forenames><affiliation>Department of Information Engineering, The Chinese University of Hong Kong</affiliation></author></authors><title>Simple and Effective Dynamic Provisioning for Power-Proportional Data
  Centers</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy consumption represents a significant cost in data center operation. A
large fraction of the energy, however, is used to power idle servers when the
workload is low. Dynamic provisioning techniques aim at saving this portion of
the energy, by turning off unnecessary servers. In this paper, we explore how
much performance gain can knowing future workload information brings to dynamic
provisioning. In particular, we study the dynamic provisioning problem under
the cost model that a running server consumes a fixed amount energy per unit
time, and develop online solutions with and without future workload information
available. We first reveal an elegant structure of the off-line dynamic
provisioning problem, which allows us to characterize and achieve the optimal
solution in a {}&quot;divide-and-conquer&quot; manner. We then exploit this insight to
design three online algorithms with competitive ratios $2-\alpha$,
$(e-\alpha)/(e-1)\approx1.58-\alpha/(e-1)$ and $e/(e-1+\alpha)$, respectively,
where $0\leq\alpha\leq1$ is the fraction of a critical window in which future
workload information is available. A fundamental observation is that
\emph{future workload information beyond the critical window will not}
\emph{improve dynamic provisioning performance}. Our algorithms are
decentralized and are simple to implement. We demonstrate their effectiveness
in simulations using real-world traces. We also compare their performance with
state-of-the-art solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0463</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0463</id><created>2011-12-02</created><authors><author><keyname>Dogandzic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Gu</keyname><forenames>Renliang</forenames></author><author><keyname>Qiu</keyname><forenames>Kun</forenames></author></authors><title>Mask Iterative Hard Thresholding Algorithms for Sparse Image
  Reconstruction of Objects with Known Contour</title><categories>stat.ML cs.IT math.IT</categories><comments>6 pages, 19 figures, 2011 45th Asilomar Conf. Signals, Syst. Comput.,
  Pacific Grove, CA, Nov. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop mask iterative hard thresholding algorithms (mask IHT and mask
DORE) for sparse image reconstruction of objects with known contour. The
measurements follow a noisy underdetermined linear model common in the
compressive sampling literature. Assuming that the contour of the object that
we wish to reconstruct is known and that the signal outside the contour is
zero, we formulate a constrained residual squared error minimization problem
that incorporates both the geometric information (i.e. the knowledge of the
object's contour) and the signal sparsity constraint. We first introduce a mask
IHT method that aims at solving this minimization problem and guarantees
monotonically non-increasing residual squared error for a given signal sparsity
level. We then propose a double overrelaxation scheme for accelerating the
convergence of the mask IHT algorithm. We also apply convex mask reconstruction
approaches that employ a convex relaxation of the signal sparsity constraint.
In X-ray computed tomography (CT), we propose an automatic scheme for
extracting the convex hull of the inspected object from the measured sinograms;
the obtained convex hull is used to capture the object contour information. We
compare the proposed mask reconstruction schemes with the existing large-scale
sparse signal reconstruction methods via numerical simulations and demonstrate
that, by exploiting both the geometric contour information of the underlying
image and sparsity of its wavelet coefficients, we can reconstruct this image
using a significantly smaller number of measurements than the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0467</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0467</id><created>2011-12-02</created><updated>2012-06-28</updated><authors><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Kirkelund</keyname><forenames>Gunvor Elisabeth</forenames></author><author><keyname>Manch&#xf3;n</keyname><forenames>Carles Navarro</forenames></author><author><keyname>Badiu</keyname><forenames>Mihai-Alin</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard Henry</forenames></author></authors><title>Merging Belief Propagation and the Mean Field Approximation: A Free
  Energy Approach</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a joint message passing approach that combines belief propagation
and the mean field approximation. Our analysis is based on the region-based
free energy approximation method proposed by Yedidia et al. We show that the
message passing fixed-point equations obtained with this combination correspond
to stationary points of a constrained region-based free energy approximation.
Moreover, we present a convergent implementation of these message passing
fixedpoint equations provided that the underlying factor graph fulfills certain
technical conditions. In addition, we show how to include hard constraints in
the part of the factor graph corresponding to belief propagation. Finally, we
demonstrate an application of our method to iterative channel estimation and
decoding in an orthogonal frequency division multiplexing (OFDM) system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0508</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0508</id><created>2011-12-02</created><authors><author><keyname>Cheng</keyname><forenames>Weiwei</forenames></author><author><keyname>H&#xfc;llermeier</keyname><forenames>Eyke</forenames></author></authors><title>Label Ranking with Abstention: Predicting Partial Orders by Thresholding
  Probability Distributions (Extended Abstract)</title><categories>cs.AI</categories><comments>4 pages, 1 figure, appeared at NIPS 2011 Choice Models and Preference
  Learning workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an extension of the setting of label ranking, in which the
learner is allowed to make predictions in the form of partial instead of total
orders. Predictions of that kind are interpreted as a partial abstention: If
the learner is not sufficiently certain regarding the relative order of two
alternatives, it may abstain from this decision and instead declare these
alternatives as being incomparable. We propose a new method for learning to
predict partial orders that improves on an existing approach, both
theoretically and empirically. Our method is based on the idea of thresholding
the probabilities of pairwise preferences between labels as induced by a
predicted (parameterized) probability distribution on the set of all rankings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0520</identifier>
 <datestamp>2012-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0520</id><created>2011-12-02</created><updated>2012-01-21</updated><authors><author><keyname>Fu</keyname><forenames>Bin</forenames></author></authors><title>On the Complexity of Approximate Sum of Sorted List</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexity for computing the approximate sum
$a_1+a_2+...+a_n$ of a sorted list of numbers $a_1\le a_2\le ...\le a_n$. We
show an algorithm that computes an $(1+\epsilon)$-approximation for the sum of
a sorted list of nonnegative numbers in an $O({1\over \epsilon}\min(\log n,
{\log ({x_{max}\over x_{min}})})\cdot (\log {1\over \epsilon}+\log\log n))$
time, where $x_{max}$ and $x_{min}$ are the largest and the least positive
elements of the input list, respectively. We prove a lower bound
$\Omega(\min(\log n,\log ({x_{max}\over x_{min}}))$ time for every
O(1)-approximation algorithm for the sum of a sorted list of nonnegative
elements. We also show that there is no sublinear time approximation algorithm
for the sum of a sorted list that contains at least one negative number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0534</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0534</id><created>2011-12-02</created><authors><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Queyranne</keyname><forenames>Maurice</forenames></author><author><keyname>Spieksma</keyname><forenames>Frits C. R.</forenames></author><author><keyname>Nobibon</keyname><forenames>Fabrice Talla</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>The interval ordering problem</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given set of intervals on the real line, we consider the problem of
ordering the intervals with the goal of minimizing an objective function that
depends on the exposed interval pieces (that is, the pieces that are not
covered by earlier intervals in the ordering). This problem is motivated by an
application in molecular biology that concerns the determination of the
structure of the backbone of a protein.
  We present polynomial-time algorithms for several natural special cases of
the problem that cover the situation where the interval boundaries are
agreeably ordered and the situation where the interval set is laminar. Also the
bottleneck variant of the problem is shown to be solvable in polynomial time.
Finally we prove that the general problem is NP-hard, and that the existence of
a constant-factor-approximation algorithm is unlikely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0539</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0539</id><created>2011-12-02</created><authors><author><keyname>Li</keyname><forenames>Qiao</forenames></author><author><keyname>Negi</keyname><forenames>Rohit</forenames></author></authors><title>Maximal Scheduling in Wireless Networks with Priorities</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a general class of low complexity distributed scheduling
algorithms in wireless networks, maximal scheduling with priorities, where a
maximal set of transmitting links in each time slot are selected according to
certain pre-specified static priorities. The proposed scheduling scheme is
simple, which is easily amendable for distributed implementation in practice,
such as using inter-frame space (IFS) parameters under the ubiquitous 802.11
protocols. To obtain throughput guarantees, we first analyze the case of
maximal scheduling with a fixed priority vector, and formulate a lower bound on
its stability region and scheduling efficiency. We further propose a low
complexity priority assignment algorithm, which can stabilize any arrival rate
that is in the union of the lower bound regions of all priorities. The
stability result is proved using fluid limits, and can be applied to very
general stochastic arrival processes. Finally, the performance of the proposed
prioritized maximal scheduling scheme is verified by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0544</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0544</id><created>2011-12-02</created><authors><author><keyname>Jeronimo</keyname><forenames>Gabriela</forenames></author><author><keyname>Perrucci</keyname><forenames>Daniel</forenames></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames></author></authors><title>On the minimum of a polynomial function on a basic closed semialgebraic
  set and applications</title><categories>math.AG cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an explicit upper bound for the algebraic degree and an explicit
lower bound for the absolute value of the minimum of a polynomial function on a
compact connected component of a basic closed semialgebraic set when this
minimum is not zero. As an application, we obtain a lower bound for the
separation of two disjoint connected components of basic closed semialgebraic
sets, when at least one of them is compact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0548</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0548</id><created>2011-12-02</created><authors><author><keyname>Childs</keyname><forenames>Andrew M.</forenames></author><author><keyname>Kimmel</keyname><forenames>Shelby</forenames></author><author><keyname>Kothari</keyname><forenames>Robin</forenames></author></authors><title>The quantum query complexity of read-many formulas</title><categories>quant-ph cs.CC</categories><comments>15 pages</comments><report-no>MIT-CTP 4330</report-no><journal-ref>Lecture Notes in Computer Science 7501, pp. 337-348 (2012)</journal-ref><doi>10.1007/978-3-642-33090-2_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum query complexity of evaluating any read-once formula with n
black-box input bits is Theta(sqrt(n)). However, the corresponding problem for
read-many formulas (i.e., formulas in which the inputs have fanout) is not well
understood. Although the optimal read-once formula evaluation algorithm can be
applied to any formula, it can be suboptimal if the inputs have large fanout.
We give an algorithm for evaluating any formula with n inputs, size S, and G
gates using O(min{n, sqrt{S}, n^{1/2} G^{1/4}}) quantum queries. Furthermore,
we show that this algorithm is optimal, since for any n,S,G there exists a
formula with n inputs, size at most S, and at most G gates that requires
Omega(min{n, sqrt{S}, n^{1/2} G^{1/4}}) queries. We also show that the
algorithm remains nearly optimal for circuits of any particular depth k &gt;= 3,
and we give a linear-size circuit of depth 2 that requires Omega (n^{5/9})
queries. Applications of these results include a Omega (n^{19/18}) lower bound
for Boolean matrix product verification, a nearly tight characterization of the
quantum query complexity of evaluating constant-depth circuits with bounded
fanout, new formula gate count lower bounds for several functions including
PARITY, and a construction of an AC^0 circuit of linear size that can only be
evaluated by a formula with Omega(n^{2-epsilon}) gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0564</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0564</id><created>2011-12-02</created><updated>2012-12-26</updated><authors><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Sur-Kolay</keyname><forenames>Susmita</forenames></author><author><keyname>Chaudhury</keyname><forenames>Ayan</forenames></author></authors><title>Linear Nearest Neighbor Synthesis of Reversible Circuits by Graph
  Partitioning</title><categories>cs.ET quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Nearest Neighbor (LNN) synthesis in reversible circuits has emerged as
an important issue in terms of technological implementation for quantum
computation. The objective is to obtain a LNN architecture with minimum gate
cost. As achieving optimal synthesis is a hard problem, heuristic methods have
been proposed in recent literature. In this work we present a graph
partitioning based approach for LNN synthesis with reduction in circuit cost.
In particular, the number of SWAP gates required to convert a given gate-level
quantum circuit to its equivalent LNN configuration is minimized. Our algorithm
determines the reordering of indices of the qubit line(s) for both single
control and multiple controlled gates. Experimental results for placing the
target qubits of Multiple Controlled Toffoli (MCT) library of benchmark
circuits show a significant reduction in gate count and quantum gate cost
compared to those of related research works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0596</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0596</id><created>2011-12-02</created><authors><author><keyname>Akshay</keyname><forenames>Antony</forenames></author></authors><title>Analysis of Kak's Quantum Cryptography Protocol from the Perspective of
  Source Strength</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the performance of Kak's quantum cryptography protocol
when intensity monitoring is used to detect the presence of Eve during
transmission. Some difficulties related to interception to obtain useful data
from the transmission are discussed. The analysis shows the resilience of the
protocol towards the man-in-the-middle attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0617</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0617</id><created>2011-12-02</created><updated>2012-07-15</updated><authors><author><keyname>Cabello</keyname><forenames>Adan</forenames></author><author><keyname>Danielsen</keyname><forenames>Lars Eirik</forenames></author><author><keyname>Lopez-Tarrida</keyname><forenames>Antonio J.</forenames></author><author><keyname>Portillo</keyname><forenames>Jose R.</forenames></author></authors><title>Quantum social networks</title><categories>physics.soc-ph cs.SI quant-ph</categories><comments>REVTeX4, 6 pages, 3 figures</comments><journal-ref>J. Phys. A: Math. Theor. 45 (2012) 285101</journal-ref><doi>10.1088/1751-8113/45/28/285101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a physical approach to social networks (SNs) in which each actor
is characterized by a yes-no test on a physical system. This allows us to
consider SNs beyond those originated by interactions based on pre-existing
properties, as in a classical SN (CSN). As an example of SNs beyond CSNs, we
introduce quantum SNs (QSNs) in which actor is characterized by a test of
whether or not the system is in a quantum state. We show that QSNs outperform
CSNs for a certain task and some graphs. We identify the simplest of these
graphs and show that graphs in which QSNs outperform CSNs are increasingly
frequent as the number of vertices increases. We also discuss more general SNs
and identify the simplest graphs in which QSNs cannot be outperformed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0631</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0631</id><created>2011-12-03</created><authors><author><keyname>Arif</keyname><forenames>Bijoy Rahman</forenames></author></authors><title>On the Footsteps to Generalized Tower of Hanoi Strategy</title><categories>cs.DM</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, our aim is to prove that our recursive algorithm to solve the
&quot;Reve's puzzle&quot; (four- peg Tower of Hanoi) is the optimal solution according to
minimum number of moves. Here we used Frame's five step algorithm to solve the
&quot;Reve's puzzle&quot;, and proved its optimality analyzing all possible strategies to
solve the problem. Minimum number of moves is important because no one ever
proved that the &quot;presumed optimal&quot; solution, the Frame-Stewart algorithm,
always gives the minimum number of moves. The basis of our proof is Bifurcation
Theorem. In fact, we can solve generalized &quot;Tower of Hanoi&quot; puzzle for any pegs
(three or more pegs) using Bifurcation Theorem. But our scope is limited to the
&quot;Reve's puzzle&quot; in this literature, but lately, we would discuss how we can
reach our final destination, the Generalized Tower of Hanoi Strategy. Another
important point is that we have used only induction method to prove all the
results throughout this literature. Moreover, some simple theorems and lemmas
are derived through logical perspective or consequence of induction method.
Lastly, we will try to answer about uniqueness of solution of this famous
puzzle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0643</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0643</id><created>2011-12-03</created><authors><author><keyname>Grygiel</keyname><forenames>Katarzyna</forenames></author><author><keyname>Idziak</keyname><forenames>Pawel M.</forenames></author><author><keyname>Zaionc</keyname><forenames>Marek</forenames></author></authors><title>How big is BCI fragment of BCK logic</title><categories>cs.LO</categories><msc-class>03B70, 03B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate quantitative properties of BCI and BCK logics. The first part
of the paper compares the number of formulas provable in BCI versus BCK logics.
We consider formulas built on implication and a fixed set of $k$ variables. We
investigate the proportion between the number of such formulas of a given
length $n$ provable in BCI logic against the number of formulas of length $n$
provable in richer BCK logic. We examine an asymptotic behavior of this
fraction when length $n$ of formulas tends to infinity. This limit gives a
probability measure that randomly chosen BCK formula is also provable in BCI.
We prove that this probability tends to zero as the number of variables tends
to infinity. The second part of the paper is devoted to the number of lambda
terms representing proofs of BCI and BCK logics. We build a proportion between
number of such proofs of the same length $n$ and we investigate asymptotic
behavior of this proportion when length of proofs tends to infinity. We
demonstrate that with probability 0 a randomly chosen BCK proof is also a proof
of a BCI formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0647</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0647</id><created>2011-12-03</created><updated>2013-08-16</updated><authors><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Thanatipanonda</keyname><forenames>Thotsaporn &quot;Aek&quot;</forenames></author></authors><title>Advanced Computer Algebra for Determinants</title><categories>cs.SC math.CO</categories><comments>16 pages</comments><msc-class>33F10 (Primary) 15A15, 05B45 (Secondary)</msc-class><journal-ref>Annals of Combinatorics 17(3), 509-523, 2013</journal-ref><doi>10.1007/s00026-013-0183-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove three conjectures concerning the evaluation of determinants, which
are related to the counting of plane partitions and rhombus tilings. One of
them was posed by George Andrews in 1980, the other two were by Guoce Xin and
Christian Krattenthaler. Our proofs employ computer algebra methods, namely,
the holonomic ansatz proposed by Doron Zeilberger and variations thereof. These
variations make Zeilberger's original approach even more powerful and allow for
addressing a wider variety of determinants. Finally, we present, as a challenge
problem, a conjecture about a closed-form evaluation of Andrews's determinant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0649</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0649</id><created>2011-12-03</created><authors><author><keyname>Zhang</keyname><forenames>Min</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Wen</keyname><forenames>Qiao-Yan</forenames></author><author><keyname>Jin</keyname><forenames>Zheng-Ping</forenames></author><author><keyname>Zhang</keyname><forenames>Hua</forenames></author></authors><title>Analysis and improvement of a strongly secure certificateless key
  exchange protocol without pairing</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recently, Yang and Tan proposed a certificateless key exchange protocol
without pairing, and claimed their scheme satisfies forward secrecy, which
means no adversary could derive an already-established session key unless the
full user secret keys (including a private key and an ephemeral secret key) of
both communication parties are compromised. However, in this paper, we point
out their protocol is actually not secure as claimed by presenting an attack
launched by an adversary who has learned the private key of one party and the
ephemeral secret key of the other, but not the full user secret keys of both
parties. Furthermore, to make up this flaw, we also provide an improved
protocol in which the private key and the ephemeral secret key are closely
intertwined with each other for generating the session key, thus above attack
can be efficiently resisted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0655</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0655</id><created>2011-12-03</created><authors><author><keyname>Gelencser</keyname><forenames>Andras</forenames></author><author><keyname>Prodromakis</keyname><forenames>Themistoklis</forenames></author><author><keyname>Toumazou</keyname><forenames>Christofer</forenames></author><author><keyname>Roska</keyname><forenames>Tamas</forenames></author></authors><title>A Biomimetic Model of the Outer Plexiform Layer by Incorporating
  Memristive Devices</title><categories>cs.CV</categories><comments>24 pages, 12 figures</comments><doi>10.1103/PhysRevE.85.041918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a biorealistic model for the first part of the early
vision processing by incorporating memristive nanodevices. The architecture of
the proposed network is based on the organisation and functioning of the outer
plexiform layer (OPL) in the vertebrate retina. We demonstrate that memristive
devices are indeed a valuable building block for neuromorphic architectures, as
their highly non-linear and adaptive response could be exploited for
establishing ultra-dense networks with similar dynamics to their biological
counterparts. We particularly show that hexagonal memristive grids can be
employed for faithfully emulating the smoothing-effect occurring at the OPL for
enhancing the dynamic range of the system. In addition, we employ a
memristor-based thresholding scheme for detecting the edges of grayscale
images, while the proposed system is also evaluated for its adaptation and
fault tolerance capacity against different light or noise conditions as well as
distinct device yields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0662</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0662</id><created>2011-12-03</created><authors><author><keyname>Tamayo</keyname><forenames>Alain</forenames></author><author><keyname>Granell</keyname><forenames>Carlos</forenames></author><author><keyname>Huerta</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Instance-based XML data binding for mobile devices</title><categories>cs.NI</categories><comments>8 pages, 1 table, 6 figures; Third International Workshop on
  Middleware for Pervasive Mobile and Embedded Computing (M-MPAC), 2011</comments><acm-class>I.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XML and XML Schema are widely used in different domains for the definition of
standards that enhance the interoperability between parts exchanging
information through the Internet. The size and complexity of some standards,
and their associated schemas, have been growing with time as new use case
scenarios and data models are added to them. The common approach to deal with
the complexity of producing XML processing code based on these schemas is the
use of XML data binding generators. Unfortunately, these tools do not always
produce code that ?ts the limitations of resource-constrained devices, such as
mobile phones, in the presence of large schemas. In this paper we present
Instance-based XML data binding, an approach to produce compact
application-specific XML processing code for mobile devices. The approach
utilises information extracted from a set of XML documents about how the
application make use of the schemas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0665</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0665</id><created>2011-12-03</created><updated>2012-11-29</updated><authors><author><keyname>Slavakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Kopsinis</keyname><forenames>Yannis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author><author><keyname>McLaughlin</keyname><forenames>Stephen</forenames></author></authors><title>Generalized Thresholding and Online Sparsity-Aware Learning in a Union
  of Subspaces</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a sparse signal recovery task in time-varying
(time-adaptive) environments. The contribution of the paper to sparsity-aware
online learning is threefold; first, a Generalized Thresholding (GT) operator,
which relates to both convex and non-convex penalty functions, is introduced.
This operator embodies, in a unified way, the majority of well-known
thresholding rules which promote sparsity. Second, a non-convexly constrained,
sparsity-promoting, online learning scheme, namely the Adaptive
Projection-based Generalized Thresholding (APGT), is developed that
incorporates the GT operator with a computational complexity that scales
linearly to the number of unknowns. Third, the novel family of partially
quasi-nonexpansive mappings is introduced as a functional analytic tool for
treating the GT operator. By building upon the rich fixed point theory, the
previous class of mappings helps us, also, to establish a link between the GT
operator and a union of linear subspaces; a non-convex object which lies at the
heart of any sparsity promoting technique, batch or online. Based on such a
functional analytic framework, a convergence analysis of the APGT is provided.
Furthermore, extensive experiments suggest that the APGT exhibits competitive
performance when compared to computationally more demanding alternatives, such
as the sparsity-promoting Affine Projection Algorithm (APA)- and Recursive
Least Squares (RLS)-based techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0674</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0674</id><created>2011-12-03</created><authors><author><keyname>Novlan</keyname><forenames>Thomas D.</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Ghosh</keyname><forenames>Arunabha</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Analytical Evaluation of Fractional Frequency Reuse for Heterogeneous
  Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>23 pages, 6 figures, submitted: Transactions on Wireless
  Communications, July 20, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference management techniques are critical to the performance of
heterogeneous cellular networks, which will have dense and overlapping coverage
areas, and experience high levels of interference. Fractional frequency reuse
(FFR) is an attractive interference management technique due to its low
complexity and overhead, and significant coverage improvement for
low-percentile (cell-edge) users. Instead of relying on system simulations
based on deterministic access point locations, this paper instead proposes an
analytical model for evaluating Strict FFR and Soft Frequency Reuse (SFR)
deployments based on the spatial Poisson point process. Our results both
capture the non-uniformity of heterogeneous deployments and produce tractable
expressions which can be used for system design with Strict FFR and SFR. We
observe that the use of Strict FFR bands reserved for the users of each tier
with the lowest average SINR provides the highest gains in terms of coverage
and rate, while the use of SFR allows for more efficient use of shared spectrum
between the tiers, while still mitigating much of the interference.
Additionally, in the context of multi-tier networks with closed access in some
tiers, the proposed framework shows the impact of cross-tier interference on
closed access FFR, and informs the selection of key FFR parameters in open
access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0689</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0689</id><created>2011-12-03</created><authors><author><keyname>Badanidiyuru</keyname><forenames>Ashwinkumar</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Lee</keyname><forenames>Hooyeon</forenames></author></authors><title>Approximating Low-Dimensional Coverage Problems</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the maximum coverage problem, restricted to set
systems of bounded VC-dimension. Our main result is a fixed-parameter tractable
approximation scheme: an algorithm that outputs a $(1-\eps)$-approximation to
the maximum-cardinality union of $k$ sets, in running time $O(f(\eps,k,d)\cdot
poly(n))$ where $n$ is the problem size, $d$ is the VC-dimension of the set
system, and $f(\eps,k,d)$ is exponential in $(kd/\eps)^c$ for some constant
$c$. We complement this positive result by showing that the function
$f(\eps,k,d)$ in the running-time bound cannot be replaced by a function
depending only on $(\eps,d)$ or on $(k,d)$, under standard complexity
assumptions.
  We also present an improved upper bound on the approximation ratio of the
greedy algorithm in special cases of the problem, including when the sets have
bounded cardinality and when they are two-dimensional halfspaces. Complementing
these positive results, we show that when the sets are four-dimensional
halfspaces neither the greedy algorithm nor local search is capable of
improving the worst-case approximation ratio of $1-1/e$ that the greedy
algorithm achieves on arbitrary instances of maximum coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0695</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0695</id><created>2011-12-03</created><authors><author><keyname>Driemel</keyname><forenames>Anne</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>On the Expected Complexity of Voronoi Diagrams on Terrains</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the combinatorial complexity of geodesic Voronoi diagrams on
polyhedral terrains using a probabilistic analysis. Aronov etal [ABT08] prove
that, if one makes certain realistic input assumptions on the terrain, this
complexity is \Theta(n + m \sqrt n) in the worst case, where n denotes the
number of triangles that define the terrain and m denotes the number of Voronoi
sites. We prove that under a relaxed set of assumptions the Voronoi diagram has
expected complexity O(n+m), given that the sites have a uniform distribution on
the domain of the terrain(or the surface of the terrain). Furthermore, we
present a worst-case construction of a terrain which implies a lower bound of
Vmega(n m2/3) on the expected worst-case complexity if these assumptions on the
terrain are dropped. As an additional result, we can show that the expected
fatness of a cell in a random planar Voronoi diagram is bounded by a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0698</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0698</id><created>2011-12-03</created><updated>2013-06-18</updated><authors><author><keyname>Tulabandhula</keyname><forenames>Theja</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Machine Learning with Operational Costs</title><categories>stat.ML cs.AI math.OC</categories><comments>Current version: Final version appearing in JMLR 2013. v2: Many parts
  have been rewritten including the introduction, Minor correction of Theorem
  6. 38 pages. Previously: v1: 36 pages, 8 figures. Short version appears in
  Proceedings of the International Symposium on Artificial Intelligence and
  Mathematics, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a way to align statistical modeling with decision making.
We provide a method that propagates the uncertainty in predictive modeling to
the uncertainty in operational cost, where operational cost is the amount spent
by the practitioner in solving the problem. The method allows us to explore the
range of operational costs associated with the set of reasonable statistical
models, so as to provide a useful way for practitioners to understand
uncertainty. To do this, the operational cost is cast as a regularization term
in a learning algorithm's objective function, allowing either an optimistic or
pessimistic view of possible costs, depending on the regularization parameter.
From another perspective, if we have prior knowledge about the operational
cost, for instance that it should be low, this knowledge can help to restrict
the hypothesis space, and can help with generalization. We provide a
theoretical generalization bound for this scenario. We also show that learning
with operational costs is related to robust optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0699</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0699</id><created>2011-12-03</created><updated>2015-04-09</updated><authors><author><keyname>Bartal</keyname><forenames>Yair</forenames></author><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>The Traveling Salesman Problem: Low-Dimensionality Implies a Polynomial
  Time Approximation Scheme</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Traveling Salesman Problem (TSP) is among the most famous NP-hard
optimization problems. We design for this problem a randomized polynomial-time
algorithm that computes a (1+eps)-approximation to the optimal tour, for any
fixed eps&gt;0, in TSP instances that form an arbitrary metric space with bounded
intrinsic dimension.
  The celebrated results of Arora (A-98) and Mitchell (M-99) prove that the
above result holds in the special case of TSP in a fixed-dimensional Euclidean
space. Thus, our algorithm demonstrates that the algorithmic tractability of
metric TSP depends on the dimensionality of the space and not on its specific
geometry. This result resolves a problem that has been open since the
quasi-polynomial time algorithm of Talwar (T-04).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0708</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0708</id><created>2011-12-03</created><updated>2013-01-18</updated><authors><author><keyname>Donoho</keyname><forenames>David L.</forenames></author><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Information-Theoretically Optimal Compressed Sensing via Spatial
  Coupling and Approximate Message Passing</title><categories>cs.IT cond-mat.stat-mech math.IT math.ST stat.TH</categories><comments>60 pages, 7 figures, Sections 3,5 and Appendices A,B are added. The
  stability constant is quantified (cf Theorem 1.7)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the compressed sensing reconstruction problem for a broad class of
random, band-diagonal sensing matrices. This construction is inspired by the
idea of spatial coupling in coding theory. As demonstrated heuristically and
numerically by Krzakala et al. \cite{KrzakalaEtAl}, message passing algorithms
can effectively solve the reconstruction problem for spatially coupled
measurements with undersampling rates close to the fraction of non-zero
coordinates.
  We use an approximate message passing (AMP) algorithm and analyze it through
the state evolution method. We give a rigorous proof that this approach is
successful as soon as the undersampling rate $\delta$ exceeds the (upper)
R\'enyi information dimension of the signal, $\uRenyi(p_X)$. More precisely,
for a sequence of signals of diverging dimension $n$ whose empirical
distribution converges to $p_X$, reconstruction is with high probability
successful from $\uRenyi(p_X)\, n+o(n)$ measurements taken according to a band
diagonal matrix.
  For sparse signals, i.e., sequences of dimension $n$ and $k(n)$ non-zero
entries, this implies reconstruction from $k(n)+o(n)$ measurements. For
`discrete' signals, i.e., signals whose coordinates take a fixed finite set of
values, this implies reconstruction from $o(n)$ measurements. The result is
robust with respect to noise, does not apply uniquely to random signals, but
requires the knowledge of the empirical distribution of the signal $p_X$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0711</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0711</id><created>2011-12-03</created><authors><author><keyname>Karamad</keyname><forenames>Ehsan</forenames></author><author><keyname>Khoshnevis</keyname><forenames>Behrouz</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author></authors><title>Quantization and Bit Allocation for Channel State Feedback for
  Relay-Assisted Wireless Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 4 figures, Submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates quantization of channel state information (CSI) and
bit allocation across wireless links in a multi-source, single-relay
cooperative cellular network. Our goal is to minimize the loss in performance,
measured as the achievable sum rate, due to limited-rate quantization of CSI.
We develop both a channel quantization scheme and allocation of limited
feedback bits to the various wireless links. We assume that the quantized CSI
is reported to a central node responsible for optimal resource allocation. We
first derive tight lower and upper bounds on the difference in rates between
the perfect CSI and quantized CSI scenarios. These bounds are then used to
derive an effective quantizer for arbitrary channel distributions. Next, we use
these bounds to optimize the allocation of bits across the links subject to a
budget on total available quantization bits. In particular, we show that the
optimal bit allocation algorithm allocates more bits to those links in the
network that contribute the most to the sum-rate. Finally, the paper
investigates the choice of the central node; we show that this choice plays a
significant role in CSI bits required to achieve a target performance level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0713</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0713</id><created>2011-12-03</created><authors><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Iqbal</keyname><forenames>Azhar</forenames></author><author><keyname>Chen</keyname><forenames>Minyou</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Quantum Strategies Win in a Defector-Dominated Population</title><categories>cs.GT nlin.AO quant-ph</categories><comments>8 pages, 7figures</comments><journal-ref>Physica A, Volume 391, Issue 11, pp. 3316-3322, 2012</journal-ref><doi>10.1016/j.physa.2012.01.048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum strategies are introduced into evolutionary games. The agents using
quantum strategies are regarded as invaders whose fraction generally is 1% of a
population in contrast to the 50% defectors. In this paper, the evolution of
strategies on networks is investigated in a defector-dominated population, when
three networks (Regular Lattice, Newman-Watts small world network, scale-free
network) are constructed and three games (Prisoners' Dilemma, Snowdrift,
Stag-Hunt) are employed. As far as these three games are concerned, the results
show that quantum strategies can always invade the population successfully.
Comparing the three networks, we find that the regular lattice is most easily
invaded by agents that adopt quantum strategies. However, for a scale-free
network it can be invaded by agents adopting quantum strategies only if a hub
is occupied by an agent with a quantum strategy or if the fraction of agents
with quantum strategies in the population is significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0721</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0721</id><created>2011-12-03</created><authors><author><keyname>Liu</keyname><forenames>Tianxi</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Huo</keyname><forenames>Qiang</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Performance Analysis of Hybrid Relay Selection in Cooperative Wireless
  Systems</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications, 2012</comments><journal-ref>IEEE Transactions on Communications, Vol. 60, No. 3, pp. 779 -
  788, 2012</journal-ref><doi>10.1109/TCOMM.2012.011312.110015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hybrid relay selection (HRS) scheme, which adaptively chooses
amplify-and-forward (AF) and decode-and-forward (DF) protocols, is very
effective to achieve robust performance in wireless networks. This paper
analyzes the frame error rate (FER) of the HRS scheme in general cooperative
wireless networks without and with utilizing error control coding at the source
node. We first develop an improved signal-to-noise ratio (SNR) threshold-based
FER approximation model. Then, we derive an analytical average FER expression
as well as an asymptotic expression at high SNR for the HRS scheme and
generalize to other relaying schemes. Simulation results are in excellent
agreement with the theoretical analysis, which validates the derived FER
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0725</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0725</id><created>2011-12-04</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Hjorungnes</keyname><forenames>Are</forenames></author><author><keyname>Burr</keyname><forenames>Alister G.</forenames></author></authors><title>Approximate ML Decision Feedback Block Equalizer for Doubly Selective
  Fading Channels</title><categories>cs.IT math.IT</categories><comments>20 pages, 5 figures, 2 tables</comments><journal-ref>IEEE Transactions on Vehicle Technologies, volume 58, number 5,
  pp. 2314-2321, Jun. 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to effetively suppress intersymbol interference (ISI) at low
complexity, we propose in this paper an approximate maximum likelihood (ML)
decision feedback block equalizer (A-ML-DFBE) for doubly selective
(frequency-selective, time-selective) fading channels. The proposed equalizer
design makes efficient use of the special time-domain representation of the
multipath channels through a matched filter, a sliding window, a Gaussian
approximation, and a decision feedback. The A-ML-DFBE has the following
features: 1) It achieves performance close to maximum likelihood sequence
estimation (MLSE), and significantly outperforms the minimum mean square error
(MMSE) based detectors; 2) It has substantially lower complexity than the
conventional equalizers; 3) It easily realizes the complexity and performance
tradeoff by adjusting the length of the sliding window; 4) It has a simple and
fixed-length feedback filter. The symbol error rate (SER) is derived to
characterize the behaviour of the A-ML-DFBE, and it can also be used to find
the key parameters of the proposed equalizer. In addition, we further prove
that the A-ML-DFBE obtains full multipath diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0727</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0727</id><created>2011-12-04</created><authors><author><keyname>Islam</keyname><forenames>Md. Saiful</forenames></author><author><keyname>Hafiz</keyname><forenames>Mohd. Zulfiquar</forenames></author><author><keyname>Begum</keyname><forenames>Zerina</forenames></author></authors><title>Quantum Cost Efficient Reversible BCD Adder for Nanotechnology Based
  Systems</title><categories>cs.AR</categories><comments>4 pages, 12 figures, 1 table, submitted to IJCEE for possible
  publication</comments><journal-ref>International Journal of Computer and Electrical Engineering,
  Vol.4, No.1, pp. 10-13, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic allows low power dissipating circuit design and founds its
application in cryptography, digital signal processing, quantum and optical
information processing. This paper presents a novel quantum cost efficient
reversible BCD adder for nanotechnology based systems using PFAG gate. It has
been demonstrated that the proposed design offers less hardware complexity and
requires minimum number of garbage outputs than the existing counterparts. The
remarkable property of the proposed designs is that its quantum realization is
given in NMR technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0736</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0736</id><created>2011-12-04</created><updated>2012-04-26</updated><authors><author><keyname>Xi</keyname><forenames>Zhengjun</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoguang</forenames></author><author><keyname>Li</keyname><forenames>Yongming</forenames></author></authors><title>Measurement-induced nonlocality based on the relative entropy</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 1 figures, version accepted Phys. Rev. A, PHYSICAL REVIEW A
  85, 042325 (2012)</comments><doi>10.1103/PhysRevA.85.042325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We quantify the measurement-induced nonlocality [Luo and Fu, Phys. Rev. Lett.
106, 120401 (2011)] from the perspective of the relative entropy. This
quantification leads to an operational interpretation for the
measurementinduced nonlocality, namely, it is the maximal entropy increase
after the locally invariant measurements. The relative entropy of nonlocality
is upper bounded by the entropy of the measured subsystem. We establish a
relationship between the relative entropy of nonlocality and the geometric
nonlocality based on the Hilbert- Schmidt norm, and show that it is equal to
the maximal distillable entanglement. Several trade-off relations are obtained
for tripartite pure states. We also give explicit expressions for the relative
entropy of nonlocality for Bell-diagonal states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0741</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0741</id><created>2011-12-04</created><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author></authors><title>On the Difficulty of Deciding Asymptotic Stability of Cubic Homogeneous
  Vector Fields</title><categories>math.OC cs.CC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that asymptotic stability (AS) of homogeneous polynomial
vector fields of degree one (i.e., linear systems) can be decided in polynomial
time e.g. by searching for a quadratic Lyapunov function. Since homogeneous
vector fields of even degree can never be AS, the next interesting degree to
consider is equal to three. In this paper, we prove that deciding AS of
homogeneous cubic vector fields is strongly NP-hard and pose the question of
determining whether it is even decidable. As a byproduct of the reduction that
establishes our NP-hardness result, we obtain a Lyapunov-inspired technique for
proving positivity of forms. We also show that for asymptotically stable
homogeneous cubic vector fields in as few as two variables, the minimum degree
of a polynomial Lyapunov function can be arbitrarily large. Finally, we show
that there is no monotonicity in the degree of polynomial Lyapunov functions
that prove AS; i.e., a homogeneous cubic vector field with no homogeneous
polynomial Lyapunov function of some degree $d$ can very well have a
homogeneous polynomial Lyapunov function of degree less than $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0742</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0742</id><created>2011-12-04</created><authors><author><keyname>Brescia</keyname><forenames>M.</forenames></author><author><keyname>Corazza</keyname><forenames>A.</forenames></author><author><keyname>Cavuoti</keyname><forenames>S.</forenames></author><author><keyname>d'Angelo</keyname><forenames>G.</forenames></author><author><keyname>D'Abrusco</keyname><forenames>R.</forenames></author><author><keyname>Donalek</keyname><forenames>C.</forenames></author><author><keyname>Djorgovski</keyname><forenames>S. G.</forenames></author><author><keyname>Deniskina</keyname><forenames>N.</forenames></author><author><keyname>Fiore</keyname><forenames>M.</forenames></author><author><keyname>Garofalo</keyname><forenames>M.</forenames></author><author><keyname>Laurino</keyname><forenames>O.</forenames></author><author><keyname>Mahabal</keyname><forenames>G. Longo A.</forenames></author><author><keyname>Manna</keyname><forenames>F.</forenames></author><author><keyname>Nocella</keyname><forenames>A.</forenames></author><author><keyname>Skordovski</keyname><forenames>B.</forenames></author></authors><title>The DAME/VO-Neural Infrastructure: an Integrated Data Mining System
  Support for the Science Community</title><categories>astro-ph.IM cs.DL</categories><comments>10 pages, Proceedings of the Final Workshop of the Grid Projects of
  the Italian National Operational Programme 2000-2006 Call 1575; Edited by
  Cometa Consortium, 2009, ISBN: 978-88-95892-02-3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomical data are gathered through a very large number of heterogeneous
techniques and stored in very diversified and often incompatible data
repositories. Moreover in the e-science environment, it is needed to integrate
services across distributed, heterogeneous, dynamic &quot;virtual organizations&quot;
formed by different resources within a single enterprise and/or external
resource sharing and service provider relationships. The DAME/VONeural project,
run jointly by the University Federico II, INAF (National Institute of
Astrophysics) Astronomical Observatories of Napoli and the California Institute
of Technology, aims at creating a single, sustainable, distributed
e-infrastructure for data mining and exploration in massive data sets, to be
offered to the astronomical (but not only) community as a web application. The
framework makes use of distributed computing environments (e.g. S.Co.P.E.) and
matches the international IVOA standards and requirements. The integration
process is technically challenging due to the need of achieving a specific
quality of service when running on top of different native platforms. In these
terms, the result of the DAME/VO-Neural project effort will be a
service-oriented architecture, obtained by using appropriate standards and
incorporating Grid paradigms and restful Web services frameworks where needed,
that will have as main target the integration of interdisciplinary distributed
systems within and across organizational domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0750</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0750</id><created>2011-12-04</created><authors><author><keyname>Brescia</keyname><forenames>M.</forenames></author><author><keyname>Cavuoti</keyname><forenames>S.</forenames></author><author><keyname>D'Abrusco</keyname><forenames>R.</forenames></author><author><keyname>Laurino</keyname><forenames>O.</forenames></author><author><keyname>Longo</keyname><forenames>G.</forenames></author></authors><title>DAME: A Distributed Data Mining &amp; Exploration Framework within the
  Virtual Observatory</title><categories>astro-ph.IM cs.DL</categories><comments>20 pages, INGRID 2010 - 5th International Workshop on Distributed
  Cooperative Laboratories: &quot;Instrumenting&quot; the Grid, May 12-14, 2010, Poznan,
  Poland; Volume Remote Instrumentation for eScience and Related Aspects, 2011,
  F. Davoli et al. (eds.), SPRINGER NY</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, many scientific areas share the same broad requirements of being
able to deal with massive and distributed datasets while, when possible, being
integrated with services and applications. In order to solve the growing gap
between the incremental generation of data and our understanding of it, it is
required to know how to access, retrieve, analyze, mine and integrate data from
disparate sources. One of the fundamental aspects of any new generation of data
mining software tool or package which really wants to become a service for the
community is the possibility to use it within complex workflows which each user
can fine tune in order to match the specific demands of his scientific goal.
These workflows need often to access different resources (data, providers,
computing facilities and packages) and require a strict interoperability on (at
least) the client side. The project DAME (DAta Mining &amp; Exploration) arises
from these requirements by providing a distributed WEB-based data mining
infrastructure specialized on Massive Data Sets exploration with Soft Computing
methods. Originally designed to deal with astrophysical use cases, where first
scientific application examples have demonstrated its effectiveness, the DAME
Suite results as a multi-disciplinary platform-independent tool perfectly
compliant with modern KDD (Knowledge Discovery in Databases) requirements and
Information &amp; Communication Technology trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0765</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0765</id><created>2011-12-04</created><updated>2012-09-07</updated><authors><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Zavlanos</keyname><forenames>Michael M.</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Spectral Design of Dynamic Networks via Local Operations</title><categories>math.OC cs.DM cs.MA cs.SI physics.soc-ph</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the relationship between the eigenvalue spectrum of the
Laplacian matrix of a network and the behavior of dynamical processes evolving
in it, we propose a distributed iterative algorithm in which a group of $n$
autonomous agents self-organize the structure of their communication network in
order to control the network's eigenvalue spectrum. In our algorithm, we assume
that each agent has access only to a local (myopic) view of the network around
it. In each iteration, agents in the network peform a decentralized decision
process to determine the edge addition/deletion that minimizes a distance
function defined in the space of eigenvalue spectra. This spectral distance
presents interesting theoretical properties that allow an efficient distributed
implementation of the decision process. Our iterative algorithm is stable by
construction, i.e., locally optimizes the network's eigenvalue spectrum, and is
shown to perform extremely well in practice. We illustrate our results with
nontrivial simulations in which we design networks matching the spectral
properties of complex networks, such as small-world and power-law networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0767</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0767</id><created>2011-12-04</created><authors><author><keyname>Ishii</keyname><forenames>Akira</forenames></author><author><keyname>Matsumoto</keyname><forenames>Takehiro</forenames></author><author><keyname>Miki</keyname><forenames>Shinji</forenames></author></authors><title>Revenue Prediction of Local Event using Mathematical Model of Hit
  Phenomena</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 3 Figures. submitted to Progress of Theoretical Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretical approach to investigate human-human interaction in society
performed using a many-body theory including human-human interaction. The
advertisement is treated as an external force. The word of mouth (WOM) effect
is included as a two-body interaction between humans. The rumor effect is
included as a three-body interaction between humans. The parameters to define
the strength of human interactions are assumed to be constant values. The
calculated result explained well the two local events &quot;Mizuki-Shigeru Road in
Sakaiminato&quot; and &quot;the sculpture festival at Tottori&quot; in Japan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0784</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0784</id><created>2011-12-04</created><authors><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Fineman</keyname><forenames>Jeremy T.</forenames></author><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author></authors><title>A New Approach to Incremental Cycle Detection and Related Problems</title><categories>cs.DS</categories><acm-class>E.1; F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting a cycle in a directed graph that grows
by arc insertions, and the related problems of maintaining a topological order
and the strong components of such a graph. For these problems, we give two
algorithms, one suited to sparse graphs, and the other to dense graphs. The
former takes the minimum of O(m^{3/2}) and O(mn^{2/3}) time to insert m arcs
into an n-vertex graph; the latter takes O(n^2 log(n)) time. Our sparse
algorithm is considerably simpler than a previous O(m^{3/2})-time algorithm; it
is also faster on graphs of sufficient density. The time bound of our dense
algorithm beats the previously best time bound of O(n^{5/2}) for dense graphs.
Our algorithms rely for their efficiency on topologically ordered vertex
numberings; bounds on the size of the numbers give bound on running times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0789</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0789</id><created>2011-12-04</created><authors><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author><author><keyname>Jutten</keyname><forenames>Christian</forenames></author><author><keyname>Mohimani</keyname><forenames>Hosein</forenames></author></authors><title>On the error of estimating the sparsest solution of underdetermined
  linear systems</title><categories>cs.IT math.IT</categories><comments>To appear in December 2011 issue of IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let A be an n by m matrix with m&gt;n, and suppose that the underdetermined
linear system As=x admits a sparse solution s0 for which ||s0||_0 &lt; 1/2
spark(A). Such a sparse solution is unique due to a well-known uniqueness
theorem. Suppose now that we have somehow a solution s_hat as an estimation of
s0, and suppose that s_hat is only `approximately sparse', that is, many of its
components are very small and nearly zero, but not mathematically equal to
zero. Is such a solution necessarily close to the true sparsest solution? More
generally, is it possible to construct an upper bound on the estimation error
||s_hat-s0||_2 without knowing s0? The answer is positive, and in this paper we
construct such a bound based on minimal singular values of submatrices of A. We
will also state a tight bound, which is more complicated, but besides being
tight, enables us to study the case of random dictionaries and obtain
probabilistic upper bounds. We will also study the noisy case, that is, where
x=As+n. Moreover, we will see that where ||s0||_0 grows, to obtain a
predetermined guaranty on the maximum of ||s_hat-s0||_2, s_hat is needed to be
sparse with a better approximation. This can be seen as an explanation to the
fact that the estimation quality of sparse recovery algorithms degrades where
||s0||_0 grows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0790</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0790</id><created>2011-12-04</created><authors><author><keyname>Duan</keyname><forenames>Ran</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Su</keyname><forenames>Hsin-Hao</forenames></author></authors><title>Scaling algorithms for approximate and exact maximum weight matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em maximum cardinality} and {\em maximum weight matching} problems can
be solved in time $\tilde{O}(m\sqrt{n})$, a bound that has resisted improvement
despite decades of research. (Here $m$ and $n$ are the number of edges and
vertices.) In this article we demonstrate that this &quot;$m\sqrt{n}$ barrier&quot; is
extremely fragile, in the following sense. For any $\epsilon&gt;0$, we give an
algorithm that computes a $(1-\epsilon)$-approximate maximum weight matching in
$O(m\epsilon^{-1}\log\epsilon^{-1})$ time, that is, optimal {\em linear time}
for any fixed $\epsilon$. Our algorithm is dramatically simpler than the best
exact maximum weight matching algorithms on general graphs and should be
appealing in all applications that can tolerate a negligible relative error.
  Our second contribution is a new {\em exact} maximum weight matching
algorithm for integer-weighted bipartite graphs that runs in time
$O(m\sqrt{n}\log N)$. This improves on the $O(Nm\sqrt{n})$-time and
$O(m\sqrt{n}\log(nN))$-time algorithms known since the mid 1980s, for $1\ll
\log N \ll \log n$. Here $N$ is the maximum integer edge weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0791</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0791</id><created>2011-12-04</created><authors><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Truszczy&#x144;ski</keyname><forenames>Miros&#x142;aw</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Strong Equivalence of Qualitative Optimization Problems</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the framework of qualitative optimization problems (or, simply,
optimization problems) to represent preference theories. The formalism uses
separate modules to describe the space of outcomes to be compared (the
generator) and the preferences on outcomes (the selector). We consider two
types of optimization problems. They differ in the way the generator, which we
model by a propositional theory, is interpreted: by the standard propositional
logic semantics, and by the equilibrium-model (answer-set) semantics. Under the
latter interpretation of generators, optimization problems directly generalize
answer-set optimization programs proposed previously. We study strong
equivalence of optimization problems, which guarantees their interchangeability
within any larger context. We characterize several versions of strong
equivalence obtained by restricting the class of optimization problems that can
be used as extensions and establish the complexity of associated reasoning
tasks. Understanding strong equivalence is essential for modular representation
of optimization problems and rewriting techniques to simplify them without
changing their inherent properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0795</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0795</id><created>2011-12-04</created><authors><author><keyname>Reinaldo</keyname><forenames>Mayol Arnao</forenames></author><author><keyname>A.</keyname><forenames>Nu&#xf1;ez Luis</forenames></author><author><keyname>Antonio</keyname><forenames>Lobo</forenames></author></authors><title>An Approach to Log Management: Prototyping a Design of Agent for Log
  Harvesting</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a work in progress implementing a solution for
harvesting and transporting information logs from network devices in a
e-science environment. The system is composed for servers, agents, active
devices and a transporting protocol. This document describes the state of
development of agents. Agents capture logs from devices, normalize, reduce and
cataloged them by using metadata. Once all these processes are done, they
transmit the cataloged data by using Transportation Protocol to a warehouse
server. Also an agent use orchestration parameters to transmit modified logs to
a data warehouse server. These parameters can be received from orchestration
applications such as Taverna. The operation of the agents and the communication
protocol solve some of the deficiencies of traditional logs management
protocols. Finally, we show some test realized over the new prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0805</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0805</id><created>2011-12-04</created><updated>2013-05-11</updated><authors><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Song</keyname><forenames>Qingyang</forenames></author><author><keyname>Guo</keyname><forenames>Lei</forenames></author><author><keyname>Jamalipour</keyname><forenames>Abbas</forenames></author></authors><title>Constellation Mapping for Physical-Layer Network Coding with M-QAM
  Modulation</title><categories>cs.IT cs.NI cs.SY math.IT</categories><comments>Final version at IEEE GLOBECOM 2012</comments><journal-ref>IEEE Global Communications Conference (GLOBECOM) 2012, pp.
  4429-4434</journal-ref><doi>10.1109/GLOCOM.2012.6503815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The denoise-and-forward (DNF) method of physical-layer network coding (PNC)
is a promising approach for wireless relaying networks. In this paper, we
consider DNF-based PNC with M-ary quadrature amplitude modulation (M-QAM) and
propose a mapping scheme that maps the superposed M-QAM signal to coded
symbols. The mapping scheme supports both square and non-square M-QAM
modulations, with various original constellation mappings (e.g. binary-coded or
Gray-coded). Subsequently, we evaluate the symbol error rate and bit error rate
(BER) of M-QAM modulated PNC that uses the proposed mapping scheme. Afterwards,
as an application, a rate adaptation scheme for the DNF method of PNC is
proposed. Simulation results show that the rate-adaptive PNC is advantageous in
various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0826</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0826</id><created>2011-12-04</created><updated>2014-08-07</updated><authors><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author></authors><title>Clustering under Perturbation Resilience</title><categories>cs.LG cs.DS</categories><comments>53 pages</comments><msc-class>68Q25, 68Q32, 68T05, 68W25, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that distances between data points in many real-world
clustering instances are often based on heuristic measures, Bilu and
Linial~\cite{BL} proposed analyzing objective based clustering problems under
the assumption that the optimum clustering to the objective is preserved under
small multiplicative perturbations to distances between points. The hope is
that by exploiting the structure in such instances, one can overcome worst case
hardness results.
  In this paper, we provide several strong results within this framework. For
center-based objectives, we present an algorithm that can optimally cluster
instances resilient to perturbations of factor $(1 + \sqrt{2})$, solving an
open problem of Awasthi et al.~\cite{ABS10}. For $k$-median, a center-based
objective of special interest, we additionally give algorithms for a more
relaxed assumption in which we allow the optimal solution to change in a small
$\epsilon$ fraction of the points after perturbation. We give the first bounds
known for $k$-median under this more realistic and more general assumption. We
also provide positive results for min-sum clustering which is a generally much
harder objective than center-based objectives. Our algorithms are based on new
linkage criteria that may be of independent interest.
  Additionally, we give sublinear-time algorithms, showing algorithms that can
return an implicit clustering from only access to a small random sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0829</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0829</id><created>2011-12-04</created><authors><author><keyname>Hayes</keyname><forenames>Thomas P.</forenames></author></authors><title>How Not to Win a Million Dollars: A Counterexample to a Conjecture of L.
  Breiman</title><categories>math.PR cs.GT</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a gambling game in which we are allowed to repeatedly bet a portion
of our bankroll at favorable odds. We investigate the question of how to
minimize the expected number of rounds needed to increase our bankroll to a
given target amount.
  Specifically, we disprove a 50-year old conjecture of L. Breiman, that there
exists a threshold strategy that optimizes the expected number of rounds; that
is, a strategy that always bets to try to win in one round whenever the
bankroll is at least a certain threshold, and that makes Kelly bets (a simple
proportional betting scheme) whenever the bankroll is below the threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0836</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0836</id><created>2011-12-05</created><authors><author><keyname>Shah</keyname><forenames>Jolly</forenames></author><author><keyname>Saxena</keyname><forenames>Vikas</forenames></author></authors><title>Performance Study on Image Encryption Schemes</title><categories>cs.CR</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, July 2011, 349-355</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image applications have been increasing in recent years.Encryption is used to
provide the security needed for image applications. In this paper, we classify
various image encryption schemes and analyze them with respect to various
parameters like tunability, visual degradation, compression friendliness,format
compliance, encryption ratio, speed, and cryptographic security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0845</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0845</id><created>2011-12-05</created><updated>2013-12-31</updated><authors><author><keyname>Kuperberg</keyname><forenames>Greg</forenames><affiliation>UC Davis</affiliation></author></authors><title>Knottedness is in NP, modulo GRH</title><categories>math.GT cs.CC</categories><comments>7 pages; minor update</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a tame knot K presented in the form of a knot diagram, we show that the
problem of determining whether K is knotted is in the complexity class NP,
assuming the generalized Riemann hypothesis (GRH). In other words, there exists
a polynomial-length certificate that can be verified in polynomial time to
prove that K is non-trivial. GRH is not needed to believe the certificate, but
only to find a short certificate. This result complements the result of Hass,
Lagarias, and Pippenger that unknottedness is in NP. Our proof is a corollary
of major results of others in algebraic geometry and geometric topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0850</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0850</id><created>2011-12-05</created><authors><author><keyname>Habich</keyname><forenames>Johannes</forenames></author><author><keyname>Feichtinger</keyname><forenames>Christian</forenames></author><author><keyname>K&#xf6;stler</keyname><forenames>Harald</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Performance engineering for the Lattice Boltzmann method on GPGPUs:
  Architectural requirements and performance results</title><categories>cs.PF</categories><comments>10 pages, 7 figures, 4 tables, preprint submitted to Computers and
  Fluids journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GPUs offer several times the floating point performance and memory bandwidth
of current standard two socket CPU servers, e.g. NVIDIA C2070 vs. Intel Xeon
Westmere X5650. The lattice Boltzmann method has been established as a flow
solver in recent years and was one of the first flow solvers to be successfully
ported and that performs well on GPUs. We demonstrate advanced optimization
strategies for a D3Q19 lattice Boltzmann based incompressible flow solver for
GPGPUs and CPUs based on NVIDIA CUDA and OpenCL. Since the implemented
algorithm is limited by memory bandwidth, we concentrate on improving memory
access. Basic data layout issues for optimal data access are explained and
discussed. Furthermore, the algorithmic steps are rearranged to improve
scattered access of the GPU memory. The importance of occupancy is discussed as
well as optimization strategies to improve overall concurrency. We arrive at a
well-optimized GPU kernel, which is integrated into a larger framework that can
handle single phase fluid flow simulations as well as particle-laden flows. Our
3D LBM GPU implementation reaches up to 650 MLUPS in single precision and 290
MLUPS in double precision on an NVIDIA Tesla C2070.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0857</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0857</id><created>2011-12-05</created><authors><author><keyname>Hellings</keyname><forenames>Jelle</forenames></author><author><keyname>Fletcher</keyname><forenames>George H. L.</forenames></author><author><keyname>Haverkort</keyname><forenames>Herman</forenames></author></authors><title>I/O efficient bisimulation partitioning on very large directed acyclic
  graphs</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the first efficient external-memory algorithm to
compute the bisimilarity equivalence classes of a directed acyclic graph (DAG).
DAGs are commonly used to model data in a wide variety of practical
applications, ranging from XML documents and data provenance models, to web
taxonomies and scientific workflows. In the study of efficient reasoning over
massive graphs, the notion of node bisimilarity plays a central role. For
example, grouping together bisimilar nodes in an XML data set is the first step
in many sophisticated approaches to building indexing data structures for
efficient XPath query evaluation. To date, however, only internal-memory
bisimulation algorithms have been investigated. As the size of real-world DAG
data sets often exceeds available main memory, storage in external memory
becomes necessary. Hence, there is a practical need for an efficient approach
to computing bisimulation in external memory.
  Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)),
where |N| and |E| are the numbers of nodes and edges, resp., in the data graph
and Sort(n) is the number of accesses to external memory needed to sort an
input of size n. We also study specializations of this algorithm to common
variations of bisimulation for tree-structured XML data sets. We empirically
verify efficient performance of the algorithms on graphs and XML documents
having billions of nodes and edges, and find that the algorithms can process
such graphs efficiently even when very limited internal memory is available.
The proposed algorithms are simple enough for practical implementation and use,
and open the door for further study of external-memory bisimulation algorithms.
To this end, the full open-source C++ implementation has been made freely
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0896</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0896</id><created>2011-12-05</created><updated>2011-12-11</updated><authors><author><keyname>Buzaglo</keyname><forenames>Sarit</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>On the Existence of Perfect Codes for Asymmetric Limited-Magnitude
  Errors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block codes, which correct asymmetric errors with limited-magnitude, are
studied. These codes have been applied recently for error correction in flash
memories. The codes will be represented by lattices and the constructions will
be based on a generalization of Sidon sequences. In particular we will consider
perfect codes for these type of errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0922</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0922</id><created>2011-12-05</created><updated>2011-12-06</updated><authors><author><keyname>Oetsch</keyname><forenames>Johannes</forenames></author><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author></authors><title>Extending Object-Oriented Languages by Declarative Specifications of
  Complex Objects using Answer-Set Programming</title><categories>cs.PL cs.AI</categories><comments>Submitted to the 34th International Conference on Software
  Engineering (ICSE), New Ideas and Emerging Results (NIER) track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require complexly structured data objects. Developing new
or adapting existing algorithmic solutions for creating such objects can be a
non-trivial and costly task if the considered objects are subject to different
application-specific constraints. Often, however, it is comparatively easy to
declaratively describe the required objects. In this paper, we propose to use
answer-set programming (ASP)---a well-established declarative programming
paradigm from the area of artificial intelligence---for instantiating objects
in standard object-oriented programming languages. In particular, we extend
Java with declarative specifications from which the required objects can be
automatically generated using available ASP solver technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0923</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0923</id><created>2011-12-05</created><authors><author><keyname>Gabbay</keyname><forenames>Murdoch J.</forenames></author></authors><title>Finite and infinite support in nominal algebra and logic: nominal
  completeness theorems for free</title><categories>cs.LO math.LO</categories><msc-class>03B70 (Primary) 68Q55 (Secondary)</msc-class><acm-class>F.3.0; F.3.2</acm-class><doi>10.2178/jsl/1344862164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By operations on models we show how to relate completeness with respect to
permissive-nominal models to completeness with respect to nominal models with
finite support. Models with finite support are a special case of
permissive-nominal models, so the construction hinges on generating from an
instance of the latter, some instance of the former in which sufficiently many
inequalities are preserved between elements. We do this using an infinite
generalisation of nominal atoms-abstraction.
  The results are of interest in their own right, but also, we factor the
mathematics so as to maximise the chances that it could be used off-the-shelf
for other nominal reasoning systems too. Models with infinite support can be
easier to work with, so it is useful to have a semi-automatic theorem to
transfer results from classes of infinitely-supported nominal models to the
more restricted class of models with finite support.
  In conclusion, we consider different permissive-nominal syntaxes and nominal
models and discuss how they relate to the results proved here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0941</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0941</id><created>2011-12-05</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Fang</keyname><forenames>Xiaole</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Wang</keyname><forenames>Qianxue</forenames></author></authors><title>Randomness Quality of CI Chaotic Generators: Applications to Internet
  Security</title><categories>cs.CR</categories><comments>6 pages,6 figures, In INTERNET'2010. The 2nd Int. Conf. on Evolving
  Internet, Valencia, Spain, pages 125-130, September 2010. IEEE Computer
  Society Press Note: Best Paper award</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the rapid development of the Internet in recent years, the need to
find new tools to reinforce trust and security through the Internet has became
a major concern. The discovery of new pseudo-random number generators with a
strong level of security is thus becoming a hot topic, because numerous
cryptosystems and data hiding schemes are directly dependent on the quality of
these generators. At the conference Internet`09, we have described a generator
based on chaotic iterations, which behaves chaotically as defined by Devaney.
In this paper, the proposal is to improve the speed and the security of this
generator, to make its use more relevant in the Internet security context. To
do so, a comparative study between various generators is carried out and
statistical results are given. Finally, an application in the information
hiding framework is presented, to give an illustrative example of the use of
such a generator in the Internet security field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0945</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0945</id><created>2011-12-05</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Cancellieri</keyname><forenames>Giovanni</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Interleaved Product LDPC Codes</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures, accepted for publication in IEEE Transactions on
  Communications</comments><journal-ref>IEEE Transactions on Communications, Vol. 60, No. 4, pp. 895-901,
  Apr. 2012</journal-ref><doi>10.1109/TCOMM.2012.030712.100173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product LDPC codes take advantage of LDPC decoding algorithms and the high
minimum distance of product codes. We propose to add suitable interleavers to
improve the waterfall performance of LDPC decoding. Interleaving also reduces
the number of low weight codewords, that gives a further advantage in the error
floor region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0950</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0950</id><created>2011-12-05</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Wang</keyname><forenames>Qianxue</forenames></author></authors><title>Class of Trustworthy Pseudo-Random Number Generators</title><categories>cs.CR</categories><comments>6 pages, 3 figures, In INTERNET 2011, the 3-rd Int. Conf. on Evolving
  Internet, Luxembourg, Luxembourg, pages 72--77, June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the widespread use of communication technologies, cryptosystems are
therefore critical to guarantee security over open networks as the Internet.
Pseudo-random number generators (PRNGs) are fundamental in cryptosystems and
information hiding schemes. One of the existing chaos-based PRNGs is using
chaotic iterations schemes. In prior literature, the iterate function is just
the vectorial boolean negation. In this paper, we propose a method using Graph
with strongly connected components as a selection criterion for chaotic iterate
function. In order to face the challenge of using the proposed chaotic iterate
functions in PRNG, these PRNGs are subjected to a statistical battery of tests,
which is the well-known NIST in the area of cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0958</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0958</id><created>2011-12-05</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Fang</keyname><forenames>Xiaole</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Wang</keyname><forenames>Qianxue</forenames></author></authors><title>On the design of a family of CI pseudo-random number generators</title><categories>cs.CR</categories><comments>4 pages, In WICOM'11, 7th Int. IEEE Conf. on Wireless Communications,
  Networking and Mobile Computing, Wuhan, China, pages 1--4, September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaos and its applications in the field of secure communications have
attracted a lot of attention. Chaos-based pseudo-random number generators are
critical to guarantee security over open networks as the Internet. We have
previously demonstrated that it is possible to define such generators with good
statistical properties by using a tool called &quot;chaotic iterations&quot;, which
depends on an iteration function. An approach to find update functions such
that the associated generator presents a random-like and chaotic behavior is
proposed in this research work. To do so, we use the vectorial Boolean negation
as a prototype and explain how to modify this iteration function without
deflating the good properties of the associated generator. Simulation results
and basic security analysis are then presented to evaluate the randomness of
this new family of generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0961</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0961</id><created>2011-12-05</created><authors><author><keyname>Schumann</keyname><forenames>Andrew</forenames></author></authors><title>Two Squares of Opposition: for Analytic and Synthetic Propositions</title><categories>cs.OH</categories><journal-ref>Bulletin of the Section of Logic, 40/3-4, 2011, pp. 165-178</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper I prove that there are two squares of opposition. The
unconventional one is built up for synthetic propositions. There a, i are
contrary, a, o (resp. e, i) are contradictory, e, o are subcontrary, a, e
(resp. i, o) are said to stand in the subalternation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0974</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0974</id><created>2011-12-05</created><authors><author><keyname>Lellmann</keyname><forenames>Jan</forenames></author><author><keyname>Lenzen</keyname><forenames>Frank</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Optimality Bounds for a Variational Relaxation of the Image Partitioning
  Problem</title><categories>cs.CV math.CO math.FA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variational convex relaxation of a class of optimal
partitioning and multiclass labeling problems, which has recently proven quite
successful and can be seen as a continuous analogue of Linear Programming (LP)
relaxation methods for finite-dimensional problems. While for the latter case
several optimality bounds are known, to our knowledge no such bounds exist in
the continuous setting. We provide such a bound by analyzing a probabilistic
rounding method, showing that it is possible to obtain an integral solution of
the original partitioning problem from a solution of the relaxed problem with
an a priori upper bound on the objective, ensuring the quality of the result
from the viewpoint of optimization. The approach has a natural interpretation
as an approximate, multiclass variant of the celebrated coarea formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0983</identifier>
 <datestamp>2013-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0983</id><created>2011-12-05</created><updated>2012-12-07</updated><authors><author><keyname>Bombrun</keyname><forenames>Alex</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Pomet</keyname><forenames>Jean-Baptiste</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>The averaged control system of fast oscillating control systems</title><categories>math.OC cs.SY</categories><comments>(2012)</comments><proxy>ccsd</proxy><journal-ref>SIAM J. Control Optim. 51 (2013), No 3, pp. 2280-2305</journal-ref><doi>10.1137/11085791X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For control systems that either have a fast explicit periodic dependence on
time and bounded controls or have periodic solutions and small controls, we
define an average control system that takes into account all possible
variations of the control, and prove that its solutions approximate all
solutions of the oscillating system as oscillations go faster. The dimension of
its velocity set is characterized geometrically. When it is maximum the average
system defines a Finsler metric, not twice differentiable in general. For
minimum time control, this average system allows one to give a rigorous proof
that averaging the Hamiltonian given by the maximum principle is a valid
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0987</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0987</id><created>2011-12-05</created><updated>2014-10-11</updated><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>Small Jump with Negation-UTM Trampoline</title><categories>cs.CC</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper divide some complexity class by using fixpoint and fixpointless
area of Decidable Universal Turing Machine (UTM). Decidable Deterministic
Turing Machine (DTM) have fixpointless combinator that add no extra resources
(like Negation), but UTM makes some fixpoint in the combinator. This means that
we can jump out of the fixpointless combinator system by making more complex
problem from diagonalisation argument of UTM.
  As a concrete example, we proof L is not P . We can make Polynomial time UTM
that emulate all Logarithm space DTM (LDTM). LDTM set close under Negation,
therefore UTM does not close under LDTM set. (We can proof this theorem like
halting problem and time/space hierarchy theorem, and also we can extend this
proof to divide time/space limited DTM set.) In the same way, we proof P is not
NP. These are new hierarchy that use UTM and Negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0992</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0992</id><created>2011-12-05</created><authors><author><keyname>Vafopoulos</keyname><forenames>Michalis</forenames></author></authors><title>The Web economy: goods, users, models and policies</title><categories>cs.CY cs.SI</categories><msc-class>91B54, 91B44, 91B74</msc-class><acm-class>J.4; K.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web emerged as an antidote to the rapidly increasing quantity of accumulated
knowledge and become successful because it facilitates massive participation
and communication with minimum costs. Today, its enormous impact, scale and
dynamism in time and space make very difficult (and sometimes impossible) to
measure and anticipate the effects in human society. In addition to that, we
demand from the Web to be fast, secure, reliable, all-inclusive and trustworthy
in any transaction. The scope of the present article is to review a part of the
Web economy literature that will help us to identify its major participants and
their functions. The goal is to understand how the Web economy differs from the
traditional setting and what implications have these differences. Secondarily,
we attempt to establish a minimal common understanding about the incentives and
properties of the Web economy. In this direction the concept of Web Goods and a
new classification of Web Users are introduced and analyzed This article, is
not, by any means, a thorough review of the economic literature related to the
Web. We focus only on its relevant part that models the Web as a standalone
economic artifact with native functionality and processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.0993</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.0993</id><created>2011-12-05</created><authors><author><keyname>Elmasry</keyname><forenames>Amr</forenames></author><author><keyname>Katajainen</keyname><forenames>Jyrki</forenames></author></authors><title>Worst-Case Optimal Priority Queues via Extended Regular Counters</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical problem of representing a collection of priority
queues under the operations \Findmin{}, \Insert{}, \Decrease{}, \Meld{},
\Delete{}, and \Deletemin{}. In the comparison-based model, if the first four
operations are to be supported in constant time, the last two operations must
take at least logarithmic time. Brodal showed that his worst-case efficient
priority queues achieve these worst-case bounds. Unfortunately, this data
structure is involved and the time bounds hide large constants. We describe a
new variant of the worst-case efficient priority queues that relies on extended
regular counters and provides the same asymptotic time and space bounds as the
original. Due to the conceptual separation of the operations on regular
counters and all other operations, our data structure is simpler and easier to
describe and understand. Also, the constants in the time and space bounds are
smaller. In addition, we give an implementation of our structure on a pointer
machine. For our pointer-machine implementation, \Decrease{} and \Meld{} are
asymptotically slower and require $O(\lg\lg{n})$ worst-case time, where $n$
denotes the number of elements stored in the resulting priority queue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1010</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1010</id><created>2011-12-05</created><updated>2012-05-11</updated><authors><author><keyname>Bliss</keyname><forenames>Catherine A.</forenames></author><author><keyname>Kloumann</keyname><forenames>Isabel M.</forenames></author><author><keyname>Harris</keyname><forenames>Kameron Decker</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Twitter reciprocal reply networks exhibit assortativity with respect to
  happiness</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 21 figures, 5 tables, In press at the Journal of
  Computational Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of social media has provided an extraordinary, if imperfect, 'big
data' window into the form and evolution of social networks. Based on nearly 40
million message pairs posted to Twitter between September 2008 and February
2009, we construct and examine the revealed social network structure and
dynamics over the time scales of days, weeks, and months. At the level of user
behavior, we employ our recently developed hedonometric analysis methods to
investigate patterns of sentiment expression. We find users' average happiness
scores to be positively and significantly correlated with those of users one,
two, and three links away. We strengthen our analysis by proposing and using a
null model to test the effect of network topology on the assortativity of
happiness. We also find evidence that more well connected users write happier
status updates, with a transition occurring around Dunbar's number. More
generally, our work provides evidence of a social sub-network structure within
Twitter and raises several methodological points of interest with regard to
social network reconstructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1012</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1012</id><created>2011-12-05</created><authors><author><keyname>Cattani</keyname><forenames>Eduardo</forenames></author><author><keyname>Cueto</keyname><forenames>Maria Angelica</forenames></author><author><keyname>Dickenstein</keyname><forenames>Alicia</forenames></author><author><keyname>Di Rocco</keyname><forenames>Sandra</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author></authors><title>Mixed Discriminants</title><categories>math.AG cs.SC math.CO</categories><comments>17 pages</comments><report-no>Mittag-Leffler-2011spring</report-no><msc-class>13P15, 14M25, 14T05, 52B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mixed discriminant of n Laurent polynomials in n variables is the
irreducible polynomial in the coefficients which vanishes whenever two of the
roots coincide. The Cayley trick expresses the mixed discriminant as an
A-discriminant. We show that the degree of the mixed discriminant is a
piecewise linear function in the Plucker coordinates of a mixed Grassmannian.
An explicit degree formula is given for the case of plane curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1038</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1038</id><created>2011-12-05</created><authors><author><keyname>Jones</keyname><forenames>Jason J.</forenames></author><author><keyname>Bond</keyname><forenames>Robert M.</forenames></author><author><keyname>Fariss</keyname><forenames>Christopher J.</forenames></author><author><keyname>Settle</keyname><forenames>Jaime E.</forenames></author><author><keyname>Kramer</keyname><forenames>Adam</forenames></author><author><keyname>Marlow</keyname><forenames>Cameron</forenames></author><author><keyname>Fowler</keyname><forenames>James H.</forenames></author></authors><title>Yahtzee: An Anonymized Group Level Matching Procedure</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers often face the problem of needing to protect the privacy of
subjects while also needing to integrate data that contains personal
information from diverse data sources in order to conduct their research. The
advent of computational social science and the enormous amount of data about
people that is being collected makes protecting the privacy of research
subjects evermore important. However, strict privacy procedures can make
joining diverse sources of data that contain information about specific
individual behaviors difficult. In this paper we present a procedure to keep
information about specific individuals from being &quot;leaked&quot; or shared in either
direction between two sources of data. To achieve this goal, we randomly assign
individuals to anonymous groups before combining the anonymized information
between the two sources of data. We refer to this method as the Yahtzee
procedure, and show that it performs as expected theoretically when we apply it
to data from Facebook and public voter records.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1040</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1040</id><created>2011-12-05</created><authors><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Xia</keyname><forenames>Ge</forenames></author></authors><title>What makes normalized weighted satisfiability tractable</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the weighted antimonotone and the weighted monotone
satisfiability problems on normalized circuits of depth at most $t \geq 2$,
abbreviated {\sc wsat$^-[t]$} and {\sc wsat$^+[t]$}, respectively. These
problems model the weighted satisfiability of antimonotone and monotone
propositional formulas (including weighted anitmonoone/monotone {\sc cnf-sat})
in a natural way, and serve as the canonical problems in the definition of the
parameterized complexity hierarchy. We characterize the parameterized
complexity of {\sc wsat$^-[t]$} and {\sc wsat$^+[t]$} with respect to the genus
of the circuit. For {\sc wsat$^-[t]$}, which is $W[t]$-complete for odd $t$ and
$W[t-1]$-complete for even $t$, the characterization is precise: We show that
{\sc wsat$^-[t]$} is fixed-parameter tractable (FPT) if the genus of the
circuit is $n^{o(1)}$ ($n$ is the number of the variables in the circuit), and
that it has the same $W$-hardness as the general {\sc wsat$^-[t]$} problem
(i.e., with no restriction on the genus) if the genus is $n^{O(1)}$. For {\sc
wsat$^+[2]$} (i.e., weighted monotone {\sc cnf-sat}), which is $W[2]$-complete,
the characterization is also precise: We show that {\sc wsat$^+[2]$} is FPT if
the genus is $n^{o(1)}$ and $W[2]$-complete if the genus is $n^{O(1)}$. For
{\sc wsat$^+[t]$} where $t &gt; 2$, which is $W[t]$-complete for even $t$ and
$W[t-1]$-complete for odd $t$, we show that it is FPT if the genus is
$O(\sqrt{\log{n}})$, and that it has the same $W$-hardness as the general {\sc
wsat$^+[t]$} problem if the genus is $n^{O(1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1041</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1041</id><created>2011-12-05</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>Stabilization of Branching Queueing Networks</title><categories>cs.PF</categories><comments>technical report for a STACS'12 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Queueing networks are gaining attraction for the performance analysis of
parallel computer systems. A Jackson network is a set of interconnected
servers, where the completion of a job at server i may result in the creation
of a new job for server j. We propose to extend Jackson networks by &quot;branching&quot;
and by &quot;control&quot; features. Both extensions are new and substantially expand the
modelling power of Jackson networks. On the other hand, the extensions raise
computational questions, particularly concerning the stability of the networks,
i.e, the ergodicity of the underlying Markov chain. We show for our extended
model that it is decidable in polynomial time if there exists a controller that
achieves stability. Moreover, if such a controller exists, one can efficiently
compute a static randomized controller which stabilizes the network in a very
strong sense; in particular, all moments of the queue sizes are finite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1045</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1045</id><created>2011-12-05</created><updated>2012-04-09</updated><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Non-Malleable Extractors, Two-Source Extractors and Privacy
  Amplification</title><categories>cs.CR cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dodis and Wichs introduced the notion of a non-malleable extractor to study
the problem of privacy amplification with an active adversary. A non-malleable
extractor is a much stronger version of a strong extractor. Previously, there
are only two known constructions of non-malleable extractors. Both
constructions only work for (n, k)-sources with k&gt;n/2. Interestingly, both
constructions are also two-source extractors.
  In this paper, we present a strong connection between non-malleable
extractors and two-source extractors. The first part of the connection shows
that non-malleable extractors can be used to construct two-source extractors.
With appropriate parameters the resulted two-source extractor beats the best
known construction of two-source extractors. This partially explains why
previous constructions of non-malleable extractors only work for sources with
entropy rate &gt;1/2, and why explicit non-malleable extractors for small
min-entropy may be hard to get. The second part of the connection shows that
certain two-source extractors can be used to construct non-malleable
extractors. Using this connection, we obtain the first construction of
non-malleable extractors for k &lt; n/2. Specifically, we give an unconditional
construction for min-entropy k=(1/2-\delta)n for some constant \delta&gt;0, and a
conditional (semi-explicit) construction that can potentially achieve k=\alpha
n for any constant \alpha&gt;0.
  Finally, despite the lack of explicit non-malleable extractors for
arbitrarily linear entropy, we give the first 2-round privacy amplification
protocol with asymptotically optimal entropy loss and communication complexity
for (n, k) sources with k=\alpha n for any constant \alpha&gt;0. This dramatically
improves previous results and answers an open problem in \cite{DLWZ11}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1048</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1048</id><created>2011-12-05</created><authors><author><keyname>Godavarty</keyname><forenames>Vinod Kumar</forenames></author></authors><title>Using Quasigroups for Generating Pseudorandom Numbers</title><categories>cs.CR</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm for generating pseudorandom numbers using
quasigroups. Random numbers have several applications in the area of secure
communication. The proposed algorithm uses a matrix of size n x n which is
pre-generated and stored. The quality of random numbers generated is compared
with other pseudorandom number generator using Marsaglia's Diehard battery of
tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1051</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1051</id><created>2011-12-05</created><authors><author><keyname>Mao</keyname><forenames>Huina</forenames></author><author><keyname>Counts</keyname><forenames>Scott</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author></authors><title>Predicting Financial Markets: Comparing Survey, News, Twitter and Search
  Engine Data</title><categories>q-fin.ST cs.CE physics.soc-ph</categories><comments>This paper includes 10 pages, 6 figures and 10 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial market prediction on the basis of online sentiment tracking has
drawn a lot of attention recently. However, most results in this emerging
domain rely on a unique, particular combination of data sets and sentiment
tracking tools. This makes it difficult to disambiguate measurement and
instrument effects from factors that are actually involved in the apparent
relation between online sentiment and market values. In this paper, we survey a
range of online data sets (Twitter feeds, news headlines, and volumes of Google
search queries) and sentiment tracking methods (Twitter Investor Sentiment,
Negative News Sentiment and Tweet &amp; Google Search volumes of financial terms),
and compare their value for financial prediction of market indices such as the
Dow Jones Industrial Average, trading volumes, and market volatility (VIX), as
well as gold prices. We also compare the predictive power of traditional
investor sentiment survey data, i.e. Investor Intelligence and Daily Sentiment
Index, against those of the mentioned set of online sentiment indicators. Our
results show that traditional surveys of Investor Intelligence are lagging
indicators of the financial markets. However, weekly Google Insight Search
volumes on financial search queries do have predictive value. An indicator of
Twitter Investor Sentiment and the frequency of occurrence of financial terms
on Twitter in the previous 1-2 days are also found to be very statistically
significant predictors of daily market log return. Survey sentiment indicators
are however found not to be statistically significant predictors of financial
market values, once we control for all other mood indicators as well as the
VIX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1086</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1086</id><created>2011-12-05</created><authors><author><keyname>Paparrizos</keyname><forenames>Ioannis</forenames></author><author><keyname>Basagiannis</keyname><forenames>Stylianos</forenames></author><author><keyname>Petridou</keyname><forenames>Sophia</forenames></author></authors><title>Quantitative Analysis for Authentication of Low-cost RFID Tags</title><categories>cs.NI</categories><comments>To appear in the 36th IEEE Conference on Local Computer Networks (LCN
  2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal analysis techniques are widely used today in order to verify and
analyze communication protocols. In this work, we launch a quantitative
verification analysis for the low- cost Radio Frequency Identification (RFID)
protocol proposed by Song and Mitchell. The analysis exploits a Discrete-Time
Markov Chain (DTMC) using the well-known PRISM model checker. We have managed
to represent up to 100 RFID tags communicating with a reader and quantify each
RFID session according to the protocol's computation and transmission cost
requirements. As a consequence, not only does the proposed analysis provide
quantitative verification results, but also it constitutes a methodology for
RFID designers who want to validate their products under specific cost
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1115</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1115</id><created>2011-12-05</created><updated>2013-03-28</updated><authors><author><keyname>Romero</keyname><forenames>Daniel M.</forenames></author><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Ugander</keyname><forenames>Johan</forenames></author></authors><title>On the Interplay between Social and Topical Structure</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People's interests and people's social relationships are intuitively
connected, but understanding their interplay and whether they can help predict
each other has remained an open question. We examine the interface of two
decisive structures forming the backbone of online social media: the graph
structure of social networks - who connects with whom - and the set structure
of topical affiliations - who is interested in what. In studying this
interface, we identify key relationships whereby each of these structures can
be understood in terms of the other. The context for our analysis is Twitter, a
complex social network of both follower relationships and communication
relationships. On Twitter, &quot;hashtags&quot; are used to label conversation topics,
and we examine hashtag usage alongside these social structures.
  We find that the hashtags that users adopt can predict their social
relationships, and also that the social relationships between the initial
adopters of a hashtag can predict the future popularity of that hashtag. By
studying weighted social relationships, we observe that while strong
reciprocated ties are the easiest to predict from hashtag structure, they are
also much less useful than weak directed ties for predicting hashtag
popularity. Importantly, we show that computationally simple structural
determinants can provide remarkable performance in both tasks. While our
analyses focus on Twitter, we view our findings as broadly applicable to
topical affiliations and social relationships in a host of diverse contexts,
including the movies people watch, the brands people like, or the locations
people frequent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1116</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1116</id><created>2011-12-05</created><updated>2013-04-20</updated><authors><author><keyname>Weimann</keyname><forenames>Oren</forenames></author><author><keyname>Yuster</keyname><forenames>Raphael</forenames></author></authors><title>Approximating the Diameter of Planar Graphs in Near Linear Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a $(1+\epsilon)$-approximation algorithm running in
$O(f(\epsilon)\cdot n \log^4 n)$ time for finding the diameter of an undirected
planar graph with non-negative edge lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1117</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1117</id><created>2011-12-05</created><updated>2012-08-17</updated><authors><author><keyname>Khabbaz</keyname><forenames>Mohammad</forenames></author><author><keyname>Bhagat</keyname><forenames>Smriti</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>Finding Heavy Paths in Graphs: A Rank Join Approach</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs have been commonly used to model many applications. A natural problem
which abstracts applications such as itinerary planning, playlist
recommendation, and flow analysis in information networks is that of finding
the heaviest path(s) in a graph. More precisely, we can model these
applications as a graph with non-negative edge weights, along with a monotone
function such as sum, which aggregates edge weights into a path weight,
capturing some notion of quality. We are then interested in finding the top-k
heaviest simple paths, i.e., the $k$ simple (cycle-free) paths with the
greatest weight, whose length equals a given parameter $\ell$. We call this the
\emph{Heavy Path Problem} (HPP). It is easy to show that the problem is
NP-Hard.
  In this work, we develop a practical approach to solve the Heavy Path problem
by leveraging a strong connection with the well-known Rank Join paradigm. We
first present an algorithm by adapting the Rank Join algorithm. We identify its
limitations and develop a new exact algorithm called HeavyPath and a scalable
heuristic algorithm. We conduct a comprehensive set of experiments on three
real data sets and show that HeavyPath outperforms the baseline algorithms
significantly, with respect to both $\ell$ and $k$. Further, our heuristic
algorithm scales to longer lengths, finding paths that are empirically within
50% of the optimum solution or better under various settings, and takes only a
fraction of the running time compared to the exact algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1120</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1120</id><created>2011-12-05</created><authors><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Classification with Invariant Scattering Representations</title><categories>cs.CV math.FA stat.ML</categories><comments>6 pages, 2 figures; IVMSP Workshop, 2011 IEEE 10th</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scattering transform defines a signal representation which is invariant to
translations and Lipschitz continuous relatively to deformations. It is
implemented with a non-linear convolution network that iterates over wavelet
and modulus operators. Lipschitz continuity locally linearizes deformations.
Complex classes of signals and textures can be modeled with low-dimensional
affine spaces, computed with a PCA in the scattering domain. Classification is
performed with a penalized model selection. State of the art results are
obtained for handwritten digit recognition over small training sets, and for
texture classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1124</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1124</id><created>2011-12-05</created><updated>2014-02-03</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Minimum Convex Partitions and Maximum Empty Polytopes</title><categories>cs.CG</categories><comments>16 pages, 4 figures; revised write-up with some running times
  improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a set of $n$ points in $\mathbb{R}^d$. A Steiner convex partition
is a tiling of ${\rm conv}(S)$ with empty convex bodies. For every integer $d$,
we show that $S$ admits a Steiner convex partition with at most $\lceil
(n-1)/d\rceil$ tiles. This bound is the best possible for points in general
position in the plane, and it is best possible apart from constant factors in
every fixed dimension $d\geq 3$. We also give the first constant-factor
approximation algorithm for computing a minimum Steiner convex partition of a
planar point set in general position. Establishing a tight lower bound for the
maximum volume of a tile in a Steiner convex partition of any $n$ points in the
unit cube is equivalent to a famous problem of Danzer and Rogers. It is
conjectured that the volume of the largest tile is $\omega(1/n)$.
  Here we give a $(1-\varepsilon)$-approximation algorithm for computing the
maximum volume of an empty convex body amidst $n$ given points in the
$d$-dimensional unit box $[0,1]^d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1125</identifier>
 <datestamp>2011-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1125</id><created>2011-12-05</created><updated>2011-12-09</updated><authors><author><keyname>Little</keyname><forenames>Daniel Y.</forenames></author><author><keyname>Sommer</keyname><forenames>Friedrich T.</forenames></author></authors><title>Learning in embodied action-perception loops through exploration</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although exploratory behaviors are ubiquitous in the animal kingdom, their
computational underpinnings are still largely unknown. Behavioral Psychology
has identified learning as a primary drive underlying many exploratory
behaviors. Exploration is seen as a means for an animal to gather sensory data
useful for reducing its ignorance about the environment. While related problems
have been addressed in Data Mining and Reinforcement Learning, the
computational modeling of learning-driven exploration by embodied agents is
largely unrepresented.
  Here, we propose a computational theory for learning-driven exploration based
on the concept of missing information that allows an agent to identify
informative actions using Bayesian inference. We demonstrate that when
embodiment constraints are high, agents must actively coordinate their actions
to learn efficiently. Compared to earlier approaches, our exploration policy
yields more efficient learning across a range of worlds with diverse
structures. The improved learning in turn affords greater success in general
tasks including navigation and reward gathering. We conclude by discussing how
the proposed theory relates to previous information-theoretic objectives of
behavior, such as predictive information and the free energy principle, and how
it might contribute to a general theory of exploratory behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1133</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1133</id><created>2011-12-05</created><updated>2012-06-08</updated><authors><author><keyname>Modayil</keyname><forenames>Joseph</forenames></author><author><keyname>White</keyname><forenames>Adam</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>Multi-timescale Nexting in a Reinforcement Learning Robot</title><categories>cs.LG cs.RO</categories><comments>(11 pages, 5 figures, This version to appear in the Proceedings of
  the Conference on the Simulation of Adaptive Behavior, 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term &quot;nexting&quot; has been used by psychologists to refer to the propensity
of people and many other animals to continually predict what will happen next
in an immediate, local, and personal sense. The ability to &quot;next&quot; constitutes a
basic kind of awareness and knowledge of one's environment. In this paper we
present results with a robot that learns to next in real time, predicting
thousands of features of the world's state, including all sensory inputs, at
timescales from 0.1 to 8 seconds. This was achieved by treating each state
feature as a reward-like target and applying temporal-difference methods to
learn a corresponding value function with a discount rate corresponding to the
timescale. We show that two thousand predictions, each dependent on six
thousand state features, can be learned and updated online at better than 10Hz
on a laptop computer, using the standard TD(lambda) algorithm with linear
function approximation. We show that this approach is efficient enough to be
practical, with most of the learning complete within 30 minutes. We also show
that a single tile-coded feature representation suffices to accurately predict
many different signals at a significant range of timescales. Finally, we show
that the accuracy of our learned predictions compares favorably with the
optimal off-line solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1136</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1136</id><created>2011-12-05</created><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Umboh</keyname><forenames>Seeun</forenames></author><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Malec</keyname><forenames>David</forenames></author></authors><title>Secretary Problems with Convex Costs</title><categories>cs.DS</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online resource allocation problems where given a set of requests
our goal is to select a subset that maximizes a value minus cost type of
objective function. Requests are presented online in random order, and each
request possesses an adversarial value and an adversarial size. The online
algorithm must make an irrevocable accept/reject decision as soon as it sees
each request. The &quot;profit&quot; of a set of accepted requests is its total value
minus a convex cost function of its total size. This problem falls within the
framework of secretary problems. Unlike previous work in that area, one of the
main challenges we face is that the objective function can be positive or
negative and we must guard against accepting requests that look good early on
but cause the solution to have an arbitrarily large cost as more requests are
accepted. This requires designing new techniques.
  We study this problem under various feasibility constraints and present
online algorithms with competitive ratios only a constant factor worse than
those known in the absence of costs for the same feasibility constraints. We
also consider a multi-dimensional version of the problem that generalizes
multi-dimensional knapsack within a secretary framework. In the absence of any
feasibility constraints, we present an O(l) competitive algorithm where l is
the number of dimensions; this matches within constant factors the best known
ratio for multi-dimensional knapsack secretary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1139</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1139</id><created>2011-12-05</created><authors><author><keyname>Heiligman</keyname><forenames>Mark</forenames></author></authors><title>Quantum Verification of Minimum Spanning Tree</title><categories>quant-ph cs.DS</categories><comments>5 papges</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Previous studies has shown that for a weighted undirected graph having $n$
vertices and $m$ edges, a minimal weight spanning tree can be found with
$O^*(\sqrt{mn})$ calls to the weight oracle. The present note shows that a
given spanning tree can be verified to be a minimal weight spanning tree with
only $O(n\bigr)$ calls to the weight oracle and $O(n+\sqrt{m}\log n)$ total
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1141</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1141</id><created>2011-12-05</created><authors><author><keyname>Garg</keyname><forenames>Nitin</forenames></author><author><keyname>Zhu</keyname><forenames>Ed</forenames></author><author><keyname>Botelho</keyname><forenames>Fabiano C.</forenames></author></authors><title>Highly-Concurrent Doubly-Linked Lists</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As file systems are increasingly being deployed on ever larger systems with
many cores and multi-gigabytes of memory, scaling the internal data structures
of file systems has taken greater importance and urgency. A doubly-linked list
is a simple and very commonly used data structure in file systems but it is not
very friendly to multi-threaded use. While special cases of lists, such as
queues and stacks, have lock-free versions that scale reasonably well, the
general form of a doubly-linked list offers no such solution. Using a mutex to
serialize all operations remains the de-facto method of maintaining a doubly
linked list. This severely limits the scalability of the list and developers
must resort to ad-hoc workarounds that involve using multiple smaller lists
(with individual locks) and deal with the resulting complexity of the system.
In this paper, we present an approach to building highly concurrent data
structures, with special focus on the implementation of highly concurrent
doubly-linked lists. Dubbed &quot;advanced doubly-linked list&quot; or &quot;adlist&quot; for
short, our list allows iteration in any direction, and insert/delete operations
over non-overlapping nodes to execute in parallel. Operations with common nodes
get serialized so as to always present a locally consistent view to the
callers. An adlist node needs an additional 8 bytes of space for keeping
synchronization information. The Data Domain File System makes extensive use of
adlists which has allowed for significant scaling of the system without
sacrificing simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1143</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1143</id><created>2011-12-05</created><authors><author><keyname>Ishii</keyname><forenames>Akira</forenames></author><author><keyname>Arakaki</keyname><forenames>Hisashi</forenames></author><author><keyname>Matsuda</keyname><forenames>Naoya</forenames></author><author><keyname>Umemura</keyname><forenames>Sanae</forenames></author><author><keyname>Urushidani</keyname><forenames>Tamiko</forenames></author><author><keyname>Yamagata</keyname><forenames>Naoya</forenames></author><author><keyname>Yoshda</keyname><forenames>Narihiko</forenames></author></authors><title>Mathematical model for hit phenomena as stochastic process of
  interactions of human interactions</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages, 16 figures, submitted to New Journal of Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical model for hit phenomena in entertainments in the society is
presented as stochastic process of interactions of human dynamics. The model
use only the time distribution of advertisement budget as input and the words
of mouth (WOM) as posting in the social network system is used as the data to
compare with the calculated results. The unit of time is daily. The WOM
distribution in time is found to be very close to the residue distribution in
time. The calculations for Japanese motion picture market due to the
mathematical model agree very well with the actual residue distribution in
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1144</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1144</id><created>2011-12-05</created><updated>2011-12-07</updated><authors><author><keyname>Wu</keyname><forenames>Meng</forenames></author><author><keyname>Deng</keyname><forenames>Jiansong</forenames></author><author><keyname>Chen</keyname><forenames>Falai</forenames></author></authors><title>The Dimension of Spline Spaces with Highest Order Smoothness over
  Hierarchical T-meshes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the dimension of spline spaces with highest order
smoothness over hierarchical T-meshes over certain type of hierarchical
T-meshes. The major step is to set up a bijection between the spline space with
highest order smoothness over a hierarchical T-mesh and a univariate spline
space whose definition depends on the l-edges of the extended T-mesh. We
decompose the univariate spline space into direct sums in the sense of
isomorphism using the theory of the short exact sequence in homological
algebra. According to the decomposition of the univariate spline space, the
dimension formula of the spline space with highest order smoothness over
certain type of hierarchical T-mesh is presented. A set of basis functions of
the spline space is also constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1147</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1147</id><created>2011-12-05</created><authors><author><keyname>Chiesa</keyname><forenames>Alessandro</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Knightian Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study single-good auctions in a setting where each player knows his own
valuation only within a constant multiplicative factor \delta{} in (0,1), and
the mechanism designer knows \delta. The classical notions of implementation in
dominant strategies and implementation in undominated strategies are naturally
extended to this setting, but their power is vastly different.
  On the negative side, we prove that no dominant-strategy mechanism can
guarantee social welfare that is significantly better than that achievable by
assigning the good to a random player.
  On the positive side, we provide tight upper and lower bounds for the
fraction of the maximum social welfare achievable in undominated strategies,
whether deterministically or probabilistically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1156</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1156</id><created>2011-12-05</created><authors><author><keyname>Vafopoulos</keyname><forenames>Michalis</forenames></author></authors><title>Looking for grass-root sources of systemic risk: the case of
  &quot;cheques-as-collateral&quot; network</title><categories>q-fin.RM cs.SI q-fin.CP</categories><msc-class>91G40, 05C82, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global financial system has become highly connected and complex. Has been
proven in practice that existing models, measures and reports of financial risk
fail to capture some important systemic dimensions. Only lately, advisory
boards have been established in high level and regulations are directly
targeted to systemic risk. In the same direction, a growing number of
researchers employ network analysis to model systemic risk in financial
networks. Current approaches are concentrated on interbank payment network
flows in national and international level. This work builds on existing
approaches to account for systemic risk assessment in micro level.
Particularly, we introduce the analysis of intra-bank financial risk
interconnections, by examining the real case of &quot;cheques-as-collateral&quot; network
for a major Greek bank. Our model offers useful information about the negative
spillovers of disruption to a financial entity in a bank's lending network and
could complement existing credit scoring models that account only for
idiosyncratic customer's financial profile. Most importantly, the proposed
methodology can be employed in many segments of the entire financial system,
providing a useful tool in the hands of regulatory authorities in assessing
more accurate estimates of systemic risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1158</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1158</id><created>2011-12-05</created><authors><author><keyname>Fang</keyname><forenames>Xi</forenames></author><author><keyname>Yang</keyname><forenames>Dejun</forenames></author><author><keyname>Xue</keyname><forenames>Guoliang</forenames></author></authors><title>Wireless Communications and Networking Technologies for Smart Grid:
  Paradigms and Challenges</title><categories>cs.NI</categories><comments>7 pages, 6 figures, keywords: Smart grid, wireless communications,
  wireless networking, smart home, microgrid, vehicle-to-grid, paradigm,
  challenge, vision</comments><msc-class>94-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart grid, regarded as the next generation power grid, uses two-way flows of
electricity and information to create a widely distributed automated energy
delivery network. In this work we present our vision on smart grid from the
perspective of wireless communications and networking technologies. We present
wireless communication and networking paradigms for four typical scenarios in
the future smart grid and also point out the research challenges of the
wireless communication and networking technologies used in smart grid
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1178</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1178</id><created>2011-12-06</created><updated>2013-06-21</updated><authors><author><keyname>Halabian</keyname><forenames>Hassan</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author><author><keyname>Viniotis</keyname><forenames>Yannis</forenames></author><author><keyname>Lung</keyname><forenames>Chung-Horng</forenames></author></authors><title>Optimal Server Assignment in Multi-Server Queueing Systems with Random
  Connectivities</title><categories>math.OC cs.DS</categories><comments>41 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of assigning $K$ identical servers to a set of $N$
parallel queues in a time-slotted queueing system. The connectivity of each
queue to each server is randomly changing with time; each server can serve at
most one queue and each queue can be served by at most one server during each
time slot. Such a queueing model has been used in addressing resource
allocation problems in wireless networks. It has been previously proven that
Maximum Weighted Matching (MWM) is a throughput-optimal server assignment
policy for such a queueing system. In this paper, we prove that for a system
with i.i.d. Bernoulli packet arrivals and connectivities, MWM minimizes, in
stochastic ordering sense, a broad range of cost functions of the queue lengths
such as total queue occupancy (which implies minimization of average queueing
delays). Then, we extend the model by considering imperfect services where it
is assumed that the service of a scheduled packet fails randomly with a certain
probability. We prove that the same policy is still optimal for the extended
model. We finally show that the results are still valid for more general
connectivity and arrival processes which follow conditional permutation
invariant distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1181</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1181</id><created>2011-12-06</created><authors><author><keyname>Halabian</keyname><forenames>Hassan</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author><author><keyname>Lung</keyname><forenames>Chung-Horng</forenames></author></authors><title>On the Stability Region of Multi-Queue Multi-Server Queueing Systems
  with Stationary Channel Distribution</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, 2 figures, Proc. ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the stability region of multi-queue
multi-server (MQMS) queueing systems with stationary channel and packet arrival
processes. Toward this, the necessary and sufficient conditions for the
stability of the system are derived under general arrival processes with finite
first and second moments. We show that when the arrival processes are
stationary, the stability region form is a polytope for which we explicitly
find the coefficients of the linear inequalities which characterize the
stability region polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1185</identifier>
 <datestamp>2012-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1185</id><created>2011-12-06</created><updated>2012-02-09</updated><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>Rationality and Escalation in Infinite Extensive Games</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1004.5257,
  arXiv:0904.3528, and arXiv:0912.1746</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this of this paper is to study infinite games and to prove
formally some properties in this framework. As a consequence we show that the
behavior (the madness) of people which leads to speculative crashes or
escalation can be fully rational. Indeed it proceeds from the statement that
resources are infinite. The reasoning is based on the concept of coinduction
conceived by computer scientists to model infinite computations and used by
economic agents unknowingly. When used consciously, this concept is not as
simple as induction and we could paraphrase Newton: &quot;Modeling the madness of
people is more difficult than modeling the motion of planets&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1186</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1186</id><created>2011-12-06</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM</affiliation></author></authors><title>The Determinacy of Context-Free Games</title><categories>cs.GT cs.LO math.LO</categories><comments>To appear in the Proceedings of the 29 th International Symposium on
  Theoretical Aspects of Computer Science, STACS 2012</comments><proxy>ccsd</proxy><doi>10.4230/LIPIcs.STACS.2012.555</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the determinacy of Gale-Stewart games whose winning sets are
accepted by real-time 1-counter B\&quot;uchi automata is equivalent to the
determinacy of (effective) analytic Gale-Stewart games which is known to be a
large cardinal assumption. We show also that the determinacy of Wadge games
between two players in charge of omega-languages accepted by 1-counter B\&quot;uchi
automata is equivalent to the (effective) analytic Wadge determinacy. Using
some results of set theory we prove that one can effectively construct a
1-counter B\&quot;uchi automaton A and a B\&quot;uchi automaton B such that: (1) There
exists a model of ZFC in which Player 2 has a winning strategy in the Wadge
game W(L(A), L(B)); (2) There exists a model of ZFC in which the Wadge game
W(L(A), L(B)) is not determined. Moreover these are the only two possibilities,
i.e. there are no models of ZFC in which Player 1 has a winning strategy in the
Wadge game W(L(A), L(B)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1187</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1187</id><created>2011-12-06</created><authors><author><keyname>Sabater</keyname><forenames>Neus</forenames><affiliation>CMLA</affiliation></author><author><keyname>Almansa</keyname><forenames>Andr&#xe9;s</forenames><affiliation>LTCI</affiliation></author><author><keyname>Morel</keyname><forenames>Jean-Michel</forenames><affiliation>CMLA</affiliation></author></authors><title>Meaningful Matches in Stereovision</title><categories>cs.CV</categories><comments>IEEE Transactions on Pattern Analysis and Machine Intelligence 99,
  Preprints (2011) 1-12</comments><proxy>ccsd</proxy><doi>10.1109/TPAMI.2011.207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a statistical method to decide whether two blocks in a
pair of of images match reliably. The method ensures that the selected block
matches are unlikely to have occurred &quot;just by chance.&quot; The new approach is
based on the definition of a simple but faithful statistical &quot;background model&quot;
for image blocks learned from the image itself. A theorem guarantees that under
this model not more than a fixed number of wrong matches occurs (on average)
for the whole image. This fixed number (the number of false alarms) is the only
method parameter. Furthermore, the number of false alarms associated with each
match measures its reliability. This &quot;a contrario&quot; block-matching method,
however, cannot rule out false matches due to the presence of periodic objects
in the images. But it is successfully complemented by a parameterless
&quot;self-similarity threshold.&quot; Experimental evidence shows that the proposed
method also detects occlusions and incoherent motions due to vehicles and
pedestrians in non simultaneous stereo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1200</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1200</id><created>2011-12-06</created><authors><author><keyname>Chau</keyname><forenames>Duc Phu</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Bremond</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Thonnat</keyname><forenames>Monique</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>A multi-feature tracking algorithm enabling adaptation to context
  variations</title><categories>cs.CV</categories><comments>The International Conference on Imaging for Crime Detection and
  Prevention (ICDP) (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this paper a tracking algorithm which is able to adapt itself
to different scene contexts. A feature pool is used to compute the matching
score between two detected objects. This feature pool includes 2D, 3D
displacement distances, 2D sizes, color histogram, histogram of oriented
gradient (HOG), color covariance and dominant color. An offline learning
process is proposed to search for useful features and to estimate their weights
for each context. In the online tracking process, a temporal window is defined
to establish the links between the detected objects. This enables to find the
object trajectories even if the objects are misdetected in some frames. A
trajectory filter is proposed to remove noisy trajectories. Experimentation on
different contexts is shown. The proposed tracker has been tested in videos
belonging to three public datasets and to the Caretaker European project. The
experimental results prove the effect of the proposed feature weight learning,
and the robustness of the proposed tracker compared to some methods in the
state of the art. The contributions of our approach over the state of the art
trackers are: (i) a robust tracking algorithm based on a feature pool, (ii) a
supervised learning scheme to learn feature weights for each context, (iii) a
new method to quantify the reliability of HOG descriptor, (iv) a combination of
color covariance and dominant color features with spatial pyramid distance to
manage the case of object occlusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1201</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1201</id><created>2011-12-06</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Fang</keyname><forenames>Xiaole</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Wang</keyname><forenames>Qianxue</forenames></author></authors><title>Evaluating Quality of Chaotic Pseudo-Random Generators: Application to
  Information Hiding</title><categories>cs.CR</categories><comments>13 pages, 15 figures, IJAS, International Journal On Advances in
  Security, 4(1-2):118--130, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guaranteeing the security of information transmitted through the Internet,
against passive or active attacks, is a major concern. The discovery of new
pseudo-random number generators with a strong level of security is a field of
research in full expansion, due to the fact that numerous cryptosystems and
data hiding schemes are directly dependent on the quality of these generators.
At the conference Internet`09, we described a generator based on chaotic
iterations which behaves chaotically as defined by Devaney. In this paper which
is an extension of the work presented at the conference Internet`10, the
proposal is to improve the speed, the security, and the evaluation of this
generator, to make its use more relevant in the Internet security context. In
order to do so, a comparative study between various generators is carried out
and statistical results are improved. Finally, an application in the
information hiding framework is presented with details, to give an illustrative
example of the use of such a generator in the Internet security field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1210</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1210</id><created>2011-12-06</created><authors><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author></authors><title>Efficient Computation of Distance Sketches in Distributed Networks</title><categories>cs.DS cs.DC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance computation is one of the most fundamental primitives used in
communication networks. The cost of effectively and accurately computing
pairwise network distances can become prohibitive in large-scale networks such
as the Internet and Peer-to-Peer (P2P) networks. To negotiate the rising need
for very efficient distance computation, approximation techniques for numerous
variants of this question have recently received significant attention in the
literature. The goal is to preprocess the graph and store a small amount of
information such that whenever a query for any pairwise distance is issued, the
distance can be well approximated (i.e., with small stretch) very quickly in an
online fashion. Specifically, the pre-processing (usually) involves storing a
small sketch with each node, such that at query time only the sketches of the
concerned nodes need to be looked up to compute the approximate distance. In
this paper, we present the first theoretical study of distance sketches derived
from distance oracles in a distributed network. We first present a fast
distributed algorithm for computing approximate distance sketches, based on a
distributed implementation of the distance oracle scheme of [Thorup-Zwick, JACM
2005]. We also show how to modify this basic construction to achieve different
tradeoffs between the number of pairs for which the distance estimate is
accurate and other parameters. These tradeoffs can then be combined to give an
efficient construction of small sketches with provable average-case as well as
worst-case performance. Our algorithms use only small-sized messages and hence
are suitable for bandwidth-constrained networks, and can be used in various
networking applications such as topology discovery and construction, token
management, load balancing, monitoring overlays, and several other problems in
distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1217</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1217</id><created>2011-12-06</created><authors><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author><author><keyname>Schuler</keyname><forenames>Christian J.</forenames></author></authors><title>Entropy Search for Information-Efficient Global Optimization</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contemporary global optimization algorithms are based on local measures of
utility, rather than a probability measure over location and value of the
optimum. They thus attempt to collect low function values, not to learn about
the optimum. The reason for the absence of probabilistic global optimizers is
that the corresponding inference problem is intractable in several ways. This
paper develops desiderata for probabilistic optimization algorithms, then
presents a concrete algorithm which addresses each of the computational
intractabilities with a sequence of approximations and explicitly adresses the
decision problem of maximizing information gain from each evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1220</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1220</id><created>2011-12-06</created><authors><author><keyname>Szell</keyname><forenames>Michael</forenames></author><author><keyname>Sinatra</keyname><forenames>Roberta</forenames></author><author><keyname>Petri</keyname><forenames>Giovanni</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Understanding mobility in a social petri dish</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 5 figures</comments><journal-ref>Scientific Reports 2, 457 (2012)</journal-ref><doi>10.1038/srep00457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the recent availability of large data sets on human movements, a full
understanding of the rules governing motion within social systems is still
missing, due to incomplete information on the socio-economic factors and to
often limited spatio-temporal resolutions. Here we study an entire society of
individuals, the players of an online-game, with complete information on their
movements in a network-shaped universe and on their social and economic
interactions. Such a &quot;socio-economic laboratory&quot; allows to unveil the intricate
interplay of spatial constraints, social and economic factors, and patterns of
mobility. We find that the motion of individuals is not only constrained by
physical distances, but also strongly shaped by the presence of socio-economic
areas. These regions can be recovered perfectly by community detection methods
solely based on the measured human dynamics. Moreover, we uncover that
long-term memory in the time-order of visited locations is the essential
ingredient for modeling the trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1224</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1224</id><created>2011-12-06</created><authors><author><keyname>Massaro</keyname><forenames>E.</forenames></author><author><keyname>Guazzini</keyname><forenames>A.</forenames></author><author><keyname>Bagnoli</keyname><forenames>F.</forenames></author><author><keyname>Li&#xf2;</keyname><forenames>P.</forenames></author></authors><title>Information dynamics algorithm for detecting communities in networks</title><categories>physics.soc-ph cs.SI</categories><comments>Submitted to &quot;Communication in Nonlinear Science and Numerical
  Simulation&quot;</comments><doi>10.1016/j.cnsns.2012.03.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of community detection is relevant in many scientific
disciplines, from social science to statistical physics. Given the impact of
community detection in many areas, such as psychology and social sciences, we
have addressed the issue of modifying existing well performing algorithms by
incorporating elements of the domain application fields, i.e. domain-inspired.
We have focused on a psychology and social network - inspired approach which
may be useful for further strengthening the link between social network studies
and mathematics of community detection. Here we introduce a community-detection
algorithm derived from the van Dongen's Markov Cluster algorithm (MCL) method
by considering networks' nodes as agents capable to take decisions. In this
framework we have introduced a memory factor to mimic a typical human behavior
such as the oblivion effect. The method is based on information diffusion and
it includes a non-linear processing phase. We test our method on two classical
community benchmark and on computer generated networks with known community
structure. Our approach has three important features: the capacity of detecting
overlapping communities, the capability of identifying communities from an
individual point of view and the fine tuning the community detectability with
respect to prior knowledge of the data. Finally we discuss how to use a Shannon
entropy measure for parameter estimation in complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1229</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1229</id><created>2011-12-06</created><updated>2012-07-05</updated><authors><author><keyname>Iannello</keyname><forenames>Fabio</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author></authors><title>On the Optimal Scheduling of Independent, Symmetric and Time-Sensitive
  Tasks</title><categories>math.OC cs.DS cs.SY</categories><comments>Submitted for possible publication to IEEE Transactions on Automatic
  Control, Jul. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a discrete-time system in which a centralized controller (CC) is
tasked with assigning at each time interval (or slot) K resources (or servers)
to K out of M&gt;=K nodes. When assigned a server, a node can execute a task. The
tasks are independently generated at each node by stochastically symmetric and
memoryless random processes and stored in a finite-capacity task queue.
Moreover, they are time-sensitive in the sense that within each slot there is a
non-zero probability that a task expires before being scheduled. The scheduling
problem is tackled with the aim of maximizing the number of tasks completed
over time (or the task-throughput) under the assumption that the CC has no
direct access to the state of the task queues. The scheduling decisions at the
CC are based on the outcomes of previous scheduling commands, and on the known
statistical properties of the task generation and expiration processes. Based
on a Markovian modeling of the task generation and expiration processes, the CC
scheduling problem is formulated as a partially observable Markov decision
process (POMDP) that can be cast into the framework of restless multi-armed
bandit (RMAB) problems. When the task queues are of capacity one, the
optimality of a myopic (or greedy) policy is proved. It is also demonstrated
that the MP coincides with the Whittle index policy. For task queues of
arbitrary capacity instead, the myopic policy is generally suboptimal, and its
performance is compared with an upper bound obtained through a relaxation of
the original problem. Overall, the settings in this paper provide a rare
example where a RMAB problem can be explicitly solved, and in which the Whittle
index policy is proved to be optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1238</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1238</id><created>2011-12-06</created><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Manganiello</keyname><forenames>Felice</forenames></author><author><keyname>Braun</keyname><forenames>Michael</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author></authors><title>Cyclic Orbit Codes</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, volume 59, number 11,
  pages 7386-7404, 2013</journal-ref><doi>10.1109/TIT.2013.2274266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network coding a constant dimension code consists of a set of
k-dimensional subspaces of F_q^n. Orbit codes are constant dimension codes
which are defined as orbits of a subgroup of the general linear group, acting
on the set of all subspaces of F_q^n. If the acting group is cyclic, the
corresponding orbit codes are called cyclic orbit codes. In this paper we give
a classification of cyclic orbit codes and propose a decoding procedure for a
particular subclass of cyclic orbit codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1245</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1245</id><created>2011-12-06</created><authors><author><keyname>Lipsky</keyname><forenames>David</forenames></author><author><keyname>Skraba</keyname><forenames>Primoz</forenames></author><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>A spectral sequence for parallelized persistence</title><categories>cs.CG cs.DC math.AT</categories><comments>15 pages, 10 figures, submitted to the ACM Symposium on Computational
  Geometry</comments><msc-class>55-04, 55T99, 55U10</msc-class><acm-class>D.1.3; G.4; I.1.2; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We approach the problem of the computation of persistent homology for large
datasets by a divide-and-conquer strategy. Dividing the total space into
separate but overlapping components, we are able to limit the total memory
residency for any part of the computation, while not degrading the overall
complexity much. Locally computed persistence information is then merged from
the components and their intersections using a spectral sequence generalizing
the Mayer-Vietoris long exact sequence.
  We describe the Mayer-Vietoris spectral sequence and give details on how to
compute with it. This allows us to merge local homological data into the global
persistent homology. Furthermore, we detail how the classical topology
constructions inherent in the spectral sequence adapt to a persistence
perspective, as well as describe the techniques from computational commutative
algebra necessary for this extension.
  The resulting computational scheme suggests a parallelization scheme, and we
discuss the communication steps involved in this scheme. Furthermore, the
computational scheme can also serve as a guideline for which parts of the
boundary matrix manipulation need to co-exist in primary memory at any given
time allowing for stratified memory access in single-core computation. The
spectral sequence viewpoint also provides easy proofs of a homology nerve lemma
as well as a persistent homology nerve lemma. In addition, the algebraic tools
we develop to approch persistent homology provide a purely algebraic
formulation of kernel, image and cokernel persistence (D. Cohen-Steiner, H.
Edelsbrunner, J. Harer, and D. Morozov. Persistent homology for kernels,
images, and cokernels. In Proceedings of the twentieth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 1011-1020. Society for Industrial and
Applied Mathematics, 2009.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1260</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1260</id><created>2011-12-06</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author></authors><title>Steganography: a class of secure and robust algorithms</title><categories>cs.CR</categories><comments>Published in The Computer Journal special issue about steganography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research work presents a new class of non-blind information hiding
algorithms that are stego-secure and robust. They are based on some finite
domains iterations having the Devaney's topological chaos property. Thanks to a
complete formalization of the approach we prove security against watermark-only
attacks of a large class of steganographic algorithms. Finally a complete study
of robustness is given in frequency DWT and DCT domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1271</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1271</id><created>2011-12-06</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author></authors><title>Performance Analysis of a Keyed Hash Function based on Discrete and
  Chaotic Proven Iterations</title><categories>cs.CR</categories><comments>Accepted the the Internet11 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of information transmitted through the Internet is an international
concern. This security is guaranteed by tools like hash functions. However, as
security flaws have been recently identified in the current standard in this
domain, new ways to hash digital media must be investigated. In this document
an original keyed hash function is evaluated. It is based on chaotic iterations
and thus possesses various topological properties as uniform repartition and
sensibility to its initial condition. These properties make our hash function
satisfy the requirements in this field. This claim is verified qualitatively
and experimentally in this research work, among other things by realizing
simulations of diffusion and confusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1294</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1294</id><created>2011-12-06</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>Additive schemes (splitting schemes) for some systems of evolutionary
  equations</title><categories>cs.NA</categories><msc-class>65N06, 65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the basis of additive schemes (splitting schemes) we construct efficient
numerical algorithms to solve approximately the initial-boundary value problems
for systems of time-dependent partial differential equations (PDEs). In many
applied problems the individual components of the vector of unknowns are
coupled together and then splitting schemes are applied in order to get a
simple problem for evaluating components at a new time level. Typically, the
additive operator-difference schemes for systems of evolutionary equations are
constructed for operators coupled in space. In this paper we investigate more
general problems where coupling of derivatives in time for components of the
solution vector takes place. Splitting schemes are developed using an additive
representation for both the primary operator of the problem and the operator at
the time derivative. Splitting schemes are based on a triangular two-component
representation of the operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1295</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1295</id><created>2011-12-06</created><updated>2012-08-19</updated><authors><author><keyname>Disanto</keyname><forenames>Filippo</forenames></author><author><keyname>Wiehe</keyname><forenames>Thomas</forenames></author></authors><title>Exact enumeration of cherries and pitchforks in ranked trees under the
  coalescent model</title><categories>math.CO cs.DM q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider exact enumerations and probabilistic properties of ranked trees
when generated under the random coalescent process. Using a new approach, based
on generating functions, we derive several statistics such as the exact
probability of finding k cherries in a ranked tree of fixed size n. We then
extend our method to consider also the number of pitchforks. We find a
recursive formula to calculate the joint and conditional probabilities of
cherries and pitch- forks when the size of the tree is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1308</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1308</id><created>2011-12-06</created><updated>2012-03-04</updated><authors><author><keyname>Bl&#xe4;ser</keyname><forenames>Markus</forenames></author><author><keyname>Coron</keyname><forenames>Jean-S&#xe9;bastien</forenames></author><author><keyname>Pospelov</keyname><forenames>Alexey</forenames></author></authors><title>Small Private Circuits</title><categories>cs.CR cs.CC</categories><comments>Withdrawing while fixing a flaw in the proof</comments><msc-class>94A60</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ishai, Sahai, and Wagner initiated in 2003 the theoretical study of securing
a circuit against an adversary who can probe its wires. They presented a
universal way of transforming an arbitrary boolean circuit of size s into a
circuit of size linear in s and quadratic in t, with perfect security against
an adversary who can read up to t wires of the circuit. We present a new method
for securing circuits against such an adversary with circuit size linear in s
and polylogarithmic in t, while meeting the original privacy requirements from
Ishai et al.
  Our solution works for arithmetic circuits over arbitrary fields of positive
characteristic. The improvement from quadratic to quasilinear complexity (in t)
comes from using the DFT instead of naive multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1313</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1313</id><created>2011-12-06</created><authors><author><keyname>Chiang</keyname><forenames>Chun-Ying</forenames></author><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Huang</keyname><forenames>Wei-Ting</forenames></author><author><keyname>Yeh</keyname><forenames>Hong-Gwa</forenames></author></authors><title>The Target Set Selection Problem on Cycle Permutation Graphs,
  Generalized Petersen Graphs and Torus Cordalis</title><categories>math.CO cs.DM cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a fundamental problem in the area of viral
marketing, called T{\scriptsize ARGET} S{\scriptsize ET} S{\scriptsize
ELECTION} problem.
  In a a viral marketing setting, social networks are modeled by graphs with
potential customers of a new product as vertices and friend relationships as
edges, where each vertex $v$ is assigned a threshold value $\theta(v)$. The
thresholds represent the different latent tendencies of customers (vertices) to
buy the new product when their friend (neighbors) do.
  Consider a repetitive process on social network $(G,\theta)$ where each
vertex $v$ is associated with two states, active and inactive, which indicate
whether $v$ is persuaded into buying the new product. Suppose we are given a
target set $S\subseteq V(G)$. Initially, all vertices in $G$ are inactive. At
time step 0, we choose all vertices in $S$ to become active.
  Then, at every time step $t&gt;0$, all vertices that were active in time step
$t-1$ remain active, and we activate any vertex $v$ if at least $\theta(v)$ of
its neighbors were active at time step $t-1$. The activation process terminates
when no more vertices can get activated. We are interested in the following
optimization problem, called T{\scriptsize ARGET} S{\scriptsize ET}
S{\scriptsize ELECTION}: Finding a target set $S$ of smallest possible size
that activates all vertices of $G$. There is an important and well-studied
threshold called strict majority threshold, where for every vertex $v$ in $G$
we have $\theta(v)=\lceil{(d(v) +1)/2}\rceil$ and $d(v)$ is the degree of $v$
in $G$. In this paper, we consider the T{\scriptsize ARGET} S{\scriptsize ET}
S{\scriptsize ELECTION} problem under strict majority thresholds and focus on
three popular regular network structures: cycle permutation graphs, generalized
Petersen graphs and torus cordalis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1314</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1314</id><created>2011-12-06</created><updated>2012-03-18</updated><authors><author><keyname>Yuan</keyname><forenames>Di</forenames></author><author><keyname>Angelakis</keyname><forenames>Vangelis</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Karipidis</keyname><forenames>Eleftherios</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>On Optimal Link Activation with Interference Cancellation in Wireless
  Networking</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Vehicular Technology on March 17,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental aspect in performance engineering of wireless networks is
optimizing the set of links that can be concurrently activated to meet given
signal-to-interference-and-noise ratio (SINR) thresholds. The solution of this
combinatorial problem is the key element in scheduling and cross-layer resource
management. Previous works on link activation assume single-user decoding
receivers, that treat interference in the same way as noise. In this paper, we
assume multiuser decoding receivers, which can cancel strongly interfering
signals. As a result, in contrast to classical spatial reuse, links being close
to each other are more likely to be active simultaneously. Our goal here is to
deliver a comprehensive theoretical and numerical study on optimal link
activation under this novel setup, in order to provide insight into the gains
from adopting interference cancellation. We therefore consider the optimal
problem setting of successive interference cancellation (SIC), as well as the
simpler, yet instructive, case of parallel interference cancellation (PIC). We
prove that both problems are NP-hard and develop compact integer linear
programming formulations that enable us to approach the global optimum
solutions. We provide an extensive numerical performance evaluation, indicating
that for low to medium SINR thresholds the improvement is quite substantial,
especially with SIC, whereas for high SINR thresholds the improvement
diminishes and both schemes perform equally well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1316</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1316</id><created>2011-12-06</created><authors><author><keyname>Polonowski</keyname><forenames>Emmanuel</forenames></author></authors><title>Generic Environments in Coq</title><categories>cs.LO</categories><comments>6 pages; The Third Coq Workshop (Coq'3), 2011</comments><report-no>TR--LACL--2011--3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a library which provides an abstract data type of environments,
as a functor parameterized by a module defining variables, and a function which
builds environments for such variables with any Type of type. Usual operations
over environments are defined, along with an extensive set of basic and more
advanced properties. Moreover, we give an implementation using lists satisfying
all the required properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1330</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1330</id><created>2011-12-06</created><authors><author><keyname>Gros</keyname><forenames>Claudius</forenames></author></authors><title>Emotional control - conditio sine qua non for advanced artificial
  intelligences?</title><categories>q-bio.NC cs.AI</categories><comments>Proceedings of PT-AI 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans dispose of two intertwined information processing pathways, cognitive
information processing via neural firing patterns and diffusive volume control
via neuromodulation. The cognitive information processing in the brain is
traditionally considered to be the prime neural correlate of human
intelligence, clinical studies indicate that human emotions intrinsically
correlate with the activation of the neuromodulatory system.
  We examine here the question: Why do humans dispose of the diffusive
emotional control system? Is this a coincidence, a caprice of nature, perhaps a
leftover of our genetic heritage, or a necessary aspect of any advanced
intelligence, being it biological or synthetic? We argue here that emotional
control is necessary to solve the motivational problem, viz the selection of
short-term utility functions, in the context of an environment where
information, computing power and time constitute scarce resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1333</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1333</id><created>2011-12-06</created><updated>2012-03-18</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author></authors><title>Reaching an Optimal Consensus: Dynamical Systems that Compute
  Intersections of Convex Sets</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, multi-agent systems minimizing a sum of objective functions,
where each component is only known to a particular node, is considered for
continuous-time dynamics with time-varying interconnection topologies. Assuming
that each node can observe a convex solution set of its optimization component,
and the intersection of all such sets is nonempty, the considered optimization
problem is converted to an intersection computation problem. By a simple
distributed control rule, the considered multi-agent system with
continuous-time dynamics achieves not only a consensus, but also an optimal
agreement within the optimal solution set of the overall optimization
objective. Directed and bidirectional communications are studied, respectively,
and connectivity conditions are given to ensure a global optimal consensus. In
this way, the corresponding intersection computation problem is solved by the
proposed decentralized continuous-time algorithm. We establish several
important properties of the distance functions with respect to the global
optimal solution set and a class of invariant sets with the help of convex and
non-smooth analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1335</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1335</id><created>2011-12-06</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author><author><keyname>Johansson</keyname><forenames>K. H.</forenames></author></authors><title>Connectivity and Set Tracking of Multi-agent Systems Guided by Multiple
  Moving Leaders</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate distributed multi-agent tracking of a convex
set specified by multiple moving leaders with unmeasurable velocities. Various
jointly-connected interaction topologies of the follower agents with
uncertainties are considered in the study of set tracking. Based on the
connectivity of the time-varying multi-agent system, necessary and sufficient
conditions are obtained for set input-to-state stability and set integral
input-to-state stability for a nonlinear neighbor-based coordination rule with
switching directed topologies. Conditions for asymptotic set tracking are also
proposed with respect to the polytope spanned by the leaders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1336</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1336</id><created>2011-12-06</created><updated>2015-08-11</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Consensus over Random Graph Processes: Network Borel-Cantelli Lemmas for
  Almost Sure Convergence</title><categories>cs.DC</categories><comments>IEEE Transactions on Information Theory, In Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed consensus computation over random graph processes is considered.
The random graph process is defined as a sequence of random variables which
take values from the set of all possible digraphs over the node set. At each
time step, every node updates its state based on a Bernoulli trial, independent
in time and among different nodes: either averaging among the neighbor set
generated by the random graph, or sticking with its current state.
Connectivity-independence and arc-independence are introduced to capture the
fundamental influence of the random graphs on the consensus convergence.
Necessary and/or sufficient conditions are presented on the success
probabilities of the Bernoulli trials for the network to reach a global almost
sure consensus, with some sharp threshold established revealing a consensus
zero-one law. Convergence rates are established by lower and upper bounds of
the $\epsilon$-computation time. We also generalize the concepts of
connectivity/arc independence to their analogues from the $*$-mixing point of
view, so that our results apply to a very wide class of graphical models,
including the majority of random graph models in the literature, e.g.,
Erd\H{o}s-R\'{e}nyi, gossiping, and Markovian random graphs. We show that under
$*$-mixing, our convergence analysis continues to hold and the corresponding
almost sure consensus conditions are established. Finally, we further
investigate almost sure finite-time convergence of random gossiping algorithms,
and prove that the Bernoulli trials play a key role in ensuring finite-time
convergence. These results add to the understanding of the interplay between
random graphs, random computations, and convergence probability for distributed
information processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1338</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1338</id><created>2011-12-06</created><updated>2012-03-18</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>The Role of Persistent Graphs in the Agreement Seeking of Social
  Networks</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the role persistent arcs play for a social network to
reach a global belief agreement under discrete-time or continuous-time
evolution. Each (directed) arc in the underlying communication graph is assumed
to be associated with a time-dependent weight function which describes the
strength of the information flow from one node to another. An arc is said to be
persistent if its weight function has infinite $\mathscr{L}_1$ or $\ell_1$ norm
for continuous-time or discrete-time belief evolutions, respectively. The graph
that consists of all persistent arcs is called the persistent graph of the
underlying network. Three necessary and sufficient conditions on agreement or
$\epsilon$-agreement are established, by which we prove that the persistent
graph fully determines the convergence to a common opinion in social networks.
It is shown how the convergence rates explicitly depend on the diameter of the
persistent graph. The results adds to the understanding of the fundamentals
behind global agreements, as it is only persistent arcs that contribute to the
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1344</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1344</id><created>2011-12-06</created><updated>2011-12-07</updated><authors><author><keyname>Lindbom</keyname><forenames>Lars</forenames><affiliation>Ericsson</affiliation></author><author><keyname>Love</keyname><forenames>Robert</forenames><affiliation>Motorola Mobility</affiliation></author><author><keyname>Krishnamurthy</keyname><forenames>Sandeep</forenames><affiliation>Motorola Mobility</affiliation></author><author><keyname>Yao</keyname><forenames>Chunhai</forenames><affiliation>Nokia Siemens Networks</affiliation></author><author><keyname>Miki</keyname><forenames>Nobuhiko</forenames><affiliation>NTT DOCOMO</affiliation></author><author><keyname>Chandrasekhar</keyname><forenames>Vikram</forenames><affiliation>Texas Instruments</affiliation></author></authors><title>Enhanced Inter-cell Interference Coordination for Heterogeneous Networks
  in LTE-Advanced: A Survey</title><categories>cs.IT math.IT</categories><comments>This is a working document describing the Enhanced Inter-cell
  Interference Coordination (E-ICIC) introduced in LTE-Advanced</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks (het-nets) - comprising of conventional macrocell base
stations overlaid with femtocells, picocells and wireless relays - offer
cellular operators burgeoning traffic demands through cell-splitting gains
obtained by bringing users closer to their access points. However, the often
random and unplanned location of these access points can cause severe near-far
problems, typically solved by coordinating base-station transmissions to
minimize interference. Towards this direction, the 3rd generation partnership
project Long Term Evolution-Advanced (3GPP-LTE or Rel-10) standard introduces
time-domain inter-cell interference coordination (ICIC) for facilitating a
seamless deployment of a het-net overlay. This article surveys the key features
encompassing the physical layer, network layer and back-hauling aspects of
time-domain ICIC in Rel-10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1360</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1360</id><created>2011-12-06</created><authors><author><keyname>Laus</keyname><forenames>Christian</forenames></author><author><keyname>Theis</keyname><forenames>Dirk Oliver</forenames></author></authors><title>On the satisfiability of random regular signed SAT formulas</title><categories>cs.DM math.CO</categories><comments>12 pages</comments><msc-class>68Q87</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular signed SAT is a variant of the well-known satisfiability problem in
which the variables can take values in a fixed set V \subset [0,1], and the
`literals' have the form &quot;x \le a&quot; or &quot;x \ge a&quot;.
  We answer some open question regarding random regular signed k-SAT formulas:
the probability that a random formula is satisfiable increases with |V|; there
is a constant upper bound on the ratio m/n of clauses m over variables n,
beyond which a random formula is asypmtotically almost never satisfied; for k=2
and V=[0,1], there is a phase transition at m/n=2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1368</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1368</id><created>2011-12-06</created><authors><author><keyname>Heikkil&#xe4;</keyname><forenames>Ville-Matias</forenames></author></authors><title>Discovering novel computer music techniques by exploring the space of
  short computer programs</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very short computer programs, sometimes consisting of as few as three
arithmetic operations in an infinite loop, can generate data that sounds like
music when output as raw PCM audio. The space of such programs was recently
explored by dozens of individuals within various on-line communities. This
paper discusses the programs resulting from this exploratory work and
highlights some rather unusual methods they use for synthesizing sound and
generating musical structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1374</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1374</id><created>2011-12-06</created><updated>2013-12-23</updated><authors><author><keyname>Balachandran</keyname><forenames>Shankar</forenames></author><author><keyname>Koroth</keyname><forenames>Sajin</forenames></author></authors><title>Sub-families of Baxter Permutations Based on Pattern Avoidance</title><categories>cs.DM math.CO</categories><msc-class>68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Baxter permutations are in bijection with floorplans that arise in chip
design. We study a family of floorplans that have certain geometric
restrictions. This naturally leads to studying a sub-family of Baxter
permutations. The sub-family of Baxter permutations are characterized by
pattern avoidance. We establish a bijection between the sub-family of
floorplans and a sub-family Baxter permutations based on the analogy between
decomposition of a floorplan into smaller blocks and \textit{block}
decomposition of permutations. Apart from the characterization, we also answer
combinatorial questions on these families. We give a rational generating
function for number of permutations in each class, an exponential lower bound
on growth rate of each class, and a quadratic time algorithm for deciding
membership in each class. Based on the recurrence relation describing the
class, we also give a polynomial time algorithm for enumeration. We finally
prove that Baxter permutations are closed under inverse based on an argument
inspired from the geometry of the corresponding mosaic floorplan.
Characterizing permutations instead of the corresponding floorplans can be
helpful in reasoning about the solution space and in designing efficient
algorithms for floorplanning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1390</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1390</id><created>2011-12-06</created><authors><author><keyname>Zhdanov</keyname><forenames>Fedor</forenames></author><author><keyname>Kalnishkan</keyname><forenames>Yuri</forenames></author></authors><title>An Identity for Kernel Ridge Regression</title><categories>cs.LG</categories><comments>35 pages; extended version of ALT 2010 paper (Proceedings of ALT
  2010, LNCS 6331, Springer, 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives an identity connecting the square loss of ridge regression
in on-line mode with the loss of the retrospectively best regressor. Some
corollaries about the properties of the cumulative loss of on-line ridge
regression are also obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1396</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1396</id><created>2011-12-06</created><authors><author><keyname>Balachandran</keyname><forenames>Shankar</forenames></author><author><keyname>Koroth</keyname><forenames>Sajin</forenames></author></authors><title>A Study on Hierarchical Floorplans of Order k</title><categories>cs.DM</categories><comments>33 pages, 13 figures</comments><msc-class>68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A floorplan is a rectangular dissection which describes the relative
placement of electronic modules on the chip. It is called a mosaic floorplan if
there are no empty rooms or cross junctions in the rectangular dissection. We
study a subclass of mosaic floorplans called hierarchical floorplans of order
$k$ (abbreviated HFO-${k}$). A floorplan is HFO-$k$ if it can be obtained by
starting with a single rectangle and recursively embedding mosaic floorplans of
at most $k$ rooms inside the rooms of intermediate floorplans. When $k=2$ this
is exactly the class of slicing floorplans as the only distinct floorplans with
two rooms are a room with a vertical slice and a room with a horizontal slice
respectdeively. And embedding such a room is equivalent to slicing the parent
room vertically/horizontally. In this paper we characterize permutations
corresponding to the Abe-labeling of HFO-$k$ floorplans and also give an
algorithm for identification of such permutations in linear time for any
particular $k$. We give a recurrence relation for exact number of HFO-5
floorplans with $n$ rooms which can be easily extended to any $k$ also. Based
on this recurrence we provide a polynomial time algorithm to generate the
number of HFO-$k$ floorplans with $n$ rooms. Considering its application in
VLSI design we also give moves on HFO-$k$ family of permutations for
combinatorial optimization using simulated annealing etc. We also explore some
interesting properties of Baxter permutations which have a bijective
correspondence with mosaic floorplans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1444</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1444</id><created>2011-12-06</created><updated>2013-02-12</updated><authors><author><keyname>Allamigeon</keyname><forenames>Xavier</forenames></author></authors><title>On the complexity of strongly connected components in directed
  hypergraphs</title><categories>cs.DS</categories><comments>v1: 32 pages, 7 figures; v2: revised version, 34 pages, 7 figures</comments><journal-ref>Algorithmica, Volume 69, Issue 2, June 2014, Pages 335-369</journal-ref><doi>10.1007/s00453-012-9729-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of some algorithmic problems on directed hypergraphs
and their strongly connected components (SCCs). The main contribution is an
almost linear time algorithm computing the terminal strongly connected
components (i.e. SCCs which do not reach any components but themselves).
&quot;Almost linear&quot; here means that the complexity of the algorithm is linear in
the size of the hypergraph up to a factor alpha(n), where alpha is the inverse
of Ackermann function, and n is the number of vertices. Our motivation to study
this problem arises from a recent application of directed hypergraphs to
computational tropical geometry.
  We also discuss the problem of computing all SCCs. We establish a superlinear
lower bound on the size of the transitive reduction of the reachability
relation in directed hypergraphs, showing that it is combinatorially more
complex than in directed graphs. Besides, we prove a linear time reduction from
the well-studied problem of finding all minimal sets among a given family to
the problem of computing the SCCs. Only subquadratic time algorithms are known
for the former problem. These results strongly suggest that the problem of
computing the SCCs is harder in directed hypergraphs than in directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1473</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1473</id><created>2011-12-07</created><authors><author><keyname>Mostafa</keyname><forenames>Sheikh Shanawaz</forenames></author><author><keyname>Reza</keyname><forenames>Khondker Jahid</forenames></author><author><keyname>Amin</keyname><forenames>Md. Ziaul</forenames></author><author><keyname>Ahmad</keyname><forenames>Mohiuddin</forenames></author></authors><title>Intelligent Paging Strategy for Multi-Carrier CDMA System</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Science Issues, Vol. 8, Issue 6,
  No 1, pp.254-260, November 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subscriber satisfaction and maximum radio resource utilization are the
pivotal criteria in communication system design. In multi-Carrier CDMA system,
different paging algorithms are used for locating user within the shortest
possible time and best possible utilization of radio resources. Different
paging algorithms underscored different techniques based on the different
purposes. However, low servicing time of sequential search and better
utilization of radio resources of concurrent search can be utilized
simultaneously by swapping of the algorithms. In this paper, intelligent
mechanism has been developed for dynamic algorithm assignment basing on
time-varying traffic demand, which is predicted by radial basis neural network;
and its performance has been analyzed are based on prediction efficiency of
different types of data. High prediction efficiency is observed with a good
correlation coefficient (0.99) and subsequently better performance is achieved
by dynamic paging algorithm assignment. This claim is substantiated by the
result of proposed intelligent paging strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1480</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1480</id><created>2011-12-07</created><authors><author><keyname>Bao</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Chen</keyname><forenames>Ming</forenames></author></authors><title>Multiuser Cellular Network</title><categories>cs.NI</categories><comments>23 pages essay for MCM2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern radio communication is faced with a problem about how to distribute
restricted frequency to users in a certain space. Since our task is to minimize
the number of repeaters, a natural idea is enlarging coverage area. However,
coverage has restrictions. First, service area has to be divided economically
as repeater's coverage is limited. In this paper, our fundamental method is to
adopt seamless cellular network division. Second, underlying physics content in
frequency distribution problem is interference between two close frequencies.
Consequently, we choose a proper frequency width of 0.1MHz and a relevantly
reliable setting to apply one frequency several times.
  We make a few general assumptions to simplify real situation. For instance,
immobile users yield to homogenous distribution; repeaters can receive and
transmit information in any given frequency in duplex operation; coverage is
mainly decided by antenna height.
  Two models are built up to solve 1000 users and 10000 users situations
respectively. In order to utilize restricted frequency and PL code, three
stratified terms - &quot;cell&quot;, &quot;cluster&quot;, &quot;group&quot; - are introduced to describe the
models in detail. Under our analysis, 91 repeaters for 1000 users and 469
repeaters for 10000 users are viable results.
  Next, to test stability and sensitivity of models, we give total
consideration to the variation of sum of users, antenna height, and frequency
width and service radius. Evaluation about models is offered qualitatively.
Finally, two practical cases are put forward to gain a partial knowledge of
mountainous area. The brief method in dealing with mountains is classified
discussion in two ideal conditions. It may provide some constructive
suggestions to avoid shortcomings or take proper measures in similar locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1484</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1484</id><created>2011-12-07</created><authors><author><keyname>Panda</keyname><forenames>S. S.</forenames></author><author><keyname>Prasad</keyname><forenames>M. S. R. S</forenames></author><author><keyname>Jena</keyname><forenames>G.</forenames></author></authors><title>POCS Based Super-Resolution Image Reconstruction Using an Adaptive
  Regularization Parameter</title><categories>cs.CV</categories><comments>4 pages,2 fig,2 tables,Published in IJCSI International Journal of
  Computer Science Issues, Vol. 8, Issue 5, No 2, September 2011 ISSN (Online):
  1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 2, September 2011 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crucial information barely visible to the human eye is often embedded in a
series of low-resolution images taken of the same scene. Super-resolution
enables the extraction of this information by reconstructing a single image, at
a high resolution than is present in any of the individual images. This is
particularly useful in forensic imaging, where the extraction of minute details
in an image can help to solve a crime. Super-resolution image restoration has
been one of the most important research areas in recent years which goals to
obtain a high resolution (HR) image from several low resolutions (LR) blurred,
noisy, under sampled and displaced images. Relation of the HR image and LR
images can be modeled by a linear system using a transformation matrix and
additive noise. However, a unique solution may not be available because of the
singularity of transformation matrix. To overcome this problem, POCS method has
been used. However, their performance is not good because the effect of noise
energy has been ignored. In this paper, we propose an adaptive regularization
approach based on the fact that the regularization parameter should be a linear
function of noise variance. The performance of the proposed approach has been
tested on several images and the obtained results demonstrate the superiority
of our approach compared with existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1489</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1489</id><created>2011-12-07</created><authors><author><keyname>Chen</keyname><forenames>Wan-Li</forenames></author></authors><title>Multi-granular Perspectives on Covering</title><categories>cs.AI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering model provides a general framework for granular computing in that
overlapping among granules are almost indispensable. For any given covering,
both intersection and union of covering blocks containing an element are
exploited as granules to form granular worlds at different abstraction levels,
respectively, and transformations among these different granular worlds are
also discussed. As an application of the presented multi-granular perspective
on covering, relational interpretation and axiomization of four types of
covering based rough upper approximation operators are investigated, which can
be dually applied to lower ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1496</identifier>
 <datestamp>2012-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1496</id><created>2011-12-07</created><updated>2012-08-07</updated><authors><author><keyname>Zhang</keyname><forenames>Kaihua</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Song</keyname><forenames>Huihui</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Re-initialization Free Level Set Evolution via Reaction Diffusion</title><categories>cs.CV</categories><comments>IEEE Trans. on Image Processing, to appear</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a novel reaction-diffusion (RD) method for implicit
active contours, which is completely free of the costly re-initialization
procedure in level set evolution (LSE). A diffusion term is introduced into
LSE, resulting in a RD-LSE equation, to which a piecewise constant solution can
be derived. In order to have a stable numerical solution of the RD based LSE,
we propose a two-step splitting method (TSSM) to iteratively solve the RD-LSE
equation: first iterating the LSE equation, and then solving the diffusion
equation. The second step regularizes the level set function obtained in the
first step to ensure stability, and thus the complex and costly
re-initialization procedure is completely eliminated from LSE. By successfully
applying diffusion to LSE, the RD-LSE model is stable by means of the simple
finite difference method, which is very easy to implement. The proposed RD
method can be generalized to solve the LSE for both variational level set
method and PDE-based level set method. The RD-LSE method shows very good
performance on boundary anti-leakage, and it can be readily extended to high
dimensional level set method. The extensive and promising experimental results
on synthetic and real images validate the effectiveness of the proposed RD-LSE
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1497</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1497</id><created>2011-12-07</created><updated>2012-02-05</updated><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author></authors><title>An Achievable Region for a General Multi-terminal Network and the
  corresponding Chain Graph Representation</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1107.4705</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random coding, along with various standard techniques such as coded
time-sharing, rate-splitting, superposition coding, and binning, are
traditionally used in obtaining achievable rate regions for multi-terminal
networks. The error analysis of such an achievable scheme relies heavily on the
properties of strongly joint typical sequences and on bounds of the cardinality
of typical sets. In this work, we obtain an achievable rate region for a
general (i.e. an arbitrary set of messages shared amongst encoding nodes, which
transmit to arbitrary decoding nodes) memoryless, single-hop, multi-terminal
network without feedback or cooperation by introducing a general framework and
notation, and carefully generalizing the derivation of the error analysis. We
show that this general inner bound may be obtained from a chain graph
representation of the encoding operations. This graph representation captures
the statistical relationship among codewords and allows one to readily obtain
the rate bounds that define the achievable rate region. The proposed graph
representation naturally leads to the derivation of all the achievable schemes
that can be generated by combining classic random coding techniques for any
memoryless network used without feedback or cooperation. We also re-derive a
few achievable regions for classic multi-terminal networks, such as the
multi-access channel, the broadcast channel, and the interference channel, to
show how this new representation allows one to quickly consider the possible
choices of encoding/decoding strategies for any given network and the
distribution of messages among the encoders and decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1517</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1517</id><created>2011-12-07</created><updated>2014-04-14</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>He</keyname><forenames>Feidun</forenames></author><author><keyname>Dong</keyname><forenames>Hongbin</forenames></author></authors><title>Pure Strategy or Mixed Strategy?</title><categories>cs.NE</categories><doi>10.1007/978-3-642-29124-1_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed strategy EAs aim to integrate several mutation operators into a single
algorithm. However few theoretical analysis has been made to answer the
question whether and when the performance of mixed strategy EAs is better than
that of pure strategy EAs. In theory, the performance of EAs can be measured by
asymptotic convergence rate and asymptotic hitting time. In this paper, it is
proven that given a mixed strategy (1+1) EAs consisting of several mutation
operators, its performance (asymptotic convergence rate and asymptotic hitting
time)is not worse than that of the worst pure strategy (1+1) EA using one
mutation operator; if these mutation operators are mutually complementary, then
it is possible to design a mixed strategy (1+1) EA whose performance is better
than that of any pure strategy (1+1) EA using one mutation operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1520</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1520</id><created>2011-12-07</created><authors><author><keyname>Rajasekharan</keyname><forenames>Jayaprakash</forenames></author><author><keyname>Eriksson</keyname><forenames>Jan</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Cooperative Game-Theoretic Approach to Spectrum Sharing in Cognitive
  Radios</title><categories>cs.GT cs.IT cs.NI math.IT</categories><comments>11 pages, 9 figures, 6 tables, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel framework for normative modeling of the spectrum
sensing and sharing problem in cognitive radios (CRs) as a transferable utility
(TU) cooperative game is proposed. Secondary users (SUs) jointly sense the
spectrum and cooperatively detect the primary user (PU) activity for
identifying and accessing unoccupied spectrum bands. The games are designed to
be balanced and super-additive so that resource allocation is possible and
provides SUs with an incentive to cooperate and form the grand coalition. The
characteristic function of the game is derived based on the worths of SUs,
calculated according to the amount of work done for the coalition in terms of
reduction in uncertainty about PU activity. According to her worth in the
coalition, each SU gets a pay-off that is computed using various one-point
solutions such as Shapley value, \tau-value and Nucleolus. Depending upon their
data rate requirements for transmission, SUs use the earned pay-off to bid for
idle channels through a socially optimal Vickrey-Clarke-Groves (VCG) auction
mechanism. Simulation results show that, in comparison with other resource
allocation models, the proposed cooperative game-theoretic model provides the
best balance between fairness, cooperation and performance in terms of data
rates achieved by each SU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1528</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1528</id><created>2011-12-07</created><authors><author><keyname>Yamagishi</keyname><forenames>Michel Eduardo Beleza</forenames></author><author><keyname>Herai</keyname><forenames>Roberto H.</forenames></author></authors><title>Chargaff's &quot;Grammar of Biology&quot;: New Fractal-like Rules</title><categories>q-bio.GN cs.CE cs.DM</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chargaff once said that &quot;I saw before me in dark contours the beginning of a
grammar of Biology&quot;. In linguistics, &quot;grammar&quot; is the set of natural language
rules, but we do not know for sure what Chargaff meant by &quot;grammar&quot; of Biology.
Nevertheless, assuming the metaphor, Chargaff himself started a &quot;grammar of
Biology&quot; discovering the so called Chargaff's rules. In this work, we further
develop his grammar. Using new concepts, we were able to discovery new genomic
rules that seem to be invariant across a large set of organisms, and show a
fractal-like property, since no matter the scale, the same pattern is observed
(self-similarity). We hope that these new invariant genomic rules may be used
in different contexts since short read data bias detection to genome assembly
quality assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1535</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1535</id><created>2011-12-07</created><authors><author><keyname>Karavelas</keyname><forenames>Menelaos I.</forenames></author><author><keyname>Tzanaki</keyname><forenames>Eleni</forenames></author></authors><title>Tight lower bounds on the number of faces of the Minkowski sum of convex
  polytopes via the Cayley trick</title><categories>cs.CG math.CO</categories><comments>19 pages</comments><msc-class>52B05 (Primary) 52B11, 52C45, 68U05 (Secondary)</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of $r$ convex $d$-polytopes $P_1,P_2,...,P_r$, where $d\ge{}3$
and $r\ge{}2$, and let $n_i$ be the number of vertices of $P_i$,
$1\le{}i\le{}r$. It has been shown by Fukuda and Weibel that the number of
$k$-faces of the Minkowski sum, $P_1+P_2+...+P_r$, is bounded from above by
$\Phi_{k+r}(n_1,n_2,...,n_r)$, where $\Phi_{\ell}(n_1,n_2,...,n_r)=
\sum_{\substack{1\le{}s_i\le{}n_i s_1+...+s_r=\ell}}
\prod_{i=1}^r\binom{n_i}{s_i}$, $\ell\ge{}r$. Fukuda and Weibel have also shown
that the upper bound mentioned above is tight for $d\ge{}4$,
$2\le{}r\le{}\lfloor\frac{d}{2}\rfloor$, and for all
$0\le{}k\le{}\lfloor\frac{d}{2}\rfloor-r$.
  In this paper we construct a set of $r$ neighborly $d$-polytopes
$P_1,P_2,...,P_r$, where $d\ge{}3$ and $2\le{}r\le{}d-1$, for which the upper
bound of Fukuda and Weibel is attained for all
$0\le{}k\le{}\lfloor\frac{d+r-1}{2}\rfloor-r$. Our approach is based on what is
known as the Cayley trick for Minkowski sums. A direct consequence of our
result is a tight asymptotic bound on the complexity of the Minkowski sum
$P_1+P_2+...+P_r$, for any fixed dimension $d$ and any $2\le{}r\le{}d-1$, when
the number of vertices of the polytopes is (asymptotically) the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1538</identifier>
 <datestamp>2012-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1538</id><created>2011-12-07</created><updated>2012-08-02</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Jungles, bundles, and fixed parameter tractability</title><categories>cs.DS math.CO</categories><comments>The new version contains simplified proofs providing better running
  times of the algorithms, as well as a wider discussion of the context</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a fixed-parameter tractable (FPT) approximation algorithm computing
the path-width of a tournament, and more generally, of a semi-complete digraph.
Based on this result, we prove that topological containment and rooted
immersion problems are FPT on semi-complete digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1546</identifier>
 <datestamp>2012-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1546</id><created>2011-12-07</created><authors><author><keyname>Mylnikov</keyname><forenames>L.</forenames></author><author><keyname>Trusov</keyname><forenames>A.</forenames></author></authors><title>On an Approach to the Design of a Logical Model of Innovation Project
  Data</title><categories>cs.OH</categories><journal-ref>Scientific and Technical Information Processing, 2011, Vol. 38,
  No. 3, pp. 208-213</journal-ref><doi>10.3103/S0147688211030142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Questions concerning the development of a logical model of innovation project
data, as well as those concerning the design of information systems for
decision?making support in the management of inno? vation projects, are
discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1548</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1548</id><created>2011-12-07</created><updated>2013-10-16</updated><authors><author><keyname>Conlon</keyname><forenames>David</forenames></author><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Sudakov</keyname><forenames>Benny</forenames></author></authors><title>Two extensions of Ramsey's theorem</title><categories>math.CO cs.DM</categories><comments>21 pages, accepted for publication in Duke Math. J</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ramsey's theorem, in the version of Erd\H{o}s and Szekeres, states that every
2-coloring of the edges of the complete graph on {1, 2,...,n} contains a
monochromatic clique of order 1/2\log n. In this paper, we consider two
well-studied extensions of Ramsey's theorem.
  Improving a result of R\&quot;odl, we show that there is a constant $c&gt;0$ such
that every 2-coloring of the edges of the complete graph on \{2, 3,...,n\}
contains a monochromatic clique S for which the sum of 1/\log i over all
vertices i \in S is at least c\log\log\log n. This is tight up to the constant
factor c and answers a question of Erd\H{o}s from 1981.
  Motivated by a problem in model theory, V\&quot;a\&quot;an\&quot;anen asked whether for
every k there is an n such that the following holds. For every permutation \pi
of 1,...,k-1, every 2-coloring of the edges of the complete graph on {1, 2,
..., n} contains a monochromatic clique a_1&lt;...&lt;a_k with
a_{\pi(1)+1}-a_{\pi(1)}&gt;a_{\pi(2)+1}-a_{\pi(2)}&gt;...&gt;a_{\pi(k-1)+1}-a_{\pi(k-1)}.
That is, not only do we want a monochromatic clique, but the differences
between consecutive vertices must satisfy a prescribed order. Alon and,
independently, Erd\H{o}s, Hajnal and Pach answered this question affirmatively.
Alon further conjectured that the true growth rate should be exponential in k.
We make progress towards this conjecture, obtaining an upper bound on n which
is exponential in a power of k. This improves a result of Shelah, who showed
that n is at most double-exponential in k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1554</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1554</id><created>2011-12-07</created><authors><author><keyname>Crolard</keyname><forenames>Tristan</forenames></author><author><keyname>Polonowski</keyname><forenames>Emmanuel</forenames></author></authors><title>A program logic for higher-order procedural variables and non-local
  jumps</title><categories>cs.LO</categories><report-no>TR-LACL-2011-4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relying on the formulae-as-types paradigm for classical logic, we define a
program logic for an imperative language with higher-order procedural variables
and non-local jumps. Then, we show how to derive a sound program logic for this
programming language. As a by-product, we obtain a non-dependent type system
which is more permissive than what is usually found in statically typed
imperative languages. As a generic example, we encode imperative versions of
delimited continuations operators shift and reset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1556</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1556</id><created>2011-12-07</created><updated>2012-02-24</updated><authors><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Active Learning of Halfspaces under a Margin Assumption</title><categories>cs.LG stat.ML</categories><comments>A more detailed exposition; Added a description of a simpler
  implementation and results of experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive and analyze a new, efficient, pool-based active learning algorithm
for halfspaces, called ALuMA. Most previous algorithms show exponential
improvement in the label complexity assuming that the distribution over the
instance space is close to uniform. This assumption rarely holds in practical
applications. Instead, we study the label complexity under a large-margin
assumption -- a much more realistic condition, as evident by the success of
margin-based algorithms such as SVM. Our algorithm is computationally efficient
and comes with formal guarantees on its label complexity. It also naturally
extends to the non-separable case and to non-linear kernels. Experiments
illustrate the clear advantage of ALuMA over other active learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1562</identifier>
 <datestamp>2012-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1562</id><created>2011-12-07</created><updated>2012-06-22</updated><authors><author><keyname>King</keyname><forenames>James</forenames></author></authors><title>Generating k-Facets by Induction on the Dimension</title><categories>cs.CG math.CO</categories><comments>This paper has been withdrawn pending publication of errata due to a
  crucial error in the proof of Theorem 4. We believe that the result is true,
  but the proof is not correct</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let S be a set of n &gt;= d points in general position in R^d. An oriented
(d-1)-simplex spanned by d points from S is called a k-facet iff the positive
side of its affine hull contains exactly k points from S. A (&lt;=k)-facet is
simply an i-facet for some i &lt;= k. Let E_k(S) denote the number of
(&lt;=k)-facets. Of particular interest is the problem of bounding E_k(S) in terms
of n, d, and k.
  We present and analyze a method of generating all oriented d-tuples of points
from S (and therefore all k-facets for 0 &lt;= k &lt;= n-d) that is inductive with
regard to the dimension d. The motivation behind this is to shed light on the
problem of bounding E_k(S) by drawing parallels with a simple method of
sampling from certain beta distributions. In particular, we aim to provide a
fresh perspective on a difficult open problem, the Generalized Upper Bound
Conjecture proposed by Eckhoff, Linhart, and Welzl.
  After presenting our analysis of the generation technique, we apply it to
obtain a simple proof of a lower bound for E_k(S). This bound was known for d=2
but holds for a wider range of k than previous bounds when d &gt;= 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1564</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1564</id><created>2011-12-07</created><authors><author><keyname>Aggarwal</keyname><forenames>Divesh</forenames></author><author><keyname>Dubey</keyname><forenames>Chandan</forenames></author></authors><title>Improved hardness results for unique shortest vector problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give several improvements on the known hardness of the unique shortest
vector problem. - We give a deterministic reduction from the shortest vector
problem to the unique shortest vector problem. As a byproduct, we get
deterministic NP-hardness for unique shortest vector problem in the
$\ell_\infty$ norm. - We give a randomized reduction from SAT to
uSVP_{1+1/poly(n)}. This shows that uSVP_{1+1/poly(n)} is NP-hard under
randomized reductions. - We show that if GapSVP_\gamma \in coNP (or coAM) then
uSVP_{\sqrt{\gamma}} \in coNP (coAM respectively). This simplifies previously
known uSVP_{n^{1/4}} \in coAM proof by Cai \cite{Cai98} to uSVP_{(n/\log
n)^{1/4}} \in coAM, and additionally generalizes it to uSVP_{n^{1/4}} \in coNP.
- We give a deterministic reduction from search-uSVP_\gamma to the
decision-uSVP_{\gamma/2}. We also show that the decision-uSVP is {\bf NP}-hard
for randomized reductions, which does not follow from Kumar-Sivakumar
\cite{KS01}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1584</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1584</id><created>2011-12-07</created><updated>2012-03-29</updated><authors><author><keyname>Shukla</keyname><forenames>Srishti</forenames></author><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Wireless Network-Coded Three-Way Relaying Using Latin Cubes</title><categories>cs.IT math.IT</categories><comments>13 Pages, 16 Figures. Some mistakes in the previous version have been
  fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of modulation schemes for the physical layer network-coded
three-way wireless relaying scenario is considered. The protocol employs two
phases: Multiple Access (MA) phase and Broadcast (BC) phase with each phase
utilizing one channel use. For the two-way relaying scenario, it was observed
by Koike-Akino et al. \cite{KPT}, that adaptively changing the network coding
map used at the relay according to the channel conditions greatly reduces the
impact of multiple access interference which occurs at the relay during the MA
phase and all these network coding maps should satisfy a requirement called
\textit{exclusive law}. This paper does the equivalent for the three-way
relaying scenario. We show that when the three users transmit points from the
same 4-PSK constellation, every such network coding map that satisfies the
exclusive law can be represented by a Latin Cube of Second Order. The network
code map used by the relay for the BC phase is explicitly obtained and is aimed
at reducing the effect of interference at the MA stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1593</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1593</id><created>2011-12-07</created><authors><author><keyname>Das</keyname><forenames>Smarajit</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Low-delay, High-rate Non-square Complex Orthogonal Designs</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximal rate of a non-square complex orthogonal design for $n$ transmit
antennas is $1/2+\frac{1}{n}$ if $n$ is even and $1/2+\frac{1}{n+1}$ if $n$ is
odd and the codes have been constructed for all $n$ by Liang (IEEE Trans.
Inform. Theory, 2003) and Lu et al. (IEEE Trans. Inform. Theory, 2005) to
achieve this rate. A lower bound on the decoding delay of maximal-rate complex
orthogonal designs has been obtained by Adams et al. (IEEE Trans. Inform.
Theory, 2007) and it is observed that Liang's construction achieves the bound
on delay for $n$ equal to 1 and 3 modulo 4 while Lu et al.'s construction
achieves the bound for $n=0,1,3$ mod 4. For $n=2$ mod 4, Adams et al. (IEEE
Trans. Inform. Theory, 2010) have shown that the minimal decoding delay is
twice the lower bound, in which case, both Liang's and Lu at al.'s construction
achieve the minimum decoding delay. % when $n=2$ mod 4. For large value of $n$,
it is observed that the rate is close to half and the decoding delay is very
large. A class of rate-1/2 codes with low decoding delay for all $n$ has been
constructed by Tarokh et al. (IEEE Trans. Inform. Theory, 1999). % have
constructed a class of rate-1/2 codes with low decoding delay for all $n$. In
this paper, another class of rate-1/2 codes is constructed for all $n$ in which
case the decoding delay is half the decoding delay of the rate-1/2 codes given
by Tarokh et al. This is achieved by giving first a general construction of
square real orthogonal designs which includes as special cases the well-known
constructions of Adams, Lax and Phillips and the construction of Geramita and
Pullman, and then making use of it to obtain the desired rate-1/2 codes. For
the case of 9 transmit antennas, the proposed rate-1/2 code is shown to be of
minimal-delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1597</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1597</id><created>2011-12-07</created><authors><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>de la Roche</keyname><forenames>Guillaume</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>Enhanced Inter-Cell Interference Coordination Challenges in
  Heterogeneous Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>12 pages, 4 figures, 2 tables</comments><journal-ref>IEEE Wireless Commun. Mag., vol. 18, no 3, Page(s):22 - 30, 2011</journal-ref><doi>10.1109/MWC.2011.5876497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3GPP LTE-Advanced has started a new study item to investigate Heterogeneous
Network (HetNet) deployments as a cost effective way to deal with the
unrelenting traffic demand. HetNets consist of a mix of macrocells, remote
radio heads, and low-power nodes such as picocells, femtocells, and relays.
Leveraging network topology, increasing the proximity between the access
network and the end-users, has the potential to provide the next significant
performance leap in wireless networks, improving spatial spectrum reuse and
enhancing indoor coverage. Nevertheless, deployment of a large number of small
cells overlaying the macrocells is not without new technical challenges. In
this article, we present the concept of heterogeneous networks and also
describe the major technical challenges associated with such network
architecture. We focus in particular on the standardization activities within
the 3GPP related to enhanced inter-cell interference coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1615</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1615</id><created>2011-12-07</created><authors><author><keyname>Barth</keyname><forenames>Dominique</forenames></author><author><keyname>Boudaoud</keyname><forenames>Boubkeur</forenames></author><author><keyname>Mautor</keyname><forenames>Thierry</forenames></author></authors><title>SLA Establishment with Guaranteed QoS in the Interdomain Network: A
  Stock Model</title><categories>cs.NI cs.LG</categories><comments>19 pages, 19 figures, IJCNC,
  http://airccse.org/journal/cnc/0711cnc13.pdf</comments><journal-ref>International Journal of Computer Networks &amp; Communications 3.4
  (July 2011) 188-206</journal-ref><doi>10.5121/ijcnc.2011.3413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new model that we present in this paper is introduced in the context of
guaranteed QoS and resources management in the inter-domain routing framework.
This model, called the stock model, is based on a reverse cascade approach and
is applied in a distributed context. So transit providers have to learn the
right capacities to buy and to stock and, therefore learning theory is applied
through an iterative process. We show that transit providers manage to learn
how to strategically choose their capacities on each route in order to maximize
their benefits, despite the very incomplete information. Finally, we provide
and analyse some simulation results given by the application of the model in a
simple case where the model quickly converges to a stable state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1616</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1616</id><created>2011-12-07</created><authors><author><keyname>Mocs&#xe1;r</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Kreith</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Buchholz</keyname><forenames>Jan</forenames></author><author><keyname>Krieger</keyname><forenames>Jan Wolfgang</forenames></author><author><keyname>Langowski</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>V&#xe1;mosi</keyname><forenames>Gy&#xf6;rgy</forenames></author></authors><title>Multiplexed multiple-{\tau} auto- and cross- correlators on a single
  FPGA</title><categories>physics.optics cs.AR physics.ins-det</categories><comments>19 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fluorescence correlation and cross-correlation spectroscopy (FCS, FCCS) are
widely used techniques to study the diffusion properties and interactions of
fluorescent molecules. Autocorrelation (ACFs) and cross-correlation functions
(CCFs) are typically acquired with fast hardware correlators. Here we introduce
a new multiple-{\tau} hardware correlator design for computing ACFs and CCFs in
real time. A scheduling algorithm minimizes the use of hardware resources by
calculating the different segments of the correlation function on a single
correlator block. The program was written in LabVIEW, enabling computation of
two multiple-{\tau} ACFs and two CCFs on a National Instruments FPGA card (NI
7833R) in real time with a minimal sampling time of 400 ns. Raw data are also
stored with a time resolution of 50 ns for later analysis. The design can be
adapted to other FPGA cards with only minor changes and extended to evaluate
more inputs and correlation functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1639</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1639</id><created>2011-12-07</created><authors><author><keyname>Fedorenko</keyname><forenames>Sergei V.</forenames></author></authors><title>A novel method for computation of the discrete Fourier transform over
  characteristic two finite field of even extension degree</title><categories>cs.IT math.IT</categories><comments>35 pages. Submitted to IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Signal Processing. 2015, Vol.63, No 20,
  5307-5317</journal-ref><doi>10.1109/TSP.2015.2453135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method for computation of the discrete Fourier transform over a
finite field with reduced multiplicative complexity is described. If the number
of multiplications is to be minimized, then the novel method for the finite
field of even extension degree is the best known method of the discrete Fourier
transform computation. A constructive method of constructing for a cyclic
convolution over a finite field is introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1645</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1645</id><created>2011-12-07</created><updated>2014-11-26</updated><authors><author><keyname>Ekhad</keyname><forenames>Shalosh B.</forenames></author><author><keyname>Zeilberger</keyname><forenames>Doron</forenames></author></authors><title>How to Gamble If You're In a Hurry</title><categories>math.PR cs.GT</categories><comments>6 pages; accompanied by a Maple package at
  http://www.math.rutgers.edu/~zeilberg/tokhniot/HIMURIM ; This version removes
  the name of the former second-named author by his own request</comments><journal-ref>Journal of Difference Equations and Application, 19(2013) ,
  520-526</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The beautiful theory of statistical gambling, started by Dubins and Savage
(for subfair games) and continued by Kelly and Breiman (for superfair games)
has mostly been studied under the unrealistic assumption that we live in a
continuous world, that money is indefinitely divisible, and that our life is
indefinitely long. Here we study these fascinating problems from a purely
discrete, finitistic, and computational, viewpoint, using Both Symbol-Crunching
and Number-Crunching (and simulation just for checking purposes).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1661</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1661</id><created>2011-12-07</created><updated>2012-04-10</updated><authors><author><keyname>Santos</keyname><forenames>Carlos</forenames></author><author><keyname>Rebelo</keyname><forenames>Armindo</forenames></author><author><keyname>Ferreira</keyname><forenames>Carla</forenames></author><author><keyname>Tribolet</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Identification of the Risk Related to a Process on Hospital Emergency
  Service: a Case Study</title><categories>cs.SE</categories><comments>Proceedings of the 10th International Symposium on Health Information
  Management Research - iSHIMR 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper, framed in a vast investigation, describes the application of
techniques and methodologies in Organizational Engineering connected to the
associated risk to the processes developed in an Emergency Service of an
important Portuguese Hospital. The transactions performed in an emergency
service and the consequent risk identification (negative behaviour associated
to those transactions) is done based on static and dynamic models, developed
during the business modelling. Any non-trivial system is better portrayed
trough a small number of reasonably independent models. From this point of view
it is important to look at the systems from a &quot;micro&quot; perspective, which allows
us to analyse the system at the transaction level. All processes have some
associated risk (inherent risk). Its identification will be decisive for future
analysis and for the consequent decision over the need, or not, to study
internal control mechanisms. This decision will depend on the risk level that
the organization considers acceptable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1662</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1662</id><created>2011-12-07</created><updated>2012-04-10</updated><authors><author><keyname>Santos</keyname><forenames>Carlos</forenames></author><author><keyname>Rebelo</keyname><forenames>Armindo</forenames></author><author><keyname>Ferreira</keyname><forenames>Carla</forenames></author><author><keyname>Tribolet</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Urgency/Emergency Health Processes' Modelling: A Case Study</title><categories>cs.SE</categories><comments>14th International Conference of Medical Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing complexity and sophistication of the organizational information
systems, and hospital ones particularly, render difficult their comprehension
and, consequently, the implementation of control mechanisms that may assure, at
all times, the auditability of the above mentioned systems, without having to
use models. This paper, framed in a wider investigation, aims to describe the
application of techniques and methodologies, in the sphere of action of
Organizational Engineering, in the modelling of business processes developed in
the main Operating Theatre of the Coimbra's University Hospital Emergency
Service, as a support for the implementation of an information system
architecture, using for that purpose the CEO framework, developed and suggested
by the Centre for Organizational Engineering (CEO), based on the UML language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1668</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1668</id><created>2011-12-07</created><authors><author><keyname>Bennett</keyname><forenames>Casey</forenames></author><author><keyname>Doub</keyname><forenames>Thomas</forenames></author></authors><title>Data Mining and Electronic Health Records: Selecting Optimal Clinical
  Treatments in Practice</title><categories>cs.DB</categories><comments>Keywords: Data Mining; Decision Support Systems, Clinical; Electronic
  Health Records; Evidence-Based Medicine; Data Warehouse</comments><journal-ref>Proceedings of the 6th International Conference on Data Mining.
  (2010) pp. 313-318</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic health records (EHR's) are only a first step in capturing and
utilizing health-related data - the problem is turning that data into useful
information. Models produced via data mining and predictive analysis profile
inherited risks and environmental/behavioral factors associated with patient
disorders, which can be utilized to generate predictions about treatment
outcomes. This can form the backbone of clinical decision support systems
driven by live data based on the actual population. The advantage of such an
approach based on the actual population is that it is &quot;adaptive&quot;. Here, we
evaluate the predictive capacity of a clinical EHR of a large mental healthcare
provider (~75,000 distinct clients a year) to provide decision support
information in a real-world clinical setting. Initial research has achieved a
70% success rate in predicting treatment outcomes using these methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1670</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1670</id><created>2011-12-07</created><authors><author><keyname>Bennett</keyname><forenames>Casey</forenames></author><author><keyname>Doub</keyname><forenames>Thomas</forenames></author><author><keyname>Bragg</keyname><forenames>April</forenames></author><author><keyname>Luellen</keyname><forenames>Jason</forenames></author><author><keyname>Van Regenmorter</keyname><forenames>Christina</forenames></author><author><keyname>Lockman</keyname><forenames>Jennifer</forenames></author><author><keyname>Reiserer</keyname><forenames>Randall</forenames></author></authors><title>Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental
  Health Setting: Toward Data-Driven Clinical Decision Support and Personalized
  Treatment</title><categories>cs.AI cs.GL</categories><comments>Keywords- Data Mining; Patient-Reported Outcomes; CDOI;
  Implementation; Electronic Health Records; Decision Support Systems,
  Clinical; Theory of Planned Behavior</comments><journal-ref>First IEEE International Conference on Healthcare Informatics,
  Imaging and Systems Biology (HISB). (2011). 229-236</journal-ref><doi>10.1109/HISB.2011.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CDOI outcome measure - a patient-reported outcome (PRO) instrument
utilizing direct client feedback - was implemented in a large, real-world
behavioral healthcare setting in order to evaluate previous findings from
smaller controlled studies. PROs provide an alternative window into treatment
effectiveness based on client perception and facilitate detection of
problems/symptoms for which there is no discernible measure (e.g. pain). The
principal focus of the study was to evaluate the utility of the CDOI for
predictive modeling of outcomes in a live clinical setting. Implementation
factors were also addressed within the framework of the Theory of Planned
Behavior by linking adoption rates to implementation practices and clinician
perceptions. The results showed that the CDOI does contain significant capacity
to predict outcome delta over time based on baseline and early change scores in
a large, real-world clinical setting, as suggested in previous research. The
implementation analysis revealed a number of critical factors affecting
successful implementation and adoption of the CDOI outcome measure, though
there was a notable disconnect between clinician intentions and actual
behavior. Most importantly, the predictive capacity of the CDOI underscores the
utility of direct client feedback measures such as PROs and their potential use
as the basis for next generation clinical decision support tools and
personalized treatment approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1675</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1675</id><created>2011-12-07</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author></authors><title>Steganography: a Class of Algorithms having Secure Properties</title><categories>cs.DM cs.CR</categories><comments>4 pages, published in Seventh International Conference on Intelligent
  Information Hiding and Multimedia Signal Processing, IIH-MSP 2011, Dalian,
  China, October 14-16, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaos-based approaches are frequently proposed in information hiding, but
without obvious justification. Indeed, the reason why chaos is useful to tackle
with discretion, robustness, or security, is rarely elucidated. This research
work presents a new class of non-blind information hidingalgorithms based on
some finite domains iterations that are Devaney's topologically chaotic. The
approach is entirely formalized and reasons to take place into the mathematical
theory of chaos are explained. Finally, stego-security and chaos security are
consequently proven for a large class of algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1680</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1680</id><created>2011-12-07</created><updated>2012-05-22</updated><authors><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author></authors><title>Quantifying synergistic information remains an unsolved problem</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author. Fully superseded by:
  arXiv:1205.4265</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author. This paper is now obsolete. For
a solution please see: arXiv:/1205.4265.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1681</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1681</id><created>2011-12-07</created><authors><author><keyname>Low</keyname><forenames>Jyue Tyan</forenames></author></authors><title>A literature review: What exactly should we preserve? How scholars
  address this question and where is the gap</title><categories>cs.DL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This review addresses the question of what exactly should we preserve, and
how the digital preservation community and scholars address this question. The
paper first introduces the much-abused-term &quot;significant properties,&quot; before
revealing how some scholars are of the opinion that characteristics of digital
objects to be preserved (i.e., significant properties) can be identified and
should be expressed formally, while others are not of that opinion. The digital
preservation community's attempt to expound on the general characteristics of
digital objects and significant properties will then be discussed. Finally, the
review shows that while there may be ways to identify the technical makeup or
general characteristics of a digital object, there is currently no formal and
objective methodology to help stakeholders identify and decide what the
significant properties of the objects are. This review thus helps open
questions and generates a formative recommendation based on expert opinion that
expressing an object's functions in an explicit and formal way (using didactic
guides from the archives community) could be the solution to help stakeholders
decide what characteristics/ elements exactly we should preserve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1684</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1684</id><created>2011-12-07</created><authors><author><keyname>Bahi</keyname><forenames>J. M.</forenames></author><author><keyname>Couchot</keyname><forenames>J. -F.</forenames></author><author><keyname>Guyeux</keyname><forenames>C.</forenames></author><author><keyname>Richard</keyname><forenames>A.</forenames></author></authors><title>On the Link Between Strongly Connected Iteration Graphs and Chaotic
  Boolean Discrete-Time Dynamical Systems</title><categories>cs.DM cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaotic functions are characterized by sensitivity to initial conditions,
transitivity, and regularity. Providing new functions with such properties is a
real challenge. This work shows that one can associate with any Boolean network
a continuous function, whose discrete-time iterations are chaotic if and only
if the iteration graph of the Boolean network is strongly connected. Then,
sufficient conditions for this strong connectivity are expressed on the
interaction graph of this network, leading to a constructive method of chaotic
function computation. The whole approach is evaluated in the chaos-based
pseudo-random number generation context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1687</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1687</id><created>2011-12-07</created><updated>2013-02-23</updated><authors><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author><author><keyname>Warsi</keyname><forenames>Naqueeb Ahmad</forenames></author></authors><title>Non-asymptotic information theoretic bound for some multi-party
  scenarios</title><categories>cs.IT math.IT quant-ph</categories><comments>This is the version that was published with a changed title in the
  Fiftieth Annual Allerton Conference on Communication, Control, and Computing,
  Oct 2012, Monticello, IL, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years, there has been a great interest in extending the
information-theoretic scenario for the non-asymptotic or one-shot case, i.e.,
where the channel is used only once. We provide the one-shot rate region for
the distributed source-coding (Slepian-Wolf) and the multiple-access channel.
Our results are based on defining a novel one-shot typical set based on smooth
entropies that yields the one-shot achievable rate regions while leveraging the
results from the asymptotic analysis. Our results are asymptotically optimal,
i.e., for the distributed source coding they yield the same rate region as the
Slepian-Wolf in the limit of unlimited independent and identically distributed
(i.i.d.) copies. Similarly for the multiple-access channel the asymptotic
analysis of our approach yields the rate region which is equal to the rate
region of the memoryless multiple-access channel in the limit of large number
of channel uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1688</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1688</id><created>2011-12-07</created><authors><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Derriere</keyname><forenames>Sebastien</forenames></author><author><keyname>Biemesderfer</keyname><forenames>Chris</forenames></author><author><keyname>Gray</keyname><forenames>Norman</forenames></author></authors><title>Why don't we already have an Integrated Framework for the Publication
  and Preservation of all Data Products?</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, submitted to the ADASS XXI proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomy has long had a working network of archives supporting the curation
of publications and data. The discipline has already created many of the
features which perplex other areas of science: (1) data repositories:
(supra)national institutes, dedicated to large projects; a culture of
user-contributed data; practical experience of long-term data preservation; (2)
dataset identifiers: the community has already piloted experiments, knows what
can undermine these efforts, and is participating in the development of
next-generation standards; (3) citation of datasets in papers: the community
has an innovative and expanding infrastructure for the curation of data and
bibliographic resources, and through them a community of author s and editors
familiar with such electronic publication efforts; as well, it has experimented
with next-generation web standards (e.g. the Semantic Web); (4) publisher
buy-in: publishers in this area have been willing to innovate within the
constraints of their commercial imperatives. What can possibly be missing? Why
don't we have an integrated framework for the publication and preservation of
all data products already? Are there technical barriers? We don't believe so.
Are there cultural or commercial forces inhibiting this? We aren't aware of
any. This Birds of a Feather session (BoF) attempted to identify existing
barriers to the creation of such a framework, and attempted to identify the
parties or groups which can contribute to the creation of a VO-powered
data-publishing framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1710</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1710</id><created>2011-12-07</created><updated>2013-02-21</updated><authors><author><keyname>Carroll-Nellenback</keyname><forenames>Jonathan J.</forenames></author><author><keyname>Shroyer</keyname><forenames>Brandon</forenames></author><author><keyname>Frank</keyname><forenames>Adam</forenames></author><author><keyname>Ding</keyname><forenames>Chen</forenames></author></authors><title>Efficient Parallelization for AMR MHD Multiphysics Calculations;
  Implementation in AstroBEAR</title><categories>astro-ph.SR cs.DC physics.comp-ph</categories><comments>Updated version of paper with improved scaling results</comments><journal-ref>Journal of Computational Physics, Volume 236, 1 March 2013, Pages
  461-476</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current Adaptive Mesh Refinement (AMR) simulations require algorithms that
are highly parallelized and manage memory efficiently. As compute engines grow
larger, AMR simulations will require algorithms that achieve new levels of
efficient parallelization and memory management. We have attempted to employ
new techniques to achieve both of these goals. Patch or grid based AMR often
employs ghost cells to decouple the hyperbolic advances of each grid on a given
refinement level. This decoupling allows each grid to be advanced
independently. In AstroBEAR we utilize this independence by threading the grid
advances on each level with preference going to the finer level grids. This
allows for global load balancing instead of level by level load balancing and
allows for greater parallelization across both physical space and AMR level.
Threading of level advances can also improve performance by interleaving
communication with computation, especially in deep simulations with many levels
of refinement. While we see improvements of up to 30% on deep simulations run
on a few cores, the speedup is typically more modest (5-20%) for larger scale
simulations. To improve memory management we have employed a distributed tree
algorithm that requires processors to only store and communicate local sections
of the AMR tree structure with neighboring processors. Using this distributed
approach we are able to get reasonable scaling efficiency (&gt; 80%) out to 12288
cores and up to 8 levels of AMR - independent of the use of threading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1715</identifier>
 <datestamp>2012-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1715</id><created>2011-12-07</created><updated>2012-08-06</updated><authors><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Rezaei</keyname><forenames>Farzad</forenames></author></authors><title>Optimal Merging Algorithms for Lossless Codes with Generalized Criteria</title><categories>cs.IT math.IT</categories><comments>40 pages long, arXiv admin note: text overlap with arXiv:1102.2207,
  arXiv:1202.0136</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents lossless prefix codes optimized with respect to a pay-off
criterion consisting of a convex combination of maximum codeword length and
average codeword length. The optimal codeword lengths obtained are based on a
new coding algorithm which transforms the initial source probability vector
into a new probability vector according to a merging rule. The coding algorithm
is equivalent to a partition of the source alphabet into disjoint sets on which
a new transformed probability vector is defined as a function of the initial
source probability vector and a scalar parameter. The pay-off criterion
considered encompasses a trade-off between maximum and average codeword length;
it is related to a pay-off criterion consisting of a convex combination of
average codeword length and average of an exponential function of the codeword
length, and to an average codeword length pay-off criterion subject to a
limited length constraint. A special case of the first related pay-off is
connected to coding problems involving source probability uncertainty and
codeword overflow probability, while the second related pay-off compliments
limited length Huffman coding algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1728</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1728</id><created>2011-12-07</created><authors><author><keyname>Grabow</keyname><forenames>Carsten</forenames></author><author><keyname>Grosskinsky</keyname><forenames>Stefan</forenames></author><author><keyname>Timme</keyname><forenames>Marc</forenames></author></authors><title>Small-world spectra in mean field theory</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI math-ph math.MP</categories><comments>5 pages, 4 figures</comments><journal-ref>Phys. Rev. Lett. 108(21), 218701 (2012)</journal-ref><doi>10.1103/PhysRevLett.108.218701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective dynamics on small-world networks emerge in a broad range of
systems with their spectra characterizing fundamental asymptotic features. Here
we derive analytic mean field predictions for the spectra of small-world models
that systematically interpolate between regular and random topologies by
varying their randomness. These theoretical predictions agree well with the
actual spectra (obtained by numerical diagonalization) for undirected and
directed networks and from fully regular to strongly random topologies. These
results may provide analytical insights to empirically found features of
dynamics on small-world networks from various research fields, including
biology, physics, engineering and social science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1730</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1730</id><created>2011-12-07</created><authors><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Tembin&#xe9;</keyname><forenames>Hamidou</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Quality-Of-Service Provisioning in Decentralized Networks: A
  Satisfaction Equilibrium Approach</title><categories>cs.IT cs.GT math.IT</categories><comments>Article accepted for publication in IEEE Journal on Selected Topics
  in Signal Processing, special issue in Game Theory in Signal Processing. 16
  pages, 6 figures</comments><doi>10.1109/JSTSP.2011.2180507</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a particular game formulation and its corresponding
notion of equilibrium, namely the satisfaction form (SF) and the satisfaction
equilibrium (SE). A game in SF models the case where players are uniquely
interested in the satisfaction of some individual performance constraints,
instead of individual performance optimization. Under this formulation, the
notion of equilibrium corresponds to the situation where all players can
simultaneously satisfy their individual constraints. The notion of SE, models
the problem of QoS provisioning in decentralized self-configuring networks.
Here, radio devices are satisfied if they are able to provide the requested
QoS. Within this framework, the concept of SE is formalized for both pure and
mixed strategies considering finite sets of players and actions. In both cases,
sufficient conditions for the existence and uniqueness of the SE are presented.
When multiple SE exist, we introduce the idea of effort or cost of satisfaction
and we propose a refinement of the SE, namely the efficient SE (ESE). At the
ESE, all players adopt the action which requires the lowest effort for
satisfaction. A learning method that allows radio devices to achieve a SE in
pure strategies in finite time and requiring only one-bit feedback is also
presented. Finally, a power control game in the interference channel is used to
highlight the advantages of modeling QoS problems following the notion of SE
rather than other equilibrium concepts, e.g., generalized Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1734</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1734</id><created>2011-12-07</created><authors><author><keyname>Domingues</keyname><forenames>Marcos Aur&#xe9;lio</forenames></author><author><keyname>Rezende</keyname><forenames>Solange Oliveira</forenames></author></authors><title>Using Taxonomies to Facilitate the Analysis of the Association Rules</title><categories>cs.DB cs.LG</categories><comments>ECML/PKDD'05 The Second International Workshop on Knowledge Discovery
  and Ontologies (KDO'05)</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Data Mining process enables the end users to analyze, understand and use
the extracted knowledge in an intelligent system or to support in the
decision-making processes. However, many algorithms used in the process
encounter large quantities of patterns, complicating the analysis of the
patterns. This fact occurs with association rules, a Data Mining technique that
tries to identify intrinsic patterns in large data sets. A method that can help
the analysis of the association rules is the use of taxonomies in the step of
post-processing knowledge. In this paper, the GART algorithm is proposed, which
uses taxonomies to generalize association rules, and the RulEE-GAR
computational module, that enables the analysis of the generalized rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1742</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1742</id><created>2011-12-07</created><authors><author><keyname>Huang</keyname><forenames>Weidong</forenames></author><author><keyname>Alem</keyname><forenames>Leila</forenames></author><author><keyname>Albasri</keyname><forenames>Jalal</forenames></author></authors><title>HandsInAir: A Wearable System for Remote Collaboration</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present HandsInAir, a real-time collaborative wearable system for remote
collaboration. The system is developed to support real world scenarios in which
a remote mobile helper guides a local mobile worker performing a physical task.
HandsInAir implements a novel approach to support mobility of remote
collaborators. This approach allows the helper to perform gestures without
having to touch tangible objects, requiring little environment support. The
system consists of two parts: the helper part and the worker part. The two
parts are connected via a wireless network and the collaboration partners
communicate with each other via audio and visual links. In this paper, we
review related work, describe technical implementation of the system and
envision future work for further improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1751</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1751</id><created>2011-12-07</created><authors><author><keyname>Cotterell</keyname><forenames>Michael E.</forenames></author><author><keyname>Miller</keyname><forenames>John A.</forenames></author><author><keyname>Horton</keyname><forenames>Tom</forenames></author></authors><title>Unicode in Domain-Specific Programming Languages for Modeling &amp;
  Simulation: ScalaTion as a Case Study</title><categories>cs.PL</categories><report-no>Technical Report #UGA-CS-LSDIS-TR-11-011, Department of Computer
  Science, University of Georgia, Athens, Georgia (May 2011)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As recent programming languages provide improved conciseness and flexibility
of syntax, the development of embedded or internal Domain-Specific Languages
has increased. The field of Modeling and Simulation has had a long history of
innovation in programming languages (e.g. Simula-67, GPSS). Much effort has
gone into the development of Simulation Programming Languages.
  The ScalaTion project is working to develop an embedded or internal
Domain-Specific Language for Modeling and Simulation which could streamline
language innovation in this domain. One of its goals is to make the code
concise, readable, and in a form familiar to experts in the domain. In some
cases the code looks very similar to textbook formulas. To enhance readability
by domain experts, a version of ScalaTion is provided that heavily utilizes
Unicode.
  This paper discusses the development of the ScalaTion DSL and the underlying
features of Scala that make this possible. It then provides an overview of
ScalaTion highlighting some uses of Unicode. Statistical analysis capabilities
needed for Modeling and Simulation are presented in some detail. The notation
developed is clear and concise which should lead to improved usability and
extendibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1757</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1757</id><created>2011-12-07</created><updated>2011-12-28</updated><authors><author><keyname>Jayram</keyname><forenames>T. S.</forenames></author><author><keyname>Pal</keyname><forenames>Soumitra</forenames></author><author><keyname>Arya</keyname><forenames>Vijay</forenames></author></authors><title>Recovery of a Sparse Integer Solution to an Underdetermined System of
  Linear Equations</title><categories>cs.IT cs.DM cs.LG math.IT</categories><comments>4 pages, contributed paper to be published at NIPS 2011 Workshop on
  Sparse Representation and Low-rank Approximation, 16 December 2011</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a system of m linear equations in n variables Ax=b where A is a
given m x n matrix and b is a given m-vector known to be equal to Ax' for some
unknown solution x' that is integer and k-sparse: x' in {0,1}^n and exactly k
entries of x' are 1. We give necessary and sufficient conditions for recovering
the solution x exactly using an LP relaxation that minimizes l1 norm of x. When
A is drawn from a distribution that has exchangeable columns, we show an
interesting connection between the recovery probability and a well known
problem in geometry, namely the k-set problem. To the best of our knowledge,
this connection appears to be new in the compressive sensing literature. We
empirically show that for large n if the elements of A are drawn i.i.d. from
the normal distribution then the performance of the recovery LP exhibits a
phase transition, i.e., for each k there exists a value m' of m such that the
recovery always succeeds if m &gt; m' and always fails if m &lt; m'. Using the
empirical data we conjecture that m' = nH(k/n)/2 where H(x) = -(x)log_2(x) -
(1-x)log_2(1-x) is the binary entropy function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1762</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1762</id><created>2011-12-07</created><updated>2012-10-25</updated><authors><author><keyname>Ahmadi</keyname><forenames>Behzad</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Heegard-Berger and Cascade Source Coding Problems with Common
  Reconstruction Constraints</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the HB problem with the CR constraint, the rate-distortion function is
derived under the assumption that the side information sequences are
(stochastically) degraded. The rate-distortion function is also calculated
explicitly for three examples, namely Gaussian source and side information with
quadratic distortion metric, and binary source and side information with
erasure and Hamming distortion metrics. The rate-distortion function is then
characterized for the HB problem with cooperating decoders and (physically)
degraded side information. For the cascade problem with the CR constraint, the
rate-distortion region is obtained under the assumption that side information
at the final node is physically degraded with respect to that at the
intermediate node. For the latter two cases, it is worth emphasizing that the
corresponding problem without the CR constraint is still open. Outer and inner
bounds on the rate-distortion region are also obtained for the cascade problem
under the assumption that the side information at the intermediate node is
physically degraded with respect to that at the final node. For the three
examples mentioned above, the bounds are shown to coincide. Finally, for the HB
problem, the rate-distortion function is obtained under the more general
requirement of constrained reconstruction, whereby the decoder's estimate must
be recovered at the encoder only within some distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1768</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1768</id><created>2011-12-08</created><authors><author><keyname>Liu</keyname><forenames>Keqin</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Extended UCB Policy for Multi-Armed Bandit with Light-Tailed Reward
  Distributions</title><categories>cs.LG math.PR math.ST stat.TH</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multi-armed bandit problems in which a player aims to accrue
reward by sequentially playing a given set of arms with unknown reward
statistics. In the classic work, policies were proposed to achieve the optimal
logarithmic regret order for some special classes of light-tailed reward
distributions, e.g., Auer et al.'s UCB1 index policy for reward distributions
with finite support. In this paper, we extend Auer et al.'s UCB1 index policy
to achieve the optimal logarithmic regret order for all light-tailed (or
equivalently, locally sub-Gaussian) reward distributions defined by the (local)
existence of the moment-generating function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1770</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1770</id><created>2011-12-08</created><authors><author><keyname>Nasser</keyname><forenames>Rajai</forenames></author></authors><title>Polar codes for the m-user multiple access channels</title><categories>cs.IT math.IT</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are constructed for m-user multiple access channels (MAC) whose
input alphabet size is a prime number. The block error probability under
successive cancelation decoding decays exponentially with the square root of
the block length. Although the sum capacity is achieved by this coding scheme,
some points in the symmetric capacity region may not be achieved. In the case
where the channel is a combination of linear channels, we provide a necessary
and sufficient condition characterizing the channels whose symmetric capacity
region is preserved upon the polarization process. We also provide a sufficient
condition for having a total loss in the dominant face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1783</identifier>
 <datestamp>2012-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1783</id><created>2011-12-08</created><updated>2012-01-27</updated><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Bensalem</keyname><forenames>Saddek</forenames></author><author><keyname>Yan</keyname><forenames>Rongjie</forenames></author><author><keyname>Ruess</keyname><forenames>Harald</forenames></author><author><keyname>Buckl</keyname><forenames>Christian</forenames></author><author><keyname>Knoll</keyname><forenames>Alois</forenames></author></authors><title>Distributed Priority Synthesis and its Applications</title><categories>cs.LO cs.GT</categories><comments>1. Timestamp the joint work &quot;Distributed Priority Synthesis&quot; from
  four institutes (Verimag, TUM, ISCAS, fortiss). 2. This version (v.2) updates
  related work in distributed synthesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of interacting components with non-deterministic variable update
and given safety requirements, the goal of priority synthesis is to restrict,
by means of priorities, the set of possible interactions in such a way as to
guarantee the given safety conditions for all possible runs. In distributed
priority synthesis we are interested in obtaining local sets of priorities,
which are deployed in terms of local component controllers sharing intended
next moves between components in local neighborhoods only. These possible
communication paths between local controllers are specified by means of a
communication architecture. We formally define the problem of distributed
priority synthesis in terms of a multi-player safety game between players for
(angelically) selecting the next transition of the components and an
environment for (demonically) updating uncontrollable variables; this problem
is NP-complete. We propose several optimizations including a solution-space
exploration based on a diagnosis method using a nested extension of the usual
attractor computation in games together with a reduction to corresponding SAT
problems. When diagnosis fails, the method proposes potential candidates to
guide the exploration. These optimized algorithms for solving distributed
priority synthesis problems have been integrated into our VissBIP framework. An
experimental validation of this implementation is performed using a range of
case studies including scheduling in multicore processors and modular robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1795</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1795</id><created>2011-12-08</created><updated>2012-07-12</updated><authors><author><keyname>Boldo</keyname><forenames>Sylvie</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Clement</keyname><forenames>Francois</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Filli&#xe2;tre</keyname><forenames>Jean-Christophe</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Mayero</keyname><forenames>Micaela</forenames><affiliation>LIPN, Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Melquiond</keyname><forenames>Guillaume</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Weis</keyname><forenames>Pierre</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Wave Equation Numerical Resolution: a Comprehensive Mechanized Proof of
  a C Program</title><categories>cs.LO math.NA</categories><comments>No. RR-7826 (2011)</comments><proxy>ccsd</proxy><journal-ref>Journal of Automated Reasoning 50, 4 (2013) 423-456</journal-ref><doi>10.1007/s10817-012-9255-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formally prove correct a C program that implements a numerical scheme for
the resolution of the one-dimensional acoustic wave equation. Such an
implementation introduces errors at several levels: the numerical scheme
introduces method errors, and floating-point computations lead to round-off
errors. We annotate this C program to specify both method error and round-off
error. We use Frama-C to generate theorems that guarantee the soundness of the
code. We discharge these theorems using SMT solvers, Gappa, and Coq. This
involves a large Coq development to prove the adequacy of the C program to the
numerical scheme and to bound errors. To our knowledge, this is the first time
such a numerical analysis program is fully machine-checked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1814</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1814</id><created>2011-12-08</created><authors><author><keyname>Isea</keyname><forenames>Raul</forenames></author><author><keyname>Bai</keyname><forenames>Er W.</forenames></author><author><keyname>Lonngren</keyname><forenames>Karl E.</forenames></author></authors><title>On the Long-term Health Care Crisis. A Possible Eradication Scenario</title><categories>q-bio.PE cs.CY</categories><comments>8 pages, 2 figures, 10 references</comments><journal-ref>WebmedCentral Epidemiology 2011;2(12):WMC002550</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The purpose of the present essay is to suggest a possible model to describe
the worldwide healthcare crisis, where diseases that have been considered to be
eradicated or under our control are re-emerging today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1828</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1828</id><created>2011-12-08</created><updated>2012-06-20</updated><authors><author><keyname>Kozma</keyname><forenames>Laszlo</forenames></author></authors><title>Minimum Average Distance Triangulations</title><categories>cs.CG cs.DS</categories><comments>ESA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding a triangulation T of a planar point set S
such as to minimize the expected distance between two points x and y chosen
uniformly at random from S. By distance we mean the length of the shortest path
between x and y along edges of T. The length of a path is the sum of the
weights of its edges. Edge weights are assumed to be given as part of the
problem for every pair of distinct points (x,y) in S^2.
  In a different variant of the problem, the points are vertices of a simple
polygon and we look for a triangulation of the interior of the polygon that is
optimal in the same sense.
  We prove that a general formulation of the problem in which the weights are
arbitrary positive numbers is strongly NP-complete. For the case when all the
weights are equal we give polynomial-time algorithms. In the end we mention
several open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1831</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1831</id><created>2011-12-08</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author></authors><title>Finding Overlapping Communities in Social Networks: Toward a Rigorous
  Approach</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>19 pages</comments><acm-class>F.2.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A &quot;community&quot; in a social network is usually understood to be a group of
nodes more densely connected with each other than with the rest of the network.
This is an important concept in most domains where networks arise: social,
technological, biological, etc. For many years algorithms for finding
communities implicitly assumed communities are nonoverlapping (leading to use
of clustering-based approaches) but there is increasing interest in finding
overlapping communities. A barrier to finding communities is that the solution
concept is often defined in terms of an NP-complete problem such as Clique or
Hierarchical Clustering.
  This paper seeks to initiate a rigorous approach to the problem of finding
overlapping communities, where &quot;rigorous&quot; means that we clearly state the
following: (a) the object sought by our algorithm (b) the assumptions about the
underlying network (c) the (worst-case) running time.
  Our assumptions about the network lie between worst-case and average-case. An
average case analysis would require a precise probabilistic model of the
network, on which there is currently no consensus. However, some plausible
assumptions about network parameters can be gleaned from a long body of work in
the sociology community spanning five decades focusing on the study of
individual communities and ego-centric networks. Thus our assumptions are
somewhat &quot;local&quot; in nature. Nevertheless they suffice to permit a rigorous
analysis of running time of algorithms that recover global structure.
  Our algorithms use random sampling similar to that in property testing and
algorithms for dense graphs. However, our networks are not necessarily dense
graphs, not even in local neighborhoods.
  Our algorithms explore a local-global relationship between ego-centric and
socio-centric networks that we hope will provide a fruitful framework for
future work both in computer science and sociology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1841</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1841</id><created>2011-12-08</created><updated>2014-06-26</updated><authors><author><keyname>Jolivet</keyname><forenames>Timo</forenames></author><author><keyname>Kari</keyname><forenames>Jarkko</forenames></author></authors><title>Consistency of multidimensional combinatorial substitutions</title><categories>cs.DM cs.FL math.CO</categories><comments>13 pages, v2 includes corrections to match the published version</comments><journal-ref>Theoretical Computer Science 454 (2012), 178-188, short version in
  CSR 2012, conference proceedings LNCS 7353, 205-216</journal-ref><doi>10.1016/j.tcs.2012.03.050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multidimensional combinatorial substitutions are rules that replace symbols
by finite patterns of symbols in $\mathbb Z^d$. We focus on the case where the
patterns are not necessarily rectangular, which requires a specific description
of the way they are glued together in the image by a substitution. Two problems
can arise when defining a substitution in such a way: it can fail to be
consistent, and the patterns in an image by the substitution might overlap.
  We prove that it is undecidable whether a two-dimensional substitution is
consistent or overlapping, and we provide practical algorithms to decide these
properties in some particular cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1848</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1848</id><created>2011-12-08</created><authors><author><keyname>Crolard</keyname><forenames>Tristan</forenames></author></authors><title>A Formally Specified Program Logic for Higher-Order Procedural Variables
  and non-local Jumps</title><categories>cs.LO</categories><report-no>TR-LACL-2011-5</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formally specified a program logic for higher-order procedural variables
and non-local jumps with Ott and Twelf. Moreover, the dependent type systems
and the translation are both executable specifications thanks to Twelf's logic
programming engine. In particular, relying on Filinski's encoding of
shift/reset using callcc/throw and a global meta-continuation (simulated in
state passing style), we have mechanically checked the correctness of a few
examples (all source files are available on request).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1851</identifier>
 <datestamp>2011-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1851</id><created>2011-12-08</created><updated>2011-12-16</updated><authors><author><keyname>Menzel</keyname><forenames>Michael</forenames></author><author><keyname>Sch&#xf6;nherr</keyname><forenames>Marten</forenames></author><author><keyname>Nimis</keyname><forenames>Jens</forenames></author><author><keyname>Tai</keyname><forenames>Stefan</forenames></author></authors><title>(MC2)2: A Generic Decision-Making Framework and its Application to Cloud
  Computing</title><categories>cs.DC</categories><comments>short version, full version available in proceedings of International
  Conference on Cloud Computing and Virtualization (CCV) 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a disruptive technology, representing a new model for
information technology (IT) solution engineering and management that promises
to introduce significant cost savings and other benefits. The adoption of Cloud
computing requires a detailed comparison of infrastructure alternatives, taking
a number of aspects into careful consideration. Existing methods of evaluation,
however, limit decision making to the relative costs of cloud computing, but do
not take a broader range of criteria into account. In this paper, we introduce
a generic, multi-criteria-based decision framework and an application for Cloud
Computing, the Multi-Criteria Comparison Method for Cloud Computing ((MC2)2).
The framework and method allow organizations to determine what infrastructure
best suits their needs by evaluating and ranking infrastructure alternatives
using multiple criteria. Therefore, (MC2)2 offers a way to differentiate
infrastructures not only by costs, but also in terms of benefits, opportunities
and risks. (MC2)2 can be adapted to facilitate a wide array of decision-making
scenarios within the domain of information technology infrastructures,
depending on the criteria selected to support the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1863</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1863</id><created>2011-12-06</created><authors><author><keyname>Halabian</keyname><forenames>Hassan</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author><author><keyname>Lung</keyname><forenames>Chung-Horng</forenames></author></authors><title>Delay Optimal Server Assignment to Symmetric Parallel Queues with Random
  Connectivities</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>6 pages, 4 figures, Proc. IEEE CDC-ECC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of assignment of $K$ identical
servers to a set of $N$ parallel queues in a time slotted queueing system. The
connectivity of each queue to each server is randomly changing with time; each
server can serve at most one queue and each queue can be served by at most one
server per time slot. Such queueing systems were widely applied in modeling the
scheduling (or resource allocation) problem in wireless networks. It has been
previously proven that Maximum Weighted Matching (MWM) is a throughput optimal
server assignment policy for such queueing systems. In this paper, we prove
that for a symmetric system with i.i.d. Bernoulli packet arrivals and
connectivities, MWM minimizes, in stochastic ordering sense, a broad range of
cost functions of the queue lengths including total queue occupancy (or
equivalently average queueing delay).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1872</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1872</id><created>2011-12-08</created><updated>2012-10-26</updated><authors><author><keyname>Aw</keyname><forenames>Alan J.</forenames></author></authors><title>The multicovering radius problem for some types of discrete structures</title><categories>math.CO cs.IT math.IT</categories><comments>To appear in Designs, Codes and Cryptography (2012)</comments><msc-class>05D40, 94B99</msc-class><journal-ref>Designs, Codes and Cryptography 72 (2) 2014</journal-ref><doi>10.1007/s10623-012-9755-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The covering radius problem is a question in coding theory concerned with
finding the minimum radius $r$ such that, given a code that is a subset of an
underlying metric space, balls of radius $r$ over its code words cover the
entire metric space. Klapper introduced a code parameter, called the
multicovering radius, which is a generalization of the covering radius. In this
paper, we introduce an analogue of the multicovering radius for permutation
codes (cf. Keevash and Ku, 2006) and for codes of perfect matchings (cf. Aw and
Ku, 2012). We apply probabilistic tools to give some lower bounds on the
multicovering radii of these codes. In the process of obtaining these results,
we also correct an error in the proof of the lower bound of the covering radius
that appeared in Keevash and Ku (2006). We conclude with a discussion of the
multicovering radius problem in an even more general context, which offers room
for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1892</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1892</id><created>2011-12-08</created><authors><author><keyname>Singh</keyname><forenames>Chandramani</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Combined Base Station Association and Power Control in Multi-channel
  Cellular Networks</title><categories>cs.NI cs.GT</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A combined base station association and power control problem is studied for
the uplink of multichannel multicell cellular networks, in which each channel
is used by exactly one cell (i.e., base station). A distributed association and
power update algorithm is proposed and shown to converge to a Nash equilibrium
of a noncooperative game. We consider network models with discrete mobiles
(yielding an atomic congestion game), as well as a continuum of mobiles
(yielding a population game). We find that the equilibria need not be Pareto
efficient, nor need they be system optimal. To address the lack of system
optimality, we propose pricing mechanisms. It is shown that these mechanisms
can be implemented in a distributed fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1895</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1895</id><created>2011-12-08</created><authors><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Equilibria of Channel Selection Games in Parallel Multiple Access
  Channel</title><categories>cs.GT</categories><comments>Article submitted to EURASIP Journal on Wireless Communications. 15
  pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the decentralized parallel multiple access channel
(MAC) when transmitters selfishly maximize their individual spectral efficiency
by selecting a single channel to transmit. More specifically, we investigate
the set of Nash equilibria (NE) of decentralized networks comprising several
transmitters communicating with a single receiver that implements single user
decoding. This scenario is modeled as a one-shot game where the players (the
transmitters) have discrete action sets (the channels). We show that the
corresponding game has always at least one NE in pure strategies, but,
depending on certain parameters, the game might possess several NE. We provide
an upper bound for the maximum number of NE as a function of the number of
transmitters and available channels. The main contribution of this paper is a
mathematical proof of the existence of a Braess-type paradox. In particular, it
is shown that under the assumption of a fully loaded network, when transmitters
are allowed to use all the available channels, the corresponding sum spectral
efficiency achieved at the NE is lower or equal than the sum spectral
efficiency achieved when transmitters can use only one channel. A formal proof
of this observation is provided in the case of small networks. For general
scenarios, we provide numerical examples that show that the same effect holds
as long as the network is kept fully loaded. We conclude the paper by
considering the case of successive interference cancellation at the receiver.
In this context, we show that the power allocation vectors at the NE are
capacity maximizers. Finally, simulations are presented to verify our
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1932</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1932</id><created>2011-12-08</created><authors><author><keyname>Chihani</keyname><forenames>Bachir</forenames></author><author><keyname>Denis</keyname><forenames>Collange</forenames></author></authors><title>A Multipath TCP model for ns-3 simulator</title><categories>cs.NI</categories><comments>Accepted Paper; Workshop on ns-3 held in conjunction with SIMUTools
  2011, Barcelona : Spain (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an implementation of Multipath TCP (MPTCP) under the NS-3 open
source network simulator. MPTCP is a promising extension of TCP currently
considered by the recent eponymous IETF working group, with the objective of
improving the performance of TCP, especially its robustness to variable network
conditions. We describe this new protocol, its main functions and our
implementation in NS-3. Besides this implementation compliant to the current
versions of the IETF drafts, we have also added and compared various packet
reordering mechanisms. We indeed notice that such mechanisms highly improve the
performance of MPTCP. We believe that our implementation could be useful for
future works in MPTCP performance evaluation, especially to compare packet
reordering algorithms or coupling congestion control mechanisms between
subfows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1933</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1933</id><created>2011-12-08</created><authors><author><keyname>Nesme</keyname><forenames>Vincent</forenames><affiliation>LAMA</affiliation></author><author><keyname>Theyssier</keyname><forenames>Guillaume</forenames><affiliation>LAMA</affiliation></author></authors><title>Selfsimilarity, Simulation and Spacetime Symmetries</title><categories>cs.DM nlin.CG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study intrinsic simulations between cellular automata and introduce a new
necessary condition for a CA to simulate another one. Although expressed for
general CA, this condition is targeted towards surjective CA and especially
linear ones. Following the approach introduced by the first author in an
earlier paper, we develop proof techniques to tell whether some linear CA can
simulate another linear CA. Besides rigorous proofs, the necessary condition
for the simulation to occur can be heuristically checked via simple
observations of typical space-time diagrams generated from finite
configurations. As an illustration, we give an example of linear reversible CA
which cannot simulate the identity and which is 'time-asymmetric', i.e. which
can neither simulate its own inverse, nor the mirror of its own inverse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1937</identifier>
 <datestamp>2011-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1937</id><created>2011-12-08</created><authors><author><keyname>Nguyen</keyname><forenames>Sao Mai</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Baranes</keyname><forenames>Adrien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Bootstrapping Intrinsically Motivated Learning with Human Demonstrations</title><categories>cs.LG cs.AI cs.RO</categories><comments>IEEE International Conference on Development and Learning, Frankfurt
  : Germany (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the coupling of internally guided learning and social
interaction, and more specifically the improvement owing to demonstrations of
the learning by intrinsic motivation. We present Socially Guided Intrinsic
Motivation by Demonstration (SGIM-D), an algorithm for learning in continuous,
unbounded and non-preset environments. After introducing social learning and
intrinsic motivation, we describe the design of our algorithm, before showing
through a fishing experiment that SGIM-D efficiently combines the advantages of
social learning and intrinsic motivation to gain a wide repertoire while being
specialised in specific subspaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1945</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1945</id><created>2011-12-08</created><updated>2012-10-10</updated><authors><author><keyname>Bera</keyname><forenames>Suman Kalyan</forenames><affiliation>IBM Research - India New Delhi</affiliation></author><author><keyname>Gupta</keyname><forenames>Shalmoli</forenames><affiliation>Indian Institute of Technology Delhi</affiliation></author><author><keyname>Kumar</keyname><forenames>Amit</forenames><affiliation>Indian Institute of Technology Delhi</affiliation></author><author><keyname>Roy</keyname><forenames>Sambuddha</forenames><affiliation>IBM Research - India New Delhi</affiliation></author></authors><title>Approximation Algorithms for Edge Partitioned Vertex Cover Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a natural generalization of the Partial Vertex Cover problem.
Here an instance consists of a graph G = (V,E), a positive cost function c: V-&gt;
Z^{+}, a partition $P_1,..., P_r$ of the edge set $E$, and a parameter $k_i$
for each partition $P_i$. The goal is to find a minimum cost set of vertices
which cover at least $k_i$ edges from the partition $P_i$. We call this the
Partition Vertex Cover problem. In this paper, we give matching upper and lower
bound on the approximability of this problem. Our algorithm is based on a novel
LP relaxation for this problem. This LP relaxation is obtained by adding
knapsack cover inequalities to a natural LP relaxation of the problem. We show
that this LP has integrality gap of $O(log r)$, where $r$ is the number of sets
in the partition of the edge set. We also extend our result to more general
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1966</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1966</id><created>2011-12-08</created><authors><author><keyname>Sapir</keyname><forenames>Marina</forenames></author></authors><title>Bipartite ranking algorithm for classification and survival analysis</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1108.2820</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised aggregation of independently built univariate predictors is
explored as an alternative regularization approach for noisy, sparse datasets.
Bipartite ranking algorithm Smooth Rank implementing this approach is
introduced. The advantages of this algorithm are demonstrated on two types of
problems. First, Smooth Rank is applied to two-class problems from bio-medical
field, where ranking is often preferable to classification. In comparison
against SVMs with radial and linear kernels, Smooth Rank had the best
performance on 8 out of 12 benchmark benchmarks. The second area of application
is survival analysis, which is reduced here to bipartite ranking in a way which
allows one to use commonly accepted measures of methods performance. In
comparison of Smooth Rank with Cox PH regression and CoxPath methods, Smooth
Rank proved to be the best on 9 out of 10 benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1968</identifier>
 <datestamp>2012-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1968</id><created>2011-12-08</created><updated>2012-07-12</updated><authors><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Vincent</keyname><forenames>Tyrone L.</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Concentration of Measure Inequalities for Toeplitz Matrices with
  Applications</title><categories>cs.IT math.IT</categories><comments>Initial Submission to the IEEE Transactions on Signal Processing on
  December 1, 2011. Revised and Resubmitted on July 12, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive Concentration of Measure (CoM) inequalities for randomized Toeplitz
matrices. These inequalities show that the norm of a high-dimensional signal
mapped by a Toeplitz matrix to a low-dimensional space concentrates around its
mean with a tail probability bound that decays exponentially in the dimension
of the range space divided by a quantity which is a function of the signal. For
the class of sparse signals, the introduced quantity is bounded by the sparsity
level of the signal. However, we observe that this bound is highly pessimistic
for most sparse signals and we show that if a random distribution is imposed on
the non-zero entries of the signal, the typical value of the quantity is
bounded by a term that scales logarithmically in the ambient dimension. As an
application of the CoM inequalities, we consider Compressive Binary Detection
(CBD).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1978</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1978</id><created>2011-12-08</created><updated>2012-03-12</updated><authors><author><keyname>Stokes</keyname><forenames>Klara</forenames></author><author><keyname>Torra</keyname><forenames>Vicen&#xe7;</forenames></author></authors><title>Reidentification and k-anonymity: a model for disclosure risk in graphs</title><categories>cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we provide a formal framework for reidentification in
general. We define n-confusion as a concept for modelling the anonymity of a
database table and we prove that n-confusion is a generalization of k-
anonymity. After a short survey on the different available definitions of k-
anonymity for graphs we provide a new definition for k-anonymous graph, which
we consider to be the correct definition. We provide a description of the
k-anonymous graphs, both for the regular and the non-regular case. We also
introduce the more flexible concept of (k,l)-anonymous graph. Our definition of
(k,l)-anonymous graph is meant to replace a previous definition of (k,
l)-anonymous graph, which we here prove to have severe weaknesses. Finally we
provide a set of algorithms for k-anonymization of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1989</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1989</id><created>2011-12-08</created><authors><author><keyname>Yang</keyname><forenames>C.</forenames></author><author><keyname>Jiang</keyname><forenames>C.</forenames></author><author><keyname>Wang</keyname><forenames>J.</forenames></author></authors><title>Coded Single-Tone Signaling and Its Application to Resource Coordination
  and Interference Management in Femtocell Networks</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1103.2503</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Resource coordination and interference management is the key to achieving the
benefits of femtocell networks. Over-the-air signaling is one of the most
effective means for distributed dynamic resource coordination and interference
management. However, the design of this type of signal is challenging. In this
paper, we address the challenges and propose an effective solution, referred to
as coded single-tone signaling (STS). The proposed coded STS scheme possesses
certain highly desirable properties, such as no dedicated resource requirement
(no overhead), no near-and-far effect, no inter-signal interference (no
multi-user interference), low peak-to-average power ratio (deep coverage). In
addition, the proposed coded STS can fully exploit frequency diversity and
provides a means for high quality wideband channel estimation. The coded STS
design is demonstrated through a concrete numerical example. Performance of the
proposed coded STS and its effect on cochannel traffic channels are evaluated
through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1990</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1990</id><created>2011-12-08</created><authors><author><keyname>Jiang</keyname><forenames>C.</forenames></author><author><keyname>Yang</keyname><forenames>L.</forenames></author></authors><title>Efficient Neighbor Discovery for Proximity-Aware Networks</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work, we propose a fast and energy-efficient neighbor discovery
scheme for proximity-aware networks such as wireless ad hoc networks. Discovery
efficiency is accomplished by the use of a special discovery signal that
provides random multiple access with low transmit power consumption and low
synchronization requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1994</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1994</id><created>2011-12-08</created><updated>2012-04-06</updated><authors><author><keyname>Grigorescu</keyname><forenames>Elena</forenames></author><author><keyname>Peikert</keyname><forenames>Chris</forenames></author></authors><title>List Decoding Barnes-Wall Lattices</title><categories>cs.IT cs.CC cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of list decoding error-correcting codes over finite fields
(under the Hamming metric) has been widely studied in recent years. Motivated
by the similar discrete structure of linear codes and point lattices in R^N,
and their many shared applications across complexity theory, cryptography, and
coding theory, we initiate the study of list decoding for lattices. Namely: for
a lattice L in R^N, given a target vector r in R^N and a distance parameter d,
output the set of all lattice points w in L that are within distance d of r.
  In this work we focus on combinatorial and algorithmic questions related to
list decoding for the well-studied family of Barnes-Wall lattices. Our main
contributions are twofold:
  1) We give tight (up to polynomials) combinatorial bounds on the worst-case
list size, showing it to be polynomial in the lattice dimension for any error
radius bounded away from the lattice's minimum distance (in the Euclidean
norm).
  2) Building on the unique decoding algorithm of Micciancio and Nicolosi (ISIT
'08), we give a list-decoding algorithm that runs in time polynomial in the
lattice dimension and worst-case list size, for any error radius. Moreover, our
algorithm is highly parallelizable, and with sufficiently many processors can
run in parallel time only poly-logarithmic in the lattice dimension.
  In particular, our results imply a polynomial-time list-decoding algorithm
for any error radius bounded away from the minimum distance, thus beating a
typical barrier for error-correcting codes posed by the Johnson radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.1996</identifier>
 <datestamp>2012-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.1996</id><created>2011-12-08</created><updated>2012-02-16</updated><authors><author><keyname>Bierkens</keyname><forenames>Joris</forenames></author><author><keyname>Kappen</keyname><forenames>Bert</forenames></author></authors><title>KL-learning: Online solution of Kullback-Leibler control problems</title><categories>math.OC cs.AI</categories><msc-class>93E35, 15B48</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a stochastic approximation method for the solution of an ergodic
Kullback-Leibler control problem. A Kullback-Leibler control problem is a
Markov decision process on a finite state space in which the control cost is
proportional to a Kullback-Leibler divergence of the controlled transition
probabilities with respect to the uncontrolled transition probabilities. The
algorithm discussed in this work allows for a sound theoretical analysis using
the ODE method. In a numerical experiment the algorithm is shown to be
comparable to the power method and the related Z-learning algorithm in terms of
convergence speed. It may be used as the basis of a reinforcement learning
style algorithm for Markov decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2000</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2000</id><created>2011-12-08</created><updated>2012-06-12</updated><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>A discrepancy lower bound for information complexity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides the first general technique for proving information lower
bounds on two-party unbounded-rounds communication problems. We show that the
discrepancy lower bound, which applies to randomized communication complexity,
also applies to information complexity. More precisely, if the discrepancy of a
two-party function $f$ with respect to a distribution $\mu$ is $Disc_\mu f$,
then any two party randomized protocol computing $f$ must reveal at least
$\Omega(\log (1/Disc_\mu f))$ bits of information to the participants. As a
corollary, we obtain that any two-party protocol for computing a random
function on $\{0,1\}^n\times\{0,1\}^n$ must reveal $\Omega(n)$ bits of
information to the participants.
  In addition, we prove that the discrepancy of the Greater-Than function is
$\Omega(1/\sqrt{n})$, which provides an alternative proof to the recent proof
of Viola \cite{Viola11} of the $\Omega(\log n)$ lower bound on the
communication complexity of this well-studied function and, combined with our
main result, proves the tight $\Omega(\log n)$ lower bound on its information
complexity.
  The proof of our main result develops a new simulation procedure that may be
of an independent interest. In a very recent breakthrough work of Kerenidis et
al. \cite{kerenidis2012lower}, this simulation procedure was the main building
block for proving that almost all known lower bound techniques for
communication complexity (and not just discrepancy) apply to information
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2012</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2012</id><created>2011-12-08</created><authors><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author></authors><title>Lie algebra conjugacy</title><categories>cs.CC cs.DS cs.SC math.RT</categories><msc-class>68Q25 (Primary) 22E46 (Secondary)</msc-class><acm-class>F.2.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of matrix Lie algebra conjugacy. Lie algebras arise
centrally in areas as diverse as differential equations, particle physics,
group theory, and the Mulmuley--Sohoni Geometric Complexity Theory program. A
matrix Lie algebra is a set L of matrices such that $A, B\in L$ implies $AB -
BA \in L$. Two matrix Lie algebras are conjugate if there is an invertible
matrix $M$ such that $L_1 = M L_2 M^{-1}$.
  We show that certain cases of Lie algebra conjugacy are equivalent to graph
isomorphism. On the other hand, we give polynomial-time algorithms for other
cases of Lie algebra conjugacy, which allow us to essentially derandomize a
recent result of Kayal on affine equivalence of polynomials. Affine equivalence
is related to many complexity problems such as factoring integers, graph
isomorphism, matrix multiplication, and permanent versus determinant.
  Specifically, we show:
  Abelian Lie algebra conjugacy is equivalent to the code equivalence problem,
and hence is as hard as graph isomorphism.
  Abelian Lie algebra conjugacy of $n \times n$ matrices can be solved in
poly(n) time when the Lie algebras have dimension O(1).
  Semisimple Lie algebra conjugacy is equivalent to graph isomorphism. A Lie
algebra is semisimple if it is a direct sum of simple Lie algebras.
  Semisimple Lie algebra conjugacy of $n \times n$ matrices can be solved in
polynomial time when the Lie algebras consist of only $O(\log n)$ simple direct
summands.
  Conjugacy of completely reducible Lie algebras---that is, a direct sum of an
abelian and a semisimple Lie algebra---can be solved in polynomial time when
the abelian part has dimension O(1) and the semisimple part has $O(\log n)$
simple direct summands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2015</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2015</id><created>2011-12-08</created><authors><author><keyname>Sharma</keyname><forenames>Anamika</forenames></author></authors><title>A Framework for Picture Extraction on Search Engine Improved and
  Meaningful Result</title><categories>cs.IR</categories><comments>5 pages,1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching is an important tool of information gathering, if information is in
the form of picture than it play a major role to take quick action and easy to
memorize. This is a human tendency to retain more picture than text. The
complexity and the occurrence of variety of query can give variation in result
and provide the humans to learn something new or get confused. This paper
presents a development of a framework that will focus on recourse
identification for the user so that they can get faster access with accurate &amp;
concise results on time and analysis of the change that is evident as the
scenario changes from text to picture retrieval. This paper also provides a
glimpse how to get accurate picture information in advance and extended
technologies searching framework. The new challenges and design techniques of
picture retrieval systems are also suggested in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2020</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2020</id><created>2011-12-09</created><authors><author><keyname>Chen</keyname><forenames>Rui</forenames></author><author><keyname>Fung</keyname><forenames>Benjamin C. M.</forenames></author><author><keyname>Desai</keyname><forenames>Bipin C.</forenames></author></authors><title>Differentially Private Trajectory Data Publication</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing prevalence of location-aware devices, trajectory data has
been generated and collected in various application domains. Trajectory data
carries rich information that is useful for many data analysis tasks. Yet,
improper publishing and use of trajectory data could jeopardize individual
privacy. However, it has been shown that existing privacy-preserving trajectory
data publishing methods derived from partition-based privacy models, for
example k-anonymity, are unable to provide sufficient privacy protection.
  In this paper, motivated by the data publishing scenario at the Societe de
transport de Montreal (STM), the public transit agency in Montreal area, we
study the problem of publishing trajectory data under the rigorous differential
privacy model. We propose an efficient data-dependent yet differentially
private sanitization algorithm, which is applicable to different types of
trajectory data. The efficiency of our approach comes from adaptively narrowing
down the output domain by building a noisy prefix tree based on the underlying
data. Moreover, as a post-processing step, we make use of the inherent
constraints of a prefix tree to conduct constrained inferences, which lead to
better utility. This is the first paper to introduce a practical solution for
publishing large volume of trajectory data under differential privacy. We
examine the utility of sanitized data in terms of count queries and frequent
sequential pattern mining. Extensive experiments on real-life trajectory data
from the STM demonstrate that our approach maintains high utility and is
scalable to large trajectory datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2021</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2021</id><created>2011-12-09</created><authors><author><keyname>Das</keyname><forenames>Debasis</forenames></author><author><keyname>Misra</keyname><forenames>Rajiv</forenames></author></authors><title>Programmable Cellular Automata Based Efficient Parallel AES Encryption
  Algorithm</title><categories>cs.DC cs.NI nlin.CG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cellular Automata(CA) is a discrete computing model which provides simple,
flexible and efficient platform for simulating complicated systems and
performing complex computation based on the neighborhoods information. CA
consists of two components 1) a set of cells and 2) a set of rules .
Programmable Cellular Automata(PCA) employs some control signals on a Cellular
Automata(CA) structure. Programmable Cellular Automata were successfully
applied for simulation of biological systems, physical systems and recently to
design parallel and distributed algorithms for solving task density and
synchronization problems. In this paper PCA is applied to develop cryptography
algorithms. This paper deals with the cryptography for a parallel AES
encryption algorithm based on programmable cellular automata. This proposed
algorithm based on symmetric key systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2024</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2024</id><created>2011-12-09</created><authors><author><keyname>Sureshkumar</keyname><forenames>K.</forenames></author><author><keyname>Rajalakshmi</keyname><forenames>R.</forenames></author><author><keyname>Vetrikanimozhi</keyname><forenames>A.</forenames></author></authors><title>Channel Estimation for MIMO MC-CDMA Systems</title><categories>cs.NI</categories><doi>10.5121/ijdps.2011.2629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concepts of MIMO MC-CDMA are not new but the new technologies to improve
their functioning are an emerging area of research. In general, most mobile
communication systems transmit bits of information in the radio space to the
receiver. The radio channels in mobile radio systems are usually multipath
fading channels, which cause inter-symbol interference (ISI) in the received
signal. To remove ISI from the signal, there is a need of strong equalizer. In
this thesis we have focused on simulating the MIMO MC-CDMA systems in MATLAB
and designed the channel estimation for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2025</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2025</id><created>2011-12-09</created><authors><author><keyname>Yee</keyname><forenames>Tin Tin</forenames></author><author><keyname>Naing</keyname><forenames>Thinn Thu</forenames></author></authors><title>PC-Cluster based Storage System Architecture for Cloud Storage</title><categories>cs.DC</categories><journal-ref>International Journal on Cloud Computing: Services and
  Architecture(IJCCSA),Vol.1, No.3, November 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Design and architecture of cloud storage system plays a vital role in cloud
computing infrastructure in order to improve the storage capacity as well as
cost effectiveness. Usually cloud storage system provides users to efficient
storage space with elasticity feature. One of the challenges of cloud storage
system is difficult to balance the providing huge elastic capacity of storage
and investment of expensive cost for it. In order to solve this issue in the
cloud storage infrastructure, low cost PC cluster based storage server is
configured to be activated for large amount of data to provide cloud users.
Moreover, one of the contributions of this system is proposed an analytical
model using M/M/1 queuing network model, which is modeled on intended
architecture to provide better response time, utilization of storage as well as
pending time when the system is running. According to the analytical result on
experimental testing, the storage can be utilized more than 90% of storage
space. In this paper, two parts have been described such as (i) design and
architecture of PC cluster based cloud storage system. On this system, related
to cloud applications, services configurations are explained in detailed. (ii)
Analytical model has been enhanced to be increased the storage utilization on
the target architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2026</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2026</id><created>2011-12-09</created><authors><author><keyname>S</keyname><forenames>Vijaykumar</forenames></author><author><keyname>G</keyname><forenames>Saravanakumar S</forenames></author></authors><title>Future Robotics Database Management System along with Cloud TPS</title><categories>cs.DB cs.RO</categories><comments>12 pages,5 figures,First model; International Journal on Cloud
  Computing: Services and Architecture(IJCCSA),Vol.1, No.3, November 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper deals with memory management issues of robotics. In our proposal
we break one of the major issues in creating humanoid. . Database issue is the
complicated thing in robotics schema design here in our proposal we suggest new
concept called NOSQL database for the effective data retrieval, so that the
humanoid robots will get the massive thinking ability in searching each items
using chained instructions. For query transactions in robotics we need an
effective consistency transactions so by using latest technology called
CloudTPS which guarantees full ACID properties so that the robot can make their
queries using multi-item transactions through this we obtain data consistency
in data retrievals. In addition we included map reduce concepts it can splits
the job to the respective workers so that it can process the data in a parallel
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2027</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2027</id><created>2011-12-09</created><authors><author><keyname>Lim</keyname><forenames>JaeDeok</forenames></author><author><keyname>Choi</keyname><forenames>ByeongCheol</forenames></author><author><keyname>Han</keyname><forenames>SeungWan</forenames></author><author><keyname>Lee</keyname><forenames>ChoelHoon</forenames></author></authors><title>Automatic Classification of X-rated Videos using Obscene Sound Analysis
  based on a Repeated Curve-like Spectrum Feature</title><categories>cs.MM</categories><comments>18 pages, 5 figures, 11 tables, IJMA(The International Journal of
  Multimedia &amp; Its Applications)</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.4, November 2011, pp.1-17</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the automatic classification of X-rated videos by
analyzing its obscene sounds. In this paper, obscene sounds refer to audio
signals generated from sexual moans and screams during sexual scenes. By
analyzing various sound samples, we determined the distinguishable
characteristics of obscene sounds and propose a repeated curve-like spectrum
feature that represents the characteristics of such sounds. We constructed
6,269 audio clips to evaluate the proposed feature, and separately constructed
1,200 X-rated and general videos for classification. The proposed feature has
an F1-score, precision, and recall rate of 96.6%, 98.2%, and 95.2%,
respectively, for the original dataset, and 92.6%, 97.6%, and 88.0% for a noisy
dataset of 5dB SNR. And, in classifying videos, the feature has more than a 90%
F1-score, 97% precision, and an 84% recall rate. From the measured performance,
X-rated videos can be classified with only the audio features and the repeated
curve-like spectrum feature is suitable to detect obscene sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2028</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2028</id><created>2011-12-09</created><authors><author><keyname>Nigam</keyname><forenames>Bhawna</forenames></author><author><keyname>Ahirwal</keyname><forenames>Poorvi</forenames></author><author><keyname>Salve</keyname><forenames>Sonal</forenames></author><author><keyname>Vamney</keyname><forenames>Swati</forenames></author></authors><title>Document Classification Using Expectation Maximization with Semi
  Supervised Learning</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the amount of online document increases, the demand for document
classification to aid the analysis and management of document is increasing.
Text is cheap, but information, in the form of knowing what classes a document
belongs to, is expensive. The main purpose of this paper is to explain the
expectation maximization technique of data mining to classify the document and
to learn how to improve the accuracy while using semi-supervised approach.
Expectation maximization algorithm is applied with both supervised and
semi-supervised approach. It is found that semi-supervised approach is more
accurate and effective. The main advantage of semi supervised approach is
&quot;Dynamically Generation of New Class&quot;. The algorithm first trains a classifier
using the labeled document and probabilistically classifies the unlabeled
documents. The car dataset for the evaluation purpose is collected from UCI
repository dataset in which some changes have been done from our side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2031</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2031</id><created>2011-12-09</created><authors><author><keyname>Haribhakta</keyname><forenames>Y. V.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Dr. Parag</forenames></author></authors><title>Learning Context for Text Categorization</title><categories>cs.IR</categories><comments>9 pages, selected in IJDKP (International Journal of Data Mining and
  Knowledge Management Process)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our work which is based on discovering context for text
document categorization. The document categorization approach is derived from a
combination of a learning paradigm known as relation extraction and an
technique known as context discovery. We demonstrate the effectiveness of our
categorization approach using reuters 21578 dataset and synthetic real world
data from sports domain. Our experimental results indicate that the learned
context greatly improves the categorization performance as compared to
traditional categorization approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2038</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2038</id><created>2011-12-09</created><authors><author><keyname>Meenakshi</keyname><forenames>A. V.</forenames></author><author><keyname>Punitham</keyname><forenames>V.</forenames></author><author><keyname>Kayalvizhi</keyname><forenames>R.</forenames></author><author><keyname>Asha</keyname><forenames>S.</forenames></author></authors><title>Fast DOA estimation using wavelet denoising on MIMO fading channel</title><categories>cs.NI cs.IT math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tool for the analysis, and simulation of
direction-of-arrival (DOA) estimation in wireless mobile communication systems
over the fading channel. It reviews two methods of Direction of arrival (DOA)
estimation algorithm. The standard Multiple Signal Classification (MUSIC) can
be obtained from the subspace based methods. In improved MUSIC procedure called
Cyclic MUSIC, it can automatically classify the signals as desired and
undesired based on the known spectral correlation property and estimate only
the desired signal's DOA. In this paper, the DOA estimation algorithm using the
de-noising pre-processing based on time-frequency conversion analysis was
proposed, and the performances were analyzed. This is focused on the
improvement of DOA estimation at a lower SNR and interference environment. This
paper provides a fairly complete image of the performance and statistical
efficiency of each of above two methods with QPSK signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2039</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2039</id><created>2011-12-09</created><authors><author><keyname>El-Khouly</keyname><forenames>Mahmoud Mohamed</forenames></author></authors><title>ECAKP: Encrypt Collect Authenticate Kill Play</title><categories>cs.CR</categories><comments>8 pages, 1 figure, 2 tables</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  ISSN: 0975-5578 (online), 0975-5934 (print). 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are taught from a young age that plagiarism (copying other's work) is
wrong. However, the problem of Illegal copies of multimedia data is exacerbated
by the widespread availability of circumvention devices, which enable people to
make infringing copies of multimedia data. Recently, Joint Video Compression
and Encryption (JVCE) has gained increased attention to reduce the
computational complexity of video compression, as well as provide encryption of
multimedia data. In this paper, a novel protection method for multimedia data
(ECAKP) is proposed. It combines encryption process and compression with
authenticating process. The method had been implemented and the results are
discussed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2040</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2040</id><created>2011-12-09</created><authors><author><keyname>V</keyname><forenames>Vijayakumar</forenames></author><author><keyname>R</keyname><forenames>Nedunchezhian</forenames></author></authors><title>Recent Trends and Research Issues in Video Association Mining</title><categories>cs.MM cs.DB</categories><comments>13 pages; 1 Figure; 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ever-growing digital libraries and video databases, it is
increasingly important to understand and mine the knowledge from video database
automatically. Discovering association rules between items in a large video
database plays a considerable role in the video data mining research areas.
Based on the research and development in the past years, application of
association rule mining is growing in different domains such as surveillance,
meetings, broadcast news, sports, archives, movies, medical data, as well as
personal and online media collections. The purpose of this paper is to provide
general framework of mining the association rules from video database. This
article is also represents the research issues in video association mining
followed by the recent trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2042</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2042</id><created>2011-12-09</created><authors><author><keyname>Alomari</keyname><forenames>Saleh Ali</forenames></author><author><keyname>Sumari</keyname><forenames>Putra</forenames></author></authors><title>Statistical Information of the Increased Demand for Watch the VOD with
  the Increased Sophistication in the Mobile Devices,Communications and
  Internet Penetration in Asia</title><categories>cs.MM</categories><comments>17 pages, 17 figures, 4 tables; The International Journal of
  Multimedia &amp; Its Applications (IJMA) Vol.3, No.4, November 2011</comments><doi>10.5121/ijma.2011.3415</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As the rapid progress of the media streaming applications such as video
streaming can be classified into two types of streaming, Live video streaming,
Video on Demand (VoD). Live video streaming is a service which allows the
clients to watch many TV channels over the internet and the clients able to use
one operation to perform is to switch the channels. Video on Demand (VoD) is
one of the most important applications for the internet of the future and has
become an interactive multimedia service which allows the users to start
watching the video of their choice at anytime and anywhere, especially after
the rapid deployment of the wireless networks and mobile devices. In this paper
provide statistical information about the Internet, communications and mobile
devices etc. This has led to an increased demand for the development,
communication and computational powers of many of the mobile wireless
subscribers/mobile devices such as laptops, PDAs, smart phones and notebook.
These techniques are utilized to obtain a video on demand service with higher
resolution and quality. Another objective in this paper is to see Malaysia
ranked as a fully developed country by the year 2020.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2044</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2044</id><created>2011-12-09</created><authors><author><keyname>Zacharia</keyname><forenames>Kurien</forenames></author><author><keyname>Elias</keyname><forenames>Eldo P.</forenames></author><author><keyname>Varghese</keyname><forenames>Surekha Mariam</forenames></author></authors><title>Modelling Gesture Based Ubiquitous Applications</title><categories>cs.MM</categories><comments>10 pages; The International Journal of Multimedia &amp; Its Applications
  (IJMA) Vol.3, No.4, November 2011</comments><msc-class>68U20, 68U05</msc-class><acm-class>H.5.2; I.3.7</acm-class><doi>10.5121/ijma.2011.3403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cost effective, gesture based modelling technique called Virtual
Interactive Prototyping (VIP) is described in this paper. Prototyping is
implemented by projecting a virtual model of the equipment to be prototyped.
Users can interact with the virtual model like the original working equipment.
For capturing and tracking the user interactions with the model image and sound
processing techniques are used. VIP is a flexible and interactive prototyping
method that has much application in ubiquitous computing environments.
Different commercial as well as socio-economic applications and extension to
interactive advertising of VIP are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2046</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2046</id><created>2011-12-09</created><authors><author><keyname>Dalal</keyname><forenames>Purvang</forenames></author><author><keyname>Kothari</keyname><forenames>Nikhil</forenames></author><author><keyname>Dasgupta</keyname><forenames>K. S.</forenames></author></authors><title>Improving TCP Performance over Wireless Network with Frequent
  Disconnections</title><categories>cs.NI cs.PF</categories><comments>16 Pages, 11 Figures; International Journal of Computer Networks &amp;
  Communications (IJCNC) Vol.3, No.6, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presented in this paper is the solution to the problem that arises when the
TCP/IP protocol suite is used to provide Internet connectivity through mobile
terminals over emerging 802.11 wireless links. Taking into consideration the
strong drive towards wireless Internet access through mobile terminals, the
problem of frequent disconnections causing serial timeouts is examined and
analyzed, with the help of extensive simulations. After a detailed review of
wireless link loss recovery mechanism and identification of related problems, a
new scheme with modifications at link layer and transport layer is proposed.
The proposed modifications which depend on interaction between two layers (i)
reduce the idle time before transmission at TCP by preventing timeout
occurrences and (ii) decouple the congestion control from recovery of the
losses due to link failure. Results of simulation based experiments demonstrate
considerable performance improvement with the proposed modifications over the
conventional TCP, when a wireless sender is experiencing frequent link
failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2058</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2058</id><created>2011-12-09</created><authors><author><keyname>Arora</keyname><forenames>Ojuswini</forenames></author><author><keyname>Garg</keyname><forenames>Dr. Amit kumar</forenames></author><author><keyname>Punia</keyname><forenames>Savita</forenames></author></authors><title>Symmetrical Dispersion Compensation For High Speed Optical Links</title><categories>cs.NI</categories><comments>6 pages, 6 figures, IJCSI journal</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 6, No 1, November 2011 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performance of high speed optical fiber based network is
analysed by using dispersion compensating module (DCM). The optimal operating
condition of the DCM is obtained by considering dispersion management
configurations for the symmetrical system i.e Pre-compensation &amp;
Post-compensation. The dispersion compensating fiber (DCF) is tested for a
single span, single channel system operating at a speed of 10 Gb/s with a
transmitting wavelength of 1550 nm, over 120 km single mode fibre by using the
compensating fiber for 24 km,30km and 35Km. So far, most of the investigations
for single mode fiber (SMF) transmission at high amplifier spacings in the
order of 90 km to 120 km is focused on conventional Non Return to Zero(NRZ)
format. The simulation results are validated by analysing the Q-factor and Bit
error rate (BER) in the numerical simulator OptSim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2067</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2067</id><created>2011-12-09</created><authors><author><keyname>A</keyname><forenames>Bhuvaneswari.</forenames></author><author><keyname>Karpagam</keyname><forenames>Dr. G. R.</forenames></author></authors><title>Ontology-Based Emergency Management System in a Social Cloud</title><categories>cs.SI</categories><comments>International Journal on Cloud Computing: Services and
  Architecture(IJCCSA),Vol.1, No.3, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for Emergency Management continually grows as the population and
exposure to catastrophic failures increase. The ability to offer appropriate
services at these emergency situations can be tackled through group
communication mechanisms. The entities involved in the group communication
include people, organizations, events, locations and essential services. Cloud
computing is a &quot;as a service&quot; style of computing that enables on-demand network
access to a shared pool of resources. So this work focuses on proposing a
social cloud constituting group communication entities using an open source
platform, Eucalyptus. The services are exposed as semantic web services, since
the availability of machine-readable metadata (Ontology) will enable the access
of these services more intelligently. The objective of this paper is to propose
an Ontology-based Emergency Management System in a social cloud and demonstrate
the same using emergency healthcare domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2071</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2071</id><created>2011-12-09</created><authors><author><keyname>Chabi</keyname><forenames>Anja Habacha</forenames></author><author><keyname>Kboubi</keyname><forenames>Ferihane</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed Ben</forenames></author></authors><title>Thematic Analysis and Visualization of Textual Corpus</title><categories>cs.IR</categories><comments>16 pages,9 figures</comments><journal-ref>International Journal of Computer Science &amp; Engineering Survey
  (IJCSES), November 2011, Volume 2 Number 4, ISSN : 0976-2760 (Online)
  0976-3252 (Print) go</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic analysis of documents is a domain of intense research at
present. The works in this domain can take several directions and touch several
levels of granularity. In the present work we are exactly interested in the
thematic analysis of the textual documents. In our approach, we suggest
studying the variation of the theme relevance within a text to identify the
major theme and all the minor themes evoked in the text. This allows us at the
second level of analysis to identify the relations of thematic associations in
a textual corpus. Through the identification and the analysis of these
association relations we suggest generating thematic paths allowing users,
within the frame work of information search system, to explore the corpus
according to their themes of interest and to discover new knowledge by
navigating in the thematic association relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2079</identifier>
 <datestamp>2012-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2079</id><created>2011-12-09</created><updated>2012-07-11</updated><authors><author><keyname>Paparo</keyname><forenames>G. D.</forenames></author><author><keyname>Martin-Delgado</keyname><forenames>M. A.</forenames></author></authors><title>Google in a Quantum Network</title><categories>quant-ph cond-mat.stat-mech cs.NI</categories><comments>RevTeX 4 file, color figures</comments><journal-ref>Sci. Rep. 2, 444; (2012)</journal-ref><doi>10.1038/srep00444</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the characterization of a class of quantum PageRank algorithms
in a scenario in which some kind of quantum network is realizable out of the
current classical internet web, but no quantum computer is yet available. This
class represents a quantization of the PageRank protocol currently employed to
list web pages according to their importance. We have found an instance of this
class of quantum protocols that outperforms its classical counterpart and may
break the classical hierarchy of web pages depending on the topology of the
web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2095</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2095</id><created>2011-12-09</created><authors><author><keyname>Nguyen</keyname><forenames>Sao Mai</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Ogino</keyname><forenames>Masaki</forenames></author><author><keyname>Asada</keyname><forenames>Minoru</forenames></author></authors><title>Real-time face swapping as a tool for understanding infant
  self-recognition</title><categories>cs.AI cs.CV</categories><proxy>ccsd</proxy><journal-ref>International Conference on Epigenetic Robotics, Glumslov : Sweden
  (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To study the preference of infants for contingency of movements and
familiarity of faces during self-recognition task, we built, as an accurate and
instantaneous imitator, a real-time face- swapper for videos. We present a
non-constraint face-swapper based on 3D visual tracking that achieves real-time
performance through parallel computing. Our imitator system is par- ticularly
suited for experiments involving children with Autistic Spectrum Disorder who
are often strongly disturbed by the constraints of other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2109</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2109</id><created>2011-12-09</created><authors><author><keyname>Sarala</keyname><forenames>B.</forenames></author><author><keyname>Venkateswarulu</keyname><forenames>D. S.</forenames></author></authors><title>MC CDMA PAPR Reduction Techniques using Discrete Transforms and
  Companding</title><categories>cs.NI</categories><comments>18 pages,16 figures; International journal of Distributed and
  Parallel Systems volume 2, number 6, Novmber 2011,ISSN:0976-9757</comments><report-no>55bs</report-no><msc-class>14J94 (Primary)</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  High Peak to Average Power Ratio (PAPR) of the transmitted signal is a
serious problem in multicarrier modulation systems. In this paper a new
technique for reduction in PAPR of the Multicarrier Code Division Multiple
Access (MC CDMA) signals based on combining the Discrete Transform either
Discrete Cosine Transform (DCT) or multi-resolution Discrete Wavelet Transform
(DWT) with companding is proposed. It is analyzed and implemented using MATLAB.
Simulation results of reduction in PAPR and power Spectral Density (PSD) of the
MC CDMA with companding and without companding are compared with the MC CDMA
with DCT and companding, DWT and companding systems. The new technique proposed
is to make use of multi-resolution DWT in combination with companding in order
to achieve a very substantial reduction in PAPR of the MC CDMA signal
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2112</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2112</id><created>2011-12-09</created><updated>2012-05-30</updated><authors><author><keyname>Kishore</keyname><forenames>Vimal</forenames></author><author><keyname>Santhanam</keyname><forenames>M. S.</forenames></author><author><keyname>Amritkar</keyname><forenames>R. E.</forenames></author></authors><title>Extreme events and event size fluctuations in biased random walks on
  networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><journal-ref>Phys. Rev. E 85, 056120 (2012)</journal-ref><doi>10.1103/PhysRevE.85.056120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random walk on discrete lattice models is important to understand various
types of transport processes. The extreme events, defined as exceedences of the
flux of walkers above a prescribed threshold, have been studied recently in the
context of complex networks. This was motivated by the occurrence of rare
events such as traffic jams, floods, and power black-outs which take place on
networks. In this work, we study extreme events in a generalized random walk
model in which the walk is preferentially biased by the network topology. The
walkers preferentially choose to hop toward the hubs or small degree nodes. In
this setting, we show that extremely large fluctuations in event-sizes are
possible on small degree nodes when the walkers are biased toward the hubs. In
particular, we obtain the distribution of event-sizes on the network. Further,
the probability for the occurrence of extreme events on any node in the network
depends on its 'generalized strength', a measure of the ability of a node to
attract walkers. The 'generalized strength' is a function of the degree of the
node and that of its nearest neighbors. We obtain analytical and simulation
results for the probability of occurrence of extreme events on the nodes of a
network using a generalized random walk model. The result reveals that the
nodes with a larger value of 'generalized strength', on average, display lower
probability for the occurrence of extreme events compared to the nodes with
lower values of 'generalized strength'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2113</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2113</id><created>2011-12-09</created><authors><author><keyname>Kompella</keyname><forenames>Varun Raj</forenames></author><author><keyname>Luciw</keyname><forenames>Matthew</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Incremental Slow Feature Analysis: Adaptive and Episodic Learning from
  High-Dimensional Input Streams</title><categories>cs.AI</categories><journal-ref>Neural Computation, 2012, Vol. 24, No. 11, Pages 2994-3024</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slow Feature Analysis (SFA) extracts features representing the underlying
causes of changes within a temporally coherent high-dimensional raw sensory
input signal. Our novel incremental version of SFA (IncSFA) combines
incremental Principal Components Analysis and Minor Components Analysis. Unlike
standard batch-based SFA, IncSFA adapts along with non-stationary environments,
is amenable to episodic training, is not corrupted by outliers, and is
covariance-free. These properties make IncSFA a generally useful unsupervised
preprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA
and MCA updates take the form of Hebbian and anti-Hebbian updating, extending
the biological plausibility of SFA. In both single node and deep network
versions, IncSFA learns to encode its input streams (such as high-dimensional
video) by informative slow features representing meaningful abstract
environmental properties. It can handle cases where batch SFA fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2117</identifier>
 <datestamp>2011-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2117</id><created>2011-12-09</created><updated>2011-12-13</updated><authors><author><keyname>Chen</keyname><forenames>Robert W.</forenames></author><author><keyname>Rosenberg</keyname><forenames>Burton</forenames></author></authors><title>How to Lose with Least Probability</title><categories>math.PR cs.GT math.CO</categories><comments>15 pages, 6 tables of values, 2 Mathematica programs, version 2</comments><msc-class>60G40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two players alternate tossing a biased coin where the probability of getting
heads is p. The current player is awarded alpha points for tails and alpha+beta
for heads. The first player reaching n points wins. For a completely unfair
coin the player going first certainly wins. For other coin biases, the player
going first has the advantage, but the advantage depends on the coin bias. We
calculate the first player's advantage and the coin bias minimizing this
advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2118</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2118</id><created>2011-12-09</created><authors><author><keyname>Goerdt</keyname><forenames>Andreas</forenames></author><author><keyname>Falke</keyname><forenames>Lutz</forenames></author></authors><title>Satisfiability thresholds beyond k-XORSAT</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider random systems of equations x_1 + ... + x_k = a; 0 &lt;= a &lt;= 2
which are interpreted as equations modulo 3: We show for k &gt;= 15 that the
satisfiability threshold of such systems occurs where the 2-core has density 1:
We show a similar result for random uniquely extendible constraints over 4
elements. Our results extend previous results of Dubois/Mandler for equations
mod 2 and k = 3 and Connamacher/Molloy for uniquely extendible constraints over
a domain of 4 elements with k = 3 arguments. Our proof technique is based on
variance calculations, using a technique introduced Dubois/Mandler. However,
several additional observations (of independent interest) are necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2127</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2127</id><created>2011-12-08</created><authors><author><keyname>Yampolskiy</keyname><forenames>Roman V.</forenames></author></authors><title>Efficiency Theory: a Unifying Theory for Information, Computation and
  Intelligence</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper serves as the first contribution towards the development of the
theory of efficiency: a unifying framework for the currently disjoint theories
of information, complexity, communication and computation. Realizing the
defining nature of the brute force approach in the fundamental concepts in all
of the above mentioned fields, the paper suggests using efficiency or
improvement over the brute force algorithm as a common unifying factor
necessary for the creation of a unified theory of information manipulation. By
defining such diverse terms as randomness, knowledge, intelligence and
computability in terms of a common denominator we are able to bring together
contributions from Shannon, Levin, Kolmogorov, Solomonoff, Chaitin, Yao and
many others under a common umbrella of the efficiency theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2128</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2128</id><created>2011-11-23</created><authors><author><keyname>Vasudevan</keyname><forenames>Rangarajan A.</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Novel Multipath Approach to Security in Mobile Ad Hoc Networks
  (MANETs)</title><categories>cs.CR</categories><comments>4 Pages</comments><journal-ref>Proceedings of International Conference on Computers and Devices
  for Communication (CODEC'04), January, 2004, Kolkata, India. pp.
  CNA_0412_CO_F_1 to CNA_0412_CO_F_4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel encryption-less algorithm to enhance
security in transmission of data packets across mobile ad hoc networks. The
paper hinges on the paradigm of multipath routing and exploits the properties
of polynomials. The first step in the algorithm is to transform the data such
that it is impossible to obtain any information without possessing the entire
transformed data. The algorithm then uses an intuitively simple idea of a
jigsaw puzzle to break the transformed data into multiple packets where these
packets form the pieces of the puzzle. Then these packets are sent along
disjoint paths to reach the receiver. A secure and efficient mechanism is
provided to convey the information that is necessary for obtaining the original
data at the receiver-end from its fragments in the packets, that is, for
solving the jigsaw puzzle. The algorithm is designed to be secure so that no
intermediate or unintended node can obtain the entire data. An authentication
code is also used to ensure authenticity of every packet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2137</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2137</id><created>2011-12-09</created><authors><author><keyname>Ibrahim</keyname><forenames>S. P. Syed</forenames></author><author><keyname>Chandran</keyname><forenames>K. R.</forenames></author></authors><title>Compact Weighted Class Association Rule Mining using Information Gain</title><categories>cs.DB</categories><comments>13 pages; International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.1, No.6, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted association rule mining reflects semantic significance of item by
considering its weight. Classification constructs the classifier and predicts
the new data instance. This paper proposes compact weighted class association
rule mining method, which applies weighted association rule mining in the
classification and constructs an efficient weighted associative classifier.
This proposed associative classification algorithm chooses one non class
informative attribute from dataset and all the weighted class association rules
are generated based on that attribute. The weight of the item is considered as
one of the parameter in generating the weighted class association rules. This
proposed algorithm calculates the weight using the HITS model. Experimental
results show that the proposed system generates less number of high quality
rules which improves the classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2141</identifier>
 <datestamp>2011-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2141</id><created>2011-11-30</created><updated>2011-12-21</updated><authors><author><keyname>Norman</keyname><forenames>Joseph W.</forenames></author></authors><title>Resolving G\&quot;odel's Incompleteness Myth: Polynomial Equations and
  Dynamical Systems for Algebraic Logic</title><categories>math.GM cs.LO</categories><comments>45 pages; revised to clarify some general notation and specific
  points on polynomials, remove extraneous material, fix typos, and introduce
  the Pythagorean fallacy</comments><msc-class>03F40, 03B53, 03B45, 03A05, 03B05, 03B35, 37N99, 65H10</msc-class><acm-class>F.4.1; G.1; I.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new computational method that uses polynomial equations and dynamical
systems to evaluate logical propositions is introduced and applied to Goedel's
incompleteness theorems. The truth value of a logical formula subject to a set
of axioms is computed from the solution to the corresponding system of
polynomial equations. A reference by a formula to its own provability is shown
to be a recurrence relation, which can be either interpreted as such to
generate a discrete dynamical system, or interpreted in a static way to create
an additional simultaneous equation. In this framework the truth values of
logical formulas and other polynomial objectives have complex data structures:
sets of elementary values, or dynamical systems that generate sets of infinite
sequences of such solution-value sets. Besides the routine result that a
formula has a definite elementary value, these data structures encode several
exceptions: formulas that are ambiguous, unsatisfiable, unsteady, or
contingent. These exceptions represent several semantically different types of
undecidability; none causes any fundamental problem for mathematics. It is
simple to calculate that Goedel's formula, which asserts that it cannot be
proven, is exceptional in specific ways: interpreted statically, the formula
defines an inconsistent system of equations (thus it is called unsatisfiable);
interpreted dynamically, it defines a dynamical system that has a periodic
orbit and no fixed point (thus it is called unsteady). These exceptions are not
catastrophic failures of logic; they are accurate mathematical descriptions of
Goedel's self-referential construction. Goedel's analysis does not reveal any
essential incompleteness in formal reasoning systems, nor any barrier to
proving the consistency of such systems by ordinary mathematical means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2143</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2143</id><created>2011-12-09</created><authors><author><keyname>G&#xf6;rke</keyname><forenames>Robert</forenames></author><author><keyname>Schumm</keyname><forenames>Andrea</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Experiments on Density-Constrained Graph Clustering</title><categories>cs.DS</categories><comments>Expanded version of a paper appearing in the proceedings of the
  Meeting on Algorithm Engineering and Experiments, ALENEX 2012. 23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering a graph means identifying internally dense subgraphs which are
only sparsely interconnected. Formalizations of this notion lead to measures
that quantify the quality of a clustering and to algorithms that actually find
clusterings. Since, most generally, corresponding optimization problems are
hard, heuristic clustering algorithms are used in practice, or other approaches
which are not based on an objective function. In this work we conduct a
comprehensive experimental evaluation of the qualitative behavior of greedy
bottom-up heuristics driven by cut-based objectives and constrained by
intracluster density, using both real-world data and artificial instances. Our
study documents that a greedy strategy based on local movement is superior to
one based on merging. We further reveal that the former approach generally
outperforms alternative setups and reference algorithms from the literature in
terms of its own objective, while a modularity-based algorithm competes
surprisingly well. Finally, we exhibit which combinations of cut-based inter-
and intracluster measures are suitable for identifying a hidden reference
clustering in synthetic random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2144</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2144</id><created>2011-12-09</created><authors><author><keyname>Godescu</keyname><forenames>Alexandru</forenames></author></authors><title>An Information Theoretic Analysis of Decision in Computer Chess</title><categories>cs.AI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basis of the method proposed in this article is the idea that information
is one of the most important factors in strategic decisions, including
decisions in computer chess and other strategy games. The model proposed in
this article and the algorithm described are based on the idea of a information
theoretic basis of decision in strategy games . The model generalizes and
provides a mathematical justification for one of the most popular search
algorithms used in leading computer chess programs, the fractional ply scheme.
However, despite its success in leading computer chess applications, until now
few has been published about this method. The article creates a fundamental
basis for this method in the axioms of information theory, then derives the
principles used in programming the search and describes mathematically the form
of the coefficients. One of the most important parameters of the fractional ply
search is derived from fundamental principles. Until now this coefficient has
been usually handcrafted or determined from intuitive elements or data mining.
There is a deep, information theoretical justification for such a parameter. In
one way the method proposed is a generalization of previous methods. More
important, it shows why the fractional depth ply scheme is so powerful. It is
because the algorithm navigates along the lines where the highest information
gain is possible. A working and original implementation has been written and
tested for this algorithm and is provided in the appendix. The article is
essentially self-contained and gives proper background knowledge and
references. The assumptions are intuitive and in the direction expected and
described intuitively by great champions of chess.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2149</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2149</id><created>2011-12-09</created><authors><author><keyname>Godescu</keyname><forenames>Alexandru</forenames></author></authors><title>Information and Search in Computer Chess</title><categories>cs.AI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes a model of chess based on information theory. A
mathematical model of the partial depth scheme is outlined and a formula for
the partial depth added for each ply is calculated from the principles of the
model. An implementation of alpha-beta with partial depth is given. The method
is tested using an experimental strategy having as objective to show the effect
of allocation of a higher amount of search resources on areas of the search
tree with higher information. The search proceeds in the direction of lines
with higher information gain. The effects on search performance of allocating
higher search resources on lines with higher information gain are tested
experimentaly and conclusive results are obtained. In order to isolate the
effects of the partial depth scheme no other heuristic is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2155</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2155</id><created>2011-12-09</created><authors><author><keyname>Karami</keyname><forenames>Ali</forenames></author><author><keyname>Baraani-Dastjerdi</keyname><forenames>Ahmad</forenames></author></authors><title>A Concurrency Control Method Based on Commitment Ordering in Mobile
  Databases</title><categories>cs.DB</categories><comments>15 pages, 13 figures, Journal: International Journal of Database
  Management Systems (IJDMS)</comments><journal-ref>International Journal of Database Management Systems (IJDMS),
  Vol.3, No.4, November 2011, 39-53</journal-ref><doi>10.5121/ijdms.2011.3404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disconnection of mobile clients from server, in an unclear time and for an
unknown duration, due to mobility of mobile clients, is the most important
challenges for concurrency control in mobile database with client-server model.
Applying pessimistic common classic methods of concurrency control (like 2pl)
in mobile database leads to long duration blocking and increasing waiting time
of transactions. Because of high rate of aborting transactions, optimistic
methods aren`t appropriate in mobile database. In this article, OPCOT
concurrency control algorithm is introduced based on optimistic concurrency
control method. Reducing communications between mobile client and server,
decreasing blocking rate and deadlock of transactions, and increasing
concurrency degree are the most important motivation of using optimistic method
as the basis method of OPCOT algorithm. To reduce abortion rate of
transactions, in execution time of transactions` operators a timestamp is
assigned to them. In other to checking commitment ordering property of
scheduler, the assigned timestamp is used in server on time of commitment. In
this article, serializability of OPCOT algorithm scheduler has been proved by
using serializability graph. Results of evaluating simulation show that OPCOT
algorithm decreases abortion rate and waiting time of transactions in compare
to 2pl and optimistic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2156</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2156</id><created>2011-12-09</created><authors><author><keyname>Gay</keyname><forenames>Simon J.</forenames></author></authors><title>Stabilizer States as a Basis for Density Matrices</title><categories>quant-ph cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the space of density matrices for n-qubit states, considered as
a (2^n)^2 dimensional real vector space, has a basis consisting of density
matrices of stabilizer states. We describe an application of this result to
automated verification of quantum protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2160</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2160</id><created>2011-12-09</created><authors><author><keyname>Gonz&#xe1;lez-Estrada</keyname><forenames>Octavio A.</forenames></author><author><keyname>R&#xf3;denas</keyname><forenames>Juan Jos&#xe9;</forenames></author><author><keyname>Bordas</keyname><forenames>St&#xe9;phane P. A.</forenames></author><author><keyname>Duflot</keyname><forenames>Marc</forenames></author><author><keyname>Kerfriden</keyname><forenames>Pierre</forenames></author><author><keyname>Giner</keyname><forenames>Eugenio</forenames></author></authors><title>On the role of enrichment and statical admissibility of recovered fields
  in a-posteriori error estimation for enriched finite element methods</title><categories>physics.comp-ph cs.NA</categories><comments>Submitted to Engineering Computations</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Purpose: This paper aims at assessing the effect of (1) the statical
admissibility of the recovered solution; (2) the ability of the recovered
solution to represent the singular solution; on the accuracy, local and global
effectivity of recovery-based error estimators for enriched finite element
methods (e.g. the extended finite element method, XFEM).
Design/methodology/approach: We study the performance of two recovery
techniques. The first is a recently developed superconvergent patch recovery
procedure with equilibration and enrichment (SPR-CX). The second is known as
the extended moving least squares recovery (XMLS), which enriches the recovered
solutions but does not enforce equilibrium constraints. Both are extended
recovery techniques as the polynomial basis used in the recovery process is
enriched with singular terms for a better description of the singular nature of
the solution. Findings: Numerical results comparing the convergence and the
effectivity index of both techniques with those obtained without the enrichment
enhancement clearly show the need for the use of extended recovery techniques
in Zienkiewicz-Zhu type error estimators for this class of problems. The
results also reveal significant improvements in the effectivities yielded by
statically admissible recovered solutions. Originality/value: This work shows
that both extended recovery procedures and statical admissibility are key to an
accurate assessment of the quality of enriched finite element approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2172</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2172</id><created>2011-12-09</created><updated>2013-01-10</updated><authors><author><keyname>Kovac</keyname><forenames>Jakub</forenames></author></authors><title>On the Complexity of Rearrangement Problems under the Breakpoint
  Distance</title><categories>cs.DM q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study complexity of rearrangement problems in the generalized breakpoint
model and settle several open questions. The model was introduced by Tannier et
al. (2009) who showed that the median problem is solvable in polynomial time in
the multichromosomal circular and mixed breakpoint models. This is intriguing,
since in most other rearrangement models (DCJ, reversal, unichromosomal or
multilinear breakpoint models), the problem is NP-hard. The complexity of the
small or even the large phylogeny problem under the breakpoint distance
remained an open problem. We improve the algorithm for the median problem and
show that it is equivalent to the problem of finding maximum cardinality
non-bipartite matching (under linear reduction). On the other hand, we prove
that the more general small phylogeny problem is NP-hard. Surprisingly, we show
that it is already NP-hard (or even APX-hard) for 4 species (a quartet
phylogeny). In other words, while finding an ancestor for 3 species is easy,
finding two ancestors for 4 species is already hard. We also show that, in the
unichromosomal and the multilinear breakpoint model, the halving problem is
NP-hard, thus refuting the conjecture of Tannier et al. Interestingly, this is
the first problem which is harder in the breakpoint model than in the DCJ or
reversal models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2183</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2183</id><created>2011-12-09</created><authors><author><keyname>Devi</keyname><forenames>P. Isakki alias</forenames></author><author><keyname>Rajagopalan</keyname><forenames>S. P.</forenames></author></authors><title>The Expert System Designed to Improve Customer Satisfaction</title><categories>cs.NE</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Customer Relationship Management becomes a leading business strategy in
highly competitive business environment. It aims to enhance the performance of
the businesses by improving the customer satisfaction and loyalty. The
objective of this paper is to improve customer satisfaction on product's colors
and design with the help of the expert system developed by using Artificial
Neural Networks. The expert system's role is to capture the knowledge of the
experts and the data from the customer requirements, and then, process the
collected data and form the appropriate rules for choosing product's colors and
design. In order to identify the hidden pattern of the customer's needs, the
Artificial Neural Networks technique has been applied to classify the colors
and design based upon a list of selected information. Moreover, the expert
system has the capability to make decisions in ranking the scores of the colors
and design presented in the selection. In addition, the expert system has been
validated with a different customer types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2187</identifier>
 <datestamp>2011-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2187</id><created>2011-12-09</created><updated>2011-12-14</updated><authors><author><keyname>Wang</keyname><forenames>Chih-Yu</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>K. J. Ray</forenames></author></authors><title>Chinese Restaurant Game - Part II: Applications to Wireless Networking,
  Cloud Computing, and Online Social Networking</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part I of this two-part paper [1], we proposed a new game, called Chinese
restaurant game, to analyze the social learning problem with negative network
externality. The best responses of agents in the Chinese restaurant game with
imperfect signals are constructed through a recursive method, and the influence
of both learning and network externality on the utilities of agents is studied.
In Part II of this two-part paper, we illustrate three applications of Chinese
restaurant game in wireless networking, cloud computing, and online social
networking. For each application, we formulate the corresponding problem as a
Chinese restaurant game and analyze how agents learn and make strategic
decisions in the problem. The proposed method is compared with four
common-sense methods in terms of agents' utilities and the overall system
performance through simulations. We find that the proposed Chinese restaurant
game theoretic approach indeed helps agents make better decisions and improves
the overall system performance. Furthermore, agents with different decision
orders have different advantages in terms of their utilities, which also
verifies the conclusions drawn in Part I of this two-part paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2188</identifier>
 <datestamp>2012-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2188</id><created>2011-12-09</created><updated>2012-02-13</updated><authors><author><keyname>Wang</keyname><forenames>Chih-Yu</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>K. J. Ray</forenames></author></authors><title>Chinese Restaurant Game - Part I: Theory of Learning with Negative
  Network Externality</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a social network, agents are intelligent and have the capability to make
decisions to maximize their utilities. They can either make wise decisions by
taking advantages of other agents' experiences through learning, or make
decisions earlier to avoid competitions from huge crowds. Both these two
effects, social learning and negative network externality, play important roles
in the decision process of an agent. While there are existing works on either
social learning or negative network externality, a general study on considering
both these two contradictory effects is still limited. We find that the Chinese
restaurant process, a popular random process, provides a well-defined structure
to model the decision process of an agent under these two effects. By
introducing the strategic behavior into the non-strategic Chinese restaurant
process, in Part I of this two-part paper, we propose a new game, called
Chinese Restaurant Game, to formulate the social learning problem with negative
network externality. Through analyzing the proposed Chinese restaurant game, we
derive the optimal strategy of each agent and provide a recursive method to
achieve the optimal strategy. How social learning and negative network
externality influence each other under various settings is also studied through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2193</identifier>
 <datestamp>2011-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2193</id><created>2011-12-09</created><authors><author><keyname>Di Pierro</keyname><forenames>Massimo</forenames></author><author><keyname>Hetrick</keyname><forenames>James</forenames></author><author><keyname>Cholia</keyname><forenames>Shreyas</forenames></author><author><keyname>Skinner</keyname><forenames>David</forenames></author></authors><title>Making QCD Lattice Data Accessible and Organized through Advanced Web
  Interfaces</title><categories>hep-lat cs.DL</categories><journal-ref>Lattice 2011 Proceedings</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gauge Connection at qcd.nersc.gov is one of the most popular repositories
of QCD lattice ensembles. It is used to access 16TB of archived QCD data from
the High Performance Storage System (HPSS) at the National Energy Research
Scientific Computing Center (NERSC). Here, we present a new web interface for
qcd.nersc.gov which allows physicists to browse and search the data, as well as
download individual files or entire ensembles in batch. Our system
distinguishes itself from others because of its ease of use and web based
workflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2205</identifier>
 <datestamp>2012-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2205</id><created>2011-12-09</created><updated>2012-01-10</updated><authors><author><keyname>DiBenedetto</keyname><forenames>Steven</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Uzun</keyname><forenames>Ersin</forenames></author></authors><title>ANDaNA: Anonymous Named Data Networking Application</title><categories>cs.CR cs.CY cs.NI</categories><comments>NDSS 2012 - Proceedings of the Network and Distributed System
  Security Symposium, San Diego, California, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-centric networking -- also known as information-centric networking
(ICN) -- shifts emphasis from hosts and interfaces (as in today's Internet) to
data. Named data becomes addressable and routable, while locations that
currently store that data become irrelevant to applications.
  Named Data Networking (NDN) is a large collaborative research effort that
exemplifies the content-centric approach to networking. NDN has some innate
privacy-friendly features, such as lack of source and destination addresses on
packets. However, as discussed in this paper, NDN architecture prompts some
privacy concerns mainly stemming from the semantic richness of names. We
examine privacy-relevant characteristics of NDN and present an initial attempt
to achieve communication privacy. Specifically, we design an NDN add-on tool,
called ANDaNA, that borrows a number of features from Tor. As we demonstrate
via experiments, it provides comparable anonymity with lower relative overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2230</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2230</id><created>2011-12-09</created><authors><author><keyname>Denny</keyname><forenames>Travis</forenames></author></authors><title>Faked states attack and quantum cryptography protocols</title><categories>cs.CR</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leveraging quantum mechanics, cryptographers have devised provably secure key
sharing protocols. Despite proving the security in theory, real-world
application falls short of the ideal. Last year, cryptanalysts completed an
experiment demonstrating a successful eavesdropping attack on commercial
quantum key distribution (QKD) systems. This attack exploits a weakness in the
typical real-world implementation of quantum cryptosystems. Cryptanalysts have
successfully attacked several protocols. In this paper, we examine the Kak
quantum cryptography protocol and how it may perform under such attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2234</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2234</id><created>2011-12-09</created><authors><author><keyname>Pilla</keyname><forenames>Prashant</forenames></author></authors><title>Enhancing Data Security by Making Data Disappear in a P2P Systems</title><categories>cs.CR</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the problem of securing data by making it disappear
after some time limit, making it impossible for it to be recovered by an
unauthorized party. This method is in response to the need to keep the data
secured and to protect the privacy of archived data on the servers, Cloud and
Peer-to-Peer architectures. Due to the distributed nature of these
architectures, it is impossible to destroy the data completely. So, we store
the data by applying encryption and then manage the key, which is easier to do
as the key is small and it can be hidden in the DHT (Distributed hash table).
Even if the keys in the DHT and the encrypted data were compromised, the data
would still be secure. This paper describes existing solutions, points to their
limitations and suggests improvements with a new secure architecture. We
evaluated and executed this architecture on the Java platform and proved that
it is more secure than other architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2239</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2239</id><created>2011-12-09</created><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Absence of influential spreaders in rumor dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 3 figures</comments><doi>10.1103/PhysRevE.85.026116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research [1] has suggested that coreness, and not degree, constitutes
a better topological descriptor to identifying influential spreaders in complex
networks. This hypothesis has been verified in the context of disease
spreading. Here, we instead focus on rumor spreading models, which are more
suited for social contagion and information propagation. To this end, we
perform extensive computer simulations on top of several real-world networks
and find opposite results. Namely, we show that the spreading capabilities of
the nodes do not depend on their $k$-core index, which instead determines
whether or not a given node prevents the diffusion of a rumor to a system-wide
scale. Our findings are relevant both for sociological studies of contagious
dynamics and for the design of efficient commercial viral processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2245</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2245</id><created>2011-12-09</created><authors><author><keyname>Nyang</keyname><forenames>DaeHun</forenames></author><author><keyname>Mohaisen</keyname><forenames>Abedelaziz</forenames></author><author><keyname>Kwon</keyname><forenames>Taekyoung</forenames></author><author><keyname>Kang</keyname><forenames>Brent</forenames></author><author><keyname>Stavrou</keyname><forenames>Angelos</forenames></author></authors><title>Decryptable to Your Eyes: Visualization of Security Protocols at the
  User Interface</title><categories>cs.CR</categories><comments>15 pages, 13 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of authentication protocols, for online banking services in
particular and any service that is of sensitive nature in general, is quite
challenging. Indeed, enforcing security guarantees has overhead thus imposing
additional computation and design considerations that do not always meet
usability and user requirements. On the other hand, relaxing assumptions and
rigorous security design to improve the user experience can lead to security
breaches that can harm the users' trust in the system.
  In this paper, we demonstrate how careful visualization design can enhance
not only the security but also the usability of the authentication process. To
that end, we propose a family of visualized authentication protocols, a
visualized transaction verification, and a &quot;decryptable to your eyes only&quot;
protocol. Through rigorous analysis, we verify that our protocols are immune to
many of the challenging authentication attacks applicable in the literature.
Furthermore, using an extensive case study on a prototype of our protocols, we
highlight the potential of our approach for real-world deployment: we were able
to achieve a high level of usability while satisfying stringent security
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2248</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2248</id><created>2011-12-09</created><authors><author><keyname>Kaur</keyname><forenames>Gurpreet</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Partha Pratim</forenames></author></authors><title>A Survey on Cooperative Diversity and Its Applications in Various
  Wireless Networks</title><categories>cs.NI</categories><comments>20 pages</comments><journal-ref>International Journal of Computer Science &amp; Engineering Survey
  (IJCSES) Vol.2, No.4, November 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative diversity is a technique in which various radio terminals relay
signals for each other. Cooperative diversity results when cooperative
communications is used primarily to leverage the spatial diversity available
among distributed radios. In this paper different cooperative diversity schemes
and their applications in various wireless networks are discussed. In this
paper the impact of cooperative diversity on the energy consumption and
lifetime of sensor network and the impact of cooperation in cognitive radio are
discussed. Here, user scheduling and radio resource allocation techniques are
also discussed which are developed in order to efficiently integrate various
cooperative diversity schemes for the emerging IEEE 802.16j based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2250</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2250</id><created>2011-12-09</created><updated>2012-04-05</updated><authors><author><keyname>Vafopoulos</keyname><forenames>Michalis</forenames></author></authors><title>Being, space and time in the Web</title><categories>cs.OH</categories><msc-class>68M11, 91D30, 93A10, 93A14, 97M10, 97M70</msc-class><acm-class>K.4.0; J.4; G.2.2; H.3.5; H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web initially emerged as an &quot;antidote&quot; to accumulated scientific
knowledge since it enables global representation and communication with minimum
costs. Its gigantic scale and interdependence incommode our ability to find
relevant information and develop trustworthy contexts. It is time for science
to compensate by providing an epistemological &quot;antidote&quot; to Web issues.
Philosophy should be in the front line by forming the salient questions and
analysis. The scope of our research is to provide a theory about the Web being
that will bridge philosophical thinking and engineering. We analyze existence
and spatiotemporality in the Web and how it transforms the traditional
actualities. The Web space is specified by incoming and outgoing links. The
primordial role of visiting durations in Web's existence is approximated by
Bergsonian time. The physical space becomes more discoverable. The human
activity can be asynchronous, synchronous and continuous. Networked individuals
operate in a flexible and spatially dispersed environment. The resulting issues
concern the self-determination of a being and the way in which the Web could be
a free and open platform for innovation and participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2251</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2251</id><created>2011-12-09</created><authors><author><keyname>Michalis</keyname><forenames>Vafopoulos</forenames></author><author><keyname>Michael</keyname><forenames>Oikonomou</forenames></author></authors><title>Recommendation systems: a joint analysis of technical aspects with
  marketing implications</title><categories>cs.IR stat.AP</categories><msc-class>90B60, 91D30, 62P25, 97M10, 97M70</msc-class><acm-class>H.3.3; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2010, Web users ordered, only in Amazon, 73 items per second and massively
contribute reviews about their consuming experience. As the Web matures and
becomes social and participatory, collaborative filters are the basic
complement in searching online information about people, events and products.
In Web 2.0, what connected consumers create is not simply content (e.g.
reviews) but context. This new contextual framework of consumption emerges
through the aggregation and collaborative filtering of personal preferences
about goods in the Web in massive scale. More importantly, facilitates
connected consumers to search and navigate the complex Web more effectively and
amplifies incentives for quality. The objective of the present article is to
jointly review the basic stylized facts of relevant research in recommendation
systems in computer and marketing studies in order to share some common
insights. After providing a comprehensive definition of goods and Users in the
Web, we describe a classification of recommendation systems based on two
families of criteria: how recommendations are formed and input data
availability. The classification is presented under a common minimal matrix
notation and is used as a bridge to related issues in the business and
marketing literature. We focus our analysis in the fields of one-to-one
marketing, network-based marketing Web merchandising and atmospherics and their
implications in the processes of personalization and adaptation in the Web.
Market basket analysis is investigated in context of recommendation systems.
Discussion on further research refers to the business implications and
technological challenges of recommendation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2254</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2254</id><created>2011-12-10</created><authors><author><keyname>Mohaisen</keyname><forenames>Abedelaziz</forenames></author><author><keyname>Tran</keyname><forenames>Huy</forenames></author><author><keyname>Chandra</keyname><forenames>Abhishek</forenames></author><author><keyname>Kim</keyname><forenames>Yongdae</forenames></author></authors><title>SocialCloud: Using Social Networks for Building Distributed Computing
  Services</title><categories>cs.DC cs.SI</categories><comments>15 pages, 8 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate a new computing paradigm, called SocialCloud, in
which computing nodes are governed by social ties driven from a bootstrapping
trust-possessing social graph. We investigate how this paradigm differs from
existing computing paradigms, such as grid computing and the conventional cloud
computing paradigms. We show that incentives to adopt this paradigm are
intuitive and natural, and security and trust guarantees provided by it are
solid. We propose metrics for measuring the utility and advantage of this
computing paradigm, and using real-world social graphs and structures of social
traces; we investigate the potential of this paradigm for ordinary users. We
study several design options and trade-offs, such as scheduling algorithms,
centralization, and straggler handling, and show how they affect the utility of
the paradigm. Interestingly, we conclude that whereas graphs known in the
literature for high trust properties do not serve distributed trusted computing
algorithms, such as Sybil defenses---for their weak algorithmic properties,
such graphs are good candidates for our paradigm for their self-load-balancing
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2257</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2257</id><created>2011-12-10</created><authors><author><keyname>Rahbari</keyname><forenames>Mina</forenames></author><author><keyname>Jamali</keyname><forenames>Mohammad Ali Jabreil</forenames></author></authors><title>Efficient Detection of Sybil Attack Based on Cryptography in Vanet</title><categories>cs.CR</categories><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.3, No.6, November 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular communications play a substantial role in providing safety
transportation by means of safety message exchange. Researchers have proposed
several solutions for securing safety messages. Protocols based on a fixed key
infrastructure are more efficient in implementation and maintain stronger
security in comparison with dynamic structures. The purpose of this paper
present a method based on a fixed key infrastructure for detection
impersonation attack, in other words, Sybil attack, in the vehicular ad hoc
network. This attack, puts a great impact on performance of the network. The
proposed method, using an cryptography mechanism to detection Sybil attack.
Finally, using Mat lab simulator the results of this approach are reviewed,
This method it has low delay for detection Sybil attack, because most
operations are done in Certification Authority, so this proposed schema is a
efficient method for detection Sybil attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2261</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2261</id><created>2011-12-10</created><authors><author><keyname>Meyyappan</keyname><forenames>Dr. T.</forenames></author><author><keyname>Thamarai</keyname><forenames>SM.</forenames></author><author><keyname>Nachiaban</keyname><forenames>N. M. Jeya</forenames></author></authors><title>Lossless Digital Image Compression Method for Bitmap Images</title><categories>cs.MM</categories><comments>10 pages, 7 figures, presented in First International Conference on
  Digital Image Processing and Pattern Recognition (DPPR-2011)In conjunction
  with (CCSEIT-2011), Manonmaniam Sundharanar University, September 23~25,
  2011, Tirunelveli, Tamil Nadu, India</comments><msc-class>65D18</msc-class><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.4, November 2011</journal-ref><doi>10.5121/ijma.2011.3407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper, the authors propose a new approach to digital image
compression using crack coding This method starts with the original image and
develop crack codes in a recursive manner, marking the pixels visited earlier
and expanding the entropy in four directions. The proposed method is
experimented with sample bitmap images and results are tabulated. The method is
implemented in uni-processor machine using C language source code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2262</identifier>
 <datestamp>2012-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2262</id><created>2011-12-10</created><updated>2012-01-09</updated><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Perfectly secure encryption of individual sequences</title><categories>cs.IT cs.CR math.IT</categories><comments>21 pages; Submitted to IEEE Trans. on Inform. Theory; Subsection 4.3
  rewritten</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analogy to the well-known notion of finite--state compressibility of
individual sequences, due to Lempel and Ziv, we define a similar notion of
&quot;finite-state encryptability&quot; of an individual plaintext sequence, as the
minimum asymptotic key rate that must be consumed by finite-state encrypters so
as to guarantee perfect secrecy in a well-defined sense. Our main basic result
is that the finite-state encryptability is equal to the finite-state
compressibility for every individual sequence. This is in parallelism to
Shannon's classical probabilistic counterpart result, asserting that the
minimum required key rate is equal to the entropy rate of the source. However,
the redundancy, defined as the gap between the upper bound (direct part) and
the lower bound (converse part) in the encryption problem, turns out to decay
at a different rate (in fact, much slower) than the analogous redundancy
associated with the compression problem. We also extend our main theorem in
several directions, allowing: (i) availability of side information (SI) at the
encrypter/decrypter/eavesdropper, (ii) lossy reconstruction at the decrypter,
and (iii) the combination of both lossy reconstruction and SI, in the spirit of
the Wyner--Ziv problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2265</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2265</id><created>2011-12-10</created><authors><author><keyname>Chakravarthy</keyname><forenames>A. S. N.</forenames></author><author><keyname>Raja</keyname><forenames>Penmetsa V. Krishna</forenames></author><author><keyname>Avadhani</keyname><forenames>Prof. P. S.</forenames></author></authors><title>A Novel Approach for Password Authentication Using Bidirectional
  Associative Memory</title><categories>cs.CR cs.NE</categories><comments>13 pages</comments><journal-ref>Advanced Computing: An International Journal ( ACIJ ), Vol.2,
  No.6, November 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Password authentication is a very important system security procedure to gain
access to user resources. In the Traditional password authentication methods a
server has check the authenticity of the users. In our proposed method users
can freely select their passwords from a predefined character set. They can
also use a graphical image as password. The password may be a character or an
image it will be converted into binary form and the binary values will be
normalized. Associative memories have been used recently for password
authentication in order to overcome drawbacks of the traditional password
authentication methods. In this paper we proposed a method using Bidirectional
Associative Memory algorithm for both alphanumeric (Text) and graphical
password. By doing so the amount of security what we provide for the user can
be enhanced. This paper along with test results show that converting user
password in to Probabilistic values and giving them as input for BAM improves
the security of the system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2271</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2271</id><created>2011-12-10</created><authors><author><keyname>Karpowicz</keyname><forenames>Anna</forenames></author><author><keyname>Szajowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Anglers' fishing problem</title><categories>math.PR cs.GT cs.SY math.OC math.ST stat.TH</categories><comments>23 pages + index</comments><report-no>Report PRE44, Institute of Mathematics and Computer Science, 2011</report-no><msc-class>60G40, 60K99, 90A46</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The considered model will be formulated as related to &quot;the fishing problem&quot;
even if the other applications of it are much more obvious. The angler goes
fishing. He uses various techniques and he has at most two fishing rods. He
buys a fishing ticket for a fixed time. The fishes are caught with the use of
different methods according to the renewal processes. The fishes' value and the
inter arrival times are given by the sequences of independent, identically
distributed (i.i.d.) random variables with the known distribution functions. It
forms the marked renewal--reward process. The angler's measure of satisfaction
is given by the difference between the utility function, depending on the value
of the fishes caught, and the cost function connected with the time of fishing.
In this way, the angler's relative opinion about the methods of fishing is
modelled. The angler's aim is to have as much satisfaction as possible and
additionally he has to leave the lake before a fixed moment. Therefore his goal
is to find two optimal stopping times in order to maximize his satisfaction. At
the first moment, he changes the technique of fishing, e.g. by excluding one
rod and intensifying on the rest. Next, he decides when he should stop the
expedition. These stopping times have to be shorter than the fixed time of
fishing. The dynamic programming methods have been used to find these two
optimal stopping times and to specify the expected satisfaction of the angler
at these times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2273</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2273</id><created>2011-12-10</created><updated>2012-07-18</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>Nutov</keyname><forenames>Zeev</forenames></author></authors><title>Steiner Forest Orientation Problems</title><categories>cs.DS</categories><comments>full version of ESA 2012 publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider connectivity problems with orientation constraints. Given a
directed graph $D$ and a collection of ordered node pairs $P$ let $P[D]=\{(u,v)
\in P: D {contains a} uv{-path}}$. In the {\sf Steiner Forest Orientation}
problem we are given an undirected graph $G=(V,E)$ with edge-costs and a set $P
\subseteq V \times V$ of ordered node pairs. The goal is to find a minimum-cost
subgraph $H$ of $G$ and an orientation $D$ of $H$ such that $P[D]=P$. We give a
4-approximation algorithm for this problem.
  In the {\sf Maximum Pairs Orientation} problem we are given a graph $G$ and a
multi-collection of ordered node pairs $P$ on $V$. The goal is to find an
orientation $D$ of $G$ such that $|P[D]|$ is maximum. Generalizing the result
of Arkin and Hassin [DAM'02] for $|P|=2$, we will show that for a mixed graph
$G$ (that may have both directed and undirected edges), one can decide in
$n^{O(|P|)}$ time whether $G$ has an orientation $D$ with $P[D]=P$ (for
undirected graphs this problem admits a polynomial time algorithm for any $P$,
but it is NP-complete on mixed graphs). For undirected graphs, we will show
that one can decide whether $G$ admits an orientation $D$ with $|P[D]| \geq k$
in $O(n+m)+2^{O(k\cdot \log \log k)}$ time; hence this decision problem is
fixed-parameter tractable, which answers an open question from Dorn et al.
[AMB'11]. We also show that {\sf Maximum Pairs Orientation} admits ratio
$O(\log |P|/\log\log |P|)$, which is better than the ratio $O(\log n/\log\log
n)$ of Gamzu et al. [WABI'10] when $|P|&lt;n$.
  Finally, we show that the following node-connectivity problem can be solved
in polynomial time: given a graph $G=(V,E)$ with edge-costs, $s,t \in V$, and
an integer $\ell$, find a min-cost subgraph $H$ of $G$ with an orientation $D$
such that $D$ contains $\ell$ internally-disjoint $st$-paths, and $\ell$
internally-disjoint $ts$-paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2275</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2275</id><created>2011-12-10</created><updated>2014-03-26</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Dell</keyname><forenames>Holger</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Marx</keyname><forenames>Daniel</forenames></author><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Paturi</keyname><forenames>Ramamohan</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author><author><keyname>Wahlstrom</keyname><forenames>Magnus</forenames></author></authors><title>On Problems as Hard as CNFSAT</title><categories>cs.DS cs.CC cs.DM</categories><comments>25 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of exact exponential time algorithms for NP-hard problems has
thrived over the last decade. While exhaustive search remains asymptotically
the fastest known algorithm for some basic problems, difficult and non-trivial
exponential time algorithms have been found for a myriad of problems, including
Graph Coloring, Hamiltonian Path, Dominating Set and 3-CNF-Sat. In some
instances, improving these algorithms further seems to be out of reach. The
CNF-Sat problem is the canonical example of a problem for which the trivial
exhaustive search algorithm runs in time O(2^n), where n is the number of
variables in the input formula. While there exist non-trivial algorithms for
CNF-Sat that run in time o(2^n), no algorithm was able to improve the growth
rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is
the optimal growth rate. The strong exponential time hypothesis (SETH) by
Impagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that,
for every epsilon&lt;1, there is a (large) integer k such that that k-CNF-Sat
cannot be computed in time 2^{epsilon n}.
  In this paper, we show that, for every epsilon &lt; 1, the problems Hitting Set,
Set Splitting, and NAE-Sat cannot be computed in time O(2^{epsilon n}) unless
SETH fails. Here n is the number of elements or variables in the input. For
these problems, we actually get an equivalence to SETH in a certain sense. We
conjecture that SETH implies a similar statement for Set Cover, and prove that,
under this assumption, the fastest known algorithms for Steinter Tree,
Connected Vertex Cover, Set Partitioning, and the pseudo-polynomial time
algorithm for Subset Sum cannot be significantly improved. Finally, we justify
our assumption about the hardness of Set Cover by showing that the parity of
the number of set covers cannot be computed in time O(2^{epsilon n}) for any
epsilon&lt;1 unless SETH fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2292</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2292</id><created>2011-12-10</created><authors><author><keyname>Pentikousis</keyname><forenames>Kostas</forenames></author><author><keyname>Badr</keyname><forenames>Hussein</forenames></author><author><keyname>Andrade</keyname><forenames>Asha</forenames></author></authors><title>A comparative study of aggregate TCP retransmission rates</title><categories>cs.NI</categories><journal-ref>International Journal of Computers and Applications, vol. 32, no.
  4, October 2010, pp 1-7. ISSN: 1206-212X</journal-ref><doi>10.2316/Journal.202.2010.4.202-2660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segment retransmissions are an essential tool in assuring reliable end-to-end
communication in the Internet. Their crucial role in TCP design and operation
has been studied extensively, in particular with respect to identifying
non-conformant, buggy, or underperforming behaviour. However, TCP segment
retransmissions are often overlooked when examining and analyzing large traffic
traces. In fact, some have come to believe that retransmissions are a rare
oddity, characteristically associated with faulty network paths, which,
typically, tend to disappear as networking technology advances and link
capacities grow. We find that this may be far from the reality experienced by
TCP flows. We quantify aggregate TCP segment retransmission rates using
publicly available network traces from six passive monitoring points attached
to the egress gateways at large sites. In virtually half of the traces examined
we observed aggregate TCP retransmission rates exceeding 1%, and of these,
about half again had retransmission rates exceeding 2%. Even for sites with low
utilization and high capacity gateway links, retransmission rates of 1%, and
sometimes higher, were not uncommon. Our results complement, extend and bring
up to date partial and incomplete results in previous work, and show that TCP
retransmissions continue to constitute a non-negligible percentage of the
overall traffic, despite significant advances across the board in
telecommunications technologies and network protocols. The results presented
are pertinent to end-to-end protocol designers and evaluators as they provide a
range of &quot;realistic&quot; scenarios under which, and a &quot;marker&quot; against which,
simulation studies can be configured and calibrated, and future protocols
evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2302</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2302</id><created>2011-12-10</created><authors><author><keyname>Andreyev</keyname><forenames>Sergey</forenames></author></authors><title>A Door into Another World</title><categories>cs.HC</categories><comments>9 pages, 6 figures</comments><acm-class>H.5.2; D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to design programs which each user can change according to his
preferences? Not an illusion of such a thing that adaptive interface provides
but really an interface ruled by users. What is the main problem of such design
and what is the solution to this problem? This short article gives a glimpse
into the theory discussed in the book &quot;The World of Movable Objects&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2306</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2306</id><created>2011-12-10</created><authors><author><keyname>Yang</keyname><forenames>Sheng</forenames><affiliation>Shitz</affiliation></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames><affiliation>Shitz</affiliation></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Secrecy Degrees of Freedom of MIMO Broadcast Channels with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>25 pages, 2 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) of the two-user Gaussian multiple-input and
multiple-output (MIMO) broadcast channel with confidential message (BCC) is
studied under the assumption that delayed channel state information (CSI) is
available at the transmitter. We characterize the optimal secrecy DoF (SDoF)
region and show that it can be achieved by a simple artificial noise alignment
(ANA) scheme. The proposed scheme sends the confidential messages superposed
with the artificial noise over several time slots. Exploiting delayed CSI, the
transmitter aligns the signal in such a way that the useful message can be
extracted at the intended receiver but is completely drowned by the artificial
noise at the unintended receiver. The proposed scheme can be interpreted as a
non-trivial extension of Maddah-Ali Tse (MAT) scheme and enables us to quantify
the resource overhead, or equivalently the DoF loss, to be paid for the secrecy
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2310</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2310</id><created>2011-12-10</created><updated>2014-07-15</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Towards Optimal and Expressive Kernelization for d-Hitting Set</title><categories>cs.DM cs.DS</categories><comments>This version gives corrected experimental results, adds additional
  figures, and more formally defines &quot;expressive kernelization&quot;</comments><msc-class>68R10</msc-class><acm-class>G.2.1; G.2.2; F.2.2; I.1.2</acm-class><journal-ref>Algorithmica 70(1):129-147, 2014</journal-ref><doi>10.1007/s00453-013-9774-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  d-Hitting Set is the NP-hard problem of selecting at most k vertices of a
hypergraph so that each hyperedge, all of which have cardinality at most d,
contains at least one selected vertex. The applications of d-Hitting Set are,
for example, fault diagnosis, automatic program verification, and the
noise-minimizing assignment of frequencies to radio transmitters.
  We show a linear-time algorithm that transforms an instance of d-Hitting Set
into an equivalent instance comprising at most O(k^d) hyperedges and vertices.
In terms of parameterized complexity, this is a problem kernel. Our
kernelization algorithm is based on speeding up the well-known approach of
finding and shrinking sunflowers in hypergraphs, which yields problem kernels
with structural properties that we condense into the concept of expressive
kernelization.
  We conduct experiments to show that our kernelization algorithm can kernelize
instances with more than 10^7 hyperedges in less than five minutes.
  Finally, we show that the number of vertices in the problem kernel can be
further reduced to O(k^{d-1}) with additional O(k^{1.5 d}) processing time by
nontrivially combining the sunflower technique with d-Hitting Set problem
kernels due to Abu-Khzam and Moser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2313</identifier>
 <datestamp>2011-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2313</id><created>2011-12-10</created><updated>2011-12-13</updated><authors><author><keyname>Chen</keyname><forenames>Huan</forenames></author><author><keyname>Janota</keyname><forenames>Mikolas</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>QBF-Based Boolean Function Bi-Decomposition</title><categories>cs.LO</categories><comments>This paper is an extension of the DATE'2012 paper &quot;QBF-Based Boolean
  Function Bi-Decomposition&quot; by Huan Chen, Mikolas Janota, Joao Marques-Silva</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean function bi-decomposition is ubiquitous in logic synthesis. It
entails the decomposition of a Boolean function using two-input simple logic
gates. Existing solutions for bi-decomposition are often based on BDDs and,
more recently, on Boolean Satisfiability. In addition, the partition of the
input set of variables is either assumed, or heuristic solutions are considered
for finding good partitions. In contrast to earlier work, this paper proposes
the use of Quantified Boolean Formulas (QBF) for computing bi- decompositions.
These bi-decompositions are optimal in terms of the achieved disjointness and
balancedness of the input set of variables. Experimental results, obtained on
representative benchmarks, demonstrate clear improvements in the quality of
computed decompositions, but also the practical feasibility of QBF-based
bi-decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2315</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2315</id><created>2011-12-10</created><authors><author><keyname>Smyrnakis</keyname><forenames>Michalis</forenames></author><author><keyname>Leslie</keyname><forenames>David S.</forenames></author></authors><title>Adaptive Forgetting Factor Fictitious Play</title><categories>stat.ML cs.LG cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now well known that decentralised optimisation can be formulated as a
potential game, and game-theoretical learning algorithms can be used to find an
optimum. One of the most common learning techniques in game theory is
fictitious play. However fictitious play is founded on an implicit assumption
that opponents' strategies are stationary. We present a novel variation of
fictitious play that allows the use of a more realistic model of opponent
strategy. It uses a heuristic approach, from the online streaming data
literature, to adaptively update the weights assigned to recently observed
actions. We compare the results of the proposed algorithm with those of
stochastic and geometric fictitious play in a simple strategic form game, a
vehicle target assignment game and a disaster management problem. In all the
tests the rate of convergence of the proposed algorithm was similar or better
than the variations of fictitious play we compared it with. The new algorithm
therefore improves the performance of game-theoretical learning in
decentralised optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2316</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2316</id><created>2011-12-10</created><authors><author><keyname>Ribeiro</keyname><forenames>H. V.</forenames></author><author><keyname>Zunino</keyname><forenames>L.</forenames></author><author><keyname>Mendes</keyname><forenames>R. S.</forenames></author><author><keyname>Lenzi</keyname><forenames>E. K.</forenames></author></authors><title>Complexity-entropy causality plane: a useful approach for distinguishing
  songs</title><categories>physics.soc-ph cs.SD physics.data-an</categories><comments>Accepted for publication in Physica A</comments><journal-ref>Physica A 391 (2012) 2421-2428</journal-ref><doi>10.1016/j.physa.2011.12.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays we are often faced with huge databases resulting from the rapid
growth of data storage technologies. This is particularly true when dealing
with music databases. In this context, it is essential to have techniques and
tools able to discriminate properties from these massive sets. In this work, we
report on a statistical analysis of more than ten thousand songs aiming to
obtain a complexity hierarchy. Our approach is based on the estimation of the
permutation entropy combined with an intensive complexity measure, building up
the complexity-entropy causality plane. The results obtained indicate that this
representation space is very promising to discriminate songs as well as to
allow a relative quantitative comparison among songs. Additionally, we believe
that the here-reported method may be applied in practical situations since it
is simple, robust and has a fast numerical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2318</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2318</id><created>2011-12-10</created><updated>2013-06-03</updated><authors><author><keyname>Mishra</keyname><forenames>B.</forenames></author><author><keyname>Meyer</keyname><forenames>G.</forenames></author><author><keyname>Bach</keyname><forenames>F.</forenames></author><author><keyname>Sepulchre</keyname><forenames>R.</forenames></author></authors><title>Low-rank optimization with trace norm penalty</title><categories>math.OC cs.LG</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the problem of low-rank trace norm minimization. We
propose an algorithm that alternates between fixed-rank optimization and
rank-one updates. The fixed-rank optimization is characterized by an efficient
factorization that makes the trace norm differentiable in the search space and
the computation of duality gap numerically tractable. The search space is
nonlinear but is equipped with a particular Riemannian structure that leads to
efficient computations. We present a second-order trust-region algorithm with a
guaranteed quadratic rate of convergence. Overall, the proposed optimization
scheme converges super-linearly to the global solution while maintaining
complexity that is linear in the number of rows and columns of the matrix. To
compute a set of solutions efficiently for a grid of regularization parameters
we propose a predictor-corrector approach that outperforms the naive
warm-restart approach on the fixed-rank quotient manifold. The performance of
the proposed algorithm is illustrated on problems of low-rank matrix completion
and multivariate linear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2328</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2328</id><created>2011-12-11</created><authors><author><keyname>Lin</keyname><forenames>Wang</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Yang</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Zeng</keyname><forenames>Zhenbing</forenames></author></authors><title>Exact Safety Verification of Hybrid Systems Using Sums-Of-Squares
  Representation</title><categories>cs.SE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss how to generate inductive invariants for safety
verification of hybrid systems. A hybrid symbolic-numeric method is presented
to compute inequality inductive invariants of the given systems. A numerical
invariant of the given system can be obtained by solving a parameterized
polynomial optimization problem via sum-of-squares (SOS) relaxation. And a
method based on Gauss-Newton refinement and rational vector recovery is
deployed to obtain the invariants with rational coefficients, which exactly
satisfy the conditions of invariants. Several examples are given to illustrate
our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2336</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2336</id><created>2011-12-11</created><authors><author><keyname>Soudani</keyname><forenames>Nasrin Mazaheri</forenames></author><author><keyname>Baraani-Dastgerdi</keyname><forenames>Ahmad</forenames></author></authors><title>The Spatial Nearest Neighbor Skyline Queries</title><categories>cs.DB</categories><comments>15 pages, 14 figures, Journal:International Journal of Database
  Management Systems (IJDMS)</comments><journal-ref>International Journal of Database Management Systems (IJDMS),
  Vol.3, No.4, November 2011, 65-79</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User preference queries are very important in spatial databases. With the
help of these queries, one can found best location among points saved in
database. In many situation users evaluate quality of a location with its
distance from its nearest neighbor among a special set of points. There has
been less attention about evaluating a location with its distance to nearest
neighbors in spatial user preference queries. This problem has application in
many domains such as service recommendation systems and investment planning.
Related works in this field are based on top-k queries. The problem with top-k
queries is that user must set weights for attributes and a function for
aggregating them. This is hard for him in most cases. In this paper a new type
of user preference queries called spatial nearest neighbor skyline queries will
be introduced in which user has some sets of points as query parameters. For
each point in database attributes are its distances to the nearest neighbors
from each set of query points. By separating this query as a subset of dynamic
skyline queries N2S2 algorithm is provided for computing it. This algorithm has
good performance compared with the general branch and bound algorithm for
skyline queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2361</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2361</id><created>2011-12-11</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Pach</keyname><forenames>Janos</forenames></author><author><keyname>Suk</keyname><forenames>Andrew</forenames></author></authors><title>The number of edges in k-quasi-planar graphs</title><categories>math.CO cs.CG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1106.0958</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph drawn in the plane is called k-quasi-planar if it does not contain k
pairwise crossing edges. It has been conjectured for a long time that for every
fixed k, the maximum number of edges of a k-quasi-planar graph with n vertices
is O(n). The best known upper bound is n(\log n)^{O(\log k)}. In the present
note, we improve this bound to (n\log n)2^{\alpha^{c_k}(n)} in the special case
where the graph is drawn in such a way that every pair of edges meet at most
once. Here \alpha(n) denotes the (extremely slowly growing) inverse of the
Ackermann function. We also make further progress on the conjecture for
k-quasi-planar graphs in which every edge is drawn as an x-monotone curve.
Extending some ideas of Valtr, we prove that the maximum number of edges of
such graphs is at most 2^{ck^6}n\log n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2372</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2372</id><created>2011-12-11</created><authors><author><keyname>Yuan</keyname><forenames>Di</forenames></author><author><keyname>Joung</keyname><forenames>Jingon</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>On Tractability Aspects of Optimal Resource Allocation in OFDMA Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint channel and rate allocation with power minimization in orthogonal
frequency-division multiple access (OFDMA) has attracted extensive attention.
Most of the research has dealt with the development of sub-optimal but
low-complexity algorithms. In this paper, the contributions comprise new
insights from revisiting tractability aspects of computing optimum. Previous
complexity analyses have been limited by assumptions of fixed power on each
subcarrier, or power-rate functions that locally grow arbitrarily fast. The
analysis under the former assumption does not generalize to problem
tractability with variable power, whereas the latter assumption prohibits the
result from being applicable to well-behaved power-rate functions. As the first
contribution, we overcome the previous limitations by rigorously proving the
problem's NP-hardness for the representative logarithmic rate function. Next,
we extend the proof to reach a much stronger result, namely that the problem
remains NP-hard, even if the channels allocated to each user is restricted to a
consecutive block with given size. We also prove that, under these
restrictions, there is a special case with polynomial-time tractability. Then,
we treat the problem class where the channels can be partitioned into an
arbitrarily large but constant number of groups, each having uniform gain for
every individual user. For this problem class, we present a polynomial-time
algorithm and prove optimality guarantee. In addition, we prove that the
recognition of this class is polynomial-time solvable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2374</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2374</id><created>2011-12-11</created><authors><author><keyname>Cui</keyname><forenames>Hongyu</forenames></author><author><keyname>Zhang</keyname><forenames>Rongqing</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Performance Analysis of Bidirectional Relay Selection with Imperfect
  Channel State Information</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Communications, 17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the performance of bidirectional relay
selection using amplify-and-forward protocol with imperfect channel state
information, i.e., delay effect and channel estimation error. The asymptotic
expression of end-to-end SER in high SNR regime is derived in a closed form,
which indicates that the delay effect causes the loss of both coding gain and
diversity order, while the channel estimation error merely affects the coding
gain. Finally, analytical results are verified by Monte-Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2386</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2386</id><created>2011-12-11</created><authors><author><keyname>Pakdelazar'</keyname><forenames>'Omid</forenames></author><author><keyname>Rezai-rad'</keyname><forenames>'Gholamali</forenames></author></authors><title>Improvement of BM3D Algorithm and Employment to Satellite and CFA Images
  Denoising</title><categories>cs.CV</categories><comments>11 pages, 7 figure</comments><doi>10.5121/ijist.2011.1303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new procedure in order to improve the performance of
block matching and 3-D filtering (BM3D) image denoising algorithm. It is
demonstrated that it is possible to achieve a better performance than that of
BM3D algorithm in a variety of noise levels. This method changes BM3D algorithm
parameter values according to noise level, removes prefiltering, which is used
in high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual
quality get improved, and BM3D complexities and processing time are reduced.
This improved BM3D algorithm is extended and used to denoise satellite and
color filter array (CFA) images. Output results show that the performance has
upgraded in comparison with current methods of denoising satellite and CFA
images. In this regard this algorithm is compared with Adaptive PCA algorithm,
that has led to superior performance for denoising CFA images, on the subject
of PSNR and visual quality. Also the processing time has decreased
significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2388</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2388</id><created>2011-12-11</created><authors><author><keyname>Xuan</keyname><forenames>Zhao-Guo</forenames></author><author><keyname>Li</keyname><forenames>Zhan</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author></authors><title>Information Filtering via Implicit Trust-based Network</title><categories>physics.data-an cs.IR</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the user-item bipartite network, collaborative filtering (CF)
recommender systems predict users' interests according to their history
collections, which is a promising way to solve the information exploration
problem. However, CF algorithm encounters cold start and sparsity problems. The
trust-based CF algorithm is implemented by collecting the users' trust
statements, which is time-consuming and must use users' private friendship
information. In this paper, we present a novel measurement to calculate users'
implicit trust-based correlation by taking into account their average ratings,
rating ranges, and the number of common rated items. By applying the similar
idea to the items, a item-based CF algorithm is constructed. The simulation
results on three benchmark data sets show that the performances of both
user-based and item-based algorithms could be enhanced greatly. Finally, a
hybrid algorithm is constructed by integrating the user-based and item-based
algorithms, the simulation results indicate that hybrid algorithm outperforms
the state-of-the-art methods. Specifically, it can not only provide more
accurate recommendations, but also alleviate the cold start problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2392</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2392</id><created>2011-12-11</created><authors><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author></authors><title>Information filtering via biased heat conduction</title><categories>physics.data-an cs.IR</categories><comments>4 pages, 3 figures</comments><journal-ref>Phys. Rev. E 87 (2011) 037101</journal-ref><doi>10.1103/PhysRevE.84.037101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heat conduction process has recently found its application in personalized
recommendation [T. Zhou \emph{et al.}, PNAS 107, 4511 (2010)], which is of high
diversity but low accuracy. By decreasing the temperatures of small-degree
objects, we present an improved algorithm, called biased heat conduction (BHC),
which could simultaneously enhance the accuracy and diversity. Extensive
experimental analyses demonstrate that the accuracy on MovieLens, Netflix and
Delicious datasets could be improved by 43.5%, 55.4% and 19.2% compared with
the standard heat conduction algorithm, and the diversity is also increased or
approximately unchanged. Further statistical analyses suggest that the present
algorithm could simultaneously identify users' mainstream and special tastes,
resulting in better performance than the standard heat conduction algorithm.
This work provides a creditable way for highly efficient information filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2394</identifier>
 <datestamp>2011-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2394</id><created>2011-12-11</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK</affiliation></author><author><keyname>Fousse</keyname><forenames>Laurent</forenames><affiliation>LJK</affiliation></author><author><keyname>Reynaud</keyname><forenames>Jean-Claude</forenames><affiliation>RC</affiliation></author></authors><title>A duality between exceptions and states</title><categories>cs.LO math.CT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note we study the semantics of two basic computational effects,
exceptions and states, from a new point of view. In the handling of exceptions
we dissociate the control from the elementary operation which recovers from the
exception. In this way it becomes apparent that there is a duality, in the
categorical sense, between exceptions and states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2396</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2396</id><created>2011-12-11</created><updated>2012-09-06</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK, Universit&#xe9; de Grenoble, France</affiliation></author><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK, Universit&#xe9; de Grenoble, France</affiliation></author><author><keyname>Fousse</keyname><forenames>Laurent</forenames><affiliation>LJK, Universit&#xe9; de Grenoble, France</affiliation></author><author><keyname>Reynaud</keyname><forenames>Jean-Claude</forenames><affiliation>LJK, Universit&#xe9; de Grenoble, France</affiliation></author></authors><title>Decorated proofs for computational effects: States</title><categories>cs.PL cs.LO math.CT</categories><comments>In Proceedings ACCAT 2012, arXiv:1208.4301</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 93, 2012, pp. 45-59</journal-ref><doi>10.4204/EPTCS.93.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntax of an imperative language does not mention explicitly the state,
while its denotational semantics has to mention it. In this paper we show that
the equational proofs about an imperative language may hide the state, in the
same way as the syntax does.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2401</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2401</id><created>2011-12-11</created><authors><author><keyname>Rekik</keyname><forenames>Jihen Drira</forenames></author><author><keyname>Baccouche</keyname><forenames>Leila</forenames></author><author><keyname>Ghezala</keyname><forenames>Henda Ben</forenames></author></authors><title>A Real-Time Database QoS-aware Service Selection Protocol for MANET</title><categories>cs.DB</categories><comments>16 pages; International Journal of Database Management Systems
  (IJDMS), November 2011, 101-116</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real-time database service selection depends typically to the system
stability in order to handle the time-constrained transactions within their
deadline. However, applying the real-time database system in the mobile ad hoc
networks requires considering the mobile nodes limited capacities. In this
paper, we propose cross-layer service selection which combines performance
metrics measured in the real-time database system to those used by the routing
protocol in order to make the best selection decision. It ensures both
timeliness and energy efficiency by avoiding low-power and busy service
provider node. A multicast packet is used in order to reduce the transmission
cost and network load when sending the same packet to multiple service
providers. In this paper, we evaluate the performance of our proposed protocol.
Simulation results, using the Network Simulator NS2, improve that the protocol
decreases the deadline miss ratio of packets, increases the service
availability and reduces the service response time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2404</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2404</id><created>2011-12-11</created><authors><author><keyname>Rekik</keyname><forenames>Jihen Drira</forenames></author><author><keyname>Baccouche</keyname><forenames>Leila</forenames></author><author><keyname>Ghezala</keyname><forenames>Henda Ben</forenames></author></authors><title>Performance Evaluation and Impact of Weighting Factors on an Energy and
  Delay Aware Dynamic Source Routing Protocol</title><categories>cs.NI</categories><comments>20 pages; International Journal of Computer Science &amp; Information
  Technology (IJCSIT) Vol 3, No 4, August 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical applications of the mobile ad-hoc network, MANET, are in disaster
recovery operations which have to respect time constraint needs. Since MANET is
affected by limited resources such as power constraints, it is a challenge to
respect the deadline of a real-time data. This paper proposes the Energy and
Delay aware based on Dynamic Source Routing protocol, ED-DSR. ED-DSR
efficiently utilizes the network resources such as the intermediate mobile
nodes energy and load. It ensures both timeliness and energy efficiency by
avoiding low-power and overloaded intermediate mobile nodes. Through
simulations, we compare our proposed routing protocol with the basic routing
protocol Dynamic Source Routing, DSR. Weighting factors are introduced to
improve the route selection. Simulation results, using the NS-2 simulator, show
that the proposed protocol prolongs the network lifetime (up to 66%), increases
the volume of packets delivered while meeting the data flows real-time
constraints and shortens the endto- end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2408</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2408</id><created>2011-12-11</created><authors><author><keyname>Al-Mejibli</keyname><forenames>Intisar</forenames></author><author><keyname>Colley</keyname><forenames>Martin</forenames></author></authors><title>Maximum Production of Transmission Messages Rate for Service Discovery
  Protocols</title><categories>cs.NI cs.AI</categories><comments>20 pages, 8 figures; International Journal of Computer Networks &amp;
  Communications (IJCNC) Vol.3, No.6, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing the number of dropped User Datagram Protocol (UDP) messages in a
network is regarded as a challenge by researchers. This issue represents
serious problems for many protocols particularly those that depend on sending
messages as part of their strategy, such us service discovery protocols. This
paper proposes and evaluates an algorithm to predict the minimum period of time
required between two or more consecutive messages and suggests the minimum
queue sizes for the routers, to manage the traffic and minimise the number of
dropped messages that has been caused by either congestion or queue overflow or
both together. The algorithm has been applied to the Universal Plug and Play
(UPnP) protocol using ns2 simulator. It was tested when the routers were
connected in two configurations; as a centralized and de centralized. The
message length and bandwidth of the links among the routers were taken in the
consideration. The result shows Better improvement in number of dropped
messages `among the routers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2409</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2409</id><created>2011-12-11</created><authors><author><keyname>Iannello</keyname><forenames>Fabio</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author></authors><title>Medium Access Control Protocols for Wireless Sensor Networks with Energy
  Harvesting</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of Medium Access Control (MAC) protocols for wireless sensor
networks (WSNs) has been conventionally tackled by assuming battery-powered
devices and by adopting the network lifetime as the main performance criterion.
While WSNs operated by energy-harvesting (EH) devices are not limited by
network lifetime, they pose new design challenges due to the uncertain amount
of harvestable energy. Novel design criteria are thus required to capture the
trade-offs between the potentially infinite network lifetime and the uncertain
energy availability. This paper addresses the analysis and design of WSNs with
EH devices by focusing on conventional MAC protocols, namely TDMA, Framed-ALOHA
(FA) and Dynamic-FA (DFA), and by accounting for the performance trade-offs and
design issues arising due to EH. A novel metric, referred to as delivery
probability, is introduced to measure the capability of a MAC protocol to
deliver the measure of any sensor in the network to the intended destination
(or fusion center, FC). The interplay between delivery efficiency and time
efficiency (i.e., the data collection rate at the FC), is investigated
analytically using Markov models. Numerical results validate the analysis and
emphasize the critical importance of accounting for both delivery probability
and time efficiency in the design of EH-WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2410</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2410</id><created>2011-12-11</created><authors><author><keyname>Al-Mejibli</keyname><forenames>Intisar</forenames></author><author><keyname>Al-Majeed</keyname><forenames>Martin Colley Salah</forenames></author></authors><title>Networks Utilization Improvements for Service Discovery Performance</title><categories>cs.NI cs.AI</categories><comments>20 pages, 22 figures; International Journal on Cloud Computing:
  Services and Architecture (IJCCSA),Vol.1, No.3, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service discovery requests' messages have a vital role in sharing and
locating resources in many of service discovery protocols. Sending more
messages than a link can handle may cause congestion and loss of messages which
dramatically influences the performance of these protocols. Re-send the lost
messages result in latency and inefficiency in performing the tasks which
user(s) require from the connected nodes. This issue become a serious problem
in two cases: first, when the number of clients which performs a service
discovery request is increasing, as this result in increasing in the number of
sent discovery messages; second, when the network resources such as bandwidth
capacity are consumed by other applications. These two cases lead to network
congestion and loss of messages. This paper propose an algorithm to improve the
services discovery protocols performance by separating each consecutive burst
of messages with a specific period of time which calculated regarding the
available network resources. It was tested when the routers were connected in
two configurations; decentralised and centralised .In addition, this paper
explains the impact of increasing the number of clients and the consumed
network resources on the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2431</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2431</id><created>2011-12-11</created><updated>2012-09-04</updated><authors><author><keyname>Mohammadi</keyname><forenames>Arash</forenames></author><author><keyname>Asif</keyname><forenames>Amir</forenames></author></authors><title>Distributed Particle Filter Implementation with Intermittent/Irregular
  Consensus Convergence</title><categories>cs.DC</categories><comments>Revised Version Submitted to IEEE Transaction on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by non-linear, non-Gaussian, distributed multi-sensor/agent
navigation and tracking applications, we propose a multi-rate consensus/fusion
based framework for distributed implementation of the particle filter (CF/DPF).
The CF/DPF framework is based on running localized particle filters to estimate
the overall state vector at each observation node. Separate fusion filters are
designed to consistently assimilate the local filtering distributions into the
global posterior by compensating for the common past information between
neighbouring nodes. The CF/DPF offers two distinct advantages over its
counterparts. First, the CF/DPF framework is suitable for scenarios where
network connectivity is intermittent and consensus can not be reached between
two consecutive observations. Second, the CF/DPF is not limited to the Gaussian
approximation for the global posterior density. A third contribution of the
paper is the derivation of the exact expression for computing the posterior
Cramer-Rao lower bound (PCRLB) for the distributed architecture based on a
recursive procedure involving the local Fisher information matrices (FIM) of
the distributed estimators. The performance of the CF/DPF algorithm closely
follows the centralized particle filter approaching the PCRLB at the signal to
noise ratios that we tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2437</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2437</id><created>2011-12-11</created><updated>2012-09-11</updated><authors><author><keyname>Korcak</keyname><forenames>Omer</forenames></author><author><keyname>Iosifidis</keyname><forenames>George</forenames></author><author><keyname>Alpcan</keyname><forenames>Tansu</forenames></author><author><keyname>Koutsopoulos</keyname><forenames>Iordanis</forenames></author></authors><title>Competition and Regulation in Wireless Services Markets</title><categories>cs.NI cs.GT cs.SY</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless services market where a set of operators compete for a
large common pool of users. The latter have a reservation utility of U0 units
or, equivalently, an alternative option to satisfy their communication needs.
The operators must satisfy these minimum requirements in order to attract the
users. We model the users decisions and interaction as an evolutionary game and
the competition among the operators as a non cooperative price game which is
proved to be a potential game. For each set of prices selected by the
operators, the evolutionary game attains a different stationary point. We show
that the outcome of both games depend on the reservation utility of the users
and the amount of spectrum W the operators have at their disposal. We express
the market welfare and the revenue of the operators as functions of these two
parameters. Accordingly, we consider the scenario where a regulating agency is
able to intervene and change the outcome of the market by tuning W and/or U0.
Different regulators may have different objectives and criteria according to
which they intervene. We analyze the various possible regulation methods and
discuss their requirements, implications and impact on the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2444</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2444</id><created>2011-12-12</created><authors><author><keyname>Schreiner</keyname><forenames>Steffen</forenames></author><author><keyname>Betev</keyname><forenames>Latchezar</forenames></author><author><keyname>Grigoras</keyname><forenames>Costin</forenames></author><author><keyname>Litmaath</keyname><forenames>Maarten</forenames></author></authors><title>A Mediated Definite Delegation Model allowing for Certified Grid Job
  Submission</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid computing infrastructures need to provide traceability and accounting of
their users&quot; activity and protection against misuse and privilege escalation. A
central aspect of multi-user Grid job environments is the necessary delegation
of privileges in the course of a job submission. With respect to these generic
requirements this document describes an improved handling of multi-user Grid
jobs in the ALICE (&quot;A Large Ion Collider Experiment&quot;) Grid Services. A security
analysis of the ALICE Grid job model is presented with derived security
objectives, followed by a discussion of existing approaches of unrestricted
delegation based on X.509 proxy certificates and the Grid middleware gLExec.
Unrestricted delegation has severe security consequences and limitations, most
importantly allowing for identity theft and forgery of delegated assignments.
These limitations are discussed and formulated, both in general and with
respect to an adoption in line with multi-user Grid jobs. Based on the
architecture of the ALICE Grid Services, a new general model of mediated
definite delegation is developed and formulated, allowing a broker to assign
context-sensitive user privileges to agents. The model provides strong
accountability and long- term traceability. A prototype implementation allowing
for certified Grid jobs is presented including a potential interaction with
gLExec. The achieved improvements regarding system security, malicious job
exploitation, identity protection, and accountability are emphasized, followed
by a discussion of non- repudiation in the face of malicious Grid jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2459</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2459</id><created>2011-12-12</created><updated>2012-02-09</updated><authors><author><keyname>Abbasi</keyname><forenames>Alireza</forenames></author><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author></authors><title>Hybrid Centrality Measures for Binary and Weighted Networks</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>a short version accepted in the 3rd workshop on Complex Network [Full
  Paper submitted to JASIST in April 2011]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing centrality measures for social network analysis suggest the
im-portance of an actor and give consideration to actor's given structural
position in a network. These existing measures suggest specific attribute of an
actor (i.e., popularity, accessibility, and brokerage behavior). In this study,
we propose new hybrid centrality measures (i.e., Degree-Degree,
Degree-Closeness and Degree-Betweenness), by combining existing measures (i.e.,
degree, closeness and betweenness) with a proposition to better understand the
importance of actors in a given network. Generalized set of measures are also
proposed for weighted networks. Our analysis of co-authorship networks dataset
suggests significant correlation of our proposed new centrality measures
(especially weighted networks) than traditional centrality measures with
performance of the scholars. Thus, they are useful measures which can be used
instead of traditional measures to show prominence of the actors in a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2460</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2460</id><created>2011-12-12</created><authors><author><keyname>Abbasi</keyname><forenames>Alireza</forenames></author><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author><author><keyname>Wigand</keyname><forenames>Rolf</forenames></author></authors><title>Social Capital and Individual Performance: A Study of Academic
  Collaboration</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>submitted to JASIST</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies on social networks highlight the importance of network structure or
structural properties of a given network and its impact on performance outcome.
One of the important properties of this network structure is referred as
&quot;social capital&quot; which is the &quot;network of contacts&quot; and the associated values
attached to these networks of contacts. In this study, our aim is to provide
empirical evidence of the influence of social capital and performance within
the context of academic collaboration. We suggest that the collaborative
process involves social capital embedded within relationships and network
structures among direct co-authors. Thus, we examine whether scholars' social
capital is associated with their citation-based performance, using
co-authorship and citation data. In order to test and validate our proposed
hypotheses, we extract publication records from Scopus having &quot;information
science&quot; in their title or keywords or abstracts during 2001 and 2010. To
overcome the limitations of traditional social network metrics for measuring
the influence of scholars' social capital within their co-authorship network,
we extend the traditional social network metrics by proposing a new measure
(Power-Diversity Index). We then use Spearman's correlation rank test to
examine the association between scholars' social capital measures and their
citation-based performance. Results suggest that research performance of
authors is positively correlated with their social capital measures. This study
highlights that the Power-diversity Index, which is introduced as a new hybrid
centrality measure, serves as an indicator of power and influence of an
individual's ability to control communication and information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2468</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2468</id><created>2011-12-12</created><authors><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author></authors><title>Creating a Live, Public Short Message Service Corpus: The NUS SMS Corpus</title><categories>cs.CL</categories><comments>It contains 31 pages, 6 figures, and 10 tables. It has been submitted
  to Language Resource and Evaluation Journal</comments><journal-ref>Language Resources and Evaluation, Aug 2012</journal-ref><doi>10.1007/s10579-012-9197-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short Message Service (SMS) messages are largely sent directly from one
person to another from their mobile phones. They represent a means of personal
communication that is an important communicative artifact in our current
digital era. As most existing studies have used private access to SMS corpora,
comparative studies using the same raw SMS data has not been possible up to
now. We describe our efforts to collect a public SMS corpus to address this
problem. We use a battery of methodologies to collect the corpus, paying
particular attention to privacy issues to address contributors' concerns. Our
live project collects new SMS message submissions, checks their quality and
adds the valid messages, releasing the resultant corpus as XML and as SQL
dumps, along with corpus statistics, every month. We opportunistically collect
as much metadata about the messages and their sender as possible, so as to
enable different types of analyses. To date, we have collected about 60,000
messages, focusing on English and Mandarin Chinese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2475</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2475</id><created>2011-12-12</created><authors><author><keyname>Haruna</keyname><forenames>Taichi</forenames></author><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author></authors><title>Permutation Complexity via Duality between Values and Orderings</title><categories>nlin.CD cs.IT math.IT physics.data-an</categories><comments>26 pages</comments><journal-ref>Physica D 240 (2011) 1370-1377</journal-ref><doi>10.1016/j.physd.2011.05.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the permutation complexity of finite-state stationary stochastic
processes based on a duality between values and orderings between values.
First, we establish a duality between the set of all words of a fixed length
and the set of all permutations of the same length. Second, on this basis, we
give an elementary alternative proof of the equality between the permutation
entropy rate and the entropy rate for a finite-state stationary stochastic
processes first proved in [Amigo, J.M., Kennel, M. B., Kocarev, L., 2005.
Physica D 210, 77-95]. Third, we show that further information on the
relationship between the structure of values and the structure of orderings for
finite-state stationary stochastic processes beyond the entropy rate can be
obtained from the established duality. In particular, we prove that the
permutation excess entropy is equal to the excess entropy, which is a measure
of global correlation present in a stationary stochastic process, for
finite-state stationary ergodic Markov processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2483</identifier>
 <datestamp>2012-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2483</id><created>2011-12-12</created><updated>2012-02-15</updated><authors><author><keyname>Liu</keyname><forenames>Nan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Maric</keyname><forenames>Ivana</forenames><affiliation>Shitz</affiliation></author><author><keyname>Cheng</keyname><forenames>Yinghui</forenames><affiliation>Shitz</affiliation></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Capacity Bounds and Exact Results for the Cognitive Z-interference
  Channel</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn due to the incorrectness of Theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the discrete memoryless Z-interference channel (ZIC) where the
transmitter of the pair that suffers from interference is cognitive. We first
provide an upper bound on the capacity of this channel. We then show that, when
the channel of the transmitter-receiver pair that does not experience
interference is deterministic, our proposed upper bound matches the known lower
bound provided by Cao and Chen in 2008. The obtained results imply that, unlike
in the Gaussian cognitive ZIC, in the considered channel superposition encoding
at the non-cognitive transmitter as well as Gel'fand-Pinsker encoding at the
cognitive transmitter are needed in order to minimize the impact of
interference. As a byproduct of the obtained capacity region, we obtain the
capacity under the generalized Gel'fand-Pinsker conditions where a
transmitter-receiver pair communicates in the presence of interference
noncausally known at the encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2489</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2489</id><created>2011-12-12</created><updated>2012-03-13</updated><authors><author><keyname>Scheiblechner</keyname><forenames>Peter</forenames></author></authors><title>Effective de Rham Cohomology - The Hypersurface Case</title><categories>math.AG cs.CC</categories><comments>6 pages, proof of Lemma 1 was unclear, main result now proved without
  it; bound slightly changed</comments><msc-class>14F40, 14Q20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove an effective bound for the degrees of generators of the algebraic de
Rham cohomology of smooth affine hypersurfaces. In particular, we show that the
de Rham cohomology H_dR^p(X) of a smooth hypersurface X of degree d in C^n can
be generated by differential forms of degree d^O(pn). This result is relevant
for the algorithmic computation of the cohomology, but is also motivated by
questions in the theory of ordinary differential equations related to the
infinitesimal Hilbert 16th problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2491</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2491</id><created>2011-12-12</created><authors><author><keyname>Haruna</keyname><forenames>Taichi</forenames></author><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author></authors><title>Permutation Excess Entropy and Mutual Information between the Past and
  Future</title><categories>nlin.CD cs.IT math.IT physics.data-an</categories><comments>14 pages, 10th International Conference on Computing Anticipatory
  Systems, Liege, Belgium, August 11, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the excess entropy, which is a measure of complexity for
stationary time series, from the ordinal point of view. We show that the
permutation excess entropy is equal to the mutual information between two
adjacent semi-infinite blocks in the space of orderings for finite-state
stationary ergodic Markov processes. This result may shed a new light on the
relationship between complexity and anticipation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2493</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2493</id><created>2011-12-12</created><updated>2013-03-11</updated><authors><author><keyname>Haruna</keyname><forenames>Taichi</forenames></author><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author></authors><title>Symbolic transfer entropy rate is equal to transfer entropy rate for
  bivariate finite-alphabet stationary ergodic Markov processes</title><categories>nlin.CD cs.IT math.IT physics.data-an</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer entropy is a measure of the magnitude and the direction of
information flow between jointly distributed stochastic processes. In recent
years, its permutation analogues are considered in the literature to estimate
the transfer entropy by counting the number of occurrences of orderings of
values, not the values themselves. It has been suggested that the method of
permutation is easy to implement, computationally low cost and robust to noise
when applying to real world time series data. In this paper, we initiate a
theoretical treatment of the corresponding rates. In particular, we consider
the transfer entropy rate and its permutation analogue, the symbolic transfer
entropy rate, and show that they are equal for any bivariate finite-alphabet
stationary ergodic Markov process. This result is an illustration of the
duality method introduced in [T. Haruna and K. Nakajima, Physica D 240, 1370
(2011)]. We also discuss the relationship among the transfer entropy rate, the
time-delayed mutual information rate and their permutation analogues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2495</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2495</id><created>2011-12-12</created><updated>2012-05-21</updated><authors><author><keyname>Gravier</keyname><forenames>Sylvain</forenames></author><author><keyname>Javelle</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Mhalla</keyname><forenames>Mehdi</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>On Weak Odd Domination and Graph-based Quantum Secret Sharing</title><categories>cs.CC quant-ph</categories><comments>Subsumes arXiv:1109.6181: Optimal accessing and non-accessing
  structures for graph protocols</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weak odd dominated (WOD) set in a graph is a subset B of vertices for which
there exists a distinct set of vertices C such that every vertex in B has an
odd number of neighbors in C. We point out the connections of weak odd
domination with odd domination, [sigma,rho]-domination, and perfect codes. We
introduce bounds on \kappa(G), the maximum size of WOD sets of a graph G, and
on \kappa'(G), the minimum size of non WOD sets of G. Moreover, we prove that
the corresponding decision problems are NP-complete. The study of weak odd
domination is mainly motivated by the design of graph-based quantum secret
sharing protocols: a graph G of order n corresponds to a secret sharing
protocol which threshold is \kappa_Q(G) = max(\kappa(G), n-\kappa'(G)). These
graph-based protocols are very promising in terms of physical implementation,
however all such graph-based protocols studied in the literature have
quasi-unanimity thresholds (i.e. \kappa_Q(G)=n-o(n) where n is the order of the
graph G underlying the protocol). In this paper, we show using probabilistic
methods, the existence of graphs with smaller \kappa_Q (i.e. \kappa_Q(G)&lt;
0.811n where n is the order of G). We also prove that deciding for a given
graph G whether \kappa_Q(G)&lt; k is NP-complete, which means that one cannot
efficiently double check that a graph randomly generated has actually a
\kappa_Q smaller than 0.811n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2516</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2516</id><created>2011-12-12</created><updated>2012-09-25</updated><authors><author><keyname>Schneider</keyname><forenames>Jesper W.</forenames></author></authors><title>Caveats for using statistical significance tests in research assessments</title><categories>cs.DL stat.AP</categories><comments>Accepted version for Journal of Informetrics</comments><doi>10.1016/j.joi.2012.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper raises concerns about the advantages of using statistical
significance tests in research assessments as has recently been suggested in
the debate about proper normalization procedures for citation indicators.
Statistical significance tests are highly controversial and numerous criticisms
have been leveled against their use. Based on examples from articles by
proponents of the use statistical significance tests in research assessments,
we address some of the numerous problems with such tests. The issues
specifically discussed are the ritual practice of such tests, their dichotomous
application in decision making, the difference between statistical and
substantive significance, the implausibility of most null hypotheses, the
crucial assumption of randomness, as well as the utility of standard errors and
confidence intervals for inferential purposes. We argue that applying
statistical significance tests and mechanically adhering to their results is
highly problematic and detrimental to critical thinking. We claim that the use
of such tests do not provide any advantages in relation to citation indicators,
interpretations of them, or the decision making processes based upon them. On
the contrary their use may be harmful. Like many other critics, we generally
believe that statistical significance tests are over- and misused in the social
sciences including scientometrics and we encourage a reform on these matters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2519</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2519</id><created>2011-12-12</created><authors><author><keyname>Vispute</keyname><forenames>Ritesh</forenames></author></authors><title>Errors in Improved Polynomial Algorithm For 3 Sat Proposed By Narendra
  Chaudhari</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are errors in the algorithm proposed by Narendra Chaudhari [2]
purporting to solve the 3-sat problem in polynomial time. The present paper
present instances for which the algorithm outputs erroneous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2558</identifier>
 <datestamp>2011-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2558</id><created>2011-12-12</created><authors><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Success-driven distribution of public goods promotes cooperation but
  preserves defection</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><comments>4 two-column pages, 4 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 84 (2011) 037102</journal-ref><doi>10.1103/PhysRevE.84.037102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Established already in the Biblical times, the Matthew effect stands for the
fact that in societies rich tend to get richer and the potent even more
powerful. Here we investigate a game theoretical model describing the evolution
of cooperation on structured populations where the distribution of public goods
is driven by the reproductive success of individuals. Phase diagrams reveal
that cooperation is promoted irrespective of the uncertainty by strategy
adoptions and the type of interaction graph, yet the complete dominance of
cooperators is elusive due to the spontaneous emergence of super-persistent
defectors that owe their survival to extremely rare microscopic patterns. This
indicates that success-driven mechanisms are crucial for effectively harvesting
benefits from collective actions, but that they may also account for the
observed persistence of maladaptive behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2584</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2584</id><created>2011-12-12</created><authors><author><keyname>Mahmoud</keyname><forenames>Mahmoud S.</forenames></author><author><keyname>Ensor</keyname><forenames>Andrew</forenames></author><author><keyname>Biem</keyname><forenames>Alain</forenames></author><author><keyname>Elmegreen</keyname><forenames>Bruce</forenames></author><author><keyname>Gulyaev</keyname><forenames>Sergei</forenames></author></authors><title>Data Provenance and Management in Radio Astronomy: A Stream Computing
  Approach</title><categories>cs.DC astro-ph.IM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New approaches for data provenance and data management (DPDM) are required
for mega science projects like the Square Kilometer Array, characterized by
extremely large data volume and intense data rates, therefore demanding
innovative and highly efficient computational paradigms. In this context, we
explore a stream-computing approach with the emphasis on the use of
accelerators. In particular, we make use of a new generation of high
performance stream-based parallelization middleware known as InfoSphere
Streams. Its viability for managing and ensuring interoperability and integrity
of signal processing data pipelines is demonstrated in radio astronomy. IBM
InfoSphere Streams embraces the stream-computing paradigm. It is a shift from
conventional data mining techniques (involving analysis of existing data from
databases) towards real-time analytic processing. We discuss using InfoSphere
Streams for effective DPDM in radio astronomy and propose a way in which
InfoSphere Streams can be utilized for large antennae arrays. We present a
case-study: the InfoSphere Streams implementation of an autocorrelating
spectrometer, and using this example we discuss the advantages of the
stream-computing approach and the utilization of hardware accelerators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2605</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2605</id><created>2011-12-12</created><authors><author><keyname>Mahfoud</keyname><forenames>Houari</forenames><affiliation>INRIA Lorraine - LORIA / LIFC</affiliation></author><author><keyname>Imine</keyname><forenames>Abdessamad</forenames><affiliation>INRIA Lorraine - LORIA / LIFC</affiliation></author></authors><title>Secure Querying of Recursive XML Views: A Standard XPath-based Technique</title><categories>cs.CR cs.DB</categories><comments>(2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the art approaches for securing XML documents allow users to
access data only through authorized views defined by annotating an XML grammar
(e.g. DTD) with a collection of XPath expressions. To prevent improper
disclosure of confidential information, user queries posed on these views need
to be rewritten into equivalent queries on the underlying documents. This
rewriting enables us to avoid the overhead of view materialization and
maintenance. A major concern here is that query rewriting for recursive XML
views is still an open problem. To overcome this problem, some works have been
proposed to translate XPath queries into non-standard ones, called Regular
XPath queries. However, query rewriting under Regular XPath can be of
exponential size as it relies on automaton model. Most importantly, Regular
XPath remains a theoretical achievement. Indeed, it is not commonly used in
practice as translation and evaluation tools are not available. In this paper,
we show that query rewriting is always possible for recursive XML views using
only the expressive power of the standard XPath. We investigate the extension
of the downward class of XPath, composed only by child and descendant axes,
with some axes and operators and we propose a general approach to rewrite
queries under recursive XML views. Unlike Regular XPath-based works, we provide
a rewriting algorithm which processes the query only over the annotated DTD
grammar and which can run in linear time in the size of the query. An
experimental evaluation demonstrates that our algorithm is efficient and scales
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2608</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2608</id><created>2011-12-12</created><authors><author><keyname>Burioni</keyname><forenames>Raffaella</forenames></author><author><keyname>Scalco</keyname><forenames>Riccardo</forenames></author><author><keyname>Casartelli</keyname><forenames>Mario</forenames></author></authors><title>Rohlin Distance and the Evolution of Influenza A virus: Weak Attractors
  and Precursors</title><categories>q-bio.PE cond-mat.other cs.CE q-bio.QM</categories><comments>13 pages, 5+4 figures</comments><journal-ref>PLoS ONE 6(12): e27924 (2011)</journal-ref><doi>10.1371/journal.pone.0027924</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of the hemagglutinin amino acids sequences of Influenza A virus
is studied by a method based on an informational metrics, originally introduced
by Rohlin for partitions in abstract probability spaces. This metrics does not
require any previous functional or syntactic knowledge about the sequences and
it is sensitive to the correlated variations in the characters disposition. Its
efficiency is improved by algorithmic tools, designed to enhance the detection
of the novelty and to reduce the noise of useless mutations. We focus on the
USA data from 1993/94 to 2010/2011 for A/H3N2 and on USA data from 2006/07 to
2010/2011 for A/H1N1 . We show that the clusterization of the distance matrix
gives strong evidence to a structure of domains in the sequence space, acting
as weak attractors for the evolution, in very good agreement with the
epidemiological history of the virus. The structure proves very robust with
respect to the variations of the clusterization parameters, and extremely
coherent when restricting the observation window. The results suggest an
efficient strategy in the vaccine forecast, based on the presence of
&quot;precursors&quot; (or &quot;buds&quot;) populating the most recent attractor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2610</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2610</id><created>2011-12-12</created><authors><author><keyname>Karanasos</keyname><forenames>Konstantinos</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Katsifodimos</keyname><forenames>Asterios</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Manolescu</keyname><forenames>Ioana</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Zoupanos</keyname><forenames>Spyros</forenames><affiliation>INRIA Saclay - Ile de France, LRI, MPII</affiliation></author></authors><title>The ViP2P Platform: XML Views in P2P</title><categories>cs.DB</categories><comments>RR-7812 (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing volumes of XML data sources on the Web or produced by
enterprises, organizations etc. raise many performance challenges for data
management applications. In this work, we are concerned with the distributed,
peer-to-peer management of large corpora of XML documents, based on distributed
hash table (or DHT, in short) overlay networks. We present ViP2P (standing for
Views in Peer-to-Peer), a distributed platform for sharing XML documents based
on a structured P2P network infrastructure (DHT). At the core of ViP2P stand
distributed materialized XML views, defined by arbitrary XML queries, filled in
with data published anywhere in the network, and exploited to efficiently
answer queries issued by any network peer. ViP2P allows user queries to be
evaluated over XML documents published by peers in two modes. First, a
long-running subscription mode, when a query can be registered in the system
and receive answers incrementally when and if published data matches the query.
Second, queries can also be asked in an ad-hoc, snapshot mode, where results
are required immediately and must be computed based on the results of other
long-running, subscription queries. ViP2P innovates over other similar
DHT-based XML sharing platforms by using a very expressive structured XML query
language. This expressivity leads to a very flexible distribution of XML
content in the ViP2P network, and to efficient snapshot query execution. ViP2P
has been tested in real deployments of hundreds of computers. We present the
platform architecture, its internal algorithms, and demonstrate its efficiency
and scalability through a set of experiments. Our experimental results outgrow
by orders of magnitude similar competitor systems in terms of data volumes,
network size and data dissemination throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2619</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2619</id><created>2011-12-09</created><authors><author><keyname>Di Maio</keyname><forenames>Paola</forenames></author></authors><title>Towards a Reference Model for Open Access and Knowledge Sharing, Lessons
  from Systems Research</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Open Access Movement has been striving to grant universal unrestricted
access to the knowledge and data outputs of publicly funded research.
leveraging the real time, virtually cost free publishing opportunities offered
by the internet and the web. However, evidence suggests that in the systems
engineering domain open access policies are not widely adopted. This paper
presents the rationale, methodology and results of an evidence based inquiry
that investigates the dichotomy between policy and practice in Open Access (OA)
of systems engineering research in the UK, explores entangled dimensions of the
problem space from a socio-technical perspective, and issues a set of
recommendations, including a reference model outline for knowledge sharing in
systems research
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2627</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2627</id><created>2011-12-12</created><authors><author><keyname>Talbi</keyname><forenames>Nesrine</forenames></author><author><keyname>Belarbi</keyname><forenames>Khaled</forenames></author></authors><title>Fast Hybrid PSO and Tabu Search Approach for Optimization of a Fuzzy
  Controller</title><categories>cs.SY</categories><comments>5 pages, 7 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 2, September 2011, 215-219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fuzzy controller type Takagi_Sugeno zero order is optimized
by the method of hybrid Particle Swarm Optimization (PSO) and Tabu Search (TS).
The algorithm automatically adjusts the membership functions of fuzzy
controller inputs and the conclusions of fuzzy rules. At each iteration of PSO,
we calculate the best solution and we seek the best neighbor by Tabu search,
this operation minimizes the number of iterations and computation time while
maintaining accuracy and minimum response time. We apply this algorithm to
optimize a fuzzy controller for a simple inverted pendulum with three rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2628</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2628</id><created>2011-12-12</created><authors><author><keyname>Krishnamoorthy</keyname><forenames>Aravindh</forenames></author><author><keyname>Muppirisetty</keyname><forenames>Leela Srikar</forenames></author><author><keyname>Jandial</keyname><forenames>Ravi</forenames></author></authors><title>Simulation Performance of MMSE Iterative Equalization with Soft Boolean
  Value Propagation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of MMSE Iterative Equalization based on MAP-SBVP and COD-MAP
algorithms (for generating extrinsic information) are compared for fading and
non-fading communication channels employing serial concatenated convolution
codes.
  MAP-SBVP is a convolution decoder using a conventional soft-MAP decoder
followed by a soft-convolution encoder using the soft-boolean value propagation
(SBVP).
  From the simulations it is observed that for MMSE Iterative Equalization,
MAP-SBVP performance is comparable to COD-MAP for fading and non-fading
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2640</identifier>
 <datestamp>2012-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2640</id><created>2011-12-12</created><updated>2012-01-28</updated><authors><author><keyname>Hern&#xe1;ndez-Orallo</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Flach</keyname><forenames>Peter</forenames></author><author><keyname>Ferri</keyname><forenames>C&#xe8;sar</forenames></author></authors><title>Threshold Choice Methods: the Missing Link</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many performance metrics have been introduced for the evaluation of
classification performance, with different origins and niches of application:
accuracy, macro-accuracy, area under the ROC curve, the ROC convex hull, the
absolute error, and the Brier score (with its decomposition into refinement and
calibration). One way of understanding the relation among some of these metrics
is the use of variable operating conditions (either in the form of
misclassification costs or class proportions). Thus, a metric may correspond to
some expected loss over a range of operating conditions. One dimension for the
analysis has been precisely the distribution we take for this range of
operating conditions, leading to some important connections in the area of
proper scoring rules. However, we show that there is another dimension which
has not received attention in the analysis of performance metrics. This new
dimension is given by the decision rule, which is typically implemented as a
threshold choice method when using scoring models. In this paper, we explore
many old and new threshold choice methods: fixed, score-uniform, score-driven,
rate-driven and optimal, among others. By calculating the loss of these methods
for a uniform range of operating conditions we get the 0-1 loss, the absolute
error, the Brier score (mean squared error), the AUC and the refinement loss
respectively. This provides a comprehensive view of performance metrics as well
as a systematic approach to loss minimisation, namely: take a model, apply
several threshold choice methods consistent with the information which is (and
will be) available about the operating condition, and compare their expected
losses. In order to assist in this procedure we also derive several connections
between the aforementioned performance metrics, and we highlight the role of
calibration in choosing the threshold choice method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2649</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2649</id><created>2011-12-12</created><authors><author><keyname>Backes</keyname><forenames>Julian</forenames></author><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>D&#xfc;rmuth</keyname><forenames>Markus</forenames></author><author><keyname>Gerling</keyname><forenames>Sebastian</forenames></author><author><keyname>Lorenz</keyname><forenames>Stefan</forenames></author></authors><title>X-pire! - A digital expiration date for images in social networks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet and its current information culture of preserving all kinds of
data cause severe problems with privacy. Most of today's Internet users,
especially teenagers, publish various kinds of sensitive information, yet
without recognizing that revealing this information might be detrimental to
their future life and career. Unflattering images that can be openly accessed
now and in the future, e.g., by potential employers, constitute a particularly
important such privacy concern. We have developed a novel, fast, and scalable
system called X-pire! that allows users to set an expiration date for images in
social networks (e.g., Facebook and Flickr) and on static websites, without
requiring any form of additional interaction with these web pages. Once the
expiration date is reached, the images become unavailable. Moreover, the
publishing user can dynamically prolong or shorten the expiration dates of his
images later, and even enforce instantaneous expiration. Rendering the approach
possible for social networks crucially required us to develop a novel technique
for embedding encrypted information within JPEG files in a way that survives
JPEG compression, even for highly optimized implementations of JPEG
post-processing with their various idiosyncrasies as commonly used in such
networks. We have implemented our system and conducted performance measurements
to demonstrate its robustness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2661</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2661</id><created>2011-12-12</created><authors><author><keyname>Yoon</keyname><forenames>Jong P.</forenames></author></authors><title>Location- and Time-Dependent VPD for Privacy-Preserving Wireless
  Accesses to Cloud Services</title><categories>cs.CR cs.DB</categories><comments>16 pages, 8 figures, International Journal on Cloud Computing:
  Services and Architecture (IJCCSA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of smartphones in recent years has changed the wireless landscape.
Smartphones have become a platform for online user interface to cloud
databases. Cloud databases may provide a large set of user-private and
sensitive data (i.e., objects), while smartphone users (i.e., subjects) provide
location-sensitive information. Secure and private services in wireless
accessing to cloud databases have been discussed actively for the past recent
years. However, the previous techniques are unsatisfactory for dynamism of
moving subjects' wireless accesses. In this paper, we propose a novel technique
to dynamically generate virtual private databases (VPD) for each access by
taking subjects' location and time information into account. The contribution
of this paper includes a privacy-preserving access control mechanism for
dynamism of wireless access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2663</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2663</id><created>2011-12-09</created><authors><author><keyname>Rajagopal</keyname><forenames>Dr. Sankar</forenames></author></authors><title>Customer Data Clustering using Data Mining Technique</title><categories>cs.DB</categories><comments>11 pages, 2 figures and 1 table</comments><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.3, No.4, November 2011</journal-ref><doi>10.5121/ijdms.2011.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification and patterns extraction from customer data is very important
for business support and decision making. Timely identification of newly
emerging trends is very important in business process. Large companies are
having huge volume of data but starving for knowledge. To overcome the
organization current issue, the new breed of technique is required that has
intelligence and capability to solve the knowledge scarcity and the technique
is called Data mining. The objectives of this paper are to identify the
high-profit, high-value and low-risk customers by one of the data mining
technique - customer clustering. In the first phase, cleansing the data and
developed the patterns via demographic clustering algorithm using IBM I-Miner.
In the second phase, profiling the data, develop the clusters and identify the
high-value low-risk customers. This cluster typically represents the 10-20
percent of customers which yields 80% of the revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1112.2679</identifier>
 <datestamp>2011-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1112.2679</id><created>2011-12-12</created><authors><author><keyname>Yuan</keyname><forenames>Xiao-Tong</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Truncated Power Method for Sparse Eigenvalue Problems</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the sparse eigenvalue problem, which is to extract
dominant (largest) sparse eigenvectors with at most $k$ non-zero components. We
propose a simple yet effective solution called truncated power method that can
approximately solve the underlying nonconvex optimization problem. A strong
sparse recovery result is proved for the truncated power method, and this
theory is our key motivation for developing the new algorithm. The proposed
method is tested on applications such as sparse principal component analysis
and the densest $k$-subgraph problem. Extensive experiments on several
synthetic and real-world large scale datasets demonstrate the competitive
empirical performance of our method.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="26000" completeListSize="102538">1122234|27001</resumptionToken>
</ListRecords>
</OAI-PMH>
