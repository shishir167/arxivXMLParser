<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:30:43Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|60001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7022</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7022</id><created>2014-04-28</created><authors><author><keyname>Gomez-Cuba</keyname><forenames>Felipe</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Scaling Laws for Infrastructure Single and Multihop Wireless Networks in
  Wideband Regimes</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With millimeter wave bands emerging as a strong candidate for 5G cellular
networks, next-generation systems may be in a unique position where spectrum is
plentiful. To assess the potential value of this spectrum, this paper derives
scaling laws on the per mobile downlink feasible rate with large bandwidth and
number of nodes, for both Infrastructure Single Hop (ISH) and Infrastructure
Multi-Hop (IMH) architectures. It is shown that, for both cases, there exist
\emph{critical bandwidth scalings} above which increasing the bandwidth no
longer increases the feasible rate per node. These critical thresholds coincide
exactly with the bandwidths where, for each architecture, the network
transitions from being degrees-of-freedom-limited to power-limited. For ISH,
this critical bandwidth threshold is lower than IMH when the number of users
per base station grows with network size. This result suggests that multi-hop
transmissions may be necessary to fully exploit large bandwidth degrees of
freedom in deployments with growing number of users per cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7041</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7041</id><created>2014-04-28</created><authors><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>Cho</keyname><forenames>Myung</forenames></author><author><keyname>Kruger</keyname><forenames>Anton</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Super-resolution Line Spectrum Estimation with Block Priors</title><categories>cs.IT math.IT math.OC</categories><comments>7 pages, double column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of super-resolution line spectrum estimation of an
undersampled signal with block prior information. The component frequencies of
the signal are assumed to take arbitrary continuous values in known frequency
blocks. We formulate a general semidefinite program to recover these
continuous-valued frequencies using theories of positive trigonometric
polynomials. The proposed semidefinite program achieves super-resolution
frequency recovery by taking advantage of known structures of frequency blocks.
Numerical experiments show great performance enhancements using our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7045</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7045</id><created>2014-04-28</created><updated>2014-05-23</updated><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayllon</keyname><forenames>Juan Manuel</forenames></author><author><keyname>Martin-Martin</keyname><forenames>Alberto</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Empirical Evidences in Citation-Based Search Engines: Is Microsoft
  Academic Search dead?</title><categories>cs.DL</categories><comments>14 pages, 7 figures, 6 tables</comments><report-no>EC3 16</report-no><journal-ref>Online Information Review, Vol. 38 Iss: 7, pp.936 - 953 (2014)</journal-ref><doi>10.1108/OIR-07-2014-0169</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The goal of this working paper is to summarize the main empirical evidences
provided by the scientific community as regards the comparison between the two
main citation based academic search engines: Google Scholar and Microsoft
Academic Search, paying special attention to the following issues: coverage,
correlations between journal rankings, and usage of these academic search
engines. Additionally, selfelaborated data is offered, which are intended to
provide current evidence about the popularity of these tools on the Web, by
measuring the number of rich files PDF, PPT and DOC in which these tools are
mentioned, the amount of external links that both products receive, and the
search queries frequency from Google Trends. The poor results obtained by MAS
led us to an unexpected and unnoticed discovery: Microsoft Academic Search is
outdated since 2013. Therefore, the second part of the working paper aims at
advancing some data demonstrating this lack of update. For this purpose we
gathered the number of total records indexed by Microsoft Academic Search since
2000. The data shows an abrupt drop in the number of documents indexed from
2,346,228 in 2010 to 8,147 in 2013 and 802 in 2014. This decrease is offered
according to 15 thematic areas as well. In view of these problems it seems
logical not only that Microsoft Academic Searchwas poorly used to search for
articles by academics and students, who mostly use Google or Google Scholar,
but virtually ignored by bibliometricians
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7048</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7048</id><created>2014-04-25</created><updated>2015-02-05</updated><authors><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Mavroeidis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Calabrese</keyname><forenames>Francesco</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Multiscale Event Detection in Social Media</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><journal-ref>Data Mining and Knowledge Discovery, vol. 29, no. 5, pp.
  1374-1405, September 2015</journal-ref><doi>10.1007/s10618-015-0421-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event detection has been one of the most important research topics in social
media analysis. Most of the traditional approaches detect events based on fixed
temporal and spatial resolutions, while in reality events of different scales
usually occur simultaneously, namely, they span different intervals in time and
space. In this paper, we propose a novel approach towards multiscale event
detection using social media data, which takes into account different temporal
and spatial scales of events in the data. Specifically, we explore the
properties of the wavelet transform, which is a well-developed multiscale
transform in signal processing, to enable automatic handling of the interaction
between temporal and spatial scales. We then propose a novel algorithm to
compute a data similarity graph at appropriate scales and detect events of
different scales simultaneously by a single graph-based clustering process.
Furthermore, we present spatiotemporal statistical analysis of the noisy
information present in the data stream, which allows us to define a novel
term-filtering procedure for the proposed event detection algorithm and helps
us study its behavior using simulated noisy data. Experimental results on both
synthetically generated data and real world data collected from Twitter
demonstrate the meaningfulness and effectiveness of the proposed approach. Our
framework further extends to numerous application domains that involve
multiscale and multiresolution data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7053</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7053</id><created>2014-04-24</created><authors><author><keyname>Yakhontov</keyname><forenames>Sergey V.</forenames></author></authors><title>Computable real function F such that F is not polynomial time computable
  on [0,1]</title><categories>cs.CC</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computable real function F on [0,1] is constructed such that there exists
an exponential time algorithm for the evaluation of the function on [0,1] on
Turing machine but there does not exist any polynomial time algorithm for the
evaluation of the function on [0,1] on Turing machine (moreover, it holds for
any rational point on (0,1))
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7055</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7055</id><created>2014-04-28</created><updated>2014-06-30</updated><authors><author><keyname>Dasarathy</keyname><forenames>Gautam</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Data Requirement for Phylogenetic Inference from Multiple Loci: A New
  Distance Method</title><categories>q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH</categories><comments>19 pages, 2 figures. Preliminary version to appear in IEEE ISIT 2014.
  Added acknowledgements and made the proof of the &quot;equality&quot; part of Theorem 3
  explicit in Appendix C</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the evolutionary history of a set of
species (phylogeny or species tree) from several genes. It is known that the
evolutionary history of individual genes (gene trees) might be topologically
distinct from each other and from the underlying species tree, possibly
confounding phylogenetic analysis. A further complication in practice is that
one has to estimate gene trees from molecular sequences of finite length. We
provide the first full data-requirement analysis of a species tree
reconstruction method that takes into account estimation errors at the gene
level. Under that criterion, we also devise a novel reconstruction algorithm
that provably improves over all previous methods in a regime of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7059</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7059</id><created>2014-04-28</created><updated>2014-04-29</updated><authors><author><keyname>Menaker</keyname><forenames>Dana</forenames></author><author><keyname>Avidan</keyname><forenames>Shai</forenames></author></authors><title>Stereo on a budget</title><categories>cs.CV</categories><comments>update flowchart in Fig. 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm for recovering depth using less than two images.
Instead of having both cameras send their entire image to the host computer,
the left camera sends its image to the host while the right camera sends only a
fraction $\epsilon$ of its image. The key aspect is that the cameras send the
information without communicating at all. Hence, the required communication
bandwidth is significantly reduced.
  While standard image compression techniques can reduce the communication
bandwidth, this requires additional computational resources on the part of the
encoder (camera). We aim at designing a light weight encoder that only touches
a fraction of the pixels. The burden of decoding is placed on the decoder
(host).
  We show that it is enough for the encoder to transmit a sparse set of pixels.
Using only $1+\epsilon$ images, with $\epsilon$ as little as 2% of the image,
the decoder can compute a depth map. The depth map's accuracy is comparable to
traditional stereo matching algorithms that require both images as input. Using
the depth map and the left image, the right image can be synthesized. No
computations are required at the encoder, and the decoder's runtime is linear
in the images' size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7060</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7060</id><created>2014-04-28</created><authors><author><keyname>Kusumoto</keyname><forenames>Mitsuru</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Testing Forest-Isomorphism in the Adjacency List Model</title><categories>cs.DS</categories><comments>ICALP 2014 to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of testing if two input forests are isomorphic or are
far from being so. An algorithm is called an $\varepsilon$-tester for
forest-isomorphism if given an oracle access to two forests $G$ and $H$ in the
adjacency list model, with high probability, accepts if $G$ and $H$ are
isomorphic and rejects if we must modify at least $\varepsilon n$ edges to make
$G$ isomorphic to $H$. We show an $\varepsilon$-tester for forest-isomorphism
with a query complexity $\mathrm{polylog}(n)$ and a lower bound of
$\Omega(\sqrt{\log{n}})$. Further, with the aid of the tester, we show that
every graph property is testable in the adjacency list model with
$\mathrm{polylog}(n)$ queries if the input graph is a forest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7061</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7061</id><created>2014-04-28</created><authors><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>Channel Selection for Network-assisted D2D Communication via No-Regret
  Bandit Learning with Calibrated Forecasting</title><categories>cs.GT</categories><comments>31 pages (one column), 9 figures</comments><doi>10.1109/TWC.2014.2365803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the distributed channel selection problem in the context of
device-to-device (D2D) communication as an underlay to a cellular network.
Underlaid D2D users communicate directly by utilizing the cellular spectrum but
their decisions are not governed by any centralized controller. Selfish D2D
users that compete for access to the resources construct a distributed system,
where the transmission performance depends on channel availability and quality.
This information, however, is difficult to acquire. Moreover, the adverse
effects of D2D users on cellular transmissions should be minimized. In order to
overcome these limitations, we propose a network-assisted distributed channel
selection approach in which D2D users are only allowed to use vacant cellular
channels. This scenario is modeled as a multi-player multi-armed bandit game
with side information, for which a distributed algorithmic solution is
proposed. The solution is a combination of no-regret learning and calibrated
forecasting, and can be applied to a broad class of multi-player stochastic
learning problems, in addition to the formulated channel selection problem.
Analytically, it is established that this approach not only yields vanishing
regret (in comparison to the global optimal solution), but also guarantees that
the empirical joint frequencies of the game converge to the set of correlated
equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7067</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7067</id><created>2014-04-28</created><authors><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS</affiliation></author><author><keyname>Fronc</keyname><forenames>Lukasz</forenames><affiliation>LAAS</affiliation></author><author><keyname>Berthomieu</keyname><forenames>Bernard</forenames><affiliation>LAAS</affiliation></author><author><keyname>Vernadat</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LAAS</affiliation></author></authors><title>Time Petri Nets with Dynamic Firing Dates: Semantics and Applications</title><categories>cs.LO</categories><proxy>ccsd</proxy><doi>10.1007/978-3-319-10512-3_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an extension of time Petri nets such that the time at which a
transition can fire, also called its firing date, may be dynamically updated.
Our extension provides two mechanisms for updating the timing constraints of a
net. First, we propose to change the static time interval of a transition each
time it is newly enabled; in this case the new time interval is given as a
function of the current marking. Next, we allow to update the firing date of a
transition when it is persistent, that is when a concurrent transition fires.
We show how to carry the widely used state class abstraction to this new kind
of time Petri nets and define a class of nets for which the abstraction is
exact. We show the usefulness of our approach with two applications: first for
scheduling preemptive task, as a poor man's substitute for stopwatch, then to
model hybrid systems with non trivial continuous behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7073</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7073</id><created>2014-04-28</created><updated>2014-04-30</updated><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Probably Approximately Correct MDP Learning and Control With Temporal
  Logic Constraints</title><categories>cs.SY cs.LG cs.LO cs.RO</categories><comments>9 pages, 5 figures, Accepted by 2014 Robotics: Science and Systems
  (RSS)</comments><msc-class>93E35</msc-class><acm-class>I.2.8; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider synthesis of control policies that maximize the probability of
satisfying given temporal logic specifications in unknown, stochastic
environments. We model the interaction between the system and its environment
as a Markov decision process (MDP) with initially unknown transition
probabilities. The solution we develop builds on the so-called model-based
probably approximately correct Markov decision process (PAC-MDP) methodology.
The algorithm attains an $\varepsilon$-approximately optimal policy with
probability $1-\delta$ using samples (i.e. observations), time and space that
grow polynomially with the size of the MDP, the size of the automaton
expressing the temporal logic specification, $\frac{1}{\varepsilon}$,
$\frac{1}{\delta}$ and a finite time horizon. In this approach, the system
maintains a model of the initially unknown MDP, and constructs a product MDP
based on its learned model and the specification automaton that expresses the
temporal logic constraints. During execution, the policy is iteratively updated
using observation of the transitions taken by the system. The iteration
terminates in finitely many steps. With high probability, the resulting policy
is such that, for any state, the difference between the probability of
satisfying the specification under this policy and the optimal one is within a
predefined bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7076</identifier>
 <datestamp>2014-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7076</id><created>2014-04-28</created><updated>2014-08-13</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Jordan</keyname><forenames>Jillian J.</forenames></author><author><keyname>Rand</keyname><forenames>David G.</forenames></author></authors><title>Heuristics guide the implementation of social preferences in one-shot
  Prisoner's Dilemma experiments</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation in one-shot anonymous interactions is a widely documented aspect
of human behaviour. Here we shed light on the motivations behind this behaviour
by experimentally exploring cooperation in a one-shot continuous-strategy
Prisoner's Dilemma (i.e. one-shot two-player Public Goods Game). We examine the
distribution of cooperation amounts, and how that distribution varies based on
the benefit-to-cost ratio of cooperation (b/c). Interestingly, we find a
trimodal distribution at all b/c values investigated. Increasing b/c decreases
the fraction of participants engaging in zero cooperation and increases the
fraction engaging in maximal cooperation, suggesting a role for efficiency
concerns. However, a substantial fraction of participants consistently engage
in 50% cooperation regardless of b/c. The presence of these persistent 50%
cooperators is surprising, and not easily explained by standard models of
social preferences. We present evidence that this behaviour is a result of
social preferences guided by simple decision heuristics, rather than the
rational examination of payoffs assumed by most social preference models. We
also find a strong correlation between play in the Prisoner's Dilemma and in a
subsequent Dictator Game, confirming previous findings suggesting a common
prosocial motivation underlying altruism and cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7078</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7078</id><created>2014-04-28</created><updated>2014-05-02</updated><authors><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Lindley</keyname><forenames>Sam</forenames></author><author><keyname>Wadler</keyname><forenames>Philip</forenames></author></authors><title>Query shredding: Efficient relational evaluation of queries over nested
  multisets (extended version)</title><categories>cs.DB cs.PL</categories><comments>Extended version of SIGMOD 2014 conference paper</comments><acm-class>H.2.4</acm-class><doi>10.1145/2588555.2612186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested relational query languages have been explored extensively, and
underlie industrial language-integrated query systems such as Microsoft's LINQ.
However, relational databases do not natively support nested collections in
query results. This can lead to major performance problems: if programmers
write queries that yield nested results, then such systems typically either
fail or generate a large number of queries. We present a new approach to query
shredding, which converts a query returning nested data to a fixed number of
SQL queries. Our approach, in contrast to prior work, handles multiset
semantics, and generates an idiomatic SQL:1999 query directly from a normal
form for nested queries. We provide a detailed description of our translation
and present experiments showing that it offers comparable or better performance
than a recent alternative approach on a range of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7092</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7092</id><created>2014-04-28</created><authors><author><keyname>Derevenetc</keyname><forenames>Egor</forenames></author><author><keyname>Meyer</keyname><forenames>Roland</forenames></author></authors><title>Robustness against Power is PSPACE-complete</title><categories>cs.LO</categories><msc-class>68Q60</msc-class><acm-class>D.2.4; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power is a RISC architecture developed by IBM, Freescale, and several other
companies and implemented in a series of POWER processors. The architecture
features a relaxed memory model providing very weak guarantees with respect to
the ordering and atomicity of memory accesses.
  Due to these weaknesses, some programs that are correct under sequential
consistency (SC) show undesirable effects when run under Power. We call these
programs not robust against the Power memory model. Formally, a program is
robust if every computation under Power has the same data and control
dependencies as some SC computation.
  Our contribution is a decision procedure for robustness of concurrent
programs against the Power memory model. It is based on three ideas. First, we
reformulate robustness in terms of the acyclicity of a happens-before relation.
Second, we prove that among the computations with cyclic happens-before
relation there is one in a certain normal form. Finally, we reduce the
existence of such a normal-form computation to a language emptiness problem.
Altogether, this yields a PSPACE algorithm for checking robustness against
Power. We complement it by a matching lower bound to show PSPACE-completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7099</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7099</id><created>2014-04-28</created><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author></authors><title>Editorial for the Bibliometric-enhanced Information Retrieval Workshop
  at ECIR 2014</title><categories>cs.IR cs.DL</categories><comments>4 pages, Bibliometric-enhanced Information Retrieval Workshop at ECIR
  2014, Amsterdam, NL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This first &quot;Bibliometric-enhanced Information Retrieval&quot; (BIR 2014) workshop
aims to engage with the IR community about possible links to bibliometrics and
scholarly communication. Bibliometric techniques are not yet widely used to
enhance retrieval processes in digital libraries, although they offer
value-added effects for users. In this workshop we will explore how statistical
modelling of scholarship, such as Bradfordizing or network analysis of
co-authorship network, can improve retrieval services for specific communities,
as well as for large, cross-domain collections. This workshop aims to raise
awareness of the missing link between information retrieval (IR) and
bibliometrics / scientometrics and to create a common ground for the
incorporation of bibliometric-enhanced services into retrieval at the digital
library interface. Our interests include information retrieval, information
seeking, science modelling, network analysis, and digital libraries. The goal
is to apply insights from bibliometrics, scientometrics, and informetrics to
concrete practical problems of information retrieval and browsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7105</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7105</id><created>2014-04-28</created><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Information Recovery from Pairwise Measurements</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>accepted to IEEE International Symposium on Information Theory (ISIT)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of information processing tasks in practice involve recovering $n$
objects from single-shot graph-based measurements, particularly those taken
over the edges of some measurement graph $\mathcal{G}$. This paper concerns the
situation where each object takes value over a group of $M$ different values,
and where one is interested to recover all these values based on observations
of certain pairwise relations over $\mathcal{G}$. The imperfection of
measurements presents two major challenges for information recovery: 1)
$\textit{inaccuracy}$: a (dominant) portion $1-p$ of measurements are
corrupted; 2) $\textit{incompleteness}$: a significant fraction of pairs are
unobservable, i.e. $\mathcal{G}$ can be highly sparse.
  Under a natural random outlier model, we characterize the $\textit{minimax
recovery rate}$, that is, the critical threshold of non-corruption rate $p$
below which exact information recovery is infeasible. This accommodates a very
general class of pairwise relations. For various homogeneous random graph
models (e.g. Erdos Renyi random graphs, random geometric graphs, small world
graphs), the minimax recovery rate depends almost exclusively on the edge
sparsity of the measurement graph $\mathcal{G}$ irrespective of other graphical
metrics. This fundamental limit decays with the group size $M$ at a square root
rate before entering a connectivity-limited regime. Under the Erdos Renyi
random graph, a tractable combinatorial algorithm is proposed to approach the
limit for large $M$ ($M=n^{\Omega(1)}$), while order-optimal recovery is
enabled by semidefinite programs in the small $M$ regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7109</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7109</id><created>2014-04-28</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Security Thresholds of Multicarrier Continuous-Variable Quantum Key
  Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>58 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the secret key rate formulas and derive security threshold
parameters of multicarrier continuous-variable quantum key distribution
(CVQKD). In a multicarrier CVQKD scenario, the Gaussian input quantum states of
the legal parties are granulated into Gaussian subcarrier CVs
(continuous-variables). The multicarrier communication formulates Gaussian
sub-channels from the physical quantum channel, each dedicated to the
transmission of a subcarrier CV. The Gaussian subcarriers are decoded by a
unitary CV operation, which results in the recovered single-carrier Gaussian
CVs. We derive the formulas through the AMQD (adaptive multicarrier quadrature
division) scheme, the SVD-assisted (singular value decomposition) AMQD, and the
multiuser AMQD-MQA (multiuser quadrature allocation). We prove that the
multicarrier CVQKD leads to improved secret key rates and higher tolerable
excess noise in comparison to single-carrier CVQKD. We derive the private
classical capacity of a Gaussian sub-channel and the security parameters of an
optimal Gaussian collective attack in the multicarrier setting. We reveal the
secret key rate formulas for one-way and two-way multicarrier CVQKD protocols,
assuming homodyne and heterodyne measurements and direct and reverse
reconciliation. The results reveal the physical boundaries of physically
allowed Gaussian attacks in a multicarrier CVQKD scenario and confirm that the
improved transmission rates lead to enhanced secret key rates and security
thresholds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7120</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7120</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Menu and Toolbar Coordination</title><categories>cs.HC</categories><comments>7 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1404.6747</comments><journal-ref>Umakant Mishra, Inventions on Menu and Toolbar Coordination,
  (December 6, 2006 ) Available at SSRN: http://ssrn.com/abstract=949239</journal-ref><doi>10.2139/ssrn.949239</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both toolbar and dropdown menu are used popularly in a graphical user
interface with a similar objective of providing easy access to the internal
functions. Often the same functions are provided through both menu and toolbar.
  Both toolbar and dropdown menu have their own advantages and disadvantages. A
menu can provide more options occupying less real estate, while toolbar can
provide a single click access without navigating through trees and branches.
  As a menu and toolbar system shares many common objectives, it is often
useful maintain some relationship to coordinate between both the elements of a
GUI system. The relationships can be easy as both of them often share the same
internal function. For example, the print option in a menu will (most likely)
call the same function as the print button on the toolbar.
  This article discusses the similarities and differences between a dropdown
menu and toolbar. Five inventions trying to focus on both menu and toolbar are
illustrated in the article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7121</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7121</id><created>2014-04-27</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Inventions on Drag and Drop in GUI</title><categories>cs.HC</categories><comments>7 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1404.6752</comments><journal-ref>Umakant Mishra, Inventions on Drag and Drop in GUI (September 7,
  2007). Available at SSRN:http://ssrn.com/abstract=1264691 or
  http://dx.doi.org/10.2139/ssrn.1264691</journal-ref><doi>10.2139/ssrn.1264691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drag and drop operation is one of the key capabilities of any Graphical User
Interface. The user can do quite complex operations simply by visually dragging
and dropping objects from one location to another. It saves user from
remembering and typing a lot of commands.
  The result of a drag and drop operation may vary depending the type of source
object and type of destination object. For example dragging a file and dropping
on a folder may copy or move the file to the destination folder, dropping that
file to a remote FTP location may upload that file using internet, dropping
that file on a printer icon may print that file, dropping that file on the
trash can may delete that file, and dropping that file on an executable may
play or open or compute or manipulate that file.
  Although a drag and drop operation prima facie seems to be a simple
operation, it can become extremely complicated depending on the type of source
objects dragged and the type of destination objects selected for dropping.
There are many limitations of a conventional drag and drop operation. This
article points out the difficulties of a drag and drop operation and
illustrates the solutions disclosed by various inventions to overcome those
difficulties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7151</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7151</id><created>2014-04-28</created><authors><author><keyname>Emran</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Elsabrouty</keyname><forenames>Maha</forenames></author></authors><title>Simplified Variable-Scaled Min Sum LDPC decoder for irregular LDPC Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures</comments><doi>10.1109/CCNC.2014.6940497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Min-Sum decoding is widely used for decoding LDPC codes in many modern
digital video broadcasting decoding due to its relative low complexity and
robustness against quantization error. However, the suboptimal performance of
the Min-Sum affects the integrated performance of wireless receivers. In this
paper, we present the idea of adapting the scaling factor of the Min-Sum
decoder with iterations through a simple approximation. For the ease of
implementation the scaling factor can be changed in a staircase fashion. The
stair step is designed to optimize the decoder performance and the required
storage for its different values. The variable scaling factor proposed
algorithm produces a non-trivial improvement of the performance of the Min-Sum
decoding as verified by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7152</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7152</id><created>2014-04-28</created><updated>2015-03-03</updated><authors><author><keyname>Compton</keyname><forenames>Ryan</forenames></author><author><keyname>Jurgens</keyname><forenames>David</forenames></author><author><keyname>Allen</keyname><forenames>David</forenames></author></authors><title>Geotagging One Hundred Million Twitter Accounts with Total Variation
  Minimization</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 8 figures, accepted to IEEE BigData 2014, Compton, Ryan,
  David Jurgens, and David Allen. &quot;Geotagging one hundred million twitter
  accounts with total variation minimization.&quot; Big Data (Big Data), 2014 IEEE
  International Conference on. IEEE, 2014</comments><msc-class>68T99</msc-class><acm-class>G.1.6; H.2.8; H.3.4</acm-class><doi>10.1109/BigData.2014.7004256</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographically annotated social media is extremely valuable for modern
information retrieval. However, when researchers can only access
publicly-visible data, one quickly finds that social media users rarely publish
location information. In this work, we provide a method which can geolocate the
overwhelming majority of active Twitter users, independent of their location
sharing preferences, using only publicly-visible Twitter data.
  Our method infers an unknown user's location by examining their friend's
locations. We frame the geotagging problem as an optimization over a social
network with a total variation-based objective and provide a scalable and
distributed algorithm for its solution. Furthermore, we show how a robust
estimate of the geographic dispersion of each user's ego network can be used as
a per-user accuracy measure which is effective at removing outlying errors.
  Leave-many-out evaluation shows that our method is able to infer location for
101,846,236 Twitter users at a median error of 6.38 km, allowing us to geotag
over 80\% of public tweets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7164</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7164</id><created>2014-04-28</created><updated>2014-04-29</updated><authors><author><keyname>Satpathy</keyname><forenames>Sanket</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Secure Coordination with a Two-Sided Helper</title><categories>cs.IT math.IT</categories><comments>ISIT 2014, 5 pages, uses IEEEtran.cls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of secure source coding with a two-sided helper in
a game-theoretic framework. Alice (A) and Helen (H) view iid correlated
information sequences $X^n$ and $Y^n$ respectively. Alice communicates to Bob
(B) at rate $R$, while H broadcasts a message to both A and B at rate $R_H$.
Additionally, A and B share secret key $K$ at rate $R_0$ that is independent of
$(X^n,Y^n)$. An active adversary, Eve (E) sees all communication links while
having access to a (possibly degraded) version of the past information. We
characterize the rate-payoff region for this problem. We also solve the problem
when the link from A to B is private. Our work recovers previous results of
Schieler-Cuff and Kittichokechai et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7169</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7169</id><created>2014-04-28</created><updated>2014-06-04</updated><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund</forenames></author></authors><title>Revisiting the Complexity of Stability of Continuous and Hybrid Systems</title><categories>cs.SY cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework to give upper bounds on the &quot;practical&quot; computational
complexity of stability problems for a wide range of nonlinear continuous and
hybrid systems. To do so, we describe stability properties of dynamical systems
using first-order formulas over the real numbers, and reduce stability problems
to the delta-decision problems of these formulas. The framework allows us to
obtain a precise characterization of the complexity of different notions of
stability for nonlinear continuous and hybrid systems. We prove that bounded
versions of the stability problems are generally decidable, and give upper
bounds on their complexity. The unbounded versions are generally undecidable,
for which we give upper bounds on their degrees of unsolvability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7170</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7170</id><created>2014-04-28</created><updated>2014-08-07</updated><authors><author><keyname>Petri</keyname><forenames>Giovanni</forenames></author><author><keyname>Expert</keyname><forenames>Paul</forenames></author></authors><title>Temporal stability of network partitions</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 12 figures</comments><journal-ref>Phys. Rev. E 90, 022813, 2014</journal-ref><doi>10.1103/PhysRevE.90.022813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to find the best temporal partition at any time-scale and
rank the relevance of partitions found at different time-scales. This method is
based on random walkers coevolving with the network and as such constitutes a
generalization of partition stability to the case of temporal networks. We show
that, when applied to a toy model and real datasets, temporal stability
uncovers structures that are persistent over meaningful time-scales as well as
important isolated events, making it an effective tool to study both abrupt
changes and gradual evolution of a network mesoscopic structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7171</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7171</id><created>2014-04-28</created><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund</forenames></author></authors><title>Delta-Complete Analysis for Bounded Reachability of Hybrid Systems</title><categories>cs.SY cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the framework of delta-complete analysis for bounded reachability
problems of general hybrid systems. We perform bounded reachability checking
through solving delta-decision problems over the reals. The techniques take
into account of robustness properties of the systems under numerical
perturbations. We prove that the verification problems become much more
mathematically tractable in this new framework. Our implementation of the
techniques, an open-source tool dReach, scales well on several highly nonlinear
hybrid system models that arise in biomedical and robotics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7173</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7173</id><created>2014-04-28</created><authors><author><keyname>Schwartz</keyname><forenames>Daniel G.</forenames></author></authors><title>Nonmonotonic Reasoning as a Temporal Activity</title><categories>cs.AI</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014), Vienna, Austria, 17-19 July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\it dynamic reasoning system} (DRS) is an adaptation of a conventional
formal logical system that explicitly portrays reasoning as a temporal
activity, with each extralogical input to the system and each inference rule
application being viewed as occurring at a distinct time step. Every DRS
incorporates some well-defined logic together with a controller that serves to
guide the reasoning process in response to user inputs. Logics are generic,
whereas controllers are application-specific. Every controller does,
nonetheless, provide an algorithm for nonmonotonic belief revision. The general
notion of a DRS comprises a framework within which one can formulate the logic
and algorithms for a given application and prove that the algorithms are
correct, i.e., that they serve to (i) derive all salient information and (ii)
preserve the consistency of the belief set. This paper illustrates the idea
with ordinary first-order predicate calculus, suitably modified for the present
purpose, and an example. The example revisits some classic nonmonotonic
reasoning puzzles (Opus the Penguin, Nixon Diamond) and shows how these can be
resolved in the context of a DRS, using an expanded version of first-order
logic that incorporates typed predicate symbols. All concepts are rigorously
defined and effectively computable, thereby providing the foundation for a
future software implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7174</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7174</id><created>2014-04-28</created><updated>2014-11-06</updated><authors><author><keyname>Eppel</keyname><forenames>Sagi</forenames></author><author><keyname>Kachman</keyname><forenames>Tal</forenames></author></authors><title>Computer vision-based recognition of liquid surfaces and phase
  boundaries in transparent vessels, with emphasis on chemistry applications</title><categories>cs.CV</categories><comments>Source code for phase boundary and liquid surface recognition
  available at:
  http://www.mathworks.com/matlabcentral/fileexchange/46893-computer-vision-based-recognition-of-liquid-surface-and-liquid-level-of-liquid-of-transparent-vessel</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The ability to recognize the liquid surface and the liquid level in
transparent containers is perhaps the most commonly used evaluation method when
dealing with fluids. Such recognition is essential in determining the liquid
volume, fill level, phase boundaries and phase separation in various fluid
systems. The recognition of liquid surfaces is particularly important in
solution chemistry, where it is essential to many laboratory techniques (e.g.,
extraction, distillation, titration). A general method for the recognition of
interfaces between liquid and air or between phase-separating liquids could
have a wide range of applications and contribute to the understanding of the
visual properties of such interfaces. This work examines a computer vision
method for the recognition of liquid surfaces and liquid levels in various
transparent containers. The method can be applied to recognition of both
liquid-air and liquid-liquid surfaces. No prior knowledge of the number of
phases is required. The method receives the image of the liquid container and
the boundaries of the container in the image and scans all possible curves that
could correspond to the outlines of liquid surfaces in the image. The method
then compares each curve to the image to rate its correspondence with the
outline of the real liquid surface by examining various image properties in the
area surrounding each point of the curve. The image properties that were found
to give the best indication of the liquid surface are the relative intensity
change, the edge density change and the gradient direction relative to the
curve normal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7186</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7186</id><created>2014-04-28</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Bae</keyname><forenames>Sang Won</forenames></author><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Taslakian</keyname><forenames>Perouz</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Theta-3 is connected</title><categories>cs.CG</categories><comments>11 pages, to appear in CGTA</comments><journal-ref>Computational Geometry: Theory and Applications, 47(9):910-917,
  2014. Special issue for CCCG 2013</journal-ref><doi>10.1016/j.comgeo.2014.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the $\theta$-graph with three cones is connected.
We also provide an alternative proof of the connectivity of the Yao graph with
three cones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7189</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7189</id><created>2014-04-28</created><authors><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author><author><keyname>Wormald</keyname><forenames>Nick</forenames></author></authors><title>It's a Small World for Random Surfers</title><categories>cs.DM math.PR</categories><comments>36 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove logarithmic upper bounds for the diameters of the random-surfer
Webgraph model and the PageRank-based selection Webgraph model, confirming the
small world phenomenon holds for them. In the special case when the generated
graph is a tree, we provide close lower and upper bounds for the diameters of
both models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7195</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7195</id><created>2014-04-28</created><authors><author><keyname>Mathieu</keyname><forenames>Michael</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Fast Approximation of Rotations and Hessians matrices</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method to represent and approximate rotation matrices is introduced.
The method represents approximations of a rotation matrix $Q$ with linearithmic
complexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates,
arranged in an FFT-like fashion. The approximation is &quot;learned&quot; using gradient
descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is
a diagonal matrix. It can be used to approximate covariance matrix of Gaussian
models in order to speed up inference, or to estimate and track the inverse
Hessian of an objective function by relating changes in parameters to changes
in gradient along the trajectory followed by the optimization procedure.
Experiments were conducted to approximate synthetic matrices, covariance
matrices of real data, and Hessian matrices of objective functions involved in
machine learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7203</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7203</id><created>2014-04-28</created><authors><author><keyname>Pilanci</keyname><forenames>Mert</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Randomized Sketches of Convex Programs with Sharp Guarantees</title><categories>cs.IT cs.DS math.IT math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random projection (RP) is a classical technique for reducing storage and
computational costs. We analyze RP-based approximations of convex programs, in
which the original optimization problem is approximated by the solution of a
lower-dimensional problem. Such dimensionality reduction is essential in
computation-limited settings, since the complexity of general convex
programming can be quite high (e.g., cubic for quadratic programs, and
substantially higher for semidefinite programs). In addition to computational
savings, random projection is also useful for reducing memory usage, and has
useful properties for privacy-sensitive optimization. We prove that the
approximation ratio of this procedure can be bounded in terms of the geometry
of constraint set. For a broad class of random projections, including those
based on various sub-Gaussian distributions as well as randomized Hadamard and
Fourier transforms, the data matrix defining the cost function can be projected
down to the statistical dimension of the tangent cone of the constraints at the
original solution, which is often substantially smaller than the original
dimension. We illustrate consequences of our theory for various cases,
including unconstrained and $\ell_1$-constrained least squares, support vector
machines, low-rank matrix estimation, and discuss implications on
privacy-sensitive optimization and some connections with de-noising and
compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7205</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7205</id><created>2014-04-28</created><authors><author><keyname>Moura</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Dam&#xe1;sio</keyname><forenames>Carlos</forenames></author></authors><title>Generalizing Modular Logic Programs</title><categories>cs.AI cs.LO</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though modularity has been studied extensively in conventional logic
programming, there are few approaches on how to incorporate modularity into
Answer Set Programming, a prominent rule-based declarative programming
paradigm. A major approach is Oikarinnen and Janhunen's Gaifman-Shapiro-style
architecture of program modules, which provides the composition of program
modules. Their module theorem properly strengthens Lifschitz and Turner's
splitting set theorem for normal logic programs. However, this approach is
limited by module conditions that are imposed in order to ensure the
compatibility of their module system with the stable model semantics, namely
forcing output signatures of composing modules to be disjoint and disallowing
positive cyclic dependencies between different modules. These conditions turn
out to be too restrictive in practice and in this paper we discuss alternative
ways of lift both restrictions independently, effectively solving the first,
widening the applicability of this framework and the scope of the module
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7206</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7206</id><created>2014-04-28</created><updated>2014-10-27</updated><authors><author><keyname>Wang</keyname><forenames>Qinsi</forenames></author><author><keyname>Zuliani</keyname><forenames>Paolo</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund M.</forenames></author></authors><title>SReach: A Bounded Model Checker for Stochastic Hybrid Systems</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a new tool, SReach, which solves probabilistic
bounded reachability problems for two classes of stochastic hybrid systems. The
first one is (nonlinear) hybrid automata with parametric uncertainty. The
second one is probabilistic hybrid automata with additional randomness for both
transition probabilities and variable resets. Standard approaches to
reachability problems for linear hybrid systems require numerical solutions for
large optimization problems, and become infeasible for systems involving both
nonlinear dynamics over the reals and stochasticity. Our approach encodes
stochastic information by using random variables, and combines the randomized
sampling, a $\delta$-complete decision procedure, and statistical tests. SReach
utilizes the $\delta$-complete decision procedure to solve reachability
problems in a sound manner, i.e., it always decides correctly if, for a given
assignment to all random variables, the system actually reaches the unsafe
region. The statistical tests adapted guarantee arbitrary small error bounds
between probabilities estimated by SReach and real ones. Compared to standard
simulation-based methods, our approach supports non-deterministic branching,
increases the coverage of simulation, and avoids the zero-crossing problem. We
demonstrate our method's feasibility by applying SReach to three representative
biological models and to additional benchmarks for nonlinear hybrid systems
with multiple probabilistic system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7211</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7211</id><created>2014-04-28</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author><author><keyname>Jiang</keyname><forenames>Feng</forenames></author></authors><title>Spatially Directional Predictive Coding for Block-based Compressive
  Sensing of Natural Images</title><categories>cs.CV</categories><comments>5 pages, 3 tables, 3 figures, published at IEEE International
  Conference on Image Processing (ICIP) 2013 Code Avaiable:
  http://idm.pku.edu.cn/staff/zhangjian/SDPC/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel coding strategy for block-based compressive sens-ing named spatially
directional predictive coding (SDPC) is proposed, which efficiently utilizes
the intrinsic spatial cor-relation of natural images. At the encoder, for each
block of compressive sensing (CS) measurements, the optimal pre-diction is
selected from a set of prediction candidates that are generated by four
designed directional predictive modes. Then, the resulting residual is
processed by scalar quantiza-tion (SQ). At the decoder, the same prediction is
added onto the de-quantized residuals to produce the quantized CS measurements,
which is exploited for CS reconstruction. Experimental results substantiate
significant improvements achieved by SDPC-plus-SQ in rate distortion
performance as compared with SQ alone and DPCM-plus-SQ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7212</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7212</id><created>2014-04-28</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Structural Group Sparse Representation for Image Compressive Sensing
  Recovery</title><categories>cs.CV</categories><comments>10 pages, 4 figures, 1 table, published at IEEE Data Compression
  Conference (DCC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sensing (CS) theory shows that a signal can be decoded from many
fewer measurements than suggested by the Nyquist sampling theory, when the
signal is sparse in some domain. Most of conventional CS recovery approaches,
however, exploited a set of fixed bases (e.g. DCT, wavelet, contourlet and
gradient domain) for the entirety of a signal, which are irrespective of the
nonstationarity of natural signals and cannot achieve high enough degree of
sparsity, thus resulting in poor rate-distortion performance. In this paper, we
propose a new framework for image compressive sensing recovery via structural
group sparse representation (SGSR) modeling, which enforces image sparsity and
self-similarity simultaneously under a unified framework in an adaptive group
domain, thus greatly confining the CS solution space. In addition, an efficient
iterative shrinkage/thresholding algorithm based technique is developed to
solve the above optimization problem. Experimental results demonstrate that the
novel CS recovery strategy achieves significant performance improvements over
the current state-of-the-art schemes and exhibits nice convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7219</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7219</id><created>2014-04-28</created><updated>2015-04-08</updated><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author></authors><title>Sublinear separators, fragility and subexponential expansion</title><categories>math.CO cs.DM</categories><comments>28 pages, 1 figure</comments><msc-class>05C75 (Primary), 05C85 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a subgraph-closed graph class with bounded maximum degree. We show
that if G has balanced separators whose size is smaller than linear by a
polynomial factor, then G has subexponential expansion. This gives a partial
converse to a result of Ne\v{s}et\v{r}il and Ossona de Mendez. As an
intermediate step, the proof uses a new kind of graph decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7227</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7227</id><created>2014-04-28</created><updated>2014-09-02</updated><authors><author><keyname>Batra</keyname><forenames>Nipun</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author><author><keyname>Singh</keyname><forenames>Pushpendra</forenames></author><author><keyname>Dutta</keyname><forenames>Haimonti</forenames></author><author><keyname>Sarangan</keyname><forenames>Venkatesh</forenames></author><author><keyname>Srivastava</keyname><forenames>Mani</forenames></author></authors><title>Data Driven Energy Efficiency in Buildings</title><categories>cs.OH</categories><comments>PhD. Qualifiers report presented at IIIT Delhi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Buildings across the world contribute significantly to the overall energy
consumption and are thus stakeholders in grid operations. Towards the
development of a smart grid, utilities and governments across the world are
encouraging smart meter deployments. High resolution (often at every 15
minutes) data from these smart meters can be used to understand and optimize
energy consumptions in buildings. In addition to smart meters, buildings are
also increasingly managed with Building Management Systems (BMS) which control
different sub-systems such as lighting and heating, ventilation, and air
conditioning (HVAC). With the advent of these smart meters, increased usage of
BMS and easy availability and widespread installation of ambient sensors, there
is a deluge of building energy data. This data has been leveraged for a variety
of applications such as demand response, appliance fault detection and
optimizing HVAC schedules. Beyond the traditional use of such data sets, they
can be put to effective use towards making buildings smarter and hence driving
every possible bit of energy efficiency. Effective use of this data entails
several critical areas from sensing to decision making and participatory
involvement of occupants. Picking from wide literature in building energy
efficiency, we identify five crust areas (also referred to as 5 Is) for
realizing data driven energy efficiency in buildings : i) instrument optimally;
ii) interconnect sub-systems; iii) inferred decision making; iv) involve
occupants and v) intelligent operations. We classify prior work as per these 5
Is and dis-cuss challenges, opportunities and applications across them.
Building upon these 5 Is we discuss a well studied problem in building energy
efficiency -non-intrusive load monitoring (NILM) and how research in this area
spans across the 5 Is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7237</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7237</id><created>2014-04-29</created><authors><author><keyname>Rao</keyname><forenames>T. Srinivasa</forenames><affiliation>InformationTechnology, UshaRama College of Engineering &amp; Technology, India</affiliation></author><author><keyname>Kurra</keyname><forenames>Rajasekhar R.</forenames><affiliation>Principal, Sri Prakash College of Engineering &amp; Technology, India</affiliation></author></authors><title>A Smart Intelligent Way of Video Authentication Using Classification and
  Decomposition of Watermarking Methods</title><categories>cs.MM</categories><comments>7 Pages, 4 Figures</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V10(3):136-142, Apr 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by
  Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V10P123</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Video Watermarking serves as a new technology mainly used to provide security
to the illegal distribution of digital video over the web. The purpose of any
video watermarking scheme is to embed extra information into video in such a
way that must be perceptually undetectable while still holding enough
information in order to extract the watermark beginning with the resultant
video. Information which is embedded within the original image is a Digital
Watermark, which could be visible or invisible. To improved more security,
embedding and extraction Watermark process should be complex against attackers.
Recent research indicates SVD (Singular Value Decomposition) algorithms are
employed owing to their simple scheme with mathematical function. In this
proposed work an advanced SVD transformation algorithm is used for embedding
and extraction process. Experimental results show proposed watermarking process
is more secured than existing SVD approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7239</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7239</id><created>2014-04-29</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Oshri</keyname><forenames>Gal</forenames></author></authors><title>Contracting Experts With Unknown Cost Structures</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of a principal looking to contract an expert to
provide a probability forecast for a categorical event. We assume all experts
have a common public prior on the event's probability, but can form more
accurate opinions by engaging in research. Various experts' research costs are
unknown to the principal. We present a truthful and efficient mechanism for the
principal's problem of contracting an expert. This results in the principal
contracting the best expert to do the work, and the principal's expected
utility is equivalent to having the second best expert in-house. Our mechanism
connects scoring rules with auctions, a connection that is useful when
obtaining new information requires costly research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7247</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7247</id><created>2014-04-29</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Human Factors of Formal Methods</title><categories>cs.SE cs.HC</categories><comments>Preprint. Final version published in Proceedings of IADIS
  International Conference Interfaces and Human Computer Interaction 2012 (IHCI
  2012), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a brief introduction to the work that aims to apply the
achievements within the area of engineering psychology to the area of formal
methods, focusing on the specification phase of a system development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7251</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7251</id><created>2014-04-29</created><updated>2015-01-19</updated><authors><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Stinner</keyname><forenames>Markus</forenames></author><author><keyname>Sidorenko</keyname><forenames>Vladimir</forenames></author></authors><title>Convolutional Codes in Rank Metric with Application to Random Network
  Coding</title><categories>cs.IT math.IT</categories><comments>presented in part at Netcod 2012, submitted to IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random network coding recently attracts attention as a technique to
disseminate information in a network. This paper considers a non-coherent
multi-shot network, where the unknown and time-variant network is used several
times. In order to create dependencies between the different shots, particular
convolutional codes in rank metric are used. These codes are so-called
(partial) unit memory ((P)UM) codes, i.e., convolutional codes with memory one.
First, distance measures for convolutional codes in rank metric are shown and
two constructions of (P)UM codes in rank metric based on the generator matrices
of maximum rank distance codes are presented. Second, an efficient
error-erasure decoding algorithm for these codes is presented. Its guaranteed
decoding radius is derived and its complexity is bounded. Finally, it is shown
how to apply these codes for error correction in random linear and affine
network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7255</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7255</id><created>2014-04-29</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Notton</keyname><forenames>Gilles</forenames><affiliation>SPE</affiliation></author></authors><title>Meteorological time series forecasting based on MLP modelling using
  heterogeneous transfer functions</title><categories>cs.LG</categories><proxy>ccsd</proxy><journal-ref>International Conference on Mathematical Modeling in Physical
  Sciences 2014, Madrid : Spain (2014)</journal-ref><doi>10.1088/1742-6596/574/1/012064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to study four meteorological and seasonal time
series coupled with a multi-layer perceptron (MLP) modeling. We chose to
combine two transfer functions for the nodes of the hidden layer, and to use a
temporal indicator (time index as input) in order to take into account the
seasonal aspect of the studied time series. The results of the prediction
concern two years of measurements and the learning step, eight independent
years. We show that this methodology can improve the accuracy of meteorological
data estimation compared to a classical MLP modelling with a homogenous
transfer function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7259</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7259</id><created>2014-04-29</created><updated>2015-10-09</updated><authors><author><keyname>Gutowski</keyname><forenames>Grzegorz</forenames></author><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Zhu</keyname><forenames>Xuding</forenames></author></authors><title>Lower bounds for on-line graph colorings</title><categories>math.CO cs.DS</categories><doi>10.1007/978-3-319-13075-0_40</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two strategies for Presenter in on-line graph coloring games. The
first one constructs bipartite graphs and forces any on-line coloring algorithm
to use $2\log_2 n - 10$ colors, where $n$ is the number of vertices in the
constructed graph. This is best possible up to an additive constant. The second
strategy constructs graphs that contain neither $C_3$ nor $C_5$ as a subgraph
and forces $\Omega(\frac{n}{\log n}^\frac{1}{3})$ colors. The best known
on-line coloring algorithm for these graphs uses $O(n^{\frac{1}{2}})$ colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7260</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7260</id><created>2014-04-29</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Refinement-Based Specification: Requirements and Architecture</title><categories>cs.SE</categories><comments>In Software Engineering 2011 (SE)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the methodology for the system requirements and
architecture w.r.t. their decomposition and refinement. It also introduces
ideas of refinement layers and of refinement-based verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7265</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7265</id><created>2014-04-29</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Zhu</keyname><forenames>Xiuna</forenames></author><author><keyname>Mou</keyname><forenames>Dongyue</forenames></author></authors><title>Do we really need to write documentation for a system? CASE tool
  add-ons: generator+editor for a precise documentation</title><categories>cs.SE</categories><comments>In Proceedings International Conference on Model-Driven Engineering
  and Software Development (MODELSWARD'13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the common problems of system development projects is that the system
documentation is often outdated and does not describe the latest version of the
system. The situation is even more complicated if we are speaking not about a
natural language description of the system, but about its formal specification.
In this paper we discuss how the problem could be solved by updating the
documentation automatically, by generating a new formal specification from the
model if the model is frequently changed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7278</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7278</id><created>2014-04-29</created><authors><author><keyname>Boja&#x144;czyk</keyname><forenames>Miko&#x142;aj</forenames></author></authors><title>Weak MSO+U with Path Quantifiers over Infinite Trees</title><categories>cs.LO</categories><comments>version of an ICALP 2014 paper with appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that over infinite trees, satisfiability is decidable for
weak monadic second-order logic extended by the unbounding quantifier U and
quantification over infinite paths. The proof is by reduction to emptiness for
a certain automaton model, while emptiness for the automaton model is decided
using profinite trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7279</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7279</id><created>2014-04-29</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author></authors><title>Assessing the players'performance in the game of bridge: A fuzzy logic
  approach</title><categories>cs.AI</categories><comments>6 pages, 2 figures, 2 tables</comments><msc-class>03E72</msc-class><journal-ref>American Journal of Applied Mathematics and Statistics, vol. 2,
  no. 3 (2014), 115-120</journal-ref><doi>10.12691/ajams-2-3-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contract bridge occupies nowadays a position of great prestige being,
together with chess, the only mind games officially recognized by the
International Olympic Committee. In the present paper an innovative method for
assessing the total performance of bridge- players' belonging to groups of
special interest(e.g. different bridge clubs during a tournament, men and
women, new and old players, etc) is introduced, which is based on principles of
fuzzy logic. For this, the cohorts under assessment are represented as fuzzy
subsets of a set of linguistic labels characterizing their performance and the
centroid defuzzification method is used to convert the fuzzy data collected
from the game to a crisp number. This new method of assessment could be used
informally as a complement of the official bridge-scoring methods for
statistical and other obvious reasons. Two real applications related to
simultaneous tournaments with pre-dealt boards, organized by the Hellenic
Bridge Federation, are also presented, illustrating the importance of our
results in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7282</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7282</id><created>2014-04-29</created><updated>2015-01-05</updated><authors><author><keyname>Magron</keyname><forenames>Victor</forenames></author><author><keyname>Allamigeon</keyname><forenames>Xavier</forenames></author><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Werner</keyname><forenames>Benjamin</forenames></author></authors><title>Formal Proofs for Nonlinear Optimization</title><categories>cs.LO math.OC</categories><comments>24 pages, 2 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formally verified global optimization framework. Given a
semialgebraic or transcendental function $f$ and a compact semialgebraic domain
$K$, we use the nonlinear maxplus template approximation algorithm to provide a
certified lower bound of $f$ over $K$. This method allows to bound in a modular
way some of the constituents of $f$ by suprema of quadratic forms with a well
chosen curvature. Thus, we reduce the initial goal to a hierarchy of
semialgebraic optimization problems, solved by sums of squares relaxations. Our
implementation tool interleaves semialgebraic approximations with sums of
squares witnesses to form certificates. It is interfaced with Coq and thus
benefits from the trusted arithmetic available inside the proof assistant. This
feature is used to produce, from the certificates, both valid underestimators
and lower bounds for each approximated constituent. The application range for
such a tool is widespread; for instance Hales' proof of Kepler's conjecture
yields thousands of multivariate transcendental inequalities. We illustrate the
performance of our formal framework on some of these inequalities as well as on
examples from the global optimization literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7287</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7287</id><created>2014-04-29</created><updated>2014-05-05</updated><authors><author><keyname>Qazi</keyname><forenames>Sameer</forenames></author><author><keyname>Moors</keyname><forenames>Tim</forenames></author></authors><title>Disjoint-Path Selection in Internet: What traceroutes tell us?</title><categories>cs.NI</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing policies used in the Internet can be restrictive, limiting
communication between source-destination pairs to one path, when often better
alternatives exist. To avoid route flapping, recovery mechanisms may be
dampened, making adaptation slow. Unstructured overlays have been proposed to
mitigate the issues of path and performance failures in the Internet by routing
through an indirect-path via overlay peer(s). Choosing alternate-paths in
overlay networks is a challenging issue. Ensuring both availability and
performance guarantees on alternate paths requires aggressive monitoring of all
overlay paths using active probing; this limits scalability. An alternate
technique to select an overlay-path is to bias its selection based on physical
disjointness criteria to bypass the failure on the primary-path. Recently,
several techniques have emerged which can optimize the selection of a
disjoint-path without incurring the high costs associated with probing paths.
In this paper, we show that using only commodity approaches, i.e. running
infrequent traceroutes between overlay hosts, a lot of information can be
revealed about the underlying physical path diversity in the overlay network
which can be used to make informed-guesses for alternate-path selection. We
test our approach using datasets between real-world hosts in AMP and RIPE
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7296</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7296</id><created>2014-04-29</created><authors><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author></authors><title>A Deep Architecture for Semantic Parsing</title><categories>cs.CL</categories><comments>In Proceedings of the Semantic Parsing Workshop at ACL 2014
  (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many successful approaches to semantic parsing build on top of the syntactic
analysis of text, and make use of distributional representations or statistical
models to match parses to ontology-specific queries. This paper presents a
novel deep learning architecture which provides a semantic parsing system
through the union of two neural models of language semantics. It allows for the
generation of ontology-specific queries from natural language statements and
questions without the need for parsing, which makes it especially suitable to
grammatically malformed or syntactically atypical text, such as tweets, as well
as permitting the development of semantic parsers for resource-poor languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7298</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7298</id><created>2014-04-29</created><authors><author><keyname>Br&#xe4;uer-Burchardt</keyname><forenames>Christian</forenames></author><author><keyname>K&#xfc;hmstedt</keyname><forenames>Peter</forenames></author><author><keyname>Notni</keyname><forenames>Gunther</forenames></author></authors><title>Code Minimization for Fringe Projection Based 3D Stereo Sensors by
  Calibration Improvement</title><categories>math.MG cs.CV</categories><comments>8 pages, 5 figures, OAGM 2014 paper</comments><report-no>OAGM/2014/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code minimization provides a speed-up of the processing time of fringe
projection based stereo sensors and possibly makes them real-time applicable.
This paper reports a methodology which enables such sensors to completely omit
Gray code or other additional code. Only a sequence of sinusoidal images is
necessary. The code reduction is achieved by involvement of the projection unit
into the measurement, double triangulation, and a precise projector calibration
or significant projector calibration improvement, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7306</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7306</id><created>2014-04-29</created><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Generalized Nonconvex Nonsmooth Low-Rank Minimization</title><categories>cs.CV cs.LG stat.ML</categories><comments>IEEE International Conference on Computer Vision and Pattern
  Recognition, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As surrogate functions of $L_0$-norm, many nonconvex penalty functions have
been proposed to enhance the sparse vector recovery. It is easy to extend these
nonconvex penalty functions on singular values of a matrix to enhance low-rank
matrix recovery. However, different from convex optimization, solving the
nonconvex low-rank minimization problem is much more challenging than the
nonconvex sparse minimization problem. We observe that all the existing
nonconvex penalty functions are concave and monotonically increasing on
$[0,\infty)$. Thus their gradients are decreasing functions. Based on this
property, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to
solve the nonconvex nonsmooth low-rank minimization problem. IRNN iteratively
solves a Weighted Singular Value Thresholding (WSVT) problem. By setting the
weight vector as the gradient of the concave penalty function, the WSVT problem
has a closed form solution. In theory, we prove that IRNN decreases the
objective function value monotonically, and any limit point is a stationary
point. Extensive experiments on both synthetic data and real images demonstrate
that IRNN enhances the low-rank matrix recovery compared with state-of-the-art
convex algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7307</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7307</id><created>2014-04-29</created><authors><author><keyname>Iwata</keyname><forenames>Yoichi</forenames></author><author><keyname>Oka</keyname><forenames>Keigo</forenames></author></authors><title>Fast Dynamic Graph Algorithms for Parameterized Problems</title><categories>cs.DS</categories><comments>SWAT 2014 to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fully dynamic graph is a data structure that (1) supports edge insertions and
deletions and (2) answers problem specific queries. The time complexity of (1)
and (2) are referred to as the update time and the query time respectively.
There are many researches on dynamic graphs whose update time and query time
are $o(|G|)$, that is, sublinear in the graph size. However, almost all such
researches are for problems in P. In this paper, we investigate dynamic graphs
for NP-hard problems exploiting the notion of fixed parameter tractability
(FPT).
  We give dynamic graphs for Vertex Cover and Cluster Vertex Deletion
parameterized by the solution size $k$. These dynamic graphs achieve almost the
best possible update time $O(\mathrm{poly}(k)\log n)$ and the query time
$O(f(\mathrm{poly}(k),k))$, where $f(n,k)$ is the time complexity of any static
graph algorithm for the problems. We obtain these results by dynamically
maintaining an approximate solution which can be used to construct a small
problem kernel. Exploiting the dynamic graph for Cluster Vertex Deletion, as a
corollary, we obtain a quasilinear-time (polynomial) kernelization algorithm
for Cluster Vertex Deletion. Until now, only quadratic time kernelization
algorithms are known for this problem.
  We also give a dynamic graph for Chromatic Number parameterized by the
solution size of Cluster Vertex Deletion, and a dynamic graph for
bounded-degree Feedback Vertex Set parameterized by the solution size. Assuming
the parameter is a constant, each dynamic graph can be updated in $O(\log n)$
time and can compute a solution in $O(1)$ time. These results are obtained by
another approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7313</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7313</id><created>2014-04-29</created><authors><author><keyname>Ramezani</keyname><forenames>Hamid</forenames></author><author><keyname>Rajan</keyname><forenames>Raj Thilak</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Cramer Rao Lower Bound for Underwater Range Estimation with Noisy Sound
  Speed Profile</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, IEEE SP letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Cramer Rao bound (CRB) for range estimation between two
underwater nodes is calculated under a Gaussian noise assumption on the
measurements. The nodes can measure their depths, their mutual time of flight,
and they have access to noisy sound speed samples at different depths. The
effect of each measurement on the CRB will be analyzed, and it will be shown
that for long distances, the effect of the sound speed measurement noise is
dominant, and its impact depends on the positions of the nodes, actual sound
speed profile, the number of sound speed samples, and the depths at which the
sound speed samples are gathered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7318</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7318</id><created>2014-04-29</created><authors><author><keyname>Qu</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author></authors><title>Non-consensus opinion model on directed networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1103/PhysRevE.90.052811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic social opinion models have been widely studied on undirected
networks, and most of them are based on spin interaction models that produce a
consensus. In reality, however, many networks such as Twitter and the World
Wide Web are directed and are composed of both unidirectional and bidirectional
links. Moreover, from choosing a coffee brand to deciding who to vote for in an
election, two or more competing opinions often coexist. In response to this
ubiquity of directed networks and the coexistence of two or more opinions in
decision-making situations, we study a non-consensus opinion model introduced
by Shao et al. \cite{shao2009dynamic} on directed networks. We define
directionality $\xi$ as the percentage of unidirectional links in a network,
and we use the linear correlation coefficient $\rho$ between the indegree and
outdegree of a node to quantify the relation between the indegree and
outdegree. We introduce two degree-preserving rewiring approaches which allow
us to construct directed networks that can have a broad range of possible
combinations of directionality $\xi$ and linear correlation coefficient $\rho$
and to study how $\xi$ and $\rho$ impact opinion competitions. We find that, as
the directionality $\xi$ or the indegree and outdegree correlation $\rho$
increases, the majority opinion becomes more dominant and the minority
opinion's ability to survive is lowered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7325</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7325</id><created>2014-04-29</created><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Ellen</keyname><forenames>Faith</forenames></author></authors><title>Tight Bounds for an Online Bin Packing Problem</title><categories>cs.DS</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following online bin packing problem is considered: Items with integer
sizes are given and variable sized bins arrive online. A bin must be used if
there is still an item remaining which fits in it when the bin arrives. The
goal is to minimize the total size of all the bins used. Previously, a lower
bound of 5/4 on the competitive ratio of this problem was achieved using jobs
of size S and 2S-1 and maximum bin size 4S-4. For this case, we obtain matching
upper and lower bounds, which vary depending on the ratio of the number of
small jobs to the number of large jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7330</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7330</id><created>2014-04-29</created><authors><author><keyname>N.</keyname><forenames>Akshay Uttama Nambi S.</forenames></author><author><keyname>T.</keyname><forenames>Prabhakar</forenames><suffix>V</suffix></author><author><keyname>Prasad</keyname><forenames>R Venkatesha</forenames></author><author><keyname>S</keyname><forenames>Jamadagni H.</forenames></author></authors><title>Zero Energy Network stack for Energy Harvested WSNs</title><categories>cs.NI</categories><comments>12 pages, 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present our ``Zero Energy Network'' (ZEN) protocol stack for energy
harvesting wireless sensor networks applications. The novelty in our work is
$4$ fold: (1) Energy harvesting aware fully featured MAC layer. Carrier
sensing, Backoff algorithms, ARQ, RTS/CTS mechanisms, Adaptive Duty Cycling are
either auto configurable or available as tunable parameters to match the
available energy (b) Energy harvesting aware Routing Protocol. The multi-hop
network establishes routes to the base station using a modified version of
AODVjr routing protocol assisted by energy predictions. (c) Application of a
time series called ``Holt-Winters'' for predicting the incoming energy. (d) A
distributed smart application running over the ZEN stack which utilizes a multi
parameter optimized perturbation technique to optimally use the available
energy. The application is capable of programming the ZEN stack in an energy
efficient manner. The energy harvested distributed smart application runs on a
realistic solar energy trace with a three year seasonality database. We
implement a smart application, capable of modifying itself to suit its own as
well as the network's energy level. Our analytical results show a close match
with the measurements conducted over EHWSN testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7335</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7335</id><created>2014-04-29</created><authors><author><keyname>Jacquemard</keyname><forenames>Florent</forenames><affiliation>Inria Paris-Rocquencourt, STMS</affiliation></author><author><keyname>Sanchez</keyname><forenames>Cl&#xe9;ment Poncelet</forenames><affiliation>Inria Paris-Rocquencourt, STMS</affiliation></author></authors><title>Antescofo Intermediate Representation</title><categories>cs.MM cs.PL</categories><comments>RR-8520 (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an intermediate language designed as a medium-level internal
representation of programs of the interactive music system Antescofo. This
representation is independent both of the Antescofo source language and of the
architecture of the execution platform. It is used in tasks such as
verification of timings, model-based conformance testing, static control-flow
analysis or simulation. This language is essentially a flat representation of
Antescofo's code, as a finite state machine extended with local and global
variables, with delays and with concurrent threads creation. It features a
small number of simple instructions which are either blocking (wait for
external event, signal or duration) or not (variable assignment, message
emission and control).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7347</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7347</id><created>2014-04-29</created><updated>2014-09-10</updated><authors><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Gheorghe</keyname><forenames>Andrei H.</forenames></author><author><keyname>Patel</keyname><forenames>Monika</forenames></author><author><keyname>Habif</keyname><forenames>Jonathan</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author></authors><title>Covert Optical Communication</title><categories>cs.IT math.IT quant-ph</categories><comments>v4: improved organization and figures, no changes to the results; v3:
  significant changes, included new theoretical results as well as
  re-organized; v2: updated figure 3 with theoretical channel capacity curves +
  minor fixes, no changes to experiments or other main results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encryption prevents unauthorized decoding, but does not ensure stealth---a
security demand that a mere presence of a message be undetectable. We
characterize the ultimate limit of covert communication that is secure against
the most powerful physically-permissible adversary. We show that, although it
is impossible over a pure-loss channel, covert communication is attainable in
the presence of any excess noise, such as a $300$K thermal blackbody. In this
case, $\mathcal{O}(\sqrt{n})$ bits can be transmitted reliably and covertly in
$n$ optical modes using standard optical communication equipment. The
all-powerful adversary may intercept all transmitted photons not received by
the intended receiver, and employ arbitrary quantum memory and measurements.
Conversely, we show that this square root scaling cannot be outperformed. We
corroborate our theory in a proof-of-concept experiment. We believe that our
findings will enable practical realizations of covert communication and
sensing, both for point-to-point and networked scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7358</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7358</id><created>2014-04-28</created><authors><author><keyname>Nam</keyname><forenames>N. H.</forenames></author></authors><title>Building active and collaborative learning environment in introductory
  physics course of faculty of technology education</title><categories>cs.CY</categories><comments>submitted in Pedagogy and Psychology Journal. ISSN: 2077-6861</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model of active and collaborative learning applied in training specific
subject makes clear advantage due to the goals of knowledge, enhanced
activeness, skills that students got to develop successful future job. Studying
and applying the model to build a learning environment in the Introductory
Physics course with the support of ICT, especially social network and eLearning
system of HNUE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7362</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7362</id><created>2014-04-29</created><authors><author><keyname>Jia</keyname><forenames>Jinzhu</forenames></author><author><keyname>Miratrix</keyname><forenames>Luke</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author><author><keyname>Gawalt</keyname><forenames>Brian</forenames></author><author><keyname>Ghaoui</keyname><forenames>Laurent El</forenames></author><author><keyname>Barnesmoore</keyname><forenames>Luke</forenames></author><author><keyname>Clavier</keyname><forenames>Sophie</forenames></author></authors><title>Concise comparative summaries (CCS) of large text corpora with a human
  experiment</title><categories>cs.CL stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOAS698 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS698</report-no><journal-ref>Annals of Applied Statistics 2014, Vol. 8, No. 1, 499-529</journal-ref><doi>10.1214/13-AOAS698</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a general framework for topic-specific summarization
of large text corpora and illustrate how it can be used for the analysis of
news databases. Our framework, concise comparative summarization (CCS), is
built on sparse classification methods. CCS is a lightweight and flexible tool
that offers a compromise between simple word frequency based methods currently
in wide use and more heavyweight, model-intensive methods such as latent
Dirichlet allocation (LDA). We argue that sparse methods have much to offer for
text analysis and hope CCS opens the door for a new branch of research in this
important field. For a particular topic of interest (e.g., China or energy),
CSS automatically labels documents as being either on- or off-topic (usually
via keyword search), and then uses sparse classification methods to predict
these labels with the high-dimensional counts of all the other words and
phrases in the documents. The resulting small set of phrases found as
predictive are then harvested as the summary. To validate our tool, we, using
news articles from the New York Times international section, designed and
conducted a human survey to compare the different summarizers with human
understanding. We demonstrate our approach with two case studies, a media
analysis of the framing of &quot;Egypt&quot; in the New York Times throughout the Arab
Spring and an informal comparison of the New York Times' and Wall Street
Journal's coverage of &quot;energy.&quot; Overall, we find that the Lasso with $L^2$
normalization can be effectively and usefully used to summarize large corpora,
regardless of document size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7374</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7374</id><created>2014-04-29</created><authors><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Explicit and almost sure conditions for K/2 degrees of freedom</title><categories>cs.IT math.IT</categories><comments>To be presented at IEEE Int. Symp. Inf. Theory 2014, Honolulu, HI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that in K-user constant single-antenna interference channels
K/2 degrees of freedom (DoF) can be achieved for almost all channel matrices.
Explicit conditions on the channel matrix to admit K/2 DoF are, however, not
available. The purpose of this paper is to identify such explicit conditions,
which are satisfied for almost all channel matrices. We also provide a
construction of corresponding asymptotically DoF-optimal input distributions.
The main technical tool used is a recent breakthrough result by Hochman in
fractal geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7383</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7383</id><created>2014-04-29</created><updated>2014-05-05</updated><authors><author><keyname>Wang</keyname><forenames>Shenghao</forenames></author><author><keyname>Han</keyname><forenames>Huajie</forenames></author><author><keyname>Gao</keyname><forenames>Kun</forenames></author><author><keyname>Wang</keyname><forenames>Zhili</forenames></author><author><keyname>Zhang</keyname><forenames>Can</forenames></author><author><keyname>Yang</keyname><forenames>Meng</forenames></author><author><keyname>Wu</keyname><forenames>Zhao</forenames></author><author><keyname>Marcelli</keyname><forenames>Augusto</forenames></author><author><keyname>Wu</keyname><forenames>Ziyu</forenames></author></authors><title>A LabVIEW based user-friendly X-ray phase-contrast imaging system
  software platform</title><categories>cs.SE physics.ins-det physics.med-ph</categories><comments>11 pages, 5 figures, 1 table</comments><acm-class>D.2.6, J.2, I.4.0</acm-class><journal-ref>Journal of X-ray Science and Technology, vol. 23, No.2, 189-199,
  2015</journal-ref><doi>10.3233/XST-150480</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  X-ray phase-contrast imaging can provide greatly improved contrast over
conventional absorption-based imaging for weakly absorbing samples, such as
biological soft tissues and fibre composites. In this manuscript, we introduce
an easy and fast way to develop a user-friendly software platform dedicated to
the new grating-based X-ray phase-contrast imaging setup recently built at the
National Synchrotron Radiation Laboratory of the University of Science and
Technology of China. Unified management and control of 21 motorized positioning
stages, of an ultra-precision piezoelectric translation stage and of the X-ray
tube are achieved with this platform. The software package also covers the
automatic image acquisition of the phase-stepping scanning with a flat panel
detector. Moreover, a data post-processing module for signals retrieval and
other custom features are in principle available. With a seamless integration
of all necessary functions in a unique package, this software platform will
greatly support the user activity during experimental runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7401</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7401</id><created>2014-02-23</created><authors><author><keyname>Ma</keyname><forenames>Minghui</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author></authors><title>Residuated Basic Logic II. Interpolation, Decidability and Embedding</title><categories>math.LO cs.LO</categories><comments>17 pages with 1 figure</comments><msc-class>03B47</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the sequent calculus $\mathsf{L_{RBL}}$ for residuated basic
logic $\mathsf{RBL}$ has strong finite model property, and that intuitionistic
logic can be embedded into basic propositional logic $\mathsf{BPL}$. Thus
$\mathsf{RBL}$ is decidable. Moreover, it follows that the class of residuated
basic algebras has the finite embeddability property, and that $\mathsf{BPL}$
is PSPACE-complete, and that intuitionistic logic can be embedded into the
modal logic $\mathsf{K4}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7410</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7410</id><created>2014-04-29</created><authors><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Size-Separable Tile Self-Assembly: A Tight Bound for Temperature-1
  Mismatch-Free Systems</title><categories>cs.CG cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new property of tile self-assembly systems that we call
size-separability. A system is size-separable if every terminal assembly is a
constant factor larger than any intermediate assembly. Size-separability is
motivated by the practical problem of filtering completed assemblies from a
variety of incomplete &quot;garbage&quot; assemblies using gel electrophoresis or other
mass-based filtering techniques.
  Here we prove that any system without cooperative bonding assembling a unique
mismatch-free terminal assembly can be used to construct a size-separable
system uniquely assembling the same shape. The proof achieves optimal scale
factor and temperature for the size-separable system. As part of the proof, we
obtain two results of independent interest on mismatch-free temperature-1
two-handed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7414</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7414</id><created>2014-04-29</created><updated>2014-06-12</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Choi</keyname><forenames>Sou-Cheng T.</forenames></author><author><keyname>Lapp</keyname><forenames>Hilmar</forenames></author><author><keyname>Maheshwari</keyname><forenames>Ketan</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Frank</forenames></author><author><keyname>Turk</keyname><forenames>Matthew</forenames></author><author><keyname>Hanwell</keyname><forenames>Marcus D.</forenames></author><author><keyname>Wilkins-Diehr</keyname><forenames>Nancy</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Howison</keyname><forenames>James</forenames></author><author><keyname>Swenson</keyname><forenames>Shel</forenames></author><author><keyname>Allen</keyname><forenames>Gabrielle D.</forenames></author><author><keyname>Elster</keyname><forenames>Anne C.</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>Venters</keyname><forenames>Colin</forenames></author></authors><title>Summary of the First Workshop on Sustainable Software for Science:
  Practice and Experiences (WSSSPE1)</title><categories>cs.SE</categories><comments>Journal of Open Research Software, 2014</comments><doi>10.5334/jors.an</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Challenges related to development, deployment, and maintenance of reusable
software for science are becoming a growing concern. Many scientists' research
increasingly depends on the quality and availability of software upon which
their works are built. To highlight some of these issues and share experiences,
the First Workshop on Sustainable Software for Science: Practice and
Experiences (WSSSPE1) was held in November 2013 in conjunction with the SC13
Conference. The workshop featured keynote presentations and a large number (54)
of solicited extended abstracts that were grouped into three themes and
presented via panels. A set of collaborative notes of the presentations and
discussion was taken during the workshop.
  Unique perspectives were captured about issues such as comprehensive
documentation, development and deployment practices, software licenses and
career paths for developers. Attribution systems that account for evidence of
software contribution and impact were also discussed. These include mechanisms
such as Digital Object Identifiers, publication of &quot;software papers&quot;, and the
use of online systems, for example source code repositories like GitHub.
  This paper summarizes the issues and shared experiences that were discussed,
including cross-cutting issues and use cases. It joins a nascent literature
seeking to understand what drives software work in science, and how it is
impacted by the reward systems of science. These incentives can determine the
extent to which developers are motivated to build software for the long-term,
for the use of others, and whether to work collaboratively or separately. It
also explores community building, leadership, and dynamics in relation to
successful scientific software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7428</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7428</id><created>2014-04-29</created><authors><author><keyname>Hunter</keyname><forenames>Anthony</forenames></author></authors><title>Analysis of Dialogical Argumentation via Finite State Machines</title><categories>cs.AI</categories><comments>10 pages</comments><journal-ref>Proceedings of the International Conference on Scalable
  Uncertainty Management (SUM'13), LNCS 8078, Pages 1-14, Springer, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialogical argumentation is an important cognitive activity by which agents
exchange arguments and counterarguments as part of some process such as
discussion, debate, persuasion and negotiation. Whilst numerous formal systems
have been proposed, there is a lack of frameworks for implementing and
evaluating these proposals. First-order executable logic has been proposed as a
general framework for specifying and analysing dialogical argumentation. In
this paper, we investigate how we can implement systems for dialogical
argumentation using propositional executable logic. Our approach is to present
and evaluate an algorithm that generates a finite state machine that reflects a
propositional executable logic specification for a dialogical argumentation
together with an initial state. We also consider how the finite state machines
can be analysed, with the minimax strategy being used as an illustration of the
kinds of empirical analysis that can be undertaken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7431</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7431</id><created>2014-04-29</created><authors><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Bartel</keyname><forenames>Alexandre</forenames></author><author><keyname>Klein</keyname><forenames>Jacques</forenames></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames></author><author><keyname>Arzt</keyname><forenames>Steven</forenames></author><author><keyname>Rasthofer</keyname><forenames>Siegfried</forenames></author><author><keyname>Bodden</keyname><forenames>Eric</forenames></author><author><keyname>Octeau</keyname><forenames>Damien</forenames></author><author><keyname>McDaniel</keyname><forenames>Patrick</forenames></author></authors><title>I know what leaked in your pocket: uncovering privacy leaks on Android
  Apps with Static Taint Analysis</title><categories>cs.SE cs.CR</categories><report-no>978-2-87971-129-4_TR-SNT-2014-9</report-no><acm-class>D.2.4; D.4.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Android applications may leak privacy data carelessly or maliciously. In this
work we perform inter-component data-flow analysis to detect privacy leaks
between components of Android applications. Unlike all current approaches, our
tool, called IccTA, propagates the context between the components, which
improves the precision of the analysis. IccTA outperforms all other available
tools by reaching a precision of 95.0% and a recall of 82.6% on DroidBench. Our
approach detects 147 inter-component based privacy leaks in 14 applications in
a set of 3000 real-world applications with a precision of 88.4%. With the help
of ApkCombiner, our approach is able to detect inter-app based privacy leaks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7435</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7435</id><created>2014-04-29</created><updated>2014-05-28</updated><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>Open induction in a bounded arithmetic for TC^0</title><categories>cs.LO math.LO</categories><comments>35 pages</comments><msc-class>03F20, 03F30</msc-class><journal-ref>Archive for Mathematical Logic 54 (2015), no. 3--4, pp. 359--394</journal-ref><doi>10.1007/s00153-014-0414-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The elementary arithmetic operations $+,\cdot,\le$ on integers are well-known
to be computable in the weak complexity class $\mathrm{TC}^0$, and it is a
basic question what properties of these operations can be proved using only
$\mathrm{TC}^0$-computable objects, i.e., in a theory of bounded arithmetic
corresponding to $\mathrm{TC}^0$. We will show that the theory $\mathit{VTC}^0$
extended with an axiom postulating the totality of iterated multiplication
(which is computable in $\mathrm{TC}^0$) proves induction for quantifier-free
formulas in the language $\langle +,\cdot,\le \rangle$ (IOpen), and more
generally, minimization for $\Sigma^b_0$ formulas in the language of Buss's
$S_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7442</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7442</id><created>2014-04-29</created><updated>2015-07-03</updated><authors><author><keyname>Ceccherini-Silberstein</keyname><forenames>Tullio</forenames></author><author><keyname>Coornaert</keyname><forenames>Michel</forenames></author><author><keyname>Fiorenzi</keyname><forenames>Francesca</forenames></author><author><keyname>Schupp</keyname><forenames>Paul E.</forenames></author><author><keyname>Touikan</keyname><forenames>Nicholas W. M.</forenames></author></authors><title>Multipass automata and group word problems</title><categories>math.GR cs.FL</categories><msc-class>03B25, 05C05, 37B10, 37B15, 68Q70, 68Q80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of multipass automata as a generalization of pushdown
automata and study the classes of languages accepted by such machines. The
class of languages accepted by deterministic multipass automata is exactly the
Boolean closure of the class of deterministic context-free languages while the
class of languages accepted by nondeterministic multipass automata is exactly
the class of poly-context-free languages, that is, languages which are the
intersection of finitely many context-free languages. We illustrate the use of
these automata by studying groups whose word problems are in the above classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7443</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7443</id><created>2014-04-29</created><updated>2015-02-03</updated><authors><author><keyname>Koroth</keyname><forenames>Sajin</forenames></author><author><keyname>Sarma</keyname><forenames>Jayalal</forenames></author></authors><title>Depth Lower Bounds against Circuits with Sparse Orientation</title><categories>cs.CC</categories><comments>Version submitted to Journal. Replaced Theorem 3 with a weaker
  version fixing an error in the earlier version of the proof. This does not
  affect the main claims of the paper</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study depth lower bounds against non-monotone circuits, parametrized by a
new measure of non-monotonicity: the orientation of a function $f$ is the
characteristic vector of the minimum sized set of negated variables needed in
any DeMorgan circuit computing $f$. We prove trade-off results between the
depth and the weight/structure of the orientation vectors in any circuit $C$
computing the Clique function on an $n$ vertex graph. We prove that if $C$ is
of depth $d$ and each gate computes a Boolean function with orientation of
weight at most $w$ (in terms of the inputs to $C$), then $d \times w$ must be
$\Omega(n)$. In particular, if the weights are $o(\frac{n}{\log^k n})$, then
$C$ must be of depth $\omega(\log^k n)$. We prove a barrier for our general
technique. However, using specific properties of the Clique function and the
Karchmer-Wigderson framework (Karchmer and Wigderson, 1988), we go beyond the
limitations and obtain lower bounds when the weight restrictions are less
stringent. We then study the depth lower bounds when the structure of the
orientation vector is restricted. Asymptotic improvements to our results (in
the restricted setting), separates NP from NC. As our main tool, we generalize
Karchmer-Wigderson gamefor monotone functions to work for non-monotone circuits
parametrized by the weight/structure of the orientation. We also prove
structural results about orientation and prove connections between number of
negations and weight of orientations required to compute a function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7447</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7447</id><created>2014-04-02</created><authors><author><keyname>de Rassenfosse</keyname><forenames>Gaetan</forenames></author><author><keyname>Dernis</keyname><forenames>Helene</forenames></author><author><keyname>Boedt</keyname><forenames>Geert</forenames></author></authors><title>An Introduction to the Patstat Database with Example Queries</title><categories>cs.DL</categories><comments>To appear in the Australian Economic Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an introduction to the Patstat patent database. It offers
guided examples of ten popular queries that are relevant for research purposes
and that cover the most important data tables. It is targeted at academic
researchers and practitioners willing to learn the basics of the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7456</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7456</id><created>2014-04-28</created><authors><author><keyname>Baydin</keyname><forenames>Atilim Gunes</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author></authors><title>Automatic Differentiation of Algorithms for Machine Learning</title><categories>cs.LG cs.SC stat.ML</categories><comments>7 pages, 1 figure</comments><msc-class>68W30, 65D25, 68T05</msc-class><acm-class>G.1.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic differentiation---the mechanical transformation of numeric computer
programs to calculate derivatives efficiently and accurately---dates to the
origin of the computer age. Reverse mode automatic differentiation both
antedates and generalizes the method of backwards propagation of errors used in
machine learning. Despite this, practitioners in a variety of fields, including
machine learning, have been little influenced by automatic differentiation, and
make scant use of available tools. Here we review the technique of automatic
differentiation, describe its two main modes, and explain how it can benefit
machine learning practitioners. To reach the widest possible audience our
treatment assumes only elementary differential calculus, and does not assume
any knowledge of linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7458</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7458</id><created>2014-04-29</created><updated>2016-01-12</updated><authors><author><keyname>Heuberger</keyname><forenames>Clemens</forenames></author><author><keyname>Krenn</keyname><forenames>Daniel</forenames></author><author><keyname>Kropf</keyname><forenames>Sara</forenames></author></authors><title>Automata in SageMath---Combinatorics meet Theoretical Computer Science</title><categories>cs.FL math.CO</categories><msc-class>11A63, 05A16, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new finite state machine package in the mathematics software system
SageMath is presented and illustrated by many examples. Several combinatorial
problems, in particular digit problems, are introduced, modeled by automata and
transducers and solved using SageMath. In particular, we compute the asymptotic
Hamming weight of a non-adjacent-form-like digit expansion, which was not known
before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7467</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7467</id><created>2014-04-07</created><updated>2014-12-04</updated><authors><author><keyname>Li</keyname><forenames>Fangfang</forenames></author><author><keyname>Xu</keyname><forenames>Guandong</forenames></author><author><keyname>Cao</keyname><forenames>Longbing</forenames></author></authors><title>Coupled Matrix Factorization within Non-IID Context</title><categories>cs.IR</categories><comments>12 pages submitted to PAKDD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems research has experienced different stages such as from
user preference understanding to content analysis. Typical recommendation
algorithms were built on the following bases: (1) assuming users and items are
IID, namely independent and identically distributed, and (2) focusing on
specific aspects such as user preferences or contents. In reality, complex
recommendation tasks involve and request (1) personalized outcomes to tailor
heterogeneous subjective preferences; and (2) explicit and implicit objective
coupling relationships between users, items, and ratings to be considered as
intrinsic forces driving preferences. This inevitably involves the non-IID
complexity and the need of combining subjective preference with objective
couplings hidden in recommendation applications. In this paper, we propose a
novel generic coupled matrix factorization (CMF) model by incorporating non-IID
coupling relations between users and items. Such couplings integrate the
intra-coupled interactions within an attribute and inter-coupled interactions
among different attributes. Experimental results on two open data sets
demonstrate that the user/item couplings can be effectively applied in RS and
CMF outperforms the benchmark methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7472</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7472</id><created>2014-04-29</created><authors><author><keyname>Mattfeld</keyname><forenames>Carl</forenames></author></authors><title>Implementing spectral methods for hidden Markov models with real-valued
  emissions</title><categories>cs.LG</categories><comments>57 pages (including front and back matter), 7 figures. Master thesis
  towards the Master's Degree in Physics, ETH Zurich, Switzerland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models (HMMs) are widely used statistical models for modeling
sequential data. The parameter estimation for HMMs from time series data is an
important learning problem. The predominant methods for parameter estimation
are based on local search heuristics, most notably the expectation-maximization
(EM) algorithm. These methods are prone to local optima and oftentimes suffer
from high computational and sample complexity. Recent years saw the emergence
of spectral methods for the parameter estimation of HMMs, based on a method of
moments approach. Two spectral learning algorithms as proposed by Hsu, Kakade
and Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012
(arXiv:1203.0683) are assessed in this work. Using experiments with synthetic
data, the algorithms are compared with each other. Furthermore, the spectral
methods are compared to the Baum-Welch algorithm, a well-established method
applying the EM algorithm to HMMs. The spectral algorithms are found to have a
much more favorable computational and sample complexity. Even though the
algorithms readily handle high dimensional observation spaces, instability
issues are encountered in this regime. In view of learning from real-world
experimental data, the representation of real-valued observations for the use
in spectral methods is discussed, presenting possible methods to represent data
for the use in the learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7478</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7478</id><created>2014-04-29</created><authors><author><keyname>Xie</keyname><forenames>Jianwei</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Secure Degrees of Freedom Regions of Multiple Access and Interference
  Channels: The Polytope Structure</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum secure degrees of freedom (s.d.o.f.) of two fundamental multi-user
network structures, the K-user Gaussian multiple access (MAC) wiretap channel
and the K-user interference channel (IC) with secrecy constraints, have been
determined recently as K(K-1)/(K(K-1)+1) [1,2] and K(K-1)/(2K-1) [3,4],
respectively. In this paper, we determine the entire s.d.o.f. regions of these
two channel models. The converse for the MAC follows from a middle step in the
converse of [1,2]. The converse for the IC includes constraints both due to
secrecy as well as due to interference. Although the portion of the region
close to the optimum sum s.d.o.f. point is governed by the upper bounds due to
secrecy constraints, the other portions of the region are governed by the upper
bounds due to interference constraints. Different from the existing literature,
in order to fully understand the characterization of the s.d.o.f. region of the
IC, one has to study the 4-user case, i.e., the 2 or 3-user cases do not
illustrate the generality of the problem. In order to prove the achievability,
we use the polytope structure of the converse region. In both MAC and IC cases,
we develop explicit schemes that achieve the extreme points of the polytope
region given by the converse. Specifically, the extreme points of the MAC
region are achieved by an m-user MAC wiretap channel with (K-m) helpers, i.e.,
by setting (K-m) users' secure rates to zero and utilizing them as pure
(structured) cooperative jammers. The extreme points of the IC region are
achieved by a (K-m)-user IC with confidential messages, m helpers, and N
external eavesdroppers, for m&gt;=1 and a finite N. A byproduct of our results in
this paper is that the sum s.d.o.f. is achieved only at one extreme point of
the s.d.o.f. region, which is the symmetric-rate extreme point, for both MAC
and IC channel models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7479</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7479</id><created>2014-04-29</created><authors><author><keyname>Cooper</keyname><forenames>Colin</forenames></author><author><keyname>Els&#xe4;sser</keyname><forenames>Robert</forenames></author><author><keyname>Radzik</keyname><forenames>Tomasz</forenames></author></authors><title>The Power of Two Choices in Distributed Voting</title><categories>cs.DS</categories><comments>22 pages</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed voting is a fundamental topic in distributed computing. In pull
voting, in each step every vertex chooses a neighbour uniformly at random, and
adopts its opinion. The voting is completed when all vertices hold the same
opinion. On many graph classes including regular graphs, pull voting requires
$\Theta(n)$ expected steps to complete, even if initially there are only two
distinct opinions.
  In this paper we consider a related process which we call two-sample voting:
every vertex chooses two random neighbours in each step. If the opinions of
these neighbours coincide, then the vertex revises its opinion according to the
chosen sample. Otherwise, it keeps its own opinion. We consider the performance
of this process in the case where two di?erent opinions reside on vertices of
some (arbitrary) sets $A$ and $B$, respectively. Here, $|A| + |B| = n$ is the
number of vertices of the graph.
  We show that there is a constant $K$ such that if the initial imbalance
between the two opinions is ?$\nu_0 = (|A| - |B|)/n \geq K \sqrt{(1/d) +
(d/n)}$, then with high probability two sample voting completes in a random $d$
regular graph in $O(\log n)$ steps and the initial majority opinion wins. We
also show the same performance for any regular graph, if $\nu_0 \geq K
\lambda_2$ where $\lambda_2$ is the second largest eigenvalue of the transition
matrix. In the graphs we consider, standard pull voting requires $\Omega(n)$
steps, and the minority can still win with probability $|B|/n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7494</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7494</id><created>2014-04-29</created><authors><author><keyname>Bipinchandra</keyname><forenames>Gandhi Kishan</forenames></author><author><keyname>Aluvalu</keyname><forenames>Prof. Rajanikanth</forenames></author><author><keyname>Singh</keyname><forenames>Dr. Ajay Shanker</forenames></author></authors><title>Intelligent Resource Allocation Technique For Desktop-as-a-Service in
  Cloud Environment</title><categories>cs.DC</categories><comments>6 pages, 3 figure, 1 table</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The specialty of desktop-as-a-service cloud computing is that user can access
their desktop and can execute applications in virtual desktops on remote
servers. Resource management and resource utilization are most significant in
the area of desktop-as-a-service, cloud computing; however, handling a large
amount of clients in the most efficient manner poses important challenges.
Especially deciding how many clients to handle on one server, and where to
execute the user applications at each time is important. This is because we
have to ensure maximum resource utilization along with user data
confidentiality, customer satisfaction, scalability, minimum Service level
agreement (SLA) violation etc. Assigning too many users to one server leads to
customer dissatisfaction, while assigning too little leads to higher
investments costs. So we have taken into consideration these two situations
also. We study different aspects to optimize the resource usage and customer
satisfaction. Here in this paper We proposed Intelligent Resource Allocation
(IRA) Technique which assures the above mentioned parameters like minimum SLA
violation. For this, priorities are assigned to user requests based on their
SLA Factors in order to maintain their confidentiality. The results of the
paper indicate that by applying IRA Technique to the already existing
overbooking mechanism will improve the performance of the system with
significant reduction in SLA violation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7509</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7509</id><created>2014-04-29</created><authors><author><keyname>Alajrami</keyname><forenames>Sami</forenames></author></authors><title>On Cloud-Based Engineering of Dependable Systems</title><categories>cs.SE</categories><comments>EDCC-2014, Student-Forum, Cloud Computing, Cloud Workflow Systems,
  Dependable, Systems, Software Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud computing paradigm is being adopted by many organizations in
different application domains as it is cost effective and offers a virtually
unlimited pool of resources. Engineering critical systems can benefit from
clouds in attaining all dependability means: fault tolerance, fault prevention,
fault removal and fault forecasting. Our research aims to investigate the
potential of supporting engineering of dependable software systems with cloud
computing and proposes an open, extensible, and elastic cloud-based software
engineering workflow system which represents and executes software processes to
improve collaboration, reliability and quality assurance, and automation in
software projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7513</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7513</id><created>2014-04-29</created><updated>2014-05-07</updated><authors><author><keyname>Babin</keyname><forenames>Guillaume</forenames></author></authors><title>A formal approach for correct-by-construction system substitution</title><categories>cs.SE</categories><comments>EDCC-2014, Student-Forum, System Substitution, state rRecovery,
  correct-bycorrection, Event-B, refinement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The substitution of a system with another one may occur in several situations
like system adaptation, system failure management, system resilience, system
reconfiguration, etc. It consists in replacing a running system by another one
when given conditions hold. This contribution summarizes our proposal to define
a formal setting for proving the correctness of system substitution. It relies
on refinement and on the Event-B method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7514</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7514</id><created>2014-04-29</created><updated>2015-02-10</updated><authors><author><keyname>Petrovici</keyname><forenames>Mihai A.</forenames></author><author><keyname>Vogginger</keyname><forenames>Bernhard</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Paul</forenames></author><author><keyname>Breitwieser</keyname><forenames>Oliver</forenames></author><author><keyname>Lundqvist</keyname><forenames>Mikael</forenames></author><author><keyname>Muller</keyname><forenames>Lyle</forenames></author><author><keyname>Ehrlich</keyname><forenames>Matthias</forenames></author><author><keyname>Destexhe</keyname><forenames>Alain</forenames></author><author><keyname>Lansner</keyname><forenames>Anders</forenames></author><author><keyname>Sch&#xfc;ffny</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Schemmel</keyname><forenames>Johannes</forenames></author><author><keyname>Meier</keyname><forenames>Karlheinz</forenames></author></authors><title>Characterization and Compensation of Network-Level Anomalies in
  Mixed-Signal Neuromorphic Modeling Platforms</title><categories>q-bio.NC cond-mat.dis-nn cs.NE</categories><journal-ref>PLOS ONE, October 10th 2014</journal-ref><doi>10.1371/journal.pone.0108590</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancing the size and complexity of neural network models leads to an ever
increasing demand for computational resources for their simulation.
Neuromorphic devices offer a number of advantages over conventional computing
architectures, such as high emulation speed or low power consumption, but this
usually comes at the price of reduced configurability and precision. In this
article, we investigate the consequences of several such factors that are
common to neuromorphic devices, more specifically limited hardware resources,
limited parameter configurability and parameter variations. Our final aim is to
provide an array of methods for coping with such inevitable distortion
mechanisms. As a platform for testing our proposed strategies, we use an
executable system specification (ESS) of the BrainScaleS neuromorphic system,
which has been designed as a universal emulation back-end for neuroscientific
modeling. We address the most essential limitations of this device in detail
and study their effects on three prototypical benchmark network models within a
well-defined, systematic workflow. For each network model, we start by defining
quantifiable functionality measures by which we then assess the effects of
typical hardware-specific distortion mechanisms, both in idealized software
simulations and on the ESS. For those effects that cause unacceptable
deviations from the original network dynamics, we suggest generic compensation
mechanisms and demonstrate their effectiveness. Both the suggested workflow and
the investigated compensation mechanisms are largely back-end independent and
do not require additional hardware configurability beyond the one required to
emulate the benchmark networks in the first place. We hereby provide a generic
methodological environment for configurable neuromorphic devices that are
targeted at emulating large-scale, functional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7516</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7516</id><created>2014-04-29</created><authors><author><keyname>Lacerda</keyname><forenames>Felipe G.</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Classical leakage resilience from fault-tolerant quantum computation</title><categories>quant-ph cs.CR</categories><comments>22 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical implementations of cryptographic algorithms leak information, which
makes them vulnerable to so-called side-channel attacks. The problem of secure
computation in the presence of leakage is generally known as leakage
resilience. In this work, we establish a connection between leakage resilience
and fault-tolerant quantum computation. We first prove that for a general
leakage model, there exists a corresponding noise model in which fault
tolerance implies leakage resilience. Then we show how to use constructions for
fault-tolerant quantum computation to implement classical circuits that are
secure in specific leakage models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7527</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7527</id><created>2014-04-29</created><updated>2014-07-03</updated><authors><author><keyname>K&#xf6;tzing</keyname><forenames>Timo</forenames></author><author><keyname>Palenta</keyname><forenames>Raphaela</forenames></author></authors><title>A Map of Update Constraints in Inductive Inference</title><categories>cs.LG</categories><comments>fixed a mistake in Theorem 21, result is the same</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate how different learning restrictions reduce learning power and
how the different restrictions relate to one another. We give a complete map
for nine different restrictions both for the cases of complete information
learning and set-driven learning. This completes the picture for these
well-studied \emph{delayable} learning restrictions. A further insight is
gained by different characterizations of \emph{conservative} learning in terms
of variants of \emph{cautious} learning.
  Our analyses greatly benefit from general theorems we give, for example
showing that learners with exclusively delayable restrictions can always be
assumed total.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7528</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7528</id><created>2014-04-29</created><updated>2014-05-07</updated><authors><author><keyname>Ashmore</keyname><forenames>Rob</forenames></author></authors><title>The Utility and Practicality of Quantifying Software Reliability</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that quantifying software reliability is important in demonstrating
that system-level risks are As Low As Reasonably Practicable (ALARP).
Furthermore, we demonstrate that such quantification is possible in at least
one meaningful case. It is, however, unlikely to be practical in every case.
This means it is unlikely to be included as an explicit objective in standards.
Hence, for those cases where software reliability can be quantified, merely
following a standard may lead to risk-reduction opportunities being missed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7530</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7530</id><created>2014-04-29</created><updated>2014-08-13</updated><authors><author><keyname>Eckles</keyname><forenames>Dean</forenames></author><author><keyname>Karrer</keyname><forenames>Brian</forenames></author><author><keyname>Ugander</keyname><forenames>Johan</forenames></author></authors><title>Design and analysis of experiments in networks: Reducing bias from
  interference</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>32 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the effects of interventions in networks is complicated when the
units are interacting, such that the outcomes for one unit may depend on the
treatment assignment and behavior of many or all other units (i.e., there is
interference). When most or all units are in a single connected component, it
is impossible to directly experimentally compare outcomes under two or more
global treatment assignments since the network can only be observed under a
single assignment. Familiar formalism, experimental designs, and analysis
methods assume the absence of these interactions, and result in biased
estimators of causal effects of interest. While some assumptions can lead to
unbiased estimators, these assumptions are generally unrealistic, and we focus
this work on realistic assumptions. Thus, in this work, we evaluate methods for
designing and analyzing randomized experiments that aim to reduce this bias and
thereby reduce overall error. In design, we consider the ability to perform
random assignment to treatments that is correlated in the network, such as
through graph cluster randomization. In analysis, we consider incorporating
information about the treatment assignment of network neighbors. We prove
sufficient conditions for bias reduction through both design and analysis in
the presence of potentially global interference. Through simulations of the
entire process of experimentation in networks, we measure the performance of
these methods under varied network structure and varied social behaviors,
finding substantial bias and error reductions. These improvements are largest
for networks with more clustering and data generating processes with both
stronger direct effects of the treatment and stronger interactions between
units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7533</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7533</id><created>2014-04-29</created><updated>2014-10-16</updated><authors><author><keyname>Bailly</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Denis</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Rabusseau</keyname><forenames>Guillaume</forenames></author></authors><title>Recognizable Series on Hypergraphs</title><categories>cs.FL math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of Hypergraph Weighted Model (HWM) that generically
associates a tensor network to a hypergraph and then computes a value by tensor
contractions directed by its hyperedges. A series r defined on a hypergraph
family is said to be recognizable if there exists a HWM that computes it. This
model generalizes the notion of rational series on strings and trees. We prove
some properties of the model and study at which conditions finite support
series are recognizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7541</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7541</id><created>2014-04-29</created><authors><author><keyname>Delgrande</keyname><forenames>James P.</forenames></author><author><keyname>Wang</keyname><forenames>Kewen</forenames></author></authors><title>An Approach to Forgetting in Disjunctive Logic Programs that Preserves
  Strong Equivalence</title><categories>cs.AI cs.LO</categories><comments>In: Proceedings of 15th International Workshop on Non-Monotonic
  Reasoning</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we investigate forgetting in disjunctive logic programs, where
forgetting an atom from a program amounts to a reduction in the signature of
that program. The goal is to provide an approach that is syntax-independent, in
that if two programs are strongly equivalent, then the results of forgetting an
atom in each program should also be strongly equivalent. Our central definition
of forgetting is impractical but satisfies this goal: Forgetting an atom is
characterised by the set of SE consequences of the program that do not mention
the atom to be forgotten. We then provide an equivalent, practical definition,
wherein forgetting an atom $p$ is given by those rules in the program that
don't mention $p$, together with rules obtained by a single inference step from
rules that do mention $p$. Forgetting is shown to have appropriate properties;
as well, the finite characterisation results in a modest (at worst quadratic)
blowup. Finally we have also obtained a prototype implementation of this
approach to forgetting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7542</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7542</id><created>2014-04-29</created><authors><author><keyname>Wiels</keyname><forenames>Virginie</forenames></author></authors><title>A formal experiment to assess the efficacy of certification standards</title><categories>cs.SE</categories><comments>EDCC-2014, AESSCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proving the efficacy of certification standards
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7548</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7548</id><created>2014-04-29</created><updated>2014-05-07</updated><authors><author><keyname>Emerson</keyname><forenames>Ryan</forenames></author><author><keyname>Ezhilchelvan</keyname><forenames>Paul</forenames></author></authors><title>Faster Transaction Commit even when Nodes Crash</title><categories>cs.DC</categories><comments>EDCC-2014, Fast-Abstracts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Atomic broadcasts play a central role in serialisable in-memory transactions.
Best performing ones block, when a node crashes, until a new view is installed.
We augment a new protocol for uninterrupted progress in the interim period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7550</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7550</id><created>2014-04-29</created><authors><author><keyname>Thakur</keyname><forenames>Gaurav</forenames></author></authors><title>The Synchrosqueezing transform for instantaneous spectral analysis</title><categories>cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Synchrosqueezing transform is a time-frequency analysis method that can
decompose complex signals into time-varying oscillatory components. It is a
form of time-frequency reassignment that is both sparse and invertible,
allowing for the recovery of the signal. This article presents an overview of
the theory and stability properties of Synchrosqueezing, as well as
applications of the technique to topics in cardiology, climate science and
economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7551</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7551</id><created>2014-04-29</created><authors><author><keyname>Itria</keyname><forenames>Massimiliano L.</forenames></author><author><keyname>Daidone</keyname><forenames>Alessandro</forenames></author><author><keyname>Ceccarelli</keyname><forenames>Andrea</forenames></author></authors><title>A Complex Event Processing Approach for Crisis-Management Systems</title><categories>cs.CY</categories><comments>information fusion, complex event processing, crowd sensing, crowd
  sourcing, decision support system, online processing, crisis management</comments><report-no>EDCC-2014, BIG4CIP-2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern advanced emergency management systems many solutions for decision
support have been provided as attempts to support humans to take important
decisions for the critical situations recovery. The critical situation
detection is a complex procedure that involves both human and machine
activities and leads to take a decision for the management and situation
recovery. This paper presents an approach for critical situation detection
which uses event correlation technologies performing online analysis of real
events through a Complex Event Processing architecture. Event correlation is
used to relate events gathered from various sources, including crowd sensing
and crowd sourcing sources, for detecting patterns and situations of interest
in the emergency management context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7558</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7558</id><created>2014-04-29</created><authors><author><keyname>Fuschi</keyname><forenames>David Luigi</forenames></author><author><keyname>Tvaronaviciene</keyname><forenames>Manuela</forenames></author><author><keyname>D'Antonio</keyname><forenames>Salvatore</forenames></author></authors><title>Monitoring service quality: methods and solutions to implement a
  managerial dash-board to improve software development</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014, Quality Assurance, Management Methods, Data
  Collection and Analysis, Software Reliability Assessment, Testing and
  Verification, Software Reliability Measurement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The software used for running and handling the inter-bank network framework
provides services with extremely strict uptime (above 99.98 percent) and
quality requirements, thus tools to trace and manage changes as well as metrics
to measure process quality are essential. Having conducted a two year long
campaign of data collection and activity monitoring it has been possible to
analyze a huge amount of process data from which many aggregated indicators
were derived, selected and evaluated for providing a managerial dash-board to
monitor software development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7559</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7559</id><created>2014-04-29</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author></authors><title>Near-Optimal Distributed Approximation of Minimum-Weight Connected
  Dominating Set</title><categories>cs.DS cs.DC</categories><comments>An extended abstract version of this result appears in the
  proceedings of 41st International Colloquium on Automata, Languages, and
  Programming (ICALP 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a near-optimal distributed approximation algorithm for
the minimum-weight connected dominating set (MCDS) problem. The presented
algorithm finds an $O(\log n)$ approximation in $\tilde{O}(D+\sqrt{n})$ rounds,
where $D$ is the network diameter and $n$ is the number of nodes.
  MCDS is a classical NP-hard problem and the achieved approximation factor
$O(\log n)$ is known to be optimal up to a constant factor, unless P=NP.
Furthermore, the $\tilde{O}(D+\sqrt{n})$ round complexity is known to be
optimal modulo logarithmic factors (for any approximation), following [Das
Sarma et al.---STOC'11].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7560</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7560</id><created>2014-04-29</created><authors><author><keyname>Nappi</keyname><forenames>Roberto</forenames></author></authors><title>Integrated Maintenance: analysis and perspective of innovation in
  railway sector</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014, Maintenance Management, Railway Diagnostic,
  Condition Monitoring, Critical Infrastructure Protection</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high costs for the management of the modern and complex industrial
control systems make it necessary to enhance the current maintenance processes.
Therefore, the need arises to clearly define the goals of the maintenance, in
order to evolve and continuously enhance the management methods, to efficiently
integrate the maintenance activities with the ones related to the production,
the service provisioning, and the operation, and to use smart computer-based
maintenance systems. This paper proposes a general maintenance approach and its
specific application to the railway sector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7563</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7563</id><created>2014-04-29</created><authors><author><keyname>Cerullo</keyname><forenames>Gianfranco</forenames></author><author><keyname>Formicola</keyname><forenames>Valerio</forenames></author><author><keyname>Iamiglio</keyname><forenames>Pietro</forenames></author><author><keyname>Sgaglione</keyname><forenames>Luigi</forenames></author></authors><title>Critical Infrastructure Protection: having SIEM technology cope with
  network heterogeneity</title><categories>cs.CR</categories><comments>EDCC-2014, BIG4CIP-2014, Critical Infrastructure Protection,
  Intrusion Detection and Diagnosis, Complex Event Processing , Security
  Information and Event Management (SIEM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated and targeted cyber-attacks to Critical Infrastructures (CIs) are
becoming more and more frequent and sophisticated. This is due to: i) the
recent technology shift towards Commercial Off-The-Shelf (COTS) products, and
ii) new economical and socio-political motivations. In this paper, we discuss
some of the most relevant security issues resulting from the adoption in CIs of
heterogeneous network infrastructures (specifically combining wireless and IP
trunks), and suggest techniques to detect, as well as to counter/mitigate
attacks. We claim that techniques such as those we propose here should be
integrated in future SIEM (Security Information and Event Management)
solutions, and we discuss how we have done so in the EC-funded MASSIF project,
with respect to a real-world CI scenario, specifically a distributed system for
power grid monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7564</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7564</id><created>2014-04-29</created><authors><author><keyname>Choras</keyname><forenames>Michal</forenames></author><author><keyname>Kozik</keyname><forenames>Rafal</forenames></author><author><keyname>Renk</keyname><forenames>Rafal</forenames></author><author><keyname>Holubowicz</keyname><forenames>Witold</forenames></author></authors><title>End-users needs and requirements for tools to support critical
  infrastructures protection</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014, CIP, CIPRNet project, decision support,
  services</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of the services described in this paper is to support decisions in
the Critical Infrastructure Protection (CIP) domain. Those services are
perceived as the most fundamental functionalities, that will serve as a basis
for the planned European simulation centre for modelling the behaviour of
Critical Infrastructures (CI). The proposed services are: CI-related data
accessing and gathering, threat forecasting and visualisation, consequence
analysis, crowd management, as well as resources and capability management. In
general, services proposed in the current paper will contribute to reducing the
problem of overwhelming decision makers by too large amount of information. In
the crisis, their decisions are made on the basis of the large amount of data
related to the current situation, such as the status of CI, localisation of
capabilities, weather and threat forecasts etc. The design of the services has
been established with the help of the future end-users. The work presented in
this paper is the result of preliminary activities performed in the FP7 project
CIPRNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7565</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7565</id><created>2014-04-29</created><updated>2014-05-19</updated><authors><author><keyname>Ebrahimy</keyname><forenames>Razgar</forenames></author></authors><title>Investigating SCADA Failures in Interdependent Critical Infrastructure
  Systems</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014, SCADA Systems, Interdependency, Critical
  Infrastructure System Failures</comments><proxy>Ana Mihut</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is based on the initial ideas of a PhD proposal which will
investigate SCADA failures in physical infrastructure systems. The results will
be used to develop a new notation to help risk assessment using dependable
computing concepts. SCADA systems are widely used within critical
infrastructures to perform system controls and deliver services to linked and
dependent systems. Failures in SCADA systems will be investigated to help us
understand and prevent cascading failures in future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7566</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7566</id><created>2014-04-29</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Chen</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Image Compressive Sensing Recovery Using Adaptively Learned Sparsifying
  Basis via L0 Minimization</title><categories>cs.CV</categories><comments>31 pages, 4 tables, 12 figures, to be published at Signal Processing,
  Code available: http://idm.pku.edu.cn/staff/zhangjian/ALSB/</comments><doi>10.1016/j.sigpro.2013.09.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From many fewer acquired measurements than suggested by the Nyquist sampling
theory, compressive sensing (CS) theory demonstrates that, a signal can be
reconstructed with high probability when it exhibits sparsity in some domain.
Most of the conventional CS recovery approaches, however, exploited a set of
fixed bases (e.g. DCT, wavelet and gradient domain) for the entirety of a
signal, which are irrespective of the non-stationarity of natural signals and
cannot achieve high enough degree of sparsity, thus resulting in poor CS
recovery performance. In this paper, we propose a new framework for image
compressive sensing recovery using adaptively learned sparsifying basis via L0
minimization. The intrinsic sparsity of natural images is enforced
substantially by sparsely representing overlapped image patches using the
adaptively learned sparsifying basis in the form of L0 norm, greatly reducing
blocking artifacts and confining the CS solution space. To make our proposed
scheme tractable and robust, a split Bregman iteration based technique is
developed to solve the non-convex L0 minimization problem efficiently.
Experimental results on a wide range of natural images for CS recovery have
shown that our proposed algorithm achieves significant performance improvements
over many current state-of-the-art schemes and exhibits good convergence
property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7569</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7569</id><created>2014-04-29</created><updated>2015-03-14</updated><authors><author><keyname>Gao</keyname><forenames>Zhihan</forenames></author></authors><title>On the metric s-t path Traveling Salesman Problem</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We study the metric $s$-$t$ path Traveling Salesman Problem (TSP). [An,
Kleinberg, and Shmoys, STOC 2012] improved on the long standing
$\frac{5}{3}$-approximation factor and presented an algorithm that achieves an
approximation factor of $\frac{1+\sqrt{5}}{2}\approx1.61803$. Later [Seb\H{o},
IPCO 2013] further improved the approximation factor to $\frac{8}{5}$. We
present a simple, self-contained analysis that unifies both results; our main
contribution is a \emph{unified correction vector}. We compare two different
linear programming (LP) relaxations of the $s$-$t$ path TSP, namely, the path
version of the Held-Karp LP relaxation for TSP and a weaker LP relaxation, and
we show that both LPs have the same (fractional) optimal value. Also, we show
that the minimum-cost of integral solutions of the two LPs are within a factor
of $\frac{3}{2}$ of each other. We prove that a half-integral solution of the
stronger LP-relaxation of cost $c$ can be rounded to an integral solution of
cost at most $\frac{3}{2}c$. Additionally, we give a bad instance that presents
obstructions to two obvious methods that aim for an approximation factor of
$\frac{3}{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7571</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7571</id><created>2014-04-29</created><authors><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Li</keyname><forenames>Feifei</forenames></author></authors><title>Continuous Matrix Approximation on Distributed Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking and approximating data matrices in streaming fashion is a
fundamental challenge. The problem requires more care and attention when data
comes from multiple distributed sites, each receiving a stream of data. This
paper considers the problem of &quot;tracking approximations to a matrix&quot; in the
distributed streaming model. In this model, there are m distributed sites each
observing a distinct stream of data (where each element is a row of a
distributed matrix) and has a communication channel with a coordinator, and the
goal is to track an eps-approximation to the norm of the matrix along any
direction. To that end, we present novel algorithms to address the matrix
approximation problem. Our algorithms maintain a smaller matrix B, as an
approximation to a distributed streaming matrix A, such that for any unit
vector x: | ||A x||^2 - ||B x||^2 | &lt;= eps ||A||_F^2. Our algorithms work in
streaming fashion and incur small communication, which is critical for
distributed computation. Our best method is deterministic and uses only
O((m/eps) log(beta N)) communication, where N is the size of stream (at the
time of the query) and beta is an upper-bound on the squared norm of any row of
the matrix. In addition to proving all algorithmic properties theoretically,
extensive experiments with real large datasets demonstrate the efficiency of
these protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7580</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7580</id><created>2014-04-29</created><updated>2015-05-19</updated><authors><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author><author><keyname>Huang</keyname><forenames>Zhang</forenames></author><author><keyname>Yuan</keyname><forenames>Chun-Ming</forenames></author></authors><title>Binomial Difference Ideal and Toric Difference Variety</title><categories>math.AG cs.SC</categories><comments>72 pages</comments><msc-class>Primary 12H10, 14M25, Secondary 14Q99, 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the concepts of binomial difference ideals and toric
difference varieties are defined and their properties are proved. Two canonical
representations for Laurent binomial difference ideals are given using the
reduced Groebner basis of Z[x]-lattices and regular and coherent difference
ascending chains, respectively. Criteria for a Laurent binomial difference
ideal to be reflexive, prime, well-mixed, perfect, and toric are given in terms
of their support lattices which are Z[x]-lattices. The reflexive, well-mixed,
and perfect closures of a Laurent binomial difference ideal are shown to be
binomial. Four equivalent definitions for toric difference varieties are
presented. Finally, algorithms are given to check whether a given Laurent
binomial difference ideal I is reflexive, prime, well-mixed, perfect, or toric,
and in the negative case, to compute the reflexive, well-mixed, and perfect
closures of I. An algorithm is given to decompose a finitely generated perfect
binomial difference ideal as the intersection of reflexive prime binomial
difference ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7584</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7584</id><created>2014-04-30</created><updated>2014-11-04</updated><authors><author><keyname>Henriques</keyname><forenames>Jo&#xe3;o F.</forenames></author><author><keyname>Caseiro</keyname><forenames>Rui</forenames></author><author><keyname>Martins</keyname><forenames>Pedro</forenames></author><author><keyname>Batista</keyname><forenames>Jorge</forenames></author></authors><title>High-Speed Tracking with Kernelized Correlation Filters</title><categories>cs.CV</categories><doi>10.1109/TPAMI.2014.2345390</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The core component of most modern trackers is a discriminative classifier,
tasked with distinguishing between the target and the surrounding environment.
To cope with natural image changes, this classifier is typically trained with
translated and scaled sample patches. Such sets of samples are riddled with
redundancies -- any overlapping pixels are constrained to be the same. Based on
this simple observation, we propose an analytic model for datasets of thousands
of translated patches. By showing that the resulting data matrix is circulant,
we can diagonalize it with the Discrete Fourier Transform, reducing both
storage and computation by several orders of magnitude. Interestingly, for
linear regression our formulation is equivalent to a correlation filter, used
by some of the fastest competitive trackers. For kernel regression, however, we
derive a new Kernelized Correlation Filter (KCF), that unlike other kernel
algorithms has the exact same complexity as its linear counterpart. Building on
it, we also propose a fast multi-channel extension of linear correlation
filters, via a linear kernel, which we call Dual Correlation Filter (DCF). Both
KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50
videos benchmark, despite running at hundreds of frames-per-second, and being
implemented in a few lines of code (Algorithm 1). To encourage further
developments, our tracking framework was made open-source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7586</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7586</id><created>2014-04-30</created><authors><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Detection in Analog Sensor Networks with a Large Scale Antenna Fusion
  Center</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, accepted by the 8th IEEE Sensor Array and
  Multichannel Signal Processing Workshop (SAM), Apr. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the distributed detection of a zero-mean Gaussian signal in an
analog wireless sensor network with a fusion center (FC) configured with a
large number of antennas. The transmission gains of the sensor nodes are
optimized by minimizing the ratio of the log probability of detection (PD) and
log probability of false alarm (PFA). We show that the problem is convex with
respect to the squared norm of the transmission gains, and that a closed-form
solution can be found using the Karush-Kuhn-Tucker conditions. Our results
indicate that a constant PD can be maintained with decreasing sensor transmit
gain provided that the number of antennas increases at the same rate. This is
contrasted with the case of a single-antenna FC, where PD is monotonically
decreasing with transmit gain. On the other hand, we show that when the
transmit power is high, the single- and multi-antenna FC both asymptotically
achieve the same PD upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7592</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7592</id><created>2014-04-30</created><authors><author><keyname>Grosek</keyname><forenames>Jacob</forenames></author><author><keyname>Kutz</keyname><forenames>J. Nathan</forenames></author></authors><title>Dynamic Mode Decomposition for Real-Time Background/Foreground
  Separation in Video</title><categories>cs.CV</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the method of dynamic mode decomposition (DMD) for
robustly separating video frames into background (low-rank) and foreground
(sparse) components in real-time. The method is a novel application of a
technique used for characterizing nonlinear dynamical systems in an
equation-free manner by decomposing the state of the system into low-rank terms
whose Fourier components in time are known. DMD terms with Fourier frequencies
near the origin (zero-modes) are interpreted as background (low-rank) portions
of the given video frames, and the terms with Fourier frequencies bounded away
from the origin are their sparse counterparts. An approximate low-rank/sparse
separation is achieved at the computational cost of just one singular value
decomposition and one linear equation solve, thus producing results orders of
magnitude faster than a leading separation method, namely robust principal
component analysis (RPCA). The DMD method that is developed here is
demonstrated to work robustly in real-time with personal laptop-class computing
power and without any parameter tuning, which is a transformative improvement
in performance that is ideal for video surveillance and recognition
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7594</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7594</id><created>2014-04-30</created><authors><author><keyname>Grosek</keyname><forenames>Jacob</forenames></author><author><keyname>Kutz</keyname><forenames>J. Nathan</forenames></author></authors><title>Selecting a Small Set of Optimal Gestures from an Extensive Lexicon</title><categories>cs.CV</categories><comments>27 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the best set of gestures to use for a given computer recognition
problem is an essential part of optimizing the recognition performance while
being mindful to those who may articulate the gestures. An objective function,
called the ellipsoidal distance ratio metric (EDRM), for determining the best
gestures from a larger lexicon library is presented, along with a numerical
method for incorporating subjective preferences. In particular, we demonstrate
an efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$
gestures where typically $n \ll m$ using a weighting of both subjective and
objective measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7604</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7604</id><created>2014-04-30</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Exact random coding error exponents of optimal bin index decoding</title><categories>cs.IT math.IT</categories><comments>19 pages; submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider ensembles of channel codes that are partitioned into bins, and
focus on analysis of exact random coding error exponents associated with
optimum decoding of the index of the bin to which the transmitted codeword
belongs. Two main conclusions arise from this analysis: (i) for independent
random selection of codewords within a given type class, the random coding
exponent of optimal bin index decoding is given by the ordinary random coding
exponent function, computed at the rate of the entire code, independently of
the exponential rate of the size of the bin. (ii) for this ensemble of codes,
sub-optimal bin index decoding, that is based on ordinary maximum likelihood
(ML) decoding, is as good as the optimal bin index decoding in terms of the
random coding error exponent achieved. Finally, for the sake of completeness,
we also outline how our analysis of exact random coding exponents extends to
the hierarchical ensemble that correspond to superposition coding and optimal
decoding, where for each bin, first, a cloud center is drawn at random, and
then the codewords of this bin are drawn conditionally independently given the
cloud center. For this ensemble, conclusions (i) and (ii), mentioned above, no
longer hold necessarily in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7608</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7608</id><created>2014-04-30</created><authors><author><keyname>Kunszt</keyname><forenames>Peter</forenames></author><author><keyname>Maffioletti</keyname><forenames>Sergio</forenames></author><author><keyname>Flanders</keyname><forenames>Dean</forenames></author><author><keyname>Eurich</keyname><forenames>Markus</forenames></author><author><keyname>Bohnert</keyname><forenames>Thomas</forenames></author><author><keyname>Edmonds</keyname><forenames>Andrew</forenames></author><author><keyname>Stockinger</keyname><forenames>Heinz</forenames></author><author><keyname>Haug</keyname><forenames>Sigve</forenames></author><author><keyname>Jamakovic-Kapic</keyname><forenames>Almerina</forenames></author><author><keyname>Flury</keyname><forenames>Placi</forenames></author><author><keyname>Leinen</keyname><forenames>Simon</forenames></author><author><keyname>Schiller</keyname><forenames>Eryk</forenames></author></authors><title>Towards a Swiss National Research Infrastructure</title><categories>cs.CY</categories><journal-ref>Euro-Par 2013: Parallel Processing Workshops. Springer Berlin
  Heidelberg, 2014. p. 157-166</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper we describe the current status and plans for a Swiss
National Research Infrastructure. Swiss academic and research institutions are
very autonomous. While being loosely coupled, they do not rely on any
centralized management entities. Therefore, a coordinated national research
infrastructure can only be established by federating the various resources
available locally at the individual institutions. The Swiss Multi-Science
Computing Grid and the Swiss Academic Compute Cloud projects serve already a
large number of diverse user communities. These projects also allow us to test
the operational setup of such a heterogeneous federated infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7610</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7610</id><created>2014-04-30</created><authors><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author><author><keyname>Satoh</keyname><forenames>Hiroko</forenames></author></authors><title>An Efficient Algorithm for Enumerating Chordless Cycles and Chordless
  Paths</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A chordless cycle (induced cycle) $C$ of a graph is a cycle without any
chord, meaning that there is no edge outside the cycle connecting two vertices
of the cycle. A chordless path is defined similarly. In this paper, we consider
the problems of enumerating chordless cycles/paths of a given graph $G=(V,E),$
and propose algorithms taking $O(|E|)$ time for each chordless cycle/path. In
the existing studies, the problems had not been deeply studied in the
theoretical computer science area, and no output polynomial time algorithm has
been proposed. Our experiments showed that the computation time of our
algorithms is constant per chordless cycle/path for non-dense random graphs and
real-world graphs. They also show that the number of chordless cycles is much
smaller than the number of cycles. We applied the algorithm to prediction of
NMR (Nuclear Magnetic Resonance) spectra, and increased the accuracy of the
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7618</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7618</id><created>2014-04-30</created><updated>2014-05-06</updated><authors><author><keyname>Singer</keyname><forenames>Robert</forenames></author><author><keyname>Kotremba</keyname><forenames>Johannes</forenames></author><author><keyname>Ra&#xdf;</keyname><forenames>Stefan</forenames></author></authors><title>Modeling and Execution of Multienterprise Business Processes</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.2737</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a fully featured multienterprise business process plattform
(ME-BPP) based on the concepts of agent-based business processes. Using the
concepts of the subject-oriented business process (S-BPM) methodology we
developed an architecture to realize a platform for the execution of
distributed business processes. The platform is implemented based on cloud
technology using commercial services. For our discussion we used the well known
Service Interaction Patterns, as they are empirically developed from typical
business-to-business interactions. We can demonstrate that all patterns can be
easily modeled and executed based on our architecture. We propose therefore a
change from a control flow based to an agent based view to model and enact
business processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7623</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7623</id><created>2014-04-30</created><authors><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author></authors><title>The stable set polytope of ($P_6$,triangle)-free graphs and new
  facet-inducing graphs</title><categories>cs.DM math.CO</categories><msc-class>52B12 (05C75)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable set polytope of a graph $G$, denoted as STAB($G$), is the convex
hull of all the incidence vectors of stable sets of $G$. To describe a linear
system which defines STAB($G$) seems to be a difficult task in the general
case. In this paper we present a complete description of the stable set
polytope of ($P_6$,triangle)-free graphs (and more generally of
($P_6$,paw)-free graphs). For that we combine different tools, in the context
of a well known result of Chv\'atal \cite{Chvatal1975} which allows to focus
just on prime facet-inducing graphs, with particular reference to a structure
result on prime ($P_6$,triangle)-free graphs due to Brandst\&quot;adt et al.
\cite{BraKleMah2005}. Also we point out some peculiarities of new
facet-inducing graphs detected along this study with the help of a software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7634</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7634</id><created>2014-04-30</created><updated>2014-08-05</updated><authors><author><keyname>Barjon</keyname><forenames>Matthieu</forenames></author><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Chaumette</keyname><forenames>Serge</forenames></author><author><keyname>Johnen</keyname><forenames>Colette</forenames></author><author><keyname>Neggaz</keyname><forenames>Yessin M.</forenames></author></authors><title>Testing Temporal Connectivity in Sparse Dynamic Graphs</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of testing whether a given dynamic graph is temporally
connected, {\it i.e} a temporal path (also called a {\em journey}) exists
between all pairs of vertices. We consider a discrete version of the problem,
where the topology is given as an evolving graph ${\cal
G}=\{G_1,G_2,...,G_{k}\}$ whose set of vertices is invariant and the set of
(directed) edges varies over time. Two cases are studied, depending on whether
a single edge or an unlimited number of edges can be crossed in a same $G_i$
(strict journeys {\it vs} non-strict journeys).
  In the case of {\em strict} journeys, a number of existing algorithms
designed for more general problems can be adapted. We adapt one of them to the
above formulation of the problem and characterize its running time complexity.
The parameters of interest are the length of the graph sequence $k=|{\cal G}|$,
the maximum {\em instant} density $\mu=max(|E_i|)$, and the {\em cumulated}
density $m=|\cup E_i|$. Our algorithm has a time complexity of $O(k\mu n)$,
where $n$ is the number of nodes. This complexity is compared to that of the
other solutions: one is always more costly (keep in mind that is solves a more
general problem), the other one is more or less costly depending on the
interplay between instant density and cumulated density. The length $k$ of the
sequence also plays a role. We characterize the key values of $k, \mu$ and $m$
for which either algorithm should be used.
  In the case of {\em non-strict} journeys, for which no algorithm is known, we
show that some pre-processing of the input graph allows us to re-use the same
algorithm than before. By chance, these operations happens to cost again
$O(k\mu n)$ time, which implies that the second problem is not more difficult
than the first.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7638</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7638</id><created>2014-04-30</created><authors><author><keyname>Divakaran</keyname><forenames>Srikrishnan</forenames></author></authors><title>An Optimal Offline Algorithm for List Update</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the static list update problem, given an ordered list $\rho_0$ (an
ordering of the list $L$ = \{ $a_a, a_2, ..., a_l$ \}), and a sequence $\sigma
= (\sigma_1, \sigma_2, ..., \sigma_m)$ of requests for items in $L$, we
characterize the list reorganizations in an optimal offline solution in terms
of an initial permutation of the list followed by a sequence of $m$ {\em
element transfers}, where an element transfer is a type of list reorganization
where only the requested item can be moved. Then we make use of this
characterization to design an $O(l^{2} (l-1)!m)$ time optimal offline
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7640</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7640</id><created>2014-04-30</created><updated>2015-07-27</updated><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Distributed Quantization for Measurement of Correlated Sparse Sources
  over Noisy Channels</title><categories>cs.IT math.IT</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design and analyze distributed vector quantization (VQ) for
compressed measurements of correlated sparse sources over noisy channels.
Inspired by the framework of compressed sensing (CS) for acquiring compressed
measurements of the sparse sources, we develop optimized quantization schemes
that enable distributed encoding and transmission of CS measurements over noisy
channels followed by joint decoding at a decoder. The optimality is addressed
with respect to minimizing the sum of mean-square error (MSE) distortions
between the sparse sources and their reconstruction vectors at the decoder. We
propose a VQ encoder-decoder design via an iterative algorithm, and derive a
lower-bound on the end-to-end MSE of the studied distributed system. Through
several simulation studies, we evaluate the performance of the proposed
distributed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7643</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7643</id><created>2014-04-30</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Performance Bounds for Vector Quantized Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, Published in 2012 International Symposium on
  Information Theory and its Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we endeavor for predicting the performance of quantized
compressive sensing under the use of sparse reconstruction estimators. We
assume that a high rate vector quantizer is used to encode the noisy
compressive sensing measurement vector. Exploiting a block sparse source model,
we use Gaussian mixture density for modeling the distribution of the source.
This allows us to formulate an optimal rate allocation problem for the vector
quantizer. Considering noisy CS quantized measurements, we analyze upper- and
lower-bounds on reconstruction error performance guarantee of two estimators -
convex relaxation based basis pursuit de-noising estimator and an
oracle-assisted least-squares estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7646</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7646</id><created>2014-04-30</created><authors><author><keyname>Gordienko</keyname><forenames>Yuri G.</forenames></author></authors><title>Migration-Driven Hierarchical Crystal Defect Aggregation - Symmetry and
  Scaling Analysis</title><categories>cond-mat.stat-mech cs.CE physics.comp-ph</categories><comments>8 pages, 5 figures; Proc. 17th European Conference on Fracture -
  ECF17 (Czech Republic, Brno, 2-5 September 2008), 249-256. $D_2(n)$ was
  corrected later in https://arxiv.org/abs/1104.5381</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently the hierarchical defect substructures were observed experimentally
in several metals and alloys before and after fracture. The general models of
crystal defect aggregation with appearance hierarchical defect substructures
are proposed and considered in the wide range of scales. Their general group
analysis is performed, and symmetries of the governing equations are
identified. The models of defect aggregate growth are considered for several
partial cases and compared with classical Lifshitz-Slyozov-Wagner theory of
coarsening, Leyvraz-Redner scaling theory of aggregate growth, etc. The reduced
equations of new models are generated and solved, and the general scaling
solutions are given. The results obtained are illustrated by preliminary
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7648</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7648</id><created>2014-04-30</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Channel-Optimized Vector Quantizer Design for Compressed Sensing
  Measurements</title><categories>cs.IT math.IT</categories><comments>Published in ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider vector-quantized (VQ) transmission of compressed sensing (CS)
measurements over noisy channels. Adopting mean-square error (MSE) criterion to
measure the distortion between a sparse vector and its reconstruction, we
derive channel-optimized quantization principles for encoding CS measurement
vector and reconstructing sparse source vector. The resulting necessary optimal
conditions are used to develop an algorithm for training channel-optimized
vector quantization (COVQ) of CS measurements by taking the end-to-end
distortion measure into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7651</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7651</id><created>2014-04-30</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Analysis-by-Synthesis-based Quantization of Compressed Sensing
  Measurements</title><categories>cs.IT math.IT</categories><comments>5 pages, Published in ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a resource-constrained scenario where a compressed sensing- (CS)
based sensor has a low number of measurements which are quantized at a low rate
followed by transmission or storage. Applying this scenario, we develop a new
quantizer design which aims to attain a high-quality reconstruction performance
of a sparse source signal based on analysis-by-synthesis framework. Through
simulations, we compare the performance of the proposed quantization algorithm
vis-a-vis existing quantization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7659</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7659</id><created>2014-04-30</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Analysis-by-Synthesis Quantization for Compressed Sensing Measurements</title><categories>cs.IT math.IT</categories><comments>12 Pages, Published in IEEE Transactions on Signal Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 22, pp.
  5789-5800, November 2013</journal-ref><doi>10.1109/TSP.2013.2280445</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a resource-limited scenario where a sensor that uses compressed
sensing (CS) collects a low number of measurements in order to observe a sparse
signal, and the measurements are subsequently quantized at a low bit-rate
followed by transmission or storage. For such a scenario, we design new
algorithms for source coding with the objective of achieving good
reconstruction performance of the sparse signal. Our approach is based on an
analysis-by-synthesis principle at the encoder, consisting of two main steps:
(1) the synthesis step uses a sparse signal reconstruction technique for
measuring the direct effect of quantization of CS measurements on the final
sparse signal reconstruction quality, and (2) the analysis step decides
appropriate quantized values to maximize the final sparse signal reconstruction
quality. Through simulations, we compare the performance of the proposed
quantization algorithms vis-a-vis existing quantization schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7665</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7665</id><created>2014-04-30</created><updated>2015-01-12</updated><authors><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Cortesi</keyname><forenames>Fabrizio L.</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>On Submodularity and Controllability in Complex Dynamical Networks</title><categories>math.OC cs.SY</categories><comments>submitted to IEEE Transactions on Control of Network Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controllability and observability have long been recognized as fundamental
structural properties of dynamical systems, but have recently seen renewed
interest in the context of large, complex networks of dynamical systems. A
basic problem is sensor and actuator placement: choose a subset from a finite
set of possible placements to optimize some real-valued controllability and
observability metrics of the network. Surprisingly little is known about the
structure of such combinatorial optimization problems. In this paper, we show
that several important classes of metrics based on the controllability and
observability Gramians have a strong structural property that allows for either
efficient global optimization or an approximation guarantee by using a simple
greedy heuristic for their maximization. In particular, the mapping from
possible placements to several scalar functions of the associated Gramian is
either a modular or submodular set function. The results are illustrated on
randomly generated systems and on a problem of power electronic actuator
placement in a model of the European power grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7666</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7666</id><created>2014-04-30</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Distributed Quantization for Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>5 Pages, Accepted for presentation in ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed coding of compressed sensing (CS) measurements using
vector quantizer (VQ). We develop a distributed framework for realizing
optimized quantizer that enables encoding CS measurements of correlated sparse
sources followed by joint decoding at a fusion center. The optimality of VQ
encoder-decoder pairs is addressed by minimizing the sum of mean-square errors
between the sparse sources and their reconstruction vectors at the fusion
center. We derive a lower-bound on the end-to-end performance of the studied
distributed system, and propose a practical encoder-decoder design through an
iterative algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7671</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7671</id><created>2014-04-30</created><authors><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Nikoletseas</keyname><forenames>Sotiris E.</forenames></author><author><keyname>Raptopoulos</keyname><forenames>Christoforos L.</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>Determining Majority in Networks with Local Interactions and very Small
  Local Memory</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study here the problem of determining the majority type in an arbitrary
connected network, each vertex of which has initially two possible types. The
vertices may have a few additional possible states and can interact in pairs
only if they share an edge. Any (population) protocol is required to stabilize
in the initial majority. We first present and analyze a protocol with 4 states
per vertex that always computes the initial majority value, under any fair
scheduler. As we prove, this protocol is optimal, in the sense that there is no
population protocol that always computes majority with fewer than 4 states per
vertex. However this does not rule out the existence of a protocol with 3
states per vertex that is correct with high probability. To this end, we
examine a very natural majority protocol with 3 states per vertex, introduced
in [Angluin et al. 2008] where its performance has been analyzed for the clique
graph. We study the performance of this protocol in arbitrary networks. We
prove that, when the two initial states are put uniformly at random on the
vertices, this protocol converges to the initial majority with probability
higher than the probability of converging to the initial minority. In contrast,
we present an infinite family of graphs, on which the protocol can fail whp,
even when the difference between the initial majority and the initial minority
is $n - \Theta(\ln{n})$. We also present another infinite family of graphs in
which the protocol of Angluin et al. takes an expected exponential time to
converge. These two negative results build upon a very positive result
concerning the robustness of the protocol on the clique. Surprisingly, the
resistance of the clique to failure causes the failure in general graphs. Our
techniques use new domination and coupling arguments for suitably defined
processes whose dynamics capture the antagonism between the states involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7688</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7688</id><created>2014-04-30</created><authors><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author><author><keyname>Filippone</keyname><forenames>Maurizio</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author><author><keyname>Roudier</keyname><forenames>Yves</forenames></author></authors><title>On User Availability Prediction and Network Applications</title><categories>cs.DC</categories><comments>Accepted for publication in IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User connectivity patterns in network applications are known to be
heterogeneous, and to follow periodic (daily and weekly) patterns. In many
cases, the regularity and the correlation of those patterns is problematic: for
network applications, many connected users create peaks of demand; in contrast,
in peer-to-peer scenarios, having few users online results in a scarcity of
available resources. On the other hand, since connectivity patterns exhibit a
periodic behavior, they are to some extent predictable. This work shows how
this can be exploited to anticipate future user connectivity and to have
applications proactively responding to it. We evaluate the probability that any
given user will be online at any given time, and assess the prediction on
six-month availability traces from three different Internet applications.
Building upon this, we show how our probabilistic approach makes it easy to
evaluate and optimize the performance in a number of diverse network
application models, and to use them to optimize systems. In particular, we show
how this approach can be used in distributed hash tables, friend-to-friend
storage, and cache pre-loading for social networks, resulting in substantial
gains in data availability and system efficiency at negligible costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7694</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7694</id><created>2014-04-30</created><authors><author><keyname>Jozsa</keyname><forenames>Richard</forenames></author><author><keyname>Mitchison</keyname><forenames>Graeme</forenames></author></authors><title>Symmetric polynomials in information theory: entropy and subentropy</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>20 pages. Supersedes arXiv:1310.6629</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy and other fundamental quantities of information theory are
customarily expressed and manipulated as functions of probabilities. Here we
study the entropy H and subentropy Q as functions of the elementary symmetric
polynomials in the probabilities, and reveal a series of remarkable properties.
Derivatives of all orders are shown to satisfy a complete monotonicity
property. H and Q themselves become multivariate Bernstein functions and we
derive the density functions of their Levy-Khintchine representations. We also
show that H and Q are Pick functions in each symmetric polynomial variable
separately. Furthermore we see that H and the intrinsically quantum
informational quantity Q become surprisingly closely related in functional
form, suggesting a special significance for the symmetric polynomials in
quantum information theory. Using the symmetric polynomials we also derive a
series of further properties of H and Q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7695</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7695</id><created>2014-04-30</created><authors><author><keyname>Fuhs</keyname><forenames>Carsten</forenames></author><author><keyname>Kop</keyname><forenames>Cynthia</forenames></author></authors><title>First-Order Formative Rules</title><categories>cs.LO</categories><comments>Extended version of a paper which is to appear in the proceedings of
  RTA-TLCA 2014 (Joint 25th International Conference on Rewriting Techniques
  and Applications and 12th International Conference on Typed Lambda Calculi
  and Applications)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the method of formative rules for first-order term
rewriting, which was previously defined for a higher-order setting. Dual to the
well-known usable rules, formative rules allow dropping some of the term
constraints that need to be solved during a termination proof. Compared to the
higher-order definition, the first-order setting allows for significant
improvements of the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7703</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7703</id><created>2014-04-30</created><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Neiman</keyname><forenames>Ofer</forenames></author><author><keyname>Solomon</keyname><forenames>Shay</forenames></author></authors><title>Light Spanners</title><categories>cs.DS</categories><comments>10 pages, 1 figure, to appear in ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $t$-spanner of a weighted undirected graph $G=(V,E)$, is a subgraph $H$
such that $d_H(u,v)\le t\cdot d_G(u,v)$ for all $u,v\in V$. The sparseness of
the spanner can be measured by its size (the number of edges) and weight (the
sum of all edge weights), both being important measures of the spanner's
quality -- in this work we focus on the latter.
  Specifically, it is shown that for any parameters $k\ge 1$ and $\epsilon&gt;0$,
any weighted graph $G$ on $n$ vertices admits a
$(2k-1)\cdot(1+\epsilon)$-stretch spanner of weight at most $w(MST(G))\cdot
O_\epsilon(kn^{1/k}/\log k)$, where $w(MST(G))$ is the weight of a minimum
spanning tree of $G$. Our result is obtained via a novel analysis of the
classic greedy algorithm, and improves previous work by a factor of $O(\log
k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7711</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7711</id><created>2014-04-30</created><updated>2014-11-14</updated><authors><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author><author><keyname>Garin</keyname><forenames>Federica</forenames></author><author><keyname>Gerencser</keyname><forenames>Balazs</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author></authors><title>Optimal one-dimensional coverage by unreliable sensors</title><categories>math.OC cs.MA</categories><comments>21 pages 2 figures</comments><msc-class>60K30, 90B18, 68M10, 49N99, 68R01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper regards the problem of optimally placing unreliable sensors in a
one-dimensional environment. We assume that sensors can fail with a certain
probability and we minimize the expected maximum distance from any point in the
environment to the closest active sensor. We provide a computational method to
find the optimal placement and we estimate the relative quality of equispaced
and random placements. We prove that the former is asymptotically equivalent to
the optimal placement when the number of sensors goes to infinity, with a cost
ratio converging to 1, while the cost of the latter remains strictly larger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7717</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7717</id><created>2014-04-30</created><updated>2014-07-17</updated><authors><author><keyname>Federici</keyname><forenames>Mizar Luca</forenames></author><author><keyname>Manenti</keyname><forenames>Lorenza</forenames></author><author><keyname>Manzoni</keyname><forenames>Sara</forenames></author></authors><title>A Checklist for the Evaluation of Pedestrian Simulation Software
  Functionalities</title><categories>cs.MA</categories><comments>20 pages, 3 figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The employment of micro-simulation (agent-based) tools in the phase of design
of public and private spaces and facilities and for the definition of transport
schemes that impact on pedestrian flows, thanks to their achieved accuracy and
predictive capacity, has become a consolidated practice. These instruments
provide support to the organization of spaces, services and facilities and to
the definition of management procedures for normal and emergency situations.
The employment of these tools is effective for various but not for all the
contexts, nevertheless new features and functions are under constant
development and new products are often launched on the market. Therefore, there
is a higher necessity of a standard criteria both for the evaluation of the
kinds of function that these software provide, at use of practitioners and
end-users, and for the definition of software requirements as a reference for
the developers that aim at being competitive on this market.
  On the basis of our experience as pedestrian modellers and as researchers in
the crowd modelling area, we designed a comprehensive and detailed ready-to-use
checklist for the quantitative evaluation of Pedestrian Simulation Software
functionalities that aims at capturing all the aspects that we claim that are
useful to undertake a professional study. These functions in our opinion are
necessary to provide accurate results in the planning of new facilities or
schemes that involve pedestrian activities. With this work we propose a set of
criteria of evaluation for these products also to encourage a debate for the
definition of objective standards for pedestrian simulation software
certification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7719</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7719</id><created>2014-04-30</created><authors><author><keyname>Qiao</keyname><forenames>Wenzhao</forenames></author><author><keyname>Roos</keyname><forenames>Nico</forenames></author></authors><title>An argumentation system for reasoning with conflict-minimal
  paraconsistent ALC</title><categories>cs.AI</categories><journal-ref>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic web is an open and distributed environment in which it is hard
to guarantee consistency of knowledge and information. Under the standard
two-valued semantics everything is entailed if knowledge and information is
inconsistent. The semantics of the paraconsistent logic LP offers a solution.
However, if the available knowledge and information is consistent, the set of
conclusions entailed under the three-valued semantics of the paraconsistent
logic LP is smaller than the set of conclusions entailed under the two-valued
semantics. Preferring conflict-minimal three-valued interpretations eliminates
this difference.
  Preferring conflict-minimal interpretations introduces non-monotonicity. To
handle the non-monotonicity, this paper proposes an assumption-based
argumentation system. Assumptions needed to close branches of a semantic
tableaux form the arguments. Stable extensions of the set of derived arguments
correspond to conflict minimal interpretations and conclusions entailed by all
conflict-minimal interpretations are supported by arguments in all stable
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7734</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7734</id><created>2014-04-30</created><authors><author><keyname>Baumann</keyname><forenames>Ringo</forenames></author><author><keyname>Dvor&#xe1;k</keyname><forenames>Wolfgang</forenames></author><author><keyname>Linsbichler</keyname><forenames>Thomas</forenames></author><author><keyname>Strass</keyname><forenames>Hannes</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Compact Argumentation Frameworks</title><categories>cs.AI</categories><comments>Contribution to the 15th International Workshop on Non-Monotonic
  Reasoning, 2014, Vienna</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract argumentation frameworks (AFs) are one of the most studied
formalisms in AI. In this work, we introduce a certain subclass of AFs which we
call compact. Given an extension-based semantics, the corresponding compact AFs
are characterized by the feature that each argument of the AF occurs in at
least one extension. This not only guarantees a certain notion of fairness;
compact AFs are thus also minimal in the sense that no argument can be removed
without changing the outcome. We address the following questions in the paper:
(1) How are the classes of compact AFs related for different semantics? (2)
Under which circumstances can AFs be transformed into equivalent compact ones?
(3) Finally, we show that compact AFs are indeed a non-trivial subclass, since
the verification problem remains coNP-hard for certain semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7736</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7736</id><created>2014-04-30</created><authors><author><keyname>Risi</keyname><forenames>Chiara</forenames></author><author><keyname>Persson</keyname><forenames>Daniel</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Massive MIMO with 1-bit ADC</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate massive multiple-input-multiple output (MIMO) uplink systems
with 1-bit analog-to-digital converters (ADCs) on each receiver antenna.
Receivers that rely on 1-bit ADC do not need energy-consuming interfaces such
as automatic gain control (AGC). This decreases both ADC building and
operational costs. Our design is based on maximal ratio combining (MRC),
zero-forcing (ZF), and least squares (LS) detection, taking into account the
effects of the 1-bit ADC on channel estimation. Through numerical results, we
show good performance of the system in terms of mutual information and symbol
error rate (SER). Furthermore, we provide an analytical approach to calculate
the mutual information and SER of the MRC receiver. The analytical approach
reduces complexity in the sense that a symbol and channel noise vectors Monte
Carlo simulation is avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7739</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7739</id><created>2014-04-30</created><updated>2015-04-12</updated><authors><author><keyname>Ben-Sasson</keyname><forenames>Eli</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author><author><keyname>Gabizon</keyname><forenames>Ariel</forenames></author><author><keyname>Raviv</keyname><forenames>Netanel</forenames></author></authors><title>Subspace Polynomials and Cyclic Subspace Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace codes have received an increasing interest recently due to their
application in error-correction for random network coding. In particular,
cyclic subspace codes are possible candidates for large codes with efficient
encoding and decoding algorithms. In this paper we consider such cyclic codes
and provide constructions of optimal codes for which their codewords do not
have full orbits. We further introduce a new way to represent subspace codes by
a class of polynomials called subspace polynomials. We present some
constructions of such codes which are cyclic and analyze their parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7748</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7748</id><created>2014-04-30</created><authors><author><keyname>Najman</keyname><forenames>Laurent</forenames></author><author><keyname>Cousty</keyname><forenames>Jean</forenames></author></authors><title>A graph-based mathematical morphology reader</title><categories>cs.CV</categories><journal-ref>Pattern Recognition Letters 47 (2014) 3-17</journal-ref><doi>10.1016/j.patrec.2014.05.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey paper aims at providing a &quot;literary&quot; anthology of mathematical
morphology on graphs. It describes in the English language many ideas stemming
from a large number of different papers, hence providing a unified view of an
active and diverse field of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7751</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7751</id><created>2014-04-30</created><authors><author><keyname>Poluru</keyname><forenames>Ravi Kumar</forenames></author><author><keyname>Reddy</keyname><forenames>T. Sunil Kumar</forenames></author><author><keyname>Nagaraju</keyname><forenames>D.</forenames></author></authors><title>Updating the Routing Position in Adhoc Networks by using Adaptive
  Position</title><categories>cs.NI</categories><comments>7 PAGES, 5 FIGURES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In geographic routing, nodes to their instantaneous neighbour call for to
maintain up-to-date positions for making successful forwarding decisions. The
geographic location coordinates of the nodes by the periodic broadcasting of
beacon packets is a trendy method used by the majority geographic routing
protocols to preserve neighbour positions. We contend and display that periodic
beaconing apart from of the node mobility and traffic patterns in the network
is not nice-looking from both update cost and routing recital points of view.
We recommend the Adaptive Position Update (APU) strategy for geographic
routing, which enthusiastically adjusts the occurrence of position updates
based on the mobility dynamics of the nodes and the forwarding patterns in the
system. APU is based on two easy principles: 1) nodes whose arrangements are
harder to guess update their positions more recurrently (and vice versa), and
(ii) nodes faster to forwarding paths update their positions more recurrently
(and vice versa). Our speculative analysis, which is validated by NS2
simulations of a well-known geographic routing procedure, Greedy Perimeter
Stateless Routing Protocol (GPSR),shows that APU can drastically reduce the
update cost and pick up the routing performance in terms of packet delivery
ratio and regular end-to-end delay in assessment with periodic beaconing and
other recently proposed updating schemes. The benefits of APU are additional
confirmed by responsibility evaluations in realistic network scenarios, which
account for localization error, practical radio propagation, and sparse system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7753</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7753</id><created>2014-04-30</created><updated>2014-06-02</updated><authors><author><keyname>Poss</keyname><forenames>Raphael</forenames></author><author><keyname>Altmeyer</keyname><forenames>Sebastian</forenames></author><author><keyname>Thompson</keyname><forenames>Mark</forenames></author><author><keyname>Jelier</keyname><forenames>Rob</forenames></author></authors><title>Aca 2.0: Questions and Answers</title><categories>cs.DL</categories><comments>18 pages, 2 tables, comments, suggestions and further contributions
  welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Academia 2.0&quot; is a proposal to organize scientific publishing around true
peer-to-peer distributed dissemination channels and eliminate the traditional
role of the academic publisher. This model will be first presented at the 2014
workshop on Reproducible Research Methodologies and New Publication Models in
Computer Engineering (TRUST'14) in the form of a high-level overview, so as to
stimulate discussion and gather feedback on its merits and feasibility. This
report complements the 6-page introductory article presented at TRUST, by
detailing the review processes, some use scenarios and answering the reviewer's
comments in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7758</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7758</id><created>2014-04-30</created><authors><author><keyname>S&#xe6;ther</keyname><forenames>Sigve Hortemo</forenames></author><author><keyname>Telle</keyname><forenames>Jan Arne</forenames></author></authors><title>Between Treewidth and Clique-width</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many hard graph problems can be solved efficiently when restricted to graphs
of bounded treewidth, and more generally to graphs of bounded clique-width. But
there is a price to be paid for this generality, exemplified by the four
problems MaxCut, Graph Coloring, Hamiltonian Cycle and Edge Dominating Set that
are all FPT parameterized by treewidth but none of which can be FPT
parameterized by clique-width unless FPT = W[1], as shown by Fomin et al [7,
8]. We therefore seek a structural graph parameter that shares some of the
generality of clique-width without paying this price. Based on splits, branch
decompositions and the work of Vatshelle [18] on Maximum Matching-width, we
consider the graph parameter sm-width which lies between treewidth and
clique-width. Some graph classes of unbounded treewidth, like
distance-hereditary graphs, have bounded sm-width. We show that MaxCut, Graph
Coloring, Hamiltonian Cycle and Edge Dominating Set are all FPT parameterized
by sm-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7760</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7760</id><created>2014-04-30</created><authors><author><keyname>Zeng</keyname><forenames>Wen</forenames></author><author><keyname>Mu</keyname><forenames>Chunyan</forenames></author><author><keyname>Koutny</keyname><forenames>Maciej</forenames></author><author><keyname>Watson</keyname><forenames>Paul</forenames></author></authors><title>A Flow Sensitive Security Model for Cloud Computing Systems</title><categories>cs.CR cs.DC</categories><comments>EDCC-2014, EDSoS-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extent and importance of cloud computing is rapidly increasing due to the
ever increasing demand for internet services and communications. Instead of
building individual information technology infrastructure to host databases or
software, a third party can host them in its large server clouds. Large
organizations may wish to keep sensitive information on their more restricted
servers rather than in the public cloud. This has led to the introduction of
federated cloud computing (FCC) in which both public and private cloud
computing resources are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7761</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7761</id><created>2014-04-29</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>Building Effective Virtual Teams: How to Overcome the Problems of Trust
  and Identity in Virtual Teams</title><categories>cs.CY</categories><journal-ref>Building effective virtual teams: How to overcome the problems of
  trust and identity in virtual teams. Global Business and Organizational
  Excellence, 30(2), 2011, pp. 6-15</journal-ref><doi>10.1002/joe.20364</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This article explores some of the challenges faced when managing virtual
teams, in particular the role played by trust and identity in virtual teams. It
outlines why teams and virtual teams have become a valuable part of the modern
organization and presents ten short case studies that illustrate the range of
activities in which virtual teams can be found. Following this, the article
examines some of the common problems encountered in virtual team working. It
discusses two broad classes of solutions. The first are solutions that are
essentially technical in nature (i.e., where changes to or improvements in
technology would help to solve or ameliorate the problem); the second are more
organizationally based (i.e., where the root of the problem is in people and
how they are managed). The article concludes that both the technical and the
organizational solutions need to be considered in parallel if an attempt to
build an effective virtual team is to be successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7763</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7763</id><created>2014-04-30</created><updated>2014-05-07</updated><authors><author><keyname>Becker</keyname><forenames>Klaus</forenames></author><author><keyname>Schatz</keyname><forenames>Bernhard</forenames></author><author><keyname>Buckl</keyname><forenames>Christian</forenames></author><author><keyname>Armbruster</keyname><forenames>Michael</forenames></author></authors><title>Deployment Calculation and Analysis for a Fail-Operational Automotive
  Platform</title><categories>cs.SE</categories><comments>EDCC-2014, EDSoS-2014, Fault-Tolerance, Fail-Operational, Deployment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In domains like automotive, safety-critical features are increasingly
realized by software. Some features might even require fail-operational
behavior, so that they must be provided even in the presence of random hardware
failures. A new fault-tolerant SW/HW architecture for electric vehicles
provides inherent safety capabilities that enable fail-operational features. In
this paper we introduce a formal model of this architecture and an approach to
calculate valid deployments of mixed-critical software-components to the
execution nodes, while ensuring fail-operational behavior of certain
components. Calculated redeployments cover the cases in which faulty execution
nodes have to be isolated. This allows to formally analyze which set of
features can be provided under decreasing available execution resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7765</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7765</id><created>2014-04-30</created><updated>2014-07-14</updated><authors><author><keyname>Baydin</keyname><forenames>Atilim Gunes</forenames></author><author><keyname>de Mantaras</keyname><forenames>Ramon Lopez</forenames></author><author><keyname>Ontanon</keyname><forenames>Santiago</forenames></author></authors><title>A semantic network-based evolutionary algorithm for computational
  creativity</title><categories>cs.NE</categories><comments>20 pages, 14 figures, revision after reviews, changed title</comments><msc-class>92D15, 91E40, 68T20, 68T30</msc-class><acm-class>I.2.4; I.2.6; G.1.6; J.4; J.3</acm-class><journal-ref>Evolutionary Intelligence, 8(1):3-21 (2015)</journal-ref><doi>10.1007/s12065-014-0119-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel evolutionary algorithm (EA) with a semantic
network-based representation. For enabling this, we establish new formulations
of EA variation operators, crossover and mutation, that we adapt to work on
semantic networks. The algorithm employs commonsense reasoning to ensure all
operations preserve the meaningfulness of the networks, using ConceptNet and
WordNet knowledge bases. The algorithm can be interpreted as a novel memetic
algorithm (MA), given that (1) individuals represent pieces of information that
undergo evolution, as in the original sense of memetics as it was introduced by
Dawkins; and (2) this is different from existing MA, where the word &quot;memetic&quot;
has been used as a synonym for local refinement after global optimization. For
evaluating the approach, we introduce an analogical similarity-based fitness
measure that is computed through structure mapping. This setup enables the
open-ended generation of networks analogous to a given base network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7770</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7770</id><created>2014-04-03</created><authors><author><keyname>Berwanger</keyname><forenames>Dietmar</forenames><affiliation>Laboratoire Specification et Verification CNRS &amp; ENS Cachan, France</affiliation></author><author><keyname>Mathew</keyname><forenames>Anup Basil</forenames><affiliation>Institute of Mathematical Sciences Chennai, India</affiliation></author></authors><title>Games with recurring certainty</title><categories>cs.LO cs.GT</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 91-96</journal-ref><doi>10.4204/EPTCS.146.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infinite games where several players seek to coordinate under imperfect
information are known to be intractable, unless the information flow is
severely restricted. Examples of undecidable cases typically feature a
situation where players become uncertain about the current state of the game,
and this uncertainty lasts forever. Here we consider games where the players
attain certainty about the current state over and over again along any play.
For finite-state games, we note that this kind of recurring certainty implies a
stronger condition of periodic certainty, that is, the events of state
certainty ultimately occur at uniform, regular intervals. We show that it is
decidable whether a given game presents recurring certainty, and that, if so,
the problem of synthesising coordination strategies under w-regular winning
conditions is solvable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7775</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7775</id><created>2014-04-30</created><updated>2014-10-07</updated><authors><author><keyname>Andrews</keyname><forenames>Zoe</forenames></author><author><keyname>Bryans</keyname><forenames>Jeremy</forenames></author><author><keyname>Payne</keyname><forenames>Richard</forenames></author><author><keyname>Kristensen</keyname><forenames>Klaus</forenames></author></authors><title>Fault Modelling in System-of-Systems Contracts</title><categories>cs.SE</categories><comments>EDCC-2014, EDSoS-2014, systems of systems, modelling, architectural
  frameworks, contracts, faults</comments><proxy>Ana Mihut</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nature of Systems of Systems (SoSs), large complex systems composed of
independent, geographically distributed and continuously evolving constituent
systems, means that faults are unavoidable. Previous work on defining
contractual specifications of the constituent systems of SoSs does not provide
any explicit consideration for faults. In this paper we address that gap by
extending an existing pattern for modelling contracts with fault modelling
concepts. The proposed extensions are introduced with respect to an Audio
Visual SoS case study from Bang and Olufsen, before discussing how they relate
to previous work on modelling faults in SoSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7778</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7778</id><created>2014-04-30</created><updated>2014-05-07</updated><authors><author><keyname>Ingram</keyname><forenames>Claire</forenames></author><author><keyname>Riddle</keyname><forenames>Steve</forenames></author><author><keyname>Fitzgerald</keyname><forenames>John</forenames></author><author><keyname>Al-Lawati</keyname><forenames>Sakina A. H. J.</forenames></author><author><keyname>Alrbaiyan</keyname><forenames>Afra</forenames></author></authors><title>SoS Fault Modelling at the Architectural Level in an Emergency Response
  Case Study</title><categories>cs.SE</categories><comments>EDCC-2014, EDSoS-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems of systems (SoSs) are particularly vulnerable to faults and other
threats to their dependability, but frequently inhabit domains that demand high
levels of dependability. For this reason fault tolerance analysis is important
in SoS engineering. The COMPASS project has previously proposed a Fault
Tolerance Architecture Framework (FMAF), consisting of a collection of
viewpoints that support systematic reasoning about faults in an SoS at the
architectural level. The FMAF has been demonstrated previously with an analysis
of an example fault in an emergency response SoS. In this paper we present
further examples of the FMAF's practical use, by analysing different types of
faults drawn from the same emergency response case study. These example faults
exercise different aspects of the FMAF, demonstrate its use in more complex
fault modelling scenarios, and raise new questions for further development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7784</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7784</id><created>2014-04-30</created><authors><author><keyname>Mualem</keyname><forenames>Ahuva</forenames></author></authors><title>Monotonicity, Revenue Equivalence and Budgets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study multidimensional mechanism design in a common scenario where players
have private information about their willingness to pay and their ability to
pay. We provide a complete characterization of dominant-strategy
incentive-compatible direct mechanisms where over-reporting the budget is not
possible. In several settings, reporting larger budgets can be made suboptimal
with a small randomized modification to the payments. We then derive a closely
related partial characterization for the general case where players can
arbitrarily misreport their private budgets. Immediate applications of these
results include simple characterizations for mechanisms with publicly-known
budgets and for mechanisms without monetary transfers.
  The celebrated revenue equivalence theorem states that the seller&quot;s revenue
for a broad class of standard auction formats and settings will be the same in
equilibrium. Our main application is a revenue equivalence theorem for
financially constrained bidders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7787</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7787</id><created>2014-04-30</created><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Spectral density of the non-backtracking operator</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>6 pages, 6 figures, submitted to EPL</comments><journal-ref>2014 EPL 107 50005</journal-ref><doi>10.1209/0295-5075/107/50005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The non-backtracking operator was recently shown to provide a significant
improvement when used for spectral clustering of sparse networks. In this paper
we analyze its spectral density on large random sparse graphs using a mapping
to the correlation functions of a certain interacting quantum disordered system
on the graph. On sparse, tree-like graphs, this can be solved efficiently by
the cavity method and a belief propagation algorithm. We show that there exists
a paramagnetic phase, leading to zero spectral density, that is stable outside
a circle of radius $\sqrt{\rho}$, where $\rho$ is the leading eigenvalue of the
non-backtracking operator. We observe a second-order phase transition at the
edge of this circle, between a zero and a non-zero spectral density. That fact
that this phase transition is absent in the spectral density of other matrices
commonly used for spectral clustering provides a physical justification of the
performances of the non-backtracking operator in spectral clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7789</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7789</id><created>2014-04-30</created><authors><author><keyname>Zhang</keyname><forenames>Pan</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Phase transitions in semisupervised clustering of sparse networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph stat.ML</categories><journal-ref>Phys. Rev. E 90, 052802 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting labels of nodes in a network, such as community memberships or
demographic variables, is an important problem with applications in social and
biological networks. A recently-discovered phase transition puts fundamental
limits on the accuracy of these predictions if we have access only to the
network topology. However, if we know the correct labels of some fraction
$\alpha$ of the nodes, we can do better. We study the phase diagram of this
&quot;semisupervised&quot; learning problem for networks generated by the stochastic
block model. We use the cavity method and the associated belief propagation
algorithm to study what accuracy can be achieved as a function of $\alpha$. For
$k = 2$ groups, we find that the detectability transition disappears for any
$\alpha &gt; 0$, in agreement with previous work. For larger $k$ where a hard but
detectable regime exists, we find that the easy/hard transition (the point at
which efficient algorithms can do better than chance) becomes a line of
transitions where the accuracy jumps discontinuously at a critical value of
$\alpha$. This line ends in a critical point with a second-order transition,
beyond which the accuracy is a continuous function of $\alpha$. We demonstrate
qualitatively similar transitions in two real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7792</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7792</id><created>2014-04-30</created><updated>2014-05-07</updated><authors><author><keyname>Couto</keyname><forenames>Luis Diogo</forenames></author><author><keyname>Foster</keyname><forenames>Simon</forenames></author><author><keyname>Payne</keyname><forenames>Richard</forenames></author></authors><title>Towards Verification of Constituent Systems through Automated Proof</title><categories>cs.SE</categories><comments>EDCC-2014, EDSoS-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores verification of constituent systems within the context of
the Symphony tool platform for Systems of Systems (SoS). Our SoS modelling
language, CML, supports various contractual specification elements, such as
state invariants and operation preconditions, which can be used to specify
contractual obligations on the constituent systems of a SoS. To support
verification of these obligations we have developed a proof obligation
generator and theorem prover plugin for Symphony. The latter uses the
Isabelle/HOL theorem prover to automatically discharge the proof obligations
arising from a CML model. Our hope is that the resulting proofs can then be
used to formally verify the conformance of each constituent system, which is
turn would result in a dependable SoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7796</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7796</id><created>2014-04-30</created><updated>2014-06-19</updated><authors><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames><affiliation>LHC</affiliation></author><author><keyname>Ayache</keyname><forenames>St&#xe9;phane</forenames><affiliation>LIF</affiliation></author></authors><title>Majority Vote of Diverse Classifiers for Late Fusion</title><categories>stat.ML cs.LG cs.MM</categories><comments>IAPR Joint International Workshops on Statistical Techniques in
  Pattern Recognition and Structural and Syntactic Pattern Recignition, Joensuu
  : Finland (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, a lot of attention has been devoted to multimedia
indexing by fusing multimodal informations. Two kinds of fusion schemes are
generally considered: The early fusion and the late fusion. We focus on late
classifier fusion, where one combines the scores of each modality at the
decision level. To tackle this problem, we investigate a recent and elegant
well-founded quadratic program named MinCq coming from the machine learning
PAC-Bayesian theory. MinCq looks for the weighted combination, over a set of
real-valued functions seen as voters, leading to the lowest misclassification
rate, while maximizing the voters' diversity. We propose an extension of MinCq
tailored to multimedia indexing. Our method is based on an order-preserving
pairwise loss adapted to ranking that allows us to improve Mean Averaged
Precision measure while taking into account the diversity of the voters that we
want to fuse. We provide evidence that this method is naturally adapted to late
fusion procedures and confirm the good behavior of our approach on the
challenging PASCAL VOC'07 benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7799</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7799</id><created>2014-04-30</created><authors><author><keyname>Vucinic</keyname><forenames>Malisa</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Tourancheau</keyname><forenames>Bernard</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Rousseau</keyname><forenames>Franck</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Duda</keyname><forenames>Andrzej</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Damon</keyname><forenames>Laurent</forenames><affiliation>ST-CROLLES</affiliation></author><author><keyname>Guizzetti</keyname><forenames>Roberto</forenames><affiliation>ST-CROLLES</affiliation></author></authors><title>OSCAR: Object Security Architecture for the Internet of Things</title><categories>cs.NI cs.CR</categories><proxy>ccsd</proxy><journal-ref>15th IEEE International Symposium on a World of Wireless, Mobile
  and Multimedia Networks (IEEE WoWMoM 2014) (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Billions of smart, but constrained objects wirelessly connected to the global
network require novel paradigms in network design. New protocol standards,
tailored to constrained devices, have been designed taking into account
requirements such as asynchronous application traffic, need for caching, and
group communication. The existing connection oriented security architecture is
not able to keep up---first, in terms of the supported features, but also in
terms of the scale and resulting latency on small constrained devices. In this
paper, we propose an architecture that leverages the security concepts both
from content-centric and traditional connection-oriented approaches. We rely on
secure channels established by means of (D)TLS for key exchange, but we get rid
of the notion of the 'state' among communicating entities. We provide a
mechanism to protect from replay attacks by coupling our scheme with the CoAP
application protocol. Our object-based security architecture (OSCAR)
intrinsically supports caching and multicast, and does not affect the radio
duty-cycling operation of constrained objects. We evaluate OSCAR in two cases:
802.15.4 Low Power and Lossy Networks (LLN) and Machine-to-Machine (M2M)
communication for two different hardware platforms and MAC layers on a real
testbed and using the Cooja emulator. We show significant energy savings at
constrained servers and reasonable delays. We also discuss the applicability of
OSCAR to Smart City deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7803</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7803</id><created>2014-04-30</created><authors><author><keyname>Vucinic</keyname><forenames>Malisa</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Romaniello</keyname><forenames>Gabriele</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Guelorget</keyname><forenames>Laurene</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Tourancheau</keyname><forenames>Bernard</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Rousseau</keyname><forenames>Franck</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Alphand</keyname><forenames>Olivier</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Duda</keyname><forenames>Andrzej</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Damon</keyname><forenames>Laurent</forenames><affiliation>ST-CROLLES</affiliation></author></authors><title>Topology Construction in RPL Networks over Beacon-Enabled 802.15.4</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>19th IEEE Symposium on Computers and Communications (IEEE ISCC
  2014) (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new scheme that allows coupling beacon-enabled
IEEE 802.15.4 with the RPL routing protocol while keeping full compliance with
both standards. We provide a means for RPL to pass the routing information to
Layer 2 before the 802.15.4 topology is created by encapsulating RPL DIO
messages in beacon frames. The scheme takes advantage of 802.15.4 command
frames to solicit RPL DIO messages. The effect of the command frames is to
reset the Trickle timer that governs sending DIO messages. We provide a
detailed analysis of the overhead incurred by the proposed scheme to understand
topology construction costs. We have evaluated the scheme using Contiki and the
instruction-level Cooja simulator and compared our results against the most
common scheme used for dissemination of the upper-layer information in
beacon-enabled PANs. The results show energy savings during the topology
construction phase and in the steady state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7809</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7809</id><created>2014-04-04</created><updated>2014-07-26</updated><authors><author><keyname>Basieva</keyname><forenames>Irina</forenames></author><author><keyname>Khrennikov</keyname><forenames>Andrei</forenames></author></authors><title>Quantum(-like) common knowledge: Binmore-Brandenburger operator approach</title><categories>cs.LO q-bio.NC</categories><comments>presented at the conference Quantum Interaction 2014, Filzbach,
  Switzerland, June 29- July 3, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the detailed account of the quantum(-like) viewpoint to common
knowledge. The Binmore-Brandenburger operator approach to the notion of common
knowledge is extended to the quantum case. We develop a special quantum(-like)
model of common knowledge based on information representations of agents which
can be operationally represented by Hermitian operators. For simplicity, we
assume that each agent constructs her/his information representation by using
just one operator. However, different agents use in general representations
based on noncommuting operators, i.e., incompatible representations. The
quantum analog of basic system of common knowledge features ${\cal K}1-{\cal
K}5$ is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7810</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7810</id><created>2014-04-30</created><authors><author><keyname>Dregi</keyname><forenames>Markus Sortland</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author></authors><title>Parameterized Complexity of Bandwidth on Trees</title><categories>cs.DS cs.CC</categories><comments>33 pages, To appear at ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bandwidth of a $n$-vertex graph $G$ is the smallest integer $b$ such that
there exists a bijective function $f : V(G) \rightarrow \{1,...,n\}$, called a
layout of $G$, such that for every edge $uv \in E(G)$, $|f(u) - f(v)| \leq b$.
In the {\sc Bandwidth} problem we are given as input a graph $G$ and integer
$b$, and asked whether the bandwidth of $G$ is at most $b$. We present two
results concerning the parameterized complexity of the {\sc Bandwidth} problem
on trees.
  First we show that an algorithm for {\sc Bandwidth} with running time
$f(b)n^{o(b)}$ would violate the Exponential Time Hypothesis, even if the input
graphs are restricted to be trees of pathwidth at most two. Our lower bound
shows that the classical $2^{O(b)}n^{b+1}$ time algorithm by Saxe [SIAM Journal
on Algebraic and Discrete Methods, 1980] is essentially optimal.
  Our second result is a polynomial time algorithm that given a tree $T$ and
integer $b$, either correctly concludes that the bandwidth of $T$ is more than
$b$ or finds a layout of $T$ of bandwidth at most $b^{O(b)}$. This is the first
parameterized approximation algorithm for the bandwidth of trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7814</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7814</id><created>2014-04-30</created><authors><author><keyname>Davoud</keyname><forenames>Mostafavi Amjad</forenames></author><author><keyname>Mina</keyname><forenames>Zolfy Lighvan</forenames></author></authors><title>A tlm-based platform to specify and verify component-based real-time
  systems</title><categories>cs.SE</categories><comments>10 pages, 7 figures</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.5, No.2, March 2014</journal-ref><doi>10.5121/ijsea.2014.5201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about modeling and verification languages with their pros and
cons. Modeling is dynamic part of system development process before
realization. The cost and risky situations obligate designer to model system
before production and modeling gives designer more flexible and dynamic image
of realized system. Formal languages and modeling methods are the ways to model
and verify systems but they have their own difficulties in specifying systems.
Some of them are very precise but hard to specify complex systems like TRIO,
and others do not support object oriented design and hardware/software
co-design in real-time systems. In this paper we are going to introduce systemC
and the more abstracted method called TLM 2.0 that solved all mentioned
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7827</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7827</id><created>2014-04-30</created><authors><author><keyname>Gherekhloo</keyname><forenames>Soheil</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Resolving Entanglements in Topological Interference Management with
  Alternating Connectivity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum-capacity of a three user interference wired network for time-varying
channels is considered. Due to the channel variations, it is assumed that the
transmitters are only able to track the connectivity between the individual
nodes, thus only the (alternating) state of the network is known. By
considering a special subset of all possible states, we show that state
splitting combined with joint encoding over the alternating states is required
to achieve the sum-capacity. Regarding upper bounds, we use a genie aided
approach to show the optimality of this scheme. This highlights that more
involved transmit strategies are required for characterizing the degrees of
freedom even if the transmitters have heavily restricted channel state
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7828</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7828</id><created>2014-04-30</created><updated>2014-10-08</updated><authors><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Deep Learning in Neural Networks: An Overview</title><categories>cs.NE cs.LG</categories><comments>88 pages, 888 references</comments><report-no>Technical Report IDSIA-03-14</report-no><journal-ref>Neural Networks, Vol 61, pp 85-117, Jan 2015</journal-ref><doi>10.1016/j.neunet.2014.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, deep artificial neural networks (including recurrent ones)
have won numerous contests in pattern recognition and machine learning. This
historical survey compactly summarises relevant work, much of it from the
previous millennium. Shallow and deep learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable,
causal links between actions and effects. I review deep supervised learning
(also recapitulating the history of backpropagation), unsupervised learning,
reinforcement learning &amp; evolutionary computation, and indirect search for
short programs encoding deep and large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.7843</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.7843</id><created>2014-04-30</created><authors><author><keyname>Farhan</keyname><forenames>Farhan</forenames></author></authors><title>Study of Timing Synchronization in MIMO-OFDM Systems Using DVB-T</title><categories>cs.IT math.IT</categories><doi>10.5121/ijit.2014.3201</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  OFDM (Orthogonal Frequency Division Multiplexing)provides the promising
physical layer for 4G and 3GPP LTE Systems in terms of efficient use of
bandwidth and data rates. This paper highlights the implementation of OFDM in
Digital Video Broadcasting-Terrestrial (DVB-T). It mainly focuses on the timing
offset problem present in OFDM systems and its proposed solution using Cyclic
Prefix (CP) as a modified SC (Schmidl and COX) algorithm. It also highlights
the timing synchronization as well as performance comparison through bit error
rate. Synchronization issues in OFDM are important and can lead to information
loss if not properly addressed. Simulations were performed to implement DVB-T
system and to compare different synchronization methods under certain
distribution model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0006</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0006</id><created>2014-04-30</created><authors><author><keyname>Kassner</keyname><forenames>Moritz</forenames></author><author><keyname>Patera</keyname><forenames>William</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author></authors><title>Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile
  Gaze-based Interaction</title><categories>cs.CV cs.HC</categories><comments>10 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Commercial head-mounted eye trackers provide useful features to customers in
industry and research but are expensive and rely on closed source hardware and
software. This limits the application areas and use of mobile eye tracking to
expert users and inhibits user-driven development, customisation, and
extension. In this paper we present Pupil -- an accessible, affordable, and
extensible open source platform for mobile eye tracking and gaze-based
interaction. Pupil comprises 1) a light-weight headset with high-resolution
cameras, 2) an open source software framework for mobile eye tracking, as well
as 3) a graphical user interface (GUI) to playback and visualize video and gaze
data. Pupil features high-resolution scene and eye cameras for monocular and
binocular gaze estimation. The software and GUI are platform-independent and
include state-of-the-art algorithms for real-time pupil detection and tracking,
calibration, and accurate gaze estimation. Results of a performance evaluation
show that Pupil can provide an average gaze estimation accuracy of 0.6 degree
of visual angle (0.08 degree precision) with a latency of the processing
pipeline of only 0.045 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0029</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0029</id><created>2014-04-30</created><updated>2014-10-15</updated><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Space-Time Physical-Layer Network Coding</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE JSAC special issue on &quot;Fundamental Approaches to
  Network Coding in Wireless Communication Systems.&quot;&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A space-time physical-layer network coding (ST- PNC) method is presented for
information exchange among multiple users over fully-connected multi-way relay
networks. The method involves two steps: i) side-information learning and ii)
space-time relay transmission. In the first step, different sets of users are
scheduled to send signals over networks and the remaining users and relays
overhear the transmitted signals, thereby learning the interference patterns.
In the second step, multiple relays cooperatively send out linear combinations
of signals received in the previous phase using space-time precoding so that
all users efficiently exploit their side-information in the form of: 1) what
they sent and 2) what they overheard in decoding. This coding concept is
illustrated through two simple network examples. It is shown that ST-PNC
improves the sum of degrees of freedom (sum-DoF) of the network compared to
existing interference management methods. With ST-PNC, the sum-DoF of a general
multi-way relay network without channel knowledge at the users is characterized
in terms of relevant system parameters, chiefly the number of users, the number
of relays, and the number of antennas at relays. A major implication of the
derived results is that efficiently harnessing both transmit- ted and overheard
signals as side-information brings significant performance improvements to
fully-connected multi-way relay networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0032</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0032</id><created>2014-04-30</created><updated>2014-10-12</updated><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Distributed Space-Time Interference Alignment with Moderately-Delayed
  CSIT</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an interference alignment method with distributed and
delayed channel state information at the transmitter (CSIT) for a class of
interference networks. The core idea of the proposed method is to align
interference signals over time at the unintended receivers in a distributed
manner. With the proposed method, achievable trade-offs between the sum of
degrees of freedom (sum-DoF) and feedback delay of CSI are characterized in
both the X-channel and three-user interference channel to reveal the impact on
how the CSI feedback delay affects the sum-DoF of the interference networks. A
major implication of derived results is that distributed and moderately-
delayed CSIT is useful to strictly improve the sum-DoF over the case of no CSI
at the transmitter in a certain class of interference networks. For a class of
X-channels, the results show how to optimally use distributed and
moderately-delayed CSIT to yield the same sum-DoF as instantaneous and global
CSIT. Further, leveraging the proposed transmission method and the known outer
bound results, the sum-capacity of the two-user X-channel with a particular set
of channel coefficients is characterized within a constant number of bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0033</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0033</id><created>2014-04-30</created><updated>2015-01-16</updated><authors><author><keyname>V&#xe1;k&#xe1;r</keyname><forenames>Matthijs</forenames></author></authors><title>Syntax and Semantics of Linear Dependent Types</title><categories>cs.LO cs.PL math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A type theory is presented that combines (intuitionistic) linear types with
type dependency, thus properly generalising both intuitionistic dependent type
theory and full linear logic. A syntax and complete categorical semantics are
developed, the latter in terms of (strict) indexed symmetric monoidal
categories with comprehension. Various optional type formers are treated in a
modular way. In particular, we will see that the historically much-debated
multiplicative quantifiers and identity types arise naturally from categorical
considerations. These new multiplicative connectives are further characterised
by several identities relating them to the usual connectives from dependent
type theory and linear logic. Finally, one important class of models, given by
families with values in some symmetric monoidal category, is investigated in
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0034</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0034</id><created>2014-04-30</created><authors><author><keyname>Hunter</keyname><forenames>Aaron</forenames></author></authors><title>Belief Revision and Trust</title><categories>cs.AI</categories><comments>Appears in the Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief revision is the process in which an agent incorporates a new piece of
information together with a pre-existing set of beliefs. When the new
information comes in the form of a report from another agent, then it is clear
that we must first determine whether or not that agent should be trusted. In
this paper, we provide a formal approach to modeling trust as a pre-processing
step before belief revision. We emphasize that trust is not simply a relation
between agents; the trust that one agent has in another is often restricted to
a particular domain of expertise. We demonstrate that this form of trust can be
captured by associating a state-partition with each agent, then relativizing
all reports to this state partition before performing belief revision. In this
manner, we incorporate only the part of a report that falls under the perceived
domain of expertise of the reporting agent. Unfortunately, state partitions
based on expertise do not allow us to compare the relative strength of trust
held with respect to different agents. To address this problem, we introduce
pseudometrics over states to represent differing degrees of trust. This allows
us to incorporate simultaneous reports from multiple agents in a way that
ensures the most trusted reports will be believed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0039</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0039</id><created>2014-04-30</created><authors><author><keyname>El-Shishtawy</keyname><forenames>T.</forenames></author></authors><title>A Mobile Management System for Reforming Subsidies Distribution in
  Developing Countries</title><categories>cs.CY</categories><journal-ref>Journal of Information &amp; Systems Management Volume 3 Number 4
  December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has a specific objective of being useful for showing how the
advances in mobile technologies can help for solving social and political
aspects involved in the reform of subsidies in developing countries. It
describes the work done to build a mobile-based supportive network that
integrates all subsidies partners: governmental, non-governmental
organizations, merchants, and beneficiaries. One main contribution of this work
is the setting of a framework for identifying the requirements of subsidies
distribution information systems. In the proposed approach, seven domains were
identified to build a Mobile Subsidizing Business Model (MSBM). Based on MSBM,
detailed requirements were gathered in three stages, with each having its
appropriate methodology. In this work, we focus on the layered architecture
implementation of the subsidizing mobile system to breakdown the complexities,
which are due to variations of mobile technologies, different business rules,
and multiple distribution scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0042</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0042</id><created>2014-04-30</created><updated>2015-06-15</updated><authors><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Villa</keyname><forenames>Silvia</forenames></author></authors><title>Learning with incremental iterative regularization</title><categories>stat.ML cs.LG math.OC math.PR</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within a statistical learning setting, we propose and study an iterative
regularization algorithm for least squares defined by an incremental gradient
method. In particular, we show that, if all other parameters are fixed a
priori, the number of passes over the data (epochs) acts as a regularization
parameter, and prove strong universal consistency, i.e. almost sure convergence
of the risk, as well as sharp finite sample bounds for the iterates. Our
results are a step towards understanding the effect of multiple epochs in
stochastic gradient techniques in machine learning and rely on integrating
statistical and optimization results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0047</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0047</id><created>2014-04-30</created><authors><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author><author><keyname>Alvi</keyname><forenames>Atif</forenames></author></authors><title>Clome: The Practical Implications of a Cloud-based Smart Home</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rich body of work in recent years has advocated the use of cloud
technologies within a home environment, but nothing has materialized so far in
terms of real-world implementations. In this paper, we argue that this is due
to the fact that none of these proposals have addressed some of the practical
challenges of moving home applications to the cloud. Specifically, we discuss
the pragmatic implications of moving to the cloud including, data and
information security, increase in network traffic, and fault tolerance. To
elicit discussion, we take a clean-slate approach and introduce a
proof-of-concept smart home, dubbed Clome, that decouples non-trivial
computation from home applications and transfers it to the cloud. We also
discuss how a Clome-like smart home with decentralized processing and storage
can be enabled through OpenFlow programmable switches, home-centric programming
platforms, and thin-client computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0049</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0049</id><created>2014-04-30</created><updated>2014-05-06</updated><authors><author><keyname>Tupper</keyname><forenames>P. F.</forenames></author></authors><title>Exemplar Dynamics Models of the Stability of Phonological Categories</title><categories>cs.CL cs.SD</categories><comments>6 pages, COGS2014</comments><msc-class>91F20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model for the stability and maintenance of phonological
categories. Examples of phonological categories are vowel sounds such as &quot;i&quot;
and &quot;e&quot;. We model such categories as consisting of collections of labeled
exemplars that language users store in their memory. Each exemplar is a
detailed memory of an instance of the linguistic entity in question. Starting
from an exemplar-level model we derive integro-differential equations for the
long-term evolution of the density of exemplars in different portions of
phonetic space. Using these latter equations we investigate under what
conditions two phonological categories merge or not. Our main conclusion is
that for the preservation of distinct phonological categories, it is necessary
that anomalous speech tokens of a given category are discarded, and not merely
stored in memory as an exemplar of another category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0054</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0054</id><created>2014-04-30</created><authors><author><keyname>De Giacomo</keyname><forenames>Giuseppe</forenames></author><author><keyname>De Masellis</keyname><forenames>Riccardo</forenames></author><author><keyname>Grasso</keyname><forenames>Marco</forenames></author><author><keyname>Maggi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author></authors><title>LTLf and LDLf Monitoring: A Technical Report</title><categories>cs.AI cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runtime monitoring is one of the central tasks to provide operational
decision support to running business processes, and check on-the-fly whether
they comply with constraints and rules. We study runtime monitoring of
properties expressed in LTL on finite traces (LTLf) and in its extension LDLf.
LDLf is a powerful logic that captures all monadic second order logic on finite
traces, which is obtained by combining regular expressions and LTLf, adopting
the syntax of propositional dynamic logic (PDL). Interestingly, in spite of its
greater expressivity, LDLf has exactly the same computational complexity of
LTLf. We show that LDLf is able to capture, in the logic itself, not only the
constraints to be monitored, but also the de-facto standard RV-LTL monitors.
This makes it possible to declaratively capture monitoring metaconstraints, and
check them by relying on usual logical services instead of ad-hoc algorithms.
This, in turn, enables to flexibly monitor constraints depending on the
monitoring state of other constraints, e.g., &quot;compensation&quot; constraints that
are only checked when others are detected to be violated. In addition, we
devise a direct translation of LDLf formulas into nondeterministic automata,
avoiding to detour to Buechi automata or alternating automata, and we use it to
implement a monitoring plug-in for the PROM suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0055</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0055</id><created>2014-04-30</created><updated>2014-12-20</updated><authors><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Quantum, Stochastic, and Pseudo Stochastic Languages with Few States</title><categories>cs.FL quant-ph</categories><comments>A new version with new results. Previous version: Arseny M. Shur,
  Abuzer Yakaryilmaz: Quantum, Stochastic, and Pseudo Stochastic Languages with
  Few States. UCNC 2014: 327-339</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic languages are the languages recognized by probabilistic finite
automata (PFAs) with cutpoint over the field of real numbers. More general
computational models over the same field such as generalized finite automata
(GFAs) and quantum finite automata (QFAs) define the same class. In 1963, Rabin
proved the set of stochastic languages to be uncountable presenting a single
2-state PFA over the binary alphabet recognizing uncountably many languages
depending on the cutpoint. In this paper, we show the same result for unary
stochastic languages. Namely, we exhibit a 2-state unary GFA, a 2-state unary
QFA, and a family of 3-state unary PFAs recognizing uncountably many languages;
all these numbers of states are optimal. After this, we completely characterize
the class of languages recognized by 1-state GFAs, which is the only nontrivial
class of languages recognized by 1-state automata. Finally, we consider the
variations of PFAs, QFAs, and GFAs based on the notion of inclusive/exclusive
cutpoint, and present some results on their expressive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0060</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0060</id><created>2014-04-30</created><authors><author><keyname>Song</keyname><forenames>Haoyu</forenames></author><author><keyname>Gong</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Hongfei</forenames></author><author><keyname>Dustzadeh</keyname><forenames>Justin</forenames></author></authors><title>Unified POF Programming for Diversified SDN Data Plane</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world OpenFlow-based SDN deployments, the ability to program
heterogeneous forwarding elements built with different forwarding architectures
is a desirable capability. In this paper, we discuss a data plane programming
framework suitable for a flexible and protocol-oblivious data plane and show
how OpenFlow can evolve to provide a generic interface for platform-independent
programming and platform-specific compiling. We also show how an abstract
instruction set can play a pivotal role to support different programming styles
mapping to different forwarding chip architectures. As an example, we compare
the compiler-mode and interpreter-mode implementations for an NPU-based
forwarding element and conclude that the compiler-mode implementation can
achieve a performance similar to that of a conventional non-SDN implementation.
Built upon our protocol-oblivious forwarding (POF) vision, this work presents
our continuous efforts to complete the ecosystem and pave the SDN evolving
path. The programming framework could be considered as a proposal for the
OpenFlow 2.0 standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0080</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0080</id><created>2014-04-30</created><authors><author><keyname>Wechsler</keyname><forenames>Bertrand</forenames></author><author><keyname>Eilat</keyname><forenames>Dan</forenames></author><author><keyname>Limal</keyname><forenames>Nicolas</forenames></author></authors><title>Information Flow Decomposition in Feedback Systems: Linear
  Time-Invariant Systems with Gaussian Channels</title><categories>cs.SY</categories><comments>technical note. 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our companion paper [1], an information identity decomposition has been
derived, which can be interpreted as a law of conservation of information flows
in feedback systems. In this paper, we further investigate this decomposition
result when specified to linear time-invariant(LTI) systems connected with
additive white Gaussian noise(AWGN) channels. It is shown that the quantities
in the decomposition are characterized in sensitivity function and the law of
conservation is verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0085</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0085</id><created>2014-04-30</created><authors><author><keyname>Khademi</keyname><forenames>Mahmoud</forenames></author><author><keyname>Morency</keyname><forenames>Louis-Philippe</forenames></author></authors><title>Relative Facial Action Unit Detection</title><categories>cs.CV</categories><comments>Accepted at IEEE Winter Conference on Applications of Computer
  Vision, Steamboat Springs Colorado, USA, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a subject-independent facial action unit (AU) detection
method by introducing the concept of relative AU detection, for scenarios where
the neutral face is not provided. We propose a new classification objective
function which analyzes the temporal neighborhood of the current frame to
decide if the expression recently increased, decreased or showed no change.
This approach is a significant change from the conventional absolute method
which decides about AU classification using the current frame, without an
explicit comparison with its neighboring frames. Our proposed method improves
robustness to individual differences such as face scale and shape, age-related
wrinkles, and transitions among expressions (e.g., lower intensity of
expressions). Our experiments on three publicly available datasets (Extended
Cohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvement
of our approach over conventional absolute techniques. Keywords: facial action
coding system (FACS); relative facial action unit detection; temporal
information;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0086</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0086</id><created>2014-04-30</created><authors><author><keyname>Daou</keyname><forenames>Hoda</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Medically Relevant Criteria used in EEG Compression for Improved
  Post-Compression Seizure Detection</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biomedical signals aid in the diagnosis of different disorders and
abnormalities. When targeting lossy compression of such signals, the medically
relevant information that lies within the data should maintain its accuracy and
thus its reliability. In fact, signal models that are inspired by the
bio-physical properties of the signals at hand allow for a compression that
preserves more naturally the clinically significant features of these signals.
In this paper, we illustrate this through the example of EEG signals; more
specifically, we analyze three specific lossy EEG compression schemes. These
schemes are based on signal models that have different degrees of reliance on
signal production and physiological characteristics of EEG. The resilience of
these schemes is illustrated through the performance of seizure detection post
compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0087</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0087</id><created>2014-05-01</created><authors><author><keyname>Tananyan</keyname><forenames>Hovhannes G.</forenames></author></authors><title>Domination games played on line graphs of complete multipartite graphs</title><categories>cs.DM math.CO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The domination game on a graph $G$ (introduced by B. Bre\v{s}ar, S.
Klav\v{z}ar, D.F. Rall \cite{BKR2010}) consists of two players, Dominator and
Staller, who take turns choosing a vertex from $G$ such that whenever a vertex
is chosen by either player, at least one additional vertex is dominated.
Dominator wishes to dominate the graph in as few steps as possible, and Staller
wishes to delay this process as much as possible. The game domination number
$\gamma _{{\small g}}(G)$ is the number of vertices chosen when Dominator
starts the game; when Staller starts, it is denoted by $\gamma _{{\small
g}}^{\prime }(G).$
  In this paper, the domination game on line graph $L\left(
K_{\overline{m}}\right) $ of complete multipartite graph $K_{\overline{m}}$
$(\overline{m}\equiv (m_{1},...,m_{n})\in \mathbb{N} ^{n})$ is considered, the
exact values for game domination numbers are obtained and optimal strategy for
both players is described. Particularly, it is proved that for $m_{1}\leq
m_{2}\leq ...\leq m_{n}$ both $\gamma _{{\small g}}\left( L\left(
K_{\overline{m}}\right) \right) =\min \left\{ \left\lceil \frac{2}{3}\left\vert
V\left( K_{\overline{m}}\right) \right\vert \right\rceil ,\right.$ $\left.
2\max \left\{ \left\lceil \frac{1}{2}\left( m_{1}+...+m_{n-1}\right)
\right\rceil ,\text{ }m_{n-1}\right\} \right\} -1$ when $n\geq 2$ and $\gamma
_{g}^{\prime }(L\left( K_{\overline{m}}\right) )=\min \left\{ \left\lceil
\frac{2}{3}\left( \left\vert V(K_{_{\overline{m}}})\right\vert -2\right)
\right\rceil ,\right.$ $\left. 2\max \left\{ \left\lceil \frac{1}{2}\left(
m_{1}+...+m_{n-1}-1\right) \right\rceil ,\text{ }m_{n-1}\right\} \right\} $
when $n\geq 4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0088</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0088</id><created>2014-05-01</created><authors><author><keyname>Venkatesh</keyname><forenames>C.</forenames></author><author><keyname>Nagaraju</keyname><forenames>D.</forenames></author><author><keyname>Reddy</keyname><forenames>T. Sunil Kumar</forenames></author></authors><title>Binary Protector: Intrusion Detection in Multitier Web Applications</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The services of internet place a key role in the daily life by enabling the
in sequence from anywhere. To provide somewhere to stay the communication and
management in applications the web services has stimulated to multitier design.
In this multitier the web servers contain front end logic and data with
database servers. In this paper, we present binary protector intrusion
detection systems which designs the network behavior of user sessions across
both the front-end web server and the back-end database. By examining both web
and subsequent database requests, we are able to rummage out attacks that
independent IDS would not be able to distinguish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0089</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0089</id><created>2014-05-01</created><authors><author><keyname>Michaloliakos</keyname><forenames>Antonios</forenames></author><author><keyname>Rogalin</keyname><forenames>Ryan</forenames></author><author><keyname>Zhang</keyname><forenames>Yonglong</forenames></author><author><keyname>Psounis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Performance Modeling of Next-Generation Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The industry is satisfying the increasing demand for wireless bandwidth by
densely deploying a large number of access points which are centrally managed,
e.g. enterprise WiFi networks deployed in university campuses, companies,
airports etc. This small cell architecture is gaining traction in the cellular
world as well, as witnessed by the direction in which 4G+ and 5G
standardization is moving. Prior academic work in analyzing such large-scale
wireless networks either uses oversimplified models for the physical layer, or
ignores other important, real-world aspects of the problem, like MAC layer
considerations, topology characteristics, and protocol overhead. On the other
hand, the industry is using for deployment purposes on-site surveys and
simulation tools which do not scale, cannot efficiently optimize the design of
such a network, and do not explain why one design choice is better than
another. In this paper we introduce a simple yet accurate analytical model
which combines the realism and practicality of industrial simulation tools with
the ability to scale, analyze the effect of various design parameters, and
optimize the performance of real- world deployments. The model takes into
account all central system parameters, including channelization, power
allocation, user scheduling, load balancing, MAC, advanced PHY techniques
(single and multi user MIMO as well as cooperative transmission from multiple
access points), topological characteristics and protocol overhead. The accuracy
of the model is verified via extensive simulations and the model is used to
study a wide range of real world scenarios, providing design guidelines on the
effect of various design parameters on performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0091</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0091</id><created>2014-05-01</created><updated>2014-08-06</updated><authors><author><keyname>Ishihara</keyname><forenames>Hajime</forenames><affiliation>Japan Advanced Institute of Science and Technology</affiliation></author></authors><title>Classical propositional logic and decidability of variables in
  intuitionistic propositional logic</title><categories>math.LO cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August 7,
  2014) lmcs:1173</journal-ref><doi>10.2168/LMCS-10(3:1)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve the answer to the question: what set of excluded middles for
propositional variables in a formula suffices to prove the formula in
intuitionistic propositional logic whenever it is provable in classical
propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0093</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0093</id><created>2014-05-01</created><updated>2014-07-23</updated><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Morteza</forenames></author></authors><title>Parameterized Streaming Algorithms for Vertex Cover</title><categories>cs.DS</categories><comments>Fixed some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As graphs continue to grow in size, we seek ways to effectively process such
data at scale. The model of streaming graph processing, in which a compact
summary is maintained as each edge insertion/deletion is observed, is an
attractive one. However, few results are known for optimization problems over
such dynamic graph streams.
  In this paper, we introduce a new approach to handling graph streams, by
instead seeking solutions for the parameterized versions of these problems
where we are given a parameter $k$ and the objective is to decide whether there
is a solution bounded by $k$. By combining kernelization techniques with
randomized sketch structures, we obtain the first streaming algorithms for the
parameterized versions of the Vertex Cover problem. We consider the following
three models for a graph stream on $n$ nodes:
  1. The insertion-only model where the edges can only be added.
  2. The dynamic model where edges can be both inserted and deleted.
  3. The \emph{promised} dynamic model where we are guaranteed that at each
timestamp there is a solution of size at most $k$.
  In each of these three models we are able to design parameterized streaming
algorithms for the Vertex Cover problem. We are also able to show matching
lower bound for the space complexity of our algorithms.
  (Due to the arXiv limit of 1920 characters for abstract field, please see the
abstract in the paper for detailed description of our results)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0099</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0099</id><created>2014-05-01</created><authors><author><keyname>Sklar</keyname><forenames>Max</forenames></author></authors><title>Fast MLE Computation for the Dirichlet Multinomial</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a collection of categorical data, we want to find the parameters of a
Dirichlet distribution which maximizes the likelihood of that data. Newton's
method is typically used for this purpose but current implementations require
reading through the entire dataset on each iteration. In this paper, we propose
a modification which requires only a single pass through the dataset and
substantially decreases running time. Furthermore we analyze both theoretically
and empirically the performance of the proposed algorithm, and provide an open
source implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0100</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0100</id><created>2014-05-01</created><authors><author><keyname>Rajni</keyname><forenames>Ms.</forenames></author><author><keyname>Reena</keyname><forenames>Ms.</forenames></author></authors><title>Review of MANETS Using Distributed Public-key Cryptography</title><categories>cs.NI cs.CR</categories><comments>no of pages - 5 and published with IJCTT</comments><journal-ref>Ms. RInternational Journal of Computer Trends and Technology
  V10(3):143-147, Apr 2014. ISSN:2231-2803. Published by Seventh Sense Research
  Group</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring security is something that is not easily done as many of the demands
of network security conflict with the demands of mobile networks, majorly
because of the nature of the mobile devices (e.g. low power consumption, low
processing load). The study of secure distributed key agreement has great
theoretical and practical significance. Securing Mobile Ad-hoc Networks using
Distributed Public-key Cryptography in pairing with Mobile Ad hoc Networks and
various protocols are essential for secure communications in open and
distributed environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0101</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0101</id><created>2014-05-01</created><authors><author><keyname>Kaushik</keyname><forenames>Dr. Manju</forenames></author><author><keyname>Jain</keyname><forenames>Rashmi</forenames></author></authors><title>Natural User Interfaces: Trend in Virtual Interaction</title><categories>cs.HC cs.SE</categories><comments>3 pages</comments><journal-ref>International journal Of Latest technology in
  Engineering,Management &amp; Applied Science (IJLTEMAS)3(4),April 2014,141-143
  published by International Standards Publication</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the fundamental constraints of natural way of interacting such as
speech, touch, contextual and environmental awareness,immersive 3D
experiences-all with a goal of a computer that can see listen, learn talk and
act. We drive a set of trends prevailing for the next generation of user
interface: Natural User Interface (NUI).New technologies are pushing the
boundaries of what is possible without touching or clicking an interface-
paving the way of interaction to information visualization and opportunities in
human towards more natural interaction than ever before. In this paper we
consider the trends in computer interaction through that must be taken into
consideration to come up-in the near future with a well-designed-NUI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0102</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0102</id><created>2014-05-01</created><updated>2014-08-11</updated><authors><author><keyname>Naesseth</keyname><forenames>Christian A.</forenames></author><author><keyname>Lindsten</keyname><forenames>Fredrik</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author></authors><title>Capacity estimation of two-dimensional channels using Sequential Monte
  Carlo</title><categories>cs.IT math.IT stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a new Sequential-Monte-Carlo-based algorithm to estimate the
capacity of two-dimensional channel models. The focus is on computing the
noiseless capacity of the 2-D one-infinity run-length limited constrained
channel, but the underlying idea is generally applicable. The proposed
algorithm is profiled against a state-of-the-art method, yielding more than an
order of magnitude improvement in estimation accuracy for a given computation
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0108</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0108</id><created>2014-05-01</created><updated>2014-08-28</updated><authors><author><keyname>Gask&#xf3;</keyname><forenames>No&#xe9;mi</forenames></author><author><keyname>Lung</keyname><forenames>Rodica Ioana</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author></authors><title>Computing Strong Nash Equilibria for Multiplayer Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An heuristic approach to compute strong Nash (Aumann) equilibria is
presented. The method is based on differential evolution and three variants of
a generative relation for strong Nash equilibria characterization. Numerical
experiments performed on the minimum effort game for up to 150 players
illustrate the efficiency of the approach. The advantages and disadvantages of
each variant is discussed in terms of precision and running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0109</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0109</id><created>2014-05-01</created><authors><author><keyname>Jha</keyname><forenames>Vaibhav</forenames></author><author><keyname>Jha</keyname><forenames>Mohit</forenames></author><author><keyname>Sharma</keyname><forenames>GK</forenames></author></authors><title>Estimation of Optimized Energy and Latency Constraints for Task
  Allocation in 3d Network on Chip</title><categories>cs.OH</categories><comments>20 Pages,17 Figure, International Journal of Computer Science &amp;
  Information Technology. arXiv admin note: substantial text overlap with
  arXiv:1404.2512</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  Vol 6,No 2,pp 67-86 April 2014</journal-ref><doi>10.5121/ijcsit.2014.6205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Network on Chip (NoC) rooted system, energy consumption is affected by
task scheduling and allocation schemes which affect the performance of the
system. In this paper we test the pre-existing proposed algorithms and
introduced a new energy skilled algorithm for 3D NoC architecture. An efficient
dynamic and cluster approaches are proposed along with the optimization using
bio-inspired algorithm. The proposed algorithm has been implemented and
evaluated on randomly generated benchmark and real life application such as
MMS, Telecom and VOPD. The algorithm has also been tested with the E3S
benchmark and has been compared with the existing mapping algorithm spiral and
crinkle and has shown better reduction in the communication energy consumption
and shows improvement in the performance of the system. On performing
experimental analysis of proposed algorithm results shows that average
reduction in energy consumption is 49%, reduction in communication cost is 48%
and average latency is 34%. Cluster based approach is mapped onto NoC using
Dynamic Diagonal Mapping (DDMap), Crinkle and Spiral algorithms and found DDmap
provides improved result. On analysis and comparison of mapping of cluster
using DDmap approach the average energy reduction is 14% and 9% with crinkle
and spiral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0126</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0126</id><created>2014-05-01</created><authors><author><keyname>Maguire</keyname><forenames>Phil</forenames></author><author><keyname>Moser</keyname><forenames>Philippe</forenames></author><author><keyname>Maguire</keyname><forenames>Rebecca</forenames></author><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author></authors><title>Is Consciousness Computable? Quantifying Integrated Information Using
  Algorithmic Information Theory</title><categories>cs.IT math.IT q-bio.NC</categories><comments>Maguire, P., Moser, P., Maguire, R. &amp; Griffith, V. (2014). Is
  consciousness computable? Quantifying integrated information using
  algorithmic information theory. In P. Bello, M. Guarini, M. McShane, &amp; B.
  Scassellati (Eds.), Proceedings of the 36th Annual Conference of the
  Cognitive Science Society. Austin, TX: Cognitive Science Society</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we review Tononi's (2008) theory of consciousness as
integrated information. We argue that previous formalizations of integrated
information (e.g. Griffith, 2014) depend on information loss. Since lossy
integration would necessitate continuous damage to existing memories, we
propose it is more natural to frame consciousness as a lossless integrative
process and provide a formalization of this idea using algorithmic information
theory. We prove that complete lossless integration requires noncomputable
functions. This result implies that if unitary consciousness exists, it cannot
be modelled computationally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0133</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0133</id><created>2014-05-01</created><updated>2014-05-08</updated><authors><author><keyname>Lin</keyname><forenames>Binbin</forenames></author><author><keyname>Yang</keyname><forenames>Ji</forenames></author><author><keyname>He</keyname><forenames>Xiaofei</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Geodesic Distance Function Learning via Heat Flow on Vector Fields</title><categories>cs.LG math.DG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a distance function or metric on a given data manifold is of great
importance in machine learning and pattern recognition. Many of the previous
works first embed the manifold to Euclidean space and then learn the distance
function. However, such a scheme might not faithfully preserve the distance
function if the original manifold is not Euclidean. Note that the distance
function on a manifold can always be well-defined. In this paper, we propose to
learn the distance function directly on the manifold without embedding. We
first provide a theoretical characterization of the distance function by its
gradient field. Based on our theoretical analysis, we propose to first learn
the gradient field of the distance function and then learn the distance
function itself. Specifically, we set the gradient field of a local distance
function as an initial vector field. Then we transport it to the whole manifold
via heat flow on vector fields. Finally, the geodesic distance function can be
obtained by requiring its gradient field to be close to the normalized vector
field. Experimental results on both synthetic and real data demonstrate the
effectiveness of our proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0135</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0135</id><created>2014-05-01</created><updated>2016-02-19</updated><authors><author><keyname>Rabi</keyname><forenames>Maben</forenames></author><author><keyname>Ramesh</keyname><forenames>Chithrupa</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Separated design of encoder and controller for networked linear
  quadratic optimal control</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>To appear in the SIAM journal on Control and Optimization</comments><msc-class>93E20, 49N05</msc-class><acm-class>I.2.8; J.7; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a networked control system, we consider the problem of encoder and
controller design. We study a discrete-time linear plant with a finite horizon
performance cost, comprising of a quadratic function of the states and
controls, and an additive communication cost. We study separation in design of
the encoder and controller, along with related closed-loop properties such as
the dual effect and certainty equivalence. We consider three basic formats for
encoder outputs: quantized samples, real-valued samples at event-triggered
times, and real-valued samples over additive noise channels. If the controller
and encoder are dynamic, then we show that the performance cost is minimized by
a separated design: the controls are updated at each time instant as per a
certainty equivalence law, and the encoder is chosen to minimize an aggregate
quadratic distortion of the estimation error. This separation is shown to hold
even though a dual effect is present in the closed-loop system. We also show
that this separated design need not be optimal when the controller or encoder
are to be chosen from within restricted classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0144</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0144</id><created>2014-05-01</created><authors><author><keyname>Merrikh-Bayat</keyname><forenames>Farshad</forenames></author><author><keyname>Mirebrahimi</keyname><forenames>Seyedeh-Nafiseh</forenames></author><author><keyname>Khalili</keyname><forenames>Mohammad-Reza</forenames></author></authors><title>Discrete-Time Fractional-Order PID Controller: Definition, Tuning,
  Digital Realization and Experimental Results</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some of the complicated control problems we have to use the controllers
that apply nonlocal operators to the error signal to generate the control.
Currently, the most famous controller with nonlocal operators is the
fractional-order PID (FOPID). Commonly, after tuning the parameters of FOPID
controller, its transfer function is discretized (for realization purposes)
using the so-called generating function. This discretization is the origin of
some errors and unexpected results in feedback systems. It may even happen that
the controller obtained by discretizing a FOPID controller works worse than a
directly-tuned discrete-time classical PID controller. Moreover, FOPID
controllers cannot directly be applied to the processes modeled by, e.g., the
ARMA or ARMAX model. The aim of this paper is to propose a discrete-time
version of the FOPID controller and discuss on its properties and applications.
Similar to the FOPID controller, the proposed structure applies nonlocal
operators (with adjustable memory length) to the error signal. Two methods for
tuning the parameters of the proposed controller are developed and it is shown
that the proposed controller has the capacity of solving complicated control
problems with a high performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0145</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0145</id><created>2014-05-01</created><authors><author><keyname>Dukes</keyname><forenames>Kais</forenames></author></authors><title>Contextual Semantic Parsing using Crowdsourced Spatial Descriptions</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a contextual parser for the Robot Commands Treebank, a new
crowdsourced resource. In contrast to previous semantic parsers that select the
most-probable parse, we consider the different problem of parsing using
additional situational context to disambiguate between different readings of a
sentence. We show that multiple semantic analyses can be searched using dynamic
programming via interaction with a spatial planner, to guide the parsing
process. We are able to parse sentences in near linear-time by ruling out
analyses early on that are incompatible with spatial context. We report a 34%
upper bound on accuracy, as our planner correctly processes spatial context for
3,394 out of 10,000 sentences. However, our parser achieves a 96.53%
exact-match score for parsing within the subset of sentences recognized by the
planner, compared to 82.14% for a non-contextual parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0147</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0147</id><created>2014-05-01</created><updated>2014-05-02</updated><authors><author><keyname>Mosayebi</keyname><forenames>Reza</forenames></author><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Receivers for Diffusion-Based Molecular Communication: Exploiting Memory
  and Sampling Rate</title><categories>physics.bio-ph cs.IT math.IT</categories><comments>Submitted to JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a diffusion-based molecular communication channel between two
nano-machines is considered. The effect of the amount of memory on performance
is characterized, and a simple memory-limited decoder is proposed and its
performance is shown to be close to that of the best possible imaginable
decoder (without any restriction on the computational complexity or its
functional form), using Genie-aided upper bounds. This effect is specialized
for the case of Molecular Concentration Shift Keying; it is shown that a
four-bits memory achieved nearly the same performance as infinite memory. Then
a general class of threshold decoders is considered and shown not to be optimal
for Poisson channel with memory, unless SNR is higher than a value specified in
the paper. Another contribution is to show that receiver sampling at a rate
higher than the transmission rate, i.e., a multi-read system, can significantly
improve the performance. The associated decision rule for this system is shown
to be a weighted sum of the samples during each symbol interval. The
performance of the system is analyzed using the saddle point approximation. The
best performance gains are achieved for an oversampling factor of three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0149</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0149</id><created>2014-05-01</created><updated>2014-12-20</updated><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author></authors><title>Coding Theoretic Construction of Quantum Ramp Secret Sharing</title><categories>cs.IT math.AG math.CO math.IT quant-ph</categories><comments>svjour3.cls, 12 pages, no figure. Version 5 added citations to
  relevant prior papers that were missing in previous versions. Contents
  unchanged</comments><msc-class>81P94 (Primary) 94A62, 94B27 (Secondary)</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a construction of a quantum ramp secret sharing scheme from a nested
pair of linear codes. Necessary and sufficient conditions for qualified sets
and forbidden sets are given in terms of combinatorial properties of nested
linear codes. An algebraic geometric construction for quantum secret sharing is
also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0157</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0157</id><created>2014-05-01</created><authors><author><keyname>Bonato</keyname><forenames>Anthony</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Kim</keyname><forenames>Myunghwan</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>Pra&#x142;at</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Tian</keyname><forenames>Amanda</forenames></author><author><keyname>Young</keyname><forenames>Stephen J.</forenames></author></authors><title>Dimensionality of social networks using motifs and eigenvalues</title><categories>cs.SI physics.soc-ph</categories><comments>26 pages</comments><doi>10.1371/journal.pone.0106052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dimensionality of social networks, and develop experiments
aimed at predicting that dimension. We find that a social network model with
nodes and links sampled from an $m$-dimensional metric space with power-law
distributed influence regions best fits samples from real-world networks when
$m$ scales logarithmically with the number of nodes of the network. This
supports a logarithmic dimension hypothesis, and we provide evidence with two
different social networks, Facebook and LinkedIn. Further, we employ two
different methods for confirming the hypothesis: the first uses the
distribution of motif counts, and the second exploits the eigenvalue
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0166</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0166</id><created>2014-05-01</created><authors><author><keyname>de Buyl</keyname><forenames>Pierre</forenames></author><author><keyname>Varoquaux</keyname><forenames>Nelle</forenames></author></authors><title>Proceedings of the 6th European Conference on Python in Science
  (EuroSciPy 2013)</title><categories>cs.PL</categories><report-no>euroscipy-proceedings2013-00</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  These are the proceedings of the 6th European Conference on Python in
Science, EuroSciPy 2013, that was held in Brussels (21-25 August 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0170</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0170</id><created>2014-05-01</created><authors><author><keyname>Barjon</keyname><forenames>Matthieu</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Chaumette</keyname><forenames>Serge</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Johnen</keyname><forenames>Colette</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Neggaz</keyname><forenames>Yessin M.</forenames><affiliation>LaBRI</affiliation></author></authors><title>Un algorithme de test pour la connexit\'e temporelle des graphes
  dynamiques de faible densit\'e</title><categories>cs.DS cs.NI</categories><proxy>ccsd</proxy><journal-ref>ALGOTEL 2014 -- 16\`emes Rencontres Francophones sur les Aspects
  Algorithmiques des T\'el\'ecommunications, Le Bois-Plage-en-R\'e : France
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of testing whether a dynamic graph is temporally
connected, i.e. a temporal path ({\em journey}) exists between all pairs of
vertices. We consider a discrete version of the problem, where the topology is
given as an evolving graph $\G=\{G_1,G_2,...,G_{k}\}$ in which only the set of
(directed) edges varies. Two cases are studied, depending on whether a single
edge or an unlimited number of edges can be crossed in a same $G_i$ (strict
journeys {\it vs} non-strict journeys). For strict journeys, two existing
algorithms designed for other problems can be adapted. However, we show that a
dedicated approach achieves a better time complexity than one of these two
algorithms in all cases, and than the other one for those graphs whose density
is low at any time (though arbitrary over time). The time complexity of our
algorithm is $O(k\mu n)$, where $k=|\G|$ is the number of time steps and
$\mu=max(|E_i|)$ is the maximum {\em instant} density, to be contrasted with
$m=|\cup E_i|$, the {\em cumulated} density. Indeed, it is not uncommon for a
mobility scenario to satisfy, for instance, both $\mu=o(n)$ and
$m=\Theta(n^2)$. We characterize the key values of $k, \mu$ and $m$ for which
our algorithm should be used. For non-strict journeys, for which no algorithm
is known, we show that a similar strategy can be used to answer the question,
still in $O(k\mu n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0174</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0174</id><created>2014-05-01</created><authors><author><keyname>Mohamed</keyname><forenames>Karim M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Ghanem</keyname><forenames>Nagia M.</forenames></author></authors><title>VSCAN: An Enhanced Video Summarization using Density-based Spatial
  Clustering</title><categories>cs.CV cs.MM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.3590 by
  other authors without attribution</comments><doi>10.1007/978-3-642-41181-6_74</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present VSCAN, a novel approach for generating static video
summaries. This approach is based on a modified DBSCAN clustering algorithm to
summarize the video content utilizing both color and texture features of the
video frames. The paper also introduces an enhanced evaluation method that
depends on color and texture features. Video Summaries generated by VSCAN are
compared with summaries generated by other approaches found in the literature
and those created by users. Experimental results indicate that the video
summaries generated by VSCAN have a higher quality than those generated by
other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0189</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0189</id><created>2014-05-01</created><authors><author><keyname>Amir</keyname><forenames>Amihood</forenames></author><author><keyname>Chan</keyname><forenames>Timothy</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author><author><keyname>Lewenstein</keyname><forenames>Noa</forenames></author></authors><title>On Hardness of Jumbled Indexing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jumbled indexing is the problem of indexing a text $T$ for queries that ask
whether there is a substring of $T$ matching a pattern represented as a Parikh
vector, i.e., the vector of frequency counts for each character. Jumbled
indexing has garnered a lot of interest in the last four years. There is a
naive algorithm that preprocesses all answers in $O(n^2|\Sigma|)$ time allowing
quick queries afterwards, and there is another naive algorithm that requires no
preprocessing but has $O(n\log|\Sigma|)$ query time. Despite a tremendous
amount of effort there has been little improvement over these running times.
  In this paper we provide good reason for this. We show that, under a
3SUM-hardness assumption, jumbled indexing for alphabets of size $\omega(1)$
requires $\Omega(n^{2-\epsilon})$ preprocessing time or $\Omega(n^{1-\delta})$
query time for any $\epsilon,\delta&gt;0$. In fact, under a stronger 3SUM-hardness
assumption, for any constant alphabet size $r\ge 3$ there exist describable
fixed constant $\epsilon_r$ and $\delta_r$ such that jumbled indexing requires
$\Omega(n^{2-\epsilon_r})$ preprocessing time or $\Omega(n^{1-\delta_r})$ query
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0190</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0190</id><created>2014-05-01</created><authors><author><keyname>Hoxha</keyname><forenames>Klesti</forenames></author><author><keyname>Kika</keyname><forenames>Alda</forenames></author><author><keyname>Gani</keyname><forenames>Eriglen</forenames></author><author><keyname>Greca</keyname><forenames>Silvana</forenames></author></authors><title>Towards a Modular Recommender System for Research Papers written in
  Albanian</title><categories>cs.IR</categories><comments>8 pages</comments><acm-class>H.3.3</acm-class><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 5 Issue 4, 2014</journal-ref><doi>10.14569/IJACSA.2014.050423</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years there has been an increase in scientific papers
publications in Albania and its neighboring countries that have large
communities of Albanian speaking researchers. Many of these papers are written
in Albanian. It is a very time consuming task to find papers related to the
researchers' work, because there is no concrete system that facilitates this
process. In this paper we present the design of a modular intelligent search
system for articles written in Albanian. The main part of it is the recommender
module that facilitates searching by providing relevant articles to the users
(in comparison with a given one). We used a cosine similarity based heuristics
that differentiates the importance of term frequencies based on their location
in the article. We did not notice big differences on the recommendation results
when using different combinations of the importance factors of the keywords,
title, abstract and body. We got similar results when using only the title and
abstract in comparison with the other combinations. Because we got fairly good
results in this initial approach, we believe that similar recommender systems
for documents written in Albanian can be build also in contexts not related to
scientific publishing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0198</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0198</id><created>2014-05-01</created><updated>2014-11-18</updated><authors><author><keyname>Chau</keyname><forenames>H. F.</forenames></author><author><keyname>Fung</keyname><forenames>C. -H. Fred</forenames></author><author><keyname>Lo</keyname><forenames>H. -K.</forenames></author></authors><title>No Superluminal Signaling Implies Unconditionally Secure Bit Commitment</title><categories>quant-ph cs.CR</categories><comments>This paper has been withdrawn by the authors due to a crucial
  oversight on an earlier work by A. Kent</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bit commitment (BC) is an important cryptographic primitive for an agent to
convince a mutually mistrustful party that she has already made a binding
choice of 0 or 1 but only to reveal her choice at a later time. Ideally, a BC
protocol should be simple, reliable, easy to implement using existing
technologies, and most importantly unconditionally secure in the sense that its
security is based on an information-theoretic proof rather than computational
complexity assumption or the existence of a trustworthy arbitrator. Here we
report such a provably secure scheme involving only one-way classical
communications whose unconditional security is based on no superluminal
signaling (NSS). Our scheme is inspired by the earlier works by Kent, who
proposed two impractical relativistic protocols whose unconditional securities
are yet to be established as well as several provably unconditionally secure
protocols which rely on both quantum mechanics and NSS. Our scheme is
conceptually simple and shows for the first time that quantum communication is
not needed to achieve unconditional security for BC. Moreover, with purely
classical communications, our scheme is practical and easy to implement with
existing telecom technologies. This completes the cycle of study of
unconditionally secure bit commitment based on known physical laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0201</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0201</id><created>2014-05-01</created><authors><author><keyname>Chodisetti</keyname><forenames>Navya</forenames></author></authors><title>Analysis of Digital Knapsack Based Sealed Bid Auction</title><categories>cs.CR</categories><comments>17, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need of totally secure online auction has led to the invention of many
auction protocols. But as new attacks are developed, auction protocols also
require corresponding strengthening. We analyze the auction protocol based on
the well-known mathematical public-key knapsack problem for the design of
asymmetric public-key knapsack trapdoor cryptosystem. Even though the knapsack
system is not cryptographically secure, it can be used in certain auction
situations. We describe the limitations of the protocol like detecting and
solving the tie between bidders, malicious behavior of participants and also
selection of price set by the seller and offer solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0203</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0203</id><created>2014-05-01</created><updated>2015-10-11</updated><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>Amir Salman</forenames></author><author><keyname>Zhu</keyname><forenames>Yan</forenames></author></authors><title>Binary Fading Interference Channel with No CSIT</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. arXiv admin
  note: text overlap with arXiv:1301.5309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacity region of the two-user Binary Fading Interference
Channel where the transmitters have no knowledge of the channel state
information. We develop new inner-bounds and outer-bounds for this problem. We
identify three regimes based on the channel parameters: weak, moderate, and
strong interference regime. Interesting, this is similar to the generalized
degrees of freedom of the two-user Gaussian interference channel where
transmitters have perfect channel knowledge. We show that for the weak
interference regime, treating interference as erasure is optimal while for the
strong interference regime, decoding interference is optimal. For the moderate
interference regime, we provide new inner and outer bounds. The inner-bound is
based on a modification of the Han-Kobayashi scheme for the erasure channel,
enhanced by time-sharing. We study the gap between our inner-bound and the
outer-bounds for the moderate interference regime and compare our results to
that of the Gaussian interference channel.
  Deriving our new outer-bounds has three main steps. We first create a
contracted channel that has fewer states compared to the original channel, in
order to make the analysis tractable. We then prove the Correlation Lemma that
shows an outer-bound on the capacity region of the contracted channel also
serves as an outer-bound for the original channel. Finally, using Conditional
Entropy Leakage Lemma, we derive our outer-bound on the capacity region of the
contracted channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0205</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0205</id><created>2014-05-01</created><authors><author><keyname>Kerschbaum</keyname><forenames>Florian</forenames></author><author><keyname>Beck</keyname><forenames>Martin</forenames></author><author><keyname>Sch&#xf6;nfeld</keyname><forenames>Dagmar</forenames></author></authors><title>Inference Control for Privacy-Preserving Genome Matching</title><categories>cs.CR</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy is of the utmost importance in genomic matching. Therefore a number
of privacy-preserving protocols have been presented using secure computation.
Nevertheless, none of these protocols prevents inferences from the result.
Goodrich has shown that this resulting information is sufficient for an
effective attack on genome databases. In this paper we present an approach that
can detect and mitigate such an attack on encrypted messages while still
preserving the privacy of both parties. Note that randomization, e.g.~using
differential privacy, will almost certainly destroy the utility of the matching
result. We combine two known cryptographic primitives -- secure computation of
the edit distance and fuzzy commitments -- in order to prevent submission of
similar genome sequences. Particularly, we contribute an efficient
zero-knowledge proof that the same input has been used in both primitives. We
show that using our approach it is feasible to preserve privacy in genome
matching and also detect and mitigate Goodrich's attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0209</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0209</id><created>2014-05-01</created><authors><author><keyname>Veetil</keyname><forenames>Sreejith T.</forenames></author><author><keyname>Kuchi</keyname><forenames>Kiran</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author></authors><title>Performance of PZF and MMSE Receivers in Cellular Networks with
  Multi-User Spatial Multiplexing</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper characterizes the performance of cellular networks employing
multiple antenna open-loop spatial multiplexing techniques. We use a stochastic
geometric framework to model distance depended inter cell interference. Using
this framework, we analyze the coverage and rate using two linear receivers,
namely, partial zero-forcing (PZF) and minimum-mean-square-estimation (MMSE)
receivers. Analytical expressions are obtained for coverage and rate
distribution that are suitable for fast numerical computation.
  In the case of the PZF receiver, we show that it is not optimal to utilize
all the receive antenna for canceling interference. With $\alpha$ as the path
loss exponent, $N_t$ transmit antenna, $N_r$ receive antenna, we show that it
is optimal to use $N_t\left\lceil\left(1-\frac{2}{\alpha}\right) \left(
\frac{N_r}{N_t} -\frac{1}{2}\right)\right\rceil$ receive antennas for
interference cancellation and the remaining antennas for signal enhancement
(array gain). For both PZF and MMSE receivers, we observe that increasing the
number of data streams provides an improvement in the mean data rate with
diminishing returns. Also transmitting $N_r$ streams is not optimal in terms of
the mean sum rate. We observe that increasing the SM rate with a PZF receiver
always degrades the cell edge data rate while the performance with MMSE
receiver is nearly independent of the SM rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0212</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0212</id><created>2014-05-01</created><authors><author><keyname>Yousefi</keyname><forenames>Siamak</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author></authors><title>Mobile Localization in Non-Line-of-Sight Using Constrained Square-Root
  Unscented Kalman Filter</title><categories>stat.AP cs.IT math.IT</categories><comments>Under review by IEEE Trans. on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization and tracking of a mobile node (MN) in non-line-of-sight (NLOS)
scenarios, based on time of arrival (TOA) measurements, is considered in this
work. To this end, we develop a constrained form of square root unscented
Kalman filter (SRUKF), where the sigma points of the unscented transformation
are projected onto the feasible region by solving constrained optimization
problems. The feasible region is the intersection of several discs formed by
the NLOS measurements. We show how we can reduce the size of the optimization
problem and formulate it as a convex quadratically constrained quadratic
program (QCQP), which depends on the Cholesky factor of the \textit{a
posteriori} error covariance matrix of SRUKF. As a result of these
modifications, the proposed constrained SRUKF (CSRUKF) is more efficient and
has better numerical stability compared to the constrained UKF. Through
simulations, we also show that the CSRUKF achieves a smaller localization error
compared to other techniques and that its performance is robust under different
NLOS conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0223</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0223</id><created>2014-05-01</created><authors><author><keyname>Ambikasaran</keyname><forenames>Sivaram</forenames></author><author><keyname>O'Neil</keyname><forenames>Michael</forenames></author></authors><title>Fast symmetric factorization of hierarchical matrices with applications</title><categories>math.NA cs.NA physics.flu-dyn stat.CO</categories><comments>15 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast direct algorithm for computing symmetric factorizations,
i.e. $A = WW^T$, of symmetric positive-definite hierarchical matrices with
weak-admissibility conditions. The computational cost for the symmetric
factorization scales as $\mathcal{O}(n \log^2 n)$ for hierarchically
off-diagonal low-rank matrices. Once this factorization is obtained, the cost
for inversion, application, and determinant computation scales as
$\mathcal{O}(n \log n)$. In particular, this allows for the near optimal
generation of correlated random variates in the case where $A$ is a covariance
matrix. This symmetric factorization algorithm depends on two key ingredients.
First, we present a novel symmetric factorization formula for low-rank updates
to the identity of the form $I+UKU^T$. This factorization can be computed in
$\mathcal{O}(n)$ time, if the rank of the perturbation is sufficiently small.
Second, combining this formula with a recursive divide-and-conquer strategy,
near linear complexity symmetric factorizations for hierarchically structured
matrices can be obtained. We present numerical results for matrices relevant to
problems in probability \&amp; statistics (Gaussian processes), interpolation
(Radial basis functions), and Brownian dynamics calculations in fluid mechanics
(the Rotne-Prager-Yamakawa tensor).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0234</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0234</id><created>2014-05-01</created><authors><author><keyname>Castanon</keyname><forenames>Greg</forenames></author><author><keyname>Elgharib</keyname><forenames>Mohamed</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author><author><keyname>Jodoin</keyname><forenames>Pierre-Marc</forenames></author></authors><title>Retrieval in Long Surveillance Videos using User Described Motion and
  Object Attributes</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a content-based retrieval method for long surveillance videos both
for wide-area (Airborne) as well as near-field imagery (CCTV). Our goal is to
retrieve video segments, with a focus on detecting objects moving on routes,
that match user-defined events of interest. The sheer size and remote locations
where surveillance videos are acquired, necessitates highly compressed
representations that are also meaningful for supporting user-defined queries.
To address these challenges we archive long-surveillance video through
lightweight processing based on low-level local spatio-temporal extraction of
motion and object features. These are then hashed into an inverted index using
locality-sensitive hashing (LSH). This local approach allows for query
flexibility as well as leads to significant gains in compression. Our second
task is to extract partial matches to the user-created query and assembles them
into full matches using Dynamic Programming (DP). DP exploits causality to
assemble the indexed low level features into a video segment which matches the
query route. We examine CCTV and Airborne footage, whose low contrast makes
motion extraction more difficult. We generate robust motion estimates for
Airborne data using a tracklets generation algorithm while we use Horn and
Schunck approach to generate motion estimates for CCTV. Our approach handles
long routes, low contrasts and occlusion. We derive bounds on the rate of false
positives and demonstrate the effectiveness of the approach for counting,
motion pattern recognition and abandoned object applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0237</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0237</id><created>2014-05-01</created><authors><author><keyname>Feng</keyname><forenames>Joe-Mei</forenames></author><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author></authors><title>An RIP-based approach to $\Sigma\Delta$ quantization for compressed
  sensing</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><doi>10.1109/LSP.2014.2336700</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a new approach to estimating the error of
reconstruction from $\Sigma\Delta$ quantized compressed sensing measurements.
  Our method is based on the restricted isometry property (RIP) of a certain
projection of the measurement matrix.
  Our result yields simple proofs and a slight generalization of the best-known
reconstruction error bounds for Gaussian and subgaussian measurement matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0253</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0253</id><created>2014-05-01</created><updated>2014-05-02</updated><authors><author><keyname>Panella</keyname><forenames>Federica</forenames></author></authors><title>Approximate Query Answering in Inconsistent Databases</title><categories>cs.DB</categories><comments>54 pages. Added abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical algorithms for query optimization presuppose the absence of
inconsistencies or uncertainties in the database and exploit only valid
semantic knowledge provided, e.g., by integrity constraints. Data inconsistency
or uncertainty, however, is a widespread critical issue in ordinary databases:
total integrity is often, in fact, an unrealistic assumption and violations to
integrity constraints may be introduced in several ways.
  In this report we present an approach for semantic query optimization that,
differently from the traditional ones, relies on not necessarily valid semantic
knowledge, e.g., provided by violated or soft integrity constraints, or induced
by applying data mining techniques. Query optimization that leverages invalid
semantic knowledge cannot guarantee the semantic equivalence between the
original user's query and its rewriting: thus a query optimized by our approach
yields approximate answers that can be provided to the users whenever fast but
possibly partial responses are required. Also, we evaluate the impact of use of
invalid semantic knowledge in the rewriting of a query by computing a measure
of the quality of the answer returned to the user, and we rely on the recent
theory of Belief Logic Programming to deal with the presence of possible
correlation in the semantic knowledge used in the rewriting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0276</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0276</id><created>2014-05-01</created><updated>2014-05-26</updated><authors><author><keyname>Whitacre</keyname><forenames>James</forenames></author><author><keyname>Schellenberg</keyname><forenames>Sven</forenames></author><author><keyname>Iorio</keyname><forenames>Antony</forenames></author></authors><title>Coal Blending: Business Value, Analysis, and Optimization</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coal blending is a critically important process in the coal mining industry
as it directly influences the number of product tonnes and the total revenue
generated by a mine site. Coal blending represents a challenging and complex
problem with numerous blending possibilities, multiple constraints and
competing objectives. At many mine sites, blending decisions are made using
heuristics that have been developed through experience or made by using
computer assisted control algorithms or linear programming. While current
blending procedures have achieved profitable outcomes in the past, they often
result in a sub-optimal utilization of high quality coal. This sub-optimality
has a considerable negative impact on mine site productivity as it can reduce
the amount of lower quality ROM that is blended and sold. This article reviews
the coal blending problem and discusses some of the difficult trade-offs and
challenges that arise in trying to address this problem. We highlight some of
the risks from making simplifying assumptions and the limitations of current
software optimization systems. We conclude by explaining how the mining
industry would significantly benefit from research and development into
optimization algorithms and technologies that are better able to combine
computer optimization algorithm capabilities with the important insights of
engineers and quality control specialists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0293</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0293</id><created>2014-05-01</created><authors><author><keyname>Nalon</keyname><forenames>Cl&#xe1;udia</forenames></author><author><keyname>Marcos</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Dixon</keyname><forenames>Clare</forenames></author></authors><title>Clausal Resolution for Modal Logics of Confluence</title><categories>cs.LO</categories><comments>15 pages, 1 figure. Preprint of the paper accepted to IJCAR 2014</comments><doi>10.1007/978-3-319-08587-6_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a clausal resolution-based method for normal multimodal logics of
confluence, whose Kripke semantics are based on frames characterised by
appropriate instances of the Church-Rosser property. Here we restrict attention
to eight families of such logics. We show how the inference rules related to
the normal logics of confluence can be systematically obtained from the
parametrised axioms that characterise such systems. We discuss soundness,
completeness, and termination of the method. In particular, completeness can be
modularly proved by showing that the conclusions of each newly added inference
rule ensures that the corresponding conditions on frames hold. Some examples
are given in order to illustrate the use of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0296</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0296</id><created>2014-05-01</created><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Lakin</keyname><forenames>Matthew R.</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author></authors><title>Reservoir Computing Approach to Robust Computation using Unreliable
  Nanoscale Networks</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we approach the physical limits of CMOS technology, advances in materials
science and nanotechnology are making available a variety of unconventional
computing substrates that can potentially replace top-down-designed
silicon-based computing devices. Inherent stochasticity in the fabrication
process and nanometer scale of these substrates inevitably lead to design
variations, defects, faults, and noise in the resulting devices. A key
challenge is how to harness such devices to perform robust computation. We
propose reservoir computing as a solution. In reservoir computing, computation
takes place by translating the dynamics of an excited medium, called a
reservoir, into a desired output. This approach eliminates the need for
external control and redundancy, and the programming is done using a
closed-form regression problem on the output, which also allows concurrent
programming using a single device. Using a theoretical model, we show that both
regular and irregular reservoirs are intrinsically robust to structural noise
as they perform computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0312</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0312</id><created>2014-05-01</created><updated>2015-02-20</updated><authors><author><keyname>Lin</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Maire</keyname><forenames>Michael</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Hays</keyname><forenames>James</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author></authors><title>Microsoft COCO: Common Objects in Context</title><categories>cs.CV</categories><comments>1) updated annotation pipeline description and figures; 2) added new
  section describing datasets splits; 3) updated author list</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new dataset with the goal of advancing the state-of-the-art in
object recognition by placing the question of object recognition in the context
of the broader question of scene understanding. This is achieved by gathering
images of complex everyday scenes containing common objects in their natural
context. Objects are labeled using per-instance segmentations to aid in precise
object localization. Our dataset contains photos of 91 objects types that would
be easily recognizable by a 4 year old. With a total of 2.5 million labeled
instances in 328k images, the creation of our dataset drew upon extensive crowd
worker involvement via novel user interfaces for category detection, instance
spotting and instance segmentation. We present a detailed statistical analysis
of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide
baseline performance analysis for bounding box and segmentation detection
results using a Deformable Parts Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0319</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0319</id><created>2014-05-01</created><updated>2014-05-07</updated><authors><author><keyname>Bhattacharyya</keyname><forenames>Anirban</forenames></author><author><keyname>Mokhov</keyname><forenames>Andrey</forenames></author><author><keyname>Pierce</keyname><forenames>Ken</forenames></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author></authors><title>On Formalisms for Dynamic Reconfiguration of Dependable Systems</title><categories>cs.SE</categories><comments>EDCC-2014, Fast-Abstracts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three formalisms of different kinds - VDM, Maude, and basic CCSdp - are
evaluated for their suitability for the modelling and verification of dynamic
software reconfiguration using as a case study the dynamic reconfiguration of a
simple office workflow for order processing. The research is ongoing, and
initial results are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0320</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0320</id><created>2014-05-01</created><authors><author><keyname>Adrovic</keyname><forenames>Danko</forenames></author><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author></authors><title>Computing all Affine Solution Sets of Binomial Systems</title><categories>cs.SC cs.MS math.AG</categories><comments>4 page extended abstract accepted by EACA 2014, a conference on
  Computer Algebra and its Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To compute solutions of sparse polynomial systems efficiently we have to
exploit the structure of their Newton polytopes. While the application of
polyhedral methods naturally excludes solutions with zero components, an
irreducible decomposition of a variety is typically understood in affine space,
including also those components with zero coordinates. For the problem of
computing solution sets in the intersection of some coordinate planes, the
direct application of a polyhedral method fails, because the original facial
structure of the Newton polytopes may alter completely when selected variables
become zero. Our new proposed method enumerates all factors contributing to a
generalized permanent and toric solutions as a special case of this
enumeration. For benchmark problems such as the adjacent 2-by-2 minors of a
general matrix, our methods scale much better than the witness set
representations of numerical algebraic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0325</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0325</id><created>2014-05-01</created><updated>2014-05-07</updated><authors><author><keyname>Aniello</keyname><forenames>L.</forenames></author><author><keyname>Bondavalli</keyname><forenames>A.</forenames></author><author><keyname>Ceccarelli</keyname><forenames>A.</forenames></author><author><keyname>Ciccotelli</keyname><forenames>C.</forenames></author><author><keyname>Cinque</keyname><forenames>M.</forenames></author><author><keyname>Frattini</keyname><forenames>F.</forenames></author><author><keyname>Guzzo</keyname><forenames>A.</forenames></author><author><keyname>Pecchia</keyname><forenames>A.</forenames></author><author><keyname>Pugliese</keyname><forenames>A.</forenames></author><author><keyname>Querzoni</keyname><forenames>L.</forenames></author><author><keyname>Russo</keyname><forenames>S.</forenames></author></authors><title>Big Data in Critical Infrastructures Security Monitoring: Challenges and
  Opportunities</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Critical Infrastructures (CIs), such as smart power grids, transport systems,
and financial infrastructures, are more and more vulnerable to cyber threats,
due to the adoption of commodity computing facilities. Despite the use of
several monitoring tools, recent attacks have proven that current defensive
mechanisms for CIs are not effective enough against most advanced threats. In
this paper we explore the idea of a framework leveraging multiple data sources
to improve protection capabilities of CIs. Challenges and opportunities are
discussed along three main research directions: i) use of distinct and
heterogeneous data sources, ii) monitoring with adaptive granularity, and iii)
attack modeling and runtime combination of multiple data analysis techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0326</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0326</id><created>2014-05-01</created><authors><author><keyname>Tananyan</keyname><forenames>Hovhannes G.</forenames></author><author><keyname>Kamalian</keyname><forenames>Rafayel R.</forenames></author></authors><title>On Simultaneous 2-locally-balanced 2-partition for Two Forests with Same
  Vertices</title><categories>math.CO cs.DM</categories><comments>3 pages</comments><journal-ref>Transactions of the Institute for Informatics and Automation
  Problems of the NAS of RA, Mathematical Problems of Computer Science 29,
  2007, p. 104-106</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of a partition of the common set of the vertices of two forests
into two subsets, when difference of their capacities in the neighborhood of
each vertex of each forest not greater than 2 is proved, and an example, which
shows that improvement of the specified constant is impossible is brought.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0327</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0327</id><created>2014-05-01</created><authors><author><keyname>Cicotti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Coppolino</keyname><forenames>Luigi</forenames></author><author><keyname>D'Antonio</keyname><forenames>Salvatore</forenames></author><author><keyname>Romano</keyname><forenames>Luigi</forenames></author></authors><title>Big Data Analytics for QoS Prediction Through Probabilistic Model
  Checking</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014, Big Data Analytics, QoS Prediction, Model
  Checking, SLA compliance monitoring</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As competitiveness increases, being able to guaranting QoS of delivered
services is key for business success. It is thus of paramount importance the
ability to continuously monitor the workflow providing a service and to timely
recognize breaches in the agreed QoS level. The ideal condition would be the
possibility to anticipate, thus predict, a breach and operate to avoid it, or
at least to mitigate its effects. In this paper we propose a model checking
based approach to predict QoS of a formally described process. The continous
model checking is enabled by the usage of a parametrized model of the monitored
system, where the actual value of parameters is continuously evaluated and
updated by means of big data tools. The paper also describes a prototype
implementation of the approach and shows its usage in a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0329</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0329</id><created>2014-05-01</created><authors><author><keyname>Cao</keyname><forenames>Yixin</forenames></author><author><keyname>Grippo</keyname><forenames>Luciano N.</forenames></author><author><keyname>Safe</keyname><forenames>Mart&#xed;n D.</forenames></author></authors><title>Forbidden Induced Subgraphs of Normal Helly Circular-Arc Graphs:
  Characterization and Detection</title><categories>cs.DM cs.DS</categories><comments>Preliminary results of this paper appeared in the proceedings of SBPO
  2012 and FAW 2014</comments><msc-class>05C62, 05C75, 05C85</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A normal Helly circular-arc graph is the intersection graph of arcs on a
circle of which no three or less arcs cover the whole circle. Lin, Soulignac,
and Szwarcfiter [Discrete Appl. Math. 2013] characterized circular-arc graphs
that are not normal Helly circular-arc graphs, and used it to develop the first
recognition algorithm for this graph class. As open problems, they ask for the
forbidden induced subgraph characterization and a direct recognition algorithm
for normal Helly circular-arc graphs, both of which are resolved by the current
paper. Moreover, when the input is not a normal Helly circular-arc graph, our
recognition algorithm finds in linear time a minimal forbidden induced subgraph
as certificate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0343</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0343</id><created>2014-05-02</created><updated>2014-11-24</updated><authors><author><keyname>&#xc7;etin</keyname><forenames>Uzay</forenames></author><author><keyname>Bingol</keyname><forenames>Haluk O.</forenames></author></authors><title>Iterated Prisoners Dilemma with limited attention</title><categories>cs.GT physics.soc-ph</categories><comments>8 pages, 3 figures</comments><proxy>Bohdan Markiv</proxy><journal-ref>Condens. Matter Phys., 2014, vol. 17, No. 3, 33001</journal-ref><doi>10.5488/CMP.17.33001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How attention scarcity effects the outcomes of a game? We present our
findings on a version of the Iterated Prisoners Dilemma (IPD) game in which
players can accept or refuse to play with their partner. We study the memory
size effect on determining the right partner to interact with. We investigate
the conditions under which the cooperators are more likely to be advantageous
than the defectors. This work demonstrates that, in order to beat defection,
players do not need a full memorization of each action of all opponents. There
exists a critical attention capacity threshold to beat defectors. This
threshold depends not only on the ratio of the defectors in the population but
also on the attention allocation strategy of the players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0348</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0348</id><created>2014-05-02</created><authors><author><keyname>Dumer</keyname><forenames>Ilya</forenames></author><author><keyname>Kovalev</keyname><forenames>Alexey A.</forenames></author><author><keyname>Pryadko</keyname><forenames>Leonid P.</forenames></author></authors><title>Numerical Techniques for Finding the Distances of Quantum Codes</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 1 figure, to appear in Proceedings of ISIT 2014 - IEEE
  International Symposium on Information Theory, Honolulu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the existing techniques for calculating code distances of classical
codes and apply these techniques to generic quantum codes. For classical and
quantum LDPC codes, we also present a new linked-cluster technique. It reduces
complexity exponent of all existing deterministic techniques designed for codes
with small relative distances (which include all known families of quantum LDPC
codes), and also surpasses the probabilistic technique for sufficiently high
code rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0351</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0351</id><created>2014-05-02</created><updated>2014-07-13</updated><authors><author><keyname>Sankey</keyname><forenames>Jody</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author></authors><title>Dovetail: Stronger Anonymity in Next-Generation Internet Routing</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current low-latency anonymity systems use complex overlay networks to conceal
a user's IP address, introducing significant latency and network efficiency
penalties compared to normal Internet usage. Rather than obfuscating network
identity through higher level protocols, we propose a more direct solution: a
routing protocol that allows communication without exposing network identity,
providing a strong foundation for Internet privacy, while allowing identity to
be defined in those higher level protocols where it adds value.
  Given current research initiatives advocating &quot;clean slate&quot; Internet designs,
an opportunity exists to design an internetwork layer routing protocol that
decouples identity from network location and thereby simplifies the anonymity
problem. Recently, Hsiao et al. proposed such a protocol (LAP), but it does not
protect the user against a local eavesdropper or an untrusted ISP, which will
not be acceptable for many users. Thus, we propose Dovetail, a next-generation
Internet routing protocol that provides anonymity against an active attacker
located at any single point within the network, including the user's ISP. A
major design challenge is to provide this protection without including an
application-layer proxy in data transmission. We address this challenge in path
construction by using a matchmaker node (an end host) to overlap two path
segments at a dovetail node (a router). The dovetail then trims away part of
the path so that data transmission bypasses the matchmaker. Additional design
features include the choice of many different paths through the network and the
joining of path segments without requiring a trusted third party. We develop a
systematic mechanism to measure the topological anonymity of our designs, and
we demonstrate the privacy and efficiency of our proposal by simulation, using
a model of the complete Internet at the AS-level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0354</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0354</id><created>2014-05-02</created><authors><author><keyname>Vipul</keyname><forenames>Umang</forenames></author></authors><title>Map-Reduce Parallelization of Motif Discovery</title><categories>cs.DC cs.CE</categories><comments>4 pages, 1 figure, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motif discovery is one of the most challenging problems in bioinformatics
today. DNA sequence motifs are becoming increasingly important in analysis of
gene regulation. Motifs are short, recurring patterns in DNA that have a
biological function. For example, they indicate binding sites for Transcription
Factors (TFs) and nucleases. There are a number of Motif Discovery algorithms
that run sequentially. The sequential nature stops these algorithms from being
parallelized. HOMER is one such Motif discovery tool, that we have decided to
use to overcome this limitation. To overcome this limitation, we propose a new
methodology for Motif Discovery, using HOMER, that parallelizes the task.
Parallelized version can potentially yield better scalability and performance.
To achieve this, we have decided to use sub-sampling and the Map Reduce model.
At each Map node, a sub-sampled version of the input DNA sequences is used as
input to HOMER. Subsampling at each map node is performed with different
parameters to ensure that no two HOMER instances receive identical inputs. The
output of the map phase and the input of the reduce phase is a list of Motifs
discovered using the sub-sampled sequences. The reduce phase calculates the
mode, most frequent Motifs, and outputs them as the final discovered Motifs. We
found marginal speed gains with this model of execution and substantial amount
of quality loss in discovered Motifs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0355</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0355</id><created>2014-05-02</created><authors><author><keyname>Gask&#xf3;</keyname><forenames>No&#xe9;mi</forenames></author><author><keyname>Suciu</keyname><forenames>Mihai</forenames></author><author><keyname>Lung</keyname><forenames>Rodica Ioana</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author></authors><title>Characterization and Detection of epsilon-Berge Zhukovskii Equilibria</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Berge equilibrium in the sense of Zhukovskii (Berge-Zhukovskii) is an
alternate solution concept in non-cooperative game theory that formalizes
cooperation in a noncooperative setting. In this paper the
epsilon-Berge-Zhukovskii equilibrium is introduced and characterized by using a
generative relation. A computational method for detecting
epsilon-Berge-Zhukovskii equilibrium based on evolutionary multiobjective
optimization algorithms is presented. Numerical examples are used to illustrate
the results obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0370</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0370</id><created>2014-05-02</created><updated>2014-08-04</updated><authors><author><keyname>D&#xf6;rpinghaus</keyname><forenames>Meik</forenames></author><author><keyname>Koliander</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Meyr</keyname><forenames>Heinrich</forenames></author></authors><title>Oversampling Increases the Pre-Log of Noncoherent Rayleigh Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory</comments><msc-class>94A15</msc-class><doi>10.1109/TIT.2014.2339820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the capacity of a continuous-time, time-selective, Rayleigh
block-fading channel in the high signal-to-noise ratio (SNR) regime. The fading
process is assumed stationary within each block and to change independently
from block to block; furthermore, its realizations are not known a priori to
the transmitter and the receiver (noncoherent setting). A common approach to
analyzing the capacity of this channel is to assume that the receiver performs
matched filtering followed by sampling at symbol rate (symbol matched
filtering). This yields a discrete-time channel in which each transmitted
symbol corresponds to one output sample. Liang &amp; Veeravalli (2004) showed that
the capacity of this discrete-time channel grows logarithmically with the SNR,
with a capacity pre-log equal to $1-{Q}/{N}$. Here, $N$ is the number of
symbols transmitted within one fading block, and $Q$ is the rank of the
covariance matrix of the discrete-time channel gains within each fading block.
In this paper, we show that symbol matched filtering is not a
capacity-achieving strategy for the underlying continuous-time channel.
Specifically, we analyze the capacity pre-log of the discrete-time channel
obtained by oversampling the continuous-time channel output, i.e., by sampling
it faster than at symbol rate. We prove that by oversampling by a factor two
one gets a capacity pre-log that is at least as large as $1-1/N$. Since the
capacity pre-log corresponding to symbol-rate sampling is $1-Q/N$, our result
implies indeed that symbol matched filtering is not capacity achieving at high
SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0386</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0386</id><created>2014-05-02</created><updated>2014-05-19</updated><authors><author><keyname>Huth</keyname><forenames>Michael</forenames></author><author><keyname>Kuo</keyname><forenames>Jim Huan-Pu</forenames></author><author><keyname>Piterman</keyname><forenames>Nir</forenames></author></authors><title>Fatal Attractors in Parity Games: Building Blocks for Partial Solvers</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attractors in parity games are a technical device for solving &quot;alternating&quot;
reachability of given node sets. A well known solver of parity games -
Zielonka's algorithm - uses such attractor computations recursively. We here
propose new forms of attractors that are monotone in that they are aware of
specific static patterns of colors encountered in reaching a given node set in
alternating fashion. Then we demonstrate how these new forms of attractors can
be embedded within greatest fixed-point computations to design solvers of
parity games that run in polynomial time but are partial in that they may not
decide the winning status of all nodes in the input game.
  Experimental results show that our partial solvers completely solve
benchmarks that were constructed to challenge existing full solvers. Our
partial solvers also have encouraging run times in practice. For one partial
solver we prove that its run-time is at most cubic in the number of nodes in
the parity game, that its output game is independent of the order in which
monotone attractors are computed, and that it solves all Buechi games and weak
games.
  We then define and study a transformation that converts partial solvers into
more precise partial solvers, and we prove that this transformation is sound
under very reasonable conditions on the input partial solvers. Noting that one
of our partial solvers meets these conditions, we apply its transformation on
1.6 million randomly generated games and so experimentally validate that the
transformation can be very effective in increasing the precision of partial
solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0398</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0398</id><created>2014-05-02</created><authors><author><keyname>Ebrahim</keyname><forenames>Mansoor</forenames></author><author><keyname>Khan</keyname><forenames>Shujaat</forenames></author><author><keyname>Khalid</keyname><forenames>Umer Bin</forenames></author></authors><title>Symmetric Algorithm Survey: A Comparative Analysis</title><categories>cs.CR</categories><journal-ref>International Journal of Computer Applications 61.20 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Security has become an important issue in modern world as the
popularity and infiltration of internet commerce and communication technologies
has emerged, making them a prospective medium to the security threats. To
surmount these security threats modern data communications uses cryptography an
effective, efficient and essential component for secure transmission of
information by implementing security parameter counting Confidentiality,
Authentication, accountability, and accuracy. To achieve data security
different cryptographic algorithms (Symmetric &amp; Asymmetric) are used that
jumbles data in to scribbled format that can only be reversed by the user that
have to desire key. This paper presents a comprehensive comparative analysis of
different existing cryptographic algorithms (symmetric) based on their
Architecture, Scalability, Flexibility, Reliability, Security and Limitation
that are essential for secure communication (Wired or Wireless).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0400</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0400</id><created>2014-05-02</created><authors><author><keyname>Ebrahim</keyname><forenames>Mansoor</forenames></author><author><keyname>Khan</keyname><forenames>Shujaat</forenames></author><author><keyname>Mohani</keyname><forenames>Syed Sheraz Ul Hasan</forenames></author></authors><title>Peer-to-Peer Network Simulators: an Analytical Review</title><categories>cs.NI</categories><comments>Asian Journal of Engineering, Sciences &amp; Technology 2.1 (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulators are the most dominant and eminent tool for analyzing and
investigating different type of networks. The simulations can be executed with
less cost as compared to large scale experiment as less computational resources
are required and if the simulation model is carefully designed then it can be
more practical than any well brought-up mathematical model. Generally P2P
research is based on the principle of simulate first and then experiment in the
real world and there is no reason that simulation results cannot be
reproducible. A lack of standard documentation makes verification of results
harder as well as due to such poor documentation implementation of well-known
overlay algorithms was very difficult. This Paper describes different types of
existing P2P simulators as well as provides a survey and comparison of existing
P2P simulators and extracting the best simulator among them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0406</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0406</id><created>2014-05-02</created><authors><author><keyname>Polberg</keyname><forenames>Sylwia</forenames></author></authors><title>Extension-based Semantics of Abstract Dialectical Frameworks</title><categories>cs.AI</categories><comments>To appear in the Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most prominent tools for abstract argumentation is the Dung's
framework, AF for short. It is accompanied by a variety of semantics including
grounded, complete, preferred and stable. Although powerful, AFs have their
shortcomings, which led to development of numerous enrichments. Among the most
general ones are the abstract dialectical frameworks, also known as the ADFs.
They make use of the so-called acceptance conditions to represent arbitrary
relations. This level of abstraction brings not only new challenges, but also
requires addressing existing problems in the field. One of the most
controversial issues, recognized not only in argumentation, concerns the
support cycles. In this paper we introduce a new method to ensure acyclicity of
the chosen arguments and present a family of extension-based semantics built on
it. We also continue our research on the semantics that permit cycles and fill
in the gaps from the previous works. Moreover, we provide ADF versions of the
properties known from the Dung setting. Finally, we also introduce a
classification of the developed sub-semantics and relate them to the existing
labeling-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0413</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0413</id><created>2014-05-02</created><authors><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Madanayake</keyname><forenames>A.</forenames></author><author><keyname>Potluri</keyname><forenames>U. S.</forenames></author></authors><title>Multiplierless Approximate 4-point DCT VLSI Architectures for Transform
  Block Coding</title><categories>cs.AR cs.MM cs.NA</categories><comments>5 pages, 1 figure, corrected Figure 1 (published paper in EL is
  incorrect)</comments><journal-ref>Electronics Letters, vol. 49, no. 24, pp. 1532-1534, 2013</journal-ref><doi>10.1049/el.2013.1352</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two multiplierless algorithms are proposed for 4x4 approximate-DCT for
transform coding in digital video. Computational architectures for 1-D/2-D
realisations are implemented using Xilinx FPGA devices. CMOS synthesis at the
45 nm node indicate real-time operation at 1 GHz yielding 4x4 block rates of
125 MHz at less than 120 mW of dynamic power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0417</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0417</id><created>2014-05-02</created><updated>2014-07-17</updated><authors><author><keyname>Janson</keyname><forenames>Thomas</forenames></author><author><keyname>Schindelhauer</keyname><forenames>Christian</forenames></author></authors><title>Ad-Hoc Network Unicast in Time O(log log n) using Beamforming</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the unicast problem for ad-hoc networks in the plane using
MIMO techniques. In particular, we use the multi-node beamforming gain and
present a self-synchronizing algorithm for the necessary carrier phase
synchronization. First, we consider $n$ nodes in a grid where the transmission
power per node is restricted to reach the neighboring node. We extend the idea
of multi-hop routing and relay the message by multiple nodes attaining joint
beamforming gain with higher reception range. In each round, the message is
repeated by relay nodes at dedicated positions after a fixed waiting period.
Such simple algorithms can send a message from any node to any other node in
time $\mathcal{O}(\log \log n - \log \lambda)$ and with asymptotical energy
$\mathcal{O}(\sqrt{n})$, the same energy an optimal multi-hop routing strategy
needs using short hops between source and target. Here, $\lambda$ denotes the
wavelength of the carrier. For $\lambda \in \Theta(1)$ we prove a tight lower
time bound of $\Omega(\log \log n)$. Then, we consider $n$ randomly distributed
nodes in a square of area $n$ and we show for a transmission range of
$\Theta(\sqrt{\log n})$ and for a wavelength of $\lambda =
\Omega(\log^{-1/2}n)$ that the unicast problem can be solved in
$\mathcal{O}(\log \log n)$ rounds as well. The corresponding transmission
energy increases to $\mathcal{O}(\sqrt{n} \log n)$. Finally, we present
simulation results visualizing the nature of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0423</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0423</id><created>2014-05-02</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>Representation of a Sentence using a Polar Fuzzy Neutrosophic Semantic
  Net</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:math/0101228,
  arXiv:math/0412424, arXiv:math/0306384 by other authors</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Special Issue on Natural Language Processing, Volume 4, Issue
  1, April 2014, pp. 1-8</journal-ref><doi>10.14569/SpecialIssue.2014.040101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semantic net can be used to represent a sentence. A sentence in a language
contains semantics which are polar in nature, that is, semantics which are
positive, neutral and negative. Neutrosophy is a relatively new field of
science which can be used to mathematically represent triads of concepts. These
triads include truth, indeterminacy and falsehood, and so also positivity,
neutrality and negativity. Thus a conventional semantic net has been extended
in this paper using neutrosophy into a Polar Fuzzy Neutrosophic Semantic Net. A
Polar Fuzzy Neutrosophic Semantic Net has been implemented in MATLAB and has
been used to illustrate a polar sentence in English language. The paper
demonstrates a method for the representation of polarity in a computers memory.
Thus, polar concepts can be applied to imbibe a machine such as a robot, with
emotions, making machine emotion representation possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0424</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0424</id><created>2014-05-02</created><authors><author><keyname>Bozianu</keyname><forenames>Rodica</forenames></author><author><keyname>Dima</keyname><forenames>Catalin</forenames></author><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author></authors><title>Safraless Synthesis for Epistemic Temporal Specifications</title><categories>cs.LO</categories><msc-class>68Q60, 03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the synthesis problem for specifications given in
linear temporal single-agent epistemic logic, KLTL (or $KL_1$), over
single-agent systems having imperfect information of the environment state. Van
der Meyden and Vardi have shown that this problem is 2Exptime complete.
However, their procedure relies on complex automata constructions that are
notoriously resistant to efficient implementations as they use Safra-like
determinization.
  We propose a &quot;Safraless&quot; synthesis procedure for a large fragment of KLTL.
The construction transforms first the synthesis problem into the problem of
checking emptiness for universal co-B\&quot;{u}chi tree automata using an
information-set construction. Then we build a safety game that can be solved
using an antichain-based symbolic technique exploiting the structure of the
underlying automata. The technique is implemented and applied to a couple of
case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0425</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0425</id><created>2014-05-02</created><authors><author><keyname>De Domenico</keyname><forenames>M.</forenames></author><author><keyname>Nicosia</keyname><forenames>V.</forenames></author><author><keyname>Arenas</keyname><forenames>A.</forenames></author><author><keyname>Latora</keyname><forenames>V.</forenames></author></authors><title>Layer aggregation and reducibility of multilayer interconnected networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI physics.bio-ph</categories><comments>6 pages, 4 figures</comments><journal-ref>Nature Communications 6, 6864 (2015)</journal-ref><doi>10.1038/ncomms7864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many complex systems can be represented as networks composed by distinct
layers, interacting and depending on each others. For example, in biology, a
good description of the full protein-protein interactome requires, for some
organisms, up to seven distinct network layers, with thousands of
protein-protein interactions each. A fundamental open question is then how much
information is really necessary to accurately represent the structure of a
multilayer complex system, and if and when some of the layers can indeed be
aggregated. Here we introduce a method, based on information theory, to reduce
the number of layers in multilayer networks, while minimizing information loss.
We validate our approach on a set of synthetic benchmarks, and prove its
applicability to an extended data set of protein-genetic interactions, showing
cases where a strong reduction is possible and cases where it is not. Using
this method we can describe complex systems with an optimal trade--off between
accuracy and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0446</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0446</id><created>2014-05-01</created><authors><author><keyname>Qin</keyname><forenames>Shao-Meng</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Solving the undirected feedback vertex set problem by local search</title><categories>cs.AI cond-mat.dis-nn</categories><comments>6 pages</comments><doi>10.1140/epjb/e2014-50289-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An undirected graph consists of a set of vertices and a set of undirected
edges between vertices. Such a graph may contain an abundant number of cycles,
then a feedback vertex set (FVS) is a set of vertices intersecting with each of
these cycles. Constructing a FVS of cardinality approaching the global minimum
value is a optimization problem in the nondeterministic polynomial-complete
complexity class, therefore it might be extremely difficult for some large
graph instances. In this paper we develop a simulated annealing local search
algorithm for the undirected FVS problem. By defining an order for the vertices
outside the FVS, we replace the global cycle constraints by a set of local
vertex constraints on this order. Under these local constraints the cardinality
of the focal FVS is then gradually reduced by the simulated annealing dynamical
process. We test this heuristic algorithm on large instances of Er\&quot;odos-Renyi
random graph and regular random graph, and find that this algorithm is
comparable in performance to the belief propagation-guided decimation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0455</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0455</id><created>2014-05-02</created><authors><author><keyname>Manzano</keyname><forenames>Marc</forenames></author><author><keyname>Calle</keyname><forenames>Eusebi</forenames></author><author><keyname>Ripoll</keyname><forenames>Jordi</forenames></author><author><keyname>Fagertun</keyname><forenames>Anna Manolova</forenames></author><author><keyname>Torres-Padrosa</keyname><forenames>Victor</forenames></author><author><keyname>Pahwa</keyname><forenames>Sakshi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author></authors><title>Epidemic and Cascading Survivability of Complex Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our society nowadays is governed by complex networks, examples being the
power grids, telecommunication networks, biological networks, and social
networks. It has become of paramount importance to understand and characterize
the dynamic events (e.g. failures) that might happen in these complex networks.
For this reason, in this paper, we propose two measures to evaluate the
vulnerability of complex networks in two different dynamic multiple failure
scenarios: epidemic-like and cascading failures. Firstly, we present
\emph{epidemic survivability} ($ES$), a new network measure that describes the
vulnerability of each node of a network under a specific epidemic intensity.
Secondly, we propose \emph{cascading survivability} ($CS$), which characterizes
how potentially injurious a node is according to a cascading failure scenario.
Then, we show that by using the distribution of values obtained from $ES$ and
$CS$ it is possible to describe the vulnerability of a given network. We
consider a set of 17 different complex networks to illustrate the suitability
of our proposals. Lastly, results reveal that distinct types of complex
networks might react differently under the same multiple failure scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0456</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0456</id><created>2014-05-02</created><authors><author><keyname>Grandoni</keyname><forenames>Fabrizio</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>W&#x142;odarczyk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>An LP-Rounding $2\sqrt{2}$ Approximation for Restricted Maximum Acyclic
  Subgraph</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical Maximum Acyclic Subgraph problem (MAS), given a
directed-edge weighted graph, we are required to find an ordering of the nodes
that maximizes the total weight of forward-directed edges. MAS admits a 2
approximation, and this approximation is optimal under the Unique Game
Conjecture.
  In this paper we consider a generalization of MAS, the Restricted Maximum
Acyclic Subgraph problem (RMAS), where each node is associated with a list of
integer labels, and we have to find a labeling of the nodes so as to maximize
the weight of edges whose head label is larger than the tail label. The best
known (almost trivial) approximation for RMAS is 4.
  The interest of RMAS is mostly due to its connections with the Vertex Pricing
problem (VP). In VP we are given an undirected graph with positive edge
budgets. A feasible solution consists of an assignment of non-negative prices
to the nodes. The profit for each edge $e$ is the sum of its endpoints prices
if that sum is at most the budget of $e$, and zero otherwise. Our goal is to
maximize the total profit. The best known approximation for VP, which works
analogously to the mentioned approximation algorithm for RMAS, is 4. Improving
on that is a challenging open problem. On the other hand, the best known 2
inapproximability result is due to a reduction from a special case of RMAS.
  In this paper we present an improved LP-rounding $2\sqrt{2}$ approximation
for RMAS. Our result shows that, in order to prove a 4 hardness of
approximation result for VP (if possible), one should consider reductions from
harder problems. Alternatively, our approach might suggest a different way to
design approximation algorithms for VP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0472</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0472</id><created>2014-05-02</created><updated>2014-06-15</updated><authors><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Pai</keyname><forenames>Nithish</forenames></author><author><keyname>Francis</keyname><forenames>Maria</forenames></author><author><keyname>Dubey</keyname><forenames>Abhishek</forenames></author></authors><title>Macaulay-Buchberger Basis Theorem for Residue Class Polynomial Rings
  with Torsion and Border Bases over Rings</title><categories>cs.SC math.AC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we generalize the Macaulay-Buchberger basis theorem to the
case, where the residue class polynomial ring over a Noetherian ring is not
necessarily a free module. Recently, this theorem has been extended from
polynomial rings over fields to rings, when residue class polynomial ring is
free in (Francis &amp; Dukkipati, 2014). As an application of this generalization
we develop a theory of border bases for ideals where the corresponding residue
class rings are finitely generated and have torsion. We present a border
division algorithm and prove termination of the algorithm for a special class
of border bases. We show the existence of such border bases and present some
characterizations in this regard. We also show that Pauer (2007) reduced
Gr\&quot;{o}bner bases with respect to all possible term orders is contained in this
class of border bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0483</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0483</id><created>2014-05-02</created><updated>2014-10-07</updated><authors><author><keyname>Karrer</keyname><forenames>Brian</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Percolation on sparse networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>6 pages, 1 figure, 1 table. This version includes a Supplemental
  Information section and some changes to the proofs and results in the main
  paper</comments><journal-ref>Phys. Rev. Lett. 113, 208702 (2014)</journal-ref><doi>10.1103/PhysRevLett.113.208702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study percolation on networks, which is used as a model of the resilience
of networked systems such as the Internet to attack or failure and as a simple
model of the spread of disease over human contact networks. We reformulate
percolation as a message passing process and demonstrate how the resulting
equations can be used to calculate, among other things, the size of the
percolating cluster and the average cluster size. The calculations are exact
for sparse networks when the number of short loops in the network is small, but
even on networks with many short loops we find them to be highly accurate when
compared with direct numerical simulations. By considering the fixed points of
the message passing process, we also show that the percolation threshold on a
network with few loops is given by the inverse of the leading eigenvalue of the
so-called non-backtracking matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0500</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0500</id><created>2014-05-02</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Riley</keyname><forenames>Michael D.</forenames></author></authors><title>On the Disambiguation of Weighted Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a disambiguation algorithm for weighted automata. The algorithm
admits two main stages: a pre-disambiguation stage followed by a transition
removal stage. We give a detailed description of the algorithm and the proof of
its correctness. The algorithm is not applicable to all weighted automata but
we prove sufficient conditions for its applicability in the case of the
tropical semiring by introducing the *weak twins property*. In particular, the
algorithm can be used with all acyclic weighted automata, relevant to
applications. While disambiguation can sometimes be achieved using
determinization, our disambiguation algorithm in some cases can return a result
that is exponentially smaller than any equivalent deterministic automaton. We
also present some empirical evidence of the space benefits of disambiguation
over determinization in speech recognition and machine translation
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0501</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0501</id><created>2014-05-02</created><authors><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author><author><keyname>Domingos</keyname><forenames>Pedro</forenames></author></authors><title>Exchangeable Variable Models</title><categories>cs.LG cs.AI</categories><comments>ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequence of random variables is exchangeable if its joint distribution is
invariant under variable permutations. We introduce exchangeable variable
models (EVMs) as a novel class of probabilistic models whose basic building
blocks are partially exchangeable sequences, a generalization of exchangeable
sequences. We prove that a family of tractable EVMs is optimal under zero-one
loss for a large class of functions, including parity and threshold functions,
and strictly subsumes existing tractable independence-based model families.
Extensive experiments show that EVMs outperform state of the art classifiers
such as SVMs and probabilistic models which are solely based on independence
assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0514</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0514</id><created>2014-05-02</created><updated>2014-11-27</updated><authors><author><keyname>Marusic</keyname><forenames>Ines</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Complexity of Equivalence and Learning for Multiplicity Tree Automata</title><categories>cs.LG cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the complexity of equivalence and learning for multiplicity tree
automata, i.e., weighted tree automata over a field. We first show that the
equivalence problem is logspace equivalent to polynomial identity testing, the
complexity of which is a longstanding open problem. Secondly, we derive lower
bounds on the number of queries needed to learn multiplicity tree automata in
Angluin's exact learning model, over both arbitrary and fixed fields.
  Habrard and Oncina (2006) give an exact learning algorithm for multiplicity
tree automata, in which the number of queries is proportional to the size of
the target automaton and the size of a largest counterexample, represented as a
tree, that is returned by the Teacher. However, the smallest
tree-counterexample may be exponential in the size of the target automaton.
Thus the above algorithm does not run in time polynomial in the size of the
target automaton, and has query complexity exponential in the lower bound.
  Assuming a Teacher that returns minimal DAG representations of
counterexamples, we give a new exact learning algorithm whose query complexity
is quadratic in the target automaton size, almost matching the lower bound, and
improving the best previously-known algorithm by an exponential factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0521</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0521</id><created>2014-05-02</created><updated>2016-03-06</updated><authors><author><keyname>Lashgari</keyname><forenames>Sina</forenames></author><author><keyname>Avestimehr</keyname><forenames>Amir Salman</forenames></author></authors><title>Blind MIMOME Wiretap Channel with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>This work has been presented in part at the IEEE International
  Symposium on Information Theory 2014 and IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Gaussian MIMOME wiretap channel where a transmitter wishes to
communicate a confidential message to a legitimate receiver in the presence of
eavesdroppers, while the eavesdroppers should not be able to decode the
confidential message. Each node in the network is equipped with arbitrary
number of antennas. Furthermore, channels are time varying, and there is no
channel state information available at the transmitter (CSIT) with respect to
eavesdroppers' channels; and transmitter only has access to delayed CSIT of the
channel to the legitimate receiver. The secure degrees of freedom (SDoF) for
such network has only been characterized for special cases, and is unknown in
general. We completely characterize the SDoF of this network for all antenna
configurations. In particular, we strictly improve the state-of-the-art
achievable scheme for this network by proposing more efficient artificial noise
alignment at the receivers. Furthermore, we develop a tight upper bound by
utilizing 4 important inequalities that provide lower bounds on the received
signal dimensions at receivers which supply delayed CSIT or no CSIT, or at a
collection of receivers where some supply no CSIT. These inequalities together
allow for analysis of signal dimensions in networks with asymmetric CSIT; and
as a result, we present a converse proof that leads to characterization of SDoF
for all possible antenna configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0524</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0524</id><created>2014-05-02</created><updated>2014-05-18</updated><authors><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>Computational Complexity of Approximate Nash Equilibrium in Large Games</title><categories>cs.GT</categories><comments>New version includes an addendum about subsequent work on the open
  problems proposed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that finding an epsilon-Nash equilibrium in a succinctly
representable game with many players is PPAD-hard for constant epsilon. Our
proof uses succinct games, i.e. games whose payoff function is represented by a
circuit. Our techniques build on a recent query complexity lower bound by
Babichenko.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0527</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0527</id><created>2014-05-02</created><updated>2014-09-05</updated><authors><author><keyname>Chen</keyname><forenames>Moya</forenames></author><author><keyname>Xin</keyname><forenames>Doris</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>Parallel computation using active self-assembly</title><categories>cs.ET cs.CC cs.DS cs.RO</categories><comments>Journal version to appear in Natural Computing. Earlier conference
  version appeared at DNA19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of the recently proposed nubot model of
molecular-scale self-assembly. The model generalises asynchronous cellular
automata to have non-local movement where large assemblies of molecules can be
pushed and pulled around, analogous to millions of molecular motors in animal
muscle effecting the rapid movement of macroscale arms and legs. We show that
the nubot model is capable of simulating Boolean circuits of polylogarithmic
depth and polynomial size, in only polylogarithmic expected time. In
computational complexity terms, we show that any problem from the complexity
class NC is solvable in polylogarithmic expected time and polynomial workspace
using nubots.
  Along the way, we give fast parallel nubot algorithms for a number of
problems including line growth, sorting, Boolean matrix multiplication and
space-bounded Turing machine simulation, all using a constant number of nubot
states (monomer types). Circuit depth is a well-studied notion of parallel
time, and our result implies that the nubot model is a highly parallel model of
computation in a formal sense. Asynchronous cellular automata are not capable
of this parallelism, and our result shows that adding a rigid-body movement
primitive to such a model, to get the nubot model, drastically increases
parallel processing abilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0533</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0533</id><created>2014-05-02</created><authors><author><keyname>Robertson</keyname><forenames>Neil</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Girth six cubic graphs have Petersen minors</title><categories>math.CO cs.DM</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every 3-regular graph with no circuit of length less than six
has a subgraph isomorphic to a subdivision of the Petersen graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0534</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0534</id><created>2014-05-02</created><updated>2014-12-10</updated><authors><author><keyname>Courtois</keyname><forenames>Nicolas T.</forenames></author></authors><title>On The Longest Chain Rule and Programmed Self-Destruction of Crypto
  Currencies</title><categories>cs.CR cs.CE</categories><comments>89 pages, work in progress, the author's blog is
  blog.bettercrypto.com</comments><acm-class>D.4.6; K.4.1; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we revisit some major orthodoxies which lie at the heart of the
bitcoin crypto currency and its numerous clones. In particular we look at The
Longest Chain Rule, the monetary supply policies and the exact mechanisms which
implement them. We claim that these built-in properties are not as brilliant as
they are sometimes claimed. A closer examination reveals that they are closer
to being... engineering mistakes which other crypto currencies have copied
rather blindly. More precisely we show that the capacity of current crypto
currencies to resist double spending attacks is poor and most current crypto
currencies are highly vulnerable. Satoshi did not implement a timestamp for
bitcoin transactions and the bitcoin software does not attempt to monitor
double spending events. As a result major attacks involving hundreds of
millions of dollars can occur and would not even be recorded. Hundreds of
millions have been invested to pay for ASIC hashing infrastructure yet
insufficient attention was paid to network neutrality and to insure that the
protection layer it promises is effective and cannot be abused. In this paper
we develop a theory of Programmed Self-Destruction of crypto currencies. We
observe that most crypto currencies have mandated abrupt and sudden
transitions. These affect their hash rate and therefore their protection
against double spending attacks which we do not limit the to the notion of 51%
attacks which is highly misleading. In addition we show that smaller bitcoin
competitors are substantially more vulnerable. In addition to small hash rate,
many bitcoin competitors mandate incredibly important adjustments in miner
reward. We exhibit examples of 'alt-coins' which validate our theory and for
which the process of programmed decline and rapid self-destruction has clearly
already started.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0538</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0538</id><created>2014-05-02</created><updated>2014-11-21</updated><authors><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Kajdanowicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Seed Selection for Spread of Influence in Social Networks: Temporal vs.
  Static Approach</title><categories>cs.SI physics.soc-ph</categories><journal-ref>New Generation Computing, Vol. 32, Issue 3-4, pp. 213-235, 2014</journal-ref><doi>10.1007/s00354-014-0402-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding optimal set of users for influencing others in the
social network has been widely studied. Because it is NP-hard, some heuristics
were proposed to find sub-optimal solutions. Still, one of the commonly used
assumption is the one that seeds are chosen on the static network, not the
dynamic one. This static approach is in fact far from the real-world networks,
where new nodes may appear and old ones dynamically disappear in course of
time.
  The main purpose of this paper is to analyse how the results of one of the
typical models for spread of influence - linear threshold - differ depending on
the strategy of building the social network used later for choosing seeds. To
show the impact of network creation strategy on the final number of influenced
nodes - outcome of spread of influence, the results for three approaches were
studied: one static and two temporal with different granularities, i.e. various
number of time windows. Social networks for each time window encapsulated
dynamic changes in the network structure. Calculation of various node
structural measures like degree or betweenness respected these changes by means
of forgetting mechanism - more recent data had greater influence on node
measure values. These measures were, in turn, used for node ranking and their
selection for seeding.
  All concepts were applied to experimental verification on five real datasets.
The results revealed that temporal approach is always better than static and
the higher granularity in the temporal social network while seeding, the more
finally influenced nodes. Additionally, outdegree measure with exponential
forgetting typically outperformed other time-dependent structural measures, if
used for seed candidate ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0541</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0541</id><created>2014-05-02</created><authors><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames></author></authors><title>How many times do we need and assumption ?</title><categories>cs.LO</categories><comments>15 pages, submitted to a workshop in Theoretical Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present a class of formulas Fn, n in Nat, that need at
least 2^n assumptions to be proved in a normal proof in Natural Deduction for
purely implicational minimal propositional logic. In purely implicational
classical propositional logic, with Peirce's rule, each Fn is proved with only
one assumption in Natural Deduction in a normal proof. Hence, the formulas Fn
have exponentially sized proofs in cut-free Sequent Calculus and Tableaux. In
fact 2^n is the lower-bound for normal proofs in ND, cut-free Sequent proofs
and Tableaux. We discuss the consequences of the existence of this class of
formulas for designing automatic proof-procedures based on these deductive
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0545</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0545</id><created>2014-05-02</created><authors><author><keyname>Gepshtein</keyname><forenames>Sergei</forenames></author><author><keyname>Tyukin</keyname><forenames>Ivan</forenames></author></authors><title>Optimal measurement of visual motion across spatial and temporal scales</title><categories>cs.CV q-bio.NC</categories><comments>28 pages, 10 figures, 2 appendices; in press in Favorskaya MN and
  Jain LC (Eds), Computer Vision in Advanced Control Systems using Conventional
  and Intelligent Paradigms, Intelligent Systems Reference Library,
  Springer-Verlag, Berlin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensory systems use limited resources to mediate the perception of a great
variety of objects and events. Here a normative framework is presented for
exploring how the problem of efficient allocation of resources can be solved in
visual perception. Starting with a basic property of every measurement,
captured by Gabor's uncertainty relation about the location and frequency
content of signals, prescriptions are developed for optimal allocation of
sensors for reliable perception of visual motion. This study reveals that a
large-scale characteristic of human vision (the spatiotemporal contrast
sensitivity function) is similar to the optimal prescription, and it suggests
that some previously puzzling phenomena of visual sensitivity, adaptation, and
perceptual organization have simple principled explanations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0546</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0546</id><created>2014-05-02</created><updated>2014-05-09</updated><authors><author><keyname>Puurula</keyname><forenames>Antti</forenames></author><author><keyname>Read</keyname><forenames>Jesse</forenames></author><author><keyname>Bifet</keyname><forenames>Albert</forenames></author></authors><title>Kaggle LSHTC4 Winning Solution</title><categories>cs.AI cs.CL cs.IR</categories><comments>Kaggle LSHTC winning solution description</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our winning submission to the 2014 Kaggle competition for Large Scale
Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of
sparse generative models extending Multinomial Naive Bayes. The
base-classifiers consist of hierarchically smoothed models combining document,
label, and hierarchy level Multinomials, with feature pre-processing using
variants of TF-IDF and BM25. Additional diversification is introduced by
different types of folds and random search optimization for different measures.
The ensemble algorithm optimizes macroFscore by predicting the documents for
each label, instead of the usual prediction of labels per document. Scores for
documents are predicted by weighted voting of base-classifier outputs with a
variant of Feature-Weighted Linear Stacking. The number of documents per label
is chosen using label priors and thresholding of vote scores. This document
describes the models and software used to build our solution. Reproducing the
results for our solution can be done by running the scripts included in the
Kaggle package. A package omitting precomputed result files is also
distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0
for Weka and Meka dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0549</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0549</id><created>2014-05-02</created><authors><author><keyname>Soliman</keyname><forenames>Omar S.</forenames></author><author><keyname>AboElhamd</keyname><forenames>Eman</forenames></author></authors><title>Classification of Diabetes Mellitus using Modified Particle Swarm
  Optimization and Least Squares Support Vector Machine</title><categories>cs.CE cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Diabetes Mellitus is a major health problem all over the world. Many
classification algorithms have been applied for its diagnoses and treatment. In
this paper, a hybrid algorithm of Modified-Particle Swarm Optimization and
Least Squares- Support Vector Machine is proposed for the classification of
type II DM patients. LS-SVM algorithm is used for classification by finding
optimal hyper-plane which separates various classes. Since LS-SVM is so
sensitive to the changes of its parameter values, Modified-PSO algorithm is
used as an optimization technique for LS-SVM parameters. This will Guarantee
the robustness of the hybrid algorithm by searching for the optimal values for
LS-SVM parameters. The pro-posed Algorithm is implemented and evaluated using
Pima Indians Diabetes Data set from UCI repository of machine learning
databases. It is also compared with different classifier algorithms which were
applied on the same database. The experimental results showed the superiority
of the proposed algorithm which could achieve an average classification
accuracy of 97.833%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0554</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0554</id><created>2014-05-03</created><authors><author><keyname>Pang</keyname><forenames>Jun</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Liu</keyname><forenames>Yang</forenames><affiliation>Nanyang Technological University</affiliation></author></authors><title>Proceedings Third International Workshop on Engineering Safety and
  Security Systems</title><categories>cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 150, 2014</journal-ref><doi>10.4204/EPTCS.150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The International Workshop on Engineering Safety and Security Systems (ESSS)
aims at contributing to the challenge of constructing reliable and secure
systems. The workshop covers areas such as formal specification, type checking,
model checking, program analysis/transformation, model-based testing and
model-driven software construction. The workshop will bring together
researchers and industry R&amp;D expertise together to exchange their knowledge,
discuss their research findings, and explore potential collaborations. The main
theme of the workshop is methods and techniques for constructing large reliable
and secure systems. The goal of the workshop is to establish a platform for the
exchange of ideas, discussion, cross-fertilization, inspiration, co-operation,
and dissemination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0559</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0559</id><created>2014-05-03</created><authors><author><keyname>Rybakov</keyname><forenames>Vladimir</forenames></author></authors><title>A Note on Parameterised Knowledge Operations in Temporal Logic</title><categories>cs.LO</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider modeling the conception of knowledge in terms of temporal logic.
The study of knowledge logical operations is originated around 1962 by
representation of knowledge and belief using modalities. Nowadays, it is very
good established area. However, we would like to look to it from a bit another
point of view, our paper models knowledge in terms of linear temporal logic
with {\em past}. We consider various versions of logical knowledge operations
which may be defined in this framework. Technically, semantics, language and
temporal knowledge logics based on our approach are constructed. Deciding
algorithms are suggested, unification in terms of this approach is commented.
This paper does not offer strong new technical outputs, instead we suggest new
approach to conception of knowledge (in terms of time).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0560</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0560</id><created>2014-05-03</created><authors><author><keyname>Sharma</keyname><forenames>Vidit</forenames></author><author><keyname>Garg</keyname><forenames>Ashish</forenames></author></authors><title>Numerical Investigation of Effects of Compound Angle and Length to
  Diameter Ratio on Adiabatic Film Cooling Effectiveness</title><categories>cs.CE cs.NA</categories><comments>13 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modification has been done in the normal injection hole of 35 degree, by
injecting the cold fluid at different angles(compound angle) in lateral
direction, providing a significant change in the shape of holes which later we
found in our numerical investigation giving good quality of effectiveness in
cooling. Different L/D ratios are also studied for each compound angle. The
numerical simulation is performed based on Reynolds Averaged
Navier-Stokes(RANS) equations with k-epsilon turbulence model by using
Fluent(Commercial Software). Adiabatic Film Cooling Effectiveness has been
studied for compound angles of (0, 30, 45 and 60 degrees) and L/D ratios of (1,
2, 3 and 4) on a hole of 6mm diameter with blowing ratio 0.5. The findings are
obtained from the results, concludes that the trend of laterally averaged
adiabatic effectiveness is the function of L/D ratio and compound angle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0562</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0562</id><created>2014-05-03</created><authors><author><keyname>Sin'ya</keyname><forenames>Ryoma</forenames></author><author><keyname>Matsuzaki</keyname><forenames>Kiminori</forenames></author><author><keyname>Sassa</keyname><forenames>Masataka</forenames></author></authors><title>Simultaneous Finite Automata: An Efficient Data-Parallel Model for
  Regular Expression Matching</title><categories>cs.FL</categories><comments>This paper has been accepted at the following conference: 2013
  International Conference on Parallel Processing (ICPP- 2013), October 1-4,
  2013 Ecole Normale Suprieure de Lyon, Lyon, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automata play important roles in wide area of computing and the growth of
multicores calls for their efficient parallel implementation. Though it is
known in theory that we can perform the computation of a finite automaton in
parallel by simulating transitions, its implementation has a large overhead due
to the simulation. In this paper we propose a new automaton called simultaneous
finite automaton (SFA) for efficient parallel computation of an automaton. The
key idea is to extend an automaton so that it involves the simulation of
transitions. Since an SFA itself has a good property of parallelism, we can
develop easily a parallel implementation without overheads. We have implemented
a regular expression matcher based on SFA, and it has achieved over 10-times
speedups on an environment with dual hexa-core CPUs in a typical case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0573</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0573</id><created>2014-05-03</created><authors><author><keyname>Antiqueira</keyname><forenames>Lucas</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>Spatial Neural Networks and their Functional Samples: Similarities and
  Differences</title><categories>cs.NE q-bio.NC</categories><comments>10 pages, 6 figures (submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models of neural networks have proven their utility in the development of
learning algorithms in computer science and in the theoretical study of brain
dynamics in computational neuroscience. We propose in this paper a spatial
neural network model to analyze the important class of functional networks,
which are commonly employed in computational studies of clinical brain imaging
time series. We developed a simulation framework inspired by multichannel brain
surface recordings (more specifically, EEG -- electroencephalogram) in order to
link the mesoscopic network dynamics (represented by sampled functional
networks) and the microscopic network structure (represented by an
integrate-and-fire neural network located in a 3D space -- hence the term
spatial neural network). Functional networks are obtained by computing pairwise
correlations between time-series of mesoscopic electric potential dynamics,
which allows the construction of a graph where each node represents one
time-series. The spatial neural network model is central in this study in the
sense that it allowed us to characterize sampled functional networks in terms
of what features they are able to reproduce from the underlying spatial
network. Our modeling approach shows that, in specific conditions of sample
size and edge density, it is possible to precisely estimate several network
measurements of spatial networks by just observing functional samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0580</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0580</id><created>2014-05-03</created><authors><author><keyname>Kaur</keyname><forenames>Prabhjot</forenames></author></authors><title>Web Content Classification: A Survey</title><categories>cs.IR</categories><comments>5 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1307.1024, arXiv:1310.4647 by other authors</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V10(2):97-101, Apr 2014</journal-ref><doi>10.14445/22312803/IJCTT-V10P117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the information contained within the web is increasing day by day,
organizing this information could be a necessary requirement.The data mining
process is to extract information from a data set and transform it into an
understandable structure for further use. Classification of web page content is
essential to many tasks in web information retrieval such as maintaining web
directories and focused crawling.The uncontrolled type of nature of web content
presents additional challenges to web page classification as compared to the
traditional text classification, but the interconnected nature of hypertext
also provides features that can assist the process. In this paper the web
classification is discussed in detail and its importance in field of data
mining is explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0586</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0586</id><created>2014-05-03</created><updated>2014-05-06</updated><authors><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author></authors><title>On Lipschitz Continuity and Smoothness of Loss Functions in Learning to
  Rank</title><categories>cs.LG stat.ML</categories><comments>Abstract edited</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In binary classification and regression problems, it is well understood that
Lipschitz continuity and smoothness of the loss function play key roles in
governing generalization error bounds for empirical risk minimization
algorithms. In this paper, we show how these two properties affect
generalization error bounds in the learning to rank problem. The learning to
rank problem involves vector valued predictions and therefore the choice of the
norm with respect to which Lipschitz continuity and smoothness are defined
becomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitz
continuity allows us to improve existing bounds. Furthermore, under smoothness
assumptions, our choice enables us to prove rates that interpolate between
$1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular
learning to rank method, gives state-of-the-art performance guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0591</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0591</id><created>2014-05-03</created><authors><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Perceptron-like Algorithms and Generalization Bounds for Learning to
  Rank</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to rank is a supervised learning problem where the output space is
the space of rankings but the supervision space is the space of relevance
scores. We make theoretical contributions to the learning to rank problem both
in the online and batch settings. First, we propose a perceptron-like algorithm
for learning a ranking function in an online setting. Our algorithm is an
extension of the classic perceptron algorithm for the classification problem.
Second, in the setting of batch learning, we introduce a sufficient condition
for convex ranking surrogates to ensure a generalization bound that is
independent of number of objects per query. Our bound holds when linear ranking
functions are used: a common practice in many learning to rank algorithms. En
route to developing the online algorithm and generalization bound, we propose a
novel family of listwise large margin ranking surrogates. Our novel surrogate
family is obtained by modifying a well-known pairwise large margin ranking
surrogate and is distinct from the listwise large margin surrogates developed
using the structured prediction framework. Using the proposed family, we
provide a guaranteed upper bound on the cumulative NDCG (or MAP) induced loss
under the perceptron-like algorithm. We also show that the novel surrogates
satisfy the generalization bound condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0597</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0597</id><created>2014-05-03</created><updated>2014-07-31</updated><authors><author><keyname>Alinia</keyname><forenames>Bahram</forenames></author><author><keyname>Hajiesmaili</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author></authors><title>On the Construction of Maximum-Quality Aggregation Trees in
  Deadline-Constrained WSNs</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In deadline-constrained data aggregation in wireless sensor networks (WSNs),
the imposed sink deadline along with the interference constraint hinders
participation of all sensor nodes in data aggregation. Thus, exploiting the
wisdom of the crowd paradigm, the total number of participant nodes in data
aggregation determines the quality of aggregation ($QoA$). Although the
previous studies have proposed optimal algorithms to maximize $QoA$ under an
imposed deadline and a given aggregation tree, there is no work on constructing
optimal tree in this context. In this paper, we cast an optimization problem to
address optimal tree construction for deadline-constrained data aggregation in
WSNs. We demonstrate that the ratio between the maximum achievable $QoA$s of
the optimal and the worst aggregation trees is as large as $O(2^D)$, where $D$
is the sink deadline and thus makes devising efficient solution of the problem
an issue of paramount value. However, the problem is challenging to solve since
we prove that it is NP-hard. We apply the recently-proposed Markov
approximation framework to devise two distributed algorithms with different
computation overheads that converge to a bounded neighborhood of the optimal
solution. Extensive simulations in a set of representative randomly-generated
scenarios show that the proposed algorithms significantly improve $QoA$ by %101
and %93 in average compared to the best, to our knowledge, existing alternative
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0601</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0601</id><created>2014-05-03</created><authors><author><keyname>Xiong</keyname><forenames>Xuehan</forenames></author><author><keyname>De la Torre</keyname><forenames>Fernando</forenames></author></authors><title>Supervised Descent Method for Solving Nonlinear Least Squares Problems
  in Computer Vision</title><categories>cs.CV</categories><comments>15 pages. In submission to TPAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision problems (e.g., camera calibration, image alignment,
structure from motion) are solved with nonlinear optimization methods. It is
generally accepted that second order descent methods are the most robust, fast,
and reliable approaches for nonlinear optimization of a general smooth
function. However, in the context of computer vision, second order descent
methods have two main drawbacks: (1) the function might not be analytically
differentiable and numerical approximations are impractical, and (2) the
Hessian may be large and not positive definite. To address these issues, this
paper proposes generic descent maps, which are average &quot;descent directions&quot; and
rescaling factors learned in a supervised fashion. Using generic descent maps,
we derive a practical algorithm - Supervised Descent Method (SDM) - for
minimizing Nonlinear Least Squares (NLS) problems. During training, SDM learns
a sequence of decent maps that minimize the NLS. In testing, SDM minimizes the
NLS objective using the learned descent maps without computing the Jacobian or
the Hessian. We prove the conditions under which the SDM is guaranteed to
converge. We illustrate the effectiveness and accuracy of SDM in three computer
vision problems: rigid image alignment, non-rigid image alignment, and 3D pose
estimation. In particular, we show how SDM achieves state-of-the-art
performance in the problem of facial feature detection. The code has been made
available at www.humansensing.cs.cmu.edu/intraface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0603</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0603</id><created>2014-05-03</created><authors><author><keyname>Makazhanov</keyname><forenames>Aibek</forenames></author><author><keyname>Barbosa</keyname><forenames>Denilson</forenames></author><author><keyname>Kondrak</keyname><forenames>Grzegorz</forenames></author></authors><title>Extracting Family Relationship Networks from Novels</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to the extraction of family relations from literary
narrative, which incorporates a technique for utterance attribution proposed
recently by Elson and McKeown (2010). In our work this technique is used in
combination with the detection of vocatives - the explicit forms of address
used by the characters in a novel. We take advantage of the fact that certain
vocatives indicate family relations between speakers. The extracted relations
are then propagated using a set of rules. We report the results of the
application of our method to Jane Austen's Pride and Prejudice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0616</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0616</id><created>2014-05-03</created><authors><author><keyname>Brofos</keyname><forenames>James</forenames></author><author><keyname>Kannan</keyname><forenames>Ajay</forenames></author><author><keyname>Shu</keyname><forenames>Rui</forenames></author></authors><title>Automated Attribution and Intertextual Analysis</title><categories>cs.CL cs.DL stat.ML</categories><comments>10 pages, 4 tables, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we employ quantitative methods from the realm of statistics and
machine learning to develop novel methodologies for author attribution and
textual analysis. In particular, we develop techniques and software suitable
for applications to Classical study, and we illustrate the efficacy of our
approach in several interesting open questions in the field. We apply our
numerical analysis techniques to questions of authorship attribution in the
case of the Greek tragedian Euripides, to instances of intertextuality and
influence in the poetry of the Roman statesman Seneca the Younger, and to cases
of &quot;interpolated&quot; text with respect to the histories of Livy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0625</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0625</id><created>2014-05-03</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Ruogu</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Atilla</forenames></author></authors><title>Throughput-Optimal Scheduling Design with Regular Service Guarantees in
  Wireless Networks</title><categories>cs.NI</categories><comments>This work has been submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the regular service requirements of video applications for
improving Quality-of-Experience (QoE) of users, we consider the design of
scheduling strategies in multi-hop wireless networks that not only maximize
system throughput but also provide regular inter-service times for all links.
Since the service regularity of links is related to the higher-order statistics
of the arrival process and the policy operation, it is highly challenging to
characterize and analyze directly. We overcome this obstacle by introducing a
new quantity, namely the time-since-last-service (TSLS), which tracks the time
since the last service. By combining it with the queue-length in the weight, we
propose a novel maximum-weight type scheduling policy, called Regular Service
Guarantee (RSG) Algorithm. The unique evolution of the TSLS counter poses
significant challenges for the analysis of the RSG Algorithm.
  To tackle these challenges, we first propose a novel Lyapunov function to
show the throughput optimality of the RSG Algorithm. Then, we prove that the
RSG Algorithm can provide service regularity guarantees by using the
Lyapunov-drift based analysis of the steady-state behavior of the stochastic
processes. In particular, our algorithm can achieve a degree of service
regularity within a factor of a fundamental lower bound we derive. This factor
is a function of the system statistics and design parameters and can be as low
as two in some special networks. Our results, both analytical and numerical,
exhibit significant service regularity improvements over the traditional
throughput-optimal policies, which reveals the importance of incorporating the
metric of time-since-last-service into the scheduling policy for providing
regulated service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0628</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0628</id><created>2014-05-03</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames></author><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames></author><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Kumar</keyname><forenames>K. Narayan</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Infinite-State Energy Games</title><categories>cs.GT cs.FL</categories><comments>11 pages</comments><report-no>EDI-INF-RR-1419</report-no><msc-class>91A43</msc-class><acm-class>F.3.1</acm-class><journal-ref>Full version (including proofs) of material presented at CSL-LICS
  2014 (Vienna, Austria)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Energy games are a well-studied class of 2-player turn-based games on a
finite graph where transitions are labeled with integer vectors which represent
changes in a multidimensional resource (the energy). One player tries to keep
the cumulative changes non-negative in every component while the other tries to
frustrate this. We consider generalized energy games played on infinite game
graphs induced by pushdown automata (modelling recursion) or their subclass of
one-counter automata. Our main result is that energy games are decidable in the
case where the game graph is induced by a one-counter automaton and the energy
is one-dimensional. On the other hand, every further generalization is
undecidable: Energy games on one-counter automata with a 2-dimensional energy
are undecidable, and energy games on pushdown automata are undecidable even if
the energy is one-dimensional. Furthermore, we show that energy games and
simulation games are inter-reducible, and thus we additionally obtain several
new (un)decidability results for the problem of checking simulation preorder
between pushdown automata and vector addition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0631</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0631</id><created>2014-05-03</created><updated>2014-05-05</updated><authors><author><keyname>Jeyakumar</keyname><forenames>Vimalkumar</forenames></author><author><keyname>Kabbani</keyname><forenames>Abdul</forenames></author><author><keyname>Mogul</keyname><forenames>Jeffrey C.</forenames></author><author><keyname>Vahdat</keyname><forenames>Amin</forenames></author></authors><title>Flexible Network Bandwidth and Latency Provisioning in the Datacenter</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Predictably sharing the network is critical to achieving high utilization in
the datacenter. Past work has focussed on providing bandwidth to endpoints, but
often we want to allocate resources among multi-node services. In this paper,
we present Parley, which provides service-centric minimum bandwidth guarantees,
which can be composed hierarchically. Parley also supports service-centric
weighted sharing of bandwidth in excess of these guarantees. Further, we show
how to configure these policies so services can get low latencies even at high
network load. We evaluate Parley on a multi-tiered oversubscribed network
connecting 90 machines, each with a 10Gb/s network interface, and demonstrate
that Parley is able to meet its goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0632</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0632</id><created>2014-05-03</created><authors><author><keyname>Mastriani</keyname><forenames>Mario</forenames></author></authors><title>Rule of Three for Superresolution of Still Images with Applications to
  Compression and Denoising</title><categories>cs.CV</categories><comments>24 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new method for superresolution of still images (in the wavelet
domain) based on the reconstruction of missing details subbands pixels at a
given ith level via Rule of Three (Ro3) between pixels of approximation subband
of such level, and pixels of approximation and detail subbands of (i+1)th
level. The histogramic profiles demonstrate that Ro3 is the appropriate
mechanism to recover missing detail subband pixels in these cases. Besides,
with the elimination of the details subbands pixels (in an eventual compression
scheme), we obtain a bigger compression rate. Experimental results demonstrate
that our approach compares favorably to more typical methods of denoising and
compression in wavelet domain. Our method does not compress, but facilitates
the action of the real compressor, in our case, Joint Photographic Experts
Group (JPEG) and JPEg2000, that is, Ro3 acts as a catalyst compression
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0637</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0637</id><created>2014-05-03</created><authors><author><keyname>Nowlan</keyname><forenames>Michael F.</forenames></author><author><keyname>Faleiro</keyname><forenames>Jose</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Crux: Locality-Preserving Distributed Systems</title><categories>cs.DC</categories><comments>9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed systems achieve scalability by balancing load across many
machines, but wide-area distribution can introduce worst-case response
latencies proportional to the network's delay diameter. Crux is a general
framework to build locality-preserving distributed systems, by transforming
some existing scalable distributed algorithm A into a new algorithm A' that
guarantees for any two clients u and v interacting via service requests to A',
these interactions exhibit worst-case response latencies proportional to the
network delay between u and v. Locality-preserving PlanetLab deployments of a
memcached distributed cache, a bamboo distributed hash table, and a redis
publish/subscribe service indicate that Crux is effective and applicable to a
variety of existing distributed algorithms. Crux achieves several orders of
magnitude latency improvement for localized interactions at the cost of
increasing per-node overheads, as each physical node must participate in
multiple instances of algorithm A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0641</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0641</id><created>2014-05-03</created><authors><author><keyname>Wan</keyname><forenames>Xiaojun</forenames></author></authors><title>x-index: a fantastic new indicator for quantifying a scientist's
  scientific impact</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  h-index has become the most popular indicator for quantifying a scientist's
scientific impact in various scientific fields. h-index is defined as the
largest number of papers with citation number larger than or equal to h and it
treats each citation equally. However, different citations usually come from
different papers with different influence and quality, and a citation from a
highly influential paper is a greater recognition of the target paper than a
citation from an ordinary paper. Based on this assumption, we proposed a new
indicator named x-index to quantify a scientist's scientific impact by
considering only the citations coming from influential papers. x-index is
defined as the largest number of papers with influential citation number larger
than or equal to x, where each influential citation comes from a paper for
which the average ACNPP (Average Citation Number Per Paper) of its authors
larger than or equal to x . Through analysis on the APS dataset, we find that
the proposed x-index has much better ability to discriminate between Physics
Prize Winners and ordinary physicists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0647</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0647</id><created>2014-05-04</created><authors><author><keyname>Ziani</keyname><forenames>Djamal</forenames></author></authors><title>Feature Selection On Boolean Symbolic Objects</title><categories>cs.IR cs.AI</categories><comments>20 pages, 10 figures</comments><acm-class>H.3.3; I.2.11; I.5.2; I.5.3</acm-class><journal-ref>Ziani D., Feature Selection on Boolean Symbolic Objects, in
  International Journal of Computational Sciences and Information Technology
  (IJCSITY), November 2013, volume 1, Number 4, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the boom in IT technology, the data sets used in application are more
and more larger and are described by a huge number of attributes, therefore,
the feature selection become an important discipline in Knowledge discovery and
data mining, allowing the experts to select the most relevant features to
improve the quality of their studies and to reduce the time processing of their
algorithm. In addition to that, the data used by the applications become
richer. They are now represented by a set of complex and structured objects,
instead of simple numerical matrixes. The purpose of our algorithm is to do
feature selection on rich data, called Boolean Symbolic Objects (BSOs). These
objects are described by multivalued features. The BSOs are considered as
higher level units which can model complex data, such as cluster of
individuals, aggregated data or taxonomies. In this paper we will introduce a
new feature selection criterion for BSOs, and we will explain how we improved
its complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0650</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0650</id><created>2014-05-04</created><authors><author><keyname>Ziani</keyname><forenames>Djamal</forenames></author></authors><title>Configuration in ERP SaaS Multi-Tenancy</title><categories>cs.SE</categories><comments>17 pages, 9 figures</comments><msc-class>68M01</msc-class><acm-class>K.6.3; K.8.1; D.2.13; D.2.9</acm-class><journal-ref>D. Ziani, &quot;Configuration in ERP SaaS Multi-Tenancy&quot;,International
  Journal of Computer Science, Engineering and Information Technology
  (IJCSEIT), April 2014,Volume 4, Number 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software as a Service (SaaS) becomes in this decade the focus of many
enterprises and research. SaaS provides software application as Web based
delivery to server many customers. This sharing of infrastructure and
application provided by Saas has a great benefit to customers, since it reduces
costs, minimizes risks, improves their competitive positioning, as well as
seeks out innovative. SaaS application is generally developed with standardized
software functionalities to serve as many customers as possible. However many
customers ask to change the standardized provided functions according to their
specific business needs, and this can be achieve through the configuration and
customization provided by the SaaS vendor. Allowing many customers to change
software configurations without impacting others customers and with preserving
security and efficiency of the provided services, becomes a big challenge to
SaaS vendors, who are oblige to design new strategies and architectures.
Multi-tenancy (MT) architectures allow multiple customers to be consolidated
into the same operational system without changing anything in the vendor source
code. In this paper, we will present how the configuration can be done on an
ERP web application in a Multi-Tenancy SaaS environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0660</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0660</id><created>2014-05-04</created><authors><author><keyname>He</keyname><forenames>Jin</forenames></author><author><keyname>Dong</keyname><forenames>Mianxiong</forenames></author><author><keyname>Ota</keyname><forenames>Kaoru</forenames></author><author><keyname>Fan</keyname><forenames>Minyu</forenames></author><author><keyname>Wang</keyname><forenames>Guangwei</forenames></author></authors><title>NetSecCC: A Scalable and Fault-tolerant Architecture without Outsourcing
  Cloud Network Security</title><categories>cs.CR cs.NI</categories><comments>10pages, 10figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern cloud computing platforms based on virtual machine monitors carry a
variety of complex business that present many network security vulnerabilities.
At present, the traditional architecture employs a number of security devices
at front-end of cloud computing to protect its network security. Under the new
environment, however, this approach can not meet the needs of cloud security.
New cloud security vendors and academia also made great efforts to solve
network security of cloud computing, unfortunately, they also cannot provide a
perfect and effective method to solve this problem. We introduce a novel
network security architecture for cloud computing (NetSecCC) that addresses
this problem. NetSecCC not only provides an effective solution for network
security issues of cloud computing, but also greatly improves in scalability,
fault-tolerant, resource utilization, etc. We have implemented a
proof-of-concept prototype about NetSecCC and proved by experiments that
NetSecCC is an effective architecture with minimal performance overhead that
can be applied to the extensive practical promotion in cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0669</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0669</id><created>2014-05-04</created><authors><author><keyname>Krishnan</keyname><forenames>Rajet</forenames><affiliation>Student Member, IEEE</affiliation></author><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames><affiliation>Student Member, IEEE</affiliation></author><author><keyname>Krishnan</keyname><forenames>N.</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Amat</keyname><forenames>A. Graell i</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Eriksson</keyname><forenames>T.</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Mazzali</keyname><forenames>N.</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Colavolpe</keyname><forenames>G.</forenames><affiliation>Senior Member, IEEE</affiliation></author></authors><title>On the Impact of Oscillator Phase Noise on the Uplink Performance in a
  Massive MIMO-OFDM System</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, Under Review in IEEE Signal Processing Letters,
  May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the effect of oscillator phase noise on the uplink
performance of a massive multiple-input multiple output system. Specifically,
we consider an orthogonal frequency division multiplexing-based uplink
transmission and analyze two scenarios: (a) all the base station (BS) antennas
are fed by a common oscillator, and (b) each of the BS antennas is fed by a
different oscillator. For the scenarios considered, we derive the instantaneous
signal-to-noise ratio on each subcarrier and analyze the ergodic capacity when
a linear receiver is used. Furthermore, we propose a phase noise tracking
algorithm based on Kalman filtering that mitigates the effect of phase noise on
the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0674</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0674</id><created>2014-05-04</created><authors><author><keyname>Malek-Mohammadi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Owrang</keyname><forenames>Arash</forenames></author><author><keyname>Koochakzadeh</keyname><forenames>Ali</forenames></author><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author></authors><title>DOA Estimation in Partially Correlated Noise Using Low-Rank/Sparse
  Matrix Decomposition</title><categories>cs.IT math.IT</categories><comments>in IEEE Sensor Array and Multichannel signal processing workshop
  (SAM), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of direction-of-arrival (DOA) estimation in unknown
partially correlated noise environments where the noise covariance matrix is
sparse. A sparse noise covariance matrix is a common model for a sparse array
of sensors consisted of several widely separated subarrays. Since interelement
spacing among sensors in a subarray is small, the noise in the subarray is in
general spatially correlated, while, due to large distances between subarrays,
the noise between them is uncorrelated. Consequently, the noise covariance
matrix of such an array has a block diagonal structure which is indeed sparse.
Moreover, in an ordinary nonsparse array, because of small distance between
adjacent sensors, there is noise coupling between neighboring sensors, whereas
one can assume that nonadjacent sensors have spatially uncorrelated noise which
makes again the array noise covariance matrix sparse. Utilizing some recently
available tools in low-rank/sparse matrix decomposition, matrix completion, and
sparse representation, we propose a novel method which can resolve possibly
correlated or even coherent sources in the aforementioned partly correlated
noise. In particular, when the sources are uncorrelated, our approach involves
solving a second-order cone programming (SOCP), and if they are correlated or
coherent, one needs to solve a computationally harder convex program. We
demonstrate the effectiveness of the proposed algorithm by numerical
simulations and comparison to the Cramer-Rao bound (CRB).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0700</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0700</id><created>2014-05-04</created><authors><author><keyname>Abdel-Rehim</keyname><forenames>A.</forenames></author><author><keyname>Alexandrou</keyname><forenames>C.</forenames></author><author><keyname>Anastopoulos</keyname><forenames>N.</forenames></author><author><keyname>Koutsou</keyname><forenames>G.</forenames></author><author><keyname>Liabotis</keyname><forenames>I.</forenames></author><author><keyname>Papadopoulou</keyname><forenames>N.</forenames></author></authors><title>PLQCD library for Lattice QCD on multi-core machines</title><categories>hep-lat cs.MS</categories><comments>7 pages, presented at the 31st International Symposium on Lattice
  Field Theory (Lattice 2013), 29 July - 3 August 2013, Mainz, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PLQCD is a stand-alone software library developed under PRACE for lattice
QCD. It provides an implementation of the Dirac operator for Wilson type
fermions and few efficient linear solvers. The library is optimized for
multi-core machines using a hybrid parallelization with OpenMP+MPI. The main
objectives of the library is to provide a scalable implementation of the Dirac
operator for efficient computation of the quark propagator. In this
contribution, a description of the PLQCD library is given together with some
benchmark results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0701</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0701</id><created>2014-05-04</created><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author></authors><title>&quot;Translation can't change a name&quot;: Using Multilingual Data for Named
  Entity Recognition</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named Entities (NEs) are often written with no orthographic changes across
different languages that share a common alphabet. We show that this can be
leveraged so as to improve named entity recognition (NER) by using unsupervised
word clusters from secondary languages as features in state-of-the-art
discriminative NER systems. We observe significant increases in performance,
finding that person and location identification is particularly improved, and
that phylogenetically close languages provide more valuable features than more
distant languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0712</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0712</id><created>2014-05-04</created><authors><author><keyname>Cheng</keyname><forenames>Bo</forenames><affiliation>Guangdong University of Foreign Studies</affiliation></author><author><keyname>Cheng</keyname><forenames>Ling</forenames><affiliation>University of the Witwatersrand</affiliation></author></authors><title>Single machine slack due-window assignment and scheduling of linear
  time-dependent deteriorating jobs and a deteriorating maintenance activity</title><categories>cs.DS</categories><comments>Submitted - Under Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the slack due-window assignment model and study a
single machine scheduling problem of linear time-dependent deteriorating jobs
and a deteriorating maintenance activity. The cost for each job consists of
four components: earliness, tardiness, window location and window size. The
objective is to schedule the jobs and to assign the maintenance activity and
due-windows such that the total cost among all the jobs is minimized. A
polynomial-time algorithm with the running time not exceeding $O(n^2logn)$ to
give a solution to this problem is introduced, where $n$ is the number of jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0713</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0713</id><created>2014-05-04</created><updated>2015-08-25</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Yaqiong</forenames></author></authors><title>Further result on acyclic chromatic index of planar graphs</title><categories>math.CO cs.DM</categories><comments>23 pages, 20 figures, mainly revised Lemma 8 in Discrete Applied
  Mathematics, 2015. arXiv admin note: text overlap with arXiv:1302.2405</comments><msc-class>05C15</msc-class><doi>10.1016/j.dam.2015.07.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that
every cycle is colored with at least three colors. The acyclic chromatic index
$\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic
edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) +
2$ for any simple graph $G$ with maximum degree $\Delta(G)$. In this paper, we
prove that every planar graph $G$ admits an acyclic edge coloring with
$\Delta(G) + 6$ colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0718</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0718</id><created>2014-05-04</created><updated>2015-04-02</updated><authors><author><keyname>Liu</keyname><forenames>Kangqi</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Generalized Signal Alignment: On the Achievable DoF for Multi-User MIMO
  Two-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>22 pages, 15 figures. To be appeared in IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the achievable degrees of freedom for multi-user MIMO
two-way relay channels, where there are $K$ source nodes, each equipped with
$M$ antennas, one relay node, equipped with $N$ antennas, and each source node
exchanges independent messages with an arbitrary set of other source nodes via
the relay. By allowing an arbitrary information exchange pattern, the
considered channel model is a unified one. It includes several existing channel
models as special cases: $K$-user MIMO Y channel, multi-pair MIMO two-way relay
channel, generalized MIMO two-way X relay channel, and $L$-cluster MIMO
multiway relay channel. Previous studies mainly considered the achievability of
the DoF cut-set bound $2N$ at the antenna configuration $N &lt; 2M$ by applying
signal alignment. This work aims to investigate the achievability of the DoF
cut-set bound $KM$ for the case $N\geq 2M$. To this end, we first derive
tighter DoF upper bounds for three special cases of the considered channel
model. Then, we propose a new transmission framework, generalized signal
alignment, to approach these bounds. The notion of GSA is to form network-coded
symbols by aligning every pair of signals to be exchanged in a compressed
subspace at the relay. A necessary and sufficient condition to construct the
relay compression matrix is given. We show that using GSA, the new DoF upper
bound is achievable when i) $\frac{N}{M} \in \big(0, 2+\frac{4}{K(K-1)}\big]
\cup \big[K-2, +\infty\big)$ for the $K$-user MIMO Y channel; ii) $\frac{N}{M}
\in \big(0, 2+\frac{4}{K}\big] \cup \big[K-2, +\infty\big)$ for the multi-pair
MIMO two-way relay channel; iii) $\frac{N}{M} \in \big(0, 2+\frac{8}{K^2}\big]
\cup \big[K-2, +\infty\big)$ for the generalized MIMO two-way X relay channel.
We also provide the antenna configuration regions for the general multi-user
MIMO two-way relay channel to achieve the total DoF $KM$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0720</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0720</id><created>2014-05-04</created><authors><author><keyname>Nickles</keyname><forenames>Matthias</forenames></author><author><keyname>Mileo</keyname><forenames>Alessandra</forenames></author></authors><title>Probabilistic Inductive Logic Programming Based on Answer Set
  Programming</title><categories>cs.AI</categories><comments>Appears in the Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new formal language for the expressive representation of
probabilistic knowledge based on Answer Set Programming (ASP). It allows for
the annotation of first-order formulas as well as ASP rules and facts with
probabilities and for learning of such weights from data (parameter
estimation). Weighted formulas are given a semantics in terms of soft and hard
constraints which determine a probability distribution over answer sets. In
contrast to related approaches, we approach inference by optionally utilizing
so-called streamlining XOR constraints, in order to reduce the number of
computed answer sets. Our approach is prototypically implemented. Examples
illustrate the introduced concepts and point at issues and topics for future
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0724</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0724</id><created>2014-05-04</created><updated>2014-05-26</updated><authors><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Sundar R.</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Rank Matching for Multihop Multiflow</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the degrees of freedom (DoF) of the layered 2 X 2 X 2 MIMO
interference channel where each node is equipped with arbitrary number of
antennas, the channels between the nodes have arbitrary rank constraints, and
subject to the rank-constraints the channel coefficients can take arbitrary
values. The DoF outer bounds reveal a fundamental rank-matching phenomenon,
reminiscent of impedance matching in circuit theory. It is well known that the
maximum power transfer in a circuit is achieved not for the maximum or minimum
load impedance but for the load impedance that matches the source impedance.
Similarly, the maximum DoF in the rank- constrained 2 X 2 X 2 MIMO interference
network is achieved not for the maximum or minimum ranks of the destination
hop, but when the ranks of the destination hop match the ranks of the source
hop. In fact, for mismatched settings of interest, the outer bounds identify a
DoF loss penalty that is precisely equal to the rank-mismatch between the two
hops. For symmetric settings, we also provide achievability results to show
that along with the min-cut max-flow bounds, the rank-mismatch bounds are the
best possible, i.e., they hold for all channels that satisfy the
rank-constraints and are tight for almost all channels that satisfy the
rank-constraints. Limited extensions - from sum-DoF to DoF region, from 2
unicasts to X message sets, from 2 hops to more than 2 hops and from 2 nodes
per layer to more than 2 nodes per layer - are considered to illustrate how the
insights generalize beyond the elemental 2 X 2 X 2 channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0733</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0733</id><created>2014-05-04</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames><affiliation>Liege &amp; Amsterdam</affiliation></author><author><keyname>Dawid</keyname><forenames>Herbert</forenames><affiliation>Bielefeld</affiliation></author><author><keyname>Merlone</keyname><forenames>Ugo</forenames><affiliation>Torino</affiliation></author></authors><title>Spatial interactions in agent-based modeling</title><categories>physics.soc-ph cs.SI q-fin.EC q-fin.GN</categories><comments>26 pages, 5 figures, 105 references; a chapter prepared for the book
  &quot;Complexity and Geographical Economics - Topics and Tools&quot;, P. Commendatore,
  S.S. Kayam and I. Kubin, Eds. (Springer, in press, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent Based Modeling (ABM) has become a widespread approach to model complex
interactions. In this chapter after briefly summarizing some features of ABM
the different approaches in modeling spatial interactions are discussed.
  It is stressed that agents can interact either indirectly through a shared
environment and/or directly with each other. In such an approach, higher-order
variables such as commodity prices, population dynamics or even institutions,
are not exogenously specified but instead are seen as the results of
interactions. It is highlighted in the chapter that the understanding of
patterns emerging from such spatial interaction between agents is a key problem
as much as their description through analytical or simulation means.
  The chapter reviews different approaches for modeling agents' behavior,
taking into account either explicit spatial (lattice based) structures or
networks. Some emphasis is placed on recent ABM as applied to the description
of the dynamics of the geographical distribution of economic activities, - out
of equilibrium. The Eurace@Unibi Model, an agent-based macroeconomic model with
spatial structure, is used to illustrate the potential of such an approach for
spatial policy analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0740</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0740</id><created>2014-05-04</created><updated>2014-11-04</updated><authors><author><keyname>Lee</keyname><forenames>Euiwoong</forenames></author></authors><title>Hardness of Graph Pricing through Generalized Max-Dicut</title><categories>cs.DS cs.CC</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Graph Pricing problem is among the fundamental problems whose
approximability is not well-understood. While there is a simple combinatorial
1/4-approximation algorithm, the best hardness result remains at 1/2 assuming
the Unique Games Conjecture (UGC). We show that it is NP-hard to approximate
within a factor better than 1/4 under the UGC, so that the simple combinatorial
algorithm might be the best possible. We also prove that for any $\epsilon &gt;
0$, there exists $\delta &gt; 0$ such that the integrality gap of
$n^{\delta}$-rounds of the Sherali-Adams hierarchy of linear programming for
Graph Pricing is at most 1/2 + $\epsilon$.
  This work is based on the effort to view the Graph Pricing problem as a
Constraint Satisfaction Problem (CSP) simpler than the standard and complicated
formulation. We propose the problem called Generalized Max-Dicut($T$), which
has a domain size $T + 1$ for every $T \geq 1$. Generalized Max-Dicut(1) is
well-known Max-Dicut. There is an approximation-preserving reduction from
Generalized Max-Dicut on directed acyclic graphs (DAGs) to Graph Pricing, and
both our results are achieved through this reduction. Besides its connection to
Graph Pricing, the hardness of Generalized Max-Dicut is interesting in its own
right since in most arity two CSPs studied in the literature, SDP-based
algorithms perform better than LP-based or combinatorial algorithms --- for
this arity two CSP, a simple combinatorial algorithm does the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0749</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0749</id><created>2014-05-04</created><authors><author><keyname>Mirtaheri</keyname><forenames>Seyed M.</forenames></author><author><keyname>Din&#xe7;kt&#xfc;rk</keyname><forenames>Mustafa Emre</forenames></author><author><keyname>Hooshmand</keyname><forenames>Salman</forenames></author><author><keyname>Bochmann</keyname><forenames>Gregor V.</forenames></author><author><keyname>Jourdan</keyname><forenames>Guy-Vincent</forenames></author><author><keyname>Onut</keyname><forenames>Iosif Viorel</forenames></author></authors><title>A Brief History of Web Crawlers</title><categories>cs.IR</categories><acm-class>H.3.1; D.2.5</acm-class><journal-ref>Proc. of CASCON 2013, Toronto, Nov. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web crawlers visit internet applications, collect data, and learn about new
web pages from visited pages. Web crawlers have a long and interesting history.
Early web crawlers collected statistics about the web. In addition to
collecting statistics about the web and indexing the applications for search
engines, modern crawlers can be used to perform accessibility and vulnerability
checks on the application. Quick expansion of the web, and the complexity added
to web applications have made the process of crawling a very challenging one.
Throughout the history of web crawling many researchers and industrial groups
addressed different issues and challenges that web crawlers face. Different
solutions have been proposed to reduce the time and cost of crawling.
Performing an exhaustive crawl is a challenging question. Additionally
capturing the model of a modern web application and extracting data from it
automatically is another open question. What follows is a brief history of
different technique and algorithms used from the early days of crawling up to
the recent days. We introduce criteria to evaluate the relative performance of
web crawlers. Based on these criteria we plot the evolution of web crawlers and
compare their performance
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0751</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0751</id><created>2014-05-04</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Ma</keyname><forenames>Wann-Jiun</forenames></author><author><keyname>Gupta</keyname><forenames>Vijay</forenames></author></authors><title>Anytime Control using Input Sequences with Markovian Processor
  Availability</title><categories>math.OC cs.SY</categories><comments>IEEE Transactions on Automatic Control, to be published</comments><msc-class>93C10, 93E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an anytime control algorithm for situations where the processing
resources available for control are time-varying in an a priori unknown
fashion. Thus, at times, processing resources are insufficient to calculate
control inputs. To address this issue, the algorithm calculates sequences of
tentative future control inputs whenever possible, which are then buffered for
possible future use. We assume that the processor availability is correlated so
that the number of control inputs calculated at any time step is described by a
Markov chain. Using a Lyapunov function based approach we derive sufficient
conditions for stochastic stability of the closed loop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0754</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0754</id><created>2014-05-04</created><authors><author><keyname>Zhang</keyname><forenames>Yichao</forenames></author><author><keyname>Aziz-Alaoui</keyname><forenames>M. A.</forenames></author><author><keyname>Bertelle</keyname><forenames>Cyrille</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Wang</keyname><forenames>Wenting</forenames></author></authors><title>Fence-sitters Protect Cooperation in Complex Networks</title><categories>physics.soc-ph cs.GT cs.SI</categories><comments>an article with 6 pages, 3 figures</comments><msc-class>91A06, 91A10, 91A22</msc-class><journal-ref>Physical Review E 88, 032127 (2013)</journal-ref><doi>10.1103/PhysRevE.88.032127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary game theory is one of the key paradigms behind many scientific
disciplines from science to engineering. In complex networks, because of the
difficulty of formulating the replicator dynamics, most of previous studies are
confined to a numerical level. In this paper, we introduce a vectorial
formulation to derive three classes of individuals' payoff analytically. The
three classes are pure cooperators, pure defectors, and fence-sitters. Here,
fence-sitters are the individuals who change their strategies at least once in
the strategy evolutionary process. As a general approach, our vectorial
formalization can be applied to all the two-strategies games. To clarify the
function of the fence-sitters, we define a parameter, payoff memory, as the
number of rounds that the individuals' payoffs are aggregated. We observe that
the payoff memory can control the fence-sitters' effects and the level of
cooperation efficiently. Our results indicate that the fence-sitters' role is
nontrivial in the complex topologies, which protects cooperation in an indirect
way. Our results may provide a better understanding of the composition of
cooperators in a circumstance where the temptation to defect is larger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0761</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0761</id><created>2014-05-04</created><authors><author><keyname>Zhang</keyname><forenames>Yichao</forenames></author><author><keyname>Aziz-Alaoui</keyname><forenames>M. A.</forenames></author><author><keyname>Bertelle</keyname><forenames>Cyrille</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Wang</keyname><forenames>Wenting</forenames></author></authors><title>Emergence of Cooperation in Non-scale-free Networks</title><categories>physics.soc-ph cs.GT cs.SI</categories><comments>6 pages, 5 figures</comments><doi>10.1088/1751-8113/47/22/225003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary game theory is one of the key paradigms behind many scientific
disciplines from science to engineering. Previous studies proposed a strategy
updating mechanism, which successfully demonstrated that the scale-free network
can provide a framework for the emergence of cooperation. Instead, individuals
in random graphs and small-world networks do not favor cooperation under this
updating rule. However, a recent empirical result shows the heterogeneous
networks do not promote cooperation when humans play a Prisoner's Dilemma. In
this paper, we propose a strategy updating rule with payoff memory. We observe
that the random graphs and small-world networks can provide even better
frameworks for cooperation than the scale-free networks in this scenario. Our
observations suggest that the degree heterogeneity may be neither a sufficient
condition nor a necessary condition for the widespread cooperation in complex
networks. Also, the topological structures are not sufficed to determine the
level of cooperation in complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0762</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0762</id><created>2014-05-04</created><authors><author><keyname>Accisano</keyname><forenames>Paul</forenames></author><author><keyname>&#xdc;ng&#xf6;r</keyname><forenames>Alper</forenames></author></authors><title>Finding a Curve in a Point Set</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a polygonal curve in $\mathbb{R}^D$ of length $n$, and $S$ be a
point set of size $k$. The Curve/Point Set Matching problem consists of finding
a polygonal curve $Q$ on $S$ such that its Fr\'echet distance from $P$ is less
than a given $\varepsilon$. In this paper, we consider this problem with the
added freedom to transform the input curve $P$ by translating it, rotating it,
or applying an arbitrary affine transform. We present exact and approximation
algorithms for several variations of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0766</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0766</id><created>2014-05-04</created><authors><author><keyname>Low</keyname><forenames>Steven H.</forenames></author></authors><title>Convex Relaxation of Optimal Power Flow, Part I: Formulations and
  Equivalence</title><categories>math.OC cs.SY</categories><comments>Citation: IEEE Transactions on Control of Network Systems,
  15(1):15-27, March 2014. This is an extended version with Appendices VIII and
  IX that provide some mathematical preliminaries and proofs of the main
  results</comments><journal-ref>S. H. Low. Convex Relaxation of Optimal Power Flow, Part I:
  Formulations and Equivalence, IEEE Transactions on Control of Network
  Systems, 15(1):15-27, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This tutorial summarizes recent advances in the convex relaxation of the
optimal power flow (OPF) problem, focusing on structural properties rather than
algorithms. Part I presents two power flow models, formulates OPF and their
relaxations in each model, and proves equivalence relations among them. Part II
presents sufficient conditions under which the convex relaxations are exact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0770</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0770</id><created>2014-05-04</created><authors><author><keyname>Yu</keyname><forenames>Yonghong</forenames></author><author><keyname>Wang</keyname><forenames>Can</forenames></author><author><keyname>Gao</keyname><forenames>Yang</forenames></author></authors><title>Attributes Coupling based Item Enhanced Matrix Factorization Technique
  for Recommender Systems</title><categories>cs.IR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender system has attracted lots of attentions since it helps users
alleviate the information overload problem. Matrix factorization technique is
one of the most widely employed collaborative filtering techniques in the
research of recommender systems due to its effectiveness and efficiency in
dealing with very large user-item rating matrices. Recently, based on the
intuition that additional information provides useful insights for matrix
factorization techniques, several recommendation algorithms have utilized
additional information to improve the performance of matrix factorization
methods. However, the majority focus on dealing with the cold start user
problem and ignore the cold start item problem. In addition, there are few
suitable similarity measures for these content enhanced matrix factorization
approaches to compute the similarity between categorical items. In this paper,
we propose attributes coupling based item enhanced matrix factorization method
by incorporating item attribute information into matrix factorization technique
as well as adapting the coupled object similarity to capture the relationship
between items. Item attribute information is formed as an item relationship
regularization term to regularize the process of matrix factorization.
Specifically, the similarity between items is measured by the Coupled Object
Similarity considering coupling between items. Experimental results on two real
data sets show that our proposed method outperforms state-of-the-art
recommendation algorithms and can effectively cope with the cold start item
problem when more item attribute information is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0773</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0773</id><created>2014-05-04</created><updated>2014-10-08</updated><authors><author><keyname>He</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Zhang</keyname><forenames>Deguang</forenames></author><author><keyname>Ma</keyname><forenames>Yutao</forenames></author></authors><title>Simplification of Training Data for Cross-Project Defect Prediction</title><categories>cs.SE</categories><comments>17 pages, 12 figures</comments><msc-class>68N30</msc-class><acm-class>D.2.8; D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-project defect prediction (CPDP) plays an important role in estimating
the most likely defect-prone software components, especially for new or
inactive projects. To the best of our knowledge, few prior studies provide
explicit guidelines on how to select suitable training data of quality from a
large number of public software repositories. In this paper, we have proposed a
training data simplification method for practical CPDP in consideration of
multiple levels of granularity and filtering strategies for data sets. In
addition, we have also provided quantitative evidence on the selection of a
suitable filter in terms of defect-proneness ratio. Based on an empirical study
on 34 releases of 10 open-source projects, we have elaborately compared the
prediction performance of different defect predictors built with five
well-known classifiers using training data simplified at different levels of
granularity and with two popular filters. The results indicate that when using
the multi-granularity simplification method with an appropriate filter, the
prediction models based on Naive Bayes can achieve fairly good performance and
outperform the benchmark method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0776</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0776</id><created>2014-05-05</created><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author></authors><title>Polynomial complexity of polar codes for non-binary alphabets, key
  agreement and Slepian-Wolf coding</title><categories>cs.IT math.IT</categories><comments>6 pages; presented at CISS 2014</comments><journal-ref>2014 48th Annual Conference on Information Sciences and Systems
  (CISS), 19-21 March 2014, Princeton, NJ</journal-ref><doi>10.1109/CISS.2014.6814146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider polar codes for memoryless sources with side information and show
that the blocklength, construction, encoding and decoding complexities are
bounded by a polynomial of the reciprocal of the gap between the compression
rate and the conditional entropy. This extends the recent results of Guruswami
and Xia to a slightly more general setting, which in turn can be applied to (1)
sources with non-binary alphabets, (2) key generation for discrete and Gaussian
sources, and (3) Slepian-Wolf coding and multiple accessing. In each of these
cases, the complexity scaling with respect to the number of users is also
controlled. In particular, we construct coding schemes for these multi-user
information theory problems which achieve optimal rates with an overall
polynomial complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0782</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0782</id><created>2014-05-05</created><updated>2014-06-20</updated><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author></authors><title>Optimality guarantees for distributed statistical estimation</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>34 pages, 1 figure. Preliminary version appearing in Neural
  Information Processing Systems 2013
  (http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large data sets often require performing distributed statistical estimation,
with a full data set split across multiple machines and limited communication
between machines. To study such scenarios, we define and study some refinements
of the classical minimax risk that apply to distributed settings, comparing to
the performance of estimators with access to the entire data. Lower bounds on
these quantities provide a precise characterization of the minimum amount of
communication required to achieve the centralized minimax risk. We study two
classes of distributed protocols: one in which machines send messages
independently over channels without feedback, and a second allowing for
interactive communication, in which a central server broadcasts the messages
from a given machine to all other machines. We establish lower bounds for a
variety of problems, including location estimation in several families and
parameter estimation in different types of regression models. Our results
include a novel class of quantitative data-processing inequalities used to
characterize the effects of limited communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0786</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0786</id><created>2014-05-05</created><authors><author><keyname>Anand</keyname><forenames>Vishal</forenames></author><author><keyname>S</keyname><forenames>Ramani</forenames></author></authors><title>Fault Localization in a Software Project using Back-Tracking Principles
  of Matrix Dependency</title><categories>cs.SE</categories><comments>5 pages, 8 figures, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>Vishal Anand , Ramani S Article: &quot;Fault Localization in a Software
  Project using Back-Tracking Principles of Matrix Dependency&quot;. International
  Journal of Engineering Trends and Technology (IJETT) V10(11):545-549, April
  2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V10P308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault identification and testing has always been the most specific concern in
the field of software development. To identify and testify the bug we should be
aware of the source of the failure or any unwanted issue. In this paper, we are
trying to extract the location of failure and trying to cope up with the bug.
Using directed graph, we tried to obtain the dependency of multiple activities
in live environment to trace the origin of fault. Software development comes up
with series of activities and we tried to show the dependency of multiple
activities on each other. Critical activities are considered as they cause
abnormal functioning of the whole system. The paper discuss about the
priorities of activities of dependency of software failure on the critical
activities. Matrix representation of activities as part of the software is
chosen to determine root of the failure using concept of dependency. It can
vary with the topography of network and software environment. When faults
occur, the possible symptoms will be reflected in the dependency matrix with
high probability in fault itself. Thus, independent faults are located in the
main diagonal of dependency matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0787</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0787</id><created>2014-05-05</created><authors><author><keyname>Sharma</keyname><forenames>Tarushi</forenames></author><author><keyname>Kaur</keyname><forenames>AmanPreet</forenames></author></authors><title>Analysis of Email Fraud detection using WEKA Tool</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is also being useful to give solutions for invasion finding and
auditing. While data mining has several applications in protection, there are
also serious privacy fears. Because of email mining, even inexperienced users
can connect data and make responsive associations. Therefore we must to
implement the privacy of persons while working on practical data mining
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0789</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0789</id><created>2014-05-05</created><updated>2014-08-19</updated><authors><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>A note on the ring loading problem</title><categories>cs.DM cs.DS</categories><msc-class>90C27, 05C21</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ring Loading Problem is an optimal routing problem arising in the
planning of optical communication networks which use bidirectional SONET rings.
In mathematical terms, it is an unsplittable multicommodity flow problem on
undirected ring networks. We prove that any split routing solution to the Ring
Loading Problem can be turned into an unsplittable solution while increasing
the load on any edge of the ring by no more than +(19/14)D, where D is the
maximum demand value. This improves upon a classical result of Schrijver,
Seymour, and Winkler (1998) who obtained a slightly larger bound of +(3/2)D. We
also present an improved lower bound of +1.1 D (previously +1.01 D) on the best
possible bound and disprove a famous long-standing conjecture of Schrijver et
al. in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0792</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0792</id><created>2014-05-05</created><authors><author><keyname>Abasi</keyname><forenames>Hasan</forenames></author><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author><author><keyname>Mazzawi</keyname><forenames>Hanna</forenames></author></authors><title>On Exact Learning Monotone DNF from Membership Queries</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of learning a monotone DNF with at most
$s$ terms of size (number of variables in each term) at most $r$ ($s$ term
$r$-MDNF) from membership queries. This problem is equivalent to the problem of
learning a general hypergraph using hyperedge-detecting queries, a problem
motivated by applications arising in chemical reactions and genome sequencing.
  We first present new lower bounds for this problem and then present
deterministic and randomized adaptive algorithms with query complexities that
are almost optimal. All the algorithms we present in this paper run in time
linear in the query complexity and the number of variables $n$. In addition,
all of the algorithms we present in this paper are asymptotically tight for
fixed $r$ and/or $s$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0795</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0795</id><created>2014-05-05</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Hermann</keyname><forenames>Alice</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>ICube</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Belief revision in the propositional closure of a qualitative algebra
  (extended version)</title><categories>cs.AI</categories><comments>This is the extended version of an article originally presented at
  the 14th International Conference on Principles of Knowledge Representation
  and Reasoning</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief revision is an operation that aims at modifying old beliefs so that
they become consistent with new ones. The issue of belief revision has been
studied in various formalisms, in particular, in qualitative algebras (QAs) in
which the result is a disjunction of belief bases that is not necessarily
representable in a QA. This motivates the study of belief revision in
formalisms extending QAs, namely, their propositional closures: in such a
closure, the result of belief revision belongs to the formalism. Moreover, this
makes it possible to define a contraction operator thanks to the Harper
identity. Belief revision in the propositional closure of QAs is studied, an
algorithm for a family of revision operators is designed, and an open-source
implementation is made freely available on the web. (This is the extended
version of an article originally presented at the 14th International Conference
on Principles of Knowledge Representation and Reasoning.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0805</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0805</id><created>2014-05-05</created><authors><author><keyname>Strass</keyname><forenames>Hannes</forenames></author></authors><title>On the Relative Expressiveness of Argumentation Frameworks, Normal Logic
  Programs and Abstract Dialectical Frameworks</title><categories>cs.AI cs.LO</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the expressiveness of the two-valued semantics of abstract
argumentation frameworks, normal logic programs and abstract dialectical
frameworks. By expressiveness we mean the ability to encode a desired set of
two-valued interpretations over a given propositional signature using only
atoms from that signature. While the computational complexity of the two-valued
model existence problem for all these languages is (almost) the same, we show
that the languages form a neat hierarchy with respect to their expressiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0806</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0806</id><created>2014-05-05</created><authors><author><keyname>Li</keyname><forenames>X. R.</forenames></author><author><keyname>Pei</keyname><forenames>D. B.</forenames></author><author><keyname>Liu</keyname><forenames>Q.</forenames></author><author><keyname>Shen</keyname><forenames>R.</forenames></author></authors><title>Design of a capacitor-less low-dropout voltage regulator</title><categories>cs.SY</categories><comments>6pages,7figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A solution to the stability of capacitor-less low-dropout regulators with a
4pF Miller capacitor in Multi-level current amplifier is proposed. With the
Miller compensation, a more than 50{\deg}phase margin is guaranteed in full
load. An extra fast transient circuit is adopted to reduce stable time and peak
voltage. When the load changes from light to heavy, the peak voltage is 40mV
and chip quiescent current is only 45uA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0809</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0809</id><created>2014-05-05</created><authors><author><keyname>Ji</keyname><forenames>Jianmin</forenames></author><author><keyname>Strass</keyname><forenames>Hannes</forenames></author></authors><title>Implementing Default and Autoepistemic Logics via the Logic of GK</title><categories>cs.AI cs.LO</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logic of knowledge and justified assumptions, also known as logic of
grounded knowledge (GK), was proposed by Lin and Shoham as a general logic for
nonmonotonic reasoning. To date, it has been used to embed in it default logic
(propositional case), autoepistemic logic, Turner's logic of universal
causation, and general logic programming under stable model semantics. Besides
showing the generality of GK as a logic for nonmonotonic reasoning, these
embeddings shed light on the relationships among these other logics. In this
paper, for the first time, we show how the logic of GK can be embedded into
disjunctive logic programming in a polynomial but non-modular translation with
new variables. The result can then be used to compute the extension/expansion
semantics of default logic, autoepistemic logic and Turner's logic of universal
causation by disjunctive ASP solvers such as claspD(-2), DLV, GNT and cmodels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0814</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0814</id><created>2014-05-05</created><authors><author><keyname>Low</keyname><forenames>Steven H.</forenames></author></authors><title>Convex Relaxation of Optimal Power Flow, Part II: Exactness</title><categories>math.OC cs.SY</categories><comments>Citation: IEEE Transactions on Control of Network Systems, June 2014.
  This is an extended version with Appendex VI that proves the main results in
  this tutorial</comments><journal-ref>S. H. Low. Convex Relaxation of Optimal Power Flow, Part II:
  Exactness, IEEE Transactions on Control of Network Systems, June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This tutorial summarizes recent advances in the convex relaxation of the
optimal power flow (OPF) problem, focusing on structural properties rather than
algorithms. Part I presents two power flow models, formulates OPF and their
relaxations in each model, and proves equivalence relations among them. Part II
presents sufficient conditions under which the convex relaxations are exact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0822</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0822</id><created>2014-05-05</created><authors><author><keyname>Sakthivel</keyname><forenames>P.</forenames></author><author><keyname>Sankar</keyname><forenames>P. Krishna</forenames></author></authors><title>Multi-Path Routing and Wavelength Assignment (RWA) Algorithm for WDM
  Based Optical Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In optical WDM networks, transmission of information along optical lines is
advantageous since it has high transmission capacity, scalability, feasibility
and also high reliability. But large amount of information is being carried;
any problem during transmission can lead to severe damage to the data being
carried. Hence it is essential to consider the routing as well as the
wavelength assignment problems and then develop a combined solution for both
the problems. In this paper, we propose to develop a routing and wavelength
assignment algorithm for selecting the suitable alternate path for the data
packets transmission. Two stages are based on the available bandwidth and the
number of wavelength used in the link as construction of alternate paths, route
and wavelength selection. In proposed work, Adaptive Routing and First-Fit
Wavelength Assignment (AR-FFWA) algorithm to be used. For each pair of source
and destination, the path with the minimum granularity values are selected as
the primary path for data transmission, allocating the sufficient wavelength
and the performances will be evaluated by using ns-2 simulation models. When we
compared to existing system the overall blocking probability will be reduced to
too low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0823</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0823</id><created>2014-05-05</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Ready for the design of voting rules?</title><categories>cs.GT math.CO</categories><comments>10 pages</comments><msc-class>91B12, 94C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of fair voting rules has been addressed quite often in the
literature. Still, the so-called inverse problem is not entirely resolved. We
summarize some achievements in this direction and formulate explicit open
questions and conjectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0825</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0825</id><created>2014-05-05</created><authors><author><keyname>Kaniovski</keyname><forenames>Serguei</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>The average representation - a cornucopia of power indices?</title><categories>cs.GT</categories><comments>10 pages, 1 table</comments><msc-class>91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the classical power indices there is a disproportion between power and
relative weights, in general. We introduce two new indices, based on weighted
representations, which are proportional to suitable relative weights and which
also share several important properties of the classical power indices.
Imposing further restrictions on the set of representations may lead to a whole
family of such indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0833</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0833</id><created>2014-05-05</created><authors><author><keyname>Zimin</keyname><forenames>Alexander</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author></authors><title>Generalized Risk-Aversion in Stochastic Multi-Armed Bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing the regret in stochastic multi-armed
bandit, when the measure of goodness of an arm is not the mean return, but some
general function of the mean and the variance.We characterize the conditions
under which learning is possible and present examples for which no natural
algorithm can achieve sublinear regret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0835</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0835</id><created>2014-05-05</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Chmelik</keyname><forenames>Martin</forenames></author><author><keyname>Daca</keyname><forenames>Przemyslaw</forenames></author></authors><title>CEGAR for Qualitative Analysis of Probabilistic Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Markov decision processes (MDPs) which are a standard model for
probabilistic systems. We focus on qualitative properties for MDPs that can
express that desired behaviors of the system arise almost-surely (with
probability 1) or with positive probability. We introduce a new simulation
relation to capture the refinement relation of MDPs with respect to qualitative
properties, and present discrete graph theoretic algorithms with quadratic
complexity to compute the simulation relation. We present an automated
technique for assume-guarantee style reasoning for compositional analysis of
MDPs with qualitative properties by giving a counter-example guided
abstraction-refinement approach to compute our new simulation relation. We have
implemented our algorithms and show that the compositional analysis leads to
significant improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0843</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0843</id><created>2014-05-05</created><updated>2015-06-12</updated><authors><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>MuxViz: A Tool for Multilayer Analysis and Visualization of Networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI physics.bio-ph</categories><comments>18 pages, 10 figures (text of the accepted manuscript)</comments><journal-ref>Journal of Complex Networks 3, 159-176 (2015)</journal-ref><doi>10.1093/comnet/cnu038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilayer relationships among entities and information about entities must
be accompanied by the means to analyze, visualize, and obtain insights from
such data. We present open-source software (muxViz) that contains a collection
of algorithms for the analysis of multilayer networks, which are an important
way to represent a large variety of complex systems throughout science and
engineering. We demonstrate the ability of muxViz to analyze and interactively
visualize multilayer data using empirical genetic, neuronal, and transportation
networks. Our software is available at https://github.com/manlius/muxViz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0847</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0847</id><created>2014-05-05</created><authors><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>Reconfiguration in bounded bandwidth and treedepth</title><categories>cs.CC cs.DM</categories><comments>14 pages</comments><acm-class>F.2.2; G.2.2; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that several reconfiguration problems known to be PSPACE-complete
remain so even when limited to graphs of bounded bandwidth. The essential step
is noticing the similarity to very limited string rewriting systems, whose
ability to directly simulate Turing Machines is classically known. This
resolves a question posed open in [Bonsma P., 2012]. On the other hand, we show
that a large class of reconfiguration problems becomes tractable on graphs of
bounded treedepth, and that this result is in some sense tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0854</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0854</id><created>2014-05-05</created><updated>2015-01-21</updated><authors><author><keyname>Goncharov</keyname><forenames>Sergey</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author><author><keyname>Rauch</keyname><forenames>Christoph</forenames></author></authors><title>(Co)algebraic Foundations for Effectful Recursive Definitions</title><categories>cs.LO</categories><comments>15 pages + technical appendix</comments><acm-class>F.3.2; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pervasive challenge in programming theory and practice are feature
combinations. Here, we propose a semantic framework that combines monad-based
computational effects (e.g. store, nondeterminism, random), underdefined or
free operations (e.g. actions in process algebra and automata, exceptions), and
recursive definitions (e.g. loops, systems of process equations). The joint
treatment of these phenomena has previously led to models tending to one of two
opposite extremes: extensional as, e.g., in domain theory, and intensional as
in classical process algebra and more generally in universal coalgebra. Our
metalanguage for effectful recursive definitions, designed in the spirit of
Moggi's computational metalanguage, flexibly combines these intensional and
extensional aspects of computations in a single framework. We base our
development on a notion of complete Elgot monad, whose defining feature is a
parametrized uniform iteration operator satisfying natural axioms in the style
of Simpson and Plotkin. We provide a mechanism of adjoining free operations to
such monads by means of cofree extensions, thus in particular allowing for a
non-trivial semantics of non-terminating computations with free effects. Our
main result states that the class of complete Elgot monads is closed under such
cofree extensions, which thus serve as domains for effectful recursive
definitions with free operations. Elgot monads do not require the iterated
computation to be guarded, and hence iteration operators are not uniquely
determined by just their defining fixpoint equation. Our results however imply
that they are uniquely determined as extending the given iteration in the base
effect and satisfying the axioms. We discuss a number of examples formalized in
our metalanguage, including (co)recursive definitions of process-algebraic
operations on side- effecting processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0867</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0867</id><created>2014-05-05</created><authors><author><keyname>Kammueller</keyname><forenames>Florian</forenames></author></authors><title>Confinement for Active Objects</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a formal framework for the security of distributed
active objects. Active objects communicate asynchronously implementing method
calls via futures. We base the formal framework on a security model that uses a
semi-lattice to enable multi-lateral security crucial for distributed
architectures. We further provide a security type system for the programming
model ASPfun of functional active objects. Type safety and a confinement
property are presented. ASPfun thus realizes secure down calls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0868</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0868</id><created>2014-05-05</created><authors><author><keyname>Bao</keyname><forenames>Zhana</forenames></author></authors><title>Finding Inner Outliers in High Dimensional Space</title><categories>cs.AI</categories><comments>9 pages, 9 Figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Outlier detection in a large-scale database is a significant and complex
issue in knowledge discovering field. As the data distributions are obscure and
uncertain in high dimensional space, most existing solutions try to solve the
issue taking into account the two intuitive points: first, outliers are
extremely far away from other points in high dimensional space; second,
outliers are detected obviously different in projected-dimensional subspaces.
However, for a complicated case that outliers are hidden inside the normal
points in all dimensions, existing detection methods fail to find such inner
outliers. In this paper, we propose a method with twice dimension-projections,
which integrates primary subspace outlier detection and secondary
point-projection between subspaces, and sums up the multiple weight values for
each point. The points are computed with local density ratio separately in
twice-projected dimensions. After the process, outliers are those points
scoring the largest values of weight. The proposed method succeeds to find all
inner outliers on the synthetic test datasets with the dimension varying from
100 to 10000. The experimental results also show that the proposed algorithm
can work in low dimensional space and can achieve perfect performance in high
dimensional space. As for this reason, our proposed approach has considerable
potential to apply it in multimedia applications helping to process images or
video with large-scale attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0869</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0869</id><created>2014-05-05</created><authors><author><keyname>Bao</keyname><forenames>Zhana</forenames></author></authors><title>Robust Subspace Outlier Detection in High Dimensional Space</title><categories>cs.AI cs.LG stat.ML</categories><comments>10 pages, 6 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Rare data in a large-scale database are called outliers that reveal
significant information in the real world. The subspace-based outlier detection
is regarded as a feasible approach in very high dimensional space. However, the
outliers found in subspaces are only part of the true outliers in high
dimensional space, indeed. The outliers hidden in normal-clustered points are
sometimes neglected in the projected dimensional subspace. In this paper, we
propose a robust subspace method for detecting such inner outliers in a given
dataset, which uses two dimensional-projections: detecting outliers in
subspaces with local density ratio in the first projected dimensions; finding
outliers by comparing neighbor's positions in the second projected dimensions.
Each point's weight is calculated by summing up all related values got in the
two steps projected dimensions, and then the points scoring the largest weight
values are taken as outliers. By taking a series of experiments with the number
of dimensions from 10 to 10000, the results show that our proposed method
achieves high precision in the case of extremely high dimensional space, and
works well in low dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0876</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0876</id><created>2014-05-05</created><authors><author><keyname>Maratea</keyname><forenames>Marco</forenames></author><author><keyname>Pulina</keyname><forenames>Luca</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>The Multi-engine ASP Solver ME-ASP: Progress Report</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MEASP is a multi-engine solver for ground ASP programs. It exploits algorithm
selection techniques based on classification to select one among a set of
out-of-the-box heterogeneous ASP solvers used as black-box engines. In this
paper we report on (i) a new optimized implementation of MEASP; and (ii) an
attempt of applying algorithm selection to non-ground programs. An experimental
analysis reported in the paper shows that (i) the new implementation of \measp
is substantially faster than the previous version; and (ii) the multi-engine
recipe can be applied to the evaluation of non-ground programs with some
benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0877</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0877</id><created>2014-05-05</created><authors><author><keyname>Kramer</keyname><forenames>Simon</forenames></author></authors><title>A Galois-Connection between Cattell's and Szondi's Personality Profiles</title><categories>cs.CE cs.CY</categories><comments>closely related to arXiv:1403.2000 as explained in the first
  paragraph</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a computable Galois-connection between, on the one hand, Cattell's
16-Personality-Factor (16PF) Profiles, one of the most comprehensive and
widely-used personality measures for non-psychiatric populations and their
containing PsychEval Personality Profiles (PPPs) for psychiatric populations,
and, on the other hand, Szondi's personality profiles (SPPs), a less well-known
but, as we show, finer personality measure for psychiatric as well as
non-psychiatric populations (conceived as a unification of the depth psychology
of S. Freud, C.G. Jung, and A. Adler). The practical significance of our result
is that our Galois-connection provides a pair of computable, interpreting
translations between the two personality spaces of PPPs (containing the 16PFs)
and SPPs: one concrete from PPP-space to SPP-space (because SPPs are finer than
PPPs) and one abstract from SPP-space to PPP-space (because PPPs are coarser
than SPPs). Thus Cattell's and Szondi's personality-test results are mutually
interpretable and inter-translatable, even automatically by computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0878</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0878</id><created>2014-05-05</created><authors><author><keyname>Orynczak</keyname><forenames>Grzegorz</forenames></author><author><keyname>Jakubek</keyname><forenames>Marcin</forenames></author><author><keyname>Wawrzyniak</keyname><forenames>Karol</forenames></author><author><keyname>Klos</keyname><forenames>Michal</forenames></author></authors><title>Market Coupling as the Universal Algorithm to Assess Zonal Divisions</title><categories>cs.CE cs.CY cs.SY q-fin.GN</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adopting a zonal structure of electricity market requires specification of
zones' borders. In this paper we use social welfare as the measure to assess
quality of various zonal divisions. The social welfare is calculated by Market
Coupling algorithm. The analyzed divisions are found by the usage of extended
Locational Marginal Prices (LMP) methodology presented in paper [1], which
takes into account variable weather conditions. The offered method of
assessment of a proposed division of market into zones is however not limited
to LMP approach but can evaluate the social welfare of divisions obtained by
any methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0879</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0879</id><created>2014-05-05</created><updated>2015-03-11</updated><authors><author><keyname>Kremnizer</keyname><forenames>Kobi</forenames></author><author><keyname>Ranchin</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Integrated Information-induced quantum collapse</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages</comments><journal-ref>Foundations of Physics 45 (2015) 889-899</journal-ref><doi>10.1007/s10701-015-9905-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel spontaneous collapse model where size is no longer the
property of a physical system which determines its rate of collapse. Instead,
we argue that the rate of spontaneous localization should depend on a system's
quantum Integrated Information (QII), a novel physical property which describes
a system's capacity to act like a quantum observer. We introduce quantum
Integrated Information, present our QII collapse model and briefly explain how
it may be experimentally tested against quantum theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0891</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0891</id><created>2014-05-05</created><authors><author><keyname>Shkel</keyname><forenames>Yanina Y.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author></authors><title>Unequal Message Protection: Asymptotic and Non-Asymptotic Tradeoffs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a form of unequal error protection that we term &quot;unequal message
protection&quot; (UMP). The message set of a UMP code is a union of $m$ disjoint
message classes. Each class has its own error protection requirement, with some
classes needing better error protection than others. We analyze the tradeoff
between rates of message classes and the levels of error protection of these
codes. We demonstrate that there is a clear performance loss compared to
homogeneous (classical) codes with equivalent parameters. This is in sharp
contrast to previous literature that considers UMP codes. To obtain our results
we generalize finite block length achievability and converse bounds due to
Polyanskiy-Poor-Verd\'{u}. We evaluate our bounds for the binary symmetric and
binary erasure channels, and analyze the asymptotic characteristic of the
bounds in the fixed error and moderate deviations regimes. In addition, we
consider two questions related to the practical construction of UMP codes.
First, we study a &quot;header&quot; construction that prefixes the message class into a
header followed by data protection using a standard homogeneous code. We show
that, in general, this construction is not optimal at finite block lengths. We
further demonstrate that our main UMP achievability bound can be obtained using
coset codes, which suggests a path to implementation of tractable UMP codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0892</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0892</id><created>2014-05-05</created><updated>2014-06-05</updated><authors><author><keyname>Baxter</keyname><forenames>John S. H.</forenames></author><author><keyname>Rajchl</keyname><forenames>Martin</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author></authors><title>A Continuous Max-Flow Approach to Multi-Labeling Problems under
  Arbitrary Region Regularization</title><categories>cs.CV</categories><comments>10 pages, 2 figures, 3 algorithms - v2: Fixed typos / grammatical
  errors and mathematical errors in the primal/dual formulation. Extended
  methods for weighted DAGs rather than DAGs with edge multiplicity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The incorporation of region regularization into max-flow segmentation has
traditionally focused on ordering and part-whole relationships. A side effect
of the development of such models is that it constrained regularization only to
those cases, rather than allowing for arbitrary region regularization. Directed
Acyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations by
allowing for the algorithm designer to specify an arbitrary directed acyclic
graph to structure a max-flow segmentation. This allows for individual 'parts'
to be a member of multiple distinct 'wholes.'
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0893</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0893</id><created>2014-05-05</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author></authors><title>Many-Access Channels: The Gaussian Case with Random User Activities</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, to appear in Proceedings of ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical multiuser information theory studies the fundamental limits of
models with a fixed (often small) number of users as the coding blocklength
goes to infinity. This work proposes a new paradigm, referred to as many-user
information theory, where the number of users is allowed to grow with the
blocklength. This paradigm is motivated by emerging systems with a massive
number of users in an area, such as machine-to-machine communication systems
and sensor networks. The focus of the current paper is the many-access channel
model, which consists of a single receiver and many transmitters, whose number
increases unboundedly with the blocklength. Moreover, an unknown subset of
transmitters may transmit in a given block and need to be identified. A new
notion of capacity is introduced and characterized for the Gaussian many-access
channel with random user activities. The capacity can be achieved by first
detecting the set of active users and then decoding their messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0894</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0894</id><created>2014-05-05</created><updated>2015-10-31</updated><authors><author><keyname>Gulcu</keyname><forenames>Talha Cihad</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>Interactive Function Computation via Polar Coding</title><categories>cs.IT math.IT</categories><comments>to appear at Problems of Information Transmission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a series of papers N. Ma and P. Ishwar (2011-13) considered a range of
distributed source coding problems that arise in the context of iterative
computation of functions, characterizing the region of achievable communication
rates. We consider the problems of interactive computation of functions by two
terminals and interactive computation in a collocated network, showing that the
rate regions for both these problems can be achieved using several rounds of
polar-coded transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0900</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0900</id><created>2014-05-05</created><updated>2014-12-03</updated><authors><author><keyname>Henze</keyname><forenames>Matthias</forenames></author><author><keyname>Jaume</keyname><forenames>Rafel</forenames></author></authors><title>Bottleneck Partial-Matching Voronoi Diagrams and Applications</title><categories>cs.CG</categories><comments>24 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two point sets in the plane, we study the minimization of the
bottleneck distance between a point set B and an equally-sized subset of a
point set A under translations. We relate this problem to a Voronoi-type
diagram and derive polynomial bounds for its complexity that are optimal in the
size of A. We devise efficient algorithms for the construction of such a
diagram and its lexicographic variant, which generalize to higher dimensions.
We use the diagram to find an optimal bottleneck matching under translations,
to compute a connecting path of minimum bottleneck cost between two positions
of B, and to determine the maximum bottleneck cost in a convex polygon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0910</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0910</id><created>2014-05-05</created><authors><author><keyname>Silv&#xe9;ria</keyname><forenames>Michelle Kr&#xfc;ger</forenames></author></authors><title>Virtual Windshields: Merging Reality and Digital Content to Improve the
  Driving Experience</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the use of the automobile as the primary mode of
transportation has been increasing and driving has become an important part of
daily life. Driving is a multi-sensory experience as drivers rely on their
senses to provide them with important information. In a vehicular context human
senses are all too often limited and obstructed. Today, road accidents
constitute the eighth leading cause of death. The escalation of technology has
propelled new ways in which driver's senses may be augmented. The enclosed
aspect of a car, allied with the configuration of the controls and displays
directed towards the driver, offer significant advantages for augmented reality
(AR) systems when considering the amount of immersion it can provide to the
user. In addition, the inherent mobility and virtually unlimited power autonomy
transform cars into perfect mobile computing platforms. However, automobiles
currently present limited network connectivity and thus the created augmented
objects are merely providing information captured by in-vehicle sensors,
cameras and other databases. By combining the new paradigm of Vehicular Ad Hoc
Networking (VANET) with AR human machine interfaces, we show that it is
possible to design novel cooperative Advanced Driver Assistance Systems (ADAS),
that base the creation of AR content on the information collected from
neighbouring vehicles or roadside infrastructures. As such we implement
prototypes of both visual and acoustic AR systems, which can significantly
improve the driving experience. We believe our results contribute to the
formulation of a vision where the vehicle is perceived as an extension of the
body which permeates the human senses to the world outside the vessel, where
the car is used as a better, multi-sensory immersive version of a mobile phone
that integrates touch, vision and sound enhancements, leveraging unique
properties of VANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0914</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0914</id><created>2014-04-18</created><authors><author><keyname>Hashim</keyname><forenames>Hayder Raheem</forenames></author></authors><title>The Discrete Logarithm problem in the ElGamal cryptosystem over the
  abelian group U(n) Where n= p^m,or 2p^m</title><categories>cs.CR</categories><comments>This paper has 6 pages with one figure and three tables. Also, it has
  been Published with &quot;International Journal of Mathematics Trends and
  Technology (IJMTT)&quot;Published by Seventh Sense Research Group</comments><journal-ref>International Journal of Mathematical Trends and
  Technology(IJMTT).V7:184-189 March 2014</journal-ref><doi>10.14445/22315373/IJMTT-V7P522</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This study is mainly about the discrete logarithm problem in the ElGamal
cryptosystem over the abelian group U(n) where n is one of the following forms
p^m, or 2p^m where p is an odd large prime and m is a positive integer. It is
another good way to deal with the ElGamal Cryptosystem using that abelian group
U(n)={x: x is a positive integer such that x&lt;n and gcd(n,x)=1} in the setting
of the discrete logarithm problem . Since I show in this paper that this new
study maintains equivalent (or better) security with the original ElGamal
cryptosystem(invented by Taher ElGamal in 1985)[1], that works over the finite
cyclic group of the finite field. It gives a better security because
theoretically ElGamal Cryptosystem with U(p^m) or with U(2p^m) is much more
secure since the possible solutions for the discrete logarithm will be
increased, and that would make this cryptosystem is hard to broken even with
thousands of years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0915</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0915</id><created>2014-05-05</created><updated>2015-01-29</updated><authors><author><keyname>Zese</keyname><forenames>Riccardo</forenames></author></authors><title>Reasoning with Probabilistic Logics</title><categories>cs.AI</categories><comments>An extended abstract / full version of a paper accepted to be
  presented at the Doctoral Consortium of the 30th International Conference on
  Logic Programming (ICLP 2014), July 19-22, Vienna, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest in the combination of probability with logics for modeling the
world has rapidly increased in the last few years. One of the most effective
approaches is the Distribution Semantics which was adopted by many logic
programming languages and in Descripion Logics. In this paper, we illustrate
the work we have done in this research field by presenting a probabilistic
semantics for description logics and reasoning and learning algorithms. In
particular, we present in detail the system TRILL P, which computes the
probability of queries w.r.t. probabilistic knowledge bases, which has been
implemented in Prolog. Note: An extended abstract / full version of a paper
accepted to be presented at the Doctoral Consortium of the 30th International
Conference on Logic Programming (ICLP 2014), July 19-22, Vienna, Austria
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0917</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0917</id><created>2014-05-05</created><updated>2015-02-18</updated><authors><author><keyname>Yun</keyname><forenames>Jinhyuk</forenames></author><author><keyname>Kim</keyname><forenames>Pan-Jun</forenames></author><author><keyname>Jeong</keyname><forenames>Hawoong</forenames></author></authors><title>Anatomy of Scientific Evolution</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>Supplementary material attached</comments><journal-ref>PLoS ONE 10, e0117388 (2015)</journal-ref><doi>10.1371/journal.pone.0117388</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quest for historically impactful science and technology provides
invaluable insight into the innovation dynamics of human society, yet many
studies are limited to qualitative and small-scale approaches. Here, we
investigate scientific evolution through systematic analysis of a massive
corpus of digitized English texts between 1800 and 2008. Our analysis reveals
great predictability for long-prevailing scientific concepts based on the
levels of their prior usage. Interestingly, once a threshold of early adoption
rates is passed even slightly, scientific concepts can exhibit sudden leaps in
their eventual lifetimes. We developed a mechanistic model to account for such
results, indicating that slowly-but-commonly adopted science and technology
surprisingly tend to have higher innate strength than fast-and-commonly adopted
ones. The model prediction for disciplines other than science was also well
verified. Our approach sheds light on unbiased and quantitative analysis of
scientific evolution in society, and may provide a useful basis for
policy-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0921</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0921</id><created>2014-04-30</created><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author></authors><title>Gabor Filter and Rough Clustering Based Edge Detection</title><categories>cs.CV cs.AI</categories><comments>Proc. IEEE Conf. #30853, International Conference on Human Computer
  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an efficient edge detection method based on Gabor
filter and rough clustering. The input image is smoothed by Gabor function, and
the concept of rough clustering is used to focus on edge detection with soft
computational approach. Hysteresis thresholding is used to get the actual
output, i.e. edges of the input image. To show the effectiveness, the proposed
technique is compared with some other edge detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0931</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0931</id><created>2014-05-05</created><updated>2014-11-12</updated><authors><author><keyname>Traversa</keyname><forenames>Fabio L.</forenames></author><author><keyname>Di Ventra</keyname><forenames>Massimiliano</forenames></author></authors><title>Universal Memcomputing Machines</title><categories>cs.NE cond-mat.mes-hall cs.ET cs.IT math.IT</categories><journal-ref>IEEE Transactions on Neural Networks and Learning Systems, vol.
  26, is. 11, pgs. 2702 - 2715, year 2015</journal-ref><doi>10.1109/TNNLS.2015.2391182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of universal memcomputing machines (UMMs): a class of
brain-inspired general-purpose computing machines based on systems with memory,
whereby processing and storing of information occur on the same physical
location. We analytically prove that the memory properties of UMMs endow them
with universal computing power - they are Turing-complete -, intrinsic
parallelism, functional polymorphism, and information overhead, namely their
collective states can support exponential data compression directly in memory.
We also demonstrate that a UMM has the same computational power as a
non-deterministic Turing machine, namely it can solve NP--complete problems in
polynomial time. However, by virtue of its information overhead, a UMM needs
only an amount of memory cells (memprocessors) that grows polynomially with the
problem size. As an example we provide the polynomial-time solution of the
subset-sum problem and a simple hardware implementation of the same. Even
though these results do not prove the statement NP=P within the Turing
paradigm, the practical realization of these UMMs would represent a paradigm
shift from present von Neumann architectures bringing us closer to brain-like
neural computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0933</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0933</id><created>2014-05-05</created><updated>2014-08-15</updated><authors><author><keyname>Mastriani</keyname><forenames>Mario</forenames></author></authors><title>Lossless compression catalyst based on binary allocation via modular
  arithmetic</title><categories>cs.IT math.IT</categories><comments>19 pages, 16 Figures, 2 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new binary (bit-level) lossless compression catalyst method based on a
modular arithmetic, called Binary Allocation via Modular Arithmetic (BAMA), has
been introduced in this paper. In other words, BAMA is for storage and
transmission of binary sequences, digital signal, images and video, also
streaming and all kinds of digital transmission. As we said, our method does
not compress, but facilitates the action of the real compressor, in our case,
any lossless compression algorithm (Run Length Encoding, Lempel-Ziv-Welch,
Huffman, Arithmetic, etc), that is, it acts as a compression catalyst. Finally,
this catalyst allows a significant increase in the compression performance of
binary sequences, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0936</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0936</id><created>2014-05-05</created><authors><author><keyname>Jha</keyname><forenames>Mohit</forenames></author><author><keyname>Shukla</keyname><forenames>Shailja</forenames></author></authors><title>Design Of Fuzzy Logic Traffic Controller For Isolated Intersections With
  Emergency Vehicle Priority System Using MATLAB Simulation</title><categories>cs.AI cs.SY</categories><comments>7 Pages,7 Figure,CSIR Sponsored X Control Instrumentation System
  Conference 2013; ISBN 978-93-82338-93-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic is the chief puzzle problem which every country faces because of the
enhancement in number of vehicles throughout the world, especially in large
urban towns. Hence the need arises for simulating and optimizing traffic
control algorithms to better accommodate this increasing demand. Fuzzy
optimization deals with finding the values of input parameters of a complex
simulated system which result in desired output. This paper presents a MATLAB
simulation of fuzzy logic traffic controller for controlling flow of traffic in
isolated intersections. This controller is based on the waiting time and queue
length of vehicles at present green phase and vehicles queue lengths at the
other phases. The controller controls the traffic light timings and phase
difference to ascertain sebaceous flow of traffic with least waiting time and
queue length. In this paper, the isolated intersection model used consists of
two alleyways in each approach. Every outlook has different value of queue
length and waiting time, systematically, at the intersection. The maximum value
of waiting time and vehicle queue length has to be selected by using proximity
sensors as inputs to controller for the ameliorate control traffic flow at the
intersection. An intelligent traffic model and fuzzy logic traffic controller
are developed to evaluate the performance of traffic controller under different
pre-defined conditions for oleaginous flow of traffic. Additionally, this fuzzy
logic traffic controller has emergency vehicle siren sensors which detect
emergency vehicle movement like ambulance, fire brigade, Police Van etc. and
gives maximum priority to him and pass preferred signal to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0941</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0941</id><created>2014-05-05</created><authors><author><keyname>Cabrio</keyname><forenames>Elena</forenames></author><author><keyname>Villata</keyname><forenames>Serena</forenames></author></authors><title>Towards a Benchmark of Natural Language Arguments</title><categories>cs.AI cs.CL</categories><journal-ref>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connections among natural language processing and argumentation theory
are becoming stronger in the latest years, with a growing amount of works going
in this direction, in different scenarios and applying heterogeneous
techniques. In this paper, we present two datasets we built to cope with the
combination of the Textual Entailment framework and bipolar abstract
argumentation. In our approach, such datasets are used to automatically
identify through a Textual Entailment system the relations among the arguments
(i.e., attack, support), and then the resulting bipolar argumentation graphs
are analyzed to compute the accepted arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0945</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0945</id><created>2014-05-05</created><authors><author><keyname>Cheriyan</keyname><forenames>Joseph</forenames></author><author><keyname>Gao</keyname><forenames>Zhihan</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Singla</keyname><forenames>Sahil</forenames></author></authors><title>On Integrality Ratios for Asymmetric TSP in the Sherali-Adams Hierarchy</title><categories>cs.DS</categories><comments>26 pages, 7 figures. An extended abstract of this work appeared in
  the proceedings of the 40th International Colloquium on Automata, Languages,
  and Programming ({ICALP} 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the ATSP (Asymmetric Traveling Salesman Problem), and our focus is
on negative results in the framework of the Sherali-Adams (SA) Lift and Project
method.
  Our main result pertains to the standard LP (linear programming) relaxation
of ATSP, due to Dantzig, Fulkerson, and Johnson. For any fixed integer $t\geq
0$ and small $\epsilon$, $0&lt;\epsilon\ll{1}$, there exists a digraph $G$ on
$\nu=\nu(t,\epsilon)=O(t/\epsilon)$ vertices such that the integrality ratio
for level~$t$ of the SA system starting with the standard LP on $G$ is $\ge
1+\frac{1-\epsilon}{2t+3} \approx \frac43, \frac65, \frac87, \dots$. Thus, in
terms of the input size, the result holds for any $t = 0,1,\dots,\Theta(\nu)$
levels. Our key contribution is to identify a structural property of digraphs
that allows us to construct fractional feasible solutions for any level~$t$ of
the SA system starting from the standard~LP. Our hard instances are simple and
satisfy the structural property.
  There is a further relaxation of the standard LP called the balanced LP, and
our methods simplify considerably when the starting LP for the SA system is the
balanced~LP; in particular, the relevant structural property (of digraphs)
simplifies such that it is satisfied by the digraphs given by the well-known
construction of Charikar, Goemans and Karloff (CGK). Consequently, the CGK
digraphs serve as hard instances, and we obtain an integrality ratio of $1
+\frac{1-\epsilon}{t+1}$ for any level~$t$ of the SA system, where
$0&lt;\epsilon\ll{1}$ and the number of vertices is
$\nu(t,\epsilon)=O((t/\epsilon)^{(t/\epsilon)})$.
  Also, our results for the standard~LP extend to the Path-ATSP (find a min
cost Hamiltonian dipath from a given source vertex to a given sink vertex).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0947</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0947</id><created>2014-05-05</created><authors><author><keyname>Ko&#x10d;isk&#xfd;</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Learning Bilingual Word Representations by Marginalizing Alignments</title><categories>cs.CL</categories><comments>Proceedings of ACL 2014 (Short Papers)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a probabilistic model that simultaneously learns alignments and
distributed representations for bilingual data. By marginalizing over word
alignments the model captures a larger semantic context than prior work relying
on hard alignments. The advantage of this approach is demonstrated in a
cross-lingual classification task, where we outperform the prior published
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0961</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0961</id><created>2014-04-29</created><authors><author><keyname>Barthelmess</keyname><forenames>Ulrike</forenames></author><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author></authors><title>Do we need Asimov's Laws?</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this essay the stance on robots is discussed. The attitude against robots
in history, starting in Ancient Greek culture until the industrial revolution
is described. The uncanny valley and some possible explanations are given. Some
differences in Western and Asian understanding of robots are listed and finally
we answer the question raised with the title.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0962</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0962</id><created>2014-05-05</created><updated>2014-08-19</updated><authors><author><keyname>Ghosh</keyname><forenames>Esha</forenames></author><author><keyname>Ohrimenko</keyname><forenames>Olga</forenames></author><author><keyname>Tamassia</keyname><forenames>Roberto</forenames></author></authors><title>Verifiable Privacy-Preserving Member and Order Queries on a List</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the authors. The submission was
  replaced with article arXiv:1408.3843</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a formal model for membership and order queries on
privacy-preserving authenticated lists. In this model, the queries are
performed on the list stored in the cloud where data integrity and privacy have
to be maintained. We then present an efficient construction of
privacy-preserving authenticated lists based on bilinear accumulators and
bilinear maps, analyze the performance, and prove the integrity and privacy of
this construction under widely accepted assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0963</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0963</id><created>2014-05-05</created><authors><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author></authors><title>Many-Broadcast Channels: Definition and Capacity in the Degraded Case</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure. Accepted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical multiuser information theory studies the fundamental limits of
models with a fixed (often small) number of users as the coding blocklength
goes to infinity. Motivated by emerging systems with a massive number of users,
this paper studies the new {\em many-user paradigm}, where the number of users
is allowed to grow with the blocklength. The focus of this paper is the
degraded many-broadcast channel model, whose number of users may grow as fast
as linearly with the blocklength. A notion of capacity in terms of message
length is defined and an example of Gaussian degraded many-broadcast channel is
studied. In addition, a numerical example for the Gaussian degraded
many-broadcast channel with fixed transmit power constraint is solved, where
every user achieves strictly positive message length asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0982</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0982</id><created>2014-05-05</created><authors><author><keyname>Belk</keyname><forenames>James</forenames></author><author><keyname>Bleak</keyname><forenames>Collin</forenames></author></authors><title>Some undecidability results for asynchronous transducers and the
  Brin-Thompson group 2V</title><categories>math.GR cs.FL math.DS math.LO</categories><comments>16 pages, 3 figures</comments><msc-class>20F10 (Primary) 68Q45, 37B99, 03B25 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a result of Kari and Ollinger, we prove that the torsion problem for
elements of the Brin-Thompson group 2V is undecidable. As a result, we show
that there does not exist an algorithm to determine whether an element of the
rational group R of Grigorchuk, Nekrashevich, and Sushchanskii has finite
order. A modification of the construction gives other undecidability results
about the dynamics of the action of elements of 2V on Cantor Space.
Arzhantseva, Lafont, and Minasyanin prove in 2012 that there exists a finitely
presented group with solvable word problem and unsolvable torsion problem. To
our knowledge, 2V furnishes the first concrete example of such a group, and
gives an example of a direct undecidability result in the extended family of R.
Thompson type groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.0999</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.0999</id><created>2014-05-05</created><authors><author><keyname>Zhang</keyname><forenames>Shiqi</forenames></author><author><keyname>Sridharan</keyname><forenames>Mohan</forenames></author><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author><author><keyname>Wyatt</keyname><forenames>Jeremy</forenames></author></authors><title>KR$^3$: An Architecture for Knowledge Representation and Reasoning in
  Robotics</title><categories>cs.AI</categories><comments>The paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an architecture that combines the complementary
strengths of declarative programming and probabilistic graphical models to
enable robots to represent, reason with, and learn from, qualitative and
quantitative descriptions of uncertainty and knowledge. An action language is
used for the low-level (LL) and high-level (HL) system descriptions in the
architecture, and the definition of recorded histories in the HL is expanded to
allow prioritized defaults. For any given goal, tentative plans created in the
HL using default knowledge and commonsense reasoning are implemented in the LL
using probabilistic algorithms, with the corresponding observations used to
update the HL history. Tight coupling between the two levels enables automatic
selection of relevant variables and generation of suitable action policies in
the LL for each HL action, and supports reasoning with violation of defaults,
noisy observations and unreliable actions in large and complex domains. The
architecture is evaluated in simulation and on physical robots transporting
objects in indoor domains; the benefit on robots is a reduction in task
execution time of 39% compared with a purely probabilistic, but still
hierarchical, approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1001</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1001</id><created>2014-05-05</created><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Migler</keyname><forenames>Theresa</forenames></author><author><keyname>Wilfong</keyname><forenames>Gordon</forenames></author></authors><title>Density decompositions of networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new topological descriptor of a network called the density
decomposition which is a partition of the nodes of a network into regions of
uniform density. The decomposition we define is unique in the sense that a
given network has exactly one density decomposition. The number of nodes in
each partition defines a density distribution which we find is measurably
similar to the degree distribution of given real networks (social, internet,
etc.) and measurably dissimilar in synthetic networks (preferential attachment,
small world, etc.). We also show how to build networks having given density
distributions, which gives us further insight into the structure of real
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1004</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1004</id><created>2014-05-05</created><updated>2014-06-29</updated><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal M.</forenames><affiliation>GREYC</affiliation></author></authors><title>Model Consistency of Partly Smooth Regularizers</title><categories>math.OC cs.IT math.IT stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies least-square regression penalized with partly smooth
convex regularizers. This class of functions is very large and versatile
allowing to promote solutions conforming to some notion of low-complexity.
Indeed, they force solutions of variational problems to belong to a
low-dimensional manifold (the so-called model) which is stable under small
perturbations of the function. This property is crucial to make the underlying
low-complexity model robust to small noise. We show that a generalized
&quot;irrepresentable condition&quot; implies stable model selection under small noise
perturbations in the observations and the design matrix, when the
regularization parameter is tuned proportionally to the noise level. This
condition is shown to be almost a necessary condition. We then show that this
condition implies model consistency of the regularized estimator. That is, with
a probability tending to one as the number of measurements increases, the
regularized estimator belongs to the correct low-dimensional model manifold.
This work unifies and generalizes several previous ones, where model
consistency is known to hold for sparse, group sparse, total variation and
low-rank regularizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1005</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1005</id><created>2014-05-05</created><updated>2014-09-27</updated><authors><author><keyname>Rastegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Fakhraei</keyname><forenames>Shobeir</forenames></author><author><keyname>Choi</keyname><forenames>Jonghyun</forenames></author><author><keyname>Jacobs</keyname><forenames>David</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Comparing apples to apples in the evaluation of binary coding methods</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss methodological issues related to the evaluation of unsupervised
binary code construction methods for nearest neighbor search. These issues have
been widely ignored in literature. These coding methods attempt to preserve
either Euclidean distance or angular (cosine) distance in the binary embedding
space. We explain why when comparing a method whose goal is preserving cosine
similarity to one designed for preserving Euclidean distance, the original
features should be normalized by mapping them to the unit hypersphere before
learning the binary mapping functions. To compare a method whose goal is to
preserves Euclidean distance to one that preserves cosine similarity, the
original feature data must be mapped to a higher dimension by including a bias
term in binary mapping functions. These conditions ensure the fair comparison
between different binary code methods for the task of nearest neighbor search.
Our experiments show under these conditions the very simple methods (e.g. LSH
and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and
OK-means).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1008</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1008</id><created>2014-05-05</created><authors><author><keyname>Boban</keyname><forenames>Mate</forenames></author></authors><title>Realistic and Efficient Channel Modeling for Vehicular Networks</title><categories>cs.NI</categories><comments>Ph.D. Thesis, Carnegie Mellon University, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, VANET research efforts have relied heavily on simulations, due to
prohibitive costs of deploying real world testbeds. Existing channel models
implemented in discrete-event VANET simulators are by and large simple
stochastic radio models, based on the statistical properties of the chosen
environment. It was shown that such models are unable to provide satisfactory
accuracy for typical VANET scenarios.
  We performed extensive measurements in different environments (open space,
highway, suburban, urban, parking lot) to characterize in detail the impact
that vehicles have on communication in terms of received power, packet delivery
rate, and effective communication range. Since the impact of vehicles was found
to be significant, we developed a model that accounts for vehicles as
three-dimensional obstacles and takes into account their impact on the line of
sight obstruction, received signal power, packet reception rate, and message
reachability. The model is based on the empirically derived vehicle dimensions,
accurate vehicle positioning, and realistic mobility patterns. We validate the
model against extensive measurements. To enable realistic modeling in urban and
suburban environments, we developed a model that incorporates static objects as
well. The model requires minimum geographic information: the location and the
dimensions of modeled objects (vehicles, buildings, and foliage). We validate
the model against measurements and show that it successfully captures both
small-scale and large-scale propagation effects in different environments
(highway, urban, suburban, open space).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1019</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1019</id><created>2014-03-26</created><authors><author><keyname>Oli</keyname><forenames>V. C. K. P. Arul</forenames></author><author><keyname>Ponram</keyname><forenames>Elayaraja</forenames></author></authors><title>Wireless Fidelity Real Time Security System</title><categories>cs.NI cs.CR cs.CY</categories><comments>8 pages, 4 figures</comments><journal-ref>VCKP Aruloli, P Elayaraja Article: Wireless Fidelity Real Time
  Security System. International Journal of Computer Science Trends and
  Technology (IJCST) V1(1):43-50, Jul-Aug 2013. Published by Eighth Sense
  Research Group</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes about how you can secure your Wireless Network from
hackers about various threats to wireless networks, How hackers makes most use
of it and what are the security steps one should take to avoid becoming victim
of such attacks. There are plenty of opportunities to connect to public Wi-Fi
hotspots when you're on the go these days. Coffee shops, hotels, restaurants
and airports are just some of the places where you can jump online, but often
these networks are open and not secure. Whether you're using a laptop, tablet
or smartphone, you'll want to connect your device securely to protect your data
as much as possible. Otherwise an unsecured Wi-Fi connection makes it easier
for hackers to access your private files and information, and it allows
strangers to use your internet connection. Here are some simple steps you can
take to help make sure your data is safe on open public Wi-Fi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1020</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1020</id><created>2014-03-19</created><authors><author><keyname>Mukherjee</keyname><forenames>Siddhartha</forenames></author></authors><title>Study on performance improvement of oil paint image filter algorithm
  using parallel pattern library</title><categories>cs.CV cs.DC</categories><comments>12 pages, 4 figures, 4 code snippets, 4 tables, 2 graphs, 2 images of
  experimental result, Conference: CCSEA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a detailed study on the performance of oil paint image
filter algorithm with various parameters applied on an image of RGB model. Oil
Paint image processing, being very performance hungry, current research tries
to find improvement using parallel pattern library. With increasing
kernel-size, the processing time of oil paint image filter algorithm increases
exponentially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1027</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1027</id><created>2014-05-05</created><authors><author><keyname>Bao</keyname><forenames>Zhana</forenames></author></authors><title>K-NS: Section-Based Outlier Detection in High Dimensional Space</title><categories>cs.AI cs.LG stat.ML</categories><comments>10 pages, 6 figures, 3 tables. arXiv admin note: substantial text
  overlap with arXiv:1405.0869</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Finding rare information hidden in a huge amount of data from the Internet is
a necessary but complex issue. Many researchers have studied this issue and
have found effective methods to detect anomaly data in low dimensional space.
However, as the dimension increases, most of these existing methods perform
poorly in detecting outliers because of &quot;high dimensional curse&quot;. Even though
some approaches aim to solve this problem in high dimensional space, they can
only detect some anomaly data appearing in low dimensional space and cannot
detect all of anomaly data which appear differently in high dimensional space.
To cope with this problem, we propose a new k-nearest section-based method
(k-NS) in a section-based space. Our proposed approach not only detects
outliers in low dimensional space with section-density ratio but also detects
outliers in high dimensional space with the ratio of k-nearest section against
average value. After taking a series of experiments with the dimension from 10
to 10000, the experiment results show that our proposed method achieves 100%
precision and 100% recall result in the case of extremely high dimensional
space, and better improvement in low dimensional space compared to our
previously proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1059</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1059</id><created>2014-05-05</created><updated>2015-02-23</updated><authors><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Massierer</keyname><forenames>Maike</forenames></author></authors><title>Index Calculus in the Trace Zero Variety</title><categories>cs.CR</categories><comments>20 pages</comments><msc-class>primary: 14G50, 11G25, 11Y40, secondary: 11T71, 14K15, 14H52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how to apply Gaudry's index calculus algorithm for abelian
varieties to solve the discrete logarithm problem in the trace zero variety of
an elliptic curve. We treat in particular the practically relevant cases of
field extensions of degree 3 or 5. Our theoretical analysis is compared to
other algorithms present in the literature, and is complemented by results from
a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1063</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1063</id><created>2014-05-05</created><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Multipair Full-Duplex Relaying with Massive Arrays and Linear Processing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multipair decode-and-forward relay channel, where multiple
sources transmit simultaneously their signals to multiple destinations with the
help of a full-duplex relay station. We assume that the relay station is
equipped with massive arrays, while all sources and destinations have a single
antenna. The relay station uses channel estimates obtained from received pilots
and zero-forcing (ZF) or maximum-ratio combining/maximum-ratio transmission
(MRC/MRT) to process the signals. To reduce significantly the loop interference
effect, we propose two techniques: i) using a massive receive antenna array; or
ii) using a massive transmit antenna array together with very low transmit
power at the relay station. We derive an exact achievable rate in closed-form
for MRC/MRT processing and an analytical approximation of the achievable rate
for ZF processing. This approximation is very tight, especially for large
number of relay station antennas. These closed-form expressions enable us to
determine the regions where the full-duplex mode outperforms the half-duplex
mode, as well as, to design an optimal power allocation scheme. This optimal
power allocation scheme aims to maximize the energy efficiency for a given sum
spectral efficiency and under peak power constraints at the relay station and
sources. Numerical results verify the effectiveness of the optimal power
allocation scheme. Furthermore, we show that, by doubling the number of
transmit/receive antennas at the relay station, the transmit power of each
source and of the relay station can be reduced by 1.5dB if the pilot power is
equal to the signal power, and by 3dB if the pilot power is kept fixed, while
maintaining a given quality-of-service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1071</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1071</id><created>2014-05-05</created><updated>2014-07-25</updated><authors><author><keyname>Baget</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Garreau</keyname><forenames>Fabien</forenames></author><author><keyname>Mugnier</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Rocher</keyname><forenames>Swan</forenames></author></authors><title>Revisiting Chase Termination for Existential Rules and their Extension
  to Nonmonotonic Negation</title><categories>cs.AI</categories><comments>This paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existential rules have been proposed for representing ontological knowledge,
specifically in the context of Ontology- Based Data Access. Entailment with
existential rules is undecidable. We focus in this paper on conditions that
ensure the termination of a breadth-first forward chaining algorithm known as
the chase. Several variants of the chase have been proposed. In the first part
of this paper, we propose a new tool that allows to extend existing acyclicity
conditions ensuring chase termination, while keeping good complexity
properties. In the second part, we study the extension to existential rules
with nonmonotonic negation under stable model semantics, discuss the relevancy
of the chase variants for these rules and further extend acyclicity results
obtained in the positive case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1081</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1081</id><created>2014-05-05</created><updated>2014-09-22</updated><authors><author><keyname>Pham</keyname><forenames>Mai Quyen</forenames></author><author><keyname>Duval</keyname><forenames>Laurent</forenames></author><author><keyname>Chaux</keyname><forenames>Caroline</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author></authors><title>A Primal-Dual Proximal Algorithm for Sparse Template-Based Adaptive
  Filtering: Application to Seismic Multiple Removal</title><categories>physics.geo-ph cs.SY math.OC</categories><journal-ref>IEEE Transactions on Signal Processing, Volume 62, Issue 16,
  August 2014, pages 4256--4269</journal-ref><doi>10.1109/TSP.2014.2331614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unveiling meaningful geophysical information from seismic data requires to
deal with both random and structured &quot;noises&quot;. As their amplitude may be
greater than signals of interest (primaries), additional prior information is
especially important in performing efficient signal separation. We address here
the problem of multiple reflections, caused by wave-field bouncing between
layers. Since only approximate models of these phenomena are available, we
propose a flexible framework for time-varying adaptive filtering of seismic
signals, using sparse representations, based on inaccurate templates. We recast
the joint estimation of adaptive filters and primaries in a new convex
variational formulation. This approach allows us to incorporate plausible
knowledge about noise statistics, data sparsity and slow filter variation in
parsimony-promoting wavelet frames. The designed primal-dual algorithm solves a
constrained minimization problem that alleviates standard regularization issues
in finding hyperparameters. The approach demonstrates significantly good
performance in low signal-to-noise ratio conditions, both for simulated and
real field seismic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1085</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1085</id><created>2014-05-05</created><authors><author><keyname>Wickramage</keyname><forenames>Narada</forenames></author></authors><title>Rapture in the Cartesian Wall between Real World Entities and their
  Abstract Models</title><categories>cs.OH</categories><comments>International Conference on Future Trends in Computing and
  Communication Technologies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper envisages that the advancements made with respect to Big
Data (BD), High Performance Computing, etc. would give rise to a new paradigm
of concrete information models, which would closely replicate the real world
and the consequences such as self-verifying information models, BD warehouses
as intermediaries between data sources and information systems, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1091</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1091</id><created>2014-05-05</created><authors><author><keyname>Kao</keyname><forenames>David T. H.</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Linear Degrees of Freedom of the MIMO X-Channel with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>to be presented in part at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the degrees of freedom (DoF) of the multiple-input multiple-output
X-channel (MIMO XC) with delayed channel state information at the transmitters
(delayed CSIT), assuming linear coding strategies at the transmitters. We
present two results: 1) the linear sum DoF for MIMO XC with general antenna
configurations, and 2) the linear DoF region for MIMO XC with symmetric
antennas. The converse for each result is based on developing a novel
rank-ratio inequality that characterizes the maximum ratio between the
dimensions of received linear subspaces at the two multiple-antenna receivers.
The achievability of the linear sum DoF is based on a three-phase strategy, in
which during the first two phases only the transmitter with fewer antennas
exploits delayed CSIT in order to minimize the dimension of its signal at the
unintended receiver. During Phase 3, both transmitters use delayed CSIT to send
linear combinations of past transmissions such that each receiver receives a
superposition of desired message data and known interference, thus
simultaneously serving both receivers. We also derive other linear DoF outer
bounds for the MIMO XC that, in addition to the outer bounds from the sum DoF
converse and the proposed transmission strategy, allow us to characterize the
linear DoF region for symmetric antenna configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1102</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1102</id><created>2014-05-05</created><updated>2014-12-03</updated><authors><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>Convex recovery of a structured signal from independent random linear
  measurements</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>18 pages, 1 figure. To appear in &quot;Sampling Theory, a Renaissance.&quot;
  v2: minor corrections. v3: updated citations and increased emphasis on
  Mendelson's contributions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter develops a theoretical analysis of the convex programming method
for recovering a structured signal from independent random linear measurements.
This technique delivers bounds for the sampling complexity that are similar
with recent results for standard Gaussian measurements, but the argument
applies to a much wider class of measurement ensembles. To demonstrate the
power of this approach, the paper presents a short analysis of phase retrieval
by trace-norm minimization. The key technical tool is a framework, due to
Mendelson and coauthors, for bounding a nonnegative empirical process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1107</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1107</id><created>2014-05-05</created><authors><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author></authors><title>On state complexity of unions of binary factor-free languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been conjectured in 2011 by Brzozowski et al. that if $K$ and $L$ are
factor-free regular languages over a binary alphabet having state complexity
$m$ and $n$, resp, then the state complexity of $K\cup L$ is at most
$mn-(m+n)+3-\min\{m,n\}$.
  We disprove this conjecture by giving a lower bound of
$mn-(m+n)-2-\lfloor\frac{\min\{m,n\}-2}{2}\rfloor$, which exceeds the
conjectured bound whenever $\min\{m,n\}\geq 10$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1112</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1112</id><created>2014-05-05</created><authors><author><keyname>Andr&#xe9;</keyname><forenames>&#xc9;tienne</forenames><affiliation>Universit&#xe9; Paris 13, France</affiliation></author><author><keyname>Benmoussa</keyname><forenames>Mohamed Mahdi</forenames><affiliation>Universit&#xe9; Paris 13, France</affiliation></author><author><keyname>Choppy</keyname><forenames>Christine</forenames><affiliation>Universit&#xe9; Paris 13, France</affiliation></author></authors><title>Translating UML State Machines to Coloured Petri Nets Using Acceleo: A
  Report</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ESSS 2014, arXiv:1405.0554</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 150, 2014, pp. 1-7</journal-ref><doi>10.4204/EPTCS.150.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  UML state machines are widely used to specify dynamic systems behaviours.
However its semantics is described informally, thus preventing the application
of model checking techniques that could guarantee the system safety. In a
former work, we proposed a formalisation of non-concurrent UML state machines
using coloured Petri nets, so as to allow for formal verification. In this
paper, we report our experience to implement this translation in an automated
manner using the model-to-text transformation tool Acceleo. Whereas Acceleo
provides interesting features that facilitated our translation process, it also
suffers from limitations uneasy to overcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1113</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1113</id><created>2014-05-05</created><authors><author><keyname>Brunel</keyname><forenames>Julien</forenames><affiliation>ONERA, Toulouse, France</affiliation></author><author><keyname>Rioux</keyname><forenames>Laurent</forenames><affiliation>Thales Research and Technology</affiliation></author><author><keyname>Paul</keyname><forenames>St&#xe9;phane</forenames><affiliation>Thales Research and Technology</affiliation></author><author><keyname>Faucogney</keyname><forenames>Anthony</forenames><affiliation>All4Tec</affiliation></author><author><keyname>Vall&#xe9;e</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames><affiliation>All4Tec</affiliation></author></authors><title>Formal Safety and Security Assessment of an Avionic Architecture with
  Alloy</title><categories>cs.SE</categories><comments>In Proceedings ESSS 2014, arXiv:1405.0554</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 150, 2014, pp. 8-19</journal-ref><doi>10.4204/EPTCS.150.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach based on Alloy to formally model and assess a system
architecture with respect to safety and security requirements. We illustrate
this approach by considering as a case study an avionic system developed by
Thales, which provides guidance to aircraft. We show how to define in Alloy a
metamodel of avionic architectures with a focus on failure propagations. We
then express the specific architecture of the case study in Alloy. Finally, we
express and check properties that refer to the robustness of the architecture
to failures and attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1114</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1114</id><created>2014-05-05</created><authors><author><keyname>Diekmann</keyname><forenames>Cornelius</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author><author><keyname>Hupel</keyname><forenames>Lars</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author><author><keyname>Carle</keyname><forenames>Georg</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>Directed Security Policies: A Stateful Network Implementation</title><categories>cs.CR cs.LO</categories><comments>In Proceedings ESSS 2014, arXiv:1405.0554</comments><proxy>EPTCS</proxy><acm-class>C.2.1; C.2.3; D.4.6</acm-class><journal-ref>EPTCS 150, 2014, pp. 20-34</journal-ref><doi>10.4204/EPTCS.150.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large systems are commonly internetworked. A security policy describes the
communication relationship between the networked entities. The security policy
defines rules, for example that A can connect to B, which results in a directed
graph. However, this policy is often implemented in the network, for example by
firewalls, such that A can establish a connection to B and all packets
belonging to established connections are allowed. This stateful implementation
is usually required for the network's functionality, but it introduces the
backflow from B to A, which might contradict the security policy. We derive
compliance criteria for a policy and its stateful implementation. In
particular, we provide a criterion to verify the lack of side effects in linear
time. Algorithms to automatically construct a stateful implementation of
security policy rules are presented, which narrows the gap between
formalization and real-world implementation. The solution scales to large
networks, which is confirmed by a large real-world case study. Its correctness
is guaranteed by the Isabelle/HOL theorem prover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1115</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1115</id><created>2014-05-05</created><authors><author><keyname>Guernic</keyname><forenames>Gurvan Le</forenames><affiliation>DGA Ma&#xee;trise de l'Information, France</affiliation></author></authors><title>In my Wish List, an Automated Tool for Fail-Secure Design Analysis: an
  Alloy-Based Feasibility Draft</title><categories>cs.CR</categories><comments>In Proceedings ESSS 2014, arXiv:1405.0554</comments><proxy>EPTCS</proxy><acm-class>C.4 [Performance of Systems] --- Design studies, Fault tolerance,
  Modeling techniques; B.8.1 [Performance and Reliability]: Reliability,
  Testing, and Fault-Tolerance</acm-class><journal-ref>EPTCS 150, 2014, pp. 50-55</journal-ref><doi>10.4204/EPTCS.150.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system is said to be fail-secure, sometimes confused with fail-safe, if it
maintains its security requirements even in the event of some faults.
Fail-secure analyses are required by some validation schemes, such as some
Common Criteria or NATO certifications. However, it is an aspect of security
which as been overlooked by the community. This paper attempts to shed some
light on the fail-secure field of study by: giving a definition of fail-secure
as used in those certification schemes, and emphasizing the differences with
fail-safe; and exhibiting a first feasibility draft of a fail-secure design
analysis tool based on the Alloy model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1116</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1116</id><created>2014-05-05</created><authors><author><keyname>Hauzar</keyname><forenames>David</forenames><affiliation>Department of Distributed and Dependable Systems, Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic</affiliation></author><author><keyname>Kofro&#x148;</keyname><forenames>Jan</forenames><affiliation>Department of Distributed and Dependable Systems, Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic</affiliation></author><author><keyname>Ba&#x161;teck&#xfd;</keyname><forenames>Pavel</forenames><affiliation>Department of Distributed and Dependable Systems, Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic</affiliation></author></authors><title>Data-flow Analysis of Programs with Associative Arrays</title><categories>cs.SE cs.PL</categories><comments>In Proceedings ESSS 2014, arXiv:1405.0554</comments><proxy>EPTCS</proxy><acm-class>D.2.4;F.3.2;I.2.2</acm-class><journal-ref>EPTCS 150, 2014, pp. 56-70</journal-ref><doi>10.4204/EPTCS.150.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic programming languages, such as PHP, JavaScript, and Python, provide
built-in data structures including associative arrays and objects with similar
semantics-object properties can be created at run-time and accessed via
arbitrary expressions. While a high level of security and safety of
applications written in these languages can be of a particular importance
(consider a web application storing sensitive data and providing its
functionality worldwide), dynamic data structures pose significant challenges
for data-flow analysis making traditional static verification methods both
unsound and imprecise. In this paper, we propose a sound and precise approach
for value and points-to analysis of programs with associative arrays-like data
structures, upon which data-flow analyses can be built. We implemented our
approach in a web-application domain-in an analyzer of PHP code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1117</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1117</id><created>2014-05-05</created><updated>2014-10-02</updated><authors><author><keyname>Dytso</keyname><forenames>Alex</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>On the Two-user Interference Channel with Lack of Knowledge of the
  Interference Codebook at one Receiver</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-user information theory it is often assumed that every node in the
network possesses all codebooks used in the network. This assumption may be
impractical in distributed ad-hoc, cognitive or heterogeneous networks. This
work considers the two-user Interference Channel with one Oblivious Receiver
(IC-OR), i.e., one receiver lacks knowledge of the interfering cookbook while
the other receiver knows both codebooks. The paper asks whether, and if so how
much, the channel capacity of the IC-OR is reduced compared to that of the
classical IC where both receivers know all codebooks. A novel outer bound is
derived and shown to be achievable to within a gap for the class of injective
semi-deterministic IC-ORs; the gap is shown to be zero for injective fully
deterministic IC-ORs. For the linear deterministic IC-OR that models the
Gaussian noise channel at high SNR, non i.i.d. Bernoulli(1/2) input bits are
shown to achieve points not achievable by i.i.d. Bernoulli(1/2) input bits used
in the same achievability scheme. For the real-valued Gaussian IC-OR the gap is
shown to be at most 1/2 bit per channel use, even though the set of optimal
input distributions for the derived outer bound could not be determined.
Towards understanding the Gaussian IC-OR, an achievability strategy is
evaluated in which the input alphabets at the non-oblivious transmitter are a
mixture of discrete and Gaussian random variables, where the cardinality of the
discrete part is appropriately chosen as a function of the channel parameters.
Surprisingly, as the oblivious receiver intuitively should not be able to
'jointly decode' the intended and interfering messages (whose codebook is
unavailable), it is shown that with this choice of input, the capacity region
of the symmetric Gaussian IC-OR is to within 3.34 bits (per channel use per
user) of an outer bound for the classical Gaussian IC with full codebook
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1119</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1119</id><created>2014-05-05</created><updated>2015-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Yishi</forenames></author><author><keyname>Yang</keyname><forenames>Chao</forenames></author><author><keyname>Yang</keyname><forenames>Anrong</forenames></author><author><keyname>Xiong</keyname><forenames>Chan</forenames></author><author><keyname>Zhou</keyname><forenames>Xingchi</forenames></author><author><keyname>Zhang</keyname><forenames>Zigang</forenames></author></authors><title>Feature selection for classification with class-separability strategy
  and data envelopment analysis</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>23 pages, 12 figures</comments><msc-class>68T10, 90C05, 94A17, 62B10, 68U35</msc-class><acm-class>I.5.2; G.1.6; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel feature selection method is presented, which is based
on Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). To
better capture the relationship between features and the class, class labels
are separated into individual variables and relevance and redundancy are
explicitly handled on each class label. Super-efficiency DEA is employed to
evaluate and rank features via their conditional dependence scores on all class
labels, and the feature with maximum super-efficiency score is then added in
the conditioning set for conditional dependence estimation in the next
iteration, in such a way as to iteratively select features and get the final
selected features. Eventually, experiments are conducted to evaluate the
effectiveness of proposed method comparing with four state-of-the-art methods
from the viewpoint of classification accuracy. Empirical results verify the
feasibility and the superiority of proposed feature selection method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1124</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1124</id><created>2014-05-05</created><authors><author><keyname>Balduccini</keyname><forenames>Marcello</forenames></author><author><keyname>Regli</keyname><forenames>William C.</forenames></author><author><keyname>Nguyen</keyname><forenames>Duc N.</forenames></author></authors><title>An ASP-Based Architecture for Autonomous UAVs in Dynamic Environments:
  Progress Report</title><categories>cs.AI</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional AI reasoning techniques have been used successfully in many
domains, including logistics, scheduling and game playing. This paper is part
of a project aimed at investigating how such techniques can be extended to
coordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments.
Specifically challenging are real-world environments where UAVs and other
network-enabled devices must communicate to coordinate---and communication
actions are neither reliable nor free. Such network-centric environments are
common in military, public safety and commercial applications, yet most
research (even multi-agent planning) usually takes communications among
distributed agents as a given. We address this challenge by developing an agent
architecture and reasoning algorithms based on Answer Set Programming (ASP).
ASP has been chosen for this task because it enables high flexibility of
representation, both of knowledge and of reasoning tasks. Although ASP has been
used successfully in a number of applications, and ASP-based architectures have
been studied for about a decade, to the best of our knowledge this is the first
practical application of a complete ASP-based agent architecture. It is also
the first practical application of ASP involving a combination of centralized
reasoning, decentralized reasoning, execution monitoring, and reasoning about
network communications. This work has been empirically validated using a
distributed network-centric software evaluation testbed and the results provide
guidance to designers in how to understand and control intelligent systems that
operate in these environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1127</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1127</id><created>2014-05-05</created><authors><author><keyname>Jiang</keyname><forenames>Wanchun</forenames></author><author><keyname>Ren</keyname><forenames>Fengyuan</forenames></author><author><keyname>Yue</keyname><forenames>Xin</forenames></author><author><keyname>Lin</keyname><forenames>Chuang</forenames></author></authors><title>Scale Congestion Control to Ultra-High Speed Ethernet</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, Ethernet is broadly used in LAN, datacenter and enterprise
networks, storage networks, high performance computing networks and so on.
Along with the popularity of Ethernet comes the requirement of enhancing
Ethernet with congestion control. On the other hand, Ethernet speed extends to
40Gbps and 100Gbps recently, and even 400Gbps in the near future. The
ultra-high speed requires congestion control algorithms to adapt to the broad
changes of bandwidth, and highlights the impacts of small delay by enlarging
the bandwidth delay product. The state-of-art standard QCN is heuristically
designed for the 1Gbps and 10Gbps Ethernet, and unaware of the challenges
accompanying the ultra-high speed.
  To scale congestion control to ultra-high speed Ethernet, we propose the
Adaptive Sliding Mode (ASM) congestion control algorithm, which is simple,
stable, has fast and smooth convergence process, can tolerate the impacts of
delay and adapt to the wide changes of bandwidth. Real experiments and
simulations confirm these good properties and show that ASM outperforms QCN.
Designing ASM, we find that the derivative of queue length is helpful to rate
adjustment because it reflects the difference between bandwidth and aggregated
sending rate. We also argue for enforcing congestion control system staying at
the congestion boundary line, along which it automatically slides to stable
point. These insights are also valuable to develop other congestion control
algorithms in ultra-high speed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1129</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1129</id><created>2014-05-05</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author><author><keyname>Gharehshiran</keyname><forenames>Omid Namvar</forenames></author><author><keyname>Hamdi</keyname><forenames>Maziyar</forenames></author></authors><title>Interactive Sensing and Decision Making in Social Networks</title><categories>cs.SI</categories><comments>Foundations and Trends in Signal Processing, Now Publishers, 2014</comments><doi>10.1561/2000000048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of social media such as real time microblogging and online
reputation systems facilitate real time sensing of social patterns and
behavior. In the last decade, sensing and decision making in social networks
have witnessed significant progress in the electrical engineering, computer
science, economics, finance, and sociology research communities. Research in
this area involves the interaction of dynamic random graphs, socio-economic
analysis, and statistical inference algorithms. This monograph provides a
survey, tutorial development, and discussion of four highly stylized examples:
social learning for interactive sensing; tracking the degree distribution of
social networks; sensing and information diffusion; and coordination of
decision making via game-theoretic learning. Each of the four examples is
motivated by practical examples, and comprises of a literature survey together
with careful problem formulation and mathematical analysis. Despite being
highly stylized, these examples provide a rich variety of models, algorithms
and analysis tools that are readily accessible to a signal processing,
control/systems theory, and applied mathematics audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1131</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1131</id><created>2014-05-05</created><updated>2014-06-13</updated><authors><author><keyname>Nassif</keyname><forenames>Ali Bou</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author></authors><title>Analyzing the Non-Functional Requirements in the Desharnais Dataset for
  Software Effort Estimation</title><categories>cs.SE</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying the quality requirements (aka Non-Functional Requirements (NFR)) of
a system is crucial in Requirements Engineering. Many software projects fail
because of neglecting or failing to incorporate the NFR during the software
life development cycle. This paper focuses on analyzing the importance of the
quality requirements attributes in software effort estimation models based on
the Desharnais dataset. The Desharnais dataset is a collection of eighty one
software projects of twelve attributes developed by a Canadian software house.
The analysis includes studying the influence of each of the quality
requirements attributes, as well as the influence of all quality requirements
attributes combined when calculating software effort using regression and
Artificial Neural Network (ANN) models. The evaluation criteria used in this
investigation include the Mean of the Magnitude of Relative Error (MMRE), the
Prediction Level (PRED), Root Mean Squared Error (RMSE), Mean Error and the
Coefficient of determination (R2). Results show that the quality attribute
Language is the most statistically significant when calculating software
effort. Moreover, if all quality requirements attributes are eliminated in the
training stage and software effort is predicted based on software size only,
the value of the error (MMRE) is doubled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1133</identifier>
 <datestamp>2014-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1133</id><created>2014-05-05</created><updated>2014-08-12</updated><authors><author><keyname>Bercea</keyname><forenames>Ioana O.</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>On Computing Maximal Independent Sets of Hypergraphs in Parallel</title><categories>cs.DS cs.DC</categories><acm-class>G.2.2; F.1.2; F.2.2; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whether or not the problem of finding maximal independent sets (MIS) in
hypergraphs is in (R)NC is one of the fundamental problems in the theory of
parallel computing. Unlike the well-understood case of MIS in graphs, for the
hypergraph problem, our knowledge is quite limited despite considerable work.
It is known that the problem is in \emph{RNC} when the edges of the hypergraph
have constant size. For general hypergraphs with $n$ vertices and $m$ edges,
the fastest previously known algorithm works in time $O(\sqrt{n})$ with
$\text{poly}(m,n)$ processors. In this paper we give an EREW PRAM algorithm
that works in time $n^{o(1)}$ with $\text{poly}(m,n)$ processors on general
hypergraphs satisfying $m \leq n^{\frac{\log^{(2)}n}{8(\log^{(3)}n)^2}}$, where
$\log^{(2)}n = \log\log n$ and $\log^{(3)}n = \log\log\log n$. Our algorithm is
based on a sampling idea that reduces the dimension of the hypergraph and
employs the algorithm for constant dimension hypergraphs as a subroutine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1143</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1143</id><created>2014-05-05</created><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>Amir Salman</forenames></author></authors><title>Approximate Capacity of the Two-User MISO Broadcast Channel with Delayed
  CSIT</title><categories>cs.IT math.IT</categories><comments>presented at the Fifty-First Annual Allerton Conference on
  Communication, Control, and Computing, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of the two-user multiple-input single-output complex
Gaussian Broadcast Channel where the transmitter has access to delayed
knowledge of the channel state information. We characterize the capacity region
of this channel to within a constant number of bits for all values of the
transmit power. The proposed signaling strategy utilizes the delayed knowledge
of the channel state information and the previously transmitted signals, in
order to create a signal of common interest for both receivers. This signal is
the quantized version of the summation of the previously transmitted signals.
To guarantee the independence of quantization noise and signal, we extend the
framework of lattice quantizers with dither, together with an interleaving
step. For converse, we use the fact that the capacity region of this problem is
upper-bounded by the capacity region of a physically degraded broadcast channel
with no channel state information where one receiver has two antennas. We then
derive an outer-bound on the capacity region of this degraded broadcast channel
which in turn provides an outer-bound on the capacity region of the two-user
multiple-input single-output complex Gaussian broadcast channel with delayed
knowledge of the channel state information. By careful examination, we show
that the achievable rate region and the outer-bound are within 1.81 bits/sec/Hz
per user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1153</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1153</id><created>2014-05-06</created><authors><author><keyname>Bashar</keyname><forenames>Farhana</forenames></author><author><keyname>Salehin</keyname><forenames>S. M. Akramus</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara D.</forenames></author></authors><title>Analysis of Degrees of Freedom of Wideband Random Multipath Fields
  Observed Over Time and Space Windows</title><categories>cs.IT math.IT</categories><comments>9 pages, 2 figures, Accepted in 2014 IEEE Workshop on Statistical
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multipath systems, available degrees of freedom can be considered as a key
performance indicator, since the channel capacity grows linearly with the
available degrees of freedom. However, a fundamental question arises: given a
size limitation on the observable region, what is the intrinsic number of
degrees of freedom available in a wideband random multipath wavefield observed
over a finite time interval? In this paper, we focus on answering this question
by modelling the wavefield as a sum of orthogonal waveforms or spatial orders.
We show that for each spatial order, (i) the observable wavefield is band
limited within an effective bandwidth rather than the given bandwidth and (ii)
the observation time varies from the given observation time. These findings
show the strong coupling between space and time as well as space and bandwidth.
In effect, for spatially diverse multipath wavefields, the classical degrees of
freedom result of &quot;time-bandwidth&quot; product does not directly extend to
&quot;time-space-bandwidth&quot; product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1155</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1155</id><created>2014-05-06</created><authors><author><keyname>Abou-zeid</keyname><forenames>Hatem</forenames></author><author><keyname>Hassanein</keyname><forenames>Hossam S.</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Feteiha</keyname><forenames>Mohamed</forenames></author></authors><title>A Lookback Scheduling Framework for Long-Term Quality-of-Service Over
  Multiple Cells</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In current cellular networks, schedulers allocate wireless channel resources
to users based on instantaneous channel gains and short-term moving averages of
user rates and queue lengths. By using only such short-term information,
schedulers ignore the users' service history in previous cells and, thus,
cannot guarantee long-term Quality of Service (QoS) when users traverse
multiple cells with varying load and capacity. In this paper, we propose a new
Long-term Lookback Scheduling (LLS) framework, which extends conventional
short-term scheduling with long-term QoS information from previously traversed
cells. We demonstrate the application of LLS for common channel-aware, as well
as channel and queue-aware schedulers. The developed long-term schedulers also
provide a controllable trade-off between emphasizing the immediate user QoS or
the long-term measures. Our simulation results show high gains in long-term QoS
without sacrificing short-term user requirements. Therefore, the proposed
scheduling approach improves subscriber satisfaction and increases operational
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1156</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1156</id><created>2014-05-06</created><updated>2015-01-08</updated><authors><author><keyname>Dong</keyname><forenames>Yishun</forenames></author><author><keyname>Farnia</keyname><forenames>Farzan</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author></authors><title>Near Optimal Energy Control and Approximate Capacity of Energy
  Harvesting Communication</title><categories>cs.IT math.IT</categories><comments>To appear in JSAC Special Issue on Wireless Communications Powered by
  Energy Harvesting and Wireless Energy Transfer. A shorter version presented
  at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an energy-harvesting communication system where a transmitter
powered by an exogenous energy arrival process and equipped with a finite
battery of size $B_{max}$ communicates over a discrete-time AWGN channel. We
first concentrate on a simple Bernoulli energy arrival process where at each
time step, either an energy packet of size $E$ is harvested with probability
$p$, or no energy is harvested at all, independent of the other time steps. We
provide a near optimal energy control policy and a simple approximation to the
information-theoretic capacity of this channel. Our approximations for both
problems are universal in all the system parameters involved ($p$, $E$ and
$B_{max}$), i.e. we bound the approximation gaps by a constant independent of
the parameter values. Our results suggest that a battery size $B_{max}\geq E$
is (approximately) sufficient to extract the infinite battery capacity of this
channel. We then extend our results to general i.i.d. energy arrival processes.
Our approximate capacity characterizations provide important insights for the
optimal design of energy harvesting communication systems in the regime where
both the battery size and the average energy arrival rate are large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1157</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1157</id><created>2014-05-06</created><authors><author><keyname>Qudaih</keyname><forenames>Hani A.</forenames></author><author><keyname>Bawazir</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Usman</keyname><forenames>Shuaibu Hassan</forenames></author><author><keyname>Ibrahim</keyname><forenames>Jamaludin</forenames></author></authors><title>Persuasive Technology Contributions Toward Enhance Information Security
  Awareness in an Organization</title><categories>cs.CY</categories><comments>7 pages, 1 table, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V10(4):180-186, Apr 2014. ISSN:2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V10P131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persuasion is part and parcel of human interaction. The human persuaders in
society have been always exit, masters of rhetoric skilled of changing our
minds, or at least our behaviors. Leaders, mothers, salesmen, and teachers are
clear examples of persuaders. Persuaders often turn to technology and digital
media to amplify their persuasive ends. Besides, our lives and how we lead them
influenced by technologies and digital media,but for the most part, their
effects on our attitudes and behaviors have been incidental, even accidental.
Although, nowadays, the use of computers to sell products and services
considered as the most frequent application of persuasive technology. In this
short paper, based on an extensive review of literatures, we aim to give a
brief introduction to persuasive technology, and how it can play a role and
contribute to enhance and deliver the best practice of IT. Some challenges of
persuasive technology have been discussed. At the end, some recommendations and
steps should be taken place to empower IT professional practices have been
listed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1167</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1167</id><created>2014-05-06</created><updated>2014-10-27</updated><authors><author><keyname>Saad</keyname><forenames>George</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author></authors><title>Self-Healing Computation</title><categories>cs.DC cs.CR</categories><comments>17 pages and 1 figure. It is submitted to SSS'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the problem of reliable multiparty computation (RC), there are $n$
parties, each with an individual input, and the parties want to jointly compute
a function $f$ over $n$ inputs. The problem is complicated by the fact that an
omniscient adversary controls a hidden fraction of the parties.
  We describe a self-healing algorithm for this problem. In particular, for a
fixed function $f$, with $n$ parties and $m$ gates, we describe how to perform
RC repeatedly as the inputs to $f$ change. Our algorithm maintains the
following properties, even when an adversary controls up to $t \leq
(\frac{1}{4} - \epsilon) n$ parties, for any constant $\epsilon &gt;0$. First, our
algorithm performs each reliable computation with the following amortized
resource costs: $O(m + n \log n)$ messages, $O(m + n \log n)$ computational
operations, and $O(\ell)$ latency, where $\ell$ is the depth of the circuit
that computes $f$. Second, the expected total number of corruptions is $O(t
(\log^{*} m)^2)$, after which the adversarially controlled parties are
effectively quarantined so that they cause no more corruptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1183</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1183</id><created>2014-05-06</created><authors><author><keyname>Berre</keyname><forenames>Daniel Le</forenames></author></authors><title>Some thoughts about benchmarks for NMR</title><categories>cs.AI</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NMR community would like to build a repository of benchmarks to push
forward the design of systems implementing NMR as it has been the case for many
other areas in AI. There are a number of lessons which can be learned from the
experience of other communi- ties. Here are a few thoughts about the
requirements and choices to make before building such a repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1189</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1189</id><created>2014-05-06</created><updated>2014-08-21</updated><authors><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author></authors><title>The Huge Multiway Table Problem</title><categories>math.OC cs.CC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><journal-ref>Discrete Optimization, 14:72-77, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding the existence of an $l\times m\times n$ integer threeway table with
given line-sums is NP-complete already for fixed $l=3$, but is in P with both
$l,m$ fixed. Here we consider {\em huge} tables, where the variable dimension
$n$ is encoded in {\em binary}. Combining recent results on integer cones and
Graver bases, we show that if the number of {\em layer types} is fixed, then
the problem is in P, whereas if it is variable, then the problem is in NP
intersect coNP. Our treatment goes through the more general class of $n$-fold
integer programming problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1192</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1192</id><created>2014-05-06</created><authors><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Schon</keyname><forenames>Claudia</forenames></author></authors><title>Semantically Guided Evolution of $\mathcal{SHI}$ ABoxes</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for the evolution of SHI ABoxes which is based
on a compilation technique of the knowledge base. For this the ABox is regarded
as an interpretation of the TBox which is close to a model. It is shown, that
the ABox can be used for a semantically guided transformation resulting in an
equisatisfiable knowledge base. We use the result of this transformation to
efficiently delete assertions from the ABox. Furthermore, insertion of
assertions as well as repair of inconsistent ABoxes is addressed. For the
computation of the necessary actions for deletion, insertion and repair, the
E-KRHyper theorem prover is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1194</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1194</id><created>2014-05-06</created><updated>2014-11-24</updated><authors><author><keyname>Boufounos</keyname><forenames>Petros T.</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author></authors><title>Quantization and Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>35 pages, 20 figures, to appear in Springer book &quot;Compressed Sensing
  and Its Applications&quot;, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantization is an essential step in digitizing signals, and, therefore, an
indispensable component of any modern acquisition system. This book chapter
explores the interaction of quantization and compressive sensing and examines
practical quantization strategies for compressive acquisition systems.
Specifically, we first provide a brief overview of quantization and examine
fundamental performance bounds applicable to any quantization approach. Next,
we consider several forms of scalar quantizers, namely uniform, non-uniform,
and 1-bit. We provide performance bounds and fundamental analysis, as well as
practical quantizer designs and reconstruction algorithms that account for
quantization. Furthermore, we provide an overview of Sigma-Delta
($\Sigma\Delta$) quantization in the compressed sensing context, and also
discuss implementation issues, recovery algorithms and performance bounds. As
we demonstrate, proper accounting for quantization and careful quantizer design
has significant impact in the performance of a compressive acquisition system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1196</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1196</id><created>2014-05-06</created><authors><author><keyname>Mingesz</keyname><forenames>Robert</forenames></author><author><keyname>Vadai</keyname><forenames>Gergely</forenames></author><author><keyname>Gingl</keyname><forenames>Zoltan</forenames></author></authors><title>What kind of noise guarantees security for the
  Kirchhoff-Loop-Johnson-Noise key exchange?</title><categories>cs.CR</categories><journal-ref>Fluctuation and Noise Letters 13:(3) E1450021 (2014)</journal-ref><doi>10.1142/S0219477514500217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is a supplement to our recent one about the analysis of the
noise properties in the Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange
system [Gingl and Mingesz, PLOS ONE 9 (2014) e96109,
doi:10.1371/journal.pone.0096109]. Here we use purely mathematical statistical
derivations to prove that only normal distribution with special scaling can
guarantee security. Our results are in agreement with earlier physical
assumptions [Kish, Phys. Lett. A 352 (2006) 178-182, doi:
10.1016/j.physleta.2005.11.062]. Furthermore, we have carried out numerical
simulations to show that the communication is clearly unsecure for improper
selection of the noise properties. Protection against attacks using time and
correlation analysis is not considered in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1207</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1207</id><created>2014-05-06</created><authors><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Qian</keyname><forenames>Jianjun</forenames></author><author><keyname>Luo</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Fanlong</forenames></author><author><keyname>Gao</keyname><forenames>Yicheng</forenames></author></authors><title>Nuclear Norm based Matrix Regression with Applications to Face
  Recognition with Occlusion and Illumination Changes</title><categories>cs.CV</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently regression analysis becomes a popular tool for face recognition. The
existing regression methods all use the one-dimensional pixel-based error
model, which characterizes the representation error pixel by pixel individually
and thus neglects the whole structure of the error image. We observe that
occlusion and illumination changes generally lead to a low-rank error image. To
make use of this low-rank structural information, this paper presents a
two-dimensional image matrix based error model, i.e. matrix regression, for
face representation and classification. Our model uses the minimal nuclear norm
of representation error image as a criterion, and the alternating direction
method of multipliers method to calculate the regression coefficients. Compared
with the current regression methods, the proposed Nuclear Norm based Matrix
Regression (NMR) model is more robust for alleviating the effect of
illumination, and more intuitive and powerful for removing the structural noise
caused by occlusion. We experiment using four popular face image databases, the
Extended Yale B database, the AR database, the Multi-PIE and the FRGC database.
Experimental results demonstrate the performance advantage of NMR over the
state-of-the-art regression based face recognition methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1213</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1213</id><created>2014-05-06</created><updated>2014-05-27</updated><authors><author><keyname>Danielsson</keyname><forenames>Oscar</forenames></author><author><keyname>Aghazadeh</keyname><forenames>Omid</forenames></author></authors><title>Human Pose Estimation from RGB Input Using Synthetic Training Data</title><categories>cs.CV</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of estimating the pose of humans using RGB image
input. More specifically, we are using a random forest classifier to classify
pixels into joint-based body part categories, much similar to the famous Kinect
pose estimator [11], [12]. However, we are using pure RGB input, i.e. no depth.
Since the random forest requires a large number of training examples, we are
using computer graphics generated, synthetic training data. In addition, we
assume that we have access to a large number of real images with bounding box
labels, extracted for example by a pedestrian detector or a tracking system. We
propose a new objective function for random forest training that uses the
weakly labeled data from the target domain to encourage the learner to select
features that generalize from the synthetic source domain to the real target
domain. We demonstrate on a publicly available dataset [6] that the proposed
objective function yields a classifier that significantly outperforms a
baseline classifier trained using the standard entropy objective [10].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1220</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1220</id><created>2014-05-06</created><authors><author><keyname>Claude</keyname><forenames>Francisco</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Ord&#xf3;&#xf1;ez</keyname><forenames>Alberto</forenames></author></authors><title>Efficient Compressed Wavelet Trees over Large Alphabets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em wavelet tree} is a flexible data structure that permits representing
sequences $S[1,n]$ of symbols over an alphabet of size $\sigma$, within
compressed space and supporting a wide range of operations on $S$. When
$\sigma$ is significant compared to $n$, current wavelet tree representations
incur in noticeable space or time overheads. In this article we introduce the
{\em wavelet matrix}, an alternative representation for large alphabets that
retains all the properties of wavelet trees but is significantly faster. We
also show how the wavelet matrix can be compressed up to the zero-order entropy
of the sequence without sacrificing, and actually improving, its time
performance. Our experimental results show that the wavelet matrix outperforms
all the wavelet tree variants along the space/time tradeoff map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1223</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1223</id><created>2014-05-06</created><updated>2014-10-06</updated><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Cheong</keyname><forenames>Otfried</forenames></author><author><keyname>Knauer</keyname><forenames>Christian</forenames></author><author><keyname>Schlipf</keyname><forenames>Lena</forenames></author></authors><title>Finding Largest Rectangles in Convex Polygons</title><categories>cs.CG</categories><comments>The time bound to approximate the maximum-perimeter rectangle is
  improved. Christian Knauer becomes coauthor</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following geometric optimization problem: find a maximum-area
rectangle and a maximum-perimeter rectangle contained in a given convex polygon
with $n$ vertices. We give exact algorithms that solve these problems in time
$O(n^3)$. We also give $(1-\varepsilon)$-approximation algorithms that take
time $O(\varepsilon^{-3/2}+ \varepsilon^{-1/2} \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1229</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1229</id><created>2014-05-06</created><authors><author><keyname>Tasharrofi</keyname><forenames>Shahab</forenames></author><author><keyname>Ternovska</keyname><forenames>Eugenia</forenames></author></authors><title>Three Semantics for Modular Systems</title><categories>cs.LO</categories><comments>Current paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we further develop the framework of Modular Systems that lays
model-theoretic foundations for combining different declarative languages,
agents and solvers. We introduce a multi-language logic of modular systems. We
define two novel semantics, a structural operational semantics, and an
inference-based semantics. We prove the new semantics are equivalent to the
original model-theoretic semantics and describe future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1234</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1234</id><created>2014-05-06</created><authors><author><keyname>Awasthi</keyname><forenames>Abhishek</forenames></author><author><keyname>L&#xe4;ssig</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Kramer</keyname><forenames>Oliver</forenames></author></authors><title>A Novel Approach to the Common Due-Date Problem on Single and Parallel
  Machines</title><categories>cs.DS math.CO</categories><comments>Book Chapter 22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel idea for the general case of the Common Due-Date
(CDD) scheduling problem. The problem is about scheduling a certain number of
jobs on a single or parallel machines where all the jobs possess different
processing times but a common due-date. The objective of the problem is to
minimize the total penalty incurred due to earliness or tardiness of the job
completions. This work presents exact polynomial algorithms for optimizing a
given job sequence for single and identical parallel machines with the run-time
complexities of $O(n \log n)$ for both cases, where $n$ is the number of jobs.
Besides, we show that our approach for the parallel machine case is also
suitable for non-identical parallel machines. We prove the optimality for the
single machine case and the runtime complexities of both. Henceforth, we extend
our approach to one particular dynamic case of the CDD and conclude the chapter
with our results for the benchmark instances provided in the OR-library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1250</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1250</id><created>2014-05-06</created><authors><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Wilhelmiina</forenames></author></authors><title>New tight approximations for Fisher's exact test</title><categories>stat.CO cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fisher's exact test is often a preferred method to estimate the significance
of statistical dependence. However, in large data sets the test is usually too
worksome to be applied, especially in an exhaustive search (data mining). The
traditional solution is to approximate the significance with the
$\chi^2$-measure, but the accuracy is often unacceptable. As a solution, we
introduce a family of upper bounds, which are fast to calculate and approximate
Fisher's $p$-value accurately. In addition, the new approximations are not
sensitive to the data size, distribution, or smallest expected counts like the
$\chi^2$-based approximation. According to both theoretical and experimental
analysis, the new approximations produce accurate results for all sufficiently
strong dependencies. The basic form of the approximation can fail with weak
dependencies, but the general form of the upper bounds can be adjusted to be
arbitrarily accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1254</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1254</id><created>2014-05-06</created><authors><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Oren</keyname><forenames>Sigal</forenames></author></authors><title>Time-Inconsistent Planning: A Computational Problem in Behavioral
  Economics</title><categories>cs.GT cs.DS cs.MA cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many settings, people exhibit behavior that is inconsistent across time
--- we allocate a block of time to get work done and then procrastinate, or put
effort into a project and then later fail to complete it. An active line of
research in behavioral economics and related fields has developed and analyzed
models for this type of time-inconsistent behavior.
  Here we propose a graph-theoretic model of tasks and goals, in which
dependencies among actions are represented by a directed graph, and a
time-inconsistent agent constructs a path through this graph. We first show how
instances of this path-finding problem on different input graphs can
reconstruct a wide range of qualitative phenomena observed in the literature on
time-inconsistency, including procrastination, abandonment of long-range tasks,
and the benefits of reduced sets of choices. We then explore a set of analyses
that quantify over the set of all graphs; among other results, we find that in
any graph, there can be only polynomially many distinct forms of
time-inconsistent behavior; and any graph in which a time-inconsistent agent
incurs significantly more cost than an optimal agent must contain a large
&quot;procrastination&quot; structure as a minor. Finally, we use this graph-theoretic
model to explore ways in which tasks can be designed to help motivate agents to
reach designated goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1279</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1279</id><created>2014-05-06</created><updated>2014-07-10</updated><authors><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author></authors><title>Generalized friendship paradox in networks with tunable degree-attribute
  correlation</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 5 figures</comments><journal-ref>Phys. Rev. E 90, 022809 (2014)</journal-ref><doi>10.1103/PhysRevE.90.022809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of interesting phenomena due to topological heterogeneities in complex
networks is the friendship paradox: Your friends have on average more friends
than you do. Recently, this paradox has been generalized for arbitrary node
attributes, called generalized friendship paradox (GFP). The origin of GFP at
the network level has been shown to be rooted in positive correlations between
degrees and attributes. However, how the GFP holds for individual nodes needs
to be understood in more detail. For this, we first analyze a solvable model to
characterize the paradox holding probability of nodes for the uncorrelated
case. Then we numerically study the correlated model of networks with tunable
degree-degree and degree-attribute correlations. In contrast to the network
level, we find at the individual level that the relevance of degree-attribute
correlation to the paradox holding probability may depend on whether the
network is assortative or dissortative. These findings help us to understand
the interplay between topological structure and node attributes in complex
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1287</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1287</id><created>2014-05-06</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author></authors><title>Semantics and Compilation of Answer Set Programming with Generalized
  Atoms</title><categories>cs.AI</categories><comments>The paper appears in the Proceedings of the 15th International
  Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><msc-class>68T30</msc-class><acm-class>I.2.4; D.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is logic programming under the stable model or
answer set semantics. During the last decade, this paradigm has seen several
extensions by generalizing the notion of atom used in these programs. Among
these, there are aggregate atoms, HEX atoms, generalized quantifiers, and
abstract constraints. In this paper we refer to these constructs collectively
as generalized atoms. The idea common to all of these constructs is that their
satisfaction depends on the truth values of a set of (non-generalized) atoms,
rather than the truth value of a single (non-generalized) atom. Motivated by
several examples, we argue that for some of the more intricate generalized
atoms, the previously suggested semantics provide unintuitive results and
provide an alternative semantics, which we call supportedly stable or SFLP
answer sets. We show that it is equivalent to the major previously proposed
semantics for programs with convex generalized atoms, and that it in general
admits more intended models than other semantics in the presence of non-convex
generalized atoms. We show that the complexity of supportedly stable models is
on the second level of the polynomial hierarchy, similar to previous proposals
and to stable models of disjunctive logic programs. Given these complexity
results, we provide a compilation method that compactly transforms programs
with generalized atoms in disjunctive normal form to programs without
generalized atoms. Variants are given for the new supportedly stable and the
existing FLP semantics, for which a similar compilation technique has not been
known so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1292</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1292</id><created>2014-05-06</created><authors><author><keyname>Khandwawala</keyname><forenames>Mustafa</forenames></author></authors><title>Belief propagation for minimum weight many-to-one matchings in the
  random complete graph</title><categories>math.PR cs.DM cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1212.6027</comments><msc-class>60C05, 68Q87</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a complete bipartite graph with vertex sets of cardinalities $n$ and $m$,
assign random weights from exponential distribution with mean 1, independently
to each edge. We show that, as $n\rightarrow\infty$, with $m = \lceil
n/\alpha\rceil$ for any fixed $\alpha&gt;1$, the minimum weight of many-to-one
matchings converges to a constant (depending on $\alpha$). Many-to-one matching
arises as an optimization step in an algorithm for genome sequencing and as a
measure of distance between finite sets. We prove that a belief propagation
(BP) algorithm converges asymptotically to the optimal solution. We use the
objective method of Aldous to prove our results. We build on previous works on
minimum weight matching and minimum weight edge-cover problems to extend the
objective method and to further the applicability of belief propagation to
random combinatorial optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1295</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1295</id><created>2014-05-06</created><updated>2014-06-13</updated><authors><author><keyname>B&#xe1;rcenas</keyname><forenames>Everardo</forenames><affiliation>Universidad Polit&#xe9;cnica de Puebla</affiliation></author><author><keyname>Lavalle</keyname><forenames>Jes&#xfa;s</forenames><affiliation>Benem&#xe9;rita Universidad Aut&#xf3;noma de Puebla</affiliation></author></authors><title>Global Numerical Constraints on Trees</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 16,
  2014) lmcs:985</journal-ref><doi>10.2168/LMCS-10(2:10)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a logical foundation to reason on tree structures with
constraints on the number of node occurrences. Related formalisms are limited
to express occurrence constraints on particular tree regions, as for instance
the children of a given node. By contrast, the logic introduced in the present
work can concisely express numerical bounds on any region, descendants or
ancestors for instance. We prove that the logic is decidable in single
exponential time even if the numerical constraints are in binary form. We also
illustrate the usage of the logic in the description of numerical constraints
on multi-directional path queries on XML documents. Furthermore, numerical
restrictions on regular languages (XML schemas) can also be concisely described
by the logic. This implies a characterization of decidable counting extensions
of XPath queries and XML schemas. Moreover, as the logic is closed under
negation, it can thus be used as an optimal reasoning framework for testing
emptiness, containment and equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1297</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1297</id><created>2014-05-06</created><authors><author><keyname>Huang</keyname><forenames>Dong</forenames></author><author><keyname>Lai</keyname><forenames>Jian-Huang</forenames></author><author><keyname>Wang</keyname><forenames>Chang-Dong</forenames></author></authors><title>Combining Multiple Clusterings via Crowd Agreement Estimation and
  Multi-Granularity Link Analysis</title><categories>stat.ML cs.LG</categories><comments>To appear in the Neurocomputing journal, 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The clustering ensemble technique aims to combine multiple clusterings into a
probably better and more robust clustering and has been receiving an increasing
attention in recent years. There are mainly two aspects of limitations in the
existing clustering ensemble approaches. Firstly, many approaches lack the
ability to weight the base clusterings without access to the original data and
can be affected significantly by the low-quality, or even ill clusterings.
Secondly, they generally focus on the instance level or cluster level in the
ensemble system and fail to integrate multi-granularity cues into a unified
model. To address these two limitations, this paper proposes to solve the
clustering ensemble problem via crowd agreement estimation and
multi-granularity link analysis. We present the normalized crowd agreement
index (NCAI) to evaluate the quality of base clusterings in an unsupervised
manner and thus weight the base clusterings in accordance with their clustering
validity. To explore the relationship between clusters, the source aware
connected triple (SACT) similarity is introduced with regard to their common
neighbors and the source reliability. Based on NCAI and multi-granularity
information collected among base clusterings, clusters, and data instances, we
further propose two novel consensus functions, termed weighted evidence
accumulation clustering (WEAC) and graph partitioning with multi-granularity
link analysis (GP-MGLA) respectively. The experiments are conducted on eight
real-world datasets. The experimental results demonstrate the effectiveness and
robustness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1298</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1298</id><created>2014-03-20</created><authors><author><keyname>Zhou</keyname><forenames>Michael X.</forenames></author></authors><title>Classic Lagrangian may not be applicable to the traveling salesman
  problem</title><categories>cs.DS</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, the dual problem for the traveling salesman problem is
constructed through the classic Lagrangian. The existence of optimality
conditions is expressed as a corresponding inverse problem. A general 4-cities
instance is given, and the numerical experiment shows that the classic
Lagrangian may not be applicable to the traveling salesman problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1300</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1300</id><created>2014-03-20</created><authors><author><keyname>Kouropoulos</keyname><forenames>Giorgos</forenames></author></authors><title>Calculation software for efficiency and penetration of a fibrous filter
  medium based on the mathematical models of air filtration</title><categories>cs.CE</categories><comments>8 pages, 2 tables, 1 figure</comments><msc-class>68N15</msc-class><acm-class>D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At this article will be created a software written in visual basic for
efficiency and penetration calculation in a fibrous filter medium for given
values of particles diameter that are retained in the filter. Initially, will
become report of mathematical models of air filtration in fibrous filters media
and then will develop the code and the graphical interface of application, that
are the base for software creation in the visual basic platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1302</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1302</id><created>2014-03-21</created><authors><author><keyname>kahanwal</keyname><forenames>Brijender</forenames></author></authors><title>Comparative Study of the Function Overloading and Function Overriding
  Using C++</title><categories>cs.PL</categories><comments>4 pages, 5 figures, 1 table</comments><journal-ref>International Journal of Advances in Engineering Sciences, 2014,
  Vol. 4(1), PP 5-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Object-Oriented Programming Systems (OOPS), these two concepts namely
function overloading and function overriding are a bit confusing to the
programmers. In this article this confusion is tried to be removed. Both of
these are the concepts which come under the polymorphism (poly means many and
morph mean forms). In the article the comparison is done in between them. For
the new programmers and the learners, it is important to understand them. The
function overloading [1] is achieved at the time of the compile and the
function overriding is achieved at the run time. The function overriding always
takes place in inheritance, but the function overloading can also take place
without inheritance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1303</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1303</id><created>2014-05-06</created><updated>2014-06-24</updated><authors><author><keyname>Barvinok</keyname><forenames>Alexander</forenames></author></authors><title>Computing the permanent of (some) complex matrices</title><categories>math.CO cs.DS math-ph math.MP math.OC</categories><comments>12 pages, results extended to hafnians and multidimensional
  permanents, minor improvements</comments><msc-class>15A15, 68C25, 68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic algorithm, which, for any given 0&lt; epsilon &lt; 1 and
an nxn real or complex matrix A=(a_{ij}) such that | a_{ij}-1| &lt; 0.19 for all
i, j computes the permanent of A within relative error epsilon in n^{O(ln n -ln
epsilon)} time. The method can be extended to computing hafnians and
multidimensional permanents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1304</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1304</id><created>2014-05-03</created><authors><author><keyname>Rahman</keyname><forenames>Akhlaqur</forenames></author><author><keyname>Tasnim</keyname><forenames>Sumaira</forenames></author></authors><title>Application of Machine Learning Techniques in Aquaculture</title><categories>cs.CE cs.LG</categories><comments>2 pages</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V10(3):214-215 Apr 2014. ISSN:2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V10P137</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present applications of different machine learning
algorithms in aquaculture. Machine learning algorithms learn models from
historical data. In aquaculture historical data are obtained from farm
practices, yields, and environmental data sources. Associations between these
different variables can be obtained by applying machine learning algorithms to
historical data. In this paper we present applications of different machine
learning algorithms in aquaculture applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1316</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1316</id><created>2014-05-06</created><authors><author><keyname>Khandwawala</keyname><forenames>Mustafa</forenames></author></authors><title>Solutions to recursive distributional equations for the mean-field TSP
  and related problems</title><categories>math.PR cs.DM</categories><msc-class>68Q87, 60C05, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several combinatorial optimization problems over random structures, the
theory of local weak convergence from probability and the cavity method from
statistical physics can be used to deduce a recursive equation for the
distribution of a quantity of interest. We show that there is a unique solution
to such a recursive distributional equation (RDE) when the optimization problem
is the traveling salesman problem (TSP) or from a related family of minimum
weight d-factor problems (which includes minimum weight matching) on a complete
graph (or complete bipartite graph) with independent and identically
distributed edge-weights from the exponential distribution. We analyze the
dynamics of the operator induced by the RDE on the space of distributions, and
prove that the iterates of the operator, starting from any arbitrary
distribution, converges to the fixed point solution, modulo shifts. The
existence of a solution to the RDE in such a problem helps in proving results
about the limit of the optimal solution of the combinatorial problem. The
convergence of the iterates of the operator is important in proving results
about the validity of belief propagation for iteratively finding the optimal
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1328</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1328</id><created>2014-05-06</created><updated>2014-06-25</updated><authors><author><keyname>Bilogrevic</keyname><forenames>Igor</forenames></author><author><keyname>Freudiger</keyname><forenames>Julien</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Uzun</keyname><forenames>Ersin</forenames></author></authors><title>What's the Gist? Privacy-Preserving Aggregation of User Profiles</title><categories>cs.CR</categories><comments>To appear in the Proceedings of ESORICS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, online service providers have started gathering
increasing amounts of personal information to build user profiles and monetize
them with advertisers and data brokers. Users have little control of what
information is processed and are often left with an all-or-nothing decision
between receiving free services or refusing to be profiled. This paper explores
an alternative approach where users only disclose an aggregate model -- the
&quot;gist&quot; -- of their data. We aim to preserve data utility and simultaneously
provide user privacy. We show that this approach can be efficiently supported
by letting users contribute encrypted and differentially-private data to an
aggregator. The aggregator combines encrypted contributions and can only
extract an aggregate model of the underlying data. We evaluate our framework on
a dataset of 100,000 U.S. users obtained from the U.S. Census Bureau and show
that (i) it provides accurate aggregates with as little as 100 users, (ii) it
generates revenue for both users and data brokers, and (iii) its overhead is
appreciably low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1332</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1332</id><created>2014-05-06</created><updated>2015-06-01</updated><authors><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>A unified framework for linear dimensionality reduction in L1</title><categories>cs.DS cs.NA math.MG math.PR</categories><comments>18 pages, 1 figure</comments><msc-class>68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a family of interpolation norms $\| \cdot \|_{1,2,s}$ on $\mathbb{R}^n$,
we provide a distribution over random matrices $\Phi_s \in \mathbb{R}^{m \times
n}$ parametrized by sparsity level $s$ such that for a fixed set $X$ of $K$
points in $\mathbb{R}^n$, if $m \geq C s \log(K)$ then with high probability,
$\frac{1}{2} \| x \|_{1,2,s} \leq \| \Phi_s (x) \|_1 \leq 2 \| x\|_{1,2,s}$ for
all $x\in X$. Several existing results in the literature reduce to special
cases of this result at different values of $s$: for $s=n$, $\| x\|_{1,2,n}
\equiv \| x \|_{1}$ and we recover that dimension reducing linear maps can
preserve the $\ell_1$-norm up to a distortion proportional to the dimension
reduction factor, which is known to be the best possible such result. For
$s=1$, $\|x \|_{1,2,1} \equiv \| x \|_{2}$, and we recover an $\ell_2 / \ell_1$
variant of the Johnson-Lindenstrauss Lemma for Gaussian random matrices.
Finally, if $x$ is $s$-sparse, then $\| x \|_{1,2,s} = \| x \|_1$ and we
recover that $s$-sparse vectors in $\ell_1^n$ embed into $\ell_1^{\mathcal{O}(s
\log(n))}$ via sparse random matrix constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1339</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1339</id><created>2014-05-06</created><authors><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Wilhelmiina</forenames></author></authors><title>General upper bounds for well-behaving goodness measures on dependency
  rules</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the search for statistical dependency rules, a crucial task is to restrict
the search space by estimating upper bounds for the goodness of yet
undiscovered rules. In this paper, we show that all well-behaving goodness
measures achieve their maximal values in the same points. Therefore, the same
generic search strategy can be applied with any of these measures. The notion
of well-behaving measures is based on the classical axioms for any proper
goodness measures, and extended to negative dependencies, as well. As an
example, we show that several commonly used goodness measures are
well-behaving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1346</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1346</id><created>2014-05-06</created><authors><author><keyname>Orobinska</keyname><forenames>Olena</forenames><affiliation>ERIC</affiliation></author></authors><title>Automatic Method Of Domain Ontology Construction based on
  Characteristics of Corpora POS-Analysis</title><categories>cs.CL</categories><comments>XV International Conference, France (2012), in Russian</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now widely recognized that ontologies, are one of the fundamental
cornerstones of knowledge-based systems. What is lacking, however, is a
currently accepted strategy of how to build ontology; what kinds of the
resources and techniques are indispensables to optimize the expenses and the
time on the one hand and the amplitude, the completeness, the robustness of en
ontology on the other hand. The paper offers a semi-automatic ontology
construction method from text corpora in the domain of radiological protection.
This method is composed from next steps: 1) text annotation with part-of-speech
tags; 2) revelation of the significant linguistic structures and forming the
templates; 3) search of text fragments corresponding to these templates; 4)
basic ontology instantiation process
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1356</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1356</id><created>2014-05-06</created><authors><author><keyname>Fafianie</keyname><forenames>Stefan</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>Streaming Kernelization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernelization is a formalization of preprocessing for combinatorially hard
problems. We modify the standard definition for kernelization, which allows any
polynomial-time algorithm for the preprocessing, by requiring instead that the
preprocessing runs in a streaming setting and uses
$\mathcal{O}(poly(k)\log|x|)$ bits of memory on instances $(x,k)$. We obtain
several results in this new setting, depending on the number of passes over the
input that such a streaming kernelization is allowed to make. Edge Dominating
Set turns out as an interesting example because it has no single-pass
kernelization but two passes over the input suffice to match the bounds of the
best standard kernelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1359</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1359</id><created>2014-05-06</created><updated>2014-11-29</updated><authors><author><keyname>Petersen</keyname><forenames>Michael Kai</forenames></author></authors><title>Latent semantics of action verbs reflect phonetic parameters of
  intensity and emotional content</title><categories>cs.CL</categories><comments>15 pages</comments><msc-class>68T50</msc-class><acm-class>I.2.4; I.2.7</acm-class><doi>10.1371/journal.pone.0121575</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conjuring up our thoughts, language reflects statistical patterns of word
co-occurrences which in turn come to describe how we perceive the world.
Whether counting how frequently nouns and verbs combine in Google search
queries, or extracting eigenvectors from term document matrices made up of
Wikipedia lines and Shakespeare plots, the resulting latent semantics capture
not only the associative links which form concepts, but also spatial dimensions
embedded within the surface structure of language. As both the shape and
movements of objects have been found to be associated with phonetic contrasts
already in toddlers, this study explores whether articulatory and acoustic
parameters may likewise differentiate the latent semantics of action verbs.
Selecting 3 x 20 emotion, face, and hand related verbs known to activate
premotor areas in the brain, their mutual cosine similarities were computed
using latent semantic analysis LSA, and the resulting adjacency matrices were
compared based on two different large scale text corpora; HAWIK and TASA.
Applying hierarchical clustering to identify common structures across the two
text corpora, the verbs largely divide into combined mouth and hand movements
versus emotional expressions. Transforming the verbs into their constituent
phonemes, the clustered small and large size movements appear differentiated by
front versus back vowels corresponding to increasing levels of arousal. Whereas
the clustered emotional verbs seem characterized by sequences of close versus
open jaw produced phonemes, generating up- or downwards shifts in formant
frequencies that may influence their perceived valence. Suggesting, that the
latent semantics of action verbs reflect parameters of intensity and emotional
polarity that appear correlated with the articulatory contrasts and acoustic
characteristics of phonemes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1360</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1360</id><created>2014-05-06</created><authors><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Wilhelmiina</forenames></author></authors><title>Assessing the statistical significance of association rules</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An association rule is statistically significant, if it has a small
probability to occur by chance. It is well-known that the traditional
frequency-confidence framework does not produce statistically significant
rules. It can both accept spurious rules (type 1 error) and reject significant
rules (type 2 error). The same problem concerns other commonly used
interestingness measures and pruning heuristics.
  In this paper, we inspect the most common measure functions - frequency,
confidence, degree of dependence, $\chi^2$, correlation coefficient, and
$J$-measure - and redundancy reduction techniques. For each technique, we
analyze whether it can make type 1 or type 2 error and the conditions under
which the error occurs. In addition, we give new theoretical results which can
be use to guide the search for statistically significant association rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1365</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1365</id><created>2014-05-06</created><authors><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Morales-Jimenez</keyname><forenames>David</forenames></author><author><keyname>Lozano</keyname><forenames>Angel</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Spectral Efficiency of Dynamic Coordinated Beamforming: A Stochastic
  Geometry Approach</title><categories>cs.IT math.IT</categories><comments>Revised version submitted to IEEE TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper characterizes the performance of coordinated beamforming with
dynamic clustering. A downlink model based on stochastic geometry is put forth
to analyze the performance of such base station (BS) coordination strategy.
Analytical expressions for the complementary cumulative distribution function
(CCDF) of the instantaneous signal-to-interference ratio (SIR) are derived in
terms of relevant system parameters, chiefly the number of BSs forming the
coordination clusters, the number of antennas per BS, and the pathloss
exponent. Utilizing this CCDF, with pilot overheads further incorporated into
the analysis, we formulate the optimization of the BS coordination clusters for
a given fading coherence. Our results indicate that (i) coordinated beamforming
is most beneficial to users that are in the outer part of their cells yet in
the inner part of their coordination cluster, and that (ii) the optimal cluster
cardinality for the typical user is small and it scales with the fading
coherence. Simulation results verify the exactness of the SIR distributions
derived for stochastic geometries, which are further compared with the
corresponding distributions for deterministic grid networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1374</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1374</id><created>2014-05-03</created><authors><author><keyname>Agarwal</keyname><forenames>Naman</forenames></author><author><keyname>Kindler</keyname><forenames>Guy</forenames></author><author><keyname>Kolla</keyname><forenames>Alexandra</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Unique Games on the Hypercube</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the validity of the Unique Games Conjecture
when the constraint graph is the boolean hypercube. We construct an almost
optimal integrality gap instance on the Hypercube for the Goemans-Williamson
semidefinite program (SDP) for Max-2-LIN$(\mathbb{Z}_2)$. We conjecture that
adding triangle inequalities to the SDP provides a polynomial time algorithm to
solve Unique Games on the hypercube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1379</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1379</id><created>2014-05-05</created><authors><author><keyname>Pichevar</keyname><forenames>Ramin</forenames></author><author><keyname>Wung</keyname><forenames>Jason</forenames></author><author><keyname>Giacobello</keyname><forenames>Daniele</forenames></author><author><keyname>Atkins</keyname><forenames>Joshua</forenames></author></authors><title>Design and Optimization of a Speech Recognition Front-End for
  Distant-Talking Control of a Music Playback Device</title><categories>cs.SD cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the challenging scenario for the distant-talking control
of a music playback device, a common portable speaker with four small
loudspeakers in close proximity to one microphone. The user controls the device
through voice, where the speech-to-music ratio can be as low as -30 dB during
music playback. We propose a speech enhancement front-end that relies on known
robust methods for echo cancellation, double-talk detection, and noise
suppression, as well as a novel adaptive quasi-binary mask that is well suited
for speech recognition. The optimization of the system is then formulated as a
large scale nonlinear programming problem where the recognition rate is
maximized and the optimal values for the system parameters are found through a
genetic algorithm. We validate our methodology by testing over the TIMIT
database for different music playback levels and noise types. Finally, we show
that the proposed front-end allows a natural interaction with the device for
limited-vocabulary voice commands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1380</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1380</id><created>2014-05-06</created><updated>2015-06-15</updated><authors><author><keyname>Zhou</keyname><forenames>Yingbo</forenames></author><author><keyname>Arpit</keyname><forenames>Devansh</forenames></author><author><keyname>Nwogu</keyname><forenames>Ifeoma</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Is Joint Training Better for Deep Auto-Encoders?</title><categories>stat.ML cs.LG cs.NE</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, when generative models of data are developed via deep
architectures, greedy layer-wise pre-training is employed. In a well-trained
model, the lower layer of the architecture models the data distribution
conditional upon the hidden variables, while the higher layers model the hidden
distribution prior. But due to the greedy scheme of the layerwise training
technique, the parameters of lower layers are fixed when training higher
layers. This makes it extremely challenging for the model to learn the hidden
distribution prior, which in turn leads to a suboptimal model for the data
distribution. We therefore investigate joint training of deep autoencoders,
where the architecture is viewed as one stack of two or more single-layer
autoencoders. A single global reconstruction objective is jointly optimized,
such that the objective for the single autoencoders at each layer acts as a
local, layer-level regularizer. We empirically evaluate the performance of this
joint training scheme and observe that it not only learns a better data model,
but also learns better higher layer representations, which highlights its
potential for unsupervised feature learning. In addition, we find that the
usage of regularizations in the joint training scheme is crucial in achieving
good performance. In the supervised setting, joint training also shows superior
performance when training deeper models. The joint training framework can thus
provide a platform for investigating more efficient usage of different types of
regularizers, especially in light of the growing volumes of available unlabeled
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1382</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1382</id><created>2014-05-06</created><authors><author><keyname>Newport</keyname><forenames>Calvin</forenames></author></authors><title>Consensus with an Abstract MAC Layer</title><categories>cs.DC</categories><comments>Appeared in PODC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study distributed consensus in the radio network setting.
We produce new upper and lower bounds for this problem in an abstract MAC layer
model that captures the key guarantees provided by most wireless MAC layers. In
more detail, we first generalize the well-known impossibility of deterministic
consensus with a single crash failure [FLP 1895] from the asynchronous message
passing model to our wireless setting. Proceeding under the assumption of no
faults, we then investigate the amount of network knowledge required to solve
consensus in our model---an important question given that these networks are
often deployed in an ad hoc manner. We prove consensus is impossible without
unique ids or without knowledge of network size (in multihop topologies). We
also prove a lower bound on optimal time complexity. We then match these lower
bounds with a pair of new deterministic consensus algorithms---one for single
hop topologies and one for multihop topologies---providing a comprehensive
characterization of the consensus problem in the wireless setting. From a
theoretical perspective, our results shed new insight into the role of network
information and the power of MAC layer abstractions in solving distributed
consensus. From a practical perspective, given the level of abstraction used by
our model, our upper bounds can be easily implemented in real wireless devices
on existing MAC layers while preserving their correctness
guarantees---facilitating the development of wireless distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1385</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1385</id><created>2014-05-06</created><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Chiang</keyname><forenames>Hsiao-Dong</forenames></author></authors><title>Quasi Steady-State Model for Power System Stability: Limitations,
  Analysis and a Remedy</title><categories>cs.SY</categories><comments>The paper has been accepted by Power Systems Computation Conference
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quasi steady-state (QSS) model tries to reach a good compromise between
accuracy and efficiency in long-term stability analysis. However, the QSS model
is unable to provide correct approximations and stability assessment for the
long-term stability model consistently. In this paper, some numerical examples
in which the QSS model was stable while the long-term stability model underwent
instabilities are presented with analysis in nonlinear system framework. At the
same time, a hybrid model which serves as a remedy to the QSS model is proposed
according to causes for failure of the QSS model and dynamic mechanisms of
long-term instabilities. Numerical examples are given to show that the
developed hybrid model can successfully capture unstable behaviors of the
long-term stability model while the QSS model fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1386</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1386</id><created>2014-05-06</created><updated>2016-03-07</updated><authors><author><keyname>Figueiredo</keyname><forenames>Isabel N.</forenames></author><author><keyname>Leal</keyname><forenames>Carlos</forenames></author><author><keyname>Romanazzi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Engquist</keyname><forenames>Bjorn</forenames></author></authors><title>Homogenization Model for Aberrant Crypt Foci</title><categories>math.AP cs.NA math.NA</categories><comments>26 pages, 4 figures</comments><msc-class>76R99, 35J15, 35B27, 47H10, 65M06, 65M50, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several explanations can be found in the literature about the origin of
colorectal cancer. There is however some agreement on the fact that the
carcinogenic process is a result of several genetic mutations of normal cells.
The colon epithelium is characterized by millions of invaginations, very small
cavities, called crypts, where most of the cellular activity occurs. It is
consensual in the medical community, that a potential first manifestation of
the carcinogenic process, observed in conventional colonoscopy images, is the
appearance of Aberrant Crypt Foci (ACF). These are clusters of abnormal crypts,
morphologically characterized by an atypical behavior of the cells that
populate the crypts. In this work an homogenization model is proposed, for
representing the cellular dynamics in the colon epithelium. The goal is to
simulate and predict, in silico, the spread and evolution of ACF, as it can be
observed in colonoscopy images. By assuming that the colon is an heterogeneous
media, exhibiting a periodic distribution of crypts, we start this work by
describing a periodic model, that represents the ACF cell-dynamics in a
two-dimensional setting. Then, homogenization techniques are applied to this
periodic model, to find a simpler model, whose solution symbolizes the averaged
behavior of ACF at the tissue level. Some theoretical results concerning the
existence of solution of the homogenized model are proven, applying a fixed
point theorem. Numerical results showing the convergence of the periodic model
to the homogenized model are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1392</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1392</id><created>2014-05-06</created><authors><author><keyname>Kumar</keyname><forenames>Shamanth</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author><author><keyname>Mehta</keyname><forenames>Sameep</forenames></author><author><keyname>Subramaniam</keyname><forenames>L. Venkata</forenames></author></authors><title>From Tweets to Events: Exploring a Scalable Solution for Twitter Streams</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unprecedented use of social media through smartphones and other
web-enabled mobile devices has enabled the rapid adoption of platforms like
Twitter. Event detection has found many applications on the web, including
breaking news identification and summarization. The recent increase in the
usage of Twitter during crises has attracted researchers to focus on detecting
events in tweets. However, current solutions have focused on static Twitter
data. The necessity to detect events in a streaming environment during fast
paced events such as a crisis presents new opportunities and challenges. In
this paper, we investigate event detection in the context of real-time Twitter
streams as observed in real-world crises. We highlight the key challenges in
this problem: the informal nature of text, and the high volume and high
velocity characteristics of Twitter streams. We present a novel approach to
address these challenges using single-pass clustering and the compression
distance to efficiently detect events in Twitter streams. Through experiments
on large Twitter datasets, we demonstrate that the proposed framework is able
to detect events in near real-time and can scale to large and noisy Twitter
streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1397</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1397</id><created>2014-05-04</created><authors><author><keyname>Ripon</keyname><forenames>Shamim</forenames></author><author><keyname>Barua</keyname><forenames>Aoyan</forenames></author><author><keyname>Uddin</keyname><forenames>Mohammad Salah</forenames></author></authors><title>Analysis Tool for UNL-Based Knowledge Representation</title><categories>cs.AI</categories><comments>8 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:cs/0404030 by other authors</comments><journal-ref>Journal of Advanced Computer Science and Technology Research
  (JACSTR) Vol. 2, No. 4, pp. 176-183, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental issue in knowledge representation is to provide a precise
definition of the knowledge that they possess in a manner that is independent
of procedural considerations, context free and easy to manipulate, exchange and
reason about. Knowledge must be accessible to everyone regardless of their
native languages. Universal Networking Language (UNL) is a declarative formal
language and a generalized form of human language in a machine independent
digital platform for defining, recapitulating, amending, storing and
dissipating knowledge among people of different affiliations. UNL extracts
semantic data from a native language for Interlingua machine translation. This
paper presents the development of a graphical tool that incorporates UNL to
provide a visual mean to represent the semantic data available in a native
text. UNL represents the semantics of a sentence as a conceptual hyper-graph.
We translate this information into XML format and create a graph from XML,
representing the actual concepts available in the native language
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1402</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1402</id><created>2014-03-24</created><authors><author><keyname>Bourgeat</keyname><forenames>Thomas</forenames></author><author><keyname>Bringer</keyname><forenames>Julien</forenames></author><author><keyname>Chabanne</keyname><forenames>Herve</forenames></author><author><keyname>Champenois</keyname><forenames>Robin</forenames></author><author><keyname>Clement</keyname><forenames>Jeremie</forenames></author><author><keyname>Ferradi</keyname><forenames>Houda</forenames></author><author><keyname>Heinrich</keyname><forenames>Marc</forenames></author><author><keyname>Melotti</keyname><forenames>Paul</forenames></author><author><keyname>Naccache</keyname><forenames>David</forenames></author><author><keyname>Voizard</keyname><forenames>Antoine</forenames></author></authors><title>New Algorithmic Approaches to Point Constellation Recognition</title><categories>cs.CV</categories><comments>14 pages, short version submitted to SEC 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Point constellation recognition is a common problem with many pattern
matching applications. Whilst useful in many contexts, this work is mainly
motivated by fingerprint matching. Fingerprints are traditionally modelled as
constellations of oriented points called minutiae. The fingerprint verifier's
task consists in comparing two point constellations. The compared
constellations may differ by rotation and translation or by much more involved
transforms such as distortion or occlusion. This paper presents three new
constellation matching algorithms. The first two methods generalize an
algorithm by Bringer and Despiegel. Our third proposal creates a very
interesting analogy between mechanical system simulation and the constellation
recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1403</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1403</id><created>2014-03-23</created><authors><author><keyname>Song</keyname><forenames>Rui</forenames></author><author><keyname>Ko</keyname><forenames>Hyunsuk</forenames></author><author><keyname>Kuo</keyname><forenames>C. C. Jay</forenames></author></authors><title>MCL-3D: a database for stereoscopic image quality assessment using
  2D-image-plus-depth source</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new stereoscopic image quality assessment database rendered using the
2D-image-plus-depth source, called MCL-3D, is described and the performance
benchmarking of several known 2D and 3D image quality metrics using the MCL-3D
database is presented in this work. Nine image-plus-depth sources are first
selected, and a depth image-based rendering (DIBR) technique is used to render
stereoscopic image pairs. Distortions applied to either the texture image or
the depth image before stereoscopic image rendering include: Gaussian blur,
additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compression
and transmission error. Furthermore, the distortion caused by imperfect
rendering is also examined. The MCL-3D database contains 693 stereoscopic image
pairs, where one third of them are of resolution 1024x728 and two thirds are of
resolution 1920x1080. The pair-wise comparison was adopted in the subjective
test for user friendliness, and the Mean Opinion Score (MOS) can be computed
accordingly. Finally, we evaluate the performance of several 2D and 3D image
quality metrics applied to MCL-3D. All texture images, depth images, rendered
image pairs in MCL-3D and their MOS values obtained in the subjective test are
available to the public (http://mcl.usc.edu/mcl-3d-database/) for future
research and development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1404</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1404</id><created>2014-05-02</created><authors><author><keyname>Soliman</keyname><forenames>Omar S.</forenames></author><author><keyname>Rassem</keyname><forenames>Aliaa</forenames></author></authors><title>A Network Intrusions Detection System based on a Quantum Bio Inspired
  Algorithm</title><categories>cs.NE cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Network intrusion detection systems (NIDSs) have a role of identifying
malicious activities by monitoring the behavior of networks. Due to the
currently high volume of networks trafic in addition to the increased number of
attacks and their dynamic properties, NIDSs have the challenge of improving
their classification performance. Bio-Inspired Optimization Algorithms (BIOs)
are used to automatically extract the the discrimination rules of normal or
abnormal behavior to improve the classification accuracy and the detection
ability of NIDS. A quantum vaccined immune clonal algorithm with the estimation
of distribution algorithm (QVICA-with EDA) is proposed in this paper to build a
new NIDS. The proposed algorithm is used as classification algorithm of the new
NIDS where it is trained and tested using the KDD data set. Also, the new NIDS
is compared with another detection system based on particle swarm optimization
(PSO). Results shows the ability of the proposed algorithm of achieving high
intrusions classification accuracy where the highest obtained accuracy is 94.8
%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1406</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1406</id><created>2014-05-06</created><authors><author><keyname>Abualhaija</keyname><forenames>Sallam</forenames></author><author><keyname>Zimmermann</keyname><forenames>Karl-Heinz</forenames></author></authors><title>D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving
  Word Sense Disambiguation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word sense disambiguation (WSD) is a problem in the field of computational
linguistics given as finding the intended sense of a word (or a set of words)
when it is activated within a certain context. WSD was recently addressed as a
combinatorial optimization problem in which the goal is to find a sequence of
senses that maximize the semantic relatedness among the target words. In this
article, a novel algorithm for solving the WSD problem called D-Bees is
proposed which is inspired by bee colony optimization (BCO)where artificial bee
agents collaborate to solve the problem. The D-Bees algorithm is evaluated on a
standard dataset (SemEval 2007 coarse-grained English all-words task corpus)and
is compared to simulated annealing, genetic algorithms, and two ant colony
optimization techniques (ACO). It will be observed that the BCO and ACO
approaches are on par.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1429</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1429</id><created>2014-05-06</created><authors><author><keyname>Cheng</keyname><forenames>Justin</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>How Community Feedback Shapes User Behavior</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>ICWSM 2014</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media systems rely on user feedback and rating mechanisms for
personalization, ranking, and content filtering. However, when users evaluate
content contributed by fellow users (e.g., by liking a post or voting on a
comment), these evaluations create complex social feedback effects. This paper
investigates how ratings on a piece of content affect its author's future
behavior. By studying four large comment-based news communities, we find that
negative feedback leads to significant behavioral changes that are detrimental
to the community. Not only do authors of negatively-evaluated content
contribute more, but also their future posts are of lower quality, and are
perceived by the community as such. Moreover, these authors are more likely to
subsequently evaluate their fellow users negatively, percolating these effects
through the community. In contrast, positive feedback does not carry similar
effects, and neither encourages rewarded authors to write more, nor improves
the quality of their posts. Interestingly, the authors that receive no feedback
are most likely to leave a community. Furthermore, a structural analysis of the
voter network reveals that evaluations polarize the community the most when
positive and negative votes are equally split.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1436</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1436</id><created>2014-05-06</created><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author><author><keyname>Frey</keyname><forenames>Brendan</forenames></author></authors><title>Training Restricted Boltzmann Machine by Perturbation</title><categories>cs.NE cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach to maximum likelihood learning of discrete graphical models
and RBM in particular is introduced. Our method, Perturb and Descend (PD) is
inspired by two ideas (I) perturb and MAP method for sampling (II) learning by
Contrastive Divergence minimization. In contrast to perturb and MAP, PD
leverages training data to learn the models that do not allow efficient MAP
estimation. During the learning, to produce a sample from the current model, we
start from a training data and descend in the energy landscape of the
&quot;perturbed model&quot;, for a fixed number of steps, or until a local optima is
reached. For RBM, this involves linear calculations and thresholding which can
be very fast. Furthermore we show that the amount of perturbation is closely
related to the temperature parameter and it can regularize the model by
producing robust features resulting in sparse hidden layer activation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1438</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1438</id><created>2014-05-06</created><authors><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author><author><keyname>Pang</keyname><forenames>Bo</forenames></author></authors><title>The effect of wording on message propagation: Topic- and
  author-controlled natural experiments on Twitter</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>11 pages, to appear in Proceedings of ACL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a person trying to spread an important message on a social network.
He/she can spend hours trying to craft the message. Does it actually matter?
While there has been extensive prior work looking into predicting popularity of
social-media content, the effect of wording per se has rarely been studied
since it is often confounded with the popularity of the author and the topic.
To control for these confounding factors, we take advantage of the surprising
fact that there are many pairs of tweets containing the same url and written by
the same user but employing different wording. Given such pairs, we ask: which
version attracts more retweets? This turns out to be a more difficult task than
predicting popular topics. Still, humans can answer this question better than
chance (but far from perfectly), and the computational methods we develop can
do better than both an average human and a strong competing method trained on
non-controlled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1439</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1439</id><created>2014-05-06</created><updated>2014-05-31</updated><authors><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>A Corpus of Sentence-level Revisions in Academic Writing: A Step towards
  Understanding Statement Strength in Communication</title><categories>cs.CL</categories><comments>6 pages, to appear in Proceedings of ACL 2014 (short paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The strength with which a statement is made can have a significant impact on
the audience. For example, international relations can be strained by how the
media in one country describes an event in another; and papers can be rejected
because they overstate or understate their findings. It is thus important to
understand the effects of statement strength. A first step is to be able to
distinguish between strong and weak statements. However, even this problem is
understudied, partly due to a lack of data. Since strength is inherently
relative, revisions of texts that make claims are a natural source of data on
strength differences. In this paper, we introduce a corpus of sentence-level
revisions from academic writing. We also describe insights gained from our
annotation efforts for this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1440</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1440</id><created>2014-05-06</created><authors><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author><author><keyname>Martin</keyname><forenames>Travis</forenames></author></authors><title>Equitable random graphs</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>5 pages, 2 figures</comments><journal-ref>Phys. Rev. E 90, 052824 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052824</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random graph models have played a dominant role in the theoretical study of
networked systems. The Poisson random graph of Erdos and Renyi, in particular,
as well as the so-called configuration model, have served as the starting point
for numerous calculations. In this paper we describe another large class of
random graph models, which we call equitable random graphs and which are
flexible enough to represent networks with diverse degree distributions and
many nontrivial types of structure, including community structure, bipartite
structure, degree correlations, stratification, and others, yet are exactly
solvable for a wide range of properties in the limit of large graph size,
including percolation properties, complete spectral density, and the behavior
of homogeneous dynamical systems, such as coupled oscillators or epidemic
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1445</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1445</id><created>2014-05-06</created><authors><author><keyname>Yang</keyname><forenames>Yimin</forenames></author><author><keyname>Wu</keyname><forenames>Q. M. Jonathan</forenames></author><author><keyname>Huang</keyname><forenames>Guangbin</forenames></author><author><keyname>Wang</keyname><forenames>Yaonan</forenames></author></authors><title>Pulling back error to the hidden-node parameter technology:
  Single-hidden-layer feedforward network without output weight</title><categories>cs.NE</categories><comments>7 pages</comments><msc-class>68Txx</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to conventional neural network theories, the feature of
single-hidden-layer feedforward neural networks(SLFNs) resorts to parameters of
the weighted connections and hidden nodes. SLFNs are universal approximators
when at least the parameters of the networks including hidden-node parameter
and output weight are exist. Unlike above neural network theories, this paper
indicates that in order to let SLFNs work as universal approximators, one may
simply calculate the hidden node parameter only and the output weight is not
needed at all. In other words, this proposed neural network architecture can be
considered as a standard SLFNs with fixing output weight equal to an unit
vector. Further more, this paper presents experiments which show that the
proposed learning method tends to extremely reduce network output error to a
very small number with only 1 hidden node. Simulation results demonstrate that
the proposed method can provide several to thousands of times faster than other
learning algorithm including BP, SVM/SVR and other ELM methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1459</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1459</id><created>2014-05-06</created><updated>2014-06-22</updated><authors><author><keyname>Figueiredo</keyname><forenames>Flavio</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara M.</forenames></author><author><keyname>Matsubara</keyname><forenames>Yasuko</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Revisit Behavior in Social Media: The Phoenix-R Model and Discoveries</title><categories>cs.SI physics.soc-ph</categories><comments>To appear on European Conference on Machine Learning and Principles
  and Practice of Knowledge Discovery in Databases 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How many listens will an artist receive on a online radio? How about plays on
a YouTube video? How many of these visits are new or returning users? Modeling
and mining popularity dynamics of social activity has important implications
for researchers, content creators and providers. We here investigate the effect
of revisits (successive visits from a single user) on content popularity. Using
four datasets of social activity, with up to tens of millions media objects
(e.g., YouTube videos, Twitter hashtags or LastFM artists), we show the effect
of revisits in the popularity evolution of such objects. Secondly, we propose
the Phoenix-R model which captures the popularity dynamics of individual
objects. Phoenix-R has the desired properties of being: (1) parsimonious, being
based on the minimum description length principle, and achieving lower root
mean squared error than state-of-the-art baselines; (2) applicable, the model
is effective for predicting future popularity values of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1463</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1463</id><created>2014-05-06</created><updated>2014-12-29</updated><authors><author><keyname>Heunen</keyname><forenames>Chris</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Vicary</keyname><forenames>Jamie</forenames><affiliation>National University of Singapore, University of Oxford</affiliation></author><author><keyname>Wester</keyname><forenames>Linde</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Mixed quantum states in higher categories</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 304-315</journal-ref><doi>10.4204/EPTCS.172.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two ways to describe the interaction between classical and quantum
information categorically: one based on completely positive maps between
Frobenius algebras, the other using symmetric monoidal 2-categories. This paper
makes a first step towards combining the two. The integrated approach allows a
unified description of quantum teleportation and classical encryption in a
single 2-category, as well as a universal security proof applicable
simultaneously to both scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1464</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1464</id><created>2014-05-06</created><updated>2015-06-10</updated><authors><author><keyname>Cullina</keyname><forenames>Daniel</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Generalized sphere-packing and sphere-covering bounds on the size of
  codes for combinatorial channels</title><categories>cs.IT cs.DM math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the classic problems of coding theory are highly symmetric, which
makes it easy to derive sphere-packing upper bounds and sphere-covering lower
bounds on the size of codes. We discuss the generalizations of sphere-packing
and sphere-covering bounds to arbitrary error models. These generalizations
become especially important when the sizes of the error spheres are nonuniform.
The best possible sphere-packing and sphere-covering bounds are solutions to
linear programs. We derive a series of bounds from approximations to packing
and covering problems and study the relationships and trade-offs between them.
We compare sphere-covering lower bounds with other graph theoretic lower bounds
such as Tur\'{a}n's theorem. We show how to obtain upper bounds by optimizing
across a family of channels that admit the same codes. We present a
generalization of the local degree bound of Kulkarni and Kiyavash and use it to
improve the best known upper bounds on the sizes of single deletion correcting
codes and single grain error correcting codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1472</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1472</id><created>2014-05-06</created><authors><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>Varia</keyname><forenames>Mayank</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>An Exploration of the Role of Principal Inertia Components in
  Information Theory</title><categories>cs.IT math.IT</categories><comments>Submitted to the 2014 IEEE Information Theory Workshop (ITW)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principal inertia components of the joint distribution of two random
variables $X$ and $Y$ are inherently connected to how an observation of $Y$ is
statistically related to a hidden variable $X$. In this paper, we explore this
connection within an information theoretic framework. We show that, under
certain symmetry conditions, the principal inertia components play an important
role in estimating one-bit functions of $X$, namely $f(X)$, given an
observation of $Y$. In particular, the principal inertia components bear an
interpretation as filter coefficients in the linear transformation of
$p_{f(X)|X}$ into $p_{f(X)|Y}$. This interpretation naturally leads to the
conjecture that the mutual information between $f(X)$ and $Y$ is maximized when
all the principal inertia components have equal value. We also study the role
of the principal inertia components in the Markov chain $B\rightarrow
X\rightarrow Y\rightarrow \widehat{B}$, where $B$ and $\widehat{B}$ are binary
random variables. We illustrate our results for the setting where $X$ and $Y$
are binary strings and $Y$ is the result of sending $X$ through an additive
noise binary channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1477</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1477</id><created>2014-05-06</created><updated>2014-05-20</updated><authors><author><keyname>Tsourakakis</keyname><forenames>Charalampos E.</forenames></author></authors><title>A Novel Approach to Finding Near-Cliques: The Triangle-Densest Subgraph
  Problem</title><categories>cs.DS cs.DM cs.SI</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many graph mining applications rely on detecting subgraphs which are
near-cliques. There exists a dichotomy between the results in the existing work
related to this problem: on the one hand the densest subgraph problem (DSP)
which maximizes the average degree over all subgraphs is solvable in polynomial
time but for many networks fails to find subgraphs which are near-cliques. On
the other hand, formulations that are geared towards finding near-cliques are
NP-hard and frequently inapproximable due to connections with the Maximum
Clique problem.
  In this work, we propose a formulation which combines the best of both
worlds: it is solvable in polynomial time and finds near-cliques when the DSP
fails. Surprisingly, our formulation is a simple variation of the DSP.
Specifically, we define the triangle densest subgraph problem (TDSP): given
$G(V,E)$, find a subset of vertices $S^*$ such that $\tau(S^*)=\max_{S
\subseteq V} \frac{t(S)}{|S|}$, where $t(S)$ is the number of triangles induced
by the set $S$. We provide various exact and approximation algorithms which the
solve the TDSP efficiently. Furthermore, we show how our algorithms adapt to
the more general problem of maximizing the $k$-clique average density. Finally,
we provide empirical evidence that the TDSP should be used whenever the output
of the DSP fails to output a near-clique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1481</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1481</id><created>2014-05-06</created><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Graphical potential games</title><categories>math.PR cs.GT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the class of potential games that are also graphical games with
respect to a given graph $G$ of connections between the players. We show that,
up to strategic equivalence, this class of games can be identified with the set
of Markov random fields on $G$.
  From this characterization, and from the Hammersley-Clifford theorem, it
follows that the potentials of such games can be decomposed to local
potentials. We use this decomposition to strongly bound the lengths of strict
better-response paths. This result extends to generalized graphical potential
games, which are played on infinite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1486</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1486</id><created>2014-05-06</created><authors><author><keyname>Koutra</keyname><forenames>Danai</forenames></author><author><keyname>Bennett</keyname><forenames>Paul</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author></authors><title>Events and Controversies: Influences of a Shocking News Event on
  Information Seeking</title><categories>cs.IR cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been suggested that online search and retrieval contributes to the
intellectual isolation of users within their preexisting ideologies, where
people's prior views are strengthened and alternative viewpoints are
infrequently encountered. This so-called &quot;filter bubble&quot; phenomenon has been
called out as especially detrimental when it comes to dialog among people on
controversial, emotionally charged topics, such as the labeling of genetically
modified food, the right to bear arms, the death penalty, and online privacy.
We seek to identify and study information-seeking behavior and access to
alternative versus reinforcing viewpoints following shocking, emotional, and
large-scale news events. We choose for a case study to analyze search and
browsing on gun control/rights, a strongly polarizing topic for both citizens
and leaders of the United States. We study the period of time preceding and
following a mass shooting to understand how its occurrence, follow-on
discussions, and debate may have been linked to changes in the patterns of
searching and browsing. We employ information-theoretic measures to quantify
the diversity of Web domains of interest to users and understand the browsing
patterns of users. We use these measures to characterize the influence of news
events on these web search and browsing patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1499</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1499</id><created>2014-05-06</created><updated>2015-09-30</updated><authors><author><keyname>Quamar</keyname><forenames>Abdul</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author></authors><title>NScale: Neighborhood-centric Large-Scale Graph Analytics in the Cloud</title><categories>cs.DB cs.SI</categories><comments>26 pages, 15 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing interest in executing complex analyses over large
graphs, many of which require processing a large number of multi-hop
neighborhoods or subgraphs. Examples include ego network analysis, motif
counting, personalized recommendations, and others. These tasks are not well
served by existing vertex-centric graph processing frameworks, where user
programs are only able to directly access the state of a single vertex. This
paper introduces NSCALE, a novel end-to-end graph processing framework that
enables the distributed execution of complex subgraph-centric analytics over
large-scale graphs in the cloud. NSCALE enables users to write programs at the
level of subgraphs rather than at the level of vertices. Unlike most previous
graph processing frameworks, which apply the user program to the entire graph,
NSCALE allows users to declaratively specify subgraphs of interest. Our
framework includes a novel graph extraction and packing (GEP) module that
utilizes a cost-based optimizer to partition and pack the subgraphs of interest
into memory on as few machines as possible. The distributed execution engine
then takes over and runs the user program in parallel, while respecting the
scope of the various subgraphs. Our experimental results show
orders-of-magnitude improvements in performance and drastic reductions in the
cost of analytics compared to vertex-centric approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1502</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1502</id><created>2014-05-07</created><authors><author><keyname>Ollila</keyname><forenames>Esa</forenames></author><author><keyname>Kim</keyname><forenames>Hyon-Jung</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Robust iterative hard thresholding for compressed sensing</title><categories>cs.IT math.IT stat.AP</categories><comments>To appear in Proc. of ISCCSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) or sparse signal reconstruction (SSR) is a signal
processing technique that exploits the fact that acquired data can have a
sparse representation in some basis. One popular technique to reconstruct or
approximate the unknown sparse signal is the iterative hard thresholding (IHT)
which however performs very poorly under non-Gaussian noise conditions or in
the face of outliers (gross errors). In this paper, we propose a robust IHT
method based on ideas from $M$-estimation that estimates the sparse signal and
the scale of the error distribution simultaneously. The method has a negligible
performance loss compared to IHT under Gaussian noise, but superior performance
under heavy-tailed non-Gaussian noise conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1503</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1503</id><created>2014-05-07</created><updated>2015-02-20</updated><authors><author><keyname>Cortes</keyname><forenames>Corinna</forenames></author><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Medina</keyname><forenames>Andres Mu&#xf1;oz</forenames></author></authors><title>Adaptation Algorithm and Theory Based on Generalized Discrepancy</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for domain adaptation improving upon a discrepancy
minimization algorithm previously shown to outperform a number of algorithms
for this task. Unlike many previous algorithms for domain adaptation, our
algorithm does not consist of a fixed reweighting of the losses over the
training sample. We show that our algorithm benefits from a solid theoretical
foundation and more favorable learning bounds than discrepancy minimization. We
present a detailed description of our algorithm and give several efficient
solutions for solving its optimization problem. We also report the results of
several experiments showing that it outperforms discrepancy minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1505</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1505</id><created>2014-05-07</created><authors><author><keyname>Kaushik</keyname><forenames>Dr. Manju</forenames></author></authors><title>System Software: Concepts and Approach</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In software industry a large number of projects continue to fail due to non
technical issue such as communication gap,requirements and poor executive. The
authors identify the reasons for which are available for software development
life cycles fall short of dealing with them. They also proposed the system
development for software development life cycle. In this paper, the concept of
system development, SDLC is further explored and a number of concepts are
discussed in this regard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1510</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1510</id><created>2014-05-07</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Maaser</keyname><forenames>Nicola</forenames></author><author><keyname>Napel</keyname><forenames>Stefan</forenames></author><author><keyname>Weber</keyname><forenames>Matthias</forenames></author></authors><title>Mostly Sunny: A Forecast of Tomorrow's Power Index Research</title><categories>cs.GT</categories><comments>12 pages</comments><msc-class>91B12, 91A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power index research has been a very active field in the last decades. Will
this continue or are all the important questions solved? We argue that there
are still many opportunities to conduct useful research with and on power
indices. Positive and normative questions keep calling for theoretical and
empirical attention. Technical and technological improvements are likely to
boost applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1511</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1511</id><created>2014-05-07</created><authors><author><keyname>Gupta</keyname><forenames>Neha</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Exploration of gaps in Bitly's spam detection and relevant counter
  measures</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existence of spam URLs over emails and Online Social Media (OSM) has become a
growing phenomenon. To counter the dissemination issues associated with long
complex URLs in emails and character limit imposed on various OSM (like
Twitter), the concept of URL shortening gained a lot of traction. URL
shorteners take as input a long URL and give a short URL with the same landing
page in return. With its immense popularity over time, it has become a prime
target for the attackers giving them an advantage to conceal malicious content.
Bitly, a leading service in this domain is being exploited heavily to carry out
phishing attacks, work from home scams, pornographic content propagation, etc.
This imposes additional performance pressure on Bitly and other URL shorteners
to be able to detect and take a timely action against the illegitimate content.
In this study, we analyzed a dataset marked as suspicious by Bitly in the month
of October 2013 to highlight some ground issues in their spam detection
mechanism. In addition, we identified some short URL based features and coupled
them with two domain specific features to classify a Bitly URL as malicious /
benign and achieved a maximum accuracy of 86.41%. To the best of our knowledge,
this is the first large scale study to highlight the issues with Bitly's spam
detection policies and proposing a suitable countermeasure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1512</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1512</id><created>2014-05-07</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Stream processing components: Isabelle/HOL formalisation and case
  studies</title><categories>cs.SE</categories><comments>Preprint with an extended introduction</comments><journal-ref>Archive of Formal Proofs, 2013, ISSN: 2150-914x</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This set of theories presents an Isabelle/HOL+Isar formalisation of stream
processing components introduces in Focus, a framework for formal specification
and development of interactive systems. This is an extended and updated version
of the formalisation, which was elaborated within the methodology 'Focus on
Isabelle'. In addition, we also applied the formalisation on three case studies
that cover different application areas: process control (Steam Boiler System),
data transmission (FlexRay communication protocol), memory and processing
components (Automotive-Gateway System).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1513</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1513</id><created>2014-05-07</created><authors><author><keyname>Alabdulmohsin</keyname><forenames>Ibrahim</forenames></author></authors><title>A Mathematical Theory of Learning</title><categories>cs.LG cs.AI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a mathematical theory of learning is proposed that has many
parallels with information theory. We consider Vapnik's General Setting of
Learning in which the learning process is defined to be the act of selecting a
hypothesis in response to a given training set. Such hypothesis can, for
example, be a decision boundary in classification, a set of centroids in
clustering, or a set of frequent item-sets in association rule mining.
Depending on the hypothesis space and how the final hypothesis is selected, we
show that a learning process can be assigned a numeric score, called learning
capacity, which is analogous to Shannon's channel capacity and satisfies
similar interesting properties as well such as the data-processing inequality
and the information-cannot-hurt inequality. In addition, learning capacity
provides the tightest possible bound on the difference between true risk and
empirical risk of the learning process for all loss functions that are
parametrized by the chosen hypothesis. It is also shown that the notion of
learning capacity equivalently quantifies how sensitive the choice of the final
hypothesis is to a small perturbation in the training set. Consequently,
algorithmic stability is both necessary and sufficient for generalization.
While the theory does not rely on concentration inequalities, we finally show
that analogs to classical results in learning theory using the Probably
Approximately Correct (PAC) model can be immediately deduced using this theory,
and conclude with information-theoretic bounds to learning capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1514</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1514</id><created>2014-05-07</created><authors><author><keyname>Bhuvaneswari</keyname><forenames>A.</forenames></author><author><keyname>Raj</keyname><forenames>E. George Dharma Prakash</forenames></author><author><keyname>Prakash</keyname><forenames>V. Sinthu Janita</forenames></author></authors><title>ACO-ESSVHOA - Ant Colony Optimization based Multi-Criteria Decision
  Making for Efficient Signal Selection in Mobile Vertical Handoff</title><categories>cs.NI</categories><comments>7 Pages, 8 figures</comments><journal-ref>IJASCSE, Vol.3,Issue 4, April 30 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of Vertical handoff has become one of the major components of
today's wireless environment due to the availability of the vast variety of
signals. The decision for a handoff should be performed catering to the needs
of the current transmission that is being carried out. Our paper describes a
modified Ant Colony Optimization based handoff mechanism, that considers
multiple criteria in its decision making process rather than a single parameter
(pheromone intensity). In general, ACO considers the pheromone intensity and
the evaporation rates as the parameters for selecting a route. In this paper,
we describe a mechanism that determines the evaporation rates of each path
connected to the source using various criteria, which in turn reflects on the
pheromone levels present in the path and hence the probability of selecting
that route. Experiments show that our process exhibits better convergence
rates, hence better usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1515</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1515</id><created>2014-05-07</created><authors><author><keyname>Mkaouar</keyname><forenames>Han&#xe9;ne</forenames></author><author><keyname>Boubaker</keyname><forenames>Olfa</forenames></author></authors><title>Compliant motion control for handling a single object by two similar
  industrial robots</title><categories>cs.RO</categories><comments>7 pages, 6 Figures</comments><journal-ref>Procedia Engineering, Vol. 41, pp. 1285-1291, 2012</journal-ref><doi>10.1016/j.proeng.2012.07.312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a compliant motion control strategy for handling a
single object by two similar industrial robots. The dynamics of the object
carried by the two robots is assimilated to the dynamics of a
mass-spring-damper system described by a piecewise linear model (PWA). The
coordination of the two robots is accomplished using a master slave
synchronization approach dedicated for PWA systems, based on the Lyapunov
theory, and solved via Linear Matrix Inequalities (LMIs). The performances of
the proposed approach are proved by simulation results and compared to a
related approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1520</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1520</id><created>2014-05-07</created><authors><author><keyname>Hoos</keyname><forenames>Holger</forenames></author><author><keyname>Lindauer</keyname><forenames>Marius</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>claspfolio 2: Advances in Algorithm Selection for Answer Set Programming</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP). Building on the
award-winning, portfolio-based ASP solver claspfolio, we present claspfolio 2,
a modular and open solver architecture that integrates several different
portfolio-based algorithm selection approaches and techniques. The claspfolio 2
solver framework supports various feature generators, solver selection
approaches, solver portfolios, as well as solver-schedule-based pre-solving
techniques. The default configuration of claspfolio 2 relies on a light-weight
version of the ASP solver clasp to generate static and dynamic instance
features. The flexible open design of claspfolio 2 is a distinguishing factor
even beyond ASP. As such, it provides a unique framework for comparing and
combining existing portfolio-based algorithm selection approaches and
techniques in a single, unified framework. Taking advantage of this, we
conducted an extensive experimental study to assess the impact of different
feature sets, selection approaches and base solver portfolios. In addition to
gaining substantial insights into the utility of the various approaches and
techniques, we identified a default configuration of claspfolio 2 that achieves
substantial performance gains not only over clasp's default configuration and
the earlier version of claspfolio 2, but also over manually tuned
configurations of clasp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1523</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1523</id><created>2014-05-07</created><authors><author><keyname>Bogaerts</keyname><forenames>Bart</forenames></author><author><keyname>Jansen</keyname><forenames>Joachim</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author><author><keyname>De Cat</keyname><forenames>Broes</forenames></author><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Simulating dynamic systems using Linear Time Calculus theories</title><categories>cs.LO</categories><doi>10.1017/S1471068414000155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP).
  Dynamic systems play a central role in fields such as planning, verification,
and databases. Fragmented throughout these fields, we find a multitude of
languages to formally specify dynamic systems and a multitude of systems to
reason on such specifications. Often, such systems are bound to one specific
language and one specific inference task. It is troublesome that performing
several inference tasks on the same knowledge requires translations of your
specification to other languages. In this paper we study whether it is possible
to perform a broad set of well-studied inference tasks on one specification.
More concretely, we extend IDP3 with several inferences from fields concerned
with dynamic specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1524</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1524</id><created>2014-05-07</created><authors><author><keyname>Mohammadi</keyname><forenames>Mohammad</forenames></author><author><keyname>Jafari</keyname><forenames>Shahram</forenames></author></authors><title>An expert system for recommending suitable ornamental fish addition to
  an aquarium based on aquarium condition</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert systems prove to be suitable replacement for human experts when human
experts are unavailable for different reasons. Various expert system has been
developed for wide range of application. Although some expert systems in the
field of fishery and aquaculture has been developed but a system that aids user
in process of selecting a new addition to their aquarium tank never been
designed. This paper proposed an expert system that suggests new addition to an
aquarium tank based on current environmental condition of aquarium and
currently existing fishes in aquarium. The system suggest the best fit for
aquarium condition and most compatible to other fishes in aquarium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1533</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1533</id><created>2014-05-07</created><updated>2014-05-08</updated><authors><author><keyname>Gaillard</keyname><forenames>Pierre</forenames><affiliation>GREGH</affiliation></author><author><keyname>Baudin</keyname><forenames>Paul</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>A consistent deterministic regression tree for non-parametric prediction
  of time series</title><categories>math.ST cs.LG stat.ML stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online prediction of bounded stationary ergodic processes. To do so,
we consider the setting of prediction of individual sequences and build a
deterministic regression tree that performs asymptotically as well as the best
L-Lipschitz constant predictors. Then, we show why the obtained regret bound
entails the asymptotical optimality with respect to the class of bounded
stationary ergodic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1535</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1535</id><created>2014-05-07</created><authors><author><keyname>Abasi</keyname><forenames>Hasan</forenames></author><author><keyname>Abdi</keyname><forenames>Ali Z.</forenames></author><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author></authors><title>Learning Boolean Halfspaces with Small Weights from Membership Queries</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of proper learning a Boolean Halfspace with integer
weights $\{0,1,\ldots,t\}$ from membership queries only. The best known
algorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$
membership queries where the best lower bound for the number of membership
queries is $n^t$ [Learning Threshold Functions with Small Weights Using
Membership Queries. COLT 1999]
  In this paper we close this gap and give an adaptive proper learning
algorithm with two rounds that asks $n^{O(t)}$ membership queries. We also give
a non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membership
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1544</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1544</id><created>2014-05-07</created><updated>2015-10-29</updated><authors><author><keyname>Otpuschennikov</keyname><forenames>Ilya</forenames></author><author><keyname>Semenov</keyname><forenames>Alexander</forenames></author><author><keyname>Kochemazov</keyname><forenames>Stepan</forenames></author></authors><title>Transalg: a Tool for Translating Procedural Descriptions of Discrete
  Functions to SAT</title><categories>cs.AI</categories><journal-ref>Proceedings of The 5th International Workshop on Computer Science
  and Engineering: Information Processing and Control Engineering (WCSE
  2015-IPCE) (2015) 289-294</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the Transalg system, designed to produce SAT
encodings for discrete functions, written as programs in a specific language.
Translation of such programs to SAT is based on propositional encoding methods
for formal computing models and on the concept of symbolic execution. We used
the Transalg system to make SAT encodings for a number of cryptographic
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1546</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1546</id><created>2014-05-07</created><updated>2014-08-20</updated><authors><author><keyname>Given-Wilson</keyname><forenames>Thomas</forenames><affiliation>NICTA</affiliation></author><author><keyname>Gorla</keyname><forenames>Daniele</forenames><affiliation>Dip. Informatica - Univ. di Roma</affiliation></author><author><keyname>Jay</keyname><forenames>Barry</forenames><affiliation>Centre for Quantum Computation and Intelligent Systems and School of Software</affiliation></author></authors><title>A Concurrent Pattern Calculus</title><categories>cs.LO cs.FL</categories><comments>Logical Methods in Computer Science (2014)</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  23, 2014) lmcs:774</journal-ref><doi>10.2168/LMCS-10(3:10)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent pattern calculus (CPC) drives interaction between processes by
comparing data structures, just as sequential pattern calculus drives
computation. By generalising from pattern matching to pattern unification,
interaction becomes symmetrical, with information flowing in both directions.
CPC provides a natural language to express trade where information exchange is
pivotal to interaction. The unification allows some patterns to be more
discriminating than others; hence, the behavioural theory must take this aspect
into account, so that bisimulation becomes subject to compatibility of
patterns. Many popular process calculi can be encoded in CPC; this allows for a
gain in expressiveness, formalised through encodings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1558</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1558</id><created>2014-05-07</created><authors><author><keyname>Gormus</keyname><forenames>Sedat</forenames></author><author><keyname>Bocus</keyname><forenames>Mohammud Zubeir</forenames></author></authors><title>Efficient Cooperative Anycasting for AMI Mesh Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have, in recent years, witnessed an increased interest towards enabling a
Smart Grid which will be a corner stone to build sustainable energy efficient
communities. An integral part of the future Smart Grid will be the
communications infrastructure which will make real time control of the grid
components possible. Automated Metering Infrastructure (AMI) is thought to be a
key enabler for monitoring and controlling the customer loads. %RPL is a
connectivity enabling mechanism for low power and lossy networks currently
being standardized by the IETF ROLL working group. RPL is deemed to be a
suitable candidate for AMI networks where the meters are connected to a
concentrator over multi hop low power and lossy links. This paper proposes an
efficient cooperative anycasting approach for wireless mesh networks with the
aim of achieving reduced traffic and increased utilisation of the network
resources. The proposed cooperative anycasting has been realised as an
enhancement on top of the Routing Protocol for Low Power and Lossy Networks
(RPL), a connectivity enabling mechanism in wireless AMI mesh networks. In this
protocol, smart meter nodes utilise an anycasting approach to facilitate
efficient transport of metering data to the concentrator node. Moreover, it
takes advantage of a distributed approach ensuring scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1565</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1565</id><created>2014-05-07</created><authors><author><keyname>Beyersdorff</keyname><forenames>Olaf</forenames></author><author><keyname>Chew</keyname><forenames>Leroy</forenames></author></authors><title>Tableau vs. Sequent Calculi for Minimal Entailment</title><categories>cs.LO</categories><comments>9 pages, 3 figures, this paper appears in the Proceedings of the 15th
  International Workshop on Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we compare two proof systems for minimal entailment: a tableau
system OTAB and a sequent calculus MLK, both developed by Olivetti (1992). Our
main result shows that OTAB-proofs can be efficiently translated into
MLK-proofs, i.e; MLK p-simulates OTAB. The simulation is technically very
involved and answers an open question posed by Olivetti (1992) on the relation
between the two calculi. We also show that the two systems are exponentially
separated, i.e; there are formulas which have polynomial size MLK-proofs, but
require exponential-size OTAB- proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1573</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1573</id><created>2014-05-07</created><updated>2014-07-07</updated><authors><author><keyname>Wang</keyname><forenames>Baokui</forenames></author><author><keyname>Pei</keyname><forenames>Zhenhua</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Evolutionary dynamics of cooperation on interdependent networks with
  Prisoner's Dilemma and Snowdrift Game</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>6 pages, 6 figures</comments><doi>10.1209/0295-5075/107/58006</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The world in which we are living is a huge network of networks and should be
described by interdependent networks. The interdependence between networks
significantly affects the evolutionary dynamics of cooperation on them.
Meanwhile, due to the diversity and complexity of social and biological
systems, players on different networks may not interact with each other by the
same way, which should be described by multiple models in evolutionary game
theory, such as the Prisoner's Dilemma and Snowdrift Game. We therefore study
the evolutionary dynamics of cooperation on two interdependent networks playing
different games respectively. We clearly evidence that, with the increment of
network interdependence, the evolution of cooperation is dramatically promoted
on the network playing Prisoner's Dilemma. The cooperation level of the network
playing Snowdrift Game reduces correspondingly, although it is almost
invisible. In particular, there exists an optimal intermediate region of
network interdependence maximizing the growth rate of the evolution of
cooperation on the network playing Prisoner's Dilemma. Remarkably, players
contacting with other network have advantage in the evolution of cooperation
than the others on the same network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1574</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1574</id><created>2014-05-07</created><updated>2014-05-14</updated><authors><author><keyname>Guo</keyname><forenames>Jin-Li</forenames></author><author><keyname>Suo</keyname><forenames>Qi</forenames></author></authors><title>Comment on &quot;Quantifying Long-term Scientific Impact&quot;</title><categories>cs.DL physics.soc-ph</categories><comments>2 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper comments on &quot;Quantifying long-term scientific impact&quot;. It indicates
that there is a mistake of [D. S. Wang , C. Song, A. L. Barabasi, Quantifying
long-term scientific impact, Science 342, 127 (2013), arXiv:1306.3293].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1584</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1584</id><created>2014-05-07</created><authors><author><keyname>Cramer</keyname><forenames>Marcos</forenames></author><author><keyname>Van Hertum</keyname><forenames>Pieter</forenames></author><author><keyname>Ambrossio</keyname><forenames>Diego Agustin</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Modelling Delegation and Revocation Schemes in IDP</title><categories>cs.LO cs.CR</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In ownership-based access control frameworks with the possibility of
delegating permissions and administrative rights, chains of delegated accesses
will form. There are different ways to treat these delegation chains when
revoking rights, which give rise to different revocation schemes. In this
paper, we show how IDP - a knowledge base system that integrates technology
from ASP, SAT and CP - can be used to efficiently implement executable
revocation schemes for an ownership-based access control system based on a
declarative specification of their properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1590</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1590</id><created>2014-05-07</created><authors><author><keyname>Gomaa</keyname><forenames>Walid</forenames></author></authors><title>Computability and Complexity over the Product Topology of Real Numbers</title><categories>cs.LO cs.CC math.LO</categories><comments>12 pages</comments><acm-class>F.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kawamura and Cook have developed a framework for studying the computability
and complexity theoretic problems over &quot;large&quot; topological spaces. This
framework has been applied to study the complexity of the differential operator
and the complexity of functionals over the space of continuous functions on the
unit interval $C[0,1]$. In this paper we apply the ideas of Kawamura and Cook
to the product space of the real numbers endowed with the product topology. We
show that no computable norm can be defined over such topology. We investigate
computability and complexity of total functions over the product space in two
cases: (1) when the computing machine submits a uniformally bounded number of
queries to the oracle and (2) when the number of queries submitted by the
machine is not uniformally bounded. In the first case we show that the function
over the product space can be reduced to a function over a finite-dimensional
space. However, in general there exists functions whose computing machines must
submit a non-uniform number of queries to the oracle indicating that computing
over the product topology can not in general be reduced to computing over
finite-dimensional spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1593</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1593</id><created>2014-05-07</created><updated>2015-11-18</updated><authors><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Kourtellaris</keyname><forenames>Christos K.</forenames></author><author><keyname>Charalambous</keyname><forenames>C. D.</forenames></author></authors><title>Information Nonanticipative Rate Distortion Function and Its
  Applications</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>34 pages, 12 figures, part of this paper was accepted for publication
  in IEEE International Symposium on Information Theory (ISIT), 2014 and in
  book Coordination Control of Distributed Systems of series Lecture Notes in
  Control and Information Sciences, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates applications of nonanticipative Rate Distortion
Function (RDF) in a) zero-delay Joint Source-Channel Coding (JSCC) design based
on average and excess distortion probability, b) in bounding the Optimal
Performance Theoretically Attainable (OPTA) by noncausal and causal codes, and
computing the Rate Loss (RL) of zero-delay and causal codes with respect to
noncausal codes. These applications are described using two running examples,
the Binary Symmetric Markov Source with parameter p, (BSMS(p)) and the
multidimensional partially observed Gaussian-Markov source. For the
multidimensional Gaussian-Markov source with square error distortion, the
solution of the nonanticipative RDF is derived, its operational meaning using
JSCC design via a noisy coding theorem is shown by providing the optimal
encoding-decoding scheme over a vector Gaussian channel, and the RL of causal
and zero-delay codes with respect to noncausal codes is computed.
  For the BSMS(p) with Hamming distortion, the solution of the nonanticipative
RDF is derived, the RL of causal codes with respect to noncausal codes is
computed, and an uncoded noisy coding theorem based on excess distortion
probability is shown. The information nonanticipative RDF is shown to be
equivalent to the nonanticipatory epsilon-entropy, which corresponds to the
classical RDF with an additional causality or nonanticipative condition imposed
on the optimal reproduction conditional distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1605</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1605</id><created>2014-05-07</created><authors><author><keyname>Staiano</keyname><forenames>Jacopo</forenames></author><author><keyname>Guerini</keyname><forenames>Marco</forenames></author></authors><title>DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News</title><categories>cs.CL cs.CY</categories><comments>To appear at ACL 2014. 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While many lexica annotated with words polarity are available for sentiment
analysis, very few tackle the harder task of emotion analysis and are usually
quite limited in coverage. In this paper, we present a novel approach for
extracting - in a totally automated way - a high-coverage and high-precision
lexicon of roughly 37 thousand terms annotated with emotion scores, called
DepecheMood. Our approach exploits in an original way 'crowd-sourced' affective
annotation implicitly provided by readers of news articles from rappler.com. By
providing new state-of-the-art performances in unsupervised settings for
regression and classification tasks, even using a na\&quot;{\i}ve approach, our
experiments show the beneficial impact of harvesting social media data for
affective lexicon building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1615</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1615</id><created>2014-05-07</created><authors><author><keyname>De Pril</keyname><forenames>Julie</forenames></author><author><keyname>Flesch</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Kuipers</keyname><forenames>Jeroen</forenames></author><author><keyname>Schoenmakers</keyname><forenames>Gijs</forenames></author><author><keyname>Vrieze</keyname><forenames>Koos</forenames></author></authors><title>Existence of Secure Equilibrium in Multi-Player Games with Perfect
  Information</title><categories>cs.GT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure equilibrium is a refinement of Nash equilibrium, which provides some
security to the players against deviations when a player changes his strategy
to another best response strategy. The concept of secure equilibrium is
specifically developed for assume-guarantee synthesis and has already been
applied in this context. Yet, not much is known about its existence in games
with more than two players. In this paper, we establish the existence of secure
equilibrium in two classes of multi-player perfect information turn-based
games: (1) in games with possibly probabilistic transitions, having countable
state and finite action spaces and bounded and continuous payoff functions, and
(2) in games with only deterministic transitions, having arbitrary state and
action spaces and Borel payoff functions with a finite range (in particular,
qualitative Borel payoff functions). We show that these results apply to
several types of games studied in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1616</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1616</id><created>2014-05-07</created><updated>2014-05-22</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Smyth</keyname><forenames>Conor</forenames></author><author><keyname>Mylona</keyname><forenames>Kalliopi</forenames></author><author><keyname>Niblo</keyname><forenames>Graham A.</forenames></author></authors><title>Benevolent characteristics promote cooperative behaviour among humans</title><categories>cs.GT q-bio.PE</categories><doi>10.1371/journal.pone.0102881</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation is fundamental to the evolution of human society. We regularly
observe cooperative behaviour in everyday life and in controlled experiments
with anonymous people, even though standard economic models predict that they
should deviate from the collective interest and act so as to maximise their own
individual payoff. However, there is typically heterogeneity across subjects:
some may cooperate, while others may not. Since individual factors promoting
cooperation could be used by institutions to indirectly prime cooperation, this
heterogeneity raises the important question of who these cooperators are. We
have conducted a series of experiments to study whether benevolence, defined as
a unilateral act of paying a cost to increase the welfare of someone else
beyond one's own, is related to cooperation in a subsequent one-shot anonymous
Prisoner's dilemma. Contrary to the predictions of the widely used inequity
aversion models, we find that benevolence does exist and a large majority of
people behave this way. We also find benevolence to be correlated with
cooperative behaviour. Finally, we show a causal link between benevolence and
cooperation: priming people to think positively about benevolent behaviour
makes them significantly more cooperative than priming them to think
malevolently. Thus benevolent people exist and cooperate more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1618</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1618</id><created>2014-05-02</created><authors><author><keyname>Sadikov</keyname><forenames>Victor</forenames></author><author><keyname>Pidkameny</keyname><forenames>Walter</forenames></author></authors><title>Complete Separation of the 3 Tiers - Divide and Conquer</title><categories>cs.SE cs.PL</categories><comments>9 pages with code examples</comments><acm-class>D.2.11; D.2.12; D.1.5; D.2.13; D.3.3; H.5.4; I.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most Java applications, including web based ones, follow the 3-tier
architecture. Although Java provides standard tools for tier-to-tier
interfaces, the separation of the tiers is usually not perfect. E.g. the
database interface, JDBC, assumes that SQL statements are issued from the
application server. Similarly, in web based Java applications, HTML code is
assumed to be produced by servlets. In terms of syntax, this turns Java source
code into mixtures of languages: Java and SQL, Java and HTML. These language
mixtures are difficult to read, modify, and maintain.
  In this paper we examine criteria and methods to achieve a good separation of
the 3 tiers and propose a technique to provide a clean separation. Our proposed
technique requires an explicit Interface and Data Definitions. These allow
isolation of the back-end, application server, and front-end development. The
Definitions also enable application design in terms of aggregated data
structures. As a result significant amounts of auxiliary code can be generated
from the Definitions, enabling the developers to concentrate on the business
logic. By and large the proposed approach greatly facilitates development and
maintenance, and overall improves the quality of the products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1623</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1623</id><created>2014-05-07</created><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Hanrot</keyname><forenames>Guillaume</forenames></author></authors><title>Markov Chain Monte Carlo Algorithms for Lattice Gaussian Sampling</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, IEEE International Symposium on Information
  Theory(ISIT) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling from a lattice Gaussian distribution is emerging as an important
problem in various areas such as coding and cryptography. The default sampling
algorithm --- Klein's algorithm yields a distribution close to the lattice
Gaussian only if the standard deviation is sufficiently large. In this paper,
we propose the Markov chain Monte Carlo (MCMC) method for lattice Gaussian
sampling when this condition is not satisfied. In particular, we present a
sampling algorithm based on Gibbs sampling, which converges to the target
lattice Gaussian distribution for any value of the standard deviation. To
improve the convergence rate, a more efficient algorithm referred to as
Gibbs-Klein sampling is proposed, which samples block by block using Klein's
algorithm. We show that Gibbs-Klein sampling yields a distribution close to the
target lattice Gaussian, under a less stringent condition than that of the
original Klein algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1630</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1630</id><created>2014-05-07</created><updated>2014-05-18</updated><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Dimitrova</keyname><forenames>Rayna</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Abstractions and sensor design in partial-information, reactive
  controller synthesis</title><categories>cs.SY</categories><comments>9 pages, 4 figures, Accepted at American Control Conference 2014</comments><msc-class>93B99</msc-class><acm-class>I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated synthesis of reactive control protocols from temporal logic
specifications has recently attracted considerable attention in various
applications in, for example, robotic motion planning, network management, and
hardware design. An implicit and often unrealistic assumption in this past work
is the availability of complete and precise sensing information during the
execution of the controllers. In this paper, we use an abstraction procedure
for systems with partial observation and propose a formalism to investigate
effects of limitations in sensing. The abstraction procedure enables the
existing synthesis methods with partial observation to be applicable and
efficient for systems with infinite (or finite but large number of) states.
This formalism enables us to systematically discover sensing modalities
necessary in order to render the underlying synthesis problems feasible. We use
counterexamples, which witness unrealizability potentially due to the
limitations in sensing and the coarseness in the abstract system, and
interpolation-based techniques to refine the model and the sensing modalities,
i.e., to identify new sensors to be included, in such synthesis problems. We
demonstrate the method on examples from robotic motion planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1649</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1649</id><created>2014-05-07</created><updated>2014-09-30</updated><authors><author><keyname>Kutten</keyname><forenames>Shay</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author></authors><title>Distributed Symmetry Breaking in Hypergraphs</title><categories>cs.DC cs.DS</categories><comments>Changes from the previous version: More references added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental local symmetry breaking problems such as Maximal Independent Set
(MIS) and coloring have been recognized as important by the community, and
studied extensively in (standard) graphs. In particular, fast (i.e.,
logarithmic run time) randomized algorithms are well-established for MIS and
$\Delta +1$-coloring in both the LOCAL and CONGEST distributed computing
models. On the other hand, comparatively much less is known on the complexity
of distributed symmetry breaking in {\em hypergraphs}. In particular, a key
question is whether a fast (randomized) algorithm for MIS exists for
hypergraphs.
  In this paper, we study the distributed complexity of symmetry breaking in
hypergraphs by presenting distributed randomized algorithms for a variety of
fundamental problems under a natural distributed computing model for
hypergraphs. We first show that MIS in hypergraphs (of arbitrary dimension) can
be solved in $O(\log^2 n)$ rounds ($n$ is the number of nodes of the
hypergraph) in the LOCAL model. We then present a key result of this paper ---
an $O(\Delta^{\epsilon}\text{polylog}(n))$-round hypergraph MIS algorithm in
the CONGEST model where $\Delta$ is the maximum node degree of the hypergraph
and $\epsilon &gt; 0$ is any arbitrarily small constant.
  To demonstrate the usefulness of hypergraph MIS, we present applications of
our hypergraph algorithm to solving problems in (standard) graphs. In
particular, the hypergraph MIS yields fast distributed algorithms for the {\em
balanced minimal dominating set} problem (left open in Harris et al. [ICALP
2013]) and the {\em minimal connected dominating set problem}. We also present
distributed algorithms for coloring, maximal matching, and maximal clique in
hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1655</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1655</id><created>2014-05-07</created><updated>2015-07-09</updated><authors><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author><author><keyname>Demirci</keyname><forenames>H. G&#xf6;kalp</forenames></author></authors><title>Debates with small transparent quantum verifiers</title><categories>cs.CC cs.FL quant-ph</categories><comments>18 pages. A revised and extended version. A preliminary version
  appeared in the proceedings of DLT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model where two opposing provers debate over the membership status
of a given string in a language, trying to convince a weak verifier whose coins
are visible to all. We show that the incorporation of just two qubits to an
otherwise classical constant-space verifier raises the class of debatable
languages from at most $\mathsf{NP}$ to the collection of all Turing-decidable
languages (recursive languages). When the verifier is further constrained to
make the correct decision with probability 1, the corresponding class goes up
from the regular languages up to at least $\mathsf{E}$. We also show that the
quantum model outperforms its classical counterpart when restricted to run in
polynomial time, and demonstrate some non-context-free languages which have
such short debates with quantum verifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1665</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1665</id><created>2014-05-07</created><updated>2014-11-07</updated><authors><author><keyname>Garg</keyname><forenames>Ankit</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>On Communication Cost of Distributed Statistical Estimation and
  Dimensionality</title><categories>cs.LG cs.IT math.IT</categories><comments>to appear at NIPS'14 with oral presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the connection between dimensionality and communication cost in
distributed learning problems. Specifically we study the problem of estimating
the mean $\vec{\theta}$ of an unknown $d$ dimensional gaussian distribution in
the distributed setting. In this problem, the samples from the unknown
distribution are distributed among $m$ different machines. The goal is to
estimate the mean $\vec{\theta}$ at the optimal minimax rate while
communicating as few bits as possible. We show that in this setting, the
communication cost scales linearly in the number of dimensions i.e. one needs
to deal with different dimensions individually. Applying this result to
previous lower bounds for one dimension in the interactive setting
\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we prove
new lower bounds of $\Omega(md/\log(m))$ and $\Omega(md)$ for the bits of
communication needed to achieve the minimax squared loss, in the interactive
and simultaneous settings respectively. To complement, we also demonstrate an
interactive protocol achieving the minimax squared loss with $O(md)$ bits of
communication, which improves upon the simple simultaneous protocol by a
logarithmic factor. Given the strong lower bounds in the general setting, we
initiate the study of the distributed parameter estimation problems with
structured parameters. Specifically, when the parameter is promised to be
$s$-sparse, we show a simple thresholding based protocol that achieves the same
squared loss while saving a $d/s$ factor of communication. We conjecture that
the tradeoff between communication and squared loss demonstrated by this
protocol is essentially optimal up to logarithmic factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1671</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1671</id><created>2014-05-07</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Kantor</keyname><forenames>Erez</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>Newport</keyname><forenames>Calvin</forenames></author></authors><title>Multi-Message Broadcast with Abstract MAC Layers and Unreliable Links</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the multi-message broadcast problem using abstract MAC layer models
of wireless networks. These models capture the key guarantees of existing MAC
layers while abstracting away low-level details such as signal propagation and
contention. We begin by studying upper and lower bounds for this problem in a
{\em standard abstract MAC layer model}---identifying an interesting dependence
between the structure of unreliable links and achievable time complexity. In
more detail, given a restriction that devices connected directly by an
unreliable link are not too far from each other in the reliable link topology,
we can (almost) match the efficiency of the reliable case. For the related
restriction, however, that two devices connected by an unreliable link are not
too far from each other in geographic distance, we prove a new lower bound that
shows that this efficiency is impossible. We then investigate how much extra
power must be added to the model to enable a new order of magnitude of
efficiency. In more detail, we consider an {\em enhanced abstract MAC layer
model} and present a new multi-message broadcast algorithm that (under certain
natural assumptions) solves the problem in this model faster than any known
solutions in an abstract MAC layer setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1675</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1675</id><created>2014-05-07</created><updated>2014-12-18</updated><authors><author><keyname>Teso</keyname><forenames>Stefano</forenames></author><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author><author><keyname>Passerini</keyname><forenames>Andrea</forenames></author></authors><title>Structured Learning Modulo Theories</title><categories>cs.AI</categories><comments>46 pages, 11 figures, submitted to Artificial Intelligence Journal
  Special Issue on Combining Constraint Solving with Mining and Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling problems containing a mixture of Boolean and numerical variables is
a long-standing interest of Artificial Intelligence. However, performing
inference and learning in hybrid domains is a particularly daunting task. The
ability to model this kind of domains is crucial in &quot;learning to design&quot; tasks,
that is, learning applications where the goal is to learn from examples how to
perform automatic {\em de novo} design of novel objects. In this paper we
present Structured Learning Modulo Theories, a max-margin approach for learning
in hybrid domains based on Satisfiability Modulo Theories, which allows to
combine Boolean reasoning and optimization over continuous linear arithmetical
constraints. The main idea is to leverage a state-of-the-art generalized
Satisfiability Modulo Theory solver for implementing the inference and
separation oracles of Structured Output SVMs. We validate our method on
artificial and real world scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1678</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1678</id><created>2014-05-07</created><updated>2015-03-12</updated><authors><author><keyname>Dang</keyname><forenames>Chinh</forenames></author><author><keyname>Moghadam</keyname><forenames>Abdolreza</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>RPCA-KFE: Key Frame Extraction for Consumer Video based Robust Principal
  Component Analysis</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key frame extraction algorithms consider the problem of selecting a subset of
the most informative frames from a video to summarize its content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1679</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1679</id><created>2014-04-29</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>What Cost Knowledge Management? The Example of Infosys</title><categories>cs.CY</categories><journal-ref>What Cost Knowledge Management? The Example of Infosys. Global
  Business and Organizational Excellence, 32(3), 2013, pp. 6-14</journal-ref><doi>10.1002/joe.21480</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The term knowledge management (KM) first came to prominence in the late
1990s. Although initially dismissed as a fad, KM continues to be featured in
articles concerning business productivity and innovation. And yet, clear-cut
examples that demonstrate the success of KM are few and far between. A brief
examination of the history of KM explores the reasons for this and looks at
some of the assumptions about what KM can achieve. A subsequent analysis of the
experiences of Infosys with KM shows that for KM to be successful,
organizational leaders need to engage in a continuous process of modification
and maintenance. Although KM initiatives can be made to yield worthwhile
returns over an extended period, there are often substantial ongoing costs
associated with them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1681</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1681</id><created>2014-05-07</created><updated>2015-03-11</updated><authors><author><keyname>Dang</keyname><forenames>Chinh</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>Representative Selection for Big Data via Sparse Graph and Geodesic
  Grassmann Manifold Distance</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to lacking details</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of identifying a very small subset of data
points that belong to a significantly larger massive dataset (i.e., Big Data).
The small number of selected data points must adequately represent and
faithfully characterize the massive Big Data. Such identification process is
known as representative selection [19]. We propose a novel representative
selection framework by generating an l1 norm sparse graph for a given Big-Data
dataset. The Big Data is partitioned recursively into clusters using a spectral
clustering algorithm on the generated sparse graph. We consider each cluster as
one point in a Grassmann manifold, and measure the geodesic distance among
these points. The distances are further analyzed using a min-max algorithm [1]
to extract an optimal subset of clusters. Finally, by considering a sparse
subgraph of each selected cluster, we detect a representative using principal
component centrality [11]. We refer to the proposed representative selection
framework as a Sparse Graph and Grassmann Manifold (SGGM) based approach. To
validate the proposed SGGM framework, we apply it onto the problem of video
summarization where only few video frames, known as key frames, are selected
among a much longer video sequence. A comparison of the results obtained by the
proposed algorithm with the ground truth, which is agreed by multiple human
judges, and with some state-of-the-art methods clearly indicates the viability
of the SGGM framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1688</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1688</id><created>2014-05-07</created><authors><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>Newport</keyname><forenames>Calvin</forenames></author><author><keyname>Radeva</keyname><forenames>Tsvetomira</forenames></author></authors><title>Trade-offs between Selection Complexity and Performance when Searching
  the Plane without Communication</title><categories>cs.DC</categories><comments>appears in PODC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the ANTS problem [Feinerman et al.] in which a group of agents
collaboratively search for a target in a two-dimensional plane. Because this
problem is inspired by the behavior of biological species, we argue that in
addition to studying the {\em time complexity} of solutions it is also
important to study the {\em selection complexity}, a measure of how likely a
given algorithmic strategy is to arise in nature due to selective pressures. In
more detail, we propose a new selection complexity metric $\chi$, defined for
algorithm ${\cal A}$ such that $\chi({\cal A}) = b + \log \ell$, where $b$ is
the number of memory bits used by each agent and $\ell$ bounds the fineness of
available probabilities (agents use probabilities of at least $1/2^\ell$). In
this paper, we study the trade-off between the standard performance metric of
speed-up, which measures how the expected time to find the target improves with
$n$, and our new selection metric.
  In particular, consider $n$ agents searching for a treasure located at
(unknown) distance $D$ from the origin (where $n$ is sub-exponential in $D$).
For this problem, we identify $\log \log D$ as a crucial threshold for our
selection complexity metric. We first prove a new upper bound that achieves a
near-optimal speed-up of $(D^2/n +D) \cdot 2^{O(\ell)}$ for $\chi({\cal A})
\leq 3 \log \log D + O(1)$. In particular, for $\ell \in O(1)$, the speed-up is
asymptotically optimal. By comparison, the existing results for this problem
[Feinerman et al.] that achieve similar speed-up require $\chi({\cal A}) =
\Omega(\log D)$. We then show that this threshold is tight by describing a
lower bound showing that if $\chi({\cal A}) &lt; \log \log D - \omega(1)$, then
with high probability the target is not found within $D^{2-o(1)}$ moves per
agent. Hence, there is a sizable gap to the straightforward $\Omega(D^2/n + D)$
lower bound in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1703</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1703</id><created>2014-05-05</created><authors><author><keyname>Fang</keyname><forenames>Ling</forenames><affiliation>National Institute of Advanced Industrial Science and Technology, Japan</affiliation></author><author><keyname>Yamagata</keyname><forenames>Yoriyuki</forenames><affiliation>National Institute of Advanced Industrial Science and Technology, Japan</affiliation></author><author><keyname>Oiwa</keyname><forenames>Yutaka</forenames><affiliation>National Institute of Advanced Industrial Science and Technology, Japan</affiliation></author></authors><title>Evaluation of A Resilience Embedded System Using Probabilistic
  Model-Checking</title><categories>cs.SE cs.LO math.PR</categories><comments>In Proceedings ESSS 2014, arXiv:1405.0554</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 150, 2014, pp. 35-49</journal-ref><doi>10.4204/EPTCS.150.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a Micro Processor Unit (MPU) receives an external electric signal as
noise, the system function will freeze or malfunction easily. A new resilience
strategy is implemented in order to reset the MPU automatically and stop the
MPU from freezing or malfunctioning. The technique is useful for embedded
systems which work in non-human environments. However, evaluating resilience
strategies is difficult because their effectiveness depends on numerous,
complex, interacting factors.
  In this paper, we use probabilistic model checking to evaluate the embedded
systems installed with the above mentioned new resilience strategy. Qualitative
evaluations are implemented with 6 PCTL formulas, and quantitative evaluations
use two kinds of evaluation. One is system failure reduction, and the other is
ADT (Average Down Time), the industry standard. Our work demonstrates the
benefits brought by the resilience strategy. Experimental results indicate that
our evaluation is cost-effective and reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1705</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1705</id><created>2014-05-07</created><authors><author><keyname>Grover</keyname><forenames>Raman</forenames></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames></author></authors><title>Scalable Fault-Tolerant Data Feeds in AsterixDB</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the support for data feed ingestion in AsterixDB,
an open-source Big Data Management System (BDMS) that provides a platform for
storage and analysis of large volumes of semi-structured data. Data feeds are a
mechanism for having continuous data arrive into a BDMS from external sources
and incrementally populate a persisted dataset and associated indexes. The need
to persist and index &quot;fast-flowing&quot; high-velocity data (and support ad hoc
analytical queries) is ubiquitous. However, the state of the art today involves
'gluing' together different systems. AsterixDB is different in being a unified
system with &quot;native support&quot; for data feed ingestion.
  We discuss the challenges and present the design and implementation of the
concepts involved in modeling and managing data feeds in AsterixDB. AsterixDB
allows the runtime behavior, allocation of resources and the offered degree of
robustness to be customized to suit the high-level application(s) that wish to
consume the ingested data. Initial experiments that evaluate scalability and
fault-tolerance of AsterixDB data feeds facility are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1715</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1715</id><created>2014-05-07</created><updated>2014-08-25</updated><authors><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author></authors><title>Some Turing-Complete Extensions of First-Order Logic</title><categories>math.LO cs.LO</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 4-17</journal-ref><doi>10.4204/EPTCS.161.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a natural Turing-complete extension of first-order logic FO. The
extension adds two novel features to FO. The first one of these is the capacity
to add new points to models and new tuples to relations. The second one is the
possibility of recursive looping when a formula is evaluated using a semantic
game. We first define a game-theoretic semantics for the logic and then prove
that the expressive power of the logic corresponds in a canonical way to the
recognition capacity of Turing machines. Finally, we show how to incorporate
generalized quantifiers into the logic and argue for a highly natural
connection between oracles and generalized quantifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1717</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1717</id><created>2014-05-07</created><authors><author><keyname>Yilmaz</keyname><forenames>Kutlu Emre</forenames></author></authors><title>Entropy Based Cartoon Texture Separation</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separating an image into cartoon and texture components comes useful in image
processing applications, such as image compression, image segmentation, image
inpainting. Yves Meyer's influential cartoon texture decomposition model
involves deriving an energy functional by choosing appropriate spaces and
functionals. Minimizers of the derived energy functional are cartoon and
texture components of an image. In this study, cartoon part of an image is
separated, by reconstructing it from pixels of multi scale Total-Variation
filtered versions of the original image which is sought to be decomposed into
cartoon and texture parts. An information theoretic pixel by pixel selection
criteria is employed to choose the contributing pixels and their scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1734</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1734</id><created>2014-05-07</created><updated>2014-05-14</updated><authors><author><keyname>Le</keyname><forenames>Tiep</forenames></author><author><keyname>Pontelli</keyname><forenames>Enrico</forenames></author><author><keyname>Son</keyname><forenames>Tran Cao</forenames></author><author><keyname>Yeoh</keyname><forenames>William</forenames></author></authors><title>Logic and Constraint Logic Programming for Distributed Constraint
  Optimization</title><categories>cs.MA cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><msc-class>68-06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of Distributed Constraint Optimization Problems (DCOPs) has gained
momentum, thanks to its suitability in capturing complex problems (e.g.,
multi-agent coordination and resource allocation problems) that are naturally
distributed and cannot be realistically addressed in a centralized manner. The
state of the art in solving DCOPs relies on the use of ad-hoc infrastructures
and ad-hoc constraint solving procedures. This paper investigates an
infrastructure for solving DCOPs that is completely built on logic programming
technologies. In particular, the paper explores the use of a general constraint
solver (a constraint logic programming system in this context) to handle the
agent-level constraint solving. The preliminary experiments show that logic
programming provides benefits over a state-of-the-art DCOP system, in terms of
performance and scalability, opening the doors to the use of more advanced
technology (e.g., search strategies and complex constraints) for solving DCOPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1740</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1740</id><created>2014-05-07</created><authors><author><keyname>Y&#x131;lmaz</keyname><forenames>Kutlu Emre</forenames></author><author><keyname>Arslan</keyname><forenames>Ahmet</forenames></author><author><keyname>Yilmazel</keyname><forenames>Ozgur</forenames></author></authors><title>Turkish Text Retrieval Experiments Using Lemur Toolkit</title><categories>cs.IR</categories><comments>3 pages</comments><journal-ref>IADIS AC 2009: Rome, Italy</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We used Lemur Toolkit, an open source toolkit designed for Information
Retrieval (IR) research, for our automated indexing and retrieval experiments
on a TREC-like test collection for Turkish. We study and compare three
retrieval models Lemur supports, especially Language modeling approach to IR,
combined with language specific preprocessing techniques. Our experiments show
that all retrieval models benefits from language specific preprocessing in
terms of retrieval quality. Also Language Modeling approach is the best
performing retrieval model when language specific preprocessing applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1752</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1752</id><created>2014-05-07</created><authors><author><keyname>Gelley</keyname><forenames>Bluma</forenames></author><author><keyname>John</keyname><forenames>Ajita</forenames></author></authors><title>Like, Comment, Repin: User Interaction on Pinterest</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the results of a study of the Pinterest activity graph. Pinterest
is a relatively new and extremely popular content-based social network.
Building on a body of work showing that the hidden network whose edges are
actual interactions between users is more informative about social
relationships that the follower-following network, we study the activity graph
composed of links formed by liking, commenting, and repinning, and show that it
is very different from the follow network. We collect data about 14 million
pins, 7 million repins, 1.6 million likes, and several hundred thousand users
and report interesting results about social activity on Pinterest. In
particular, we discover that only 12.3% of a user's followers interact with
their pins, and over 70% of activity on each user's boards is done by
non-followers, but on average, followers who are active perform twice as many
actions as non-followers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1761</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1761</id><created>2014-05-07</created><authors><author><keyname>Franceschetti</keyname><forenames>Massimo</forenames></author></authors><title>On Landau's eigenvalue theorem and information cut-sets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variation of Landau's eigenvalue theorem describing the phase transition of
the eigenvalues of a time-frequency limiting, self adjoint operator is
presented. The total number of degrees of freedom of square-integrable,
multi-dimensional, bandlimited functions is defined in terms of Kolmogorov's
$n$-width and computed in some limiting regimes where the original theorem
cannot be directly applied. Results are used to characterize up to order the
total amount of information that can be transported in time and space by
multiple-scattered electromagnetic waves, rigorously addressing a question
originally posed in the early works of Toraldo di Francia and Gabor.
Applications in the context of wireless communication and electromagnetic
sensing are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1773</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1773</id><created>2014-05-07</created><authors><author><keyname>Yuan</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Cun-Hui</forenames></author></authors><title>On Tensor Completion via Nuclear Norm Minimization</title><categories>stat.ML cs.IT math.IT math.NA math.OC math.PR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Many problems can be formulated as recovering a low-rank tensor. Although an
increasingly common task, tensor recovery remains a challenging problem because
of the delicacy associated with the decomposition of higher order tensors. To
overcome these difficulties, existing approaches often proceed by unfolding
tensors into matrices and then apply techniques for matrix completion. We show
here that such matricization fails to exploit the tensor structure and may lead
to suboptimal procedure. More specifically, we investigate a convex
optimization approach to tensor completion by directly minimizing a tensor
nuclear norm and prove that this leads to an improved sample size requirement.
To establish our results, we develop a series of algebraic and probabilistic
techniques such as characterization of subdifferetial for tensor nuclear norm
and concentration inequalities for tensor martingales, which may be of
independent interests and could be useful in other tensor related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1781</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1781</id><created>2014-05-07</created><authors><author><keyname>Bhattacharya</keyname><forenames>Arka</forenames></author></authors><title>Approximation Algorithms for the Asymmetric Traveling Salesman Problem :
  Describing two recent methods</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provides a description of the two recent approximation algorithms
for the Asymmetric Traveling Salesman Problem, giving the intuitive description
of the works of Feige-Singh[1] and Asadpour et.al\ [2].\newline [1] improves
the previous $O(\log n)$ approximation algorithm, by improving the constant
from 0.84 to 0.66 and modifying the work of Kaplan et. al\ [3] and also shows
an efficient reduction from ATSPP to ATSP. Combining both the results, they
finally establish an approximation ratio of $\left(\frac{4}{3}+\epsilon
\right)\log n$ for ATSPP,\ considering a small $\epsilon&gt;0$,\ improving the
work of Chekuri and Pal.[4]\newline Asadpour et.al, in their seminal work\ [2],
gives an $O\left(\frac{\log n}{\log \log n}\right)$ randomized algorithm for
the ATSP, by symmetrizing and modifying the solution of the Held-Karp
relaxation problem and then proving an exponential family distribution for
probabilistically constructing a maximum entropy spanning tree from a spanning
tree polytope and then finally defining the thin-ness property and transforming
a thin spanning tree into an Eulerian walk.\ The optimization methods used in\
[2] are quite elegant and the approximation ratio could further be improved, by
manipulating the thin-ness of the cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1782</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1782</id><created>2014-05-07</created><updated>2015-01-29</updated><authors><author><keyname>Kolte</keyname><forenames>Ritesh</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>When are dynamic relaying strategies necessary in half-duplex wireless
  networks?</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple question: when are dynamic relaying strategies essential in
optimizing the diversity-multiplexing tradeoff (DMT) in half-duplex wireless
relay networks? This is motivated by apparently two contrasting results even
for a simple 3 node network, with a single half-duplex relay. When all channels
are assumed to be i.i.d. fading, a static schedule where the relay listens half
the time and transmits half the time combined with quantize-map-forward (QMF)
relaying is known to achieve the full-duplex performance. However, when there
is no direct link between source and destination, a dynamic-decode-forward
(DDF) strategy is needed to achieve the optimal tradeoff. In this case, a
static schedule is strictly suboptimal and the optimal tradeoff is
significantly worse than the full-duplex performance. In this paper we study
the general case when the direct link is neither as strong as the other links
nor fully non-existent, and identify regimes where dynamic schedules are
necessary and those where static schedules are enough. We identify 4
qualitatively different regimes for the single relay channel where the tradeoff
between diversity and multiplexing is significantly different. We show that in
all these regimes one of the above two strategies is sufficient to achieve the
optimal tradeoff by developing a new upper bound on the best achievable
tradeoff under channel state information available only at the receivers. A
natural next question is whether these two strategies are sufficient to achieve
the DMT of more general half-duplex wireless networks. We propose a
generalization of the two existing schemes through a dynamic QMF (DQMF)
strategy, where the relay listens for a fraction of time depending on received
CSI but not long enough to be able to decode. We show that such a DQMF strategy
is needed to achieve the optimal DMT in a parallel channel with two relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1790</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1790</id><created>2014-05-07</created><updated>2014-08-10</updated><authors><author><keyname>Pei</keyname><forenames>Sen</forenames></author><author><keyname>Muchnik</keyname><forenames>Lev</forenames></author><author><keyname>Andrade</keyname><forenames>Jose S.</forenames><suffix>Jr.</suffix></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>Searching for superspreaders of information in real-world social media</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures</comments><journal-ref>Scientific Reports, 4, 5547 (2014)</journal-ref><doi>10.1038/srep05547</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of predictors have been suggested to detect the most influential
spreaders of information in online social media across various domains such as
Twitter or Facebook. In particular, degree, PageRank, k-core and other
centralities have been adopted to rank the spreading capability of users in
information dissemination media. So far, validation of the proposed predictors
has been done by simulating the spreading dynamics rather than following real
information flow in social networks. Consequently, only model-dependent
contradictory results have been achieved so far for the best predictor. Here,
we address this issue directly. We search for influential spreaders by
following the real spreading dynamics in a wide range of networks. We find that
the widely-used degree and PageRank fail in ranking users' influence. We find
that the best spreaders are consistently located in the k-core across
dissimilar social platforms such as Twitter, Facebook, Livejournal and
scientific publishing in the American Physical Society. Furthermore, when the
complete global network structure is unavailable, we find that the sum of the
nearest neighbors' degree is a reliable local proxy for user's influence. Our
analysis provides practical instructions for optimal design of strategies for
&quot;viral&quot; information dissemination in relevant applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1794</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1794</id><created>2014-05-07</created><updated>2014-07-23</updated><authors><author><keyname>Abolhassani</keyname><forenames>Melika</forenames></author><author><keyname>Bateni</keyname><forenames>MohammadHossein</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Mahini</keyname><forenames>Hamid</forenames></author><author><keyname>Sawant</keyname><forenames>Anshul</forenames></author></authors><title>Network Cournot Competition</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cournot competition is a fundamental economic model that represents firms
competing in a single market of a homogeneous good. Each firm tries to maximize
its utility---a function of the production cost as well as market price of the
product---by deciding on the amount of production. In today's dynamic and
diverse economy, many firms often compete in more than one market
simultaneously, i.e., each market might be shared among a subset of these
firms. In this situation, a bipartite graph models the access restriction where
firms are on one side, markets are on the other side, and edges demonstrate
whether a firm has access to a market or not. We call this game \emph{Network
Cournot Competition} (NCC). In this paper, we propose algorithms for finding
pure Nash equilibria of NCC games in different situations. First, we carefully
design a potential function for NCC, when the price functions for markets are
linear functions of the production in that market. However, for nonlinear price
functions, this approach is not feasible. We model the problem as a nonlinear
complementarity problem in this case, and design a polynomial-time algorithm
that finds an equilibrium of the game for strongly convex cost functions and
strongly monotone revenue functions. We also explore the class of price
functions that ensures strong monotonicity of the revenue function, and show it
consists of a broad class of functions. Moreover, we discuss the uniqueness of
equilibria in both of these cases which means our algorithms find the unique
equilibria of the games. Last but not least, when the cost of production in one
market is independent from the cost of production in other markets for all
firms, the problem can be separated into several independent classical
\emph{Cournot Oligopoly} problems. We give the first combinatorial algorithm
for this widely studied problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1802</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1802</id><created>2014-05-08</created><authors><author><keyname>Hassan</keyname><forenames>Mai H.</forenames></author><author><keyname>Hossain</keyname><forenames>Md. J.</forenames></author></authors><title>Cooperative Beamforming for Cognitive Radio-Based Broadcasting Systems
  with Asynchronous Interferences</title><categories>math.OC cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to address the asynchronous interference issue for a generalized
scenario with multiple primary and multiple secondary receivers, in this paper,
we propose an innovative cooperative beamforming technique. In particular, the
cooperative beamforming design is formulated as an optimization problem that
maximizes the weighted sum achievable transmission rate of secondary
destinations while it maintains the asynchronous interferences at the primary
receivers below their target thresholds. In light of the intractability of the
problem, we propose a two-phase suboptimal cooperative beamforming technique.
First, it finds the beamforming directions corresponding to different secondary
destinations. Second, it allocates the power among different beamforming
directions. Due to the multiple interference constraints corresponding to
multiple primary receivers, the power allocation scheme in the second phase is
still complex. Therefore, we also propose a low complex power allocation
algorithm. The proposed beamforming technique is extended for the cases, when
cooperating CR nodes (CCRNs) have statistical or erroneous channel knowledge of
the primary receivers. We also investigate the performance of joint CCRN
selection and beamforming technique. The presented numerical results show that
the proposed beamforming technique can significantly reduce the asynchronous
interference signals at the primary receivers and increase the sum transmission
rate of secondary destinations compared to the well known zero-forcing
beamforming (ZFBF) technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1811</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1811</id><created>2014-05-08</created><authors><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Smaragdakis</keyname><forenames>Georgios</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. K.</forenames></author></authors><title>Opportunities in a Federated Cloud Marketplace</title><categories>cs.NI cs.DC</categories><comments>4 pages</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent measurement studies show that there are massively distributed hosting
and computing infrastructures deployed in the Internet. Such infrastructures
include large data centers and organizations' computing clusters. When idle,
these resources can readily serve local users. Such users can be smartphone or
tablet users wishing to access services such as remote desktop or CPU/bandwidth
intensive activities. Particularly, when they are likely to have high latency
to access, or may have no access at all to, centralized cloud providers. Today,
however, there is no global marketplace where sellers and buyers of available
resources can trade. The recently introduced marketplaces of Amazon and other
cloud infrastructures are limited by the network footprint of their own
infrastructures and availability of such services in the target country and
region. In this article we discuss the potentials for a federated cloud
marketplace where sellers and buyers of a number of resources, including
storage, computing, and network bandwidth, can freely trade. This ecosystem can
be regulated through brokers who act as service level monitors and auctioneers.
We conclude by discussing the challenges and opportunities in this space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1814</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1814</id><created>2014-05-08</created><authors><author><keyname>M</keyname><forenames>Thangaraj</forenames></author><author><keyname>V</keyname><forenames>Gayathri</forenames></author></authors><title>A Retrieval Mechanism for Multi-versioned Digital Collection Using TAG</title><categories>cs.IR cs.DL</categories><comments>6 pages, 7 figures, Published with International Journal of Computer
  &amp; Organization Trends (IJCOT)</comments><journal-ref>Dr. M. Thangaraj , V. Gayathri. &quot;A Retrieval Mechanism for
  Multi-versioned Digital Collection Using TAG&quot;, International Journal of
  Computer &amp; organization Trends (IJCOT),V7(1):14-19 April 2014. ISSN:2249-2593</journal-ref><doi>10.14445/22492593/IJCOT-V7P303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the marvellous growth of the digital library in each year, the problems
with indexing and searching a digital library is increased in a high rate. When
the researchers search for the earlier versions, only a few recent versions in
the back volumes can be retrieved soon. It is unpredictable that researchers
require the earlier versions in a specific boundary. In order to facilitate the
researchers, who may access any version at any time, we propose a VTAG
technique for indexing. Our experiments indicate that the proposed retrieval
technique, VTAG, effectively retrieves any version in considerable amount of
time than the existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1815</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1815</id><created>2014-05-08</created><authors><author><keyname>Das</keyname><forenames>Deepjoy</forenames></author><author><keyname>Saharia</keyname><forenames>Dr. Sarat</forenames></author></authors><title>Implementation And Performance Evaluation Of Background Subtraction
  Algorithms</title><categories>cs.CV</categories><doi>10.5121/ijcsa.2014.4206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study evaluates three background subtraction techniques. The techniques
ranges from very basic algorithm to state of the art published techniques
categorized based on speed, memory requirements and accuracy. Such a review can
effectively guide the designer to select the most suitable method for a given
application in a principled way. The algorithms used in the study ranges from
varying levels of accuracy and computational complexity. Few of them can also
deal with real time challenges like rain, snow, hails, swaying branches,
objects overlapping, varying light intensity or slow moving objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1818</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1818</id><created>2014-05-08</created><authors><author><keyname>Sarma</keyname><forenames>Prof N. V. S. N</forenames></author><author><keyname>Gopi</keyname><forenames>Mahesh</forenames></author></authors><title>Energy Efficient Clustering using Jumper Firefly Algorithm in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>8 pages, 2 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>Prof N.V.S.N Sarma , Mahesh Gopi. Article: Energy Efficient
  Clustering using Jumper Firefly Algorithm in Wireless Sensor Networks.
  International Journal of Engineering Trends and Technology (IJETT), V10P304
  April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Network (WSN) is a major and very interesting technology,
which consists of small battery powered sensor nodes with limited power
resources. The sensor nodes are inaccessible to the user once they are
deployed. Replacing the battery is not possible every time. Hence in order to
improve the lifetime of the network, energy efficiency of the net- work needs
to be maximized by decreasing the energy consumption of all the sensor nodes
and balancing energy consumption of every node. Several protocols have been
proposed earlier to improve the network lifetime using optimization algorithms.
Firefly is a metaheuristic approach. In this paper, Energy efficient clustering
for wireless sensor networks using Firefly and Jumper Firefly algorithms are
simulated. A new cost function has been defined to minimize the intra- cluster
distance to optimize the energy consumption of the network. The performance is
compared with the existing protocol LEACH (Low Energy Adaptive Clustering
Hierarchy).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1821</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1821</id><created>2014-05-08</created><authors><author><keyname>Mehdi</keyname><forenames>Haifa</forenames></author><author><keyname>Boubaker</keyname><forenames>Olfa</forenames></author></authors><title>Robust Tracking Control for Constrained Robots</title><categories>cs.RO</categories><comments>6 pages, 2 figures</comments><journal-ref>Procedia Engineering, vol. 41, 1292-1297, 2012</journal-ref><doi>10.1016/j.proeng.2012.07.313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel robust tracking control law is proposed for
constrained robots under unknown stiffness environment. The stability and the
robustness of the controller are proved using a Lyapunov-based approach where
the relationship between the error dynamics of the robotic system and its
energy is investigated. Finally, a 3DOF constrained robotic arm is used to
prove the stability, the robustness and the safety of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1823</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1823</id><created>2014-05-08</created><authors><author><keyname>Saeed</keyname><forenames>Ahmed</forenames></author><author><keyname>Neishaboori</keyname><forenames>Azin</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Harras</keyname><forenames>Khaled</forenames></author></authors><title>Up and Away: A Cheap UAV Cyber-Physical Testbed (Work in Progress)</title><categories>cs.SY</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-Physical Systems (CPS) have the promise of presenting the next
evolution in computing with potential applications that include aerospace,
transportation, robotics, and various automation systems. These applications
motivate advances in the different sub-fields of CPS (e.g. mobile computing and
communication, control, and vision). However, deploying and testing complete
CPSs is known to be a complex and expensive task. In this paper, we present the
design, implementation, and evaluation of Up and Away (UnA): a testbed for
Cyber-Physical Systems that use UAVs as their physical component. UnA aims at
abstracting the control of physical components of the system to reduce the
complexity of UAV oriented Cyber-Physical Systems experiments. In addition, UnA
provides an API to allow for converting CPS simulations into physical
experiments using a few simple steps. We present a case study bringing a
mobile-camera-based surveillance system simulation to life using UnA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1827</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1827</id><created>2014-05-08</created><updated>2015-01-26</updated><authors><author><keyname>Klaus</keyname><forenames>Lorenz</forenames></author></authors><title>A Strongly Polynomial Reduction for Linear Programs over Grids</title><categories>cs.CC math.OC</categories><comments>minor changes</comments><msc-class>90C05, 90C27, 90C33, 90C40</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the duality relation between linear programs over grids
(Grid-LPs) and generalized linear complementarity problems (GLCPs) with hidden
K-matrices. The two problems, moreover, share their combinatorial structure
with discounted Markov decision processes (MDPs). Through proposing reduction
schemes for the GLCP, we obtain a strongly polynomial reduction from Grid-LPs
to linear programs over cubes (Cube-LPs). As an application, we obtain a scheme
to reduce discounted MDPs to their binary counterparts. This result also
suggests that Cube-LPs are the key problems with respect to solvability of
linear programming in strongly polynomial time. We then consider two-player
stochastic games with perfect information as a natural generalization of
discounted MDPs. We identify the subclass of the GLCPs with P-matrices that
corresponds to these games and also provide a characterization in terms of
unique-sink orientations. A strongly polynomial reduction from the games to
their binary counterparts is obtained through a generalization of our reduction
for Grid-LPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1828</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1828</id><created>2014-05-08</created><updated>2016-02-13</updated><authors><author><keyname>Ghari</keyname><forenames>Meghdad</forenames></author></authors><title>Tableau Proof Systems for Justification Logics</title><categories>math.LO cs.LO</categories><comments>21 pages</comments><msc-class>03B60, 03F07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present analytic tableau proof systems for various
justification logics. We show that the tableau systems are sound and complete
with respect to Mkrtychev models. In order to prove the completeness of the
analytic tableaux, we give a syntactic proof of cut elimination. We show the
subformula property for our tableaux, and prove the decidability of
justification logics for finite constant specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1833</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1833</id><created>2014-05-08</created><updated>2014-05-09</updated><authors><author><keyname>Bogaerts</keyname><forenames>Bart</forenames></author><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>FO(C): A Knowledge Representation Language of Causality</title><categories>cs.LO cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cause-effect relations are an important part of human knowledge. In real
life, humans often reason about complex causes linked to complex effects. By
comparison, existing formalisms for representing knowledge about causal
relations are quite limited in the kind of specifications of causes and effects
they allow. In this paper, we present the new language C-Log, which offers a
significantly more expressive representation of effects, including such
features as the creation of new objects. We show how C-Log integrates with
first-order logic, resulting in the language FO(C). We also compare FO(C) with
several related languages and paradigms, including inductive definitions,
disjunctive logic programming, business rules and extensions of Datalog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1836</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1836</id><created>2014-05-08</created><authors><author><keyname>Guo</keyname><forenames>Meng</forenames></author><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Cooperative Decentralized Multi-agent Control under Local LTL Tasks and
  Connectivity Constraints</title><categories>cs.SY cs.MA</categories><comments>full version of CDC 2014 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for the decentralized control of a team of agents that
are assigned local tasks expressed as Linear Temporal Logic (LTL) formulas.
Each local LTL task specification captures both the requirements on the
respective agent's behavior and the requests for the other agents'
collaborations needed to accomplish the task. Furthermore, the agents are
subject to communication constraints. The presented solution follows the
automata-theoretic approach to LTL model checking, however, it avoids the
computationally demanding construction of synchronized product system between
the agents. We suggest a decentralized coordination among the agents through a
dynamic leader-follower scheme, to guarantee the low-level connectivity
maintenance at all times and a progress towards the satisfaction of the
leader's task. By a systematic leader switching, we ensure that each agent's
task will be accomplished.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1837</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1837</id><created>2014-05-08</created><updated>2014-09-08</updated><authors><author><keyname>Lacic</keyname><forenames>Emanuel</forenames></author><author><keyname>Kowald</keyname><forenames>Dominik</forenames></author><author><keyname>Eberhard</keyname><forenames>Lukas</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author><author><keyname>Parra</keyname><forenames>Denis</forenames></author><author><keyname>Marinho</keyname><forenames>Leandro</forenames></author></authors><title>Utilizing Online Social Network and Location-Based Data to Recommend
  Products and Categories in Online Marketplaces</title><categories>cs.IR</categories><comments>20 pages book chapter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has unveiled the importance of online social networks for
improving the quality of recommender systems and encouraged the research
community to investigate better ways of exploiting the social information for
recommendations. To contribute to this sparse field of research, in this paper
we exploit users' interactions along three data sources (marketplace, social
network and location-based) to assess their performance in a barely studied
domain: recommending products and domains of interests (i.e., product
categories) to people in an online marketplace environment. To that end we
defined sets of content- and network-based user similarity features for each
data source and studied them isolated using an user-based Collaborative
Filtering (CF) approach and in combination via a hybrid recommender algorithm,
to assess which one provides the best recommendation performance.
Interestingly, in our experiments conducted on a rich dataset collected from
SecondLife, a popular online virtual world, we found that recommenders relying
on user similarity features obtained from the social network data clearly
yielded the best results in terms of accuracy in case of predicting products,
whereas the features obtained from the marketplace and location-based data
sources also obtained very good results in case of predicting categories. This
finding indicates that all three types of data sources are important and should
be taken into account depending on the level of specialization of the
recommendation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1841</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1841</id><created>2014-05-08</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author></authors><title>Keeping a Crowd Safe: On the Complexity of Parameterized Verification
  (Corrected version)</title><categories>cs.LO cs.FL</categories><comments>A former version of this paper was published in the Proceedings of
  STACS 2014. This version corrects two mistakes in Sections 3.3. and 3.4, and
  some typos</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We survey some results on the automatic verification of parameterized
programs without identities. These are systems composed of arbitrarily many
components, all of them running exactly the same finite-state program. We
discuss the complexity of deciding that no component reaches an unsafe state.
The note is addressed at theoretical computer scientists in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1842</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1842</id><created>2014-05-08</created><authors><author><keyname>Lacic</keyname><forenames>Emanuel</forenames></author><author><keyname>Kowald</keyname><forenames>Dominik</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author></authors><title>SocRecM: A Scalable Social Recommender Engine for Online Marketplaces</title><categories>cs.IR</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present work-in-progress on SocRecM, a novel social
recommendation framework for online marketplaces. We demonstrate that SocRecM
is not only easy to integrate with existing Web technologies through a RESTful,
scalable and easy-to-extend service-based architecture but also reveal the
extent to which various social features and recommendation approaches are
useful in an online social marketplace environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1851</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1851</id><created>2014-05-08</created><authors><author><keyname>Selvi</keyname><forenames>P. Cynthia</forenames></author><author><keyname>Shanavas</keyname><forenames>A. R. Mohammed</forenames></author></authors><title>Output Privacy Protection With Pattern-Based Heuristic Algorithm</title><categories>cs.DB</categories><comments>12 pages</comments><doi>10.5121/ijcsit.2014.6210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at
bridging the gap between the collaborative data mining and data confidentiality
There are many different approaches which have been adopted for PPDM, of them
the rule hiding approach is used in this article. This approach ensures output
privacy that prevent the mined patterns(itemsets) from malicious inference
problems. An efficient algorithm named as Pattern-based Maxcover Algorithm is
proposed with experimental results. This algorithm minimizes the dissimilarity
between the source and the released database; Moreover the patterns protected
cannot be retrieved from the released database by an adversary or counterpart
even with an arbitrarily low support threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1853</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1853</id><created>2014-05-08</created><authors><author><keyname>Elshaer</keyname><forenames>Hisham</forenames></author><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Irmer</keyname><forenames>Ralf</forenames></author></authors><title>Downlink and Uplink Decoupling: a Disruptive Architectural Design for 5G
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 7 figures, conference paper, submitted to IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell association in cellular networks has traditionally been based on the
downlink received signal power only, despite the fact that up and downlink
transmission powers and interference levels differed significantly. This
approach was adequate in homogeneous networks with macro base stations all
having similar transmission power levels. However, with the growth of
heterogeneous networks where there is a big disparity in the transmit power of
the different base station types, this approach is highly inefficient. In this
paper, we study the notion of Downlink and Uplink Decoupling (DUDe) where the
downlink cell association is based on the downlink received power while the
uplink is based on the pathloss. We present the motivation and assess the gains
of this 5G design approach with simulations that are based on Vodafone's LTE
field trial network in a dense urban area, employing a high resolution
ray-tracing pathloss prediction and realistic traffic maps based on live
network measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1857</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1857</id><created>2014-05-08</created><updated>2014-09-17</updated><authors><author><keyname>Kundu</keyname><forenames>Atreyee</forenames></author><author><keyname>Balachandran</keyname><forenames>Niranjan</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author></authors><title>Deterministic and probabilistic algorithms for stabilizing discrete-time
  switched linear systems</title><categories>cs.SY math.OC</categories><comments>11 pages, 2 figures</comments><msc-class>93C30, 93D20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study algorithmic synthesis of the class of stabilizing
switching signals for discrete-time switched linear systems proposed in [12]. A
weighted digraph is associated in a natural way to a switched system, and the
switching signal is expressed as an infinite walk on this weighted digraph. We
employ graph-theoretic tools and discuss different algorithms for designing
walks whose corresponding switching signals satisfy the stabilizing switching
conditions proposed in [12]. We also address the issue of how likely/generic it
is for a family of systems to admit stabilizing switching signals, and under
mild assumptions give sufficient conditions for the same. Our solutions have
both deterministic and probabilistic flavours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1861</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1861</id><created>2014-05-08</created><updated>2014-06-27</updated><authors><author><keyname>Andrychowicz</keyname><forenames>Marcin</forenames></author><author><keyname>Dziembowski</keyname><forenames>Stefan</forenames></author><author><keyname>Malinowski</keyname><forenames>Daniel</forenames></author><author><keyname>Mazurek</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Modeling Bitcoin Contracts by Timed Automata</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a peer-to-peer cryptographic currency system. Since its
introduction in 2008, Bitcoin has gained noticeable popularity, mostly due to
its following properties: (1) the transaction fees are very low, and (2) it is
not controlled by any central authority, which in particular means that nobody
can &quot;print&quot; the money to generate inflation. Moreover, the transaction syntax
allows to create the so-called contracts, where a number of
mutually-distrusting parties engage in a protocol to jointly perform some
financial task, and the fairness of this process is guaranteed by the
properties of Bitcoin. Although the Bitcoin contracts have several potential
applications in the digital economy, so far they have not been widely used in
real life. This is partly due to the fact that they are cumbersome to create
and analyze, and hence risky to use.
  In this paper we propose to remedy this problem by using the methods
originally developed for the computer-aided analysis for hardware and software
systems, in particular those based on the timed automata. More concretely, we
propose a framework for modeling the Bitcoin contracts using the timed automata
in the UPPAAL model checker. Our method is general and can be used to model
several contracts. As a proof-of-concept we use this framework to model some of
the Bitcoin contracts from our recent previous work. We then automatically
verify their security in UPPAAL, finding (and correcting) some subtle errors
that were difficult to spot by the manual analysis. We hope that our work can
draw the attention of the researchers working on formal modeling to the problem
of the Bitcoin contract verification, and spark off more research on this
topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1864</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1864</id><created>2014-05-08</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Dialogues for proof search</title><categories>math.LO cs.AI</categories><comments>Submitted to ARQNL (Automated Reasoning in Quantified Non-Classical
  Logics)</comments><msc-class>68T15</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialogue games are a two-player semantics for a variety of logics, including
intuitionistic and classical logic. Dialogues can be viewed as a kind of
analytic calculus not unlike tableaux. Can dialogue games be an effective
foundation for proof search in intuitionistic logic (both first-order and
propositional)? We announce Kuno, an automated theorem prover for
intuitionistic first-order logic based on dialogue games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1873</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1873</id><created>2014-05-08</created><authors><author><keyname>Koteska</keyname><forenames>Bojana</forenames></author><author><keyname>Misev</keyname><forenames>Anastas</forenames></author></authors><title>Change Management and Version Control of Scientific Applications</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 2, April 2014</journal-ref><doi>10.5121/ijcsit.2014.6211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development process of scientific applications is largely dependent on
scientific progress and the experimental research results. Thus, dealing with
frequent changes is one of the main problems faced by the developers of
scientific software. Taking into account the results of the survey conducted
among scientists in the HP-SEE project, the implementation of change management
and version control software processes is inevitable. In this paper, we propose
software engineering principles that should be included in the development
process to improve the version control and change management. Moreover, we give
some specific recommendations for their implementation, thereby making a slight
modification of already generally accepted templates and methods. The
development steps practiced by scientists should not be replaced completely,
but they need to be supplemented with appropriate practices, documents and
formal methods. We also emphasize the reasons for the inclusion of these two
processes and the consequences that may arise as a result of their
non-application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1879</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1879</id><created>2014-05-08</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author><author><keyname>Fridman</keyname><forenames>E.</forenames></author></authors><title>A Round-Robin Type Protocol for Distributed Estimation with $H_\infty$
  Consensus</title><categories>cs.SY</categories><comments>Accepted for publication in Systems &amp; Control Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers a distributed robust estimation problem over a network
with directed topology involving continuous time observers. While measurements
are available to the observers continuously, the nodes interact according to a
Round-Robin rule, at discrete time instances. The results of the paper are
sufficient conditions which guarantee a suboptimal $H_\infty$ level of
consensus between observers with sampled interconnections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1885</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1885</id><created>2014-05-08</created><updated>2014-05-25</updated><authors><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Optimal Ferrers Diagram Rank-Metric Codes</title><categories>cs.IT math.IT</categories><comments>to be presented in Algebra, Codes, and Networks, Bordeaux, June 16 -
  20, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal rank-metric codes in Ferrers diagrams are considered. Such codes
consist of matrices having zeros at certain fixed positions and can be used to
construct good codes in the projective space. Four techniques and constructions
of Ferrers diagram rank-metric codes are presented, each providing optimal
codes for different diagrams and parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1891</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1891</id><created>2014-05-08</created><updated>2015-06-17</updated><authors><author><keyname>Naveed</keyname><forenames>Muhammad</forenames></author><author><keyname>Ayday</keyname><forenames>Erman</forenames></author><author><keyname>Clayton</keyname><forenames>Ellen W.</forenames></author><author><keyname>Fellay</keyname><forenames>Jacques</forenames></author><author><keyname>Gunter</keyname><forenames>Carl A.</forenames></author><author><keyname>Hubaux</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Malin</keyname><forenames>Bradley A.</forenames></author><author><keyname>Wang</keyname><forenames>XiaoFeng</forenames></author></authors><title>Privacy in the Genomic Era</title><categories>cs.CR</categories><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genome sequencing technology has advanced at a rapid pace and it is now
possible to generate highly-detailed genotypes inexpensively. The collection
and analysis of such data has the potential to support various applications,
including personalized medical services. While the benefits of the genomics
revolution are trumpeted by the biomedical community, the increased
availability of such data has major implications for personal privacy; notably
because the genome has certain essential features, which include (but are not
limited to) (i) an association with traits and certain diseases, (ii)
identification capability (e.g., forensics), and (iii) revelation of family
relationships. Moreover, direct-to-consumer DNA testing increases the
likelihood that genome data will be made available in less regulated
environments, such as the Internet and for-profit companies. The problem of
genome data privacy thus resides at the crossroads of computer science,
medicine, and public policy. While the computer scientists have addressed data
privacy for various data types, there has been less attention dedicated to
genomic data. Thus, the goal of this paper is to provide a systematization of
knowledge for the computer science community. In doing so, we address some of
the (sometimes erroneous) beliefs of this field and we report on a survey we
conducted about genome data privacy with biomedical specialists. Then, after
characterizing the genome privacy problem, we review the state-of-the-art
regarding privacy attacks on genomic data and strategies for mitigating such
attacks, as well as contextualizing these attacks from the perspective of
medicine and public policy. This paper concludes with an enumeration of the
challenges for genome data privacy and presents a framework to systematize the
analysis of threats and the design of countermeasures as the field moves
forward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1893</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1893</id><created>2014-05-08</created><updated>2014-07-17</updated><authors><author><keyname>Ban</keyname><forenames>Kristina</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author></authors><title>Initial Comparison of Linguistic Networks Measures for Parallel Texts</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>In proceeding of: 5th International Conference on Information
  Technologies and Information Society -ITIS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents preliminary results of Croatian syllable networks
analysis. Syllable network is a network in which nodes are syllables and links
between them are constructed according to their connections within words. In
this paper we analyze networks of syllables generated from texts collected from
the Croatian Wikipedia and Blogs. As a main tool we use complex network
analysis methods which provide mechanisms that can reveal new patterns in a
language structure. We aim to show that syllable networks have much higher
clustering coefficient in comparison to Erd\&quot;os-Renyi random networks. The
results indicate that Croatian syllable networks exhibit certain properties of
a small world networks. Furthermore, we compared Croatian syllable networks
with Portuguese and Chinese syllable networks and we showed that they have
similar properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1894</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1894</id><created>2014-05-08</created><authors><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Kusters</keyname><forenames>Vincent</forenames></author><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Halving Balls in Deterministic Linear Time</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\D$ be a set of $n$ pairwise disjoint unit balls in $\R^d$ and $P$ the
set of their center points. A hyperplane $\Hy$ is an \emph{$m$-separator} for
$\D$ if each closed halfspace bounded by $\Hy$ contains at least $m$ points
from $P$. This generalizes the notion of halving hyperplanes, which correspond
to $n/2$-separators. The analogous notion for point sets has been well studied.
Separators have various applications, for instance, in divide-and-conquer
schemes. In such a scheme any ball that is intersected by the separating
hyperplane may still interact with both sides of the partition. Therefore it is
desirable that the separating hyperplane intersects a small number of balls
only. We present three deterministic algorithms to bisect or approximately
bisect a given set of disjoint unit balls by a hyperplane: Firstly, we present
a simple linear-time algorithm to construct an $\alpha n$-separator for balls
in $\R^d$, for any $0&lt;\alpha&lt;1/2$, that intersects at most $cn^{(d-1)/d}$
balls, for some constant $c$ that depends on $d$ and $\alpha$. The number of
intersected balls is best possible up to the constant $c$. Secondly, we present
a near-linear time algorithm to construct an $(n/2-o(n))$-separator in $\R^d$
that intersects $o(n)$ balls. Finally, we give a linear-time algorithm to
construct a halving line in $\R^2$ that intersects $O(n^{(5/6)+\epsilon})$
disks.
  Our results improve the runtime of a disk sliding algorithm by Bereg,
Dumitrescu and Pach. In addition, our results improve and derandomize an
algorithm to construct a space decomposition used by L{\&quot;o}ffler and Mulzer to
construct an onion (convex layer) decomposition for imprecise points (any point
resides at an unknown location within a given disk).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1902</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1902</id><created>2014-05-08</created><authors><author><keyname>Schulz</keyname><forenames>Christian</forenames></author><author><keyname>von Tycowicz</keyname><forenames>Christoph</forenames></author><author><keyname>Seidel</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Hildebrandt</keyname><forenames>Klaus</forenames></author></authors><title>Proofs of two Theorems concerning Sparse Spacetime Constraints</title><categories>cs.GR math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the SIGGRAPH 2014 paper [SvTSH14] an approach for animating deformable
objects using sparse spacetime constraints is introduced. This report contains
the proofs of two theorems presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1905</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1905</id><created>2014-05-08</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Yang</keyname><forenames>Hui</forenames></author><author><keyname>Do</keyname><forenames>Younghae</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author><author><keyname>Lee</keyname><forenames>GyuWon</forenames></author></authors><title>Asymmetrically interacting spreading dynamics on complex layered
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>29 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of disease through a physical-contact network and the spread of
information about the disease on a communication network are two intimately
related dynamical processes. We investigate the asymmetrical interplay between
the two types of spreading dynamics, each occurring on its own layer, by
focusing on the two fundamental quantities underlying any spreading process:
epidemic threshold and the final infection ratio. We find that an epidemic
outbreak on the contact layer can induce an outbreak on the communication
layer, and information spreading can effectively raise the epidemic threshold.
When structural correlation exists between the two layers, the information
threshold remains unchanged but the epidemic threshold can be enhanced, making
the contact layer more resilient to epidemic outbreak. We develop a physical
theory to understand the intricate interplay between the two types of spreading
dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1906</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1906</id><created>2014-05-08</created><authors><author><keyname>Xu</keyname><forenames>Xiangru</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author></authors><title>Leader-following Consensus of Multi-agent Systems over Finite Fields</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The leader-following consensus problem of multi-agent systems over finite
fields ${\mathbb F}_p$ is considered in this paper. Dynamics of each agent is
governed by a linear equation over ${\mathbb F}_p$, where a distributed control
protocol is utilized by the followers.Sufficient and/or necessary conditions on
system matrices and graph weights in ${\mathbb F}_p$ are provided for the
followers to track the leader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1912</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1912</id><created>2014-05-08</created><authors><author><keyname>Czenky</keyname><forenames>M&#xe1;rta</forenames></author></authors><title>The Efficiency Examination of Teaching of Different Normalization
  Methods</title><categories>cs.DB</categories><acm-class>H.2.1; K.3.2</acm-class><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.6, No.2, April 2014</journal-ref><doi>10.5121/ijdms.2014.6201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normalization is an important database design method, in the course of the
teaching of data modeling the understanding and applying of this method cause
problems for students the most. For improving the efficiency of learning
normalization we looked for alternative normalization methods and introduced
them into education. We made a survey among engineer students how efficient
could they execute the normalization with different methods. We executed
statistical and data mining examinations to decide whether any of the methods
resulted significantly better solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1916</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1916</id><created>2014-05-08</created><authors><author><keyname>Kajiwara</keyname><forenames>Kazuki</forenames></author><author><keyname>Phung-Duc</keyname><forenames>Tuan</forenames></author></authors><title>Asymptotic and Numerical Analysis of Multiserver Retrial Queue with
  Guard Channel for Cellular Networks</title><categories>cs.PF</categories><journal-ref>Proceedings of the Eight International Conference on
  Matrix-Analytic Methods in Stochastic Models (MAM8), NIT Calicut, Kerala,
  India, pp. 85--102, January 06--10, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a retrial queueing model for a base station in cellular
networks where fresh calls and handover calls are available. Fresh calls are
initiated from the cell of the base station. On the other hand, a handover call
has been connecting to a base station and moves to another one. In order to
keep the continuation of the communication, it is desired that an available
channel in the new base station is immediately assigned to the handover call.
To this end, a channel is reserved as the guard channel for handover calls in
base stations. Blocked fresh and handover calls join a virtual orbit and repeat
their attempts in a later time. We assume that a base station can recognize
retrial calls and give them the same priority as that of handover calls. We
model a base station by a multiserver retrial queue with priority customers for
which a level-dependent QBD process is formulated. We obtain Taylor series
expansion for the nonzero elements of the rate matrices of the level-dependent
QBD. Using the expansion results, we obtain an asymptotic upper bound for the
joint stationary distribution of the number of busy channels and that of
customers in the orbit. Furthermore, we derive an efficient numerical algorithm
to calculate the joint stationary distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1924</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1924</id><created>2014-05-08</created><authors><author><keyname>Hanane</keyname><forenames>Tebbi</forenames></author><author><keyname>Hamid</keyname><forenames>Azzoune</forenames></author></authors><title>An Expert System for Automatic Reading of A Text Written in Standard
  Arabic</title><categories>cs.CL</categories><journal-ref>International Journal on Natural Language Computing (IJNLC) Vol.
  3, No.2, April 2014</journal-ref><doi>10.5121/ijnlc.2014.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present our expert system of Automatic reading or speech
synthesis based on a text written in Standard Arabic, our work is carried out
in two great stages: the creation of the sound data base, and the
transformation of the written text into speech (Text To Speech TTS). This
transformation is done firstly by a Phonetic Orthographical Transcription (POT)
of any written Standard Arabic text with the aim of transforming it into his
corresponding phonetics sequence, and secondly by the generation of the voice
signal which corresponds to the chain transcribed. We spread out the different
of conception of the system, as well as the results obtained compared to others
works studied to realize TTS based on Standard Arabic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1932</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1932</id><created>2014-05-08</created><authors><author><keyname>Ghaffari</keyname><forenames>Kimia</forenames></author><author><keyname>Delgosha</keyname><forenames>Mohammad Soltani</forenames></author><author><keyname>Abdolvand</keyname><forenames>Neda</forenames></author></authors><title>Towards Cloud Computing: A SWOT Analysis on its Adoption in SMEs</title><categories>cs.DC</categories><doi>10.5121/ijitcs.2014.4202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, emergence of cloud computing has notably made an
evolution in the IT industry by putting forward an &quot;everything as a service&quot;
idea .Cloud Computing is of growing interest to companies throughout the world,
but there are many barriers associated with its adoption which should be
eliminated. This paper aims to investigate Cloud Computing and discusses the
drivers and inhibitors of its adoption. Moreover, an attempt has been made to
identify the key stakeholders of Cloud Computing and outline the current
security challenges. A SWOT analysis which consists of strengths, weaknesses,
opportunities and threats has also carried out in which Cloud Computing
adoption for SMEs (Small and Medium-sized Enterprises) is evaluated. Finally,
the paper concludes with some further research areas in the field of Cloud
Computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1958</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1958</id><created>2014-05-08</created><updated>2015-08-21</updated><authors><author><keyname>Hassan</keyname><forenames>Mohamed</forenames></author></authors><title>A Self-Adaptive Network Protection System</title><categories>cs.NE cs.AI cs.CR</categories><comments>91. arXiv admin note: text overlap with arXiv:1204.1336 by other
  authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this treatise we aim to build a hybrid network automated (self-adaptive)
security threats discovery and prevention system; by using unconventional
techniques and methods, including fuzzy logic and biological inspired
algorithms under the context of soft computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1963</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1963</id><created>2014-05-03</created><updated>2014-07-22</updated><authors><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Dong</keyname><forenames>Mianxiong</forenames></author><author><keyname>Ota</keyname><forenames>Kaoru</forenames></author><author><keyname>Wu</keyname><forenames>Jun</forenames></author><author><keyname>Sato</keyname><forenames>Takuro</forenames></author></authors><title>Distributed Interference-Aware Energy-Efficient Resource Allocation for
  Device-to-Device Communications Underlaying Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 3 figures, IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of device-to-device (D2D) into cellular networks poses many
new challenges in the resource allocation design due to the co-channel
interference caused by spectrum reuse and limited battery life of user
equipments (UEs). In this paper, we propose a distributed interference-aware
energy-efficient resource allocation algorithm to maximize each UE's energy
efficiency (EE) subject to its specific quality of service (QoS) and maximum
transmission power constraints. We model the resource allocation problem as a
noncooperative game, in which each player is self-interested and wants to
maximize its own EE. The formulated EE maximization problem is a non-convex
problem and is transformed into a convex optimization problem by exploiting the
properties of the nonlinear fractional programming. An iterative optimization
algorithm is proposed and verified through computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1964</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1964</id><created>2014-05-08</created><authors><author><keyname>Barbato</keyname><forenames>Antimo</forenames></author><author><keyname>Capone</keyname><forenames>Antonio</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Martignon</keyname><forenames>Fabio</forenames></author><author><keyname>Paris</keyname><forenames>Stefano</forenames></author></authors><title>A Distributed Demand-Side Management Framework for the Smart Grid</title><categories>cs.GT cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a fully distributed Demand-Side Management system for
Smart Grid infrastructures, especially tailored to reduce the peak demand of
residential users. In particular, we use a dynamic pricing strategy, where
energy tariffs are function of the overall power demand of customers. We
consider two practical cases: (1) a fully distributed approach, where each
appliance decides autonomously its own scheduling, and (2) a hybrid approach,
where each user must schedule all his appliances. We analyze numerically these
two approaches, showing that they are characterized practically by the same
performance level in all the considered grid scenarios. We model the proposed
system using a non-cooperative game theoretical approach, and demonstrate that
our game is a generalized ordinal potential one under general conditions.
Furthermore, we propose a simple yet effective best response strategy that is
proved to converge in a few steps to a pure Nash Equilibrium, thus
demonstrating the robustness of the power scheduling plan obtained without any
central coordination of the operator or the customers. Numerical results,
obtained using real load profiles and appliance models, show that the
system-wide peak absorption achieved in a completely distributed fashion can be
reduced up to 55%, thus decreasing the capital expenditure (CAPEX) necessary to
meet the growing energy demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1965</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1965</id><created>2014-04-16</created><authors><author><keyname>Sinha</keyname><forenames>Ayushi</forenames><affiliation>Department of Computer Science, The Johns Hopkins University, Baltimore, MD</affiliation></author><author><keyname>Roncal</keyname><forenames>William Gray</forenames><affiliation>Department of Computer Science, The Johns Hopkins University, Baltimore, MD</affiliation><affiliation>The Johns Hopkins University Applied Physics Laboratory, Laurel, MD</affiliation></author><author><keyname>Kasthuri</keyname><forenames>Narayanan</forenames><affiliation>Department of Molecular and Cellular Biology, Harvard University, Cambridge, MA</affiliation><affiliation>Center for Brain Science, Harvard University, Cambridge, MA</affiliation></author><author><keyname>Lichtman</keyname><forenames>Jeff W.</forenames><affiliation>Department of Molecular and Cellular Biology, Harvard University, Cambridge, MA</affiliation><affiliation>Center for Brain Science, Harvard University, Cambridge, MA</affiliation></author><author><keyname>Burns</keyname><forenames>Randal</forenames><affiliation>Department of Computer Science, The Johns Hopkins University, Baltimore, MD</affiliation></author><author><keyname>Kazhdan</keyname><forenames>Michael</forenames><affiliation>Department of Computer Science, The Johns Hopkins University, Baltimore, MD</affiliation></author></authors><title>Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes
  using High-Resolution Neural EM Data</title><categories>cs.CV</categories><comments>2 pages, 1 figure; The 3rd Annual Hopkins Imaging Conference, The
  Johns Hopkins University, Baltimore, MD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurately estimating the wiring diagram of a brain, known as a connectome,
at an ultrastructure level is an open research problem. Specifically, precisely
tracking neural processes is difficult, especially across many image slices.
Here, we propose a novel method to automatically identify and annotate small
subcellular structures present in axons, known as axoplasmic reticula, through
a 3D volume of high-resolution neural electron microscopy data. Our method
produces high precision annotations, which can help improve automatic
segmentation by using our results as seeds for segmentation, and as cues to aid
segment merging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1966</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1966</id><created>2014-04-15</created><authors><author><keyname>Rajalakshmi</keyname><forenames>M.</forenames></author><author><keyname>Subashini</keyname><forenames>Dr. P.</forenames></author></authors><title>Texture Based Image Segmentation of Chili Pepper X-Ray Images Using
  Gabor Filter</title><categories>cs.CV cs.LG</categories><comments>7 pages, 2 figures, 8 tables</comments><journal-ref>IJASCSE, Volume 3, Issue 3, 2014, pg 44-51</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture segmentation is the process of partitioning an image into regions
with different textures containing a similar group of pixels. Detecting the
discontinuity of the filter's output and their statistical properties help in
segmenting and classifying a given image with different texture regions. In
this proposed paper, chili x-ray image texture segmentation is performed by
using Gabor filter. The texture segmented result obtained from Gabor filter fed
into three texture filters, namely Entropy, Standard Deviation and Range
filter. After performing texture analysis, features can be extracted by using
Statistical methods. In this paper Gray Level Co-occurrence Matrices and First
order statistics are used as feature extraction methods. Features extracted
from statistical methods are given to Support Vector Machine (SVM) classifier.
Using this methodology, it is found that texture segmentation is followed by
the Gray Level Co-occurrence Matrix feature extraction method gives a higher
accuracy rate of 84% when compared with First order feature extraction method.
  Key Words: Texture segmentation, Texture filter, Gabor filter, Feature
extraction methods, SVM classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1967</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1967</id><created>2014-05-08</created><authors><author><keyname>Khairnar</keyname><forenames>Prajakta P.</forenames></author><author><keyname>Manjare</keyname><forenames>C. A.</forenames></author></authors><title>Image Resolution and Contrast Enhancement of Satellite Geographical
  Images with Removal of Noise using Wavelet Transforms</title><categories>cs.CV</categories><comments>5 pages, 10 figures</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT),Volume 10, Number 12,Apr-2014 International Conference of Recent
  Trends in Engineering and Technology (ICRTET-2014),paper code 223</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the technique for resolution and contrast enhancement of
satellite geographical images based on discrete wavelet transform (DWT),
stationary wavelet transform (SWT) and singular value decomposition (SVD) has
been proposed. In this, the noise is added in the input low resolution and low
contrast image. The median filter is used remove noise from the input image.
This low resolution, low contrast image without noise is decomposed into four
sub-bands by using DWT and SWT. The resolution enhancement technique is based
on the interpolation of high frequency components obtained by DWT and input
image. SWT is used to enhance input image. DWT is used to decompose an image
into four frequency sub bands and these four sub-bands are interpolated using
bicubic interpolation technique. All these sub-bands are reconstructed as high
resolution image by using inverse DWT (IDWT). To increase the contrast the
proposed technique uses DWT and SVD. GHE is used to equalize an image. The
equalized image is decomposed into four sub-bands using DWT and new LL sub-band
is reconstructed using SVD. All sub-bands are reconstructed using IDWT to
generate high resolution and contrast image over conventional techniques. The
experimental result shows superiority of the proposed technique over
conventional techniques.
  Key words: Discrete wavelet transform (DWT), General histogram equalization
(GHE), Median filter, Singular value decomposition (SVD), Stationary wavelet
transform (SWT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1980</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1980</id><created>2014-04-16</created><authors><author><keyname>Consoli</keyname><forenames>Sergio</forenames></author><author><keyname>Moreno-Perez</keyname><forenames>Jose Andres</forenames></author><author><keyname>Darby-Dowman</keyname><forenames>Kenneth</forenames></author><author><keyname>Mladenovic</keyname><forenames>Nenad</forenames></author></authors><title>Mejora de la exploracion y la explotacion de las heuristicas
  constructivas para el MLSTP</title><categories>cs.DM math.CO</categories><comments>9 pages, in Spanish. Quinto Congreso Espanol de Metaheuristicas,
  Algoritmos Evolutivos y Bioinspirados (MAEB 2007), Tenerife, Spain, available
  at: http://www.redheur.org/files/MAEBs/MAEB07.pdf; Proceedings of the Quinto
  Congreso Espanol de Metaheuristicas, Algoritmos Evolutivos y Bioinspirados,
  2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies constructive heuristics for the minimum labelling spanning
tree (MLST) problem. The purpose is to find a spanning tree that uses edges
that are as similar as possible. Given an undirected labeled connected graph
(i.e., with a label or color for each edge), the minimum labeling spanning tree
problem seeks a spanning tree whose edges have the smallest possible number of
distinct labels. The model can represent many real-world problems in
telecommunication networks, electric networks, and multimodal transportation
networks, among others, and the problem has been shown to be NP-complete even
for complete graphs. A primary heuristic, named the maximum vertex covering
algorithm has been proposed. Several versions of this constructive heuristic
have been proposed to improve its efficiency. Here we describe the problem,
review the literature and compare some variants of this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1992</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1992</id><created>2014-05-08</created><authors><author><keyname>Francisco</keyname><forenames>John</forenames></author><author><keyname>Sadikov</keyname><forenames>Victor</forenames></author></authors><title>Structured Approach to Web Development</title><categories>cs.SE cs.PL</categories><comments>eight pages with example code</comments><acm-class>I.7.2; D.2.11; H.5.4; D.2.13</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world of Web application development, programmers are commonly
called upon to use the Hypertext Markup Language (HTML) as a programming
language, something for which it was never intended and for which it is
woefully inadequate. HTML is a data language, nothing more. It lacks high level
programming constructions like procedures, conditions, and loops. Moreover it
provides no intrinsic mechanism to insert or associate dynamic application
data. Lastly, despite the visibly apparent structure of a web page when viewed
in a browser, the responsible HTML code bears little to no discernible
corresponding structure, making it very difficult to read, augment, and
maintain.
  This paper examines the various drawbacks inherent in HTML when used in Web
development and examines the various augmenting technologies available in the
industry today and their drawbacks. It then proposes an alternative, complete
with the necessary constructs, structure, and data associating facilities based
upon server-side, Extensible Stylesheet Language Transforms (XSLT). This
alternative approach gives rise to an entirely new, higher level, markup
language that can be readily used in web development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1993</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1993</id><created>2014-05-08</created><authors><author><keyname>Mohamed</keyname><forenames>Mbida</forenames></author><author><keyname>Abdellah</keyname><forenames>Ezzati</forenames></author></authors><title>OSCMAC_Duty_Cycle_with_Multi_Helpers_CT_Mode_WILEM_Technology_in_Wireless_Sensor_Networks</title><categories>cs.NI</categories><comments>7 pages , 4 figures, International Journal of wireless and Mobile
  Networks IJWMN ( 2014 )</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Wireless Sensor Networks (WSNs) grow to be one of the dominant
technology trends; new needs are continuously emerging and demanding more
complex constraints in a duty cycle, such as extend the life time communication
. The MAC layer plays a crucial role in these networks; it controls the
communication module and manages the medium sharing. In this work we use
OSC-MAC tackles combining with the performance of cooperative transmission (CT)
in multi-hop WSN and the Wi-Lem technology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.1999</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.1999</id><created>2014-03-21</created><authors><author><keyname>Sethares</keyname><forenames>William A.</forenames></author><author><keyname>Bay&#x131;n</keyname><forenames>Sel&#xe7;uk &#x15e;.</forenames></author></authors><title>Model-Driven Applications of Fractional Derivatives and Integrals</title><categories>cs.CV</categories><comments>22 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional order derivatives and integrals (differintegrals) are viewed from
a frequency-domain perspective using the formalism of Riesz, providing a
computational tool as well as a way to interpret the operations in the
frequency domain. Differintegrals provide a logical extension of current
techniques, generalizing the notion of integral and differential operators and
acting as kind of frequency-domain filtering that has many of the advantages of
a nonlocal linear operator. Several important properties of differintegrals are
presented, and sample applications are given to one- and two-dimensional
signals. Computer code to carry out the computations is made available on the
author's website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2000</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2000</id><created>2014-05-08</created><authors><author><keyname>Abdelnasser</keyname><forenames>Amr</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Tier-Aware Resource Allocation in OFDMA Macrocell-Small Cell Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a joint sub-channel and power allocation framework for downlink
transmission an orthogonal frequency-division multiple access (OFDMA)-based
cellular network composed of a macrocell overlaid by small cells. In this
framework, the resource allocation (RA) problems for both the macrocell and
small cells are formulated as optimization problems. Numerical results confirm
the performance gains of our proposed RA formulation for the macrocell over the
traditional resource allocation based on minimizing the transmission power.
Besides, it is shown that the formulation based on convex relaxation yields a
similar behavior to the MINLP formulation. Also, the distributed solution
converges to the same solution obtained by solving the corresponding convex
optimization problem in a centralized fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2011</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2011</id><created>2014-05-08</created><authors><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Patt-Shamir</keyname><forenames>Boaz</forenames></author></authors><title>Improved Distributed Steiner Forest Construction</title><categories>cs.DC</categories><comments>title page + 11 pages + 30 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new distributed algorithms for constructing a Steiner Forest in
the CONGEST model. Our deterministic algorithm finds, for any given constant
$\epsilon&gt;0$, a $(2+\epsilon)$-approximation in
$\tilde{O}(sk+\sqrt{\min(st,n)})$ rounds, where $s$ is the shortest path
diameter, $t$ is the number of terminals, $k$ is the number of terminal
components in the input, and $n$ is the number of nodes. Our randomized
algorithm finds, with high probability, an $O(\log n)$- approximation in time
$\tilde{O}(k+\min(s,\sqrt n)+D)$, where $D$ is the unweighted diameter of the
network. We also prove a matching lower bound of
$\tilde{\Omega}(k+\min(s,\sqrt{n})+D)$ on the running time of any distributed
approximation algorithm for the Steiner Forest problem. Previous algorithms
were randomized, and obtained either an $O(\log n)$-approximation in
$\tilde{O}(sk)$ time, or an $O(1/\epsilon)$-approximation in
$\tilde{O}((\sqrt{n}+t)^{1+\epsilon}+D)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2013</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2013</id><created>2014-05-08</created><updated>2015-04-16</updated><authors><author><keyname>Sakr</keyname><forenames>Ahmed Hamdi</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Cognitive and Energy Harvesting-Based D2D Communication in Cellular
  Networks: Stochastic Geometry Modeling and Analysis</title><categories>cs.NI math.PR</categories><comments>IEEE Transactions on Communications, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While cognitive radio enables spectrum-efficient wireless communication,
radio frequency (RF) energy harvesting from ambient interference is an enabler
for energy-efficient wireless communication. In this paper, we model and
analyze cognitive and energy harvesting-based D2D communication in cellular
networks. The cognitive D2D transmitters harvest energy from ambient
interference and use one of the channels allocated to cellular users (in uplink
or downlink), which is referred to as the D2D channel, to communicate with the
corresponding receivers. We investigate two spectrum access policies for
cellular communication in the uplink or downlink, namely, random spectrum
access (RSA) policy and prioritized spectrum access (PSA) policy. In RSA, any
of the available channels including the channel used by the D2D transmitters
can be selected randomly for cellular communication, while in PSA the D2D
channel is used only when all of the other channels are occupied. A D2D
transmitter can communicate successfully with its receiver only when it
harvests enough energy to perform channel inversion toward the receiver, the
D2D channel is free, and the $\mathsf{SINR}$ at the receiver is above the
required threshold; otherwise, an outage occurs for the D2D communication. We
use tools from stochastic geometry to evaluate the performance of the proposed
communication system model with general path-loss exponent in terms of outage
probability for D2D and cellular users. We show that energy harvesting can be a
reliable alternative to power cognitive D2D transmitters while achieving
acceptable performance. Under the same $\mathsf{SINR}$ outage requirements as
for the non-cognitive case, cognitive channel access improves the outage
probability for D2D users for both the spectrum access policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2017</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2017</id><created>2014-05-08</created><updated>2014-05-11</updated><authors><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Analytical Modeling of Mode Selection and Power Control for Underlay D2D
  Communication in Cellular Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication enables the user equipments (UEs)
located in close proximity to bypass the cellular base stations (BSs) and
directly connect to each other, and thereby, offload traffic from the cellular
infrastructure. D2D communication can improve spatial frequency reuse and
energy efficiency in cellular networks. This paper presents a comprehensive and
tractable analytical framework for D2D-enabled uplink cellular networks with a
flexible mode selection scheme along with truncated channel inversion power
control. Different from the existing mode selection schemes where the decision
on mode selection is made based only on the D2D link distance (i.e., distance
between two UEs using D2D mode of communication), the proposed mode selection
scheme for a UE accounts for both the D2D link distance and cellular link
distance (i.e., distance between the UE and the BS). The developed framework is
used to analyze and understand how the underlaying D2D communication affects
the cellular network performance. Through comprehensive numerical analysis, we
investigate the expected performance gains and provide guidelines for selecting
the network parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2029</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2029</id><created>2014-04-10</created><updated>2015-11-19</updated><authors><author><keyname>Fehenberger</keyname><forenames>Tobias</forenames></author><author><keyname>Hanik</keyname><forenames>Norbert</forenames></author></authors><title>Mutual Information as a Figure of Merit for Optical Fiber Systems</title><categories>cs.IT math.IT physics.optics</categories><comments>Minor changes were made in comparison to the previous version, An
  extended version of this paper (with MI lower bounds instead of MI estimates)
  is available, see http://arxiv.org/abs/1501.01495</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced channel decoders rely on soft-decision decoder inputs for which
mutual information (MI) is the natural figure of merit. In this paper, we
analyze an optical fiber system by evaluating MI as the maximum achievable rate
of transmission of such a system. MI is estimated by means of histograms for
which the correct bin number is determined in a blind way. The MI estimate
obtained this way shows excellent accuracy in comparison with the true MI of
16-state quadrature amplitude modulation (QAM) over an additive white Gaussian
noise channel with additional phase noise, which is a simplified model of a
nonlinear optical fiber channel. We thereby justify to use the MI estimation
method to accurately estimate the MI of an optical fiber system. In the second
part of this work, a transoceanic fiber system with 6000 km of standard
single-mode fiber is simulated and its MI determined. Among rectangular QAMs,
16-QAM is found to be the optimal modulation scheme for this link as to
performance in terms of MI and requirements on components and digital signal
processing. For the reported MI of 3.1 bits/symbol, a minimum coding overhead
of 29% is required when the channel memory is not taken into account. By
employing ideal single-channel digital back-propagation, an increase in MI by
0.25 bits/symbol and 0.28 bits/symbol is reported for 16-QAM and 64-QAM,
respectively, lowering the required overhead to 19% and 16%. When the channel
spacing is decreased to be close to the Nyquist rate, the dual-polarization
spectral efficiency is 5.7 bits/s/Hz, an increase of more than 2 bits/symbol
compared to a 50 GHz spacing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2031</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2031</id><created>2014-05-08</created><authors><author><keyname>Bochra</keyname><forenames>Rahali</forenames></author><author><keyname>Mohammed</keyname><forenames>Feham</forenames></author><author><keyname>Tao</keyname><forenames>Junwu</forenames></author></authors><title>Analysis of S Band Substrate Integrated Waveguide Power Divider,
  Circulator and Coupler</title><categories>cs.ET</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.2888 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Substrate Integrated Waveguide (SIW) technology is a very promising
technique with which we can take the advantages of both waveguides and planar
transmission lines. Therefore, in [2.1-3] GHz band various microwave components
and devices have been designed successfully using Ansoft HFSS software. We then
proceeded to the realization of the coupler and then made measurements of the
frequency response in the range [2.1-3] GHz using a network analyzer. Thus,
results of this modeling are presented, discussed and allow to integrate these
devices in planar circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2034</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2034</id><created>2014-04-11</created><updated>2014-06-26</updated><authors><author><keyname>Chen</keyname><forenames>Hsien-Pu</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author><author><keyname>Schmera</keyname><forenames>Gabor</forenames></author></authors><title>On the &quot;cracking&quot; scheme in the paper &quot;A directional coupler attack
  against the Kish key distribution system&quot; by Gunn, Allison and Abbott</title><categories>cs.CR cs.ET</categories><comments>This version is accepted for publication in Metrology and Measurement
  Systems</comments><journal-ref>Metrology and Measurement Systems 21 (2014) 389-400</journal-ref><doi>10.2478/mms-2014-0033</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recently, Gunn, Allison and Abbott (GAA)
[http://arxiv.org/pdf/1402.2709v2.pdf] proposed a new scheme to utilize
electromagnetic waves for eavesdropping on the Kirchhoff-law-Johnson-noise
(KLJN) secure key distribution. We proved in a former paper [Fluct. Noise Lett.
13 (2014) 1450016] that GAA's mathematical model is unphysical. Here we analyze
GAA's cracking scheme and show that, in the case of a loss-free cable, it
provides less eavesdropping information than in the earlier
(Bergou)-Scheuer-Yariv mean-square-based attack [Kish LB, Scheuer J, Phys.
Lett. A 374 (2010) 2140-2142], while it offers no information in the case of a
lossy cable. We also investigate GAA's claim to be experimentally capable of
distinguishing - using statistics over a few correlation times only - the
distributions of two Gaussian noises with a relative variance difference of
less than 10^-8. Normally such distinctions would require hundreds of millions
of correlations times to be observable. We identify several potential
experimental artifacts as results of poor KLJN design, which can lead to GAA's
assertions: deterministic currents due to spurious harmonic components caused
by ground loops, DC offset, aliasing, non-Gaussian features including
non-linearities and other non-idealities in generators, and the time-derivative
nature of GAA's scheme which tends to enhance all of these artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2048</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2048</id><created>2014-05-07</created><authors><author><keyname>Sukharev</keyname><forenames>Jeffrey</forenames></author><author><keyname>Zhukov</keyname><forenames>Leonid</forenames></author><author><keyname>Popescul</keyname><forenames>Alexandrin</forenames></author></authors><title>Learning Alternative Name Spellings</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Name matching is a key component of systems for entity resolution or record
linkage. Alternative spellings of the same names are a com- mon occurrence in
many applications. We use the largest collection of genealogy person records in
the world together with user search query logs to build name matching models.
The procedure for building a crowd-sourced training set is outlined together
with the presentation of our method. We cast the problem of learning
alternative spellings as a machine translation problem at the character level.
We use in- formation retrieval evaluation methodology to show that this method
substantially outperforms on our data a number of standard well known phonetic
and string similarity methods in terms of precision and re- call. Additionally,
we rigorously compare the performance of standard methods when compared with
each other. Our result can lead to a significant practical impact in entity
resolution applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2049</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2049</id><created>2014-05-08</created><authors><author><keyname>Rao</keyname><forenames>K. Sankeerth</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>A New Upperbound for the Oblivious Transfer Capacity of Discrete
  Memoryless Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>7 pages, 3 figures, extended version of submission to IEEE
  Information Theory Workshop, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a new upper bound on the string oblivious transfer capacity of
discrete memoryless channels. The main tool we use is the tension region of a
pair of random variables introduced in Prabhakaran and Prabhakaran (2014) where
it was used to derive upper bounds on rates of secure sampling in the source
model. In this paper, we consider secure computation of string oblivious
transfer in the channel model. Our bound is based on a monotonicity property of
the tension region in the channel model. We show that our bound strictly
improves upon the upper bound of Ahlswede and Csisz\'ar (2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2051</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2051</id><created>2014-05-07</created><authors><author><keyname>Fournier</keyname><forenames>Laurent</forenames></author></authors><title>Merchant Sharing Towards a Zero Marginal Cost Economy</title><categories>q-fin.EC cs.CE q-fin.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is the first attempt to formalize a new field of economics;
studding the Intangibles Goods available on the Internet. We are taking
advantage of the digital world's specific rules, in particular the zero
marginal cost, to propose a theory of trading &amp; sharing unified. A function
based money is created as a world-wide currency; &quot;cup&quot;. We argue that our
system discourage speculation activities while it makes easy captured taxes for
governments. The implementation removes the today's paywall on the Internet and
provides a simple-to-use, open-source, free-of-charge, highly-secure,
person-to-person, privacy-respectful, digital payment tool for citizens, using
standard smart-phones with a strong authentication. Next step will be the
propagation of the network application and we expect many shared benefits for
the whole economics development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2058</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2058</id><created>2014-05-08</created><authors><author><keyname>Saptawijaya</keyname><forenames>Ari</forenames></author><author><keyname>Pereira</keyname><forenames>Lu&#xed;s Moniz</forenames></author></authors><title>Joint Tabling of Logic Program Abductions and Updates</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP), 10
  pages plus bibliography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abductive logic programs offer a formalism to declaratively represent and
reason about problems in a variety of areas: diagnosis, decision making,
hypothetical reasoning, etc. On the other hand, logic program updates allow us
to express knowledge changes, be they internal (or self) and external (or
world) changes. Abductive logic programs and logic program updates thus
naturally coexist in problems that are susceptible to hypothetical reasoning
about change. Taking this as a motivation, in this paper we integrate abductive
logic programs and logic program updates by jointly exploiting tabling features
of logic programming. The integration is based on and benefits from the two
implementation techniques we separately devised previously, viz., tabled
abduction and incremental tabling for query-driven propagation of logic program
updates. A prototype of the integrated system is implemented in XSB Prolog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2061</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2061</id><created>2014-03-24</created><authors><author><keyname>Vajapeyam</keyname><forenames>Sriram</forenames></author></authors><title>Understanding Shannon's Entropy metric for Information</title><categories>cs.IT math.IT</categories><comments>A primer for novices, presenting an intuitive way of understanding
  the topic</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon's metric of &quot;Entropy&quot; of information is a foundational concept of
information theory. This article is a primer for novices that presents an
intuitive way of understanding, remembering, and/or reconstructing Shannon's
Entropy metric for information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2062</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2062</id><created>2014-02-24</created><authors><author><keyname>Wan</keyname><forenames>Pengfei</forenames></author><author><keyname>Cheung</keyname><forenames>Gene</forenames></author><author><keyname>Chou</keyname><forenames>Philip A.</forenames></author><author><keyname>Florencio</keyname><forenames>Dinei</forenames></author><author><keyname>Zhang</keyname><forenames>Cha</forenames></author><author><keyname>Au</keyname><forenames>Oscar C.</forenames></author></authors><title>Precision Enhancement of 3D Surfaces from Multiple Compressed Depth Maps</title><categories>cs.CV</categories><comments>This work was accepted as ongoing work paper in IEEE MMSP'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In texture-plus-depth representation of a 3D scene, depth maps from different
camera viewpoints are typically lossily compressed via the classical transform
coding / coefficient quantization paradigm. In this paper we propose to reduce
distortion of the decoded depth maps due to quantization. The key observation
is that depth maps from different viewpoints constitute multiple descriptions
(MD) of the same 3D scene. Considering the MD jointly, we perform a POCS-like
iterative procedure to project a reconstructed signal from one depth map to the
other and back, so that the converged depth maps have higher precision than the
original quantized versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2063</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2063</id><created>2014-04-17</created><authors><author><keyname>Scurlock</keyname><forenames>Bob J.</forenames></author></authors><title>Use of ARAS 360 to Facilitate Rapid Development of Articulated Total
  Body Biomechanical Physics Simulations</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of 3-dimensional environments to be used within a
biomechanical physics simulation framework, such as Articulated Total Body, can
be laborious and time intensive. This brief article demonstrates how the ARAS
360 software package can aid the user by speeding up development time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2066</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2066</id><created>2014-05-08</created><authors><author><keyname>Dallal</keyname><forenames>Jehad Al</forenames></author></authors><title>How and When to Flatten Java Classes?</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving modularity and reusability are two key objectives in
object-oriented programming. These objectives are achieved by applying several
key concepts, such as data encapsulation and inheritance. A class in an
object-oriented system is the basic unit of design. Assessing the quality of an
object-oriented class may require flattening the class and representing it as
it really is, including all accessible inherited class members. Thus, class
flattening helps in exploring the impact of inheritance on improving code
quality. This paper explains how to flatten Java classes and discusses the
relationship between class flattening and some applications of interest to
software practitioners, such as refactoring and indicating external quality
attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2088</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2088</id><created>2014-04-29</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>Electronic Health Records: Cure-all or Chronic Condition?</title><categories>cs.CY</categories><journal-ref>Global Business and Organizational Excellence, 33(4), 2014, pp.
  63-74</journal-ref><doi>10.1002/joe.21554</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computer-based information systems feature in almost every aspect of our
lives, and yet most of us receive handwritten prescriptions when we visit our
doctors and rely on paper-based medical records in our healthcare. Although
electronic health record (EHR) systems have long been promoted as a
cost-effective and efficient alternative to this situation, clear-cut evidence
of their success has not been forthcoming. An examination of some of the
underlying problems that prevent EHR systems from delivering the benefits that
their proponents tout identifies four broad objectives - reducing cost,
reducing errors, improving coordination and improving adherence to standards -
and shows that they are not always met. The three possible causes for this
failure to deliver involve problems with the codification of knowledge, group
and tacit knowledge, and coordination and communication. There is, however,
reason to be optimistic that EHR systems can fulfil a healthy part, if not all,
of their potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2092</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2092</id><created>2014-05-08</created><authors><author><keyname>Simeone</keyname><forenames>O.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Erkip</keyname><forenames>E.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>S.</forenames><affiliation>Shitz</affiliation></author></authors><title>Full-Duplex Cloud Radio Access Networks: An Information-Theoretic
  Viewpoint</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional design of cellular systems prescribes the separation of
uplink and downlink transmissions via time-division or frequency-division
duplex. Recent advances in analog and digital domain self-interference
interference cancellation challenge the need for this arrangement and open up
the possibility to operate base stations, especially low-power ones, in a
full-duplex mode. As a means to cope with the resulting downlink-to-uplink
interference among base stations, this letter investigates the impact of the
Cloud Radio Access Network (C-RAN) architecture. The analysis follows an
information theoretic approach based on the classical Wyner model. The
analytical results herein confirm the significant potential advantages of the
C-RAN architecture in the presence of full-duplex base stations, as long as
sufficient fronthaul capacity is available and appropriate mobile station
scheduling, or successive interference cancellation at the mobile stations, is
implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2102</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2102</id><created>2014-05-08</created><authors><author><keyname>Ma</keyname><forenames>Anna</forenames></author><author><keyname>Flenner</keyname><forenames>Arjuna</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>Improving Image Clustering using Sparse Text and the Wisdom of the
  Crowds</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to improve image clustering using sparse text and the
wisdom of the crowds. In particular, we present a method to fuse two different
kinds of document features, image and text features, and use a common
dictionary or &quot;wisdom of the crowds&quot; as the connection between the two
different kinds of documents. With the proposed fusion matrix, we use topic
modeling via non-negative matrix factorization to cluster documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2106</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2106</id><created>2014-05-08</created><authors><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author></authors><title>Information Theoretical Estimators Toolbox</title><categories>cs.IT math.IT math.ST stat.ME stat.TH</categories><comments>5 pages; ITE toolbox: https://bitbucket.org/szzoli/ite/</comments><msc-class>62Gxx, 94A15, 94A17, 62H20, 46E22</msc-class><acm-class>G.3; H.1.1</acm-class><journal-ref>Journal of Machine Learning Research 15:283-287, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ITE (information theoretical estimators) a free and open source,
multi-platform, Matlab/Octave toolbox that is capable of estimating many
different variants of entropy, mutual information, divergence, association
measures, cross quantities, and kernels on distributions. Thanks to its highly
modular design, ITE supports additionally (i) the combinations of the
estimation techniques, (ii) the easy construction and embedding of novel
information theoretical estimators, and (iii) their immediate application in
information theoretical optimization problems. ITE also includes a prototype
application in a central problem class of signal processing, independent
subspace analysis and its extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2113</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2113</id><created>2014-05-08</created><authors><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Krishnan</keyname><forenames>Nikhil</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Empirical Bayes and Full Bayes for Signal Estimation</title><categories>cs.IT math.IT</categories><comments>This work was presented at the Information Theory and Application
  workshop (ITA), San Diego, CA, Feb. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider signals that follow a parametric distribution where the parameter
values are unknown. To estimate such signals from noisy measurements in scalar
channels, we study the empirical performance of an empirical Bayes (EB)
approach and a full Bayes (FB) approach. We then apply EB and FB to solve
compressed sensing (CS) signal estimation problems by successively denoising a
scalar Gaussian channel within an approximate message passing (AMP) framework.
Our numerical results show that FB achieves better performance than EB in
scalar channel denoising problems when the signal dimension is small. In the CS
setting, the signal dimension must be large enough for AMP to work well; for
large signal dimensions, AMP has similar performance with FB and EB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2128</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2128</id><created>2014-05-08</created><authors><author><keyname>Cai</keyname><forenames>Xiaohao</forenames></author></authors><title>Variational Image Segmentation Model Coupled with Image Restoration
  Achievements</title><categories>cs.CV math.NA</categories><comments>23 pages</comments><msc-class>65Kxx, 65Yxx</msc-class><acm-class>G.1.0; G.1.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Image segmentation and image restoration are two important topics in image
processing with great achievements. In this paper, we propose a new multiphase
segmentation model by combining image restoration and image segmentation
models. Utilizing image restoration aspects, the proposed segmentation model
can effectively and robustly tackle high noisy images, blurry images, images
with missing pixels, and vector-valued images. In particular, one of the most
important segmentation models, the piecewise constant Mumford-Shah model, can
be extended easily in this way to segment gray and vector-valued images
corrupted for example by noise, blur or missing pixels after coupling a new
data fidelity term which comes from image restoration topics. It can be solved
efficiently using the alternating minimization algorithm, and we prove the
convergence of this algorithm with three variables under mild condition.
Experiments on many synthetic and real-world images demonstrate that our method
gives better segmentation results in comparison to others state-of-the-art
segmentation models especially for blurry images and images with missing pixels
values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2157</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2157</id><created>2014-05-09</created><authors><author><keyname>Batni</keyname><forenames>Arash</forenames><affiliation>Faculty of ECE, ShahidBeheshti University G.C., Tehran, Iran</affiliation></author><author><keyname>Safaei</keyname><forenames>Farshad</forenames><affiliation>Faculty of ECE, ShahidBeheshti University G.C., Tehran, Iran</affiliation><affiliation>Institute for Studies in Theoretical Physics and Mathematics</affiliation></author></authors><title>A New Multi-Tiered Solid State Disk Using Slc/Mlc Combined Flash Memory</title><categories>cs.OH</categories><comments>13 pages, 18 figures</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 4, No.2, April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storing digital information, ensuring the accuracy, steady and uninterrupted
access to the data are considered as fundamental challenges in enterprise-class
organizations and companies. In recent years, new types of storage systems such
as solid state disks (SSD) have been introduced. Unlike hard disks that have
mechanical structure, SSDs are based on flash memory and thus have electronic
structure. Generally a SSD consists of a number of flash memory chips, some
buffers of the volatile memory type, and an embedded microprocessor, which have
been interconnected by a port. This microprocessor run a small file system
which called flash translation layer (FTL). This software controls and
schedules buffers, data transfers and all flash memory tasks. SSDs have some
advantages over hard disks such as high speed, low energy consumption, lower
heat and noise, resistance against damage, and smaller size. Besides, some
disadvantages such as limited endurance and high price are still challenging.
In this study, the effort is to combine two common technologies - SLC and MLC
chips - used in the manufacture of SSDs in a single SSD to decrease the side
effects of current SSDs. The idea of using multi-layer SSD is regarded as an
efficient solution in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2163</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2163</id><created>2014-05-09</created><authors><author><keyname>Bashar</keyname><forenames>Farhana</forenames></author><author><keyname>Salehin</keyname><forenames>S. M. Akramus</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara D.</forenames></author></authors><title>Band Limited Signals Observed Over Finite Spatial and Temporal Windows:
  An Upper Bound to Signal Degrees of Freedom</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of degrees of freedom of signals observed within spatially diverse
broadband multipath fields is an area of ongoing investigation and has a wide
range of applications, including characterising broadband MIMO and cooperative
networks. However, a fundamental question arises: given a size limitation on
the observation region, what is the upper bound on the degrees of freedom of
signals observed within a broadband multipath field over a finite time window?
In order to address this question, we characterize the multipath field as a sum
of a finite number of orthogonal waveforms or spatial modes. We show that (i)
the &quot;effective observation time&quot; is independent of spatial modes and different
from actual observation time, (ii) in wideband transmission regimes, the
&quot;effective bandwidth&quot; is spatial mode dependent and varies from the given
frequency bandwidth. These findings clearly indicate the strong coupling
between space and time as well as space and frequency in spatially diverse
wideband multipath fields. As a result, signal degrees of freedom does not
agree with the well-established degrees of freedom result as a product of
spatial degrees of freedom and time-frequency degrees of freedom. Instead,
analogous to Shannon's communication model where signals are encoded in only
one spatial mode, the available signal degrees of freedom in spatially diverse
wideband multipath fields is the time-bandwidth product result extended from
one spatial mode to finite modes. We also show that the degrees of freedom is
affected by the acceptable signal to noise ratio (SNR) in each spatial mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2167</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2167</id><created>2014-05-09</created><authors><author><keyname>Yamamoto</keyname><forenames>Ken</forenames></author><author><keyname>Yamazaki</keyname><forenames>Yoshihiro</forenames></author></authors><title>Structure and modeling of the network of two-Chinese-character compound
  words in the Japanese language</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>15 pages, 9 figures</comments><journal-ref>Physica A 412, 84-91 (2014)</journal-ref><doi>10.1016/j.physa.2014.06.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a numerical model of the network of two-Chinese-character
compound words (two-character network, for short). In this network, a Chinese
character is a node and a two-Chinese-character compound word links two nodes.
The basic framework of the model is that an important character gets many
edges. As the importance of a character, we use the frequency of each character
appearing in publications. The direction of edge is given according to a random
number assigned to nodes. The network generated by the model is small-world and
scale-free, and reproduces statistical properties in the actual two-character
network quantitatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2168</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2168</id><created>2014-05-09</created><authors><author><keyname>Shadkam</keyname><forenames>Elham</forenames></author><author><keyname>Bijari</keyname><forenames>Mehdi</forenames></author></authors><title>Evaluation The Efficiency Of Cuckoo Optimization Algorithm</title><categories>cs.NE cs.NA math.NA</categories><comments>9 pages</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.4, No.2, April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new evolutionary algorithm, for continuous nonlinear
optimization problems, is surveyed. This method is inspired by the life of a
bird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated by
using the Rastrigin function. The problem is a non-linear continuous function
which is used for evaluating optimization algorithms. The efficiency of the COA
has been studied by obtaining optimal solution of various dimensions Rastrigin
function in this paper. The mentioned function also was solved by FA and ABC
algorithms. Comparing the results shows the COA has better performance than
other algorithms. Application of algorithm to test function has proven its
capability to deal with difficult optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2199</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2199</id><created>2014-05-09</created><authors><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author><author><keyname>Pal</keyname><forenames>Anita</forenames></author></authors><title>Scheduling algorithm to select $k$ optimal programme slots in television
  channels: A graph theoretic approach</title><categories>cs.DM</categories><comments>25 pages</comments><msc-class>05C15, 05C17, 05C85, 05C90,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, it is shown that all programmes of all television channels can
be modelled as an interval graph. The programme slots are taken as the vertices
of the graph and if the time duration of two {programme slots} have non-empty
intersection, the corresponding vertices are considered to be connected by an
edge. The number of viewers of a programme is taken as the weight of the
vertex. A set of programmes that are mutually exclusive in respect of time
scheduling is called a session. We assume that a company sets the objective of
selecting the popular programmes in $k$ parallel sessions among different
channels so as to make its commercial advertisement reach the maximum number of
viewers, that is, a company selects $k$ suitable programme slots simultaneously
for advertisement. The aim of the paper is, therefore, to {help} the companies
to select the programme slots, which are mutually exclusive with respect to the
time schedule of telecasting time, in such a way that the total number of
viewers of the selected programme in $k$ parallel slots rises to the optimum
level. It is shown that the solution of this problem is obtained by solving the
maximum weight $k$-colouring problem on an interval {graph}. An algorithm is
designed to solve this just-in-time optimization problem using $O(kMn^2)$ time,
where $n$ and $M$ represent the total number of programmes of all channels and
the upper bound of the viewers of all programmes of all channels respectively.
The problem considered in this paper is a daily life problem which is modeled
by $k$-colouring problem on interval graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2202</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2202</id><created>2014-05-09</created><updated>2014-10-10</updated><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Reference Receiver Based Digital Self-Interference Cancellation in MIMO
  Full-Duplex Transceivers</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures. To be presented in the 2014 IEEE Broadband
  Wireless Access Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and analyze a novel self-interference cancellation
structure for in-band MIMO full-duplex transceivers. The proposed structure
utilizes reference receiver chains to obtain reference signals for digital
self-interference cancellation, which means that all the transmitter-induced
nonidealities will be included in the digital cancellation signal. To the best
of our knowledge, this type of a structure has not been discussed before in the
context of full-duplex transceivers. First, we will analyze the overall
achievable performance of the proposed cancellation scheme, while also
providing some insight into the possible bottlenecks. We also provide a
detailed formulation of the actual cancellation procedure, and perform an
analysis into the effect of the received signal of interest on
self-interference coupling channel estimation. The achieved performance of the
proposed reference receiver based digital cancellation procedure is then
assessed and verified with full waveform simulations. The analysis and waveform
simulation results show that under practical transmitter RF/analog impairment
levels, the proposed reference receiver based cancellation architecture can
provide substantially better self-interference suppression than any existing
solution, despite deploying only low-complexity linear digital processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2210</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2210</id><created>2014-05-09</created><authors><author><keyname>Lewandowski</keyname><forenames>Dirk</forenames></author></authors><title>Evaluating the retrieval effectiveness of Web search engines using a
  representative query sample</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engine retrieval effectiveness studies are usually small-scale, using
only limited query samples. Furthermore, queries are selected by the
researchers. We address these issues by taking a random representative sample
of 1,000 informational and 1,000 navigational queries from a major German
search engine and comparing Google's and Bing's results based on this sample.
Jurors were found through crowdsourcing, data was collected using specialised
software, the Relevance Assessment Tool (RAT). We found that while Google
outperforms Bing in both query types, the difference in the performance for
informational queries was rather low. However, for navigational queries, Google
found the correct answer in 95.3 per cent of cases whereas Bing only found the
correct answer 76.6 per cent of the time. We conclude that search engine
performance on navigational queries is of great importance, as users in this
case can clearly identify queries that have returned correct results. So,
performance on this query type may contribute to explaining user satisfaction
with search engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2212</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2212</id><created>2014-05-09</created><authors><author><keyname>Lewandowski</keyname><forenames>Dirk</forenames></author></authors><title>Why we need an independent index of the Web</title><categories>cs.IR cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The path to greater diversity, as we have seen, cannot be achieved by merely
hoping for a new search engine nor will government support for a single
alternative achieve this goal. What is instead required is to create the
conditions that will make establishing such a search engine possible in the
first place. I describe how building and maintaining a proprietary index is the
greatest deterrent to such an undertaking. We must first overcome this
obstacle. Doing so will still not solve the problem of the lack of diversity in
the search engine marketplace. But it may establish the conditions necessary to
achieve that desired end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2216</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2216</id><created>2014-05-09</created><authors><author><keyname>khambekar</keyname><forenames>Nilesh</forenames></author><author><keyname>Spooner</keyname><forenames>Chad M.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Vipin</forenames></author></authors><title>Going Towards Discretized Spectrum Space: Quantification of Spectrum
  Consumption Spaces and a Quantified Spectrum Access Paradigm</title><categories>cs.NI</categories><comments>Under Submission to a Journal, 30 pages Uploaded May 9 2014, 5:30 AM
  PDT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum sharing approach is a paradigm shift from the conventional
static and exclusive approach to spectrum allocation. The existing
methodologies to define use of the spectrum and quantify its efficiency are
based on the static spectrum assignment paradigm and not suitable for the
dynamic spectrum sharing paradigm. There is a need to separately quantify the
spectrum consumed by the individual transmitters and receivers when multiple
heterogeneous wireless networks are sharing the spectrum in time, space, and
frequency dimensions. By discretizing the spectrum dimensions, we define a
methodology for quantifying the spectrum consumption spaces. This is an attempt
to adopt the discretized signal processing principle and apply it to spectrum
management functions that would bring in simplicity, flexibility, and precision
among other advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2220</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2220</id><created>2014-05-09</created><authors><author><keyname>Wang</keyname><forenames>Li-Xin</forenames></author></authors><title>Gaussian-Chain Filters for Heavy-Tailed Noise with Application to
  Detecting Big Buyers and Big Sellers in Stock Market</title><categories>q-fin.TR cs.CE cs.CV cs.SY q-fin.ST</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new heavy-tailed distribution --- Gaussian-Chain (GC)
distribution, which is inspirited by the hierarchical structures prevailing in
social organizations. We determine the mean, variance and kurtosis of the
Gaussian-Chain distribution to show its heavy-tailed property, and compute the
tail distribution table to give specific numbers showing how heavy is the
heavy-tails. To filter out the heavy-tailed noise, we construct two filters ---
2nd and 3rd-order GC filters --- based on the maximum likelihood principle.
Simulation results show that the GC filters perform much better than the
benchmark least-squares algorithm when the noise is heavy-tail distributed.
Using the GC filters, we propose a trading strategy, named Ride-the-Mood, to
follow the mood of the market by detecting the actions of the big buyers and
the big sellers in the market based on the noisy, heavy-tailed price data.
Application of the Ride-the-Mood strategy to five blue-chip Hong Kong stocks
over the recent two-year period from April 2, 2012 to March 31, 2014 shows that
their returns are higher than the returns of the benchmark Buy-and-Hold
strategy and the Hang Seng Index Fund.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2221</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2221</id><created>2014-05-09</created><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On Capacity of the Dirty Paper Channel with Fading Dirt in the Strong
  Fading Regime</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical writing on dirty paper capacity result establishes that full
interference pre-cancellation can be attained in Gelfand-Pinsker problem with
additive state and additive white Gaussian noise. This result holds under the
idealized assumption that perfect channel knowledge is available at both
transmitter and receiver. While channel knowledge at the receiver can be
obtained through pilot tones, transmitter channel knowledge is harder to
acquire. For this reason, we are interested in characterizing the capacity
under the more realistic assumption that only partial channel knowledge is
available at the transmitter. We study, more specifically, the dirty paper
channel in which the interference sequence in multiplied by fading value
unknown to the transmitter but known at the receiver. For this model, we
establish an approximate characterization of capacity for the case in which
fading values vary greatly in between channel realizations. In this regime,
which we term the strong fading regime, the capacity pre-log factor is equal to
the inverse of the number of possible fading realizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2226</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2226</id><created>2014-05-09</created><updated>2014-11-24</updated><authors><author><keyname>Hadjam</keyname><forenames>Fatima</forenames></author><author><keyname>Moraga</keyname><forenames>Claudio</forenames></author></authors><title>Introduction to RIMEP2: A Multi-Expression Programming System for the
  Design of Reversible Digital Circuits</title><categories>cs.ET</categories><comments>17 text pages, 8 Figures, Research Report, Contact author:
  Fatima.Hadjam@googlemail.com</comments><report-no>FSC-2014-02; European Centre for Soft Computing</report-no><acm-class>B.6.3; C.2.4; D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computers are considered as a future alternative to circumvent the
heat dissipation problem of VLSI circuits. The synthesis of reversible circuits
is a very promising area of study considering the expected further
technological advances towards quantum computing. In this report, we propose a
linear genetic programming system to design reversible circuits -RIMEP2-. The
system has evolved reversible circuits starting from scratch without resorting
to a pre-existing library. The results show that among the 26 considered
benchmarks, RIMEP2 outperformed the best published solutions for 20 of them and
matched the remaining 6. RIMEP2 is presented in this report as a promising
method with a considerable potential for reversible circuit design. It will be
considered as work reference for future studies based on this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2227</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2227</id><created>2014-05-09</created><authors><author><keyname>Chakraborty</keyname><forenames>Saptarshi</forenames></author><author><keyname>Das</keyname><forenames>Dhrubajyoti</forenames></author></authors><title>An Overview of Face Liveness Detection</title><categories>cs.CV</categories><comments>International Journal on Information Theory (IJIT), Vol.3, No.2,
  April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition is a widely used biometric approach. Face recognition
technology has developed rapidly in recent years and it is more direct, user
friendly and convenient compared to other methods. But face recognition systems
are vulnerable to spoof attacks made by non-real faces. It is an easy way to
spoof face recognition systems by facial pictures such as portrait photographs.
A secure system needs Liveness detection in order to guard against such
spoofing. In this work, face liveness detection approaches are categorized
based on the various types techniques used for liveness detection. This
categorization helps understanding different spoof attacks scenarios and their
relation to the developed solutions. A review of the latest works regarding
face liveness detection works is presented. The main aim is to provide a simple
path for the future development of novel and more secured face liveness
detection approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2234</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2234</id><created>2014-05-09</created><authors><author><keyname>Bojanczyk</keyname><forenames>Mikolaj</forenames></author><author><keyname>Dittmann</keyname><forenames>Christoph</forenames></author><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames></author></authors><title>Decomposition Theorems and Model-Checking for the Modal $\mu$-Calculus</title><categories>math.LO cs.LO</categories><acm-class>F.4.1</acm-class><doi>10.1145/2603088.2603144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a general decomposition theorem for the modal $\mu$-calculus $L_\mu$
in the spirit of Feferman and Vaught's theorem for disjoint unions. In
particular, we show that if a structure (i.e., transition system) is composed
of two substructures $M_1$ and $M_2$ plus edges from $M_1$ to $M_2$, then the
formulas true at a node in $M$ only depend on the formulas true in the
respective substructures in a sense made precise below. As a consequence we
show that the model-checking problem for $L_\mu$ is fixed-parameter tractable
(fpt) on classes of structures of bounded Kelly-width or bounded DAG-width. As
far as we are aware, these are the first fpt results for $L_\mu$ which do not
follow from embedding into monadic second-order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2246</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2246</id><created>2014-05-09</created><authors><author><keyname>Li</keyname><forenames>Le</forenames></author><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Zhao</keyname><forenames>Kaili</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author><author><keyname>Fan</keyname><forenames>Zhuoyi</forenames></author></authors><title>Graph Regularized Non-negative Matrix Factorization By Maximizing
  Correntropy</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative matrix factorization (NMF) has proved effective in many
clustering and classification tasks. The classic ways to measure the errors
between the original and the reconstructed matrix are $l_2$ distance or
Kullback-Leibler (KL) divergence. However, nonlinear cases are not properly
handled when we use these error measures. As a consequence, alternative
measures based on nonlinear kernels, such as correntropy, are proposed.
However, the current correntropy-based NMF only targets on the low-level
features without considering the intrinsic geometrical distribution of data. In
this paper, we propose a new NMF algorithm that preserves local invariance by
adding graph regularization into the process of max-correntropy-based matrix
factorization. Meanwhile, each feature can learn corresponding kernel from the
data. The experiment results of Caltech101 and Caltech256 show the benefits of
such combination against other NMF algorithms for the unsupervised image
clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2262</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2262</id><created>2014-05-09</created><authors><author><keyname>Gashler</keyname><forenames>Michael S.</forenames></author><author><keyname>Ashmore</keyname><forenames>Stephen C.</forenames></author></authors><title>Training Deep Fourier Neural Networks To Fit Time-Series Data</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for training a deep neural network containing sinusoidal
activation functions to fit to time-series data. Weights are initialized using
a fast Fourier transform, then trained with regularization to improve
generalization. A simple dynamic parameter tuning method is employed to adjust
both the learning rate and regularization term, such that stability and
efficient training are both achieved. We show how deeper layers can be utilized
to model the observed sequence using a sparser set of sinusoid units, and how
non-uniform regularization can improve generalization by promoting the shifting
of weight toward simpler units. The method is demonstrated with time-series
problems to show that it leads to effective extrapolation of nonlinear trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2271</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2271</id><created>2014-05-09</created><updated>2014-09-12</updated><authors><author><keyname>Ray</keyname><forenames>Camellia</forenames></author><author><keyname>Das</keyname><forenames>Jayanta Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author></authors><title>On Analysis and Generation of some Biologically Important Boolean
  Functions</title><categories>cs.SY cs.CE</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean networks are used to model biological networks such as gene
regulatory networks. Often Boolean networks show very chaotic behaviour which
is sensitive to any small perturbations. In order to reduce the chaotic
behaviour and to attain stability in the gene regulatory network, nested
Canalizing Functions (NCFs) are best suited. NCFs and its variants have a wide
range of applications in systems biology. Previously, many works were done on
the application of canalizing functions, but there were fewer methods to check
if any arbitrary Boolean function is canalizing or not. In this paper, by using
Karnaugh Map this problem is solved and also it has been shown that when the
canalizing functions of variable is given, all the canalizing functions of
variable could be generated by the method of concatenation. In this paper we
have uniquely identified the number of NCFs having a particular Hamming
Distance (H.D) generated by each variable as starting canalizing input.
Partially NCFs of 4 variables has also been studied in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2278</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2278</id><created>2014-05-09</created><authors><author><keyname>Lyon</keyname><forenames>R. J.</forenames></author><author><keyname>Brooke</keyname><forenames>J. M.</forenames></author><author><keyname>Knowles</keyname><forenames>J. D.</forenames></author><author><keyname>Stappers</keyname><forenames>B. W.</forenames></author></authors><title>Hellinger Distance Trees for Imbalanced Streams</title><categories>cs.LG astro-ph.IM stat.ML</categories><comments>6 Pages, 2 figures, to be published in Proceedings 22nd International
  Conference on Pattern Recognition (ICPR) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifiers trained on data sets possessing an imbalanced class distribution
are known to exhibit poor generalisation performance. This is known as the
imbalanced learning problem. The problem becomes particularly acute when we
consider incremental classifiers operating on imbalanced data streams,
especially when the learning objective is rare class identification. As
accuracy may provide a misleading impression of performance on imbalanced data,
existing stream classifiers based on accuracy can suffer poor minority class
performance on imbalanced streams, with the result being low minority class
recall rates. In this paper we address this deficiency by proposing the use of
the Hellinger distance measure, as a very fast decision tree split criterion.
We demonstrate that by using Hellinger a statistically significant improvement
in recall rates on imbalanced data streams can be achieved, with an acceptable
increase in the false positive rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2281</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2281</id><created>2014-05-08</created><authors><author><keyname>Hannig</keyname><forenames>Frank</forenames></author><author><keyname>Teich</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Proceedings of the First Workshop on Resource Awareness and Adaptivity
  in Multi-Core Computing (Racing 2014)</title><categories>cs.DC cs.OS cs.RO cs.SE</categories><comments>Website of the workshop: http://www12.cs.fau.de/racing2014/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers accepted at the 1st Workshop on Resource
Awareness and Adaptivity in Multi-Core Computing (Racing 2014), held in
Paderborn, Germany, May 29-30, 2014. Racing 2014 was co-located with the IEEE
European Test Symposium (ETS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2294</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2294</id><created>2014-04-25</created><authors><author><keyname>Zou</keyname><forenames>Shaofeng</forenames></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Shi</keyname><forenames>Xinghua</forenames></author></authors><title>Nonparametric Detection of Anomalous Data via Kernel Mean Embedding</title><categories>cs.LG stat.ML</categories><comments>Submitted to IEEE Transactions on Information Theory, March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An anomaly detection problem is investigated, in which there are totally n
sequences with s anomalous sequences to be detected. Each normal sequence
contains m independent and identically distributed (i.i.d.) samples drawn from
a distribution p, whereas each anomalous sequence contains m i.i.d. samples
drawn from a distribution q that is distinct from p. The distributions p and q
are assumed to be unknown a priori. Two scenarios, respectively with and
without a reference sequence generated by p, are studied. Distribution-free
tests are constructed using maximum mean discrepancy (MMD) as the metric, which
is based on mean embeddings of distributions into a reproducing kernel Hilbert
space (RKHS). For both scenarios, it is shown that as the number n of sequences
goes to infinity, if the value of s is known, then the number m of samples in
each sequence should be at the order O(log n) or larger in order for the
developed tests to consistently detect s anomalous sequences. If the value of s
is unknown, then m should be at the order strictly larger than O(log n).
Computational complexity of all developed tests is shown to be polynomial.
Numerical results demonstrate that our tests outperform (or perform as well as)
the tests based on other competitive traditional statistical approaches and
kernel-based approaches under various cases. Consistency of the proposed test
is also demonstrated on a real data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2295</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2295</id><created>2014-05-09</created><updated>2015-05-04</updated><authors><author><keyname>Altieri</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia G.</forenames></author></authors><title>On Fundamental Trade-offs of Device-to-Device Communications in Large
  Wireless Networks</title><categories>cs.IT math.IT</categories><comments>33 pages, 9 figures. Updated version, to appear in IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the gains, in terms of served requests, attainable through
out-of-band device-to-device (D2D) video exchanges in large cellular networks.
A stochastic framework, in which users are clustered to exchange videos, is
introduced, considering several aspects of this problem: the video-caching
policy, user matching for exchanges, aspects regarding scheduling and
transmissions. A family of \emph{admissible protocols} is introduced: in each
protocol the users are clustered by means of a hard-core point process and,
within the clusters, video exchanges take place. Two metrics, quantifying the
&quot;local&quot; and &quot;global&quot; fraction of video requests served through D2D are defined,
and relevant trade-off regions involving these metrics, as well as
quality-of-service constraints, are identified. A simple communication strategy
is proposed and analyzed, to obtain inner bounds to the trade-off regions, and
draw conclusions on the performance attainable through D2D. To this end, an
analysis of the time-varying interference that the nodes experience, and tight
approximations of its Laplace transform are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2300</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2300</id><created>2014-04-14</created><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Br&#xfc;ckner</keyname><forenames>Guido</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Complexity of Higher-Degree Orthogonal Graph Embedding in the Kandinsky
  Model</title><categories>cs.CG cs.DS</categories><comments>39 pages, 19 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that finding orthogonal grid-embeddings of plane graphs (planar with
fixed combinatorial embedding) with the minimum number of bends in the
so-called Kandinsky model (which allows vertices of degree $&gt; 4$) is
NP-complete, thus solving a long-standing open problem. On the positive side,
we give an efficient algorithm for several restricted variants, such as graphs
of bounded branch width and a subexponential exact algorithm for general plane
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2305</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2305</id><created>2014-05-06</created><authors><author><keyname>Giacomelli</keyname><forenames>Piero</forenames></author><author><keyname>Smedberg</keyname><forenames>Asa</forenames></author></authors><title>The Eve of 3D Printing in Telemedicine: State of the Art and Future
  Challenges</title><categories>cs.OH</categories><comments>5 pages submitted to The Sixth International Conference on eHealth,
  Telemedicine, and Social Medicine (eTELEMED 2014)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  3D printing has raised a lot of attention from fields outside the
manufacturing one in the last years. In this paper, we will illustrate some
recent advances of 3D printing technology, applied to the field of telemedicine
and remote patient care. The potentiality of this technology will be detailed
without lab examples. Some crucial aspect such as the regulation of these
devices and the need of some standards will also be discussed. The purpose of
this paper is to present some of the most promising applications of such
technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2316</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2316</id><created>2014-05-09</created><authors><author><keyname>Poling</keyname><forenames>Bryan</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author></authors><title>Better Feature Tracking Through Subspace Constraints</title><categories>cs.CV</categories><comments>8 pages, 2 figures. CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature tracking in video is a crucial task in computer vision. Usually, the
tracking problem is handled one feature at a time, using a single-feature
tracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives.
While this approach works quite well when dealing with high-quality video and
&quot;strong&quot; features, it often falters when faced with dark and noisy video
containing low-quality features. We present a framework for jointly tracking a
set of features, which enables sharing information between the different
features in the scene. We show that our method can be employed to track
features for both rigid and nonrigid motions (possibly of few moving bodies)
even when some features are occluded. Furthermore, it can be used to
significantly improve tracking results in poorly-lit scenes (where there is a
mix of good and bad features). Our approach does not require direct modeling of
the structure or the motion of the scene, and runs in real time on a single CPU
core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2329</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2329</id><created>2014-05-09</created><authors><author><keyname>Pimentel</keyname><forenames>Elaine</forenames></author><author><keyname>Olarte</keyname><forenames>Carlos</forenames></author><author><keyname>Nigam</keyname><forenames>Vivek</forenames></author></authors><title>A Proof Theoretic Study of Soft Concurrent Constraint Programming</title><categories>cs.LO</categories><acm-class>F.3.1; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, 14(4-5), 649-663, 2014</journal-ref><doi>10.1017/S147106841400026X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent Constraint Programming (CCP) is a simple and powerful model for
concurrency where agents interact by telling and asking constraints. Since
their inception, CCP-languages have been designed for having a strong
connection to logic. In fact, the underlying constraint system can be built
from a suitable fragment of intuitionistic (linear) logic --ILL-- and processes
can be interpreted as formulas in ILL. Constraints as ILL formulas fail to
represent accurately situations where &quot;preferences&quot; (called soft constraints)
such as probabilities, uncertainty or fuzziness are present. In order to
circumvent this problem, c-semirings have been proposed as algebraic structures
for defining constraint systems where agents are allowed to tell and ask soft
constraints. Nevertheless, in this case, the tight connection to logic and
proof theory is lost. In this work, we give a proof theoretical interpretation
to soft constraints: they can be defined as formulas in a suitable fragment of
ILL with subexponentials (SELL) where subexponentials, ordered in a c-semiring
structure, are interpreted as preferences. We hence achieve two goals: (1)
obtain a CCP language where agents can tell and ask soft constraints and (2)
prove that the language in (1) has a strong connection with logic. Hence we
keep a declarative reading of processes as formulas while providing a logical
framework for soft-CCP based systems. An interesting side effect of (1) is that
one is also able to handle probabilities (and other modalities) in SELL, by
restricting the use of the promotion rule for non-idempotent c-semirings.This
finer way of controlling subexponentials allows for considering more
interesting spaces and restrictions, and it opens the possibility of specifying
more challenging computational systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2330</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2330</id><created>2014-05-07</created><authors><author><keyname>Alam</keyname><forenames>Sawood</forenames></author><author><keyname>Cartledge</keyname><forenames>Charles L.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Support for Various HTTP Methods on the Web</title><categories>cs.NI cs.DL</categories><comments>16 pages, 6 figures, and 11 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We examine how well various HTTP methods are supported by public web
services. We sample 40,870 live URIs from the DMOZ collection (a curated
directory of World Wide Web URIs) and found that about 55% URIs claim support
(in the Allow header) for GET and POST methods, but less than 2% of the URIs
claim support for one or more of PUT, PATCH, or DELETE methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2349</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2349</id><created>2014-05-09</created><updated>2015-01-15</updated><authors><author><keyname>H&#x105;z&#x142;a</keyname><forenames>Jan</forenames></author><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author></authors><title>Upper Tail Estimates with Combinatorial Proofs</title><categories>cs.DM</categories><comments>Full version of the paper from STACS 2015</comments><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalisations of a simple, combinatorial proof of a Chernoff bound
similar to the one by Impagliazzo and Kabanets (RANDOM, 2010).
  In particular, we prove a randomized version of the hitting property of
expander random walks and apply it to obtain a concentration bound for expander
random walks which is essentially optimal for small deviations and a large
number of steps. At the same time, we present a simpler proof that still yields
a &quot;right&quot; bound settling a question asked by Impagliazzo and Kabanets.
  Next, we obtain a simple upper tail bound for polynomials with input
variables in $[0, 1]$ which are not necessarily independent, but obey a certain
condition inspired by Impagliazzo and Kabanets. The resulting bound is used by
Holenstein and Sinha (FOCS, 2012) in the proof of a lower bound for the number
of calls in a black-box construction of a pseudorandom generator from a one-way
function.
  We then show that the same technique yields the upper tail bound for the
number of copies of a fixed graph in an Erd\H{o}s-R\'enyi random graph,
matching the one given by Janson, Oleszkiewicz and Ruci\'nski (Israel J. Math,
2002).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2362</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2362</id><created>2014-05-09</created><authors><author><keyname>Fang</keyname><forenames>Yan</forenames></author><author><keyname>Cotter</keyname><forenames>Matthew J.</forenames></author><author><keyname>Chiarulli</keyname><forenames>Donald M.</forenames></author><author><keyname>Levitan</keyname><forenames>Steven P.</forenames></author></authors><title>Image Segmentation Using Frequency Locking of Coupled Oscillators</title><categories>cs.CV q-bio.NC</categories><comments>7 pages, 14 figures, the 51th Design Automation Conference 2014, Work
  in Progress Poster Session</comments><acm-class>C.1.3</acm-class><doi>10.1109/CNNA.2014.6888657</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronization of coupled oscillators is observed at multiple levels of
neural systems, and has been shown to play an important function in visual
perception. We propose a computing system based on locally coupled oscillator
networks for image segmentation. The system can serve as the preprocessing
front-end of an image processing pipeline where the common frequencies of
clusters of oscillators reflect the segmentation results. To demonstrate the
feasibility of our design, the system is simulated and tested on a human face
image dataset and its performance is compared with traditional intensity
threshold based algorithms. Our system shows both better performance and higher
noise tolerance than traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2363</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2363</id><created>2014-05-09</created><authors><author><keyname>Kaynama</keyname><forenames>Shahab</forenames></author><author><keyname>Gillula</keyname><forenames>Jeremy H.</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire J.</forenames></author></authors><title>A sampling-based approach to scalable constraint satisfaction in linear
  sampled-data systems---Part I: Computation</title><categories>cs.SY cs.RO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampled-data (SD) systems, which are composed of both discrete- and
continuous-time components, are arguably one of the most common classes of
cyberphysical systems in practice; most modern controllers are implemented on
digital platforms while the plant dynamics that are being controlled evolve
continuously in time. As with all cyberphysical systems, ensuring hard
constraint satisfaction is key in the safe operation of SD systems. A powerful
analytical tool for guaranteeing such constraint satisfaction is the viability
kernel: the set of all initial conditions for which a safety-preserving control
law (that is, a control law that satisfies all input and state constraints)
exists. In this paper we present a novel sampling-based algorithm that tightly
approximates the viability kernel for high-dimensional sampled-data linear
time-invariant (LTI) systems. Unlike prior work in this area, our algorithm
formally handles both the discrete and continuous characteristics of SD
systems. We prove the correctness and convergence of our approximation
technique, provide discussions on heuristic methods to optimally bias the
sampling process, and demonstrate the results on a twelve-dimensional flight
envelope protection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2376</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2376</id><created>2014-05-09</created><authors><author><keyname>Tschantz</keyname><forenames>Michael Carl</forenames></author><author><keyname>Datta</keyname><forenames>Amit</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Wing</keyname><forenames>Jeannette M.</forenames></author></authors><title>A Methodology for Information Flow Experiments</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information flow analysis has largely ignored the setting where the analyst
has neither control over nor a complete model of the analyzed system. We
formalize such limited information flow analyses and study an instance of it:
detecting the usage of data by websites. We prove that these problems are ones
of causal inference. Leveraging this connection, we push beyond traditional
information flow analysis to provide a systematic methodology based on
experimental science and statistical analysis. Our methodology allows us to
systematize prior works in the area viewing them as instances of a general
approach. Our systematic study leads to practical advice for improving work on
detecting data usage, a previously unformalized area. We illustrate these
concepts with a series of experiments collecting data on the use of information
by websites, which we statistically analyze.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2377</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2377</id><created>2014-05-09</created><authors><author><keyname>Brofos</keyname><forenames>James</forenames></author></authors><title>A Hybrid Monte Carlo Architecture for Parameter Optimization</title><categories>stat.ML cs.LG stat.ME</categories><comments>5 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much recent research has been conducted in the area of Bayesian learning,
particularly with regard to the optimization of hyper-parameters via Gaussian
process regression. The methodologies rely chiefly on the method of maximizing
the expected improvement of a score function with respect to adjustments in the
hyper-parameters. In this work, we present a novel algorithm that exploits
notions of confidence intervals and uncertainties to enable the discovery of
the best optimal within a targeted region of the parameter space. We
demonstrate the efficacy of our algorithm with respect to machine learning
problems and show cases where our algorithm is competitive with the method of
maximizing expected improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2378</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2378</id><created>2014-05-09</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Aloupis</keyname><forenames>Greg</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Snoeyink</keyname><forenames>Jack</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Covering Folded Shapes</title><categories>cs.CG</categories><comments>19 pages, 10 figures, to appear in Journal of Computational Geometry
  (JoCG)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can folding a piece of paper flat make it larger? We explore whether a shape
$S$ must be scaled to cover a flat-folded copy of itself. We consider both
single folds and arbitrary folds (continuous piecewise isometries $S\rightarrow
R^2$). The underlying problem is motivated by computational origami, and is
related to other covering and fixturing problems, such as Lebesgue's universal
cover problem and force closure grasps. In addition to considering special
shapes (squares, equilateral triangles, polygons and disks), we give upper and
lower bounds on scale factors for single folds of convex objects and arbitrary
folds of simply connected objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2386</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2386</id><created>2014-05-09</created><authors><author><keyname>Datta</keyname><forenames>Srayan</forenames></author></authors><title>Predicting Central Topics in a Blog Corpus from a Networks Perspective</title><categories>cs.IR cs.CL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's content-centric Internet, blogs are becoming increasingly popular
and important from a data analysis perspective. According to Wikipedia, there
were over 156 million public blogs on the Internet as of February 2011. Blogs
are a reflection of our contemporary society. The contents of different blog
posts are important from social, psychological, economical and political
perspectives. Discovery of important topics in the blogosphere is an area which
still needs much exploring. We try to come up with a procedure using
probabilistic topic modeling and network centrality measures which identifies
the central topics in a blog corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2387</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2387</id><created>2014-05-10</created><authors><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Samarasinghe</keyname><forenames>Tharaka</forenames></author><author><keyname>Evans</keyname><forenames>Jamie</forenames></author></authors><title>Transmission Rank Selection for Opportunistic Beamforming with Quality
  of Service Constraints</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE ICC 2014, Sydney, Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multi-cell multi-user MISO broadcast channel.
The system operates according to the opportunistic beamforming framework in a
multi-cell environment with variable number of transmit beams (may
alternatively be referred as the transmission rank) at each base station. The
maximum number of co-scheduled users in a cell is equal to its transmission
rank, thus increasing it will have the effect of increasing the multiplexing
gain. However, this will simultaneously increase the amount of interference in
the network, which will decrease the rate of communication. This paper focuses
on optimally setting the transmission rank at each base station such that a set
of Quality of Service (QoS) constraints, that will ensure a guaranteed minimum
rate per beam at each base station, is not violated. Expressions representing
the achievable region of transmission ranks are obtained considering different
network settings. The achievable transmission rank region consists of all
achievable transmission rank tuples that satisfy the QoS constraints. Numerical
results are also presented to provide further insights on the feasibility
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2403</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2403</id><created>2014-05-10</created><authors><author><keyname>Huck</keyname><forenames>Alexis</forenames></author><author><keyname>de Vieilleville</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Weiss</keyname><forenames>Pierre</forenames></author><author><keyname>Grizonnet</keyname><forenames>Manuel</forenames></author></authors><title>Hyperspectral pan-sharpening: a variational convex constrained
  formulation to impose parallel level lines, solved with ADMM</title><categories>cs.CV</categories><comments>4 pages, detailed version of proceedings of conference IEEE WHISPERS
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the issue of hyperspectral pan-sharpening, which
consists in fusing a (low spatial resolution) hyperspectral image HX and a
(high spatial resolution) panchromatic image P to obtain a high spatial
resolution hyperspectral image. The problem is addressed under a variational
convex constrained formulation. The objective favors high resolution spectral
bands with level lines parallel to those of the panchromatic image. This term
is balanced with a total variation term as regularizer. Fit-to-P data and
fit-to-HX data constraints are effectively considered as mathematical
constraints, which depend on the statistics of the data noise measurements. The
developed Alternating Direction Method of Multipliers (ADMM) optimization
scheme enables us to solve this problem efficiently despite the non
differentiabilities and the huge number of unknowns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2406</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2406</id><created>2014-05-10</created><authors><author><keyname>Sugimoto</keyname><forenames>Norikazu</forenames></author><author><keyname>Tangkaratt</keyname><forenames>Voot</forenames></author><author><keyname>Wensveen</keyname><forenames>Thijs</forenames></author><author><keyname>Zhao</keyname><forenames>Tingting</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Morimoto</keyname><forenames>Jun</forenames></author></authors><title>Efficient Reuse of Previous Experiences to Improve Policies in Real
  Environment</title><categories>cs.RO</categories><comments>14 pages, 9 figures, http://www.cns.atr.jp/bri/en/. arXiv admin note:
  substantial text overlap with arXiv:1301.3966</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we show that a movement policy can be improved efficiently
using the previous experiences of a real robot. Reinforcement Learning (RL) is
becoming a popular approach to acquire a nonlinear optimal policy through trial
and error. However, it is considered very difficult to apply RL to real robot
control since it usually requires many learning trials. Such trials cannot be
executed in real environments because unrealistic time is necessary and the
real system's durability is limited. Therefore, in this study, instead of
executing many learning trials, we propose to use a recently developed RL
algorithm, importance-weighted PGPE, by which the robot can efficiently reuse
previously sampled data to improve it's policy parameters. We apply
importance-weighted PGPE to CB-i, our real humanoid robot, and show that it can
learn a target reaching movement and a cart-pole swing up movement in a real
environment without using any prior knowledge of the task or any carefully
designed initial trajectory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2407</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2407</id><created>2014-05-10</created><authors><author><keyname>Speck</keyname><forenames>Reto</forenames></author><author><keyname>Blanke</keyname><forenames>Tobias</forenames></author><author><keyname>Kristel</keyname><forenames>Cony</forenames></author><author><keyname>Frankl</keyname><forenames>Michal</forenames></author><author><keyname>Rodriguez</keyname><forenames>Kepa</forenames></author><author><keyname>Daelen</keyname><forenames>Veerle Vanden</forenames></author></authors><title>The Past and the Future of Holocaust Research: From Disparate Sources to
  an Integrated European Holocaust Research Infrastructure</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The European Holocaust Research Infrastructure (EHRI) has been set up by the
European Union to create a sustainable complex of services for researchers.
EHRI will bring together information about dispersed collections, based on
currently more than 20 partner organisations in 13 countries and many other
archives. EHRI, which brings together historians, archivists and specialists in
digital humanities, strives to develop innovative on-line tools for finding,
researching and sharing knowledge about the Holocaust. While connecting
information about Holocaust collections, it strives to create tools and
approaches applicable to other digital archival projects. The paper describes
its current progress and collaboration across the disciplines involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2409</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2409</id><created>2014-05-10</created><updated>2014-05-15</updated><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Huang</keyname><forenames>Chung-Hao</forenames></author><author><keyname>Ruess</keyname><forenames>Harald</forenames></author><author><keyname>Stattelmann</keyname><forenames>Stefan</forenames></author></authors><title>G4LTL-ST: Automatic Generation of PLC Programs</title><categories>cs.LO</categories><comments>This is the full version of the CAV'14 paper. Research concepts
  developed this paper are mainly from the technical report &quot;Numerical LTL
  synthesis for cyber-physical systems&quot;, coauthored by Chih-Hong Cheng (ABB
  Research) and Edward A. Lee (UC Berkeley)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  G4LTL-ST automatically synthesizes control code for industrial Programmable
Logic Controls (PLC) from timed behavioral specifications of input-output
signals. These specifications are expressed in a linear temporal logic (LTL)
extended with non-linear arithmetic constraints and timing constraints on
signals. G4LTL-ST generates code in IEC 61131-3-compatible Structured Text,
which is compiled into executable code for a large number of industrial
field-level devices. The synthesis algorithm of G4LTL-ST implements
pseudo-Boolean abstraction of data constraints and the compilation of timing
constraints into LTL, together with a counterstrategy-guided abstraction
refinement synthesis loop. Since temporal logic specifications are notoriously
difficult to use in practice, G4LTL-ST supports engineers in specifying
realizable control problems by suggesting suitable restrictions on the behavior
of the control environment from failed synthesis attempts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2417</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2417</id><created>2014-05-10</created><authors><author><keyname>Rahman</keyname><forenames>Md Habibur</forenames></author><author><keyname>Nasiruddin</keyname><forenames>Mohammad</forenames></author></authors><title>Impact of Two Realistic Mobility Models for Vehicular Safety
  Applications</title><categories>cs.NI</categories><comments>6 pages, 9 figures, ICIEV 2014</comments><msc-class>68Uxx</msc-class><acm-class>C.2.6</acm-class><journal-ref>3rd International Conference on Informatics, Electronics &amp; Vision
  (ICIEV 2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular safety applications intended for VANETs. It can be separated by
inter-vehicle communication. It is needed for a vehicle can travel safety with
high velocity and must interconnect quickly dependably. In this work, examined
the impact of the IDM-IM and IDM-LC mobility model on AODV, AOMDV, DSDV and
OLSR routing protocol using Nakagami propagation model and IEEE 802.11p MAC
protocol in a particular urban scenario of Dhaka city. The periodic broadcast
(PBC) agent is employed to transmit messages between vehicles in case of
emergency or collision avoidance for vehicular safety communication. The
simulation results recommend numerous concerns such as lower packet drop rate,
delay, jitter, route cost and mean-hop is necessary to be measured before
developing a robust safety application of VANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2418</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2418</id><created>2014-05-10</created><authors><author><keyname>Andersson</keyname><forenames>Kerstin</forenames></author></authors><title>Exact Probability Distribution versus Entropy</title><categories>cs.IT math.IT</categories><doi>10.3390/e16105198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem addressed concerns the determination of the average number of
successive attempts of guessing a word of a certain length consisting of
letters with given probabilities of occurrence. Both first- and second-order
approximations to a natural language are considered. The guessing strategy used
is guessing words in decreasing order of probability. When word and alphabet
sizes are large, approximations are necessary in order to estimate the number
of guesses. Several kinds of approximations are discussed demonstrating
moderate requirements concerning both memory and CPU time. When considering
realistic sizes of alphabets and words (100) the number of guesses can be
estimated within minutes with reasonable accuracy (a few percent). For many
probability distributions the density of the logarithm of probability products
is close to a normal distribution. For those cases it is possible to derive an
analytical expression for the average number of guesses. The proportion of
guesses needed on average compared to the total number decreases almost
exponentially with the word length. The leading term in an asymptotic expansion
can be used to estimate the number of guesses for large word lengths.
Comparisons with analytical lower bounds and entropy expressions are also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2419</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2419</id><created>2014-05-10</created><authors><author><keyname>Bakouri</keyname><forenames>Mohsen A.</forenames></author></authors><title>Sensorless Physiological Control of Implantable Rotary Blood Pumps for
  Heart Failure Patients Using Modern Control Techniques</title><categories>q-bio.QM cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sufferers of heart failure disease have a life expectancy of one year and
heart transplantation is usually the only guarantee of survival beyond this
period. The number of donor hearts available currently is less than 3,000 per
annum worldwide. Apart from the relatively fortunate people who receive donor
hearts for transplant, the only alternative for people with HF is the
implantation of rotary blood pump (IRBP). In fact, an IRBP with its continuous
operation requires a more complex controller to achieve basic physiological
requirements. The essential control requirement of an IRBP needs to mimic the
way that the heart pumps as much blood to the arterial circulation as it
receives from the venous circulation. This research aims to design, develop and
implement novel control strategies combining sensorless and non-invasive data
measurements to provide an adaptive and fairly robust preload sensitive
controller for IRBPs subjected to varying patient conditions, model
uncertainties and external disturbances. A sensorless pulastile flow estimator
was developed using collected data from animal experiments. Based on this
estimator, advanced physiological control algorithms for regulation of an IRBP
were developed to automatically adjust the pump speed to cater for changes in
the metabolic demand.The performance of the developed control algorithms are
assessed using a lumped parameter model of the CVS that was previously
developed using actual data from healthy pigs over a wide range of operating
conditions. Immediate responses of the controllers to short-term circulatory
changes as well as adaptive characteristics of the controllers in response to
long-term changes are examined in a parameter-optimised model of CVS - IRBP
interactions. Simulation results prove that the proposed controllers are fairly
robust against model uncertainties, parameter variations and external
disturbances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2420</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2420</id><created>2014-05-10</created><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Optimal Learners for Multiclass Problems</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental theorem of statistical learning states that for binary
classification problems, any Empirical Risk Minimization (ERM) learning rule
has close to optimal sample complexity. In this paper we seek for a generic
optimal learner for multiclass prediction. We start by proving a surprising
result: a generic optimal multiclass learner must be improper, namely, it must
have the ability to output hypotheses which do not belong to the hypothesis
class, even though it knows that all the labels are generated by some
hypothesis from the class. In particular, no ERM learner is optimal. This
brings back the fundmamental question of &quot;how to learn&quot;? We give a complete
answer to this question by giving a new analysis of the one-inclusion
multiclass learner of Rubinstein et al (2006) showing that its sample
complexity is essentially optimal. Then, we turn to study the popular
hypothesis class of generalized linear classifiers. We derive optimal learners
that, unlike the one-inclusion algorithm, are computationally efficient.
Furthermore, we show that the sample complexity of these learners is better
than the sample complexity of the ERM rule, thus settling in negative an open
question due to Collins (2005).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2421</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2421</id><created>2014-05-10</created><updated>2014-10-03</updated><authors><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Sarzo</keyname><forenames>Fabio</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author></authors><title>Plug-and-play voltage and frequency control of islanded microgrids with
  meshed topology</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new decentralized control scheme for Islanded
microGrids (ImGs) composed by the interconnection of Distributed Generation
Units (DGUs). Local controllers regulate voltage and frequency at the Point of
Common Coupling (PCC) of each DGU and they are able to guarantee stability of
the overall ImG. The control design procedure is decentralized, since, besides
two global scalar quantities, the synthesis of a local controller uses only
information on the corresponding DGU and lines connected to it. Most important,
our design procedure enables Plug-and-Play (PnP) operations: when a DGU is
plugged in or out, only DGUs physically connected to it have to retune their
local controllers. We study the performance of the proposed controllers
simulating different scenarios in MatLab/Simulink and using performance indexes
proposed in IEEE standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2424</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2424</id><created>2014-05-10</created><updated>2015-02-27</updated><authors><author><keyname>Foucaud</keyname><forenames>Florent</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Naserasr</keyname><forenames>Reza</forenames></author><author><keyname>Parreau</keyname><forenames>Aline</forenames></author><author><keyname>Valicov</keyname><forenames>Petru</forenames></author></authors><title>Identification, location-domination and metric dimension on interval and
  permutation graphs. II. Algorithms and complexity</title><categories>cs.DM cs.DS math.CO</categories><comments>22 pages, 8 figures. The new version contains a new algorithm. The
  combinatorial bounds of the original version have been removed and will be
  included in another paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problems of finding optimal identifying codes, (open)
locating-dominating sets and resolving sets (denoted IDENTIFYING CODE, (OPEN)
LOCATING-DOMINATING SET and METRIC DIMENSION) of an interval or a permutation
graph. In these problems, one asks to distinguish all vertices of a graph by a
subset of the vertices, using either the neighbourhood within the solution set
or the distances to the solution vertices. Using a general reduction for this
class of problems, we prove that the decision problems associated to these four
notions are NP-complete, even for graphs that are at the same time interval
graphs and permutation graphs and have diameter 2. While IDENTIFYING CODE and
(OPEN) LOCATING-DOMINATING SET are trivially fixed-parameter-tractable when
parameterized by solution size, it is known that in the same setting METRIC
DIMENSION is W[2]-hard. We show that for interval graphs, this parameterization
of METRIC DIMENSION is fixed-parameter-tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2430</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2430</id><created>2014-05-10</created><authors><author><keyname>Di Luna</keyname><forenames>G. A.</forenames></author><author><keyname>Flocchini</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>S. Gan</forenames></author><author><keyname>Santoro</keyname><forenames>N.</forenames></author><author><keyname>Viglietta</keyname><forenames>G.</forenames></author></authors><title>Robots with Lights: Overcoming Obstructed Visibility Without Colliding</title><categories>cs.DC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots with lights is a model of autonomous mobile computational entities
operating in the plane in Look-Compute-Move cycles: each agent has an
externally visible light which can assume colors from a fixed set; the lights
are persistent (i.e., the color is not erased at the end of a cycle), but
otherwise the agents are oblivious. The investigation of computability in this
model, initially suggested by Peleg, is under way, and several results have
been recently established. In these investigations, however, an agent is
assumed to be capable to see through another agent. In this paper we start the
study of computing when visibility is obstructable, and investigate the most
basic problem for this setting, Complete Visibility: The agents must reach
within finite time a configuration where they can all see each other and
terminate. We do not make any assumption on a-priori knowledge of the number of
agents, on rigidity of movements nor on chirality. The local coordinate system
of an agent may change at each activation. Also, by definition of lights, an
agent can communicate and remember only a constant number of bits in each
cycle. In spite of these weak conditions, we prove that Complete Visibility is
always solvable, even in the asynchronous setting, without collisions and using
a small constant number of colors. The proof is constructive. We also show how
to extend our protocol for Complete Visibility so that, with the same number of
colors, the agents solve the (non-uniform) Circle Formation problem with
obstructed visibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2432</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2432</id><created>2014-05-10</created><authors><author><keyname>Tran-Thanh</keyname><forenames>Long</forenames></author><author><keyname>Yu</keyname><forenames>Jia Yuan</forenames></author></authors><title>Functional Bandits</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the functional bandit problem, where the objective is to find an
arm that optimises a known functional of the unknown arm-reward distributions.
These problems arise in many settings such as maximum entropy methods in
natural language processing, and risk-averse decision-making, but current
best-arm identification techniques fail in these domains. We propose a new
approach, that combines functional estimation and arm elimination, to tackle
this problem. This method achieves provably efficient performance guarantees.
In addition, we illustrate this method on a number of important functionals in
risk management and information theory, and refine our generic theoretical
results in those cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2434</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2434</id><created>2014-05-10</created><authors><author><keyname>Lijiang</keyname><forenames>Chen</forenames></author></authors><title>Coordinate System Selection for Minimum Error Rate Training in
  Statistical Machine Translation</title><categories>cs.CL</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Minimum error rate training (MERT) is a widely used training procedure for
statistical machine translation. A general problem of this approach is that the
search space is easy to converge to a local optimum and the acquired weight set
is not in accord with the real distribution of feature functions. This paper
introduces coordinate system selection (RSS) into the search algorithm for
MERT. Contrary to previous approaches in which every dimension only corresponds
to one independent feature function, we create several coordinate systems by
moving one of the dimensions to a new direction. The basic idea is quite simple
but critical that the training procedure of MERT should be based on a
coordinate system formed by search directions but not directly on feature
functions. Experiments show that by selecting coordinate systems with tuning
set results, better results can be obtained without any other language
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2435</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2435</id><created>2014-05-10</created><authors><author><keyname>Trahtman</keyname><forenames>A. N.</forenames></author></authors><title>The length of a minimal synchronizing word and the \v{C}erny conjecture</title><categories>cs.FL cs.DM</categories><comments>11 pages, 3 examples. arXiv admin note: substantial text overlap with
  arXiv:1202.4626</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A word w is called a synchronizing word of deterministic finite automaton
(DFA) if $w$ sends all states of the automaton to a unique state. In 1964, Jan
\v{C}erny discovered a sequence of n-state complete DFA possessing a minimal
synchronizing word of length $(n-1)^2$. The \v{C}erny conjecture claims that it
is also the upper bound on the length of such a word for a complete DFA. The
problem has motivated great and constantly growing number of investigations and
generalizations and together with the Road Coloring problem is considered as a
most fascinating old problem in the theory of finite automata.
  Some properties of synchronization are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2447</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2447</id><created>2014-05-10</created><updated>2014-09-29</updated><authors><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Nishimura</keyname><forenames>Naomi</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>Reconfiguration over tree decompositions</title><categories>cs.CC cs.DS</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A vertex-subset graph problem $Q$ defines which subsets of the vertices of an
input graph are feasible solutions. The reconfiguration version of a
vertex-subset problem $Q$ asks whether it is possible to transform one feasible
solution for $Q$ into another in at most $\ell$ steps, where each step is a
vertex addition or deletion, and each intermediate set is also a feasible
solution for $Q$ of size bounded by $k$. Motivated by recent results
establishing W[1]-hardness of the reconfiguration versions of most
vertex-subset problems parameterized by $\ell$, we investigate the complexity
of such problems restricted to graphs of bounded treewidth. We show that the
reconfiguration versions of most vertex-subset problems remain PSPACE-complete
on graphs of treewidth at most $t$ but are fixed-parameter tractable
parameterized by $\ell + t$ for all vertex-subset problems definable in monadic
second-order logic (MSOL). To prove the latter result, we introduce a technique
which allows us to circumvent cardinality constraints and define
reconfiguration problems in MSOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2452</identifier>
 <datestamp>2014-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2452</id><created>2014-05-10</created><updated>2014-08-13</updated><authors><author><keyname>Anari</keyname><forenames>Nima</forenames></author><author><keyname>Goel</keyname><forenames>Gagan</forenames></author><author><keyname>Nikzad</keyname><forenames>Afshin</forenames></author></authors><title>Mechanism Design for Crowdsourcing: An Optimal 1-1/e Competitive
  Budget-Feasible Mechanism for Large Markets</title><categories>cs.GT</categories><comments>Accepted to FOCS 2014</comments><msc-class>91B26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a mechanism design problem in the context of
large-scale crowdsourcing markets such as Amazon's Mechanical Turk,
ClickWorker, CrowdFlower. In these markets, there is a requester who wants to
hire workers to accomplish some tasks. Each worker is assumed to give some
utility to the requester. Moreover each worker has a minimum cost that he wants
to get paid for getting hired. This minimum cost is assumed to be private
information of the workers. The question then is - if the requester has a
limited budget, how to design a direct revelation mechanism that picks the
right set of workers to hire in order to maximize the requester's utility.
  We note that although the previous work has studied this problem, a crucial
difference in which we deviate from earlier work is the notion of large-scale
markets that we introduce in our model. Without the large market assumption, it
is known that no mechanism can achieve an approximation factor better than
0.414 and 0.5 for deterministic and randomized mechanisms respectively (while
the best known deterministic and randomized mechanisms achieve an approximation
ratio of 0.292 and 0.33 respectively). In this paper, we design a
budget-feasible mechanism for large markets that achieves an approximation
factor of 1-1/e (i.e. almost 0.63). Our mechanism can be seen as a
generalization of an alternate way to look at the proportional share mechanism
which is used in all the previous works so far on this problem. Interestingly,
we also show that our mechanism is optimal by showing that no truthful
mechanism can achieve a factor better than 1-1/e; thus, fully resolving this
setting. Finally we consider the more general case of submodular utility
functions and give new and improved mechanisms for the case when the markets
are large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2458</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2458</id><created>2014-05-10</created><authors><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>Quasi-linear Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a heuristic for designing vector non-linear network codes for
non-multicast networks, which we call quasi-linear network codes. The method
presented has two phases: finding an approximate linear network code over the
reals, and then quantizing it to a vector non-linear network code using a
fixed-point representation. Apart from describing the method, we draw some
links between some network parameters and the rate of the resulting code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2475</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2475</id><created>2014-05-10</created><authors><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author><author><keyname>Zheltov</keyname><forenames>Pavel</forenames></author></authors><title>Estimation of Overspread Scattering Functions</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2403309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many radar scenarios, the radar target or the medium is assumed to possess
randomly varying parts. The properties of a target are described by a random
process known as the spreading function. Its second order statistics under the
WSSUS assumption are given by the scattering function. Recent developments in
operator sampling theory suggest novel channel sounding procedures that allow
for the determination of the spreading function given complete statistical
knowledge of the operator echo from a single sounding by a weighted pulse
train.
  We construct and analyze a novel estimator for the scattering function based
on these findings. Our results apply whenever the scattering function is
supported on a compact subset of the time-frequency plane. We do not make any
restrictions either on the geometry of this support set, or on its area. Our
estimator can be seen as a generalization of an averaged periodogram estimator
for the case of a non-rectangular geometry of the support set of the scattering
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2476</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2476</id><created>2014-05-10</created><updated>2015-04-14</updated><authors><author><keyname>Beros</keyname><forenames>Achilles</forenames></author><author><keyname>de la Higuera</keyname><forenames>Colin</forenames></author></authors><title>A Canonical Semi-Deterministic Transducer</title><categories>cs.LG</categories><comments>A shorter version has been published in the proceedings of ICGI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of a canonical form for semi-deterministic transducers
with incomparable sets of output strings. Based on this, we develop an
algorithm which learns semi-deterministic transducers given access to
translation queries. We also prove that there is no learning algorithm for
semi-deterministic transducers that uses only domain knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2484</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2484</id><created>2014-05-10</created><authors><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Rocco</keyname><forenames>Marco</forenames></author><author><keyname>Trov&#xf2;</keyname><forenames>Francesco</forenames></author></authors><title>Truthful Learning Mechanisms for Multi-Slot Sponsored Search Auctions
  with Externalities</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sponsored search auctions constitute one of the most successful applications
of microeconomic mechanisms. In mechanism design, auctions are usually designed
to incentivize advertisers to bid their truthful valuations and to assure both
the advertisers and the auctioneer a non-negative utility. Nonetheless, in
sponsored search auctions, the click-through-rates (CTRs) of the advertisers
are often unknown to the auctioneer and thus standard truthful mechanisms
cannot be directly applied and must be paired with an effective learning
algorithm for the estimation of the CTRs. This introduces the critical problem
of designing a learning mechanism able to estimate the CTRs at the same time as
implementing a truthful mechanism with a revenue loss as small as possible
compared to an optimal mechanism designed with the true CTRs. Previous work
showed that, when dominant-strategy truthfulness is adopted, in single-slot
auctions the problem can be solved using suitable exploration-exploitation
mechanisms able to achieve a per-step regret (over the auctioneer's revenue) of
order $O(T^{-1/3})$ (where T is the number of times the auction is repeated).
It is also known that, when truthfulness in expectation is adopted, a per-step
regret (over the social welfare) of order $O(T^{-1/2})$ can be obtained. In
this paper we extend the results known in the literature to the case of
multi-slot auctions. In this case, a model of the user is needed to
characterize how the advertisers' valuations change over the slots. We adopt
the cascade model that is the most famous model in the literature for sponsored
search auctions. We prove a number of novel upper bounds and lower bounds both
on the auctioneer's revenue loss and social welfare w.r.t. to the VCG auction
and we report numerical simulations investigating the accuracy of the bounds in
predicting the dependency of the regret on the auction parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2489</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2489</id><created>2014-05-10</created><authors><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author></authors><title>The Zen of Graduate-level Programming</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ubiquity of technology in our daily lives and the economic stability of
the technology sector in recent years, especially in areas with a computer
science footing, has led to an increase in computer science enrollment in many
parts of the world. To keep up with this trend, the undergraduate computer
science curriculum has undergone many revisions, analysis, and discussion.
Unfortunately, the graduate level curriculum is lagging far behind in computer
science education literature and research. To remedy this, we present the
blueprint and execution of a graduate level course in programming, designed
specifically to cater to the needs of graduate students with a diverse
background both in CS and other fields. To this end, the course is divided into
two halves. In the first half, students are introduced to different programming
concepts, such as multi-paradigm programming, data structures, concurrency, and
security to bring them up to speed and provide a level playing field. In the
second half, all of these concepts are employed as building blocks to solve
real-world problems from data mining, natural language processing, computer
vision, and other fields. In addition, the paper also discusses in detail the
evaluation instruments employed for the course. Moreover, we also share
anecdotal information around student feedback, course design, and grading that
may be useful for others who wish to replicate our curriculum or sketch a
similar course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2492</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2492</id><created>2014-05-10</created><authors><author><keyname>Boostanimehr</keyname><forenames>Hamidreza</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Unified and Distributed QoS-Driven Cell Association Algorithms in
  Heterogeneous Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the cell association problem in the downlink of a
multi-tier heterogeneous network (HetNet), where base stations (BSs) have
finite number of resource blocks (RBs) available to distribute among their
associated users. Two problems are defined and treated in this paper: sum
utility of long term rate maximization with long term rate quality of service
(QoS) constraints, and global outage probability minimization with outage QoS
constraints. The first problem is well-suited for low mobility environments,
while the second problem provides a framework to deal with environments with
fast fading. The defined optimization problems in this paper are solved in two
phases: cell association phase followed by the optional RB distribution phase.
We show that the cell association phase of both problems have the same
structure. Based on this similarity, we propose a unified distributed algorithm
with low levels of message passing to for the cell association phase. This
distributed algorithm is derived by relaxing the association constraints and
using Lagrange dual decomposition method. In the RB distribution phase, the
remaining RBs after the cell association phase are distributed among the users.
Simulation results show the superiority of our distributed cell association
scheme compared to schemes that are based on maximum signal to interference
plus noise ratio (SINR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2494</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2494</id><created>2014-05-10</created><authors><author><keyname>Caroprese</keyname><forenames>Luciano</forenames></author><author><keyname>Trubitsyna</keyname><forenames>Irina</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author><author><keyname>Zumpano</keyname><forenames>Ester</forenames></author></authors><title>A Measure of Arbitrariness in Abductive Explanations</title><categories>cs.LO</categories><comments>25 pages (including appendix)</comments><acm-class>D.1.6; I.2.3</acm-class><journal-ref>Theory and Practice of Logic Programming, 14(4-5): 665-679 (2014)</journal-ref><doi>10.1017/S1471068414000271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the framework of abductive logic programming extended with integrity
constraints. For this framework, we introduce a new measure of the simplicity
of an explanation based on its degree of \emph{arbitrariness}: the more
arbitrary the explanation, the less appealing it is, with explanations having
no arbitrariness - they are called constrained - being the preferred ones. In
the paper, we study basic properties of constrained explanations. For the case
when programs in abductive theories are stratified we establish results
providing a detailed picture of the complexity of the problem to decide whether
constrained explanations exist. (To appear in Theory and Practice of Logic
Programming (TPLP).)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2496</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2496</id><created>2014-05-11</created><authors><author><keyname>Druce</keyname><forenames>Jeffrey M.</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis D.</forenames></author><author><keyname>Gonella</keyname><forenames>Stefano</forenames></author></authors><title>Anomaly-Sensitive Dictionary Learning for Unsupervised Diagnostics of
  Solid Media</title><categories>cs.CV</categories><comments>Submitted to the Proceedings of the Royal Society A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a strategy for the detection and triangulation of
structural anomalies in solid media. The method revolves around the
construction of sparse representations of the medium's dynamic response,
obtained by learning instructive dictionaries which form a suitable basis for
the response data. The resulting sparse coding problem is recast as a modified
dictionary learning task with additional spatial sparsity constraints enforced
on the atoms of the learned dictionaries, which provides them with a prescribed
spatial topology that is designed to unveil anomalous regions in the physical
domain. The proposed methodology is model agnostic, i.e., it forsakes the need
for a physical model and requires virtually no a priori knowledge of the
structure's material properties, as all the inferences are exclusively informed
by the data through the layers of information that are available in the
intrinsic salient structure of the material's dynamic response. This
characteristic makes the approach powerful for anomaly identification in
systems with unknown or heterogeneous property distribution, for which a model
is unsuitable or unreliable. The method is validated using both synthetically
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2501</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2501</id><created>2014-05-11</created><authors><author><keyname>Bart&#xe1;k</keyname><forenames>Roman</forenames></author><author><keyname>Zhou</keyname><forenames>Neng-Fa</forenames></author></authors><title>Using Tabled Logic Programming to Solve the Petrobras Planning Problem</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><acm-class>D.1.6; I.2.3; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tabling has been used for some time to improve efficiency of Prolog programs
by memorizing answered queries. The same idea can be naturally used to memorize
visited states during search for planning. In this paper we present a planner
developed in the Picat language to solve the Petrobras planning problem. Picat
is a novel Prolog-like language that provides pattern matching, deterministic
and non-deterministic rules, and tabling as its core modelling and solving
features. We demonstrate these capabilities using the Petrobras problem, where
the goal is to plan transport of cargo items from ports to platforms using
vessels with limited capacity. Monte Carlo Tree Search has been so far the best
technique to tackle this problem and we will show that by using tabling we can
achieve much better runtime efficiency and better plan quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2502</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2502</id><created>2014-05-11</created><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author></authors><title>Maximal Entanglement - A New Measure of Entanglement</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages, presented at IWCIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximal correlation is a measure of correlation for bipartite distributions.
This measure has two intriguing features: (1) it is monotone under local
stochastic maps; (2) it gives the same number when computed on i.i.d. copies of
a pair of random variables. This measure of correlation has recently been
generalized for bipartite quantum states, for which the same properties have
been proved. In this paper, based on maximal correlation, we define a new
measure of entanglement which we call maximal entanglement. We show that this
measure of entanglement is faithful (is zero on separable states and positive
on entangled states), is monotone under local quantum operations, and gives the
same number when computed on tensor powers of a bipartite state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2512</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2512</id><created>2014-05-11</created><updated>2015-05-18</updated><authors><author><keyname>Daltrophe</keyname><forenames>Hadassa</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author></authors><title>Mending the Big-Data Missing Information</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Consider a high-dimensional data set, such that for every data-point there is
incomplete information. Each object in the data set represents a real entity,
which models as a point in high-dimensional space. We assume that all real
entities are embedded in the same space, which means they have the same
dimension. We model the lack of information for a given object as affine
subspace in $\mathbb{R}^d$ with dimension $k$.
  Our goal in this paper is to find clusters of objects. The main problem is to
cope with the partial information. We studied a simple algorithm we call
\emph{Data clustering using flats minimum distances}, using the following
assumptions: 1) There are $m$ clusters. 2) Each cluster is modeled as a ball in
$\mathbb{R}^d$. 3) Each cluster contains a $\frac{n}{m}$, $k$-dimensional
affine subspaces . 4) All $k$-dimensional affine subspaces, which belong in the
same cluster, are intersected with the ball of the cluster. 5) Each
$k$-dimensional affine subspace, that belong to a cluster, is selected
uniformly among all $k$-dimensional affine subspaces that intersect the ball's
cluster. A data set that satisfy these assumptions will be called
\textit{separable data}.
  Our suggested algorithm calculates pair-wise projection of the data. We use
probabilistic considerations to prove the algorithm correctness. These
probabilistic results are of independent interest, as can serve to better
understand the geometry of high dimensional objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2517</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2517</id><created>2014-05-11</created><authors><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author></authors><title>A $35 Firewall for the Developing World</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of recent efforts aim to bridge the global digital divide,
particularly with respect to Internet access. We take this endeavor one step
further and argue that Internet access and web security go hand in glove in the
developing world. To remedy the situation, we explore whether low-cost
platforms, such as Raspberry Pi (\$35) and Cubieboard (\$59), can be used to
implement security mechanisms. Using a firewall as a motivating security
application we benchmark its performance on these platforms to test our thesis.
Our results show that these platforms can indeed serve as enablers of security
functions for small sized deployments in the developing world, while only
consuming less than $2.5 worth of electricity per device per annum. In
addition, we argue that the use of these platforms also addresses maintenance
challenges such as update roll-out and distribution. Furthermore, a number of
additional network functions, such as caching and WAN acceleration can also be
implemented atop this simple infrastructure. Finally, we posit that this
deployment can be used for in-network monitoring to facilitate ICT4D research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2523</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2523</id><created>2014-05-11</created><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author></authors><title>Performance Provisioning and Energy Efficiency in Cloud and Distributed
  Computing Systems</title><categories>cs.DC cs.NI cs.PF</categories><comments>PhD thesis with 139 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the issue of energy consumption in high performance
computing (HPC) systems has attracted a great deal of attention. In response to
this, many energy-aware algorithms have been developed in different layers of
HPC systems, including the hardware layer, service layer and system layer.
These algorithms are of two types: first, algorithms which directly try to
improve the energy by tweaking frequency operation or scheduling algorithms;
and second, algorithms which focus on improving the performance of the system,
with the assumption that efficient running of a system may indirectly save more
energy.
  In this thesis, we develop algorithms in both layers. First, we introduce
three algorithms to directly improve the energy of scheduled tasks at the
hardware level by using Dynamic Voltage Frequency Scaling (DVFS). Second, we
propose two algorithms for modelling and resource provisioning of MapReduce
applications (a well-known parametric distributed framework currently used by
Google, Yahoo, Facebook and LinkedIn) based on its configuration parameters.
Certainly, estimating the performance (e.g., execution time or CPU clock ticks)
of a MapReduce application can be later used for smart scheduling of such
applications in clouds or clusters.
  To evaluate the algorithms, we have conducted extensive simulation and real
experiments on a 5-node physical cluster with up to 25 virtual nodes, using
both synthetic and real world applications. Also, the proposed new algorithms
are compared with existing algorithms by experimentation, and the experimental
results reveal new information on the performance of these algorithms, as well
as on the properties of MapReduce and DVFS. In the end, three open problems are
revealed by the experimental observations, and their importance is explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2524</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2524</id><created>2014-05-11</created><authors><author><keyname>Liang</keyname><forenames>Chulong</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Zhuang</keyname><forenames>Qiutao</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>Spatial Coupling of Generator Matrix: A General Approach to Design of
  Good Codes at a Target BER</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any given short code (referred to as the basic code), block Markov
superposition transmission (BMST) provides a simple way to obtain predictable
extra coding gain by spatial coupling the generator matrix of the basic code.
This paper presents a systematic design methodology for BMST systems to
approach the channel capacity at any given target bit-error-rate (BER) of
interest. To simplify the design, we choose the basic code as the Cartesian
product of a short block code. The encoding memory is then inferred from the
genie-aided lower bound according to the performance gap of the short block
code to the corresponding Shannon limit at the target BER. In addition to the
sliding-window decoding algorithm, we propose to perform one more phase
decoding to remove residual (rare) errors. A new technique that assumes a noisy
genie is proposed to upper bound the performance. Under some mild assumptions,
these genie-aided bounds can be used to predict the performance of the proposed
two-phase decoding algorithm in the extremely low BER region. Using the
Cartesian product of a repetition code as the basic code, we construct a BMST
system with an encoding memory 30 whose performance at the BER of $10^{-15}$
can be predicted within one dB away from the Shannon limit over the
binary-input additive white Gaussian noise channel (BI-AWGNC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2530</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2530</id><created>2014-05-11</created><updated>2014-06-23</updated><authors><author><keyname>Arad</keyname><forenames>Dor</forenames></author><author><keyname>Mordechai</keyname><forenames>Yael</forenames></author><author><keyname>Shachnai</keyname><forenames>Hadas</forenames></author></authors><title>Tighter Bounds for Makespan Minimization on Unrelated Machines</title><categories>cs.DS</categories><comments>12 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1011.1168 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of scheduling $n$ jobs to minimize the makespan on
$m$ unrelated machines, where job $j$ requires time $p_{ij}$ if processed on
machine $i$. A classic algorithm of Lenstra et al. yields the best known
approximation ratio of $2$ for the problem. Improving this bound has been a
prominent open problem for over two decades. In this paper we obtain a tighter
bound for a wide subclass of instances which can be identified efficiently.
Specifically, we define the feasibility factor of a given instance as the
minimum fraction of machines on which each job can be processed. We show that
there is a polynomial-time algorithm that, given values $L$ and $T$, and an
instance having a sufficiently large feasibility factor $h \in (0,1]$, either
proves that no schedule of mean machine completion time $L$ and makespan $T$
exists, or else finds a schedule of makespan at most $T + L/h &lt; 2T$. For the
restricted version of the problem, where for each job $j$ and machine $i$,
$p_{ij} \in \{p_j, \infty\}$, we show that a simpler algorithm yields a better
bound, thus improving for highly feasible instances the best known ratio of
$33/17 + \epsilon$, for any fixed $\epsilon &gt;0$, due to Svensson.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2538</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2538</id><created>2014-05-11</created><authors><author><keyname>Zhou</keyname><forenames>Neng-Fa</forenames></author></authors><title>Combinatorial Search With Picat</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Picat, a new member of the logic programming family, follows a different
doctrine than Prolog in offering the core logic programming concepts: arrays
and maps as built-in data types; implicit pattern matching with explicit
unification and explicit non-determinism; functions for deterministic
computations; and loops for convenient scripting and modeling purposes. Picat
provides facilities for solving combinatorial search problems, including a
common interface with CP, SAT, and MIP solvers, tabling for dynamic
programming, and a module for planning. Picat's planner module, which is
implemented by the use of tabling, has produced surprising and encouraging
results. Thanks to term-sharing and resource-bounded tabled search, Picat
overwhelmingly outperforms the cutting-edge ASP and PDDL planners on the
planning benchmarks used in recent ASP competitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2539</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2539</id><created>2014-05-11</created><authors><author><keyname>Vaghela</keyname><forenames>Dushyant</forenames></author><author><keyname>Naina</keyname><forenames>Prof. Kapildev</forenames></author></authors><title>A Review of Image Mosaicing Techniques</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Image Mosaicing is a method of constructing multiple images of the same scene
into a larger image. The output of the image mosaic will be the union of two
input images. Image-mosaicing algorithms are used to get mosaiced image. Image
Mosaicing processed is basically divided in to 5 phases. Which includes;
Feature point extraction, Image registration, Homography computation, Warping
and Blending if Image. Various corner detection algorithm is being used for
Feature extraction. This corner produces an efficient and informative output
mosaiced image. Image mosaicing is widely used in creating 3D images, medical
imaging, computer vision, data from satellites, and military automatic target
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2553</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2553</id><created>2014-05-11</created><authors><author><keyname>Sin'ya</keyname><forenames>Ryoma</forenames></author></authors><title>Graph Spectral Properties of Deterministic Finite Automata</title><categories>cs.FL</categories><comments>This paper has been accepted at the following conference: 18th
  International Conference on Developments in Language Theory (DLT 2014),
  August 26 - 29, 2014, Ekaterinburg, Russia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a minimal automaton has a minimal adjacency matrix rank and a
minimal adjacency matrix nullity using equitable partition (from graph spectra
theory) and Nerode partition (from automata theory). This result naturally
introduces the notion of matrix rank into a regular language L, the minimal
adjacency matrix rank of a deterministic automaton that recognises L. We then
define and focus on rank-one languages: the class of languages for which the
rank of minimal automaton is one. We also define the expanded canonical
automaton of a rank-one language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2555</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2555</id><created>2014-05-11</created><updated>2014-05-26</updated><authors><author><keyname>Data</keyname><forenames>Deepesh</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Mishra</keyname><forenames>Manoj</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>How to Securely Compute the Modulo-Two Sum of Binary Sources</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 1 figure, extended version of submission to IEEE Information
  Theory Workshop, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In secure multiparty computation, mutually distrusting users in a network
want to collaborate to compute functions of data which is distributed among the
users. The users should not learn any additional information about the data of
others than what they may infer from their own data and the functions they are
computing. Previous works have mostly considered the worst case context (i.e.,
without assuming any distribution for the data); Lee and Abbe (2014) is a
notable exception. Here, we study the average case (i.e., we work with a
distribution on the data) where correctness and privacy is only desired
asymptotically.
  For concreteness and simplicity, we consider a secure version of the function
computation problem of K\&quot;orner and Marton (1979) where two users observe a
doubly symmetric binary source with parameter p and the third user wants to
compute the XOR. We show that the amount of communication and randomness
resources required depends on the level of correctness desired. When zero-error
and perfect privacy are required, the results of Data et al. (2014) show that
it can be achieved if and only if a total rate of 1 bit is communicated between
every pair of users and private randomness at the rate of 1 is used up. In
contrast, we show here that, if we only want the probability of error to vanish
asymptotically in block length, it can be achieved by a lower rate (binary
entropy of p) for all the links and for private randomness; this also
guarantees perfect privacy. We also show that no smaller rates are possible
even if privacy is only required asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2559</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2559</id><created>2014-05-11</created><authors><author><keyname>Nogina</keyname><forenames>Elena</forenames></author></authors><title>On Logic of Formal Provability and Explicit Proofs</title><categories>math.LO cs.LO</categories><comments>15 pages</comments><msc-class>03F45, 03F03, 03F30, 03B45, 03B42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1933, G\&quot;odel considered two modal approaches to describing provability.
One captured formal provability and resulted in the logic GL and Solovay's
Completeness Theorem. The other was based on the modal logic S4 and led to
Artemov's Logic of Proofs LP. In this paper, we study introduced by the author
logic GLA, which is a fusion of GL and LP in the union of their languages. GLA
is supplied with a Kripke-style semantics and the corresponding completeness
theorem. Soundness and completeness of GLA with respect to the arithmetical
provability semantics is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2562</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2562</id><created>2014-05-11</created><authors><author><keyname>Suyari</keyname><forenames>Hiroki</forenames></author><author><keyname>Scarfone</keyname><forenames>Antonio Maria</forenames></author></authors><title>$\alpha$-divergence derived as the generalized rate function in a
  power-law system</title><categories>math-ph cs.IT math.IT math.MP</categories><comments>5 pages</comments><msc-class>60F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized binomial distribution in Tsallis statistics (power-law
system) is explicitly formulated from the precise $q$-Stirling's formula. The
$\alpha $-divergence (or $q$-divergence) is uniquely derived from the
generalized binomial distribution in the sense that when $\alpha\rightarrow-1$
(i.e., $q\rightarrow1$) it recovers KL divergence obtained from the standard
binomial distribution. Based on these combinatorial considerations, it is shown
that $\alpha$-divergence (or $q$-divergence) is appeared as the generalized
rate function in the large deviation estimate in Tsallis statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2564</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2564</id><created>2014-05-11</created><authors><author><keyname>Oliveira</keyname><forenames>George Souza</forenames></author><author><keyname>da Silva</keyname><forenames>Anderson Faustino</forenames></author></authors><title>Towards an Efficient Prolog System by Code Introspection</title><categories>cs.PL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP). Several Prolog
interpreters are based on the Warren Abstract Machine (WAM), an elegant model
to compile Prolog programs. In order to improve the performance several
strategies have been proposed, such as: optimize the selection of clauses,
specialize the unification, global analysis, native code generation and
tabling. This paper proposes a different strategy to implement an efficient
Prolog System, the creation of specialized emulators on the fly. The proposed
strategy was implemented and evaluated at YAP Prolog System, and the
experimental evaluation showed interesting results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2566</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2566</id><created>2014-05-11</created><authors><author><keyname>Azizi</keyname><forenames>Elham</forenames></author><author><keyname>Galagan</keyname><forenames>James E.</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Learning modular structures from network data and node variables</title><categories>stat.ML cs.SI physics.soc-ph q-bio.QM stat.AP</categories><comments>22 pages, 6 figures, 3 tables, 3 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard technique for understanding underlying dependency structures among
a set of variables posits a shared conditional probability distribution for the
variables measured on individuals within a group. This approach is often
referred to as module networks, where individuals are represented by nodes in a
network, groups are termed modules, and the focus is on estimating the network
structure among modules. However, estimation solely from node-specific
variables can lead to spurious dependencies, and unverifiable structural
assumptions are often used for regularization. Here, we propose an extended
model that leverages direct observations about the network in addition to
node-specific variables. By integrating complementary data types, we avoid the
need for structural assumptions. We illustrate theoretical and practical
significance of the model and develop a reversible-jump MCMC learning procedure
for learning modules and model parameters. We demonstrate the method accuracy
in predicting modular structures from synthetic data and capability to learn
influence structures in twitter data and regulatory modules in the
Mycobacterium tuberculosis gene regulatory network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2571</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2571</id><created>2014-05-11</created><updated>2015-03-04</updated><authors><author><keyname>Haraguchi</keyname><forenames>Kazuya</forenames></author></authors><title>An Efficient Local Search for Partial Latin Square Extension Problem</title><categories>cs.DS</categories><comments>17 pages, 2 figures</comments><acm-class>G.1.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partial Latin square (PLS) is a partial assignment of n symbols to an nxn
grid such that, in each row and in each column, each symbol appears at most
once. The partial Latin square extension problem is an NP-hard problem that
asks for a largest extension of a given PLS. In this paper we propose an
efficient local search for this problem. We focus on the local search such that
the neighborhood is defined by (p,q)-swap, i.e., removing exactly p symbols and
then assigning symbols to at most q empty cells. For p in {1,2,3}, our
neighborhood search algorithm finds an improved solution or concludes that no
such solution exists in O(n^{p+1}) time. We also propose a novel swap
operation, Trellis-swap, which is a generalization of (1,q)-swap and
(2,q)-swap. Our Trellis-neighborhood search algorithm takes O(n^{3.5}) time to
do the same thing. Using these neighborhood search algorithms, we design a
prototype iterated local search algorithm and show its effectiveness in
comparison with state-of-the-art optimization solvers such as IBM ILOG CPLEX
and LocalSolver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2576</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2576</id><created>2014-05-11</created><authors><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Spatial Coordination Strategies in Future Ultra-Dense Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>An extended version of a paper submitted to ISWCS'14, Special Session
  on Empowering Technologies of 5G Wireless Communications</comments><report-no>ART-COMP PE7/396 Research Project Technical Report</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra network densification is considered a major trend in the evolution of
cellular networks, due to its ability to bring the network closer to the user
side and reuse resources to the maximum extent. In this paper we explore
spatial resources coordination as a key empowering technology for next
generation (5G) ultra-dense networks. We propose an optimization framework for
flexibly associating system users with a densely deployed network of access
nodes, opting for the exploitation of densification and the control of overhead
signaling. Combined with spatial precoding processing strategies, we design
network resources management strategies reflecting various features, namely
local vs global channel state information knowledge exploitation, centralized
vs distributed implementation, and non-cooperative vs joint multi-node data
processing. We apply these strategies to future UDN setups, and explore the
impact of critical network parameters, that is, the densification levels of
users and access nodes as well as the power budget constraints, to users
performance. We demonstrate that spatial resources coordination is a key factor
for capitalizing on the gains of ultra dense network deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2580</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2580</id><created>2014-05-11</created><authors><author><keyname>Haber</keyname><forenames>Aleksandar</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Sparse approximate inverses of Gramians and impulse response matrices of
  large-scale interconnected systems</title><categories>cs.SY math.OC</categories><comments>11 pages, 4 figures. Submitted to IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that inverses of well-conditioned, finite-time Gramians
and impulse response matrices of large-scale interconnected systems described
by sparse state-space models, can be approximated by sparse matrices. The
approximation methodology established in this paper opens the door to the
development of novel methods for distributed estimation, identification and
control of large-scale interconnected systems. The novel estimators
(controllers) compute local estimates (control actions) simply as linear
combinations of inputs and outputs (states) of local subsystems. The size of
these local data sets essentially depends on the condition number of the
finite-time observability (controllability) Gramian. Furthermore, the developed
theory shows that the sparsity patterns of the system matrices of the
distributed estimators (controllers) are primarily determined by the sparsity
patterns of state-space matrices of large-scale systems. The computational and
memory complexity of the approximation algorithms are $O(N)$, where $N$ is the
number of local subsystems of the interconnected system. Consequently, the
proposed approximation methodology is computationally feasible for
interconnected systems with an extremely large number of local subsystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2584</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2584</id><created>2014-05-11</created><authors><author><keyname>Tejwani</keyname><forenames>Rahul</forenames><affiliation>University at Buffalo</affiliation></author></authors><title>Sentiment Analysis: A Survey</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis (also known as opinion mining) refers to the use of
natural language processing, text analysis and computational linguistics to
identify and extract subjective information in source materials. Mining
opinions expressed in the user generated content is a challenging yet
practically very useful problem. This survey would cover various approaches and
methodology used in Sentiment Analysis and Opinion Mining in general. The focus
would be on Internet text like, Product review, tweets and other social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2590</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2590</id><created>2014-05-11</created><authors><author><keyname>Tachmazidis</keyname><forenames>Ilias</forenames></author><author><keyname>Antoniou</keyname><forenames>Grigoris</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author></authors><title>Efficient Computation of the Well-Founded Semantics over Big Data</title><categories>cs.AI</categories><comments>16 pages, 4 figures, ICLP 2014, 30th International Conference on
  Logic Programming July 19-22, Vienna, Austria</comments><doi>10.1017/S1471068414000131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data originating from the Web, sensor readings and social media result in
increasingly huge datasets. The so called Big Data comes with new scientific
and technological challenges while creating new opportunities, hence the
increasing interest in academia and industry. Traditionally, logic programming
has focused on complex knowledge structures/programs, so the question arises
whether and how it can work in the face of Big Data. In this paper, we examine
how the well-founded semantics can process huge amounts of data through mass
parallelization. More specifically, we propose and evaluate a parallel approach
using the MapReduce framework. Our experimental results indicate that our
approach is scalable and that well-founded semantics can be applied to billions
of facts. To the best of our knowledge, this is the first work that addresses
large scale nonmonotonic reasoning without the restriction of stratification
for predicates of arbitrary arity. To appear in Theory and Practice of Logic
Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2597</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2597</id><created>2014-05-11</created><authors><author><keyname>Zhao</keyname><forenames>Shancheng</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>Decoding and Computing Algorithms for Linear Superposition LDPC Coded
  Systems</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper is concerned with linear superposition systems in which all
components of the superimposed signal are coded with an identical binary
low-density parity-check (LDPC) code. We focus on the design of decoding and
computing algorithms. The main contributions of this paper include: 1) we
present three types of iterative multistage decoding/computing algorithms,
which are referred to as decoding-computing (DC) type, computing-decoding (CD)
type and computing-decoding computing (CDC) type, respectively; 2) we propose a
joint decoding/computing algorithm by treating the system as a nonbinary LDPC
(NB-LDPC) coded system; 3) we propose a time-varying signaling scheme for
multi-user communication channels. The proposed algorithms may find
applications in superposition modulation (SM), multiple-access channels (MAC),
Gaussian interference channels (GIFC) and two-way relay channels (TWRC). For SM
system, numerical results show that 1) the proposed CDC type iterative
multistage algorithm performs better than the standard DC type iterative
multistage algorithm, and 2) the joint decoding/computing algorithm performs
better than the proposed iterative multistage algorithms in high spectral
efficiency regime. For GIFC, numerical results show that, from moderate to
strong interference, the time-varying signaling scheme significantly
outperforms the constant signaling scheme when decoded with the joint
decoding/computing algorithm (about 8.5 dB for strong interference). For TWRC,
numerical results show that the joint decoding/computing algorithm performs
better than the CD type algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2600</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2600</id><created>2014-05-11</created><authors><author><keyname>Wang</keyname><forenames>Yuyi</forenames></author><author><keyname>Ramon</keyname><forenames>Jan</forenames></author><author><keyname>Guo</keyname><forenames>Zheng-Chu</forenames></author></authors><title>Learning from networked examples</title><categories>cs.AI cs.LG stat.ML</categories><comments>31 pages, submission to jmlr, 1st version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning algorithms are based on the assumption that training
examples are drawn identically and independently. However, this assumption does
not hold anymore when learning from a networked sample because two or more
training examples may share some common objects, and hence share the features
of these shared objects. We first show that the classic approach of ignoring
this problem potentially can have a disastrous effect on the accuracy of
statistics, and then consider alternatives. One of these is to only use
independent examples, discarding other information. However, this is clearly
suboptimal. We analyze sample error bounds in a networked setting, providing
both improved and new results. Next, we propose an efficient weighting method
which achieves a better sample error bound than those of previous methods. Our
approach is based on novel concentration inequalities for networked variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2602</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2602</id><created>2014-05-11</created><authors><author><keyname>Chen</keyname><forenames>Bocong</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Zhang</keyname><forenames>Guanghui</forenames></author></authors><title>Self-dual cyclic codes over finite chain rings</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><msc-class>94B15, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R$ be a finite commutative chain ring with unique maximal ideal $\langle
\gamma\rangle$, and let $n$ be a positive integer coprime with the
characteristic of $R/\langle \gamma\rangle$. In this paper, the algebraic
structure of cyclic codes of length $n$ over $R$ is investigated. Some new
necessary and sufficient conditions for the existence of nontrivial self-dual
cyclic codes are provided. An enumeration formula for the self-dual cyclic
codes is also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2605</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2605</id><created>2014-05-11</created><authors><author><keyname>Ghozlan</keyname><forenames>Hassan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Phase Modulation for Discrete-time Wiener Phase Noise Channels with
  Oversampling at High SNR</title><categories>cs.IT math.IT</categories><comments>To appear in ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discrete-time Wiener phase noise channel model is introduced in which
multiple samples are available at the output for every input symbol. A lower
bound on the capacity is developed. At high signal-to-noise ratio (SNR), if the
number of samples per symbol grows with the square root of the SNR, the
capacity pre-log is at least 3/4. This is strictly greater than the capacity
pre-log of the Wiener phase noise channel with only one sample per symbol,
which is 1/2. It is shown that amplitude modulation achieves a pre-log of 1/2
while phase modulation achieves a pre-log of at least 1/4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2606</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2606</id><created>2014-05-11</created><authors><author><keyname>Joseph</keyname><forenames>Joshua</forenames></author><author><keyname>Velez</keyname><forenames>Javier</forenames></author><author><keyname>Roy</keyname><forenames>Nicholas</forenames></author></authors><title>Structural Return Maximization for Reinforcement Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batch Reinforcement Learning (RL) algorithms attempt to choose a policy from
a designer-provided class of policies given a fixed set of training data.
Choosing the policy which maximizes an estimate of return often leads to
over-fitting when only limited data is available, due to the size of the policy
class in relation to the amount of data available. In this work, we focus on
learning policy classes that are appropriately sized to the amount of data
available. We accomplish this by using the principle of Structural Risk
Minimization, from Statistical Learning Theory, which uses Rademacher
complexity to identify a policy class that maximizes a bound on the return of
the best policy in the chosen policy class, given the available data. Unlike
similar batch RL approaches, our bound on return requires only extremely weak
assumptions on the true system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2622</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2622</id><created>2014-05-11</created><authors><author><keyname>Zhang</keyname><forenames>Wenbin</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>News-Based Group Modeling and Forecasting</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study news group modeling and forecasting methods using
quantitative data generated by our large-scale natural language processing
(NLP) text analysis system. A news group is a set of news entities, like top
U.S. cities, governors, senators, golfers, or movie actors. Our fame
distribution analysis of news groups shows that log-normal and power-law
distributions generally could describe news groups in many aspects. We use
several real news groups including cities, politicians, and CS professors, to
evaluate our news group models in terms of time series data distribution
analysis, group-fame probability analysis, and fame-changing analysis over long
time. We also build a practical news generation model using a HMM (Hidden
Markov Model) based approach. Most importantly, our analysis shows the future
entity fame distribution has a power-law tail. That is, only a small number of
news entities in a group could become famous in the future. Based on these
analysis we are able to answer some interesting forecasting problems - for
example, what is the future average fame (or maximum fame) of a specific news
group? And what is the probability that some news entity become very famous
within a certain future time range? We also give concrete examples to
illustrate our forecasting approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2627</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2627</id><created>2014-05-12</created><authors><author><keyname>Borril</keyname><forenames>Paul</forenames></author><author><keyname>Burgess</keyname><forenames>Mark</forenames></author><author><keyname>Craw</keyname><forenames>Todd</forenames></author><author><keyname>Dvorkin</keyname><forenames>Mike</forenames></author></authors><title>A Promise Theory Perspective on Data Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networking is undergoing a transformation throughout our industry. The need
for scalable network control and automation shifts the focus from hardware
driven products with ad hoc control to Software Defined Networks. This process
is now well underway. In this paper, we adopt the perspective of the Promise
Theory to examine the current and future states of networking technologies. The
goal is to see beyond specific technologies, topologies and approaches and
define principles. Promise Theory's bottom-up modeling has been applied to
server management for many years and lends itself to principles of
self-healing, scalability and robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2636</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2636</id><created>2014-05-12</created><authors><author><keyname>Lacoste</keyname><forenames>Xavier</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Faverge</keyname><forenames>Mathieu</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Bosilca</keyname><forenames>George</forenames><affiliation>ICL</affiliation></author><author><keyname>Ramet</keyname><forenames>Pierre</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, LaBRI</affiliation></author><author><keyname>Thibault</keyname><forenames>Samuel</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Taking advantage of hybrid systems for sparse direct solvers via
  task-based runtimes</title><categories>cs.DC</categories><comments>Heterogeneity in Computing Workshop (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ongoing hardware evolution exhibits an escalation in the number, as well
as in the heterogeneity, of computing resources. The pressure to maintain
reasonable levels of performance and portability forces application developers
to leave the traditional programming paradigms and explore alternative
solutions. PaStiX is a parallel sparse direct solver, based on a dynamic
scheduler for modern hierarchical manycore architectures. In this paper, we
study the benefits and limits of replacing the highly specialized internal
scheduler of the PaStiX solver with two generic runtime systems: PaRSEC and
StarPU. The tasks graph of the factorization step is made available to the two
runtimes, providing them the opportunity to process and optimize its traversal
in order to maximize the algorithm efficiency for the targeted hardware
platform. A comparative study of the performance of the PaStiX solver on top of
its native internal scheduler, PaRSEC, and StarPU frameworks, on different
execution environments, is performed. The analysis highlights that these
generic task-based runtimes achieve comparable results to the
application-optimized embedded scheduler on homogeneous platforms. Furthermore,
they are able to significantly speed up the solver on heterogeneous
environments by taking advantage of the accelerators while hiding the
complexity of their efficient manipulation from the programmer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2639</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2639</id><created>2014-05-12</created><updated>2015-12-01</updated><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author></authors><title>Sharp Finite-Time Iterated-Logarithm Martingale Concentration</title><categories>math.PR cs.LG stat.ML</categories><comments>25 pages</comments><msc-class>60E15, 60G17 (Primary), 60G40, 60G42, 60G44 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give concentration bounds for martingales that are uniform over finite
times and extend classical Hoeffding and Bernstein inequalities. We also
demonstrate our concentration bounds to be optimal with a matching
anti-concentration inequality, proved using the same method. Together these
constitute a finite-time version of the law of the iterated logarithm, and shed
light on the relationship between it and the central limit theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2641</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2641</id><created>2014-05-12</created><updated>2014-05-21</updated><authors><author><keyname>K</keyname><forenames>Jyothi</forenames></author><author><keyname>J</keyname><forenames>Prabhakar C.</forenames></author></authors><title>Multi Modal Face Recognition Using Block Based Curvelet Features</title><categories>cs.CV</categories><comments>17 pages, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present multimodal 2D +3D face recognition method using
block based curvelet features. The 3D surface of face (Depth Map) is computed
from the stereo face images using stereo vision technique. The statistical
measures such as mean, standard deviation, variance and entropy are extracted
from each block of curvelet subband for both depth and intensity images
independently.In order to compute the decision score, the KNN classifier is
employed independently for both intensity and depth map. Further, computed
decision scoresof intensity and depth map are combined at decision level to
improve the face recognition rate. The combination of intensity and depth map
is verified experimentally using benchmark face database. The experimental
results show that the proposed multimodal method is better than individual
modality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2642</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2642</id><created>2014-05-12</created><updated>2014-05-22</updated><authors><author><keyname>Delhibabu</keyname><forenames>Radhakrishnan</forenames></author></authors><title>An Abductive Framework for Horn Knowledge Base Dynamics</title><categories>cs.LO cs.AI</categories><comments>Applied Mathematics &amp; Information Sciences, 2014</comments><msc-class>68T15, 68T27, 68T35</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
We introduced the Horn knowledge base dynamics to deal with two important
points: first, to handle belief states that need not be deductively closed; and
the second point is the ability to declare certain parts of the belief as
immutable. In this paper, we address another, radically new approach to this
problem. This approach is very close to the Hansson's dyadic representation of
belief. Here, we consider the immutable part as defining a new logical system.
By a logical system, we mean that it defines its own consequence relation and
closure operator. Based on this, we provide an abductive framework for Horn
knowledge base dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2652</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2652</id><created>2014-05-12</created><updated>2014-09-15</updated><authors><author><keyname>Ortner</keyname><forenames>Ronald</forenames></author><author><keyname>Maillard</keyname><forenames>Odalric-Ambrym</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>Selecting Near-Optimal Approximate State Representations in
  Reinforcement Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a reinforcement learning setting introduced in (Maillard et al.,
NIPS 2011) where the learner does not have explicit access to the states of the
underlying Markov decision process (MDP). Instead, she has access to several
models that map histories of past interactions to states. Here we improve over
known regret bounds in this setting, and more importantly generalize to the
case where the models given to the learner do not contain a true model
resulting in an MDP representation but only approximations of it. We also give
improved error bounds for state aggregation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2664</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2664</id><created>2014-05-12</created><updated>2015-06-18</updated><authors><author><keyname>Zhao</keyname><forenames>Ji</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author></authors><title>FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test</title><categories>cs.AI cs.LG stat.ML</categories><journal-ref>Neural Computation, 2015 June, Vol. 27, No. 6, Pages 1345-1372</journal-ref><doi>10.1162/NECO_a_00732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum mean discrepancy (MMD) is a recently proposed test statistic for
two-sample test. Its quadratic time complexity, however, greatly hampers its
availability to large-scale applications. To accelerate the MMD calculation, in
this study we propose an efficient method called FastMMD. The core idea of
FastMMD is to equivalently transform the MMD with shift-invariant kernels into
the amplitude expectation of a linear combination of sinusoid components based
on Bochner's theorem and Fourier transform (Rahimi &amp; Recht, 2007). Taking
advantage of sampling of Fourier transform, FastMMD decreases the time
complexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$
are the size and dimension of the sample set, respectively. Here $L$ is the
number of basis functions for approximating kernels which determines the
approximation accuracy. For kernels that are spherically invariant, the
computation can be further accelerated to $O(L N \log d)$ by using the Fastfood
technique (Le et al., 2013). The uniform convergence of our method has also
been theoretically proved in both unbiased and biased estimates. We have
further provided a geometric explanation for our method, namely ensemble of
circular discrepancy, which facilitates us to understand the insight of MMD,
and is hopeful to help arouse more extensive metrics for assessing two-sample
test. Experimental results substantiate that FastMMD is with similar accuracy
as exact MMD, while with faster computation speed and lower variance than the
existing MMD approximation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2684</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2684</id><created>2014-05-12</created><updated>2014-05-20</updated><authors><author><keyname>Sarkar</keyname><forenames>Tanmoy</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Reversible and Irreversible Data Hiding Technique</title><categories>cs.CR</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography (literally meaning covered writing) is the art and science of
embedding secret message into seemingly harmless message. Stenography is
practice from olden days where in ancient Greece people used wooden blocks to
inscribe secret data and cover the date with wax and write normal message on
it. Today stenography is used in various field like multimedia, networks,
medical, military etc. With increasing technology trends steganography is
becoming more and more advanced where people not only interested on hiding
messages in multimedia data (cover data) but also at the receiving end they are
willing to obtain original cover data without any distortion after extracting
secret message. This paper will discuss few irreversible data hiding techniques
and also, some recently proposed reversible data hiding approach using images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2685</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2685</id><created>2014-05-12</created><authors><author><keyname>Dave</keyname><forenames>Manish B</forenames></author><author><keyname>Nakrani</keyname><forenames>Mitesh B</forenames></author></authors><title>Malicious User Detection in Spectrum Sensing for WRAN Using Different
  Outliers Detection Techniques</title><categories>cs.NI</categories><comments>5 pages, 8 figures, 1 table, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>Manish B Dave , Mitesh B Nakrani. &quot;Malicious User Detection in
  Spectrum Sensing for WRAN Using Different Outliers Detection
  Techniques&quot;,IJETT, V9(7),326-330 March 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V9P263</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio it is of prime importance that the presence of Primary
Users (PU) is detected correctly at each of the time. In order to do so the
help from all present Secondary Users (SU) is taken and such a taken is known
as co-operative spectrum sensing. Ideally it is assumed that all the secondary
users give the correct result to the control center. But there are certain
conditions under which the secondary users deliberately forward wrong result to
the control center so as to degrade the performance of the cognitive network.
In this paper we study the different techniques for detecting the malicious
users or outliers. We take into consideration practical environmental condition
such that the received signal of the secondary users is made to undergo fading
and noise is also introduced in the signal. We further go on to examine each of
the outlier detector techniques and find out the most suitable at various
instants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2690</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2690</id><created>2014-05-12</created><authors><author><keyname>A</keyname><forenames>Prashanth L.</forenames></author></authors><title>Policy Gradients for CVaR-Constrained MDPs</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a risk-constrained version of the stochastic shortest path (SSP)
problem, where the risk measure considered is Conditional Value-at-Risk (CVaR).
We propose two algorithms that obtain a locally risk-optimal policy by
employing four tools: stochastic approximation, mini batches, policy gradients
and importance sampling. Both the algorithms incorporate a CVaR estimation
procedure, along the lines of Bardou et al. [2009], which in turn is based on
Rockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio
principle for estimating the gradient of the sum of one cost function
(objective of the SSP) and the gradient of the CVaR of the sum of another cost
function (in the constraint of SSP). The algorithms differ in the manner in
which they approximate the CVaR estimates/necessary gradients - the first
algorithm uses stochastic approximation, while the second employ mini-batches
in the spirit of Monte Carlo methods. We establish asymptotic convergence of
both the algorithms. Further, since estimating CVaR is related to rare-event
simulation, we incorporate an importance sampling based variance reduction
scheme into our proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2693</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2693</id><created>2014-05-12</created><updated>2014-05-29</updated><authors><author><keyname>Castro</keyname><forenames>Sergio</forenames></author><author><keyname>Mens</keyname><forenames>Kim</forenames></author><author><keyname>Moura</keyname><forenames>Paulo</forenames></author></authors><title>Customisable Handling of Java References in Prolog Programs</title><categories>cs.PL</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integration techniques for combining programs written in distinct language
paradigms facilitate the implementation of specialised modules in the best
language for their task. In the case of Java-Prolog integration, a known
problem is the proper representation of references to Java objects on the
Prolog side. To solve it adequately, multiple dimensions should be considered,
including reference representation, opacity of the representation, identity
preservation, reference life span, and scope of the inter-language conversion
policies. This paper presents an approach that addresses all these dimensions,
generalising and building on existing representation patterns of foreign
references in Prolog, and taking inspiration from similar inter-language
representation techniques found in other domains. Our approach maximises
portability by making few assumptions about the Prolog engine interacting with
Java (e.g., embedded or executed as an external process). We validate our work
by extending JPC, an open-source integration library, with features supporting
our approach. Our JPC library is currently compatible with three different open
source Prolog engines (SWI, YAP} and XSB) by means of drivers. To appear in
Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2695</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2695</id><created>2014-05-12</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author></authors><title>The cusped hyperbolic census is complete</title><categories>math.GT cs.CG</categories><comments>32 pages, 17 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From its creation in 1989 through subsequent extensions, the widely-used
&quot;SnapPea census&quot; now aims to represent all cusped finite-volume hyperbolic
3-manifolds that can be obtained from &lt;= 8 ideal tetrahedra. Its construction,
however, has relied on inexact computations and some unproven (though
reasonable) assumptions, and so its completeness was never guaranteed. For the
first time, we prove here that the census meets its aim: we rigorously certify
that every ideal 3-manifold triangulation with &lt;= 8 tetrahedra is either (i)
homeomorphic to one of the census manifolds, or (ii) non-hyperbolic.
  In addition, we extend the census to 9 tetrahedra, and likewise prove this to
be complete. We also present the first list of all minimal triangulations of
all census manifolds, including non-geometric as well as geometric
triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2702</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2702</id><created>2014-05-12</created><updated>2014-07-17</updated><authors><author><keyname>&#x160;i&#x161;ovi&#x107;</keyname><forenames>Sabina</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author></authors><title>Comparison of the language networks from literature and blogs</title><categories>cs.CL cs.SI physics.soc-ph</categories><journal-ref>37th IEEE International Convention on Information and
  Communication Technology, Electronics and Microelectronics (MIPRO 2014),
  pp.1824--1829, (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the comparison of the linguistic networks from
literature and blog texts. The linguistic networks are constructed from texts
as directed and weighted co-occurrence networks of words. Words are nodes and
links are established between two nodes if they are directly co-occurring
within the sentence. The comparison of the networks structure is performed at
global level (network) in terms of: average node degree, average shortest path
length, diameter, clustering coefficient, density and number of components.
Furthermore, we perform analysis on the local level (node) by comparing the
rank plots of in and out degree, strength and selectivity. The
selectivity-based results point out that there are differences between the
structure of the networks constructed from literature and blogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2705</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2705</id><created>2014-05-12</created><updated>2014-06-13</updated><authors><author><keyname>Zuliani</keyname><forenames>Paolo</forenames></author></authors><title>Statistical Model Checking for Biological Applications</title><categories>cs.LO cs.SY q-bio.QM</categories><doi>10.1007/s10009-014-0343-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we survey recent work on the use of statistical model checking
techniques for biological applications. We begin with an overview of the basic
modelling techniques for biochemical reactions and their corresponding
stochastic simulation algorithm - the Gillespie algorithm. We continue by
giving a brief description of the relation between stochastic models and
continuous (ordinary differential equation) models. Next we present a
literature survey, divided in two general areas. In the first area we focus on
works addressing verification of biological models, while in the second area we
focus on papers tackling the parameter synthesis problem. We conclude with some
open problems and directions for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2708</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2708</id><created>2014-05-12</created><authors><author><keyname>Rehman</keyname><forenames>Nafay Hifzur</forenames></author><author><keyname>Verma</keyname><forenames>Neelam</forenames></author></authors><title>Application of Modified Multi Model Predictive Control Algorithm to
  Fluid Catalytic Cracking Unit</title><categories>cs.SY</categories><comments>7 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a modified multi model predictive control algorithm for
the control of riser outlet temperature and regenerator temperature for the
fluid catalytic cracking unit (FCCU). The models of the fluid catalytic
cracking unit are estimated using subspace identification (N4SID) algorithm.
The PRBS signal is applied as an input signal to estimate the FCCU models.
Since the estimated model does not give 100% fit; especially for nonlinear
systems having more than one operating conditions, multi-model approach is
proposed. In multi model, more than one model of FCCU used in MPC design. The
main advantages of proposed method are that it can handle hard input and output
constraints and it can be used for multi input multi output processes (MIMO)
without increasing the complexity in control design. MATLAB/Simulink is used to
estimate the models of FCCU and simulate the results for the controller. The
simulation results show that the proposed algorithm provides better result for
both reference tracking and disturbance rejection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2733</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2733</id><created>2014-05-12</created><updated>2015-07-21</updated><authors><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Massierer</keyname><forenames>Maike</forenames></author></authors><title>An optimal representation for the trace zero subgroup</title><categories>cs.CR math.AG</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an optimal-size representation for the elements of the trace zero
subgroup of the Picard group of an elliptic or hyperelliptic curve of any
genus, with respect to a field extension of any prime degree. The
representation is via the coefficients of a rational function, and it is
compatible with scalar multiplication of points. We provide efficient
compression and decompression algorithms, and complement them with
implementation results. We discuss in detail the practically relevant cases of
small genus and extension degree, and compare with the other known compression
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2735</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2735</id><created>2014-05-12</created><authors><author><keyname>Abdelrahman</keyname><forenames>Omer H.</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Signalling Storms in 3G Mobile Networks</title><categories>cs.NI cs.CR cs.PF</categories><comments>IEEE ICC 2014 - Communications and Information Systems Security
  Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the characteristics of signalling storms that have been caused by
certain common apps and recently observed in cellular networks, leading to
system outages. We then develop a mathematical model of a mobile user's
signalling behaviour which focuses on the potential of causing such storms, and
represent it by a large Markov chain. The analysis of this model allows us to
determine the key parameters of mobile user device behaviour that can lead to
signalling storms. We then identify the parameter values that will lead to
worst case load for the network itself in the presence of such storms. This
leads to explicit results regarding the manner in which individual mobile
behaviour can cause overload conditions on the network and its signalling
servers, and provides insight into how this may be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2736</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2736</id><created>2014-05-12</created><updated>2014-06-13</updated><authors><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Subspace codes from Ferrers diagrams</title><categories>cs.IT math.CO math.IT</categories><comments>minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give new constructions of Ferrer diagram rank metric codes,
which achieve the largest possible dimension. In particular, we prove several
cases of a conjecture by T. Etzion and N. Silberstein. We also establish a
sharp lower bound on the dimension of linear rank metric anticodes with a given
profile. Combining our results with the multilevel construction, we produce
examples of subspace codes with the largest known cardinality for the given
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2738</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2738</id><created>2014-05-12</created><updated>2014-06-17</updated><authors><author><keyname>Arapinis</keyname><forenames>Myrto</forenames><affiliation>School of Computer Science, University of Birmingham</affiliation></author><author><keyname>Delaune</keyname><forenames>St&#xe9;phanie</forenames><affiliation>LSV, CNRS &amp; ENS Cachan</affiliation></author><author><keyname>Kremer</keyname><forenames>Steve</forenames><affiliation>INRIA Nancy - Grand-Est</affiliation></author></authors><title>Dynamic Tags for Security Protocols</title><categories>cs.CR</categories><comments>50 pages with 30 references</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 19,
  2014) lmcs:690</journal-ref><doi>10.2168/LMCS-10(2:11)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design and verification of cryptographic protocols is a notoriously
difficult task, even in symbolic models which take an abstract view of
cryptography. This is mainly due to the fact that protocols may interact with
an arbitrary attacker which yields a verification problem that has several
sources of unboundedness (size of messages, number of sessions, etc. In this
paper, we characterize a class of protocols for which deciding security for an
unbounded number of sessions is decidable. More precisely, we present a simple
transformation which maps a protocol that is secure for a bounded number of
protocol sessions (a decidable problem) to a protocol that is secure for an
unbounded number of sessions. The precise number of sessions that need to be
considered is a function of the security property and we show that for several
classical security properties a single session is sufficient. Therefore, in
many cases our results yields a design strategy for security protocols: (i)
design a protocol intended to be secure for a {single session}; and (ii) apply
our transformation to obtain a protocol which is secure for an unbounded number
of sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2749</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2749</id><created>2014-05-12</created><authors><author><keyname>Matsuo</keyname><forenames>Akira</forenames></author><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author><author><keyname>Imoto</keyname><forenames>Nobuyuki</forenames></author></authors><title>A quantum algorithm for additive approximation of Ising partition
  functions</title><categories>quant-ph cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>18 pages, 12 figures</comments><journal-ref>Phys. Rev. A 90, 022304 (2014)</journal-ref><doi>10.1103/PhysRevA.90.022304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate quantum computational complexity of calculating partition
functions of Ising models. We construct a quantum algorithm for an additive
approximation of Ising partition functions on square lattices. To this end, we
utilize the overlap mapping developed by Van den Nest, D\&quot;ur, and Briegel
[Phys. Rev. Lett. 98, 117207 (2007)] and its interpretation through
measurement-based quantum computation (MBQC). We specify an algorithmic domain,
on which the proposed algorithm works, and an approximation scale, which
determines the accuracy of the approximation. We show that the proposed
algorithm does a nontrivial task, which would be intractable on any classical
computer, by showing the problem solvable by the proposed quantum algorithm are
BQP-complete. In the construction of the BQP-complete problem coupling
strengths and magnetic fields take complex values. However, the Ising models
that are of central interest in statistical physics and computer science
consist of real coupling strengths and magnetic fields. Thus we extend the
algorithmic domain of the proposed algorithm to such a real physical parameter
region and calculate the approximation scale explicitly. We found that the
overlap mapping and its MBQC interpretation improves the approximation scale
exponentially compared to a straightforward constant depth quantum algorithm.
On the other hand, the proposed quantum algorithm also provides us a partial
evidence that there exist no efficient classical algorithm for a multiplicative
approximation of the Ising partition functions even on the square lattice. This
result supports that the proposed quantum algorithm does a nontrivial task also
in the physical parameter region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2760</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2760</id><created>2014-05-12</created><authors><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author><author><keyname>Abdelrahman</keyname><forenames>Omer H.</forenames></author></authors><title>Search in the Universe of Big Networks and Data</title><categories>cs.NI</categories><comments>IEEE Network Magazine - Special Issue on Networking for Big Data,
  July-August 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching in the Internet for some object characterised by its attributes in
the form of data, such as a hotel in a certain city whose price is less than
something, is one of our most common activities when we access the Web. We
discuss this problem in a general setting, and compute the average amount of
time and the energy it takes to find an object in an infinitely large search
space. We consider the use of N search agents which act concurrently. Both the
case where the search agent knows which way it needs to go to find the object,
and the case where the search agent is perfectly ignorant and may even head
away from the object being sought. We show that under mild conditions regarding
the randomness of the search and the use of a time-out, the search agent will
always find the object despite the fact that the search space is infinite. We
obtain a formula for the average search time and the average energy expended by
N search agents acting concurrently and independently of each other. We see
that the time-out itself can be used to minimise the search time and the amount
of energy that is consumed to find an object. An approximate formula is derived
for the number of search agents that can help us guarantee that an object is
found in a given time, and we discuss how the competition between search agents
and other agents that try to hide the data object, can be used by opposing
parties to guarantee their own success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2767</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2767</id><created>2014-05-12</created><authors><author><keyname>&#xc7;akmak</keyname><forenames>Burak</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard H.</forenames></author></authors><title>S-AMP: Approximate Message Passing for General Matrix Ensembles</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a novel iterative estimation algorithm for linear
observation systems called S-AMP whose fixed points are the stationary points
of the exact Gibbs free energy under a set of (first- and second-) moment
consistency constraints in the large system limit. S-AMP extends the
approximate message-passing (AMP) algorithm to general matrix ensembles. The
generalization is based on the S-transform (in free probability) of the
spectrum of the measurement matrix. Furthermore, we show that the optimality of
S-AMP follows directly from its design rather than from solving a separate
optimization problem as done for AMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2786</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2786</id><created>2014-05-12</created><authors><author><keyname>Rao</keyname><forenames>Xiongbin</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Distributed Compressive CSIT Estimation and Feedback for FDD Multi-user
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>16 double-column pages, accepted for publication in IEEE Transactions
  on Signal Processing</comments><doi>10.1109/TSP.2014.2324991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To fully utilize the spatial multiplexing gains or array gains of massive
MIMO, the channel state information must be obtained at the transmitter side
(CSIT). However, conventional CSIT estimation approaches are not suitable for
FDD massive MIMO systems because of the overwhelming training and feedback
overhead. In this paper, we consider multi-user massive MIMO systems and deploy
the compressive sensing (CS) technique to reduce the training as well as the
feedback overhead in the CSIT estimation. The multi-user massive MIMO systems
exhibits a hidden joint sparsity structure in the user channel matrices due to
the shared local scatterers in the physical propagation environment. As such,
instead of naively applying the conventional CS to the CSIT estimation, we
propose a distributed compressive CSIT estimation scheme so that the compressed
measurements are observed at the users locally, while the CSIT recovery is
performed at the base station jointly. A joint orthogonal matching pursuit
recovery algorithm is proposed to perform the CSIT recovery, with the
capability of exploiting the hidden joint sparsity in the user channel
matrices. We analyze the obtained CSIT quality in terms of the normalized mean
absolute error, and through the closed-form expressions, we obtain simple
insights into how the joint channel sparsity can be exploited to improve the
CSIT recovery performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2794</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2794</id><created>2014-05-09</created><updated>2014-05-15</updated><authors><author><keyname>Mantadelis</keyname><forenames>Thepfrastos</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author><author><keyname>Moura</keyname><forenames>Paulo</forenames></author></authors><title>Tabling, Rational Terms, and Coinduction Finally Together!</title><categories>cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><doi>10.1017/S147106841400012X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP). Tabling is a
commonly used technique in logic programming for avoiding cyclic behavior of
logic programs and enabling more declarative program definitions. Furthermore,
tabling often improves computational performance. Rational term are terms with
one or more infinite sub-terms but with a finite representation. Rational terms
can be generated in Prolog by omitting the occurs check when unifying two
terms. Applications of rational terms include definite clause grammars,
constraint handling systems, and coinduction. In this paper, we report our
extension of YAP's Prolog tabling mechanism to support rational terms. We
describe the internal representation of rational terms within the table space
and prove its correctness. We then use this extension to implement a tabling
based approach to coinduction. We compare our approach with current coinductive
transformations and describe the implementation. In addition, we present an
algorithm that ensures a canonical representation for rational terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2795</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2795</id><created>2014-05-08</created><authors><author><keyname>Aloulou</keyname><forenames>Amira</forenames></author><author><keyname>Boubaker</keyname><forenames>Olfa</forenames></author></authors><title>A Relevant Reduction Method for Dynamic Modeling of a Seven-linked
  Humanoid Robot in the Three-dimensional Space</title><categories>cs.RO</categories><comments>8 pages, 2 figures</comments><journal-ref>Procedia Engineering, vol. 41, pp. 1277-1284, 2012</journal-ref><doi>10.1016/j.proeng.2012.07.311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the dynamic modeling of a seven linked humanoid robot, is
accurately developed, in the three dimensional space using the Newton-Euler
formalism. The aim of this study is to provide a clear and a systematic
approach so that starting from generalized motion equations of all rigid bodies
of the humanoid robot one can establish a reduced dynamical model. The
resulting model can be expended either for simulation propositions or
implemented for any given control law. In addition, transformations and
developments, proposed here, can be exploited for modeling any other
three-dimensional humanoid robot with a different morphology and variable
number of rigid bodies and degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2798</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2798</id><created>2014-05-12</created><authors><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Sun</keyname><forenames>Ke</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author><author><keyname>Marchand-Maillet</keyname><forenames>Stephane</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author></authors><title>Two-Stage Metric Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted for publication in ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel two-stage metric learning algorithm. We
first map each learning instance to a probability distribution by computing its
similarities to a set of fixed anchor points. Then, we define the distance in
the input data space as the Fisher information distance on the associated
statistical manifold. This induces in the input data space a new family of
distance metric with unique properties. Unlike kernelized metric learning, we
do not require the similarity measure to be positive semi-definite. Moreover,
it can also be interpreted as a local metric learning algorithm with well
defined distance approximation. We evaluate its performance on a number of
datasets. It outperforms significantly other metric learning methods and SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2801</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2801</id><created>2014-05-12</created><updated>2014-06-24</updated><authors><author><keyname>Saad</keyname><forenames>Aya</forenames></author><author><keyname>Fruehwirth</keyname><forenames>Thom</forenames></author><author><keyname>Gervet</keyname><forenames>Carmen</forenames></author></authors><title>The P-Box CDF-Intervals: Reliable Constraint Reasoning with Quantifiable
  Information</title><categories>cs.LO</categories><comments>12 pages + references, accepted paper in the ICLP2014, 14 Postscript
  figures, uses new_tlp.cls and acmtrans.bst</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new constraint domain for reasoning about data with
uncertainty. It extends convex modeling with the notion of p-box to gain
additional quantifiable information on the data whereabouts. Unlike existing
approaches, the p-box envelops an unknown probability instead of approximating
its representation. The p-box bounds are uniform cumulative distribution
functions (cdf) in order to employ linear computations in the probabilistic
domain. The reasoning by means of p-box cdf-intervals is an interval
computation which is exerted on the real domain then it is projected onto the
cdf domain. This operation conveys additional knowledge represented by the
obtained probabilistic bounds. The empirical evaluation of our implementation
shows that, with minimal overhead, the output solution set realizes a full
enclosure of the data along with tighter bounds on its probabilistic
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2806</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2806</id><created>2014-05-12</created><updated>2015-09-23</updated><authors><author><keyname>Gemine</keyname><forenames>Quentin</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Corn&#xe9;lusse</keyname><forenames>Bertrand</forenames></author></authors><title>Active network management for electrical distribution systems: problem
  formulation, benchmark, and approximate solution</title><categories>cs.SY cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing share of renewable and distributed generation in
electrical distribution systems, Active Network Management (ANM) becomes a
valuable option for a distribution system operator to operate his system in a
secure and cost-effective way without relying solely on network reinforcement.
ANM strategies are short-term policies that control the power injected by
generators and/or taken off by loads in order to avoid congestion or voltage
issues. While simple ANM strategies consist in curtailing temporary excess
generation, more advanced strategies rather attempt to move the consumption of
loads to anticipated periods of high renewable generation. However, such
advanced strategies imply that the system operator has to solve large-scale
optimal sequential decision-making problems under uncertainty. Uncertainty must
be explicitly accounted for because neither demand nor generation can be
accurately forecasted. We first formalize the ANM problem, which in addition to
be sequential and uncertain, has a non-linear nature stemming from the power
flow equations and a discrete nature arising from the activation of power
modulation signals. This ANM problem is then cast as a stochastic mixed integer
non-linear program, for which we provide quantitative results using state of
the art open source solvers and perform a sensitivity analysis over the amount
of flexibility available in the system and the number of scenarios considered
in the deterministic equivalent of the stochastic program. To foster further
research on this problem, we make available a test bed based on a 75-bus
distribution network at http://www.montefiore.ulg.ac.be/~anm/ . This test bed
contains a simulator of the distribution system, with stochastic models for the
generation and consumption devices, and callbacks to implement and test various
ANM strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2809</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2809</id><created>2014-05-12</created><authors><author><keyname>Kalamkar</keyname><forenames>Sanket</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>On the Effect of Primary User Traffic on Secondary Throughput and Outage
  Probability Under Rayleigh Flat Fading Channel</title><categories>cs.IT math.IT</categories><comments>Accepted in 10th International Conference on Signal Processing and
  Communications 2014 (SPCOM'14), Bangalore, India, July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effect of the primary traffic on a secondary user's (SU) throughput under
Rayleigh flat fading channel is investigated. For this case, closed form
expressions are derived for the average probability of detection and the
average probability of false alarm. Based on these expressions, the average SU
throughput under the desired signal-to-noise ratio (SNR) constraint in order to
maintain the quality of the secondary link is found analytically considering
the random arrival or departure of the primary user. It is shown that the
spectrum sensing performance and SU throughput degrade with increase in the
primary traffic and the deep fade condition of the channel over which the
detection is performed. The degree of degradation in SU throughput is seen to
be severed further due to the interference link from the primary transmitter to
the secondary receiver. Under these detrimental effects, a sensing-throughput
trade-off for SU is illustrated. Finally, the combined effect of the primary
traffic, fading, imperfect spectrum sensing and the interference link from a
primary transmitter is studied on the outage probability at SU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2814</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2814</id><created>2014-05-12</created><updated>2014-07-08</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Probabilistic Band-Splitting for a Buffered Cooperative Cognitive
  Terminal</title><categories>cs.NI</categories><comments>Accepted in PIMRC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a cognitive protocol that involves cooperation
between the primary and secondary users. In addition to its own queue, the
secondary user (SU) has a queue to store, and then relay, the undelivered
primary packets. When the primary queue is nonempty, the SU remains idle and
attempts to decode the primary packet. When the primary queue is empty, the SU
splits the total channel bandwidth into two orthogonal subbands and assigns
each to a queue probabilistically. We show the advantage of the proposed
protocol over the prioritized cognitive relaying (PCR) protocol in which the SU
assigns a priority in transmission to the primary packets over its own packets.
We present two problem formulations, one based on throughput and the other on
delay. Both optimization problems are shown to be linear programs for a given
bandwidth assignment. Numerical results demonstrate the benefits of the
proposed protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2815</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2815</id><created>2014-05-12</created><updated>2014-07-27</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>On Orthogonal Band Allocation for Multi-User Multi-Band Cognitive Radio
  Networks: Stability Analysis</title><categories>cs.IT cs.NI math.IT</categories><comments>Conditional Acceptance in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the problem of band allocation of $M_s$ buffered
secondary users (SUs) to $M_p$ primary bands licensed to (owned by) $M_p$
buffered primary users (PUs). The bands are assigned to SUs in an orthogonal
(one-to-one) fashion such that neither band sharing nor multi-band allocations
are permitted. In order to study the stability region of the secondary network,
the optimization problem used to obtain the stability region's envelope
(closure) is established and is shown to be a linear program which can be
solved efficiently and reliably. We compare our orthogonal allocation system
with two typical low-complexity and intuitive band allocation systems. In one
system, each cognitive user chooses a band randomly in each time slot with some
assignment probability designed such that the system maintained stable, while
in the other system fixed (deterministic) band assignment is adopted throughout
the lifetime of the network. We derive the stability regions of these two
systems. We prove mathematically, as well as through numerical results, the
advantages of our proposed orthogonal system over the other two systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2816</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2816</id><created>2014-05-12</created><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Maximum Throughput of a Secondary User Cooperating with an Energy-Aware
  Primary User</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted WiOpt 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a cooperation protocol between a secondary user (SU) and
a primary user (PU) which dedicates a free frequency subband for the SU if
cooperation results in energy saving. Time is slotted and users are equipped
with buffers. Under the proposed protocol, the PU releases portion of its
bandwidth for secondary transmission. Moreover, it assigns a portion of the
time slot duration for the SU to relay primary packets and achieve a higher
successful packet reception probability at the primary receiver. We assume that
the PU has three states: idle, forward, and retransmission states. At each of
these states, the SU accesses the channel with adaptive transmission
parameters. The PU cooperates with the SU if and only if the achievable average
number of transmitted primary packets per joule is higher than the number of
transmitted packets per joule when it operates alone. The numerical results
show the beneficial gains of the proposed cooperative cognitive protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2820</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2820</id><created>2014-05-12</created><authors><author><keyname>Meneghetti</keyname><forenames>Alessio</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author><author><keyname>Tomasi</keyname><forenames>Alessandro</forenames></author></authors><title>A weight-distribution bound for entropy extractors using linear binary
  codes</title><categories>cs.IT math.IT math.PR</categories><comments>10 pages, 2 figures. submitted to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bound on the bias reduction of a random number generator by
processing based on binary linear codes. We introduce a new bound on the total
variation distance of the processed output based on the weight distribution of
the code generated by the chosen binary matrix. Starting from this result we
show a lower bound for the entropy rate of the output of linear binary
extractors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2822</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2822</id><created>2014-05-12</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Imitation-based Social Spectrum Sharing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum sharing is a promising technology for improving the spectrum
utilization. In this paper, we study how secondary users can share the spectrum
in a distributed fashion based on social imitations. The imitation-based
mechanism leverages the social intelligence of the secondary user crowd and
only requires a low computational power for each individual user. We introduce
the information sharing graph to model the social information sharing
relationship among the secondary users. We propose an imitative spectrum access
mechanism on a general information sharing graph such that each secondary user
first estimates its expected throughput based on local observations, and then
imitates the channel selection of another neighboring user who achieves a
higher throughput. We show that the imitative spectrum access mechanism
converges to an imitation equilibrium, where no beneficial imitation can be
further carried out on the time average. Numerical results show that the
imitative spectrum access mechanism can achieve efficient spectrum utilization
and meanwhile provide good fairness across secondary users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2826</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2826</id><created>2014-05-12</created><authors><author><keyname>Correa</keyname><forenames>Jos&#xe9; R.</forenames></author><author><keyname>Harks</keyname><forenames>Tobias</forenames></author><author><keyname>Kreuzen</keyname><forenames>Vincent J. C.</forenames></author><author><keyname>Matuschke</keyname><forenames>Jannik</forenames></author></authors><title>Fare Evasion in Transit Networks</title><categories>cs.GT cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public transit systems in urban areas usually require large state subsidies,
primarily due to high fare evasion rates. In this paper, we study new models
for optimizing fare inspection strategies in transit networks based on bilevel
programming. In the first level, the leader (the network operator) determines
probabilities for inspecting passengers at different locations, while in the
second level, the followers (the fare-evading passengers) respond by optimizing
their routes given the inspection probabilities and travel times. To model the
followers' behavior we study both a non-adaptive variant, in which passengers
select a path a priori and continue along it throughout their journey, and an
adaptive variant, in which they gain information along the way and use it to
update their route. For these problems, which are interesting in their own
right, we design exact and approximation algorithms and we prove a tight bound
of 3/4 on the ratio of the optimal cost between adaptive and non-adaptive
strategies. For the leader's optimization problem, we study a fixed-fare and a
flexible-fare variant, where ticket prices may or may not be set at the
operator's will. For the latter variant, we design an LP based approximation
algorithm. Finally, using a local search procedure that shifts inspection
probabilities within an initially determined support set, we perform an
extensive computational study for all variants of the problem on instances of
the Dutch railway and the Amsterdam subway network. This study reveals that our
solutions are within 95% of theoretical upper bounds drawn from the LP
relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2833</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2833</id><created>2014-05-12</created><updated>2015-05-21</updated><authors><author><keyname>Kumar</keyname><forenames>Akshay</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>On the Latency and Energy Efficiency of Erasure-Coded Cloud Storage
  Systems</title><categories>cs.DC</categories><comments>Submitted to IEEE Transactions on Cloud Computing. Contains 24 pages,
  13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increase in data storage and power consumption at data-centers has made
it imperative to design energy efficient Distributed Storage Systems (DSS). The
energy efficiency of DSS is strongly influenced not only by the volume of data,
frequency of data access and redundancy in data storage, but also by the
heterogeneity exhibited by the DSS in these dimensions. To this end, we propose
and analyze the energy efficiency of a heterogeneous distributed storage system
in which $n$ storage servers (disks) store the data of $R$ distinct classes.
Data of class $i$ is encoded using a $(n,k_{i})$ erasure code and the (random)
data retrieval requests can also vary across classes. We show that the energy
efficiency of such systems is closely related to the average latency and hence
motivates us to study the energy efficiency via the lens of average latency.
Through this connection, we show that erasure coding serves the dual purpose of
reducing latency and increasing energy efficiency. We present a queuing
theoretic analysis of the proposed model and establish upper and lower bounds
on the average latency for each data class under various scheduling policies.
Through extensive simulations, we present qualitative insights which reveal the
impact of coding rate, number of servers, service distribution and number of
redundant requests on the average latency and energy efficiency of the DSS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2846</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2846</id><created>2014-05-12</created><updated>2014-12-15</updated><authors><author><keyname>Berg</keyname><forenames>Ernst D.</forenames></author></authors><title>Introduction to Dynamic Unary Encoding</title><categories>cs.IT cs.DS math.IT</categories><comments>Seven pages of text, two pages of flow charts and two pages of data.
  Introduces an encoding scheme and a mathematical object</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic unary encoding takes unary encoding to the next level. Every n-bit
binary string is an encoding of dynamic unary and every n-bit binary string is
encodable by dynamic unary. By utilizing both forms of unary code and a single
bit of parity information dynamic unary encoding partitions 2^n non-negative
integers into n sets of disjoint cycles of n-bit elements. These cycles have
been employed as virtual data sets, binary transforms and as a mathematical
object. Characterization of both the cycles and of the cycle spectrum is given.
Examples of encoding and decoding algorithms are given. Examples of other
constructs utilizing the principles of dynamic unary encoding are presented.
The cycle as a mathematical object is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2848</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2848</id><created>2014-05-12</created><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Orsi</keyname><forenames>Giorgio</forenames></author><author><keyname>Pieris</keyname><forenames>Andreas</forenames></author></authors><title>Query Rewriting and Optimization for Ontological Databases</title><categories>cs.DB</categories><comments>arXiv admin note: text overlap with arXiv:1312.5914 by other authors</comments><msc-class>68P15</msc-class><acm-class>H.2.4; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontological queries are evaluated against a knowledge base consisting of an
extensional database and an ontology (i.e., a set of logical assertions and
constraints which derive new intensional knowledge from the extensional
database), rather than directly on the extensional database. The evaluation and
optimization of such queries is an intriguing new problem for database
research. In this paper, we discuss two important aspects of this problem:
query rewriting and query optimization. Query rewriting consists of the
compilation of an ontological query into an equivalent first-order query
against the underlying extensional database. We present a novel query rewriting
algorithm for rather general types of ontological constraints which is
well-suited for practical implementations. In particular, we show how a
conjunctive query against a knowledge base, expressed using linear and sticky
existential rules, that is, members of the recently introduced Datalog+/-
family of ontology languages, can be compiled into a union of conjunctive
queries (UCQ) against the underlying database. Ontological query optimization,
in this context, attempts to improve this rewriting process so to produce
possibly small and cost-effective UCQ rewritings for an input query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2850</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2850</id><created>2014-05-09</created><updated>2014-05-14</updated><authors><author><keyname>Areias</keyname><forenames>Miguel</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author></authors><title>A Simple and Efficient Lock-Free Hash Trie Design for Concurrent Tabling</title><categories>cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A critical component in the implementation of a concurrent tabling system is
the design of the table space. One of the most successful proposals for
representing tables is based on a two-level trie data structure, where one trie
level stores the tabled subgoal calls and the other stores the computed
answers. In this work, we present a simple and efficient lock-free design where
both levels of the tries can be shared among threads in a concurrent
environment. To implement lock-freedom we took advantage of the CAS atomic
instruction that nowadays can be widely found on many common architectures. CAS
reduces the granularity of the synchronization when threads access concurrent
areas, but still suffers from low-level problems such as false sharing or cache
memory side-effects. In order to be as effective as possible in the concurrent
search and insert operations over the table space data structures, we based our
design on a hash trie data structure in such a way that it minimizes potential
low-level synchronization problems by dispersing as much as possible the
concurrent areas. Experimental results in the Yap Prolog system show that our
new lock-free hash trie design can effectively reduce the execution time and
scale better than previous designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2852</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2852</id><created>2014-05-12</created><authors><author><keyname>Chen</keyname><forenames>Taolue</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>On the Total Variation Distance of Labelled Markov Chains</title><categories>cs.LO</categories><comments>This is a technical report for a LICS'14 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labelled Markov chains (LMCs) are widely used in probabilistic verification,
speech recognition, computational biology, and many other fields. Checking two
LMCs for equivalence is a classical problem subject to extensive studies, while
the total variation distance provides a natural measure for the &quot;inequivalence&quot;
of two LMCs: it is the maximum difference between probabilities that the LMCs
assign to the same event.
  In this paper we develop a theory of the total variation distance between two
LMCs, with emphasis on the algorithmic aspects: (1) we provide a
polynomial-time algorithm for determining whether two LMCs have distance 1,
i.e., whether they can almost always be distinguished; (2) we provide an
algorithm for approximating the distance with arbitrary precision; and (3) we
show that the threshold problem, i.e., whether the distance exceeds a given
threshold, is NP-hard and hard for the square-root-sum problem. We also make a
connection between the total variation distance and Bernoulli convolutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2856</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2856</id><created>2014-05-12</created><authors><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Cowls</keyname><forenames>Josh</forenames></author><author><keyname>Meyer</keyname><forenames>Eric T.</forenames></author><author><keyname>Schroeder</keyname><forenames>Ralph</forenames></author><author><keyname>Margetts</keyname><forenames>Helen</forenames></author></authors><title>Mapping the UK Webspace: Fifteen Years of British Universities on the
  Web</title><categories>cs.DL cs.CY physics.soc-ph</categories><comments>To appear in the proceeding of WebSci 2014</comments><doi>10.1145/2615569.2615691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper maps the national UK web presence on the basis of an analysis of
the .uk domain from 1996 to 2010. It reviews previous attempts to use web
archives to understand national web domains and describes the dataset. Next, it
presents an analysis of the .uk domain, including the overall number of links
in the archive and changes in the link density of different second-level
domains over time. We then explore changes over time within a particular
second-level domain, the academic subdomain .ac.uk, and compare linking
practices with variables, including institutional affiliation, league table
ranking, and geographic location. We do not detect institutional affiliation
affecting linking practices and find only partial evidence of league table
ranking affecting network centrality, but find a clear inverse relationship
between the density of links and the geographical distance between
universities. This echoes prior findings regarding offline academic activity,
which allows us to argue that real-world factors like geography continue to
shape academic relationships even in the Internet age. We conclude with
directions for future uses of web archive resources in this emerging area of
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2861</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2861</id><created>2014-05-12</created><updated>2015-08-19</updated><authors><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Narayanan</keyname><forenames>Ashok</forenames></author><author><keyname>Oran</keyname><forenames>David</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Wood</keyname><forenames>Christopher A.</forenames></author></authors><title>Secure Fragmentation for Content-Centric Networks (extended version)</title><categories>cs.NI cs.CR</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-Centric Networking (CCN) is a communication paradigm that emphasizes
content distribution. Named-Data Networking (NDN) is an instantiation of CCN, a
candidate Future Internet Architecture. NDN supports human-readable content
naming and router-based content caching which lends itself to efficient,
secure, and scalable content distribution. Because of NDN's fundamental
requirement that each content object must be signed by its producer,
fragmentation has been considered incompatible with NDN since it precludes
authentication of individual content fragments by routers. The alternative is
to perform hop-by-hop reassembly, which incurs prohibitive delays. In this
paper, we show that secure and efficient content fragmentation is both possible
and even advantageous in NDN and similar content-centric network architectures
that involve signed content. We design a concrete technique that facilitates
efficient and secure content fragmentation in NDN, discuss its security
guarantees and assess performance. We also describe a prototype implementation
and compare performance of cut-through with hop-by-hop fragmentation and
reassembly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2872</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2872</id><created>2014-05-12</created><authors><author><keyname>Papadopoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Kurniawati</keyname><forenames>Hanna</forenames></author><author><keyname>Patrikalakis</keyname><forenames>Nicholas M.</forenames></author></authors><title>Analysis of Asymptotically Optimal Sampling-based Motion Planning
  Algorithms for Lipschitz Continuous Dynamical Systems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last 20 years significant effort has been dedicated to the
development of sampling-based motion planning algorithms such as the
Rapidly-exploring Random Trees (RRT) and its asymptotically optimal version
(e.g. RRT*). However, asymptotic optimality for RRT* only holds for linear and
fully actuated systems or for a small number of non-linear systems (e.g.
Dubin's car) for which a steering function is available. The purpose of this
paper is to show that asymptotically optimal motion planning for dynamical
systems with differential constraints can be achieved without the use of a
steering function. We develop a novel analysis on sampling-based planning
algorithms that sample the control space. This analysis demonstrated that
asymptotically optimal path planning for any Lipschitz continuous dynamical
system can be achieved by sampling the control space directly. We also
determine theoretical bounds on the convergence rates for this class of
algorithms. As the number of iterations increases, the trajectory generated by
these algorithms, approaches the optimal control trajectory, with probability
one. Simulation results are promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2874</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2874</id><created>2014-05-12</created><updated>2014-12-29</updated><authors><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames><affiliation>Queen Mary University of London</affiliation></author></authors><title>A Study of Entanglement in a Categorical Framework of Natural Language</title><categories>cs.CL cs.AI math.CT quant-ph</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 249-261</journal-ref><doi>10.4204/EPTCS.172.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In both quantum mechanics and corpus linguistics based on vector spaces, the
notion of entanglement provides a means for the various subsystems to
communicate with each other. In this paper we examine a number of
implementations of the categorical framework of Coecke, Sadrzadeh and Clark
(2010) for natural language, from an entanglement perspective. Specifically,
our goal is to better understand in what way the level of entanglement of the
relational tensors (or the lack of it) affects the compositional structures in
practical situations. Our findings reveal that a number of proposals for verb
construction lead to almost separable tensors, a fact that considerably
simplifies the interactions between the words. We examine the ramifications of
this fact, and we show that the use of Frobenius algebras mitigates the
potential problems to a great extent. Finally, we briefly examine a machine
learning method that creates verb tensors exhibiting a sufficient level of
entanglement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2875</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2875</id><created>2014-05-12</created><updated>2015-09-02</updated><authors><author><keyname>Ho</keyname><forenames>Chien-Ju</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms
  for Repeated Principal-Agent Problems</title><categories>cs.DS cs.GT cs.LG</categories><comments>This is the full version of a paper in the ACM Conference on
  Economics and Computation (ACM-EC), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing markets have emerged as a popular platform for matching
available workers with tasks to complete. The payment for a particular task is
typically set by the task's requester, and may be adjusted based on the quality
of the completed work, for example, through the use of &quot;bonus&quot; payments. In
this paper, we study the requester's problem of dynamically adjusting
quality-contingent payments for tasks. We consider a multi-round version of the
well-known principal-agent model, whereby in each round a worker makes a
strategic choice of the effort level which is not directly observable by the
requester. In particular, our formulation significantly generalizes the
budget-free online task pricing problems studied in prior work.
  We treat this problem as a multi-armed bandit problem, with each &quot;arm&quot;
representing a potential contract. To cope with the large (and in fact,
infinite) number of arms, we propose a new algorithm, AgnosticZooming, which
discretizes the contract space into a finite number of regions, effectively
treating each region as a single arm. This discretization is adaptively
refined, so that more promising regions of the contract space are eventually
discretized more finely. We analyze this algorithm, showing that it achieves
regret sublinear in the time horizon and substantially improves over
non-adaptive discretization (which is the only competing approach in the
literature).
  Our results advance the state of art on several different topics: the theory
of crowdsourcing markets, principal-agent problems, multi-armed bandits, and
dynamic pricing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2876</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2876</id><created>2014-05-12</created><updated>2014-09-28</updated><authors><author><keyname>Sakr</keyname><forenames>Ahmed Hamdi</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Location-Aware Cross-Tier Coordinated Multipoint Transmission in
  Two-Tier Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tier cellular networks are considered as an effective solution to
enhance the coverage and data rate offered by cellular systems. In a multi-tier
network, high power base stations (BSs) such as macro BSs are overlaid by lower
power small cells such as femtocells and/or picocells. However, co-channel
deployment of multiple tiers of BSs gives rise to the problem of cross-tier
interference that significantly impacts the performance of wireless networks.
Multicell cooperation techniques, such as coordinated multipoint (CoMP)
transmission, have been proposed as a promising solution to mitigate the impact
of the cross-tier interference in multi-tier networks. In this paper, we
propose a novel scheme for Location-Aware Cross-Tier Cooperation (LA-CTC)
between BSs in different tiers for downlink CoMP transmission in two-tier
cellular networks. On one hand, the proposed scheme only uses CoMP transmission
to enhance the performance of the users who suffer from high cross-tier
interference due to the co-channel deployment of small cells such as picocells.
On the other hand, users with good signal-to-interference-plus-noise ratio
(${\rm SINR}$) conditions are served directly by a single BS from any of the
two tiers. Thus, the data exchange between the cooperating BSs over the
backhaul network can be reduced when compared to the traditional CoMP
transmission scheme. We use tools from stochastic geometry to quantify the
performance gains obtained by using the proposed scheme in terms of outage
probability, achievable data rate, and load per BS. We compare the performance
of the proposed scheme with that of other schemes in the literature such as the
schemes which use cooperation to serve all users and schemes that use range
expansion to offload users to the small cell tier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2878</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2878</id><created>2014-05-12</created><authors><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Approximate Policy Iteration Schemes: A Comparison</title><categories>cs.AI cs.LG stat.ML</categories><comments>ICML (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on several approximate
variations of the Policy Iteration algorithm: Approximate Policy Iteration,
Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search
by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),
and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all
algorithms, we describe performance bounds, and make a comparison by paying a
particular attention to the concentrability constants involved, the number of
iterations and the memory required. Our analysis highlights the following
points: 1) The performance guarantee of CPI can be arbitrarily better than that
of API/API($\alpha$), but this comes at the cost of a relative---exponential in
$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$
enjoys the best of both worlds: its performance guarantee is similar to that of
CPI, but within a number of iterations similar to that of API. 3) Contrary to
API that requires a constant memory, the memory needed by CPI and PSDP$_\infty$
is proportional to their number of iterations, which may be problematic when
the discount factor $\gamma$ is close to 1 or the approximation error
$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make
an overall trade-off between memory and performance. Simulations with these
schemes confirm our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2883</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2883</id><created>2014-05-12</created><authors><author><keyname>Talamadupula</keyname><forenames>Kartik</forenames></author><author><keyname>Smith</keyname><forenames>David E.</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>The Metrics Matter! On the Incompatibility of Different Flavors of
  Replanning</title><categories>cs.AI cs.MA</categories><comments>Prior version appears in DMAP 2013 at ICAPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When autonomous agents are executing in the real world, the state of the
world as well as the objectives of the agent may change from the agent's
original model. In such cases, the agent's planning process must modify the
plan under execution to make it amenable to the new conditions, and to resume
execution. This brings up the replanning problem, and the various techniques
that have been proposed to solve it. In all, three main techniques -- based on
three different metrics -- have been proposed in prior automated planning work.
An open question is whether these metrics are interchangeable; answering this
requires a normalized comparison of the various replanning quality metrics. In
this paper, we show that it is possible to support such a comparison by
compiling all the respective techniques into a single substrate. Using this
novel compilation, we demonstrate that these different metrics are not
interchangeable, and that they are not good surrogates for each other. Thus we
focus attention on the incompatibility of the various replanning flavors with
each other, founded in the differences between the metrics that they
respectively seek to optimize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2891</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2891</id><created>2014-04-15</created><authors><author><keyname>Bova</keyname><forenames>Simone</forenames></author><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Model Checking Existential Logic on Partially Ordered Sets</title><categories>cs.LO cs.DS</categories><comments>accepted at CSL-LICS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of checking whether an existential sentence (that is, a
first-order sentence in prefix form built using existential quantifiers and all
Boolean connectives) is true in a finite partially ordered set (in short, a
poset). A poset is a reflexive, antisymmetric, and transitive digraph. The
problem encompasses the fundamental embedding problem of finding an isomorphic
copy of a poset as an induced substructure of another poset.
  Model checking existential logic is already NP-hard on a fixed poset; thus we
investigate structural properties of posets yielding conditions for
fixed-parameter tractability when the problem is parameterized by the sentence.
We identify width as a central structural property (the width of a poset is the
maximum size of a subset of pairwise incomparable elements); our main
algorithmic result is that model checking existential logic on classes of
finite posets of bounded width is fixed-parameter tractable. We observe a
similar phenomenon in classical complexity, where we prove that the isomorphism
problem is polynomial-time tractable on classes of posets of bounded width;
this settles an open problem in order theory.
  We surround our main algorithmic result with complexity results on less
restricted, natural neighboring classes of finite posets, establishing its
tightness in this sense. We also relate our work with (and demonstrate its
independence of) fundamental fixed-parameter tractability results for model
checking on digraphs of bounded degree and bounded clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2892</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2892</id><created>2014-05-12</created><updated>2015-08-04</updated><authors><author><keyname>Bedn&#xe1;rov&#xe1;</keyname><forenames>Zuzana</forenames></author><author><keyname>Geffert</keyname><forenames>Viliam</forenames></author><author><keyname>Reinhardt</keyname><forenames>Klaus</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>New Results on the Minimum Amount of Useful Space</title><categories>cs.FL cs.CC quant-ph</categories><comments>21 pages. An extended and revised version with two new authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several new results on minimal space requirements to recognize a
nonregular language: (i) realtime nondeterministic Turing machines can
recognize a nonregular unary language within weak $\log\log n$ space, (ii)
$\log\log n$ is a tight space lower bound for accepting general nonregular
languages on weak realtime pushdown automata, (iii) there exist unary
nonregular languages accepted by realtime alternating one-counter automata
within weak $\log n$ space, (iv) there exist nonregular languages accepted by
two-way deterministic pushdown automata within strong $\log\log n$ space, and,
(v) there exist unary nonregular languages accepted by two-way one-counter
automata using quantum and classical states with middle $\log n$ space and
bounded error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2894</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2894</id><created>2014-05-12</created><authors><author><keyname>Kadhe</keyname><forenames>Swanand</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>Weakly Secure Regenerating Codes for Distributed Storage</title><categories>cs.IT math.IT</categories><comments>Extended version of the paper accepted in NetCod 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of secure distributed data storage under the paradigm
of \emph{weak security}, in which no \emph{meaningful information} is leaked to
the eavesdropper. More specifically, the eavesdropper cannot get any
information about any individual message file or a small group of files. The
key benefit of the weak security paradigm is that it incurs no loss in the
storage capacity, which makes it practically appealing.
  In this paper, we present a coding scheme, using a coset coding based outer
code and a Product-Matrix Minimum Bandwidth Regenerating code (proposed by
Rashmi et al.) as an inner code, that achieves weak security when the
eavesdropper can observe any single storage node. We show that the proposed
construction has good security properties and requires small finite field size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2906</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2906</id><created>2014-05-12</created><authors><author><keyname>Chaudhary</keyname><forenames>Sushant</forenames></author></authors><title>Load Frequency Control For Distributed Grid Power System Single Area &amp;
  Multi-area System</title><categories>cs.SY</categories><comments>4Pages, 14 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project presents decentralized control scheme for Load-Frequency Control
in power System. In this era renewable energy is most promising solution to
man's ever increasing energy needs. But the power production by these resources
cannot be controlled unlike in thermal plants. A number of optimal control
techniques are adopted to implement a reliable stabilizing controller. It is
necessary to interconnect more distributed generation in power systems because
of environmental concerns. Primary concern includes global environmental and
energy depletion problem. A serious attempt has been undertaken aiming a
investigating the load frequency control problem in a power system consisting
of two power generation unit and multiple variable load units. The robustness
and reliability of the various control schemes is examined through simulations.
A system involving thermal plants and a hydro plant is modeled using MAT LAB
2011b.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2907</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2907</id><created>2014-05-12</created><authors><author><keyname>Lari</keyname><forenames>Vahid</forenames></author><author><keyname>Tanase</keyname><forenames>Alexandru</forenames></author><author><keyname>Hannig</keyname><forenames>Frank</forenames></author><author><keyname>Teich</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Massively Parallel Processor Architectures for Resource-aware Computing</title><categories>cs.AR cs.DC</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a class of massively parallel processor architectures called
invasive tightly coupled processor arrays (TCPAs). The presented processor
class is a highly parameterizable template, which can be tailored before
runtime to fulfill costumers' requirements such as performance, area cost, and
energy efficiency. These programmable accelerators are well suited for
domain-specific computing from the areas of signal, image, and video processing
as well as other streaming processing applications. To overcome future scaling
issues (e.g., power consumption, reliability, resource management, as well as
application parallelization and mapping), TCPAs are inherently designed in a
way to support self-adaptivity and resource awareness at hardware level. Here,
we follow a recently introduced resource-aware parallel computing paradigm
called invasive computing where an application can dynamically claim, execute,
and release resources. Furthermore, we show how invasive computing can be used
as an enabler for power management. Finally, we will introduce ideas on how to
realize fault-tolerant loop execution on such massively parallel architectures
through employing on-demand spatial redundancies at the processor array level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2908</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2908</id><created>2014-05-12</created><authors><author><keyname>Paul</keyname><forenames>Johny</forenames></author><author><keyname>Stechele</keyname><forenames>Walter</forenames></author><author><keyname>Kr&#xf6;hnert</keyname><forenames>Manfred</forenames></author><author><keyname>Asfour</keyname><forenames>Tamim</forenames></author></authors><title>Resource-Aware Programming for Robotic Vision</title><categories>cs.CV cs.DC cs.RO</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanoid robots are designed to operate in human centered environments. They
face changing, dynamic environments in which they need to fulfill a multitude
of challenging tasks. Such tasks differ in complexity, resource requirements,
and execution time. Latest computer architectures of humanoid robots consist of
several industrial PCs containing single- or dual-core processors. According to
the SIA roadmap for semiconductors, many-core chips with hundreds to thousands
of cores are expected to be available in the next decade. Utilizing the full
power of a chip with huge amounts of resources requires new computing paradigms
and methodologies.
  In this paper, we analyze a resource-aware computing methodology named
Invasive Computing, to address these challenges. The benefits and limitations
of the new programming model is analyzed using two widely used computer vision
algorithms, the Harris Corner detector and SIFT (Scale Invariant Feature
Transform) feature matching. The result indicate that the new programming model
together with the extensions within the application layer, makes them highly
adaptable; leading to better quality in the results obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2909</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2909</id><created>2014-05-12</created><authors><author><keyname>Glocker</keyname><forenames>Elisabeth</forenames></author><author><keyname>Chen</keyname><forenames>Qingqing</forenames></author><author><keyname>Zaidi</keyname><forenames>Asheque M.</forenames></author><author><keyname>Schlichtmann</keyname><forenames>Ulf</forenames></author><author><keyname>Schmitt-Landsiedel</keyname><forenames>Doris</forenames></author></authors><title>Emulated ASIC Power and Temperature Monitor System for FPGA Prototyping
  of an Invasive MPSoC Computing Architecture</title><categories>cs.AR</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution the emulation of an ASIC temperature and power
monitoring system (TPMon) for FPGA prototyping is presented and tested to
control processor temperatures under different control targets and operating
strategies. The approach for emulating the power monitor is based on an
instruction-level energy model. For emulating the temperature monitor, a
thermal RC model is used. The monitoring system supplies an invasive MPSoC
computing architecture with hardware status information (power and temperature
data of the processors within the system). These data are required for
resource-aware load distribution. As a proof of concept different operating
strategies and control targets were evaluated for a 2-tile invasive MPSoC
computing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2910</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2910</id><created>2014-05-12</created><authors><author><keyname>Mattes</keyname><forenames>Oliver</forenames></author><author><keyname>Karl</keyname><forenames>Wolfgang</forenames></author></authors><title>Evaluating the Self-Optimization Process of the Adaptive Memory
  Management Architecture Self-aware Memory</title><categories>cs.DC</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the continuously increasing integration level, manycore processor
systems are likely to be the coming system structure not only in HPC but also
for desktop or mobile systems. Nowadays manycore processors like Tilera TILE,
KALRAY MPPA or Intel SCC combine a rising number of cores in a tiled
architecture and are mainly designed for high performance applications with
focus on direct inter-core communication. The current architectures have
limitations by central or sparse components like memory controllers, memory I/O
or inflexible memory management.
  In the future highly dynamic workloads with multiple concurrently running
applications, changing I/O characteristics and a not predictable memory usage
have to be utilized on these manycore systems. Consequently the memory
management has to become more flexible and distributed in nature and adaptive
mechanisms and system structures are needed. With Self-aware Memory (SaM), a
decentralized, scalable and autonomous self-optimizing memory architecture is
developed. This adaptive memory management can achieve higher flexibility and
an easy usage of memory.
  In this paper the concept of an ongoing decentralized self-optimization is
introduced and the evaluation of its various parameters is presented. The
results show that the overhead of the decentralized optimization process is
amortized by the optimized runtime using the appropriate parameter settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2911</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2911</id><created>2014-05-12</created><authors><author><keyname>Kr&#xf6;hnert</keyname><forenames>Manfred</forenames></author><author><keyname>Vahrenkamp</keyname><forenames>Nikolaus</forenames></author><author><keyname>Paul</keyname><forenames>Johny</forenames></author><author><keyname>Stechele</keyname><forenames>Walter</forenames></author><author><keyname>Asfour</keyname><forenames>Tamim</forenames></author></authors><title>Resource Prediction for Humanoid Robots</title><categories>cs.RO</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanoid robots are designed to operate in human centered environments where
they execute a multitude of challenging tasks, each differing in complexity,
resource requirements, and execution time. In such highly dynamic surroundings
it is desirable to anticipate upcoming situations in order to predict future
resource requirements such as CPU or memory usage. Resource prediction
information is essential for detecting upcoming resource bottlenecks or
conflicts and can be used enhance resource negotiation processes or to perform
speculative resource allocation.
  In this paper we present a prediction model based on Markov chains for
predicting the behavior of the humanoid robot ARMAR-III in human robot
interaction scenarios. Robot state information required by the prediction
algorithm is gathered through self-monitoring and combined with environmental
context information. Adding resource profiles allows generating probability
distributions of possible future resource demands. Online learning of model
parameters is made possible through disclosure mechanisms provided by the robot
framework ArmarX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2912</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2912</id><created>2014-05-12</created><authors><author><keyname>Kicherer</keyname><forenames>Mario</forenames></author><author><keyname>Karl</keyname><forenames>Wolfgang</forenames></author></authors><title>Heterogeneity-aware Fault Tolerance using a Self-Organizing Runtime
  System</title><categories>cs.OS</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the diversity and implicit redundancy in terms of processing units and
compute kernels, off-the-shelf heterogeneous systems offer the opportunity to
detect and tolerate faults during task execution in hardware as well as in
software. To automatically leverage this diversity, we introduce an extension
of an online-learning runtime system that combines the benefits of the existing
performance-oriented task mapping with task duplication, a diversity-oriented
mapping strategy and heterogeneity-aware majority voter. This extension uses a
new metric to dynamically rate the remaining benefit of unreliable processing
units and a memory management mechanism for automatic data transfers and
checkpointing in the host and device memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2913</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2913</id><created>2014-05-12</created><authors><author><keyname>D&#xf6;bel</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Muschner</keyname><forenames>Robert</forenames></author><author><keyname>H&#xe4;rtig</keyname><forenames>Hermann</forenames></author></authors><title>Resource-Aware Replication on Heterogeneous Multicores: Challenges and
  Opportunities</title><categories>cs.DC cs.OS</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decreasing hardware feature sizes and increasing heterogeneity in multicore
hardware require software that can adapt to these platforms' properties. We
implemented ROMAIN, an OS service providing redundant multithreading on top of
the FIASCO.OC microkernel to address the increasing unreliability of hardware.
In this paper we review challenges and opportunities for ROMAIN to adapt to
such multicore platforms in order to decrease execution overhead, resource
requirements, and vulnerability against faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2914</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2914</id><created>2014-05-12</created><authors><author><keyname>Aliee</keyname><forenames>Hananeh</forenames></author><author><keyname>Chen</keyname><forenames>Liang</forenames></author><author><keyname>Ebrahimi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Gla&#xdf;</keyname><forenames>Michael</forenames></author><author><keyname>Khosravi</keyname><forenames>Faramarz</forenames></author><author><keyname>Tahoori</keyname><forenames>Mehdi B.</forenames></author></authors><title>Towards Cross-layer Reliability Analysis of Transient and Permanent
  Faults</title><categories>cs.DC</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing complexity of Multi-Processor Systems on Chip (MPSoCs),
system-level design methodologies have got a lot of attention in recent years.
However, the significant gap between the system-level reliability analysis and
the level where the actual faults occur necessitates a cross-layer approach in
which the sufficient data about the effects of faults at low levels are passed
to the system level. So far, the cross-layer reliability analysis techniques
focus on a specific type of faults, e.g., either permanent or transient faults.
In this work, we aim at proposing a cross-layer reliability analysis which
considers different fault types concurrently and connects reliability analysis
techniques at different levels of abstraction using adapters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2915</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2915</id><created>2014-05-12</created><authors><author><keyname>Kessler</keyname><forenames>Christoph</forenames></author><author><keyname>Dastgeer</keyname><forenames>Usman</forenames></author><author><keyname>Li</keyname><forenames>Lu</forenames></author></authors><title>Optimized Composition: Generating Efficient Code for Heterogeneous
  Systems from Multi-Variant Components, Skeletons and Containers</title><categories>cs.DC</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this survey paper, we review recent work on frameworks for the high-level,
portable programming of heterogeneous multi-/manycore systems (especially,
GPU-based systems) using high-level constructs such as annotated user-level
software components, skeletons (i.e., predefined generic components) and
containers, and discuss the optimization problems that need to be considered in
selecting among multiple implementation variants, generating code and providing
runtime support for efficient execution on such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2916</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2916</id><created>2014-05-12</created><authors><author><keyname>Drebes</keyname><forenames>Andi</forenames></author><author><keyname>Heydemann</keyname><forenames>Karine</forenames></author><author><keyname>Pop</keyname><forenames>Antoniu</forenames></author><author><keyname>Cohen</keyname><forenames>Albert</forenames></author><author><keyname>Drach</keyname><forenames>Nathalie</forenames></author></authors><title>Automatic Detection of Performance Anomalies in Task-Parallel Programs</title><categories>cs.DC cs.PF cs.SE</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To efficiently exploit the resources of new many-core architectures,
integrating dozens or even hundreds of cores per chip, parallel programming
models have evolved to expose massive amounts of parallelism, often in the form
of fine-grained tasks. Task-parallel languages, such as OpenStream, X10,
Habanero Java and C or StarSs, simplify the development of applications for new
architectures, but tuning task-parallel applications remains a major challenge.
Performance bottlenecks can occur at any level of the implementation, from the
algorithmic level (e.g., lack of parallelism or over-synchronization), to
interactions with the operating and runtime systems (e.g., data placement on
NUMA architectures), to inefficient use of the hardware (e.g., frequent cache
misses or misaligned memory accesses); detecting such issues and determining
the exact cause is a difficult task.
  In previous work, we developed Aftermath, an interactive tool for trace-based
performance analysis and debugging of task-parallel programs and run-time
systems. In contrast to other trace-based analysis tools, such as Paraver or
Vampir, Aftermath offers native support for tasks, i.e., visualization,
statistics and analysis tools adapted for performance debugging at task
granularity. However, the tool currently does not provide support for the
automatic detection of performance bottlenecks and it is up to the user to
investigate the relevant aspects of program execution by focusing the
inspection on specific slices of a trace file. In this paper, we present
ongoing work on two extensions that guide the user through this process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2917</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2917</id><created>2014-05-12</created><authors><author><keyname>Zaib</keyname><forenames>Aurang</forenames></author><author><keyname>Raju</keyname><forenames>Prashanth</forenames></author><author><keyname>Wild</keyname><forenames>Thomas</forenames></author><author><keyname>Herkersdorf</keyname><forenames>Andreas</forenames></author></authors><title>A Layered Modeling and Simulation Approach to investigate Resource-aware
  Computing in MPSoCs</title><categories>cs.OH</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing complexity of modern multi-processor system on chip (MPSoC) and
the decreasing feature size have introduced new challenges. System designers
have to consider now aspects which were not part of the design process in past
times. Resource-aware Computing is one of such emerging design concerns which
can help to improve performance, dependability and resource utilization of
overall system. Resource-aware execution takes into account the resource status
when executing tasks on MPSoCs. Exploration of resource-aware computing at
early design stages of complex systems is mandatory and appropriate
methodologies to do this in an efficient manner are thus required. In this
paper, we present a modular approach which provides modeling and simulation
support for investigation of resource-aware execution in MPSoCs. The proposed
methodology enables rapid exploration of the design space by modeling and
simulating the resource-awareness in a separate layer while widely reusing the
legacy system model in the other layer. Our experiments illustrate the benefits
of our approach for the exploration of resource-aware execution on MPSoCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2918</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2918</id><created>2014-05-12</created><authors><author><keyname>Karcher</keyname><forenames>Thomas</forenames></author><author><keyname>Guckes</keyname><forenames>Christopher</forenames></author><author><keyname>Tichy</keyname><forenames>Walter F.</forenames></author></authors><title>Autotuning and Self-Adaptability in Concurrency Libraries</title><categories>cs.SE</categories><comments>Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</comments><proxy>Frank Hannig</proxy><report-no>Racing/2014/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autotuning is an established technique for optimizing the performance of
parallel applications. However, programmers must prepare applications for
autotuning, which is tedious and error prone coding work. We demonstrate how
applications become ready for autotuning with few or no modifications by
extending Threading Building Blocks (TBB), a library for parallel programming,
with autotuning. The extended TBB library optimizes all application-independent
tuning parameters fully automatically. We compare manual effort, autotuning
overhead and performance gains on 17 examples. While some examples benefit only
slightly, others speed up by 28% over standard TBB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2936</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2936</id><created>2014-05-12</created><authors><author><keyname>Daneshmand</keyname><forenames>Hadi</forenames></author><author><keyname>Gomez-Rodriguez</keyname><forenames>Manuel</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Estimating Diffusion Network Structures: Recovery Conditions, Sample
  Complexity &amp; Soft-thresholding Algorithm</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>To appear in the 31st International Conference on Machine Learning
  (ICML), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information spreads across social and technological networks, but often the
network structures are hidden from us and we only observe the traces left by
the diffusion processes, called cascades. Can we recover the hidden network
structures from these observed cascades? What kind of cascades and how many
cascades do we need? Are there some network structures which are more difficult
than others to recover? Can we design efficient inference algorithms with
provable guarantees?
  Despite the increasing availability of cascade data and methods for inferring
networks from these data, a thorough theoretical understanding of the above
questions remains largely unexplored in the literature. In this paper, we
investigate the network structure inference problem for a general family of
continuous-time diffusion models using an $l_1$-regularized likelihood
maximization framework. We show that, as long as the cascade sampling process
satisfies a natural incoherence condition, our framework can recover the
correct network structure with high probability if we observe $O(d^3 \log N)$
cascades, where $d$ is the maximum number of parents of a node and $N$ is the
total number of nodes. Moreover, we develop a simple and efficient
soft-thresholding inference algorithm, which we use to illustrate the
consequences of our theoretical results, and show that our framework
outperforms other alternatives in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2941</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2941</id><created>2014-05-12</created><authors><author><keyname>wang</keyname><forenames>Jiang</forenames></author><author><keyname>Nie</keyname><forenames>Xiaohan</forenames></author><author><keyname>Xia</keyname><forenames>Yin</forenames></author><author><keyname>Wu</keyname><forenames>Ying</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Cross-view Action Modeling, Learning and Recognition</title><categories>cs.CV</categories><comments>CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing methods on video-based action recognition are generally
view-dependent, i.e., performing recognition from the same views seen in the
training data. We present a novel multiview spatio-temporal AND-OR graph
(MST-AOG) representation for cross-view action recognition, i.e., the
recognition is performed on the video from an unknown and unseen view. As a
compositional model, MST-AOG compactly represents the hierarchical
combinatorial structures of cross-view actions by explicitly modeling the
geometry, appearance and motion variations. This paper proposes effective
methods to learn the structure and parameters of MST-AOG. The inference based
on MST-AOG enables action recognition from novel views. The training of MST-AOG
takes advantage of the 3D human skeleton data obtained from Kinect cameras to
avoid annotating enormous multi-view video frames, which is error-prone and
time-consuming, but the recognition does not need 3D information and is based
on 2D video input. A new Multiview Action3D dataset has been created and will
be released. Extensive experiments have demonstrated that this new action
representation significantly improves the accuracy and robustness for
cross-view action recognition on 2D videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2948</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2948</id><created>2014-02-02</created><authors><author><keyname>Deng</keyname><forenames>H. Lydia</forenames></author><author><keyname>Scales</keyname><forenames>John A.</forenames></author></authors><title>Characterizing the Topography of Multi-dimensional Energy Landscapes</title><categories>cs.NA math.NA</categories><comments>14 figures and an appendix containing proof of main result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic issue in optimization, inverse theory,neural networks, computational
chemistry and many other problems is the geometrical characterization of high
dimensional functions. In inverse calculations one aims to characterize the set
of models that fit the data (among other constraints). If the data misfit
function is unimodal then one can find its peak by local optimization methods
and characterize its width (related to the range of data-fitting models) by
estimating derivatives at this peak. On the other hand, if there are local
extrema, then a number of interesting and difficult problems arise. Are the
local extrema important compared to the global or can they be eliminated (e.g.,
by smoothing) without significant loss of information? Is there a sufficiently
small number of local extrema that they can be enumerated via local
optimization? What are the basins of attraction of these local extrema? Can two
extrema be joined by a path that never goes uphill? Can the whole problem be
reduced to one of enumerating the local extrema and their basins of attraction?
For locally ill-conditioned functions, premature convergence of local
optimization can be confused with the presence of local extrema. Addressing any
of these issues requires topographic information about the functions under
study. But in many applications these functions may have hundreds or thousands
of variables and can only be evaluated pointwise (by some numerical method for
instance). In this paper we describe systematic (but generic) methods of
analysing the topography of high dimensional functions using local optimization
methods applied to randomly chosen starting models. We provide a number of
quantitative measures of function topography that have proven to be useful in
practical problems along with error estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2957</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2957</id><created>2014-05-12</created><authors><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Buzzi</keyname><forenames>Stefano</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author><author><keyname>Hanly</keyname><forenames>Stephen</forenames></author><author><keyname>Lozano</keyname><forenames>Angel</forenames></author><author><keyname>Soong</keyname><forenames>Anthony C. K.</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong Charlie</forenames></author></authors><title>What Will 5G Be?</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>IEEE Journal on Selected Areas in Communications, Special Issue on
  5G Communication Systems, Editorial/Tutorial Paper, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What will 5G be? What it will not be is an incremental advance on 4G. The
previous four generations of cellular technology have each been a major
paradigm shift that has broken backwards compatibility. And indeed, 5G will
need to be a paradigm shift that includes very high carrier frequencies with
massive bandwidths, extreme base station and device densities and unprecedented
numbers of antennas. But unlike the previous four generations, it will also be
highly integrative: tying any new 5G air interface and spectrum together with
LTE and WiFi to provide universal high-rate coverage and a seamless user
experience. To support this, the core network will also have to reach
unprecedented levels of flexibility and intelligence, spectrum regulation will
need to be rethought and improved, and energy and cost efficiencies will become
even more critical considerations. This paper discusses all of these topics,
identifying key challenges for future research and preliminary 5G
standardization activities, while providing a comprehensive overview of the
current literature, and in particular of the papers appearing in this special
issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2962</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2962</id><created>2014-05-12</created><authors><author><keyname>Venturino</keyname><forenames>Luca</forenames></author><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Risi</keyname><forenames>Chiara</forenames></author><author><keyname>Buzzi</keyname><forenames>Stefano</forenames></author></authors><title>Energy-Efficient Scheduling and Power Allocation in Downlink OFDMA
  Networks with Base Station Coordination</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear on IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of energy-efficient resource allocation in
the downlink of a cellular OFDMA system. Three definitions of the energy
efficiency are considered for system design, accounting for both the radiated
and the circuit power. User scheduling and power allocation are optimized
across a cluster of coordinated base stations with a constraint on the maximum
transmit power (either per subcarrier or per base station). The asymptotic
noise-limited regime is discussed as a special case. %The performance of both
an isolated and a non-isolated cluster of coordinated base stations is examined
in the numerical experiments. Results show that the maximization of the energy
efficiency is approximately equivalent to the maximization of the spectral
efficiency for small values of the maximum transmit power, while there is a
wide range of values of the maximum transmit power for which a moderate
reduction of the data rate provides a large saving in terms of dissipated
energy. Also, the performance gap among the considered resource allocation
strategies reduces as the out-of-cluster interference increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2982</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2982</id><created>2014-05-12</created><updated>2014-05-14</updated><authors><author><keyname>Li</keyname><forenames>Wei-Chiang</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author></authors><title>Multicell Coordinated Beamforming with Rate Outage Constraint--Part I:
  Complexity Analysis</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the coordinated beamforming (CoBF) design in the
multiple-input single-output interference channel, assuming only channel
distribution information given a priori at the transmitters. The CoBF design is
formulated as an optimization problem that maximizes a predefined system
utility, e.g., the weighted sum rate or the weighted max-min-fairness (MMF)
rate, subject to constraints on the individual probability of rate outage and
power budget. While the problem is non-convex and appears difficult to handle
due to the intricate outage probability constraints, so far it is still unknown
if this outage constrained problem is computationally tractable. To answer
this, we conduct computational complexity analysis of the outage constrained
CoBF problem. Specifically, we show that the outage constrained CoBF problem
with the weighted sum rate utility is intrinsically difficult, i.e., NP-hard.
Moreover, the outage constrained CoBF problem with the weighted MMF rate
utility is also NP-hard except the case when all the transmitters are equipped
with single antenna. The presented analysis results confirm that efficient
approximation methods are indispensable to the outage constrained CoBF problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2984</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2984</id><created>2014-05-12</created><updated>2014-05-14</updated><authors><author><keyname>Li</keyname><forenames>Wei-Chiang</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author></authors><title>Multicell Coordinated Beamforming with Rate Outage Constraint--Part II:
  Efficient Approximation Algorithms</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the coordinated beamforming (CoBF) design for the
multiple-input single-output interference channel, provided that only channel
distribution information is known to the transmitters. The problem under
consideration is a probabilistically constrained optimization problem which
maximizes a predefined system utility subject to constraints on rate outage
probability and power budget of each transmitter. Our recent analysis has shown
that the outage-constrained CoBF problem is intricately difficult, e.g.,
NP-hard. Therefore, the focus of this paper is on suboptimal but
computationally efficient algorithms. Specifically, by leveraging on the block
successive upper bound minimization (BSUM) method in optimization, we propose a
Gauss-Seidel type algorithm, called distributed BSUM algorithm, which can
handle differentiable, monotone and concave system utilities. By exploiting a
weighted minimum mean-square error (WMMSE) reformulation, we further propose a
Jocobi-type algorithm, called distributed WMMSE algorithm, which can optimize
the weighted sum rate utility in a fully parallel manner. To provide a
performance benchmark, a relaxed approximation method based on polyblock outer
approximation is also proposed. Simulation results show that the proposed
algorithms are significantly superior to the existing successive convex
approximation method in both performance and computational efficiency, and can
yield promising approximation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2986</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2986</id><created>2014-05-12</created><authors><author><keyname>Venticinque</keyname><forenames>Alessio</forenames></author><author><keyname>Mazzocca</keyname><forenames>Nicola</forenames></author><author><keyname>Venticinque</keyname><forenames>Salvatore</forenames></author><author><keyname>Ficco</keyname><forenames>Massimo</forenames></author></authors><title>Semantic Support for Log Analysis of Safety-Critical Embedded Systems</title><categories>cs.SE</categories><comments>EDCC-2014, BIG4CIP-2014, Embedded systems, testing, semantic
  discovery, ontology, big data</comments><proxy>Ana Mihut</proxy><report-no>BIG4CIP/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing is a relevant activity for the development life-cycle of Safety
Critical Embedded systems. In particular, much effort is spent for analysis and
classification of test logs from SCADA subsystems, especially when failures
occur. The human expertise is needful to understand the reasons of failures,
for tracing back the errors, as well as to understand which requirements are
affected by errors and which ones will be affected by eventual changes in the
system design. Semantic techniques and full text search are used to support
human experts for the analysis and classification of test logs, in order to
speedup and improve the diagnosis phase. Moreover, retrieval of tests and
requirements, which can be related to the current failure, is supported in
order to allow the discovery of available alternatives and solutions for a
better and faster investigation of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2987</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2987</id><created>2014-05-12</created><updated>2015-02-18</updated><authors><author><keyname>Arias-Londo&#xf1;o</keyname><forenames>Juli&#xe1;n D.</forenames></author><author><keyname>Godino-Llorente</keyname><forenames>Juan I.</forenames></author></authors><title>Entropies from Markov Models as Complexity Measures of Embedded
  Attractors</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author in order to attend
  comments and corrections proposed during the peer review process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of measuring complexity from embedded
attractors as a way to characterize changes in the dynamical behaviour of
different types of systems by observing their outputs. With the aim of
measuring the stability of the trajectories of the attractor along time, this
paper proposes three new estimations of entropy that are derived from a Markov
model of the embedded attractor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2992</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2992</id><created>2014-05-12</created><authors><author><keyname>Baldoni</keyname><forenames>Roberto</forenames></author><author><keyname>Caruso</keyname><forenames>Mario</forenames></author><author><keyname>Cerocchi</keyname><forenames>Adriano</forenames></author><author><keyname>Ciccotelli</keyname><forenames>Claudio</forenames></author><author><keyname>Montanari</keyname><forenames>Luca</forenames></author><author><keyname>Nicoletti</keyname><forenames>Luca</forenames></author></authors><title>Correlating power consumption and network traffic for improving data
  centers resiliency</title><categories>cs.NI</categories><comments>EDCC-2014, BIG4CIP-2014</comments><proxy>Ana Mihut</proxy><report-no>BIG4CIP/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of business critical applications and information
infrastructures are moving to the cloud. This means they are hosted in large
scale data centers with other business applications and infrastructures with
less (or none) mission critical constraints. This mixed and complex environment
makes very challenging the process of monitoring critical applications and
handling (detecting and recovering) possible failures of servers' data center
that could affect responsiveness and/or reliability of mission critical
applications. Monitoring mechanisms used in data center are usually intrusive
in the sense that they need to install agents on each single server. This has
considerable drawbacks: huge usage of human resources to install and patch the
system and interference with the critical application because agents share
application resources. In order to detect (and possibly predict) failures in
data centers the paper does a first attempt in showing the correlation between
network traffic and servers' power consumption. This is an important step in
deriving non-intrusive monitoring systems, as both network traffic and power
consumption can be captured without installing any software at the servers.
This will improve in its turn the overall resiliency of the data center and its
self-managing capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2995</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2995</id><created>2014-05-12</created><authors><author><keyname>Garofalo</keyname><forenames>Alessia</forenames></author><author><keyname>Di Sarno</keyname><forenames>Cesario</forenames></author><author><keyname>Matteucci</keyname><forenames>Ilaria</forenames></author><author><keyname>Vallini</keyname><forenames>Marco</forenames></author><author><keyname>Formicola</keyname><forenames>Valerio</forenames></author></authors><title>Closing the loop of SIEM analysis to Secure Critical Infrastructures</title><categories>cs.CR</categories><comments>EDCC-2014, BIG4CIP-2014, Security Information and Event Management,
  Decision Support System, Hydroelectric Dam</comments><proxy>Ana Mihut</proxy><report-no>BIG4CIP/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Critical Infrastructure Protection is one of the main challenges of last
years. Security Information and Event Management (SIEM) systems are widely used
for coping with this challenge. However, they currently present several
limitations that have to be overcome. In this paper we propose an enhanced SIEM
system in which we have introduced novel components to i) enable multiple layer
data analysis; ii) resolve conflicts among security policies, and discover
unauthorized data paths in such a way to be able to reconfigure network
devices. Furthermore, the system is enriched by a Resilient Event Storage that
ensures integrity and unforgeability of events stored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.2998</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.2998</id><created>2014-05-12</created><updated>2014-05-14</updated><authors><author><keyname>Romanovsky</keyname><forenames>Alexander</forenames></author><author><keyname>Killijian</keyname><forenames>Marc-Olivier</forenames></author></authors><title>The Tenth European Dependable Computer Conference</title><categories>cs.SE</categories><comments>EDCC-2014</comments><proxy>Ana Mihut</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 21st century society relies on computing systems more than ever.
Computers are no longer simply machines that are used by organizations or at
home. They are embedded everywhere, from cell phones to cars or industrial
control devices, and large-scale cloud computing providers are sharing them
among many organizations in an unprecedented scale. As computers have become
indispensable, their failures may significantly perturb our daily lives. The
increased hardware and software complexity, as well as the scaling of
manufacturing technologies towards nanometer size devices, pose new challenges
to the developers. As a consequence the development, testing, and benchmarking
of dependable systems has become a vital topic of research, both for academia
and industry.
  EDCC is the leading European conference for presenting and discussing the
latest research in dependable computing. As in previous years, its tenth
edition aims at providing a European-hosted venue for researchers and
practitioners from all over the world to present and discuss their latest
research results on dependability, security, fault-tolerance, and testing.
Original papers are solicited on theory, techniques, systems, and tools for the
design, validation, operation and evaluation of dependable and secure computing
systems, covering any fault model, from traditional hardware and software
faults to accidental and malicious human interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3006</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3006</id><created>2014-05-12</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Compositional properties of crypto-based components</title><categories>cs.CR</categories><comments>Preprint. Archive of Formal Proofs, 2014, ISSN: 2150-914x</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an Isabelle/HOL+Isar set of theories which allows to
specify crypto-based components and to verify their composition properties wrt.
cryptographic aspects. We introduce a formalisation of the security property of
data secrecy, the corresponding definitions and proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3017</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3017</id><created>2014-05-12</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Formalisation and Analysis of Component Dependencies</title><categories>cs.SE</categories><comments>Preprint with an extended introduction, Archive of Formal Proofs,
  2014, ISSN: 2150-914x</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This set of theories presents a formalisation in Isabelle/HOL+Isar of data
dependencies between components. The approach allows to analyse system
structure oriented towards efficient checking of system: it aims at elaborating
for a concrete system, which parts of the system (or system model) are
necessary to check a given property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3033</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3033</id><created>2014-05-13</created><authors><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Waqas</keyname><forenames>Ahmad</forenames></author><author><keyname>Ismaili</keyname><forenames>Imdad Ali</forenames></author><author><keyname>Hakro</keyname><forenames>Dil Nawaz</forenames></author><author><keyname>Soomro</keyname><forenames>Waseem Javaid</forenames></author></authors><title>Phonetic based SoundEx &amp; ShapeEx algorithm for Sindhi Spell Checker
  System</title><categories>cs.CL</categories><comments>9 pages, 6 figures, 5 Tables, Sindhi Computing, Sindhi Language</comments><journal-ref>Adv. Environ. Biol., 8(4), 1147-1155, AENSI Publisher, 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a novel combinational phonetic algorithm for Sindhi
Language, to be used in developing Sindhi Spell Checker which has yet not been
developed prior to this work. The compound textual forms and glyphs of Sindhi
language presents a substantial challenge for developing Sindhi spell checker
system and generating similar suggestion list for misspelled words. In order to
implement such a system, phonetic based Sindhi language rules and patterns must
be considered into account for increasing the accuracy and efficiency. The
proposed system is developed with a blend between Phonetic based SoundEx
algorithm and ShapeEx algorithm for pattern or glyph matching, generating
accurate and efficient suggestion list for incorrect or misspelled Sindhi
words. A table of phonetically similar sounding Sindhi characters for SoundEx
algorithm is also generated along with another table containing similar glyph
or shape based character groups for ShapeEx algorithm. Both these are first
ever attempt of any such type of categorization and representation for Sindhi
Language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3057</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3057</id><created>2014-05-13</created><updated>2015-05-01</updated><authors><author><keyname>Sen</keyname><forenames>Pinar</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ali Ozgur</forenames></author></authors><title>A Low-Complexity Graph-Based LMMSE Receiver for MIMO ISI Channels with
  M-QAM Modulation</title><categories>cs.IT math.IT</categories><comments>28 pages, 6 figures, 2 tables, submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a low complexity graph-based linear minimum mean
square error (LMMSE) equalizer in order to remove inter-symbol and inter-stream
interference in multiple input multiple output (MIMO) communication. The
proposed state space representation inflicted on the graph provides linearly
increasing computational complexity with block length. Also, owing to the
Gaussian assumption used in the presented cycle-free factor graph, the
complexity of the suggested equalizer structure is not affected by the size of
the signalling space. In addition, we introduce an efficient way of computing
extrinsic bit log-likelihood ratio (LLR) values for LMMSE estimation compatible
with higher order alphabets which is shown to perform better than the other
methods in the literature. Overall, we provide an efficient receiver structure
reaching high data rates in frequency selective MIMO systems whose performance
is shown to be very close to a genie-aided matched filter bound through
extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3069</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3069</id><created>2014-05-13</created><updated>2015-06-11</updated><authors><author><keyname>Ganty</keyname><forenames>Pierre</forenames></author><author><keyname>Iosif</keyname><forenames>Radu</forenames></author></authors><title>Interprocedural Reachability for Flat Integer Programs</title><categories>cs.FL</categories><comments>38 pages, 1 figure</comments><acm-class>D.2.4; F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study programs with integer data, procedure calls and arbitrary call
graphs. We show that, whenever the guards and updates are given by octagonal
relations, the reachability problem along control flow paths within some
language w1* ... wd* over program statements is decidable in Nexptime. To
achieve this upper bound, we combine a program transformation into the same
class of programs but without procedures, with an Np-completeness result for
the reachability problem of procedure-less programs. Besides the program, the
expression w1* ... wd* is also mapped onto an expression of a similar form but
this time over the transformed program statements. Several arguments involving
context-free grammars and their generative process enable us to give tight
bounds on the size of the resulting expression. The currently existing gap
between Np-hard and Nexptime can be closed to Np-complete when a certain
parameter of the analysis is assumed to be constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3072</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3072</id><created>2014-05-13</created><updated>2014-07-21</updated><authors><author><keyname>Poss</keyname><forenames>Raphael</forenames></author></authors><title>Haskell for OCaml programmers</title><categories>cs.PL</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This introduction to Haskell is written to optimize learning by programmers
who already know OCaml.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3073</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3073</id><created>2014-05-13</created><updated>2014-07-21</updated><authors><author><keyname>Poss</keyname><forenames>Raphael</forenames></author></authors><title>Categories from scratch</title><categories>cs.LO</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of category from mathematics happens to be useful to computer
programmers in many ways. Unfortunately, all &quot;good&quot; explanations of categories
so far have been designed by mathematicians, or at least theoreticians with a
strong background in mathematics, and this makes categories especially
inscrutable to external audiences. More specifically, the common explanatory
route to approach categories is usually: &quot;here is a formal specification of
what a category is; then look at these known things from maths and theoretical
computer science, and admire how they can be described using the notions of
category theory.&quot; This approach is only successful if the audience can fully
understand a conceptual object using only its formal specification. In
practice, quite a few people only adopt conceptual objects by abstracting from
two or more contexts where the concepts are applicable, instead. This is the
road taken below: reconstruct the abstractions from category theory using
scratches of understanding from various fields of computer engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3080</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3080</id><created>2014-05-13</created><authors><author><keyname>Zhao</keyname><forenames>Peilin</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Accelerating Minibatch Stochastic Gradient Descent using Stratified
  Sampling</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is a popular optimization method which has
been applied to many important machine learning tasks such as Support Vector
Machines and Deep Neural Networks. In order to parallelize SGD, minibatch
training is often employed. The standard approach is to uniformly sample a
minibatch at each step, which often leads to high variance. In this paper we
propose a stratified sampling strategy, which divides the whole dataset into
clusters with low within-cluster variance; we then take examples from these
clusters using a stratified sampling technique. It is shown that the
convergence rate can be significantly improved by the algorithm. Encouraging
experimental results confirm the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3093</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3093</id><created>2014-05-13</created><authors><author><keyname>Blagus</keyname><forenames>Neli</forenames></author><author><keyname>Weiss</keyname><forenames>Gregor</forenames></author><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author></authors><title>Sampling node group structure of social and information networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lately, network sampling proved as a promising tool for simplifying large
real-world networks and thus providing for their faster and more efficient
analysis. Still, understanding the changes of network structure and properties
under different sampling methods remains incomplete. In this paper, we analyze
the presence of characteristic group of nodes (i.e., communities, modules and
mixtures of the two) in social and information networks. Moreover, we observe
the changes of node group structure under two sampling methods, random node
selection based on degree and breadth-first sampling. We show that the sampled
information networks contain larger number of mixtures than original networks,
while the structure of sampled social networks exhibits stronger
characterization by communities. The results also reveal there exist no
significant differences in the behavior of both sampling methods. Accordingly,
the selection of sampling method impact on the changes of node group structure
to a much smaller extent that the type and the structure of analyzed network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3094</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3094</id><created>2014-05-13</created><authors><author><keyname>Boubaker</keyname><forenames>Olfa</forenames></author></authors><title>The inverted Pendulum: A fundamental Benchmark in Control Theory and
  Robotics</title><categories>cs.RO cs.SY</categories><comments>IEEE International Conference on Education and e-Learning Innovations
  (ICEELI), 1-3 July 2012, Sousse, Tunisia</comments><doi>10.1109/ICEELI.2012.6360606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For at least fifty years, the inverted pendulum has been the most popular
benchmark, among others, for teaching and researches in control theory and
robotics. This paper presents the key motivations for the use of that system
and explains, in details, the main reflections on how the inverted pendulum
benchmark gives an effective and efficient application. Several real
experiences, virtual models and web-based remote control laboratories will be
presented with emphasis on the practical design implementation of this system.
A bibliographical survey of different design control approaches and trendy
robotic problems will be presented through applications to the inverted
pendulum system. In total, 150 references in the open literature, dating back
to 1960, are compiled to provide an overall picture of historical, current and
challenging developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3095</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3095</id><created>2014-05-13</created><updated>2014-07-21</updated><authors><author><keyname>Xu</keyname><forenames>Jiawei</forenames></author><author><keyname>Wang</keyname><forenames>Ruisheng</forenames></author><author><keyname>Yue</keyname><forenames>Shigang</forenames></author><author><keyname>Kiong</keyname><forenames>Loo Chu</forenames></author></authors><title>A Cognitive Model for Humanoid Robot Navigation and Mapping using
  Alderbaran NAO</title><categories>cs.RO</categories><comments>This paper has been withdrawn by the author due to the problems in
  section II</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to build a cognitive model for the humanoid robot,
especially, we are interested in the navigation and mapping on the humanoid
robot. The agents used are the Alderbaran NAO robot. The framework is
effectively applied to the integration of AI, computer vision, and signal
processing problems. Our model can be divided into two parts, cognitive mapping
and perception. Cognitive mapping is assumed as three parts, whose
representations were proposed a network of ASRs, an MFIS, and a hierarchy of
Place Representations. On the other hand, perception is the traditional
computer vision problem, which is the image sensing, feature extraction and
interested objects tracking. The points of our project can be concluded as the
following. Firstly, the robotics should realize where it is. Second, we would
like to test the theory that this is how humans map their environment. The
humanoid robot inspires the human vision searching by integrating the visual
mechanism and computer vision techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3097</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3097</id><created>2014-05-13</created><updated>2014-05-23</updated><authors><author><keyname>Konev</keyname><forenames>Boris</forenames></author><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author></authors><title>Computer-Aided Proof of Erdos Discrepancy Properties</title><categories>cs.DM cs.LO</categories><comments>Revised and extended journal version of arXiv:1402.2184,
  http://arxiv.org/abs/1402.2184</comments><acm-class>F.2.2; I.2.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1930s Paul Erdos conjectured that for any positive integer $C$ in any
infinite $\pm 1$ sequence $(x_n)$ there exists a subsequence $x_d, x_{2d},
x_{3d},\dots, x_{kd}$, for some positive integers $k$ and $d$, such that $\mid
\sum_{i=1}^k x_{i\cdot d} \mid &gt;C$. The conjecture has been referred to as one
of the major open problems in combinatorial number theory and discrepancy
theory. For the particular case of $C=1$ a human proof of the conjecture
exists; for $C=2$ a bespoke computer program had generated sequences of length
$1124$ of discrepancy $2$, but the status of the conjecture remained open even
for such a small bound. We show that by encoding the problem into Boolean
satisfiability and applying the state of the art SAT solvers, one can obtain a
discrepancy $2$ sequence of length $1160$ and a proof of the Erd\H{o}s
discrepancy conjecture for $C=2$, claiming that no discrepancy 2 sequence of
length $1161$, or more, exists. In the similar way, we obtain a precise bound
of $127\,645$ on the maximal lengths of both multiplicative and completely
multiplicative sequences of discrepancy $3$. We also demonstrate that
unrestricted discrepancy 3 sequences can be longer than $130\,000$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3099</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3099</id><created>2014-05-13</created><authors><author><keyname>Breitner</keyname><forenames>Joachim</forenames></author></authors><title>The Correctness of Launchbury's Natural Semantics for Lazy Evaluation</title><categories>cs.PL</categories><comments>22 pages</comments><acm-class>D.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his seminal paper &quot;A Natural Semantics for Lazy Evaluation&quot;, John
Launchbury proves his semantics correct with respect to a denotational
semantics. We machine-checked the proof and found it to fail, and provide two
ways to fix it: One by taking a detour via a modified natural semantics with an
explicit stack, and one by adjusting the denotational semantics of heaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3100</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3100</id><created>2014-05-13</created><updated>2014-05-22</updated><authors><author><keyname>Monacchi</keyname><forenames>Andrea</forenames></author><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author><author><keyname>D'Alessandro</keyname><forenames>Salvatore</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>GREEND: An Energy Consumption Dataset of Households in Italy and Austria</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Home energy management systems can be used to monitor and optimize
consumption and local production from renewable energy. To assess solutions
before their deployment, researchers and designers of those systems demand for
energy consumption datasets. In this paper, we present the GREEND dataset,
containing detailed power usage information obtained through a measurement
campaign in households in Austria and Italy. We provide a description of
consumption scenarios and discuss design choices for the sensing
infrastructure. Finally, we benchmark the dataset with state-of-the-art
techniques in load disaggregation, occupancy detection and appliance usage
mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3103</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3103</id><created>2014-05-13</created><authors><author><keyname>Arous</keyname><forenames>Yosra</forenames></author><author><keyname>Boubaker</keyname><forenames>Olfa</forenames></author></authors><title>Gait trajectory generation for a five link bipedal robot based on a
  reduced dynamical model</title><categories>cs.RO</categories><comments>16th IEEE Mediterranean Electrotechnical Conference (MELECON), pp.
  993-996, 25-28 March 2012, Yasmine Hammamet, Tunisia</comments><doi>10.1109/MELCON.2012.6196594</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a simple trajectory generation method for biped walking is
proposed. The dynamic model of the five link bipedal robot is first reduced
using several biologically inspired assumptions. A sinusoidal curve is then
imposed to the ankle of the swing leg's trajectory. The reduced model is
finally obtained and solved: it is an homogeneous second order differential
equations with constant coefficients. The algebraic solution obtained ensures a
stable rhythmic gait for the bipedal robot. It's continuous in the defined time
interval, easy to implement when the boundary conditions are well defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3117</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3117</id><created>2014-05-13</created><authors><author><keyname>Shi</keyname><forenames>Bichen</forenames></author><author><keyname>Ifrim</keyname><forenames>Georgiana</forenames></author><author><keyname>Hurley</keyname><forenames>Neil</forenames></author></authors><title>Be In The Know: Connecting News Articles to Relevant Twitter
  Conversations</title><categories>cs.SI cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era of data-driven journalism, data analytics can deliver tools to
support journalists in connecting to new and developing news stories, e.g., as
echoed in micro-blogs such as Twitter, the new citizen-driven media. In this
paper, we propose a framework for tracking and automatically connecting news
articles to Twitter conversations as captured by Twitter hashtags. For example,
such a system could alert journalists about news that get a lot of Twitter
reaction, so that they can investigate those conversations for new developments
in the story, promote their article to a set of interested consumers, or
discover general sentiment towards the story. Mapping articles to appropriate
hashtags is nevertheless very challenging, due to different language styles
used in articles versus tweets, the streaming aspect of news and tweets, as
well as the user behavior when marking certain tweet-terms as hashtags. As a
case-study, we continuously track the RSS feeds of Irish Times news articles
and a focused Twitter stream over a two months period, and present a system
that assigns hashtags to each article, based on its Twitter echo. We propose a
machine learning approach for classifying and ranking article-hashtag pairs.
Our empirical study shows that our system delivers high precision for this
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3137</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3137</id><created>2014-05-13</created><authors><author><keyname>Kelif</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Simon</keyname><forenames>Olivier</forenames></author></authors><title>Impact of Directional Receiving Antennas on Wireless Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, submitted to VTC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in high data rates internet access, by the mean of LTE
based wireless networks. In the aim to improve performance of wireless
networks, we propose an approach focused on the use of UE equipped by
directional receiving antennas. Indeed, these antennas allow to mitigate the
interference and to improve the link budget. Therefore, the Signal to
Interference plus Noise Ratio (SINR) can be improved, and consequently the
performance and quality of service (QoS), too. We establish the analytical
expression of the SINR reached by a user with directional antenna, whatever its
location. This expression shows that directional antennas allow an improvement
of the SINR, and to quantify it. We develop different scenarios to compare the
use of directional antennas instead of omnidirectional ones. They allow to
quantify the impact of directional antennas in terms of performance and QoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3162</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3162</id><created>2014-05-13</created><authors><author><keyname>Yu</keyname><forenames>Felix X.</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Gong</keyname><forenames>Yunchao</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Circulant Binary Embedding</title><categories>stat.ML cs.LG</categories><comments>ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary embedding of high-dimensional data requires long codes to preserve the
discriminative power of the input space. Traditional binary coding methods
often suffer from very high computation and storage costs in such a scenario.
To address this problem, we propose Circulant Binary Embedding (CBE) which
generates binary codes by projecting the data with a circulant matrix. The
circulant structure enables the use of Fast Fourier Transformation to speed up
the computation. Compared to methods that use unstructured matrices, the
proposed method improves the time complexity from $\mathcal{O}(d^2)$ to
$\mathcal{O}(d\log{d})$, and the space complexity from $\mathcal{O}(d^2)$ to
$\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a novel
time-frequency alternating optimization to learn data-dependent circulant
projections, which alternatively minimizes the objective in original and
Fourier domains. We show by extensive experiments that the proposed approach
gives much better performance than the state-of-the-art approaches for fixed
time, and provides much faster computation with no performance degradation for
fixed number of bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3164</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3164</id><created>2014-05-13</created><authors><author><keyname>Pishdad</keyname><forenames>Leila</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>A New Reduction Scheme for Gaussian Sum Filters</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many signal processing applications it is required to estimate the
unobservable state of a dynamic system from its noisy measurements. For linear
dynamic systems with Gaussian Mixture (GM) noise distributions, Gaussian Sum
Filters (GSF) provide the MMSE state estimate by tracking the GM posterior.
However, since the number of the clusters of the GM posterior grows
exponentially over time, suitable reduction schemes need to be used to maintain
the size of the bank in GSF. In this work we propose a low computational
complexity reduction scheme which uses an initial state estimation to find the
active noise clusters and removes all the others. Since the performance of our
proposed method relies on the accuracy of the initial state estimation, we also
propose five methods for finding this estimation. We provide simulation results
showing that with suitable choice of the initial state estimation (based on the
shape of the noise models), our proposed reduction scheme provides better state
estimations both in terms of accuracy and precision when compared with other
reduction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3166</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3166</id><created>2014-05-13</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>Clonal-Based Cellular Automata in Bioinformatics</title><categories>cs.CE</categories><comments>14 Pages. arXiv admin note: substantial text overlap with
  arXiv:1404.0453</comments><journal-ref>Journal of Advanced Research in Applied Artificial Intelligence &amp;
  Neural Network Vol.1, Issue1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at providing a survey on the problems that can be easily
addressed by clonalbased cellular automata in bioinformatics. Researchers try
to address the problems in bioinformatics independent of each problem. None of
the researchers has tried to relate the major problems in bioinformatics and
find a solution using common frame work. We tried to find various problems in
bioinformatics which can be addressed easily by clonal based cellular automata.
Extensive literature survey is conducted. We have considered some papers in
various journals and conferences for conduct of our research. This paper
provides intuition towards relating various problems in bioinformatics
logically and tries to attain a common frame work with respect to clonal based
cellular automata classifier for addressing the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3167</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3167</id><created>2014-05-13</created><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Clustering, Hamming Embedding, Generalized LSH and the Max Norm</title><categories>cs.LG</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the convex relaxation of clustering and hamming embedding, focusing
on the asymmetric case (co-clustering and asymmetric hamming embedding),
understanding their relationship to LSH as studied by (Charikar 2002) and to
the max-norm ball, and the differences between their symmetric and asymmetric
versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3173</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3173</id><created>2014-05-11</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author><author><keyname>Xiong</keyname><forenames>Ruiqin</forenames></author><author><keyname>Ma</keyname><forenames>Siwei</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Image Restoration Using Joint Statistical Modeling in Space-Transform
  Domain</title><categories>cs.MM cs.CV</categories><comments>14 pages, 18 figures, 7 Tables, to be published in IEEE Transactions
  on Circuits System and Video Technology (TCSVT). High resolution pdf version
  and Code can be found at: http://idm.pku.edu.cn/staff/zhangjian/IRJSM/</comments><doi>10.1109/TCSVT.2014.2302380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel strategy for high-fidelity image restoration by
characterizing both local smoothness and nonlocal self-similarity of natural
images in a unified statistical manner. The main contributions are three-folds.
First, from the perspective of image statistics, a joint statistical modeling
(JSM) in an adaptive hybrid space-transform domain is established, which offers
a powerful mechanism of combining local smoothness and nonlocal self-similarity
simultaneously to ensure a more reliable and robust estimation. Second, a new
form of minimization functional for solving image inverse problem is formulated
using JSM under regularization-based framework. Finally, in order to make JSM
tractable and robust, a new Split-Bregman based algorithm is developed to
efficiently solve the above severely underdetermined inverse problem associated
with theoretical proof of convergence. Extensive experiments on image
inpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise
removal applications verify the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3175</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3175</id><created>2014-05-13</created><authors><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>D numbers theory: a generalization of Dempster-Shafer evidence theory</title><categories>cs.AI</categories><comments>31 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1404.0540</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient modeling of uncertain information in real world is still an open
issue. Dempster-Shafer evidence theory is one of the most commonly used
methods. However, the Dempster-Shafer evidence theory has the assumption that
the hypothesis in the framework of discernment is exclusive of each other. This
condition can be violated in real applications, especially in linguistic
decision making since the linguistic variables are not exclusive of each others
essentially. In this paper, a new theory, called as D numbers theory (DNT), is
systematically developed to address this issue. The combination rule of two D
numbers is presented. An coefficient is defined to measure the exclusive degree
among the hypotheses in the framework of discernment. The combination rule of
two D numbers is presented. If the exclusive coefficient is one which means
that the hypothesis in the framework of discernment is exclusive of each other
totally, the D combination is degenerated as the classical Dempster combination
rule. Finally, a linguistic variables transformation of D numbers is presented
to make a decision. A numerical example on linguistic evidential decision
making is used to illustrate the efficiency of the proposed D numbers theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3176</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3176</id><created>2014-05-13</created><updated>2014-12-30</updated><authors><author><keyname>Puerto</keyname><forenames>Justo</forenames></author><author><keyname>Perea</keyname><forenames>Federico</forenames></author></authors><title>On two solution concepts in a class of multicriteria games</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we compare two solution concepts for general multicriteria
zero-sum matrix games: minimax and Pareto-optimal security payoff vectors. We
characterize the two criteria based on properties similar to the ones that have
been used in the corresponding counterparts in the single criterion case,
although they need to be complemented with two new consistency properties.
Whereas in standard single criterion games minimax and optimal security payffs
coincide, whenever we have multiple criteria these two solution concepts
differ. We provide explanations for the common roots of these two concepts and
highlight the intrinsic differences between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3177</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3177</id><created>2014-05-13</created><authors><author><keyname>Wang</keyname><forenames>Shangping</forenames></author><author><keyname>zhao</keyname><forenames>Ru</forenames></author></authors><title>Lattice-Based Ring Signature Scheme under the Random Oracle Model</title><categories>cs.CR</categories><comments>19 pages</comments><msc-class>94A60, 81T25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the basis of the signatures scheme without trapdoors from lattice, which
is proposed by Vadim Lyubashevsky in 2012, we present a new ring signature
scheme from lattice. The proposed ring signature scheme is an extension of the
signatures scheme without trapdoors. We proved that our scheme is strongly
unforgeable against adaptive chosen message in the random oracle model, and
proved that the security of our scheme can be reduced to the hardness of the
small integer solution (SIS) problem by rejection samplings. Compared with the
existing lattice-based ring signature schemes, our new scheme is more efficient
and with shorter signature length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3182</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3182</id><created>2014-05-13</created><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Scalable Coordinated Beamforming for Dense Wireless Cooperative Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the ever growing demand for both high throughput and uniform coverage
in future wireless networks, dense network deployment will be ubiquitous, for
which co- operation among the access points is critical. Considering the
computational complexity of designing coordinated beamformers for dense
networks, low-complexity and suboptimal precoding strategies are often adopted.
However, it is not clear how much performance loss will be caused. To enable
optimal coordinated beamforming, in this paper, we propose a framework to
design a scalable beamforming algorithm based on the alternative direction
method of multipliers (ADMM) method. Specifically, we first propose to apply
the matrix stuffing technique to transform the original optimization problem to
an equivalent ADMM-compliant problem, which is much more efficient than the
widely-used modeling framework CVX. We will then propose to use the ADMM
algorithm, a.k.a. the operator splitting method, to solve the transformed
ADMM-compliant problem efficiently. In particular, the subproblems of the ADMM
algorithm at each iteration can be solved with closed-forms and in parallel.
Simulation results show that the proposed techniques can result in significant
computational efficiency compared to the state- of-the-art interior-point
solvers. Furthermore, the simulation results demonstrate that the optimal
coordinated beamforming can significantly improve the system performance
compared to sub-optimal zero forcing beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3183</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3183</id><created>2014-05-13</created><updated>2014-05-14</updated><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>The edge chromatic number of outer-1-planar graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is outer-1-planar if it can be drawn in the plane so that all
vertices are on the outer face and each edge is crossed at most once. In this
paper, we completely determine the edge chromatic number of outer 1-planar
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3188</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3188</id><created>2014-05-13</created><updated>2014-05-14</updated><authors><author><keyname>Gerami</keyname><forenames>Majid</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>Repair for Distributed Storage Systems in Packet Erasure Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliability is essential for storing files in many applications of
distributed storage systems. To maintain reliability, when a storage node
fails, a new node should be regenerated by a repair process. Most of the
previous results on the repair problem assume perfect (error-free) links in the
networks. However, in practice, especially in a wireless network, the
transmitted packets (for repair) may be lost due to, e.g., link failure or
buffer overflow. We study the repair problem of distributed storage systems in
packet erasure networks, where a packet loss is modeled as an erasure. The
minimum repair-bandwidth, namely the amount of information sent from the
surviving nodes to the new node, is established under the ideal assumption of
infinite number of packet transmissions. We also study the bandwidth-storage
tradeoffs in erasure networks. Then, the use of repairing storage nodes (nodes
with smaller storage space) is proposed to reduce the repair-bandwidth. We
study the minimal storage of repairing storage nodes. For the case of a finite
number of packet transmissions, the probability of successful repairing is
investigated. We show that the repair with a finite number of packet
transmissions may use much larger bandwidth than the minimum repair-bandwidth.
Finally, we propose a combinatorial optimization problem, which results in the
optimal repair-bandwidth for the given packet erasure probability and finite
packet transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3195</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3195</id><created>2014-05-13</created><authors><author><keyname>Sabrin</keyname><forenames>Kaeser M</forenames></author><author><keyname>Ali</keyname><forenames>M Haider</forenames></author></authors><title>An Intelligent Pixel Replication Technique by Binary Decomposition for
  Digital Image Zooming</title><categories>cs.CV</categories><journal-ref>Proceedings of the 26th Image and Vision Computing New Zealand
  Conference (IVCNZ 2011), P.Delmas, B.Wuensche, J. James, Eds., 29 Nov - 1 Dec
  2011, Auckland, New Zealand, IVCNZ, pp. 547 - 552, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image zooming is the process of enlarging the spatial resolution of a given
digital image. We present a novel technique that intelligently modifies the
classical pixel replication method for zooming. Our method decomposes a given
image into layer of binary images, interpolates them by magnifying the binary
patterns preserving their geometric shape and finally aggregates them all to
obtain the zoomed image. Although the quality of our zoomed images is much
higher than that of nearest neighbor and bilinear interpolation and comparable
with bicubic interpolation, the running time of our technique is extremely fast
like nearest neighbor interpolation and much faster than bilinear and bicubic
interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3199</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3199</id><created>2014-05-13</created><authors><author><keyname>Rahimi</keyname><forenames>Hasnae</forenames></author><author><keyname>Bakkali</keyname><forenames>Hanan EL</forenames></author></authors><title>A New Trust Reputation System for E-Commerce Applications</title><categories>cs.CR cs.CY</categories><comments>7 pages, 12 references. International Journal of Computer Science
  Issues (IJCSI) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust Trust Reputation Systems (TRS) provide a most trustful reputation
score for a specific product or service so as to support relying parties taking
the right decision while interacting with an e-commerce application. Thus, TRS
must rely on an appropriate architecture and suitable algorithms that are able
to improve the selection, storage, generation and classification of textual
feedbacks. In this work, we propose a new architecture for TRS in e-commerce
applications. In fact, we propose an intelligent layer which displays to each
feedback provider, who has already given his recommendation on a product, a
collection of prefabricated feedbacks related to the same product. The proposed
reputation algorithm generates better trust degree of the user, trust degree of
the feedback and a better global reputation score of the product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3207</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3207</id><created>2014-05-12</created><authors><author><keyname>S.</keyname><forenames>Parvathavarthini</forenames></author><author><keyname>R</keyname><forenames>Shanthakumari</forenames></author></authors><title>An Adaptive Watermarking Process in Hadamard Transform</title><categories>cs.MM</categories><comments>7 Pages, 10 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive visible/invisible watermarking scheme is done to prevent the
privacy and preserving copyright protection of digital data using Hadamard
transform based on the scaling factor of the image. The value of scaling factor
depends on the control parameter. The scaling factor is calculated to embedded
the watermark. Depend upon the control parameter the visible and invisible
watermarking is determined. The proposed Hadamard transform domain method is
more robust again image/signal processing attacks. Furthermore, it also shows
that the proposed method confirm the efficiency through various performance
analysis and experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3210</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3210</id><created>2014-05-13</created><authors><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author><author><keyname>Caceres</keyname><forenames>Rajmonda</forenames></author><author><keyname>Carter</keyname><forenames>Kevin</forenames></author></authors><title>Locally Boosted Graph Aggregation for Community Detection</title><categories>cs.LG cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.3258</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the right graph representation from noisy, multi-source data has
garnered significant interest in recent years. A central tenet of this problem
is relational learning. Here the objective is to incorporate the partial
information each data source gives us in a way that captures the true
underlying relationships. To address this challenge, we present a general,
boosting-inspired framework for combining weak evidence of entity associations
into a robust similarity metric. Building on previous work, we explore the
extent to which different local quality measurements yield graph
representations that are suitable for community detection. We present empirical
results on a variety of datasets demonstrating the utility of this framework,
especially with respect to real datasets where noise and scale present serious
challenges. Finally, we prove a convergence theorem in an ideal setting and
outline future research into other application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3218</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3218</id><created>2014-05-13</created><updated>2014-10-10</updated><authors><author><keyname>Bellodi</keyname><forenames>Elena</forenames></author><author><keyname>Lamma</keyname><forenames>Evelina</forenames></author><author><keyname>Riguzzi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Costa</keyname><forenames>Vitor Santos</forenames></author><author><keyname>Zese</keyname><forenames>Riccardo</forenames></author></authors><title>Lifted Variable Elimination for Probabilistic Logic Programming</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP). arXiv
  admin note: text overlap with arXiv:1402.0565 by other authors</comments><doi>10.1017/S1471068414000283</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifted inference has been proposed for various probabilistic logical
frameworks in order to compute the probability of queries in a time that
depends on the size of the domains of the random variables rather than the
number of instances. Even if various authors have underlined its importance for
probabilistic logic programming (PLP), lifted inference has been applied up to
now only to relational languages outside of logic programming. In this paper we
adapt Generalized Counting First Order Variable Elimination (GC-FOVE) to the
problem of computing the probability of queries to probabilistic logic programs
under the distribution semantics. In particular, we extend the Prolog Factor
Language (PFL) to include two new types of factors that are needed for
representing ProbLog programs. These factors take into account the existing
causal independence relationships among random variables and are managed by the
extension to variable elimination proposed by Zhang and Poole for dealing with
convergent variables and heterogeneous factors. Two new operators are added to
GC-FOVE for treating heterogeneous factors. The resulting algorithm, called
LP$^2$ for Lifted Probabilistic Logic Programming, has been implemented by
modifying the PFL implementation of GC-FOVE and tested on three benchmarks for
lifted inference. A comparison with PITA and ProbLog2 shows the potential of
the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3222</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3222</id><created>2014-05-13</created><updated>2014-11-03</updated><authors><author><keyname>Arnold</keyname><forenames>Taylor</forenames></author><author><keyname>Tibshirani</keyname><forenames>Ryan</forenames></author></authors><title>Efficient Implementations of the Generalized Lasso Dual Path Algorithm</title><categories>stat.CO cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider efficient implementations of the generalized lasso dual path
algorithm of Tibshirani and Taylor (2011). We first describe a generic approach
that covers any penalty matrix D and any (full column rank) matrix X of
predictor variables. We then describe fast implementations for the special
cases of trend filtering problems, fused lasso problems, and sparse fused lasso
problems, both with X=I and a general matrix X. These specialized
implementations offer a considerable improvement over the generic
implementation, both in terms of numerical stability and efficiency of the
solution path computation. These algorithms are all available for use in the
genlasso R package, which can be found in the CRAN repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3223</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3223</id><created>2014-05-13</created><authors><author><keyname>Badouel</keyname><forenames>Eric</forenames><affiliation>INRIA - IRISA, LIRIMA</affiliation></author><author><keyname>H&#xe9;lou&#xeb;t</keyname><forenames>Lo&#xef;c</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Kouamou</keyname><forenames>Georges-Edouard</forenames><affiliation>LIRIMA</affiliation></author><author><keyname>Morvan</keyname><forenames>Christophe</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>A Grammatical Approach to Data-centric Case Management in a Distributed
  Collaborative Environment</title><categories>cs.DC</categories><proxy>ccsd</proxy><report-no>RR-8528</report-no><journal-ref>N&amp;deg; RR-8528 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a purely declarative approach to artifact-centric case
management systems, and a decentralization scheme for this model. Each case is
presented as a tree-like structure; nodes bear information that combines data
and computations. Each node belongs to a given stakeholder, and semantic rules
govern the evolution of the tree structure, as well as how data values derive
from information stemming from the context of the node. Stakeholders
communicate through asynchronous message passing without shared memory,
enabling convenient distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3224</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3224</id><created>2014-05-13</created><updated>2015-02-24</updated><authors><author><keyname>Kaufmann</keyname><forenames>Emilie</forenames><affiliation>LTCI</affiliation></author><author><keyname>Capp&#xe9;</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>IMT</affiliation></author></authors><title>On the Complexity of A/B Testing</title><categories>math.ST cs.LG stat.ML stat.TH</categories><proxy>ccsd</proxy><journal-ref>Conference on Learning Theory, Jun 2014, Barcelona, Spain. JMLR:
  Workshop and Conference Proceedings, 35, pp.461-481</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A/B testing refers to the task of determining the best option among two
alternatives that yield random outcomes. We provide distribution-dependent
lower bounds for the performance of A/B testing that improve over the results
currently available both in the fixed-confidence (or delta-PAC) and
fixed-budget settings. When the distribution of the outcomes are Gaussian, we
prove that the complexity of the fixed-confidence and fixed-budget settings are
equivalent, and that uniform sampling of both alternatives is optimal only in
the case of equal variances. In the common variance case, we also provide a
stopping rule that terminates faster than existing fixed-confidence algorithms.
In the case of Bernoulli distributions, we show that the complexity of
fixed-budget setting is smaller than that of fixed-confidence setting and that
uniform sampling of both alternatives -though not optimal- is advisable in
practice when combined with an appropriate stopping criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3229</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3229</id><created>2014-05-13</created><authors><author><keyname>Tagorti</keyname><forenames>Manel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Rate of Convergence and Error Bounds for LSTD($\lambda$)</title><categories>cs.LG cs.AI math.OC math.ST stat.TH</categories><comments>(2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider LSTD($\lambda$), the least-squares temporal-difference algorithm
with eligibility traces algorithm proposed by Boyan (2002). It computes a
linear approximation of the value function of a fixed policy in a large Markov
Decision Process. Under a $\beta$-mixing assumption, we derive, for any value
of $\lambda \in (0,1)$, a high-probability estimate of the rate of convergence
of this algorithm to its limit. We deduce a high-probability bound on the error
of this algorithm, that extends (and slightly improves) that derived by Lazaric
et al. (2012) in the specific case where $\lambda=0$. In particular, our
analysis sheds some light on the choice of $\lambda$ with respect to the
quality of the chosen linear space and the number of samples, that complies
with simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3230</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3230</id><created>2014-05-13</created><updated>2014-08-28</updated><authors><author><keyname>Karimi</keyname><forenames>S.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>A monolithic multi-time-step computational framework for first-order
  transient systems with disparate scales</title><categories>cs.NA math.NA</categories><doi>10.1016/j.cma.2014.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing robust simulation tools for problems involving multiple
mathematical scales has been a subject of great interest in computational
mathematics and engineering. A desirable feature to have in a numerical
formulation for multiscale transient problems is to be able to employ different
time-steps (multi-time-step coupling), and different time integrators and
different numerical formulations (mixed methods) in different regions of the
computational domain. We present two new monolithic multi-time-step mixed
coupling methods for first-order transient systems. We shall employ unsteady
advection-diffusion-reaction equation with linear decay as the model problem,
which offers several unique challenges in terms of non-self-adjoint spatial
operator and rich features in the solutions. We shall employ the dual Schur
domain decomposition technique to handle the decomposition of domain into
subdomains. Two different methods of enforcing compatibility along the
subdomain interface will be used in the time discrete setting. A systematic
theoretical analysis (which includes numerical stability, influence of
perturbations, bounds on drift along the subdomain interface) will be
performed. The first coupling method ensures that there is no drift along the
subdomain interface but does not facilitate explicit/implicit coupling. The
second coupling method allows explicit/implicit coupling with controlled (but
non-zero) drift in the solution along the subdomain interface. Several
canonical problems will be solved to numerically verify the theoretical
predictions, and to illustrate the overall performance of the proposed coupling
methods. Finally, we shall illustrate the robustness of the proposed coupling
methods using a multi-time-step transient simulation of a fast bimolecular
advective-diffusive-reactive system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3235</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3235</id><created>2014-05-13</created><authors><author><keyname>Tajani</keyname><forenames>Chakir</forenames></author><author><keyname>Abouchabaka</keyname><forenames>Jaafar</forenames></author></authors><title>An Alternating KMF Algorithm to Solve the Cauchy Problem for Laplaces
  Equation</title><categories>cs.NA math.NA</categories><comments>International Journal of Computer Applications Volume 38 8 January
  2012</comments><doi>10.5120/4709-6876</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This work concerns the use of the iterative algorithm (KMF algorithm)
proposed by Kozlov, Mazya and Fomin to solve the Cauchy problem for Laplaces
equation. This problem consists to recovering the lacking data on some part of
the boundary using the over specified conditions on the other part of the
boundary. We describe an alternating formulation of the KMF algorithm and its
relationship with a classical formulation. The implementation of this algorithm
for a regular domain is performed by the finite element method using the
software Freefem. The numerical tests developed show the effectiveness of the
proposed algorithm since it allows to have more accurate results as well as
reducing the number of iterations needed for convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3240</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3240</id><created>2014-05-13</created><authors><author><keyname>Kosower</keyname><forenames>David A.</forenames></author><author><keyname>Lopez-Villarejo</keyname><forenames>J. J.</forenames></author></authors><title>Flowgen: Flowchart-Based Documentation for C++ Codes</title><categories>cs.SE cs.CE hep-ex hep-lat hep-ph</categories><comments>17 pages, 10 figures, supplemental material (two ancillary files)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Flowgen tool, which generates flowcharts from annotated C++
source code. The tool generates a set of interconnected high-level UML activity
diagrams, one for each function or method in the C++ sources. It provides a
simple and visual overview of complex implementations of numerical algorithms.
Flowgen is complementary to the widely-used Doxygen documentation tool. The
ultimate aim is to render complex C++ computer codes accessible, and to enhance
collaboration between programmers and algorithm or science specialists. We
describe the tool and a proof-of-concept application to the VINCIA plug-in for
simulating collisions at CERN's Large Hadron Collider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3250</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3250</id><created>2014-05-13</created><updated>2014-07-29</updated><authors><author><keyname>Gribkoff</keyname><forenames>Eric</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Understanding the Complexity of Lifted Inference and Asymmetric Weighted
  Model Counting</title><categories>cs.AI cs.DB cs.LO</categories><msc-class>cs.DB, cs.AI</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study lifted inference for the Weighted First-Order Model
Counting problem (WFOMC), which counts the assignments that satisfy a given
sentence in first-order logic (FOL); it has applications in Statistical
Relational Learning (SRL) and Probabilistic Databases (PDB). We present several
results. First, we describe a lifted inference algorithm that generalizes prior
approaches in SRL and PDB. Second, we provide a novel dichotomy result for a
non-trivial fragment of FO CNF sentences, showing that for each sentence the
WFOMC problem is either in PTIME or #P-hard in the size of the input domain; we
prove that, in the first case our algorithm solves the WFOMC problem in PTIME,
and in the second case it fails. Third, we present several properties of the
algorithm. Finally, we discuss limitations of lifted inference for symmetric
probabilistic databases (where the weights of ground literals depend only on
the relation name, and not on the constants of the domain), and prove the
impossibility of a dichotomy result for the complexity of probabilistic
inference for the entire language FOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3263</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3263</id><created>2014-05-13</created><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Mahabadi</keyname><forenames>Rabeeh Karimi</forenames></author><author><keyname>Tran-Dinh</keyname><forenames>Quoc</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Scalable sparse covariance estimation via self-concordance</title><categories>stat.ML cs.IT math.IT math.OC</categories><comments>7 pages, 1 figure, Accepted at AAAI-14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the class of convex minimization problems, composed of a
self-concordant function, such as the $\log\det$ metric, a convex data fidelity
term $h(\cdot)$ and, a regularizing -- possibly non-smooth -- function
$g(\cdot)$. This type of problems have recently attracted a great deal of
interest, mainly due to their omnipresence in top-notch applications. Under
this \emph{locally} Lipschitz continuous gradient setting, we analyze the
convergence behavior of proximal Newton schemes with the added twist of a
probable presence of inexact evaluations. We prove attractive convergence rate
guarantees and enhance state-of-the-art optimization schemes to accommodate
such developments. Experimental results on sparse covariance estimation show
the merits of our algorithm, both in terms of recovery efficiency and
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3267</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3267</id><created>2014-05-13</created><updated>2014-10-27</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Hall</keyname><forenames>Georgina</forenames></author></authors><title>Exact Recovery in the Stochastic Block Model</title><categories>cs.SI math.PR physics.soc-ph</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model (SBM) with two communities, or equivalently the
planted bisection model, is a popular model of random graph exhibiting a
cluster behaviour. In the symmetric case, the graph has two equally sized
clusters and vertices connect with probability $p$ within clusters and $q$
across clusters. In the past two decades, a large body of literature in
statistics and computer science has focused on providing lower-bounds on the
scaling of $|p-q|$ to ensure exact recovery. In this paper, we identify a sharp
threshold phenomenon for exact recovery: if $\alpha=pn/\log(n)$ and
$\beta=qn/\log(n)$ are constant (with $\alpha&gt;\beta$), recovering the
communities with high probability is possible if $\frac{\alpha+\beta}{2} -
\sqrt{\alpha \beta}&gt;1$ and impossible if $\frac{\alpha+\beta}{2} - \sqrt{\alpha
\beta}&lt;1$. In particular, this improves the existing bounds. This also sets a
new line of sight for efficient clustering algorithms. While maximum likelihood
(ML) achieves the optimal threshold (by definition), it is in the worst-case
NP-hard. This paper proposes an efficient algorithm based on a semidefinite
programming relaxation of ML, which is proved to succeed in recovering the
communities close to the threshold, while numerical experiments suggest it may
achieve the threshold. An efficient algorithm which succeeds all the way down
to the threshold is also obtained using a partial recovery algorithm combined
with a local improvement procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3272</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3272</id><created>2014-05-13</created><updated>2014-05-20</updated><authors><author><keyname>Kersting</keyname><forenames>Nicholas</forenames></author></authors><title>Fast and Fuzzy Private Set Intersection</title><categories>cs.CR cs.CL</categories><comments>20 pages, 6 figures, plus source code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Private Set Intersection (PSI) is usually implemented as a sequence of
encryption rounds between pairs of users, whereas the present work implements
PSI in a simpler fashion: each set only needs to be encrypted once, after which
each pair of users need only one ordinary set comparison. This is typically
orders of magnitude faster than ordinary PSI at the cost of some ``fuzziness&quot;
in the matching, which may nonetheless be tolerable or even desirable. This is
demonstrated in the case where the sets consist of English words processed with
WordNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3282</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3282</id><created>2014-05-13</created><authors><author><keyname>Althoff</keyname><forenames>Tim</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>How to Ask for a Favor: A Case Study on the Success of Altruistic
  Requests</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>To appear at ICWSM 2014. 10pp, 3 fig. Data and other info available
  at http://www.mpi-sws.org/~cristian/How_to_Ask_for_a_Favor.html</comments><acm-class>I.2.7; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requests are at the core of many social media systems such as question &amp;
answer sites and online philanthropy communities. While the success of such
requests is critical to the success of the community, the factors that lead
community members to satisfy a request are largely unknown. Success of a
request depends on factors like who is asking, how they are asking, when are
they asking, and most critically what is being requested, ranging from small
favors to substantial monetary donations. We present a case study of altruistic
requests in an online community where all requests ask for the very same
contribution and do not offer anything tangible in return, allowing us to
disentangle what is requested from textual and social factors. Drawing from
social psychology literature, we extract high-level social features from text
that operationalize social relations between recipient and donor and
demonstrate that these extracted relations are predictive of success. More
specifically, we find that clearly communicating need through the narrative is
essential and that that linguistic indications of gratitude, evidentiality, and
generalized reciprocity, as well as high status of the asker further increase
the likelihood of success. Building on this understanding, we develop a model
that can predict the success of unseen requests, significantly improving over
several baselines. We link these findings to research in psychology on helping
behavior, providing a basis for further analysis of success in social media
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3292</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3292</id><created>2014-05-13</created><authors><author><keyname>Izbicki</keyname><forenames>Rafael</forenames></author><author><keyname>Stern</keyname><forenames>Rafael Bassi</forenames></author></authors><title>Learning with many experts: model selection and sparsity</title><categories>stat.ME cs.LG</categories><comments>This is the pre-peer reviewed version</comments><journal-ref>Izbicki, R., Stern, R. B. &quot;Learning with many experts: Model
  selection and sparsity.&quot; Statistical Analysis and Data Mining 6.6 (2013):
  565-577</journal-ref><doi>10.1002/sam.11206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experts classifying data are often imprecise. Recently, several models have
been proposed to train classifiers using the noisy labels generated by these
experts. How to choose between these models? In such situations, the true
labels are unavailable. Thus, one cannot perform model selection using the
standard versions of methods such as empirical risk minimization and cross
validation. In order to allow model selection, we present a surrogate loss and
provide theoretical guarantees that assure its consistency. Next, we discuss
how this loss can be used to tune a penalization which introduces sparsity in
the parameters of a traditional class of models. Sparsity provides more
parsimonious models and can avoid overfitting. Nevertheless, it has seldom been
discussed in the context of noisy labels due to the difficulty in model
selection and, therefore, in choosing tuning parameters. We apply these
techniques to several sets of simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3295</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3295</id><created>2014-05-13</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>Effects of Sampling Methods on Prediction Quality. The Case of
  Classifying Land Cover Using Decision Trees</title><categories>stat.ML cs.LG stat.AP</categories><journal-ref>Proceedings of COMPSTAT 2014: 585-592. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clever sampling methods can be used to improve the handling of big data and
increase its usefulness. The subject of this study is remote sensing,
specifically airborne laser scanning point clouds representing different
classes of ground cover. The aim is to derive a supervised learning model for
the classification using CARTs. In order to measure the effect of different
sampling methods on the classification accuracy, various experiments with
varying types of sampling methods, sample sizes, and accuracy metrics have been
designed. Numerical results for a subset of a large surveying project covering
the lower Rhine area in Germany are shown. General conclusions regarding
sampling design are drawn and presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3296</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3296</id><created>2014-05-13</created><authors><author><keyname>Johnson</keyname><forenames>Samuel D.</forenames></author><author><keyname>Lu</keyname><forenames>Tsai-Ching</forenames></author></authors><title>Algorithm Instance Games</title><categories>cs.GT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces algorithm instance games (AIGs) as a conceptual
classification applying to games in which outcomes are resolved from joint
strategies algorithmically. For such games, a fundamental question asks: How do
the details of the algorithm's description influence agents' strategic
behavior?
  We analyze two versions of an AIG based on the set-cover optimization
problem. In these games, joint strategies correspond to instances of the
set-cover problem, with each subset (of a given universe of elements)
representing the strategy of a single agent. Outcomes are covers computed from
the joint strategies by a set-cover algorithm. In one variant of this game,
outcomes are computed by a deterministic greedy algorithm, and the other
variant utilizes a non-deterministic form of the greedy algorithm. We
characterize Nash equilibrium strategies for both versions of the game, finding
that agents' strategies can vary considerably between the two settings. In
particular, we find that the version of the game based on the deterministic
algorithm only admits Nash equilibrium in which agents choose strategies (i.e.,
subsets) containing at most one element, with no two agents picking the same
element. On the other hand, in the version of the game based on the
non-deterministic algorithm, Nash equilibrium strategies can include agents
with zero, one, or every element, and the same element can appear in the
strategies of multiple agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3302</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3302</id><created>2014-05-13</created><updated>2015-07-27</updated><authors><author><keyname>Maruccio</keyname><forenames>Claudio</forenames></author><author><keyname>De Lorenzis</keyname><forenames>Laura</forenames></author><author><keyname>Persano</keyname><forenames>Luana</forenames></author><author><keyname>Pisignano</keyname><forenames>Dario</forenames></author></authors><title>Computational homogenization of fibrous piezoelectric materials</title><categories>cs.CE</categories><comments>22 pages, 13 figures</comments><journal-ref>Computational Mechanics, Volume 55, Issue 5, pp. 983-998, 2015</journal-ref><doi>10.1007/s00466-015-1147-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexible piezoelectric devices made of polymeric materials are widely used
for micro- and nano-electro-mechanical systems. In particular, numerous recent
applications concern energy harvesting. Due to the importance of computational
modeling to understand the influence that microscale geometry and constitutive
variables exert on the macroscopic behavior, a numerical approach is developed
here for multiscale and multiphysics modeling of thin piezoelectric sheets made
of aligned arrays of polymeric nanofibers, manufactured by electrospinning. At
the microscale, the representative volume element consists in piezoelectric
polymeric nanofibers, assumed to feature a piezoelastic behavior and subjected
to electromechanical contact constraints. The latter are incorporated into the
virtual work equations by formulating suitable electric, mechanical and
coupling potentials and the constraints are enforced by using the penalty
method. From the solution of the micro-scale boundary value problem, a suitable
scale transition procedure leads to identifying the performance of a
macroscopic thin piezoelectric shell element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3311</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3311</id><created>2014-05-13</created><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author></authors><title>Beta Reduction is Invariant, Indeed (Long Version)</title><categories>cs.LO</categories><comments>29 pages</comments><acm-class>F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slot and van Emde Boas' weak invariance thesis states that reasonable
machines can simulate each other within a polynomially overhead in time. Is
$\lambda$-calculus a reasonable machine? Is there a way to measure the
computational complexity of a $\lambda$-term? This paper presents the first
complete positive answer to this long-standing problem. Moreover, our answer is
completely machine-independent and based over a standard notion in the theory
of $\lambda$-calculus: the length of a leftmost-outermost derivation to normal
form is an invariant cost model. Such a theorem cannot be proved by directly
relating $\lambda$-calculus with Turing machines or random access machines,
because of the size explosion problem: there are terms that in a linear number
of steps produce an exponentially long output. The first step towards the
solution is to shift to a notion of evaluation for which the length and the
size of the output are linearly related. This is done by adopting the linear
substitution calculus (LSC), a calculus of explicit substitutions modelled
after linear logic and proof-nets and admitting a decomposition of
leftmost-outermost derivations with the desired property. Thus, the LSC is
invariant with respect to, say, random access machines. The second step is to
show that LSC is invariant with respect to the $\lambda$-calculus. The size
explosion problem seems to imply that this is not possible: having the same
notions of normal form, evaluation in the LSC is exponentially longer than in
the $\lambda$-calculus. We solve such an impasse by introducing a new form of
shared normal form and shared reduction, deemed useful. Useful evaluation
avoids those steps that only unshare the output without contributing to
$\beta$-redexes, i.e., the steps that cause the blow-up in size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3316</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3316</id><created>2014-05-13</created><authors><author><keyname>Besbes</keyname><forenames>Omar</forenames></author><author><keyname>Gur</keyname><forenames>Yonatan</forenames></author><author><keyname>Zeevi</keyname><forenames>Assaf</forenames></author></authors><title>Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with
  Non-stationary Rewards</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-armed bandit (MAB) problem a gambler needs to choose at each round
of play one of K arms, each characterized by an unknown reward distribution.
Reward realizations are only observed when an arm is selected, and the
gambler's objective is to maximize his cumulative expected earnings over some
given horizon of play T. To do this, the gambler needs to acquire information
about arms (exploration) while simultaneously optimizing immediate rewards
(exploitation); the price paid due to this trade off is often referred to as
the regret, and the main question is how small can this price be as a function
of the horizon length T. This problem has been studied extensively when the
reward distributions do not change over time; an assumption that supports a
sharp characterization of the regret, yet is often violated in practical
settings. In this paper, we focus on a MAB formulation which allows for a broad
range of temporal uncertainties in the rewards, while still maintaining
mathematical tractability. We fully characterize the (regret) complexity of
this class of MAB problems by establishing a direct link between the extent of
allowable reward &quot;variation&quot; and the minimal achievable regret. Our analysis
draws some connections between two rather disparate strands of literature: the
adversarial and the stochastic MAB frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3318</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3318</id><created>2014-05-13</created><authors><author><keyname>Neufeld</keyname><forenames>James</forenames></author><author><keyname>Gy&#xf6;rgy</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Adaptive Monte Carlo via Bandit Allocation</title><categories>cs.AI cs.LG</categories><comments>The 31st International Conference on Machine Learning (ICML 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sequentially choosing between a set of unbiased
Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final
combined estimate. By reducing this task to a stochastic multi-armed bandit
problem, we show that well developed allocation strategies can be used to
achieve an MSE that approaches that of the best estimator chosen in retrospect.
We then extend these developments to a scenario where alternative estimators
have different, possibly stochastic costs. The outcome is a new set of adaptive
Monte Carlo strategies that provide stronger guarantees than previous
approaches while offering practical advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3322</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3322</id><created>2014-05-13</created><updated>2015-06-11</updated><authors><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>Inapproximability of Nash Equilibrium</title><categories>cs.GT</categories><comments>Fourth revision includes a new corollary for $\epsilon$-relative Nash
  equilibrium in two-player games, as well as new figures. Second revision
  includes new corollaries for Bayesian Nash equilibrium in two-player
  incomplete information games and for market equilibrium in a non-monotone
  market. arXiv admin note: text overlap with arXiv:1405.0524</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that finding an $\epsilon$-approximate Nash equilibrium is
PPAD-complete for constant $\epsilon$ and a particularly simple class of games:
polymatrix, degree 3 graphical games, in which each player has only two
actions.
  As corollaries, we also prove similar inapproximability results for Bayesian
Nash equilibrium in a two-player incomplete information game with a constant
number of actions, for relative $\epsilon$-Nash equilibrium in a two-player
game, for market equilibrium in a non-monotone market, for the generalized
circuit problem defined by Chen, Deng, and Teng [CDT'09], and for approximate
competitive equilibrium from equal incomes with indivisible goods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3323</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3323</id><created>2014-05-13</created><authors><author><keyname>Douglas</keyname><forenames>Niall</forenames></author></authors><title>Large Code Base Change Ripple Management in C++: My thoughts on how a
  new Boost C++ Library could help</title><categories>cs.PL</categories><comments>Proceedings of the C++ Now 2014 Conference, Aspen, Colorado, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  C++ 98/03 already has a reputation for overwhelming complexity compared to
other programming languages. The raft of new features in C++ 11/14 suggests
that the complexity in the next generation of C++ code bases will overwhelm
still further. The planned C++ 17 will probably worsen matters in ways
difficult to presently imagine.
  Countervailing against this rise in software complexity is the hard
de-exponentialisation of computer hardware capacity growth expected no later
than 2020, and which will have even harder to imagine consequences on all
computer software. WG21 C++ 17 study groups SG2 (Modules), SG7 (Reflection),
SG8 (Concepts), and to a lesser extent SG10 (Feature Test) and SG12 (Undefined
Behaviour), are all fundamentally about significantly improving complexity
management in C++ 17, yet WG21's significant work on improving C++ complexity
management is rarely mentioned explicitly.
  This presentation pitches a novel implementation solution for some of these
complexity scaling problems, tying together SG2 and SG7 with parts of SG3
(Filesystem): a standardised but very lightweight transactional graph database
based on Boost.ASIO, Boost.AFIO and Boost.Graph at the very core of the C++
runtime, making future C++ codebases considerably more tractable and affordable
to all users of C++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3342</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3342</id><created>2014-05-13</created><authors><author><keyname>Shafiee</keyname><forenames>M. Ehsan</forenames></author><author><keyname>Zechman</keyname><forenames>Emily M.</forenames></author></authors><title>An Agent-based Modeling Framework for Sociotechnical Simulation of Water
  Distribution Contamination Events</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the event that a bacteriological or chemical toxin is intro- duced to a
water distribution network, a large population of consumers may become exposed
to the contaminant. A contamination event may be poorly predictable dynamic
process due to the interactions of consumers and utility managers during an
event. Consumers that become aware of a threat may select protective actions
that change their water demands from typical demand patterns, and new hydraulic
conditions can arise that differ from conditions that are predicted when
demands are considered as exogenous inputs. Consequently, the movement of the
contaminant plume in the pipe network may shift from its expected trajectory. A
sociotechnical model is developed here to integrate agent-based models of
consumers with an engineering water distribution system model and capture the
dynamics between consumer behaviors and the water distribution system for
predicting contaminant transport and public exposure. Consumers are simulated
as agents with behaviors defined for water use activities, mobility,
word-of-mouth communication, and demand reduction, based on a set of rules
representing an agents autonomy and reaction to health impacts, the
environment, and the actions of other agents. As consumers decrease their water
use, the demand exerted on the water distribution system is updated; as the
flow directions and volumes shift in response, the location of the contaminant
plume is updated and the amount of contaminant consumed by each agent is
calculated. The framework is tested through simulating realistic contamination
scenarios for a virtual city and water distribution system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3347</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3347</id><created>2014-05-13</created><updated>2014-12-31</updated><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>XianFang</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Self-dual codes and quadratic residue codes over the ring
  $\mathbb{Z}_9+u\mathbb{Z}_9$</title><categories>cs.IT math.IT math.RA</categories><comments>12 pages. arXiv admin note: substantial text overlap with
  arXiv:1402.6771</comments><msc-class>94B05, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new definitions of the Gray weight and the Gray
map for linear codes over $\mathbb{Z}_9+u\mathbb{Z}_9$ with $u^2=u$. Some
results on self-dual codes over this ring are investigated. Further, the
structural properties of quadratic residue codes are also considered. Two
self-dual codes with parameters $[22,11,5]$ and $[24,12,9]$ over $\mathbb{Z}_9$
are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3351</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3351</id><created>2014-05-13</created><authors><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Group-based Sparse Representation for Image Restoration</title><categories>cs.CV</categories><comments>34 pages, 6 tables, 19 figures, to be published in IEEE Transactions
  on Image Processing; Project, Code and High resolution PDF version can be
  found: http://idm.pku.edu.cn/staff/zhangjian/. arXiv admin note: text overlap
  with arXiv:1404.7566</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional patch-based sparse representation modeling of natural images
usually suffer from two problems. First, it has to solve a large-scale
optimization problem with high computational complexity in dictionary learning.
Second, each patch is considered independently in dictionary learning and
sparse coding, which ignores the relationship among patches, resulting in
inaccurate sparse coding coefficients. In this paper, instead of using patch as
the basic unit of sparse representation, we exploit the concept of group as the
basic unit of sparse representation, which is composed of nonlocal patches with
similar structures, and establish a novel sparse representation modeling of
natural images, called group-based sparse representation (GSR). The proposed
GSR is able to sparsely represent natural images in the domain of group, which
enforces the intrinsic local sparsity and nonlocal self-similarity of images
simultaneously in a unified framework. Moreover, an effective self-adaptive
dictionary learning method for each group with low complexity is designed,
rather than dictionary learning from natural images. To make GSR tractable and
robust, a split Bregman based technique is developed to solve the proposed
GSR-driven minimization problem for image restoration efficiently. Extensive
experiments on image inpainting, image deblurring and image compressive sensing
recovery manifest that the proposed GSR modeling outperforms many current
state-of-the-art schemes in both PSNR and visual perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3352</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3352</id><created>2014-05-13</created><updated>2014-06-30</updated><authors><author><keyname>Lu</keyname><forenames>F.</forenames></author><author><keyname>Chen</keyname><forenames>Z.</forenames></author></authors><title>Newton-Type Iterative Solver for Multiple View $L2$ Triangulation</title><categories>cs.CV cs.GR</categories><comments>15 pages, 1 figure, 4 tables, 30 references, C++ source codes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that the L2 optimal solutions to most real multiple
view L2 triangulation problems can be efficiently obtained by two-stage
Newton-like iterative methods, while the difficulty of such problems mainly
lies in how to verify the L2 optimality. Such a working two-stage bundle
adjustment approach features: first, the algorithm is initialized by symmedian
point triangulation, a multiple-view generalization of the mid-point method;
second, a symbolic-numeric method is employed to compute derivatives
accurately; third, globalizing strategy such as line search or trust region is
smoothly applied to the underlying iteration which assures algorithm robustness
in general cases.
  Numerical comparison with tfml method shows that the local minimizers
obtained by the two-stage iterative bundle adjustment approach proposed here
are also the L2 optimal solutions to all the calibrated data sets available
online by the Oxford visual geometry group. Extensive numerical experiments
indicate the bundle adjustment approach solves more than 99% the real
triangulation problems optimally. An IEEE 754 double precision C++
implementation shows that it takes only about 0.205 second tocompute allthe
4983 points in the Oxford dinosaur data setvia Gauss-Newton iteration hybrid
with a line search strategy on a computer with a 3.4GHz Intel i7 CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3353</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3353</id><created>2014-05-13</created><authors><author><keyname>Nghiem</keyname><forenames>Minh-Quoc</forenames></author><author><keyname>Kristianto</keyname><forenames>Giovanni Yoko</forenames></author><author><keyname>Topic</keyname><forenames>Goran</forenames></author><author><keyname>Aizawa</keyname><forenames>Akiko</forenames></author></authors><title>Which one is better: presentation-based or content-based math search?</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mathematical content is a valuable information source and retrieving this
content has become an important issue. This paper compares two searching
strategies for math expressions: presentation-based and content-based
approaches. Presentation-based search uses state-of-the-art math search system
while content-based search uses semantic enrichment of math expressions to
convert math expressions into their content forms and searching is done using
these content-based expressions. By considering the meaning of math
expressions, the quality of search system is improved over presentation-based
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3354</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3354</id><created>2014-05-13</created><authors><author><keyname>Yang</keyname><forenames>Mingrui</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank</forenames></author></authors><title>New Coherence and RIP Analysis for Weak Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.1949</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define a new coherence index, named the global 2-coherence,
of a given dictionary and study its relationship with the traditional mutual
coherence and the restricted isometry constant. By exploring this relationship,
we obtain more general results on sparse signal reconstruction using greedy
algorithms in the compressive sensing (CS) framework. In particular, we obtain
an improved bound over the best known results on the restricted isometry
constant for successful recovery of sparse signals using orthogonal matching
pursuit (OMP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3360</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3360</id><created>2014-05-14</created><authors><author><keyname>Hasan</keyname><forenames>Cengis</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames></author></authors><title>On the Nash Stability in the Hedonic Coalition Formation Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the Nash stability in hedonic coalition formation games.
We address the following issue: for a general problem formulation, is there any
utility allocation method ensuring a Nash-stable partition? We propose the
definition of the Nash-stable core and we analyze the conditions for having a
non-empty Nash-stable core. More precisely, we prove that using relaxed
efficiency in utility sharing allows to ensure a non empty Nash-stable core.
Then, a decentralized algorithm called Nash stability establisher is proposed
for finding the Nash stability in a game whenever at least one exists. The
problem of finding the Nash stability is formulated as a non-cooperative game.
In the proposed approach, during each round, each player determines its
strategy in its turn according to a random round-robin scheduler. We prove that
the algorithm converges to an equilibrium if it exists, which is the indicator
of the Nash stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3362</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3362</id><created>2014-05-14</created><authors><author><keyname>Aziz</keyname><forenames>Rehan Abdul</forenames></author><author><keyname>Chu</keyname><forenames>Geoffrey</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter James</forenames></author></authors><title>Grounding Bound Founded Answer Set Programs</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear in Theory and Practice of Logic Programming (TPLP)
  Bound Founded Answer Set Programming (BFASP) is an extension of Answer Set
Programming (ASP) that extends stable model semantics to numeric variables.
While the theory of BFASP is defined on ground rules, in practice BFASP
programs are written as complex non-ground expressions. Flattening of BFASP is
a technique used to simplify arbitrary expressions of the language to a small
and well defined set of primitive expressions. In this paper, we first show how
we can flatten arbitrary BFASP rule expressions, to give equivalent BFASP
programs. Next, we extend the bottom-up grounding technique and magic set
transformation used by ASP to BFASP programs. Our implementation shows that for
BFASP problems, these techniques can significantly reduce the ground program
size, and improve subsequent solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3365</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3365</id><created>2014-05-14</created><authors><author><keyname>Bi</keyname><forenames>Yi</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author><author><keyname>Feng</keyname><forenames>Zhiyong</forenames></author></authors><title>A Well-Founded Semantics for FOL-Programs</title><categories>cs.PL</categories><comments>10 pages, ICLP2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An FOL-program consists of a background theory in a decidable fragment of
first-order logic and a collection of rules possibly containing first-order
formulas. The formalism stems from recent approaches to tight integrations of
ASP with description logics. In this paper, we define a well-founded semantics
for FOL-programs based on a new notion of unfounded sets on consistent as well
as inconsistent sets of literals, and study some of its properties. The
semantics is defined for all FOL-programs, including those where it is
necessary to represent inconsistencies explicitly. The semantics supports a
form of combined reasoning by rules under closed world as well as open world
assumptions, and it is a generalization of the standard well-founded semantics
for normal logic programs. We also show that the well-founded semantics defined
here approximates the well-supported answer set semantics for normal DL
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3367</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3367</id><created>2014-05-14</created><authors><author><keyname>Aziz</keyname><forenames>Rehan Abdul</forenames></author></authors><title>Bound Founded Answer Set Programming</title><categories>cs.AI</categories><comments>An extended abstract / full version of a paper accepted to be
  presented at the Doctoral Consortium of the 30th International Conference on
  Logic Programming (ICLP 2014), July 19-22, Vienna, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a powerful modelling formalism that is very
efficient in solving combinatorial problems. ASP solvers implement the stable
model semantics that eliminates circular derivations between Boolean variables
from the solutions of a logic program. Due to this, ASP solvers are better
suited than propositional satisfiability (SAT) and Constraint Programming (CP)
solvers to solve a certain class of problems whose specification includes
inductive definitions such as reachability in a graph. On the other hand, ASP
solvers suffer from the grounding bottleneck that occurs due to their inability
to model finite domain variables. Furthermore, the existing stable model
semantics are not sufficient to disallow circular reasoning on the bounds of
numeric variables. An example where this is required is in modelling shortest
paths between nodes in a graph. Just as reachability can be encoded as an
inductive definition with one or more base cases and recursive rules, shortest
paths between nodes can also be modelled with similar base cases and recursive
rules for their upper bounds. This deficiency of stable model semantics
introduces another type of grounding bottleneck in ASP systems that cannot be
removed by naively merging ASP with CP solvers, but requires a theoretical
extension of the semantics from Booleans and normal rules to bounds over
numeric variables and more general rules. In this work, we propose Bound
Founded Answer Set Programming (BFASP) that resolves this issue and
consequently, removes all types of grounding bottleneck inherent in ASP
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3368</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3368</id><created>2014-05-14</created><authors><author><keyname>Jiang</keyname><forenames>Lurong</forenames></author><author><keyname>Jin</keyname><forenames>Xinyu</forenames></author><author><keyname>Xia</keyname><forenames>Yongxiang</forenames></author><author><keyname>Ouyang</keyname><forenames>Bo</forenames></author><author><keyname>Wu</keyname><forenames>Duanpo</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author></authors><title>A Scale-Free Topology Construction Model for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>13pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A local-area and energy-efficient (LAEE) evolution model for wireless sensor
networks is proposed. The process of topology evolution is divided into two
phases. In the first phase, nodes are distributed randomly in a fixed region.
In the second phase, according to the spatial structure of wireless sensor
networks, topology evolution starts from the sink, grows with an
energy-efficient preferential attachment rule in the new node's local-area, and
stops until all nodes are connected into network. Both analysis and simulation
results show that the degree distribution of LAEE follows the power law. This
topology construction model has better tolerance against energy depletion or
random failure than other non-scale-free WSN topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3376</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3376</id><created>2014-05-14</created><authors><author><keyname>Hunter</keyname><forenames>Anthony</forenames></author><author><keyname>Thimm</keyname><forenames>Matthias</forenames></author></authors><title>Probabilistic Argumentation with Epistemic Extensions and Incomplete
  Information</title><categories>cs.AI</categories><msc-class>68T30</msc-class><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract argumentation offers an appealing way of representing and evaluating
arguments and counterarguments. This approach can be enhanced by a probability
assignment to each argument. There are various interpretations that can be
ascribed to this assignment. In this paper, we regard the assignment as
denoting the belief that an agent has that an argument is justifiable, i.e.,
that both the premises of the argument and the derivation of the claim of the
argument from its premises are valid. This leads to the notion of an epistemic
extension which is the subset of the arguments in the graph that are believed
to some degree (which we defined as the arguments that have a probability
assignment greater than 0.5). We consider various constraints on the
probability assignment. Some constraints correspond to standard notions of
extensions, such as grounded or stable extensions, and some constraints give us
new kinds of extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3377</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3377</id><created>2014-05-14</created><authors><author><keyname>Kibelloh</keyname><forenames>Mboni</forenames></author><author><keyname>Bao</keyname><forenames>Yukun</forenames></author></authors><title>Perceptions of International Female Students Towards E-learning in
  Resolving High Education and Family Role Strain</title><categories>cs.CY</categories><comments>31 pages</comments><doi>10.2190/EC.50.4.b</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a common phenomenon for many mature female international students
enrolled in high education overseas to experience strain from managing
conflicting roles of student and family, and difficulties of cross-cultural
adjustment. The purpose of this study is to examine perceptions and behavioral
intentions of international female students towards e-learning as a tool for
resolving overseas high education and family strain from a technology
acceptance standpoint. To achieve this goal, Davis's (1989) technology
acceptance model is used as the study's conceptual framework, to investigate
perceived usefulness, ease of use and behavioral intentions towards e-learning.
The research draws on face-to-face interviews with 21 female international
students enrolled in classroom taught degree programs at a university in Wuhan,
China. The data is analyzed through coding and transcribing. The findings
reveal that given its convenience, e-learning is generally perceived as
practical in balancing study with family as well as feasible in saving time,
money and energy. However, key concerns were raised over the issues of poor and
costly Internet connectivity in developing countries, as well as perceived
negative reputation, lack of face-to-face interaction and lack of motivation in
online environment. Important issues and recommendations are raised for
consideration when promoting e-learning programme. This study emphasis on the
need to revisit gender supporting policies and effective marketing to
re-position the prevailing image of e-learning as a reputable and reliable
education delivery method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3378</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3378</id><created>2014-05-14</created><authors><author><keyname>Lahoz-Beltra</keyname><forenames>Rafael</forenames></author></authors><title>The &quot;crisis of noosphere&quot; as a limiting factor to achieve the point of
  technological singularity</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most significant developments in the history of human being is the
invention of a way of keeping records of human knowledge, thoughts and ideas.
In 1926, the work of several thinkers such as Edouard Le Roy, Vladimir
Vernadsky and Teilhard de Chardin led to the concept of noosphere, thus the
idea that human cognition and knowledge transforms the biosphere coming to be
something like the planet's thinking layer. At present, is commonly accepted by
some thinkers that the Internet is the medium that brings life to noosphere.
According to Vinge and Kurzweil's technological singularity hypothesis,
noosphere would be in the future the natural environment in which
'human-machine superintelligence' emerges after to reach the point of
technological singularity. In this paper we show by means of a numerical model
the impossibility that our civilization reaches the point of technological
singularity in the near future. We propose that this point may be reached when
Internet data centers are based on &quot;computer machines&quot; to be more effective in
terms of power consumption than current ones. We speculate about what we have
called 'Nooscomputer' or N-computer a hypothetical machine which would consume
far less power allowing our civilization to reach the point of technological
singularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3381</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3381</id><created>2014-05-14</created><authors><author><keyname>Kibelloh</keyname><forenames>Mboni</forenames></author><author><keyname>Bao</keyname><forenames>Yukun</forenames></author></authors><title>Can Online MBA Programs Allow Professional Working Mothers to Balance
  Work, Family, and Career Progression? A Case Study in China</title><categories>cs.CY</categories><comments>31 pages</comments><doi>10.1007/s40299-013-0101-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Career progression is a general concern of professional working mothers in
China. The purpose of this paper is to report a qualitative study of Chinese
professional working mothers that explored the perceptions of online Master's
of Business Administration (MBA) programmes as a tool for career progression
for working mothers balancing work and family in China. The objective was to
examine existing work-family and career progression conflicts, the perceived
usefulness of online MBA in balancing work-family and career aspirations, and
the perceived ease of use of e-learning. Using Davis's (1989) technology
acceptance model (TAM), the research drew on in-depth interviews with 10 female
part-time MBA students from a university in Wuhan. The data were analysed
through coding and transcribing. The findings showed that conflicts arose where
demanding work schedules competed with family obligations, studies, and caring
for children and the elderly. Online MBA programmes were viewed as a viable
tool for balancing work and family and studying, given its flexible time
management capabilities. However, consideration must be given to address
students' motivation issues, lack of networking, lack of face-to-face
interaction, and quality. The research findings emphasise the pragmatic need to
re-align higher education policy and practice to position higher education
e-learning as a trustable education delivery channel in China. By shedding
light on the prevailing work-family conflict experienced by women seeking
career advancement, this study suggests developing better gender-supporting
policies and innovative e-learning practices to champion online MBA programme
for this target niche.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3382</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3382</id><created>2014-05-14</created><authors><author><keyname>Khoshrou</keyname><forenames>Samaneh</forenames></author><author><keyname>Cardoso</keyname><forenames>Jaime S.</forenames></author><author><keyname>Teixeira</keyname><forenames>Luis F.</forenames></author></authors><title>Active Mining of Parallel Video Streams</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The practicality of a video surveillance system is adversely limited by the
amount of queries that can be placed on human resources and their vigilance in
response. To transcend this limitation, a major effort under way is to include
software that (fully or at least semi) automatically mines video footage,
reducing the burden imposed to the system. Herein, we propose a semi-supervised
incremental learning framework for evolving visual streams in order to develop
a robust and flexible track classification system. Our proposed method learns
from consecutive batches by updating an ensemble in each time. It tries to
strike a balance between performance of the system and amount of data which
needs to be labelled. As no restriction is considered, the system can address
many practical problems in an evolving multi-camera scenario, such as concept
drift, class evolution and various length of video streams which have not been
addressed before. Experiments were performed on synthetic as well as real-world
visual data in non-stationary environments, showing high accuracy with fairly
little human collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3391</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3391</id><created>2014-05-14</created><authors><author><keyname>Stojanovic</keyname><forenames>Sana</forenames><affiliation>INRIA Nancy - Grand Est / LSIIT, ICube</affiliation></author><author><keyname>Narboux</keyname><forenames>Julien</forenames><affiliation>INRIA Nancy - Grand Est / LSIIT, ICube</affiliation></author><author><keyname>Bezem</keyname><forenames>Marc</forenames></author><author><keyname>Janicic</keyname><forenames>Predrag</forenames></author></authors><title>A Vernacular for Coherent Logic</title><categories>cs.LO</categories><comments>CICM 2014 - Conferences on Intelligent Computer Mathematics (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple, yet expressive proof representation from which proofs
for different proof assistants can easily be generated. The representation uses
only a few inference rules and is based on a frag- ment of first-order logic
called coherent logic. Coherent logic has been recognized by a number of
researchers as a suitable logic for many ev- eryday mathematical developments.
The proposed proof representation is accompanied by a corresponding XML format
and by a suite of XSL transformations for generating formal proofs for
Isabelle/Isar and Coq, as well as proofs expressed in a natural language form
(formatted in LATEX or in HTML). Also, our automated theorem prover for
coherent logic exports proofs in the proposed XML format. All tools are
publicly available, along with a set of sample theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3393</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3393</id><created>2014-05-14</created><authors><author><keyname>Duck</keyname><forenames>Gregory J.</forenames></author><author><keyname>Haemmerle</keyname><forenames>Remy</forenames></author><author><keyname>Sulzmann</keyname><forenames>Martin</forenames></author></authors><title>On Termination, Confluence and Consistent CHR-based Type Inference</title><categories>cs.PL</categories><doi>10.1017/S1471068414000246</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the application of Constraint Handling Rules (CHR) for the
specification of type inference systems, such as that used by Haskell.
Confluence of CHR guarantees that the answer provided by type inference is
correct and consistent. The standard method for establishing confluence relies
on an assumption that the CHR program is terminating. However, many examples in
practice give rise to non-terminating CHR programs, rendering this method
inapplicable. Despite no guarantee of termination or confluence, the Glasgow
Haskell Compiler (GHC) supports options that allow the user to proceed with
type inference anyway, e.g. via the use of the UndecidableInstances flag. In
this paper we formally identify and verify a set of relaxed criteria, namely
range-restrictedness, local confluence, and ground termination, that ensure the
consistency of CHR-based type inference that maps to potentially
non-terminating CHR programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3394</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3394</id><created>2014-05-14</created><authors><author><keyname>Wang</keyname><forenames>Shangping</forenames></author><author><keyname>Feng</keyname><forenames>Fang</forenames></author></authors><title>Large Universe Attribute-Based Encryption Scheme from Lattices</title><categories>cs.CR</categories><comments>18 pages</comments><msc-class>94A60, 81T25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a large universe attribute-based encryption (ABE ) scheme from
lattices. It is inspired by Brent Waters' scheme which is a large universe
attribute-based encryption using bilinear map. It is a very practical scheme
but this scheme may not be security with the developing quantum computer. So we
extend their good idea of large universe attribute-based encryption to lattices
based cryptosystem. And our scheme is the first large universe ABE scheme from
lattices. In a large universe ABE system any string can be used as attribute
and attributes need not be determined at system setup. This is a desirable
feature. And the master private key of our scheme is too short too a matrix.
Moreover, our scheme is high efficient due to the ciphertext of our scheme is
divided into three parts. Finally, under Learning with Errors assumption, we
prove our scheme is secure under the select attribute attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3396</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3396</id><created>2014-05-14</created><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author></authors><title>Reducing Dueling Bandits to Cardinal Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms for reducing the Dueling Bandits problem to the
conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits
problem is an online model of learning with ordinal feedback of the form &quot;A is
preferred to B&quot; (as opposed to cardinal feedback like &quot;A has value 2.5&quot;),
giving it wide applicability in learning from implicit user feedback and
revealed and stated preferences. In contrast to existing algorithms for the
Dueling Bandits problem, our reductions -- named $\Doubler$, $\MultiSbm$ and
$\DoubleSbm$ -- provide a generic schema for translating the extensive body of
known results about conventional Multi-Armed Bandit algorithms to the Dueling
Bandits setting. For $\Doubler$ and $\MultiSbm$ we prove regret upper bounds in
both finite and infinite settings, and conjecture about the performance of
$\DoubleSbm$ which empirically outperforms the other two as well as previous
algorithms in our experiments. In addition, we provide the first almost optimal
regret bound in terms of second order terms, such as the differences between
the values of the arms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3410</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3410</id><created>2014-05-14</created><authors><author><keyname>Chen</keyname><forenames>Tieming</forenames></author><author><keyname>Zhang</keyname><forenames>Xu</forenames></author><author><keyname>Jin</keyname><forenames>Shichao</forenames></author><author><keyname>Kim</keyname><forenames>Okhee</forenames></author></authors><title>Efficient classification using parallel and scalable compressed model
  and Its application on intrusion detection</title><categories>cs.LG cs.CR</categories><doi>10.1016/j.eswa.2014.04.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to achieve high efficiency of classification in intrusion detection,
a compressed model is proposed in this paper which combines horizontal
compression with vertical compression. OneR is utilized as horizontal
com-pression for attribute reduction, and affinity propagation is employed as
vertical compression to select small representative exemplars from large
training data. As to be able to computationally compress the larger volume of
training data with scalability, MapReduce based parallelization approach is
then implemented and evaluated for each step of the model compression process
abovementioned, on which common but efficient classification methods can be
directly used. Experimental application study on two publicly available
datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the
classification using the compressed model proposed can effectively speed up the
detection procedure at up to 184 times, most importantly at the cost of a
minimal accuracy difference with less than 1% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3411</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3411</id><created>2014-05-14</created><authors><author><keyname>Chandio</keyname><forenames>Aftab Ahmed</forenames></author><author><keyname>Yu</keyname><forenames>Zhibin</forenames></author><author><keyname>Syed</keyname><forenames>Feroz Shah</forenames></author><author><keyname>Korejo</keyname><forenames>Imtiaz Ali</forenames></author></authors><title>A Case Study on Job Scheduling Policy for Workload Characterization and
  Power Efficiency</title><categories>cs.DC</categories><comments>6 pages, 4 figures</comments><journal-ref>Sindh University Research Journal (Science Series) Vol. 45(A-1)
  23- 28 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing popularity of cloud computing, datacenters are becoming
more important than ever before. A typical datacenter typically consists of a
large number of homogeneous or heterogeneous servers connected by networks.
Unfortunately, these servers and network equipment are often under-utilized and
power hungry. To improve the utilization of hardware resources and make them
power efficiency in datacenters, workload characterization and analysis is at
the foundation. In this paper, we characterize and analyze the job arriving
rate, arriving time, job length, power consumption, and temperature dissipation
in a real world datacenter by using statistical methods. From the
characterization, we find unique features in the workload can be used to
optimize the resource utilization and power consumption of datacenters
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3422</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3422</id><created>2014-05-14</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>L&#xe9;v&#xea;que</keyname><forenames>Benjamin</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>Planar graphs with $\Delta\geq 7$ and no triangle adjacent to a $C_4$
  are minimally edge and total choosable</title><categories>cs.DM math.CO</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For planar graphs, we consider the problems of \emph{list edge coloring} and
\emph{list total coloring}. Edge coloring is the problem of coloring the edges
while ensuring that two edges that are adjacent receive different colors. Total
coloring is the problem of coloring the edges and the vertices while ensuring
that two edges that are adjacent, two vertices that are adjacent, or a vertex
and an edge that are incident receive different colors. In their list
extensions, instead of having the same set of colors for the whole graph, every
vertex or edge is assigned some set of colors and has to be colored from it. A
graph is minimally edge or total choosable if it is list edge
$\Delta$-colorable or list total $(\Delta+1)$-colorable, respectively, where
$\Delta$ is the maximum degree in the graph.
  It is already known that planar graphs with $\Delta\geq 8$ and no triangle
adjacent to a $C_4$ are minimally edge and total choosable (Li Xu 2011), and
that planar graphs with $\Delta\geq 7$ and no triangle sharing a vertex with a
$C_4$ or no triangle adjacent to a $C_k$ ($\forall 3 \leq k \leq 6$) are
minimally total colorable (Wang Wu 2011). We strengthen here these results and
prove that planar graphs with $\Delta\geq 7$ and no triangle adjacent to a
$C_4$ are minimally edge and total choosable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3426</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3426</id><created>2014-05-14</created><authors><author><keyname>Johansson</keyname><forenames>Moa</forenames></author><author><keyname>Rosen</keyname><forenames>Dan</forenames></author><author><keyname>Smallbone</keyname><forenames>Nicholas</forenames></author><author><keyname>Claessen</keyname><forenames>Koen</forenames></author></authors><title>Hipster: Integrating Theory Exploration in a Proof Assistant</title><categories>cs.LO cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Hipster, a system integrating theory exploration with
the proof assistant Isabelle/HOL. Theory exploration is a technique for
automatically discovering new interesting lemmas in a given theory development.
Hipster can be used in two main modes. The first is exploratory mode, used for
automatically generating basic lemmas about a given set of datatypes and
functions in a new theory development. The second is proof mode, used in a
particular proof attempt, trying to discover the missing lemmas which would
allow the current goal to be proved. Hipster's proof mode complements and
boosts existing proof automation techniques that rely on automatically
selecting existing lemmas, by inventing new lemmas that need induction to be
proved. We show example uses of both modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3427</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3427</id><created>2014-05-14</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Faggian</keyname><forenames>Claudia</forenames></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames></author><author><keyname>Yoshimizu</keyname><forenames>Akira</forenames></author></authors><title>The Geometry of Synchronization (Long Version)</title><categories>cs.LO</categories><comments>26 pages</comments><acm-class>F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We graft synchronization onto Girard's Geometry of Interaction in its most
concrete form, namely token machines. This is realized by introducing
proof-nets for SMLL, an extension of multiplicative linear logic with a
specific construct modeling synchronization points, and of a multi-token
abstract machine model for it. Interestingly, the correctness criterion ensures
the absence of deadlocks along reduction and in the underlying machine, this
way linking logical and operational properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3448</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3448</id><created>2014-05-14</created><authors><author><keyname>Beheshtifard</keyname><forenames>Ziaeddin</forenames></author><author><keyname>Meybodi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Online Channel Assignment in Multi-Radio Wireless Mesh Networks Using
  Learning Automata</title><categories>cs.NI</categories><comments>7 pages, 4 figures</comments><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 3, pp. 726-732, 2013</journal-ref><doi>10.7321/jscse.v3.n3.110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we look into the problem of channel assignment in
multi-channel multi-radio wireless mesh networks. We propose a new learning
automata based channel assignment scheme that adaptively improve network
overall throughput by expecting channel state. Since the ability of sending
packets via upstream links will be evaluation bases for assigning channels to
radio interfaces on each node. We use a link capacity function that potentially
reflects degree of interferences imposed by selected channels by each node.
According to dynamics of system, proposed algorithm assigns channels to radio
interface in distributed fashion such that minimize interference in
neighborhood of a node. We analyze the stability of the system via appropriate
Lyapunov-like trajectory; we show that stability and optimum point of the
system is converged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3451</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3451</id><created>2014-05-14</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Vyskocil</keyname><forenames>Jiri</forenames></author><author><keyname>Geuvers</keyname><forenames>Herman</forenames></author></authors><title>Developing Corpus-based Translation Methods between Informal and Formal
  Mathematics: Project Description</title><categories>cs.AI cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this project is to (i) accumulate annotated informal/formal
mathematical corpora suitable for training semi-automated translation between
informal and formal mathematics by statistical machine-translation methods,
(ii) to develop such methods oriented at the formalization task, and in
particular (iii) to combine such methods with learning-assisted automated
reasoning that will serve as a strong semantic component. We describe these
ideas, the initial set of corpora, and some initial experiments done over them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3454</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3454</id><created>2014-05-14</created><updated>2014-05-28</updated><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author></authors><title>A Straightforward Preprocessing Approach for Accelerating Convex Hull
  Computations on the GPU</title><categories>cs.CG</categories><comments>Preprint version of a short paper, for reviewing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An effective strategy for accelerating the calculation of convex hulls for
point sets is to filter the input points by discarding interior points. In this
paper, we present such a straightforward and efficient preprocessing approach
by exploiting the GPU. The basic idea behind our approach is to discard the
points that locate inside a convex polygon formed by 16 extreme points. Due to
the fact that the extreme points of a point set do not alter when all points
are rotated in the same angle, four groups of extreme points with min or max x
or y coordinates can be found in the original point set and three rotated point
sets. These 16 extreme points are then used to form a convex polygon. We check
all input points and discard the points that locate inside the convex polygon.
We use the remaining points to calculate the expected convex hull. Experimental
results show that: when employing the proposed preprocessing algorithm, it
achieves the speedups of about 4x ~5x on average and 5x ~ 6x in the best cases
over the cases where the proposed approach is not used. In addition, more than
99% input points can be discarded in most experimental tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3458</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3458</id><created>2014-05-14</created><authors><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author></authors><title>An Overview of Transience Bounds in Max-Plus Algebra</title><categories>math.CO cs.DM</categories><comments>13 pages, 2 figures</comments><msc-class>15A80 (Primary) 05C20, 05C22, 05C50 (Secondary)</msc-class><journal-ref>G. L. Litvinov and S. N. Sergeev (eds.) Tropical and Idempotent
  Mathematics and Applications, volume 616 of Contemporary Mathematics, pages
  277-289. American Mathematical Society, Providence, 2014</journal-ref><doi>10.1090/conm/616/12306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey and discuss upper bounds on the length of the transient phase of
max-plus linear systems and sequences of max-plus matrix powers. In particular,
we explain how to extend a result by Nachtigall to yield a new approach for
proving such bounds and we state an asymptotic tightness result by using an
example given by Hartmann and Arguelles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3460</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3460</id><created>2014-05-14</created><authors><author><keyname>Riquelme</keyname><forenames>Fabi&#xe1;n</forenames></author></authors><title>Satisfaction in societies with opinion leaders and mediators: properties
  and an axiomatization</title><categories>cs.GT</categories><comments>12 pages</comments><msc-class>91D10, 91A12, 91A35, 91A80, 91D30</msc-class><acm-class>J.4; F.1.1; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the opinion leader-follower through mediators
systems (OLFM systems) a multiple-action collective choice model for societies.
In those societies three kind of actors are considered: opinion leaders that
can exert certain influence over the decision of other actors, followers that
can be convinced to modify their original decisions, and independent actors
that neither are influenced nor can influence; mediators are actors that both
are influenced and influence other actors. This is a generalization of the
opinion leader-follower systems (OLF systems) proposed by van den Brink R, et
al. (2011).
  The satisfaction score is defined on the set of actors. For each actor it
measures the number of society initial decisions in which the final collective
decision coincides with the one that the actor initially selected. We
generalize in OLFM systems some properties that the satisfaction score meets
for OLF systems. By using these properties, we provide an axiomatization of the
satisfaction score for the case in which followers maintain their own initial
decisions unless all their opinion leaders share an opposite inclination. This
new axiomatization generalizes the one given by van den Brink R, et al. (2012)
for OLF systems under the same restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3461</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3461</id><created>2014-05-14</created><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author></authors><title>Formulating problems for real algebraic geometry</title><categories>cs.SC</categories><comments>To be presented at The &quot;Encuentros de \'Algebra Computacional y
  Aplicaciones, EACA 2014&quot; (Meetings on Computer Algebra and Applications) in
  Barcelona</comments><msc-class>68W30, 03C10</msc-class><acm-class>I.1.2</acm-class><journal-ref>Proceedings XIV Encuentros de \'Algebra Computacional y
  Aplicaciones (EACA '14), pp. 107-110, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss issues of problem formulation for algorithms in real algebraic
geometry, focussing on quantifier elimination by cylindrical algebraic
decomposition. We recall how the variable ordering used can have a profound
effect on both performance and output and summarise what may be done to assist
with this choice. We then survey other questions of problem formulation and
algorithm optimisation that have become pertinent following advances in CAD
theory, including both work that is already published and work that is
currently underway. With implementations now in reach of real world
applications and new theory meaning algorithms are far more sensitive to the
input, our thesis is that intelligently formulating problems for algorithms,
and indeed choosing the correct algorithm variant for a problem, is key to
improving the practical use of both quantifier elimination and symbolic real
algebraic geometry in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3468</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3468</id><created>2014-05-14</created><authors><author><keyname>Cuomo</keyname><forenames>S.</forenames></author><author><keyname>Farina</keyname><forenames>R.</forenames></author><author><keyname>Galletti</keyname><forenames>A.</forenames></author><author><keyname>Marcellino</keyname><forenames>L.</forenames></author></authors><title>An error estimate of Gaussian Recursive Filter in 3Dvar problem</title><categories>cs.NA math.NA</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational kernel of the three-dimensional variational data assimilation
(3D-Var) problem is a linear system, generally solved by means of an iterative
method. The most costly part of each iterative step is a matrix-vector product
with a very large covariance matrix having Gaussian correlation structure. This
operation may be interpreted as a Gaussian convolution, that is a very
expensive numerical kernel. Recursive Filters (RFs) are a well known way to
approximate the Gaussian convolution and are intensively applied in the
meteorology, in the oceanography and in forecast models. In this paper, we deal
with an oceanographic 3D-Var data assimilation scheme, named OceanVar, where
the linear system is solved by using the Conjugate Gradient (GC) method by
replacing, at each step, the Gaussian convolution with RFs. Here we give
theoretical issues on the discrete convolution approximation with a first order
(1st-RF) and a third order (3rd-RF) recursive filters. Numerical experiments
confirm given error bounds and show the benefits, in terms of accuracy and
performance, of the 3-rd RF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3475</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3475</id><created>2014-05-14</created><updated>2014-10-20</updated><authors><author><keyname>Munemasa</keyname><forenames>Akihiro</forenames></author><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author><author><keyname>Taniguchi</keyname><forenames>Tetsuji</forenames></author></authors><title>On the smallest eigenvalues of the line graphs of some trees</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><msc-class>05C05, 05C50, 05C76</msc-class><journal-ref>Linear Algebra and its Applications 466 (2015) 501-511</journal-ref><doi>10.1016/j.laa.2014.10.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the characteristic polynomials of the line graphs of
generalized Bethe trees. We give an infinite family of such graphs sharing the
same smallest eigenvalue. Our family generalizes the family of coronas of
complete graphs discovered by Cvetkovi\'c and Stevanovi\'c.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3477</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3477</id><created>2014-05-14</created><updated>2015-01-07</updated><authors><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Cache-enabled Small Cell Networks: Modeling and Tradeoffs</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted to EURASIP Journal on Wireless Communications and
  Networking, Special Issue on Technical Advances in the Design and Deployment
  of Future Heterogeneous Networks)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network model where small base stations (SBSs) have caching
capabilities as a means to alleviate the backhaul load and satisfy users'
demand. The SBSs are stochastically distributed over the plane according to a
Poisson point process (PPP), and serve their users either (i) by bringing the
content from the Internet through a finite rate backhaul or (ii) by serving
them from the local caches. We derive closed-form expressions for the outage
probability and the average delivery rate as a function of the
signal-to-interference-plus-noise ratio (SINR), SBS density, target file
bitrate, storage size, file length and file popularity. We then analyze the
impact of key operating parameters on the system performance. It is shown that
a certain outage probability can be achieved either by increasing the number of
base stations or the total storage size. Our results and analysis provide key
insights into the deployment of cache-enabled small cell networks (SCNs), which
are seen as a promising solution for future heterogeneous cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3486</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3486</id><created>2014-05-14</created><authors><author><keyname>Zhang</keyname><forenames>Zhizheng</forenames></author><author><keyname>Zhao</keyname><forenames>Kaikai</forenames></author></authors><title>ESmodels: An Epistemic Specification Solver</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (To appear in Theory and Practice of Logic Programming (TPLP))
  ESmodels is designed and implemented as an experiment platform to investigate
the semantics, language, related reasoning algorithms, and possible
applications of epistemic specifications.We first give the epistemic
specification language of ESmodels and its semantics. The language employs only
one modal operator K but we prove that it is able to represent luxuriant modal
operators by presenting transformation rules. Then, we describe basic
algorithms and optimization approaches used in ESmodels. After that, we discuss
possible applications of ESmodels in conformant planning and constraint
satisfaction. Finally, we conclude with perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3487</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3487</id><created>2014-05-14</created><authors><author><keyname>Baudi&#x161;</keyname><forenames>Petr</forenames></author></authors><title>COCOpf: An Algorithm Portfolio Framework</title><categories>cs.AI</categories><comments>POSTER2014. arXiv admin note: text overlap with arXiv:1206.5780 by
  other authors without attribution</comments><acm-class>G.1.6</acm-class><journal-ref>Poster 2014 --- the 18th International Student Conference on
  Electrical Engineering. Czech Technical University, Prague, Czech Republic
  (2014)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Algorithm portfolios represent a strategy of composing multiple heuristic
algorithms, each suited to a different class of problems, within a single
general solver that will choose the best suited algorithm for each input. This
approach recently gained popularity especially for solving combinatoric
problems, but optimization applications are still emerging. The COCO platform
of the BBOB workshop series is the current standard way to measure performance
of continuous black-box optimization algorithms.
  As an extension to the COCO platform, we present the Python-based COCOpf
framework that allows composing portfolios of optimization algorithms and
running experiments with different selection strategies. In our framework, we
focus on black-box algorithm portfolio and online adaptive selection. As a
demonstration, we measure the performance of stock SciPy optimization
algorithms and the popular CMA algorithm alone and in a portfolio with two
simple selection strategies. We confirm that even a naive selection strategy
can provide improved performance across problem classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3491</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3491</id><created>2014-05-14</created><authors><author><keyname>Gajduk</keyname><forenames>Andrej</forenames></author><author><keyname>Utkovski</keyname><forenames>Zoran</forenames></author><author><keyname>Basnarkov</keyname><forenames>Lasko</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Energy-efficiency in Decentralized Wireless Networks: A Game-theoretic
  Approach inspired by Evolutionary Biology</title><categories>cs.NI cs.GT</categories><comments>The paper is accepted for publication at the International Workshop
  on Physics-inspired Paradigms in Wireless Communications and Networks -
  PHYSCOMNET 2014, in conjunction with the 12th Intl. Symposium on Modelling
  and Optimization in Mobile, Ad Hoc, and Wireless Networks - WIOPT, May 12-16,
  2014, Hammamet, Tunisia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency is gaining importance in wireless communication networks
which have nodes with limited energy supply and signal processing capabilities.
We present a numerical study of cooperative communication scenarios based on
simple local rules. This is in contrast to most of the approaches in the
literature which enforce cooperation by using complex algorithms and require
strategic complexity of the network nodes. The approach is motivated by recent
results in evolutionary biology which suggest that, if certain mechanism is at
work, cooperation can be favoured by natural selection, i. e. even selfish
actions of the individual nodes can lead to emergence of cooperative behaviour
in the network. The results of the simulations in the context of wireless
communication networks verify these observations and indicate that
uncomplicated local rules, followed by simple fitness evaluation, can generate
network behaviour which yields global energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3496</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3496</id><created>2014-05-14</created><authors><author><keyname>Baudi&#x161;</keyname><forenames>Petr</forenames></author></authors><title>Current Concepts in Version Control Systems</title><categories>cs.SE</categories><comments>Written in 2009</comments><acm-class>D.2.7</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We give the reader a comprehensive overview of the state of the Version
Control software engineering field, describing and analysing the concepts,
architectural approaches and methods researched and included in the currently
widely used version control systems and propose some possible future research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3505</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3505</id><created>2013-11-17</created><authors><author><keyname>Wenmackers</keyname><forenames>Sylvia</forenames></author><author><keyname>Vanpoucke</keyname><forenames>Danny E. P.</forenames></author><author><keyname>Douven</keyname><forenames>Igor</forenames></author></authors><title>Probability of Inconsistencies in Theory Revision: A multi-agent model
  for updating logically interconnected beliefs under bounded confidence</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 5 figures, 6 Tables</comments><journal-ref>Eur. Phys. J. B 85:44(2012)</journal-ref><doi>10.1140/e2011-20617-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model for studying communities of epistemically interacting
agents who update their belief states by averaging (in a specified way) the
belief states of other agents in the community. The agents in our model have a
rich belief state, involving multiple independent issues which are interrelated
in such a way that they form a theory of the world. Our main goal is to
calculate the probability for an agent to end up in an inconsistent belief
state due to updating (in the given way). To that end, an analytical expression
is given and evaluated numerically, both exactly and using statistical
sampling. It is shown that, under the assumptions of our model, an agent always
has a probability of less than 2% of ending up in an inconsistent belief state.
Moreover, this probability can be made arbitrarily small by increasing the
number of independent issues the agents have to judge or by increasing the
group size. A real-world situation to which this model applies is a group of
experts participating in a Delphi-study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3507</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3507</id><created>2014-05-14</created><authors><author><keyname>Ghanem</keyname><forenames>Samah A. M.</forenames></author><author><keyname>Ara</keyname><forenames>Munnujahan</forenames></author></authors><title>Secure Data Transmission in Cooperative Modes: Relay and MAC</title><categories>cs.IT math.IT</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation in clouds provides a promising technique for 5G wireless
networks, supporting higher data rates. Security of data transmission over
wireless clouds could put constraints on devices; whether to cooperate or not.
Therefore, our aim is to provide analytical framework for the security on the
physical layer of such setup and to define the constraints embodied with
cooperation in small size wireless clouds. In this paper, two legitimate
transmitters Alice and John cooperate to increase the reliable transmission
rate received by their common legitimate receiver Bob, where one eavesdropper,
Eve exists. We provide the achievable secure data transmission rates with
cooperative relaying and when no cooperation exists creating a Multiple Access
Channel (MAC). The paper considers the analysis of different cooperative
scenarios: a cooperative scenario with two relaying devices, a cooperative
scenario without relaying, a non-cooperative scenario, and cooperation from one
side. We derive analytical expressions for the optimal power allocation that
maximizes the achievable secrecy rates for the different set of scenarios where
the implication of cooperation on the achievable secrecy rates was analyzed. We
propose a distributed algorithm that allows the devices to select whether to
cooperate or not and to choose their optimal power allocation based on the
cooperation framework selected. Moreover, we defined distance constraints to
enforce the benefits of cooperation between devices in a wireless cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3515</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3515</id><created>2014-05-14</created><authors><author><keyname>Kim</keyname><forenames>Yoon</forenames></author><author><keyname>Chiu</keyname><forenames>Yi-I</forenames></author><author><keyname>Hanaki</keyname><forenames>Kentaro</forenames></author><author><keyname>Hegde</keyname><forenames>Darshan</forenames></author><author><keyname>Petrov</keyname><forenames>Slav</forenames></author></authors><title>Temporal Analysis of Language through Neural Language Models</title><categories>cs.CL</categories><journal-ref>Proceedings of the ACL 2014 Workshop on Language Technologies and
  Computational Social Science. June, 2014. 61--65</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a method for automatically detecting change in language across
time through a chronologically trained neural language model. We train the
model on the Google Books Ngram corpus to obtain word vector representations
specific to each year, and identify words that have changed significantly from
1900 to 2009. The model identifies words such as &quot;cell&quot; and &quot;gay&quot; as having
changed during that time period. The model simultaneously identifies the
specific years during which such words underwent change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3518</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3518</id><created>2014-05-14</created><updated>2014-06-28</updated><authors><author><keyname>Kim</keyname><forenames>Yoon</forenames></author><author><keyname>Zhang</keyname><forenames>Owen</forenames></author></authors><title>Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme
  for Sentiment Analysis and Text Classification</title><categories>cs.CL cs.IR</categories><journal-ref>Proceedings of the 5th Workshop on Computational Approaches to
  Subjectivity, Sentiment and Social Media Analysis. June, 2014. 79--83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple but novel supervised weighting scheme for adjusting term
frequency in tf-idf for sentiment analysis and text classification. We compare
our method to baseline weighting schemes and find that it outperforms them on
multiple benchmarks. The method is robust and works well on both snippets and
longer documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3531</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3531</id><created>2014-05-14</created><updated>2014-11-05</updated><authors><author><keyname>Chatfield</keyname><forenames>Ken</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Return of the Devil in the Details: Delving Deep into Convolutional Nets</title><categories>cs.CV</categories><comments>Published in proceedings of BMVC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The latest generation of Convolutional Neural Networks (CNN) have achieved
impressive results in challenging benchmarks on image recognition and object
detection, significantly raising the interest of the community in these
methods. Nevertheless, it is still unclear how different CNN methods compare
with each other and with previous state-of-the-art shallow representations such
as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts
a rigorous evaluation of these new techniques, exploring different deep
architectures and comparing them on a common ground, identifying and disclosing
important implementation details. We identify several useful properties of
CNN-based representations, including the fact that the dimensionality of the
CNN output layer can be reduced significantly without having an adverse effect
on performance. We also identify aspects of deep and shallow methods that can
be successfully shared. In particular, we show that the data augmentation
techniques commonly applied to CNN-based methods can also be applied to shallow
methods, and result in an analogous performance boost. Source code and models
to reproduce the experiments in the paper is made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3532</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3532</id><created>2014-05-14</created><updated>2015-02-19</updated><authors><author><keyname>Parreau</keyname><forenames>Aline</forenames></author><author><keyname>Rigo</keyname><forenames>Michel</forenames></author><author><keyname>Rowland</keyname><forenames>Eric</forenames></author><author><keyname>Vandomme</keyname><forenames>Elise</forenames></author></authors><title>A new approach to the $2$-regularity of the $\ell$-abelian complexity of
  $2$-automatic sequences</title><categories>cs.FL math.CO</categories><comments>44 pages, 2 figures; publication version</comments><msc-class>11B85, 68R15, 68Q70</msc-class><journal-ref>The Electronic Journal of Combinatorics 22 (2015) #P1.27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a sequence satisfying a certain symmetry property is
$2$-regular in the sense of Allouche and Shallit, i.e., the $\mathbb{Z}$-module
generated by its $2$-kernel is finitely generated. We apply this theorem to
develop a general approach for studying the $\ell$-abelian complexity of
$2$-automatic sequences. In particular, we prove that the period-doubling word
and the Thue--Morse word have $2$-abelian complexity sequences that are
$2$-regular. Along the way, we also prove that the $2$-block codings of these
two words have $1$-abelian complexity sequences that are $2$-regular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3534</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3534</id><created>2014-05-14</created><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Fan</keyname><forenames>Fengtao</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Dimension Detection with Local Homology</title><categories>cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the dimension of a hidden manifold from a point sample has become
an important problem in the current data-driven era. Indeed, estimating the
shape dimension is often the first step in studying the processes or phenomena
associated to the data. Among the many dimension detection algorithms proposed
in various fields, a few can provide theoretical guarantee on the correctness
of the estimated dimension. However, the correctness usually requires certain
regularity of the input: the input points are either uniformly randomly sampled
in a statistical setting, or they form the so-called
$(\varepsilon,\delta)$-sample which can be neither too dense nor too sparse.
  Here, we propose a purely topological technique to detect dimensions. Our
algorithm is provably correct and works under a more relaxed sampling
condition: we do not require uniformity, and we also allow Hausdorff noise. Our
approach detects dimension by determining local homology. The computation of
this topological structure is much less sensitive to the local distribution of
points, which leads to the relaxation of the sampling conditions. Furthermore,
by leveraging various developments in computational topology, we show that this
local homology at a point $z$ can be computed \emph{exactly} for manifolds
using Vietoris-Rips complexes whose vertices are confined within a local
neighborhood of $z$. We implement our algorithm and demonstrate the accuracy
and robustness of our method using both synthetic and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3536</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3536</id><created>2014-05-14</created><authors><author><keyname>Nicol</keyname><forenames>Olivier</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Mary</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Preux</keyname><forenames>Philippe</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author></authors><title>Improving offline evaluation of contextual bandit algorithms via
  bootstrapping techniques</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><journal-ref>International Conference on Machine Learning 32 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many recommendation applications such as news recommendation, the items
that can be rec- ommended come and go at a very fast pace. This is a challenge
for recommender systems (RS) to face this setting. Online learning algorithms
seem to be the most straight forward solution. The contextual bandit framework
was introduced for that very purpose. In general the evaluation of a RS is a
critical issue. Live evaluation is of- ten avoided due to the potential loss of
revenue, hence the need for offline evaluation methods. Two options are
available. Model based meth- ods are biased by nature and are thus difficult to
trust when used alone. Data driven methods are therefore what we consider here.
Evaluat- ing online learning algorithms with past data is not simple but some
methods exist in the litera- ture. Nonetheless their accuracy is not satisfac-
tory mainly due to their mechanism of data re- jection that only allow the
exploitation of a small fraction of the data. We precisely address this issue
in this paper. After highlighting the limita- tions of the previous methods, we
present a new method, based on bootstrapping techniques. This new method comes
with two important improve- ments: it is much more accurate and it provides a
measure of quality of its estimation. The latter is a highly desirable property
in order to minimize the risks entailed by putting online a RS for the first
time. We provide both theoretical and ex- perimental proofs of its superiority
compared to state-of-the-art methods, as well as an analysis of the convergence
of the measure of quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3539</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3539</id><created>2014-05-14</created><updated>2015-05-04</updated><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author><author><keyname>Ganz</keyname><forenames>Adam</forenames></author></authors><title>Pattern Recognition in Narrative: Tracking Emotional Expression in
  Context</title><categories>cs.AI cs.CL</categories><comments>21 pages, 7 figures</comments><msc-class>62H25, 62H30, 62.07</msc-class><acm-class>H.2.8; H.3; I.5; I.7.0; J.5</acm-class><journal-ref>Journal of Data Mining &amp; Digital Humanities, 2015 (May 26, 2015)
  jdmdh:647</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using geometric data analysis, our objective is the analysis of narrative,
with narrative of emotion being the focus in this work. The following two
principles for analysis of emotion inform our work. Firstly, emotion is
revealed not as a quality in its own right but rather through interaction. We
study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the
3-way relationship of Emma, Charles and Rodolphe in the novel {\em Madame
Bovary}. Secondly, emotion, that is expression of states of mind of subjects,
is formed and evolves within the narrative that expresses external events and
(personal, social, physical) context. In addition to the analysis methodology
with key aspects that are innovative, the input data used is crucial. We use,
firstly, dialogue, and secondly, broad and general description that
incorporates dialogue. In a follow-on study, we apply our unsupervised
narrative mapping to data streams with very low emotional expression. We map
the narrative of Twitter streams. Thus we demonstrate map analysis of general
narratives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3546</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3546</id><created>2014-05-14</created><updated>2014-09-15</updated><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Dodaro</keyname><forenames>Carmine</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>Anytime Computation of Cautious Consequences in Answer Set Programming</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming</comments><msc-class>68T27</msc-class><acm-class>D.1.6; I.2.4</acm-class><doi>10.1017/S1471068414000325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query answering in Answer Set Programming (ASP) is usually solved by
computing (a subset of) the cautious consequences of a logic program. This task
is computationally very hard, and there are programs for which computing
cautious consequences is not viable in reasonable time. However, current ASP
solvers produce the (whole) set of cautious consequences only at the end of
their computation. This paper reports on strategies for computing cautious
consequences, also introducing anytime algorithms able to produce sound answers
during the computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3547</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3547</id><created>2014-05-14</created><authors><author><keyname>Swift</keyname><forenames>Terrance</forenames></author></authors><title>Incremental Tabling in Support of Knowledge Representation and Reasoning</title><categories>cs.PL</categories><comments>Theory and Practice of Logic Programming Volume 14 2014</comments><doi>10.1017/S1471068414000209</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resolution-based Knowledge Representation and Reasoning (KRR) systems, such
as Flora-2, Silk or Ergo, can scale to tens or hundreds of millions of facts,
while supporting reasoning that includes Hilog, inheritance, defeasibility
theories, and equality theories. These systems handle the termination and
complexity issues that arise from the use of these features by a heavy use of
tabled resolution. In fact, such systems table by default all rules defined by
users, unless they are simple facts.
  Performing dynamic updates within such systems is nearly impossible unless
the tables themselves can be made to react to changes. Incremental tabling as
first implemented in XSB (Saha 2006) partially addressed this problem, but the
implementation was limited in scope and not always easy to use. In this paper,
we introduce transparent incremental tabling which at the semantic level
supports updates in the 3-valued well-founded semantics, while guaranteeing
full consistency of all tabled queries. Transparent incremental tabling also
has significant performance improvements over previous implementations,
including lazy recomputation, and control over the dependency structures used
to determine how tables are updated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3548</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3548</id><created>2014-05-14</created><authors><author><keyname>Sciancalepore</keyname><forenames>Vincenzo</forenames></author><author><keyname>Giustiniano</keyname><forenames>Domenico</forenames></author><author><keyname>Banchs</keyname><forenames>Albert</forenames></author><author><keyname>Picu</keyname><forenames>Andreea</forenames></author></authors><title>Offloading Cellular Traffic through Opportunistic Communications:
  Analysis and Optimization</title><categories>cs.NI</categories><comments>15 pages, Submitted JSAC on D2D communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offloading traffic through opportunistic communications has been recently
proposed as a way to relieve the current overload of cellular networks.
Opportunistic communication can occur when mobile device users are
(temporarily) in each other's proximity, such that the devices can establish a
local peer-to-peer connection (e.g., via Bluetooth). Since opportunistic
communication is based on the spontaneous mobility of the participants, it is
inherently unreliable. This poses a serious challenge to the design of any
cellular offloading solutions, that must meet the applications' requirements.
In this paper, we address this challenge from an optimization analysis
perspective, in contrast to the existing heuristic solutions. We first model
the dissemination of content (injected through the cellular interface) in an
opportunistic network with heterogeneous node mobility. Then, based on this
model, we derive the optimal content injection strategy, which minimizes the
load of the cellular network while meeting the applications' constraints.
Finally, we propose an adaptive algorithm based on control theory that
implements this optimal strategy without requiring any data on the mobility
patterns or the mobile nodes' contact rates. The proposed approach is
extensively evaluated with both a heterogeneous mobility model as well as
real-world contact traces, showing that it substantially outperforms previous
approaches proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3556</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3556</id><created>2014-05-14</created><authors><author><keyname>Cruz</keyname><forenames>Flavio</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author><author><keyname>Goldstein</keyname><forenames>Seth Copen</forenames></author><author><keyname>Pfenning</keyname><forenames>Frank</forenames></author></authors><title>A Linear Logic Programming Language for Concurrent Programming over
  Graph Structures</title><categories>cs.PL</categories><comments>ICLP 2014, TPLP 2014</comments><doi>10.1017/S1471068414000167</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have designed a new logic programming language called LM (Linear Meld) for
programming graph-based algorithms in a declarative fashion. Our language is
based on linear logic, an expressive logical system where logical facts can be
consumed. Because LM integrates both classical and linear logic, LM tends to be
more expressive than other logic programming languages. LM programs are
naturally concurrent because facts are partitioned by nodes of a graph data
structure. Computation is performed at the node level while communication
happens between connected nodes. In this paper, we present the syntax and
operational semantics of our language and illustrate its use through a number
of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3557</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3557</id><created>2014-05-14</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author><author><keyname>Amar</keyname><forenames>Gilad</forenames></author><author><keyname>Shaltiel</keyname><forenames>Ran</forenames></author></authors><title>The Interestingness Tool for Search in the Web</title><categories>cs.IR</categories><comments>13 pages, 6 figures, 4 tables, 3rd SKY'2012 International Workshop on
  Software Knowledge, Barcelona, Spain, October 2012, SciTePress, Portugal</comments><acm-class>H.3.3</acm-class><doi>10.5220/0004178900540063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interestingness,as the composition of Relevance and Unexpectedness, has been
tested by means of Web search cases studies and led to promising results. But
for thorough investigation and routine practical application one needs a
flexible and robust tool. This work describes such an Interestingness based
search tool, its software architecture and actual implementation. One of its
flexibility traits is the choice of Interestingness functions: it may work with
Match-Mismatch and Tf-Idf, among other functions. The tool has been
experimentally verified by application to various domains of interest. It has
been validated by comparison of results with those of commercial search engines
and results from differing Interestingness functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3558</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3558</id><created>2014-05-14</created><authors><author><keyname>Gogioso</keyname><forenames>Stefano</forenames></author></authors><title>Aspects of Statistical Physics in Computational Complexity</title><categories>cs.CC cond-mat.dis-nn cond-mat.stat-mech</categories><comments>56 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this review paper is to give a panoramic of the impact of spin
glass theory and statistical physics in the study of the K-sat problem. The
introduction of spin glass theory in the study of the random K-sat problem has
indeed left a mark on the field, leading to some groundbreaking descriptions of
the geometry of its solution space, and helping to shed light on why it seems
to be so hard to solve.
  Most of the geometrical intuitions have their roots in the
Sherrington-Kirkpatrick model of spin glass. We'll start Chapter 2 by
introducing the model from a mathematical perspective, presenting a selection
of rigorous results and giving a first intuition about the cavity method. We'll
then switch to a physical perspective, to explore concepts like pure states,
hierarchical clustering and replica symmetry breaking.
  Chapter 3 will be devoted to the spin glass formulation of K-sat, while the
most important phase transitions of K-sat (clustering, condensation, freezing
and SAT/UNSAT) will be extensively discussed in Chapter 4, with respect their
complexity, free-entropy density and the Parisi 1RSB parameter.
  The concept of algorithmic barrier will be presented in Chapter 5 and
exemplified in detail on the Belief Propagation (BP) algorithm. The BP
algorithm will be introduced and motivated, and numerical analysis of a
BP-guided decimation algorithm will be used to show the role of the clustering,
condensation and freezing phase transitions in creating an algorithmic barrier
for BP.
  Taking from the failure of BP in the clustered and condensed phases, Chapter
6 will finally introduce the Cavity Method to deal with the shattering of the
solution space, and present its application to the development of the Survey
Propagation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3559</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3559</id><created>2014-05-14</created><authors><author><keyname>Corani</keyname><forenames>Giorgio</forenames></author><author><keyname>Mignatti</keyname><forenames>Andrea</forenames></author></authors><title>Credal Model Averaging for classification: representing prior ignorance
  and expert opinions</title><categories>stat.ME cs.AI q-bio.PE stat.ML</categories><comments>15 pages 6 figures Preprint submitted to the International Journal of
  Approximate Reasoning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian model averaging (BMA) is the state of the art approach for
overcoming model uncertainty. Yet, especially on small data sets, the results
yielded by BMA might be sensitive to the prior over the models. Credal Model
Averaging (CMA) addresses this problem by substituting the single prior over
the models by a set of priors (credal set). Such approach solves the problem of
how to choose the prior over the models and automates sensitivity analysis. We
discuss various CMA algorithms for building an ensemble of logistic regressors
characterized by different sets of covariates. We show how CMA can be
appropriately tuned to the case in which one is prior-ignorant and to the case
in which instead domain knowledge is available. CMA detects prior-dependent
instances, namely instances in which a different class is more probable
depending on the prior over the models. On such instances CMA suspends the
judgment, returning multiple classes. We thoroughly compare different BMA and
CMA variants on a real case study, predicting presence of Alpine marmot burrows
in an Alpine valley. We find that BMA is almost a random guesser on the
instances recognized as prior-dependent by CMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3570</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3570</id><created>2014-05-14</created><authors><author><keyname>Gall</keyname><forenames>Daniel</forenames></author><author><keyname>Fr&#xfc;hwirth</keyname><forenames>Thom</forenames></author></authors><title>Exchanging Conflict Resolution in an Adaptable Implementation of ACT-R</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP).
  Accepted paper for ICLP 2014. 12 pages + appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computational cognitive science, the cognitive architecture ACT-R is very
popular. It describes a model of cognition that is amenable to computer
implementation, paving the way for computational psychology. Its underlying
psychological theory has been investigated in many psychological experiments,
but ACT-R lacks a formal definition of its underlying concepts from a
mathematical-computational point of view. Although the canonical implementation
of ACT-R is now modularized, this production rule system is still hard to adapt
and extend in central components like the conflict resolution mechanism (which
decides which of the applicable rules to apply next).
  In this work, we present a concise implementation of ACT-R based on
Constraint Handling Rules which has been derived from a formalization in prior
work. To show the adaptability of our approach, we implement several different
conflict resolution mechanisms discussed in the ACT-R literature. This results
in the first implementation of one such mechanism. For the other mechanisms, we
empirically evaluate if our implementation matches the results of reference
implementations of ACT-R.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3574</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3574</id><created>2014-05-13</created><authors><author><keyname>Sumpf</keyname><forenames>Tilman J.</forenames><affiliation>Biomedizinische NMR Forschungs GmbH am Max-Planck-Institut f&#xfc;r biophysikalische Chemie, G&#xf6;ttingen</affiliation></author><author><keyname>Petrovic</keyname><forenames>Andreas</forenames><affiliation>Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria, and Institute for Medical Engineering, Graz University of Technology, Graz, Austria</affiliation></author><author><keyname>Uecker</keyname><forenames>Martin</forenames><affiliation>Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, California</affiliation></author><author><keyname>Knoll</keyname><forenames>Florian</forenames><affiliation>Center for Biomedical Imaging, New York University School of Medicine, New York</affiliation></author><author><keyname>Frahm</keyname><forenames>Jens</forenames><affiliation>Biomedizinische NMR Forschungs GmbH am Max-Planck-Institut f&#xfc;r biophysikalische Chemie, G&#xf6;ttingen</affiliation></author></authors><title>Fast T2 Mapping with Improved Accuracy Using Undersampled Spin-echo MRI
  and Model-based Reconstructions with a Generating Function</title><categories>physics.med-ph cs.CE q-bio.QM</categories><comments>10 pages, 7 figures</comments><journal-ref>Medical Imaging, IEEE Transactions on 33 (2014) 2213-2222</journal-ref><doi>10.1109/TMI.2014.2333370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model-based reconstruction technique for accelerated T2 mapping with
improved accuracy is proposed using undersampled Cartesian spin-echo MRI data.
The technique employs an advanced signal model for T2 relaxation that accounts
for contributions from indirect echoes in a train of multiple spin echoes. An
iterative solution of the nonlinear inverse reconstruction problem directly
estimates spin-density and T2 maps from undersampled raw data. The algorithm is
validated for simulated data as well as phantom and human brain MRI at 3 T. The
performance of the advanced model is compared to conventional pixel-based
fitting of echo-time images from fully sampled data. The proposed method yields
more accurate T2 values than the mono-exponential model and allows for
undersampling factors of at least 6. Although limitations are observed for very
long T2 relaxation times, respective reconstruction problems may be overcome by
a gradient dampening approach. The analytical gradient of the utilized cost
function is included as Appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3576</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3576</id><created>2014-05-14</created><authors><author><keyname>Maslennikova</keyname><forenames>Marina</forenames></author></authors><title>Complexity of checking whether two automata are synchronized by the same
  language</title><categories>cs.FL</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deterministic finite automaton is said to be synchronizing if it has a
reset word, i.e. a word that brings all states of the automaton to a particular
one. We prove that it is a PSPACE-complete problem to check whether the
language of reset words for a given automaton coincides with the language of
reset words for some particular automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3599</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3599</id><created>2014-05-14</created><authors><author><keyname>Usatyuk</keyname><forenames>Vasily</forenames></author></authors><title>Application of lattice reduction block Korkin-Zolotarev method to
  MIMO-decoding</title><categories>cs.DM</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article present a application of Block Korkin---Zolotarev lattice
reduction method for Lattice Reduction---Aided decoding under MIMO---channel.
We give a upper bound estimate on the lattice reduced by block
Korkin---Zolotarev method (BKZ) for different value of the block size and
detecting by SIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3603</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3603</id><created>2014-05-14</created><authors><author><keyname>Marple</keyname><forenames>Kyle</forenames></author><author><keyname>Gupta</keyname><forenames>Gopal</forenames></author></authors><title>Dynamic Consistency Checking in Goal-Directed Answer Set Programming</title><categories>cs.LO</categories><comments>12 pages. Accepted to ICLP 2014. To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><acm-class>D.1.6</acm-class><doi>10.1017/S1471068414000118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In answer set programming, inconsistencies arise when the constraints placed
on a program become unsatisfiable. In this paper, we introduce a technique for
dynamic consistency checking for our goal-directed method for computing answer
sets, under which only those constraints deemed relevant to the partial answer
set are tested, allowing inconsistent knowledgebases to be successfully
queried. However, the algorithm guarantees that, if a program has at least one
consistent answer set, any partial answer set returned will be a subset of some
consistent answer set. To appear in Theory and Practice of Logic Programming
(TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3608</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3608</id><created>2014-05-14</created><authors><author><keyname>Anti&#x107;</keyname><forenames>Christian</forenames></author></authors><title>On cascade products of answer set programs</title><categories>cs.LO</categories><comments>Appears in Theory and Practice of Logic Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Describing complex objects by elementary ones is a common strategy in
mathematics and science in general. In their seminal 1965 paper, Kenneth Krohn
and John Rhodes showed that every finite deterministic automaton can be
represented (or &quot;emulated&quot;) by a cascade product of very simple automata. This
led to an elegant algebraic theory of automata based on finite semigroups
(Krohn-Rhodes Theory). Surprisingly, by relating logic programs and automata,
we can show in this paper that the Krohn-Rhodes Theory is applicable in Answer
Set Programming (ASP). More precisely, we recast the concept of a cascade
product to ASP, and prove that every program can be represented by a product of
very simple programs, the reset and standard programs. Roughly, this implies
that the reset and standard programs are the basic building blocks of ASP with
respect to the cascade product. In a broader sense, this paper is a first step
towards an algebraic theory of products and networks of nonmonotonic reasoning
systems based on Krohn-Rhodes Theory, aiming at important open issues in ASP
and AI in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3612</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3612</id><created>2014-05-14</created><updated>2014-07-15</updated><authors><author><keyname>Generous</keyname><forenames>Nicholas</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM</affiliation></author><author><keyname>Fairchild</keyname><forenames>Geoffrey</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM</affiliation></author><author><keyname>Deshpande</keyname><forenames>Alina</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM</affiliation></author><author><keyname>Del Valle</keyname><forenames>Sara Y.</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM</affiliation></author><author><keyname>Priedhorsky</keyname><forenames>Reid</forenames><affiliation>Los Alamos National Laboratory, Los Alamos, NM</affiliation></author></authors><title>Global disease monitoring and forecasting with Wikipedia</title><categories>cs.SI cs.LG physics.soc-ph</categories><comments>27 pages; 4 figures; 4 tables. Version 2: Cite McIver &amp; Brownstein
  and adjust novelty claims accordingly; revise title; various revisions for
  clarity</comments><report-no>LA-UR 14-22535</report-no><acm-class>H.5.3; H.3.5; J.3</acm-class><journal-ref>PLOS Comput. Biol., vol. 10, no. 11, p. e1003892, Nov. 2014</journal-ref><doi>10.1371/journal.pcbi.1003892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infectious disease is a leading threat to public health, economic stability,
and other key social structures. Efforts to mitigate these impacts depend on
accurate and timely monitoring to measure the risk and progress of disease.
Traditional, biologically-focused monitoring techniques are accurate but costly
and slow; in response, new techniques based on social internet data such as
social media and search queries are emerging. These efforts are promising, but
important challenges in the areas of scientific peer review, breadth of
diseases and countries, and forecasting hamper their operational usefulness.
  We examine a freely available, open data source for this use: access logs
from the online encyclopedia Wikipedia. Using linear models, language as a
proxy for location, and a systematic yet simple article selection procedure, we
tested 14 location-disease combinations and demonstrate that these data
feasibly support an approach that overcomes these challenges. Specifically, our
proof-of-concept yields models with $r^2$ up to 0.92, forecasting value up to
the 28 days tested, and several pairs of models similar enough to suggest that
transferring models from one location to another without re-training is
feasible.
  Based on these preliminary results, we close with a research agenda designed
to overcome these challenges and produce a disease monitoring and forecasting
system that is significantly more effective, robust, and globally comprehensive
than the current state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3622</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3622</id><created>2014-05-14</created><authors><author><keyname>Le</keyname><forenames>Anh</forenames></author><author><keyname>Keller</keyname><forenames>Lorenzo</forenames></author><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Cici</keyname><forenames>Blerim</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>MicroCast: Cooperative Video Streaming using Cellular and D2D
  Connections</title><categories>cs.NI cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a group of mobile users, within proximity of each other, who are
interested in watching the same online video at roughly the same time. The
common practice today is that each user downloads the video independently on
her mobile device using her own cellular connection, which wastes access
bandwidth and may also lead to poor video quality. We propose a novel
cooperative system where each mobile device uses simultaneously two network
interfaces: (i) the cellular to connect to the video server and download parts
of the video and (ii) WiFi to connect locally to all other devices in the group
and exchange those parts. Devices cooperate to efficiently utilize all network
resources and are able to adapt to varying wireless network conditions. In the
local WiFi network, we exploit overhearing, and we further combine it with
network coding. The end result is savings in cellular bandwidth and improved
user experience (faster download) by a factor on the order up to the group
size.
  We follow a complete approach, from theory to practice. First, we formulate
the problem using a network utility maximization (NUM) framework, decompose the
problem, and provide a distributed solution. Then, based on the structure of
the NUM solution, we design a modular system called MicroCast and we implement
it as an Android application. We provide both simulation results of the NUM
solution and experimental evaluation of MicroCast on a testbed consisting of
Android phones. We demonstrate that the proposed approach brings significant
performance benefits without battery penalty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3623</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3623</id><created>2014-05-14</created><authors><author><keyname>Gransden</keyname><forenames>Thomas</forenames></author><author><keyname>Walkinshaw</keyname><forenames>Neil</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author></authors><title>Mining State-Based Models from Proof Corpora</title><categories>cs.LO</categories><comments>To Appear at Conferences on Intelligent Computer Mathematics 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive theorem provers have been used extensively to reason about
various software/hardware systems and mathematical theorems. The key challenge
when using an interactive prover is finding a suitable sequence of proof steps
that will lead to a successful proof requires a significant amount of human
intervention. This paper presents an automated technique that takes as input
examples of successful proofs and infers an Extended Finite State Machine as
output. This can in turn be used to generate proofs of new conjectures. Our
preliminary experiments show that the inferred models are generally accurate
(contain few false-positive sequences) and that representing existing proofs in
such a way can be very useful when guiding new ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3629</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3629</id><created>2014-05-14</created><updated>2015-08-12</updated><authors><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Dissipation of information in channels with input constraints</title><categories>cs.IT math.IT</categories><comments>revised; include appendix B on contraction coefficient for mutual
  information on general alphabets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the basic tenets in information theory, the data processing inequality
states that output divergence does not exceed the input divergence for any
channel. For channels without input constraints, various estimates on the
amount of such contraction are known, Dobrushin's coefficient for the total
variation being perhaps the most well-known. This work investigates channels
with average input cost constraint. It is found that while the contraction
coefficient typically equals one (no contraction), the information nevertheless
dissipates. A certain non-linear function, the \emph{Dobrushin curve} of the
channel, is proposed to quantify the amount of dissipation. Tools for
evaluating the Dobrushin curve of additive-noise channels are developed based
on coupling arguments. Some basic applications in stochastic control,
uniqueness of Gibbs measures and fundamental limits of noisy circuits are
discussed.
  As an application, it shown that in the chain of $n$ power-constrained relays
and Gaussian channels the end-to-end mutual information and maximal squared
correlation decay as $\Theta(\frac{\log\log n}{\log n})$, which is in stark
contrast with the exponential decay in chains of discrete channels. Similarly,
the behavior of noisy circuits (composed of gates with bounded fan-in) and
broadcasting of information on trees (of bounded degree) does not experience
threshold behavior in the signal-to-noise ratio (SNR). Namely, unlike the case
of discrete channels, the probability of bit error stays bounded away from
$1\over 2$ regardless of the SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3630</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3630</id><created>2013-12-20</created><authors><author><keyname>Choliy</keyname><forenames>Vasyl</forenames></author></authors><title>DDscat.C++ User and programmer guide</title><categories>physics.comp-ph cs.MS</categories><comments>26 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DDscat.C++ 7.3.0 is a freely available open-source C++ software package
applying the &quot;discrete dipole approximation&quot; (DDA) to calculate scattering and
absorption of electromagnetic waves by targets with arbitrary geometries and a
complex refractive index. DDscat.C++ is a clone of well known DDscat Fortran-90
software. We refer to DDscat as to the parent code in this document. Versions
7.3.0 of both codes have the identical functionality but the quite different
implementation. Started as a teaching project, the DDscat.C++ code differs from
the parent code DDscat in programming techniques and features, essential for
C++ but quite seldom in Fortran.
  As DDscat.C++ in its current version is just a clone, usage of DDscat.C++ for
electromagnetic calculations is the same as of DDscat. Please, refer to &quot;User
Guide for the Discrete Dipole Approximation Code DDSCAT 7.3&quot; to start using the
code(s).
  This document consists of two parts. In the first part we present Quick start
guide for users who want to begin to use the code. Only differencies between
DDscat.C++ and DDscat are explained. The second part of the document explains
programming tips for the persons who want to change the code, to add the
functionality or help the author with code refactoring and debugging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3631</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3631</id><created>2014-05-14</created><updated>2015-12-14</updated><authors><author><keyname>Ong</keyname><forenames>Kian Win</forenames></author><author><keyname>Papakonstantinou</keyname><forenames>Yannis</forenames></author><author><keyname>Vernoux</keyname><forenames>Romain</forenames></author></authors><title>The SQL++ Query Language: Configurable, Unifying and Semi-structured</title><categories>cs.DB</categories><comments>13 pages, [14166]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoSQL databases support semi-structured data, typically modeled as JSON. They
also provide limited (but expanding) query languages. Their idiomatic, non-SQL
language constructs, the many variations, and the lack of formal semantics
inhibit deep understanding of the query languages, and also impede progress
towards clean, powerful, declarative query languages.
  This paper specifies the syntax and semantics of SQL++, which is applicable
to both JSON native stores and SQL databases. The SQL++ semi-structured data
model is a superset of both JSON and the SQL data model. SQL++ offers powerful
computational capabilities for processing semi-structured data akin to prior
non-relational query languages, notably OQL and XQuery. Yet, SQL++ is SQL
backwards compatible and is generalized towards JSON by introducing only a
small number of query language extensions to SQL.
  Recognizing that a query language standard is probably premature for the fast
evolving area of NoSQL databases, SQL++ includes configuration options that
formally itemize the semantics variations that language designers may choose
from. The options often pertain to the treatment of semi-structuredness
(missing attributes, heterogeneous types, etc), where more than one sensible
approaches are possible.
  SQL++ is unifying: By appropriate choices of configuration options, the SQL++
semantics can morph into the semantics of existing semi-structured database
query languages. The extensive experimental validation shows how SQL and four
semi-structured database query languages (MongoDB, Cassandra CQL, Couchbase
N1QL and AsterixDB AQL) are formally described by appropriate settings of the
configuration options.
  Early adoption signs of SQL++ are positive: Version 4 of Couchbase's N1QL is
explained as syntactic sugar over SQL++. AsterixDB will soon support the full
SQL++ and Apache Drill is in the process of aligning with SQL++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3637</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3637</id><created>2014-05-14</created><updated>2014-05-14</updated><authors><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanlin</forenames></author></authors><title>Vicious Circle Principle and Logic Programs with Aggregates</title><categories>cs.AI</categories><doi>10.1017/S1471068414000222</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a knowledge representation language $\mathcal{A}log$ which
extends ASP with aggregates. The goal is to have a language based on simple
syntax and clear intuitive and mathematical semantics. We give some properties
of $\mathcal{A}log$, an algorithm for computing its answer sets, and comparison
with other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3675</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3675</id><created>2014-05-14</created><authors><author><keyname>Comini</keyname><forenames>Marco</forenames></author><author><keyname>Titolo</keyname><forenames>Laura</forenames></author><author><keyname>Villanueva</keyname><forenames>Alicia</forenames></author></authors><title>Abstract Diagnosis for tccp using a Linear Temporal Logic</title><categories>cs.PL</categories><doi>10.1017/S1471068414000349</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic techniques for program verification usually suffer the well-known
state explosion problem. Most of the classical approaches are based on browsing
the structure of some form of model (which represents the behavior of the
program) to check if a given specification is valid. This implies that a part
of the model has to be built, and sometimes the needed fragment is quite huge.
  In this work, we provide an alternative automatic decision method to check
whether a given property, specified in a linear temporal logic, is valid w.r.t.
a tccp program. Our proposal (based on abstract interpretation techniques) does
not require to build any model at all. Our results guarantee correctness but,
as usual when using an abstract semantics, completeness is lost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3681</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3681</id><created>2014-05-14</created><updated>2014-12-29</updated><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Terminality implies non-signalling</title><categories>quant-ph cs.LO math.CT</categories><comments>In Proceedings QPL 2014, arXiv:1412.8102</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 172, 2014, pp. 27-35</journal-ref><doi>10.4204/EPTCS.172.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 'process theory' is any theory of systems and processes which admits
sequential and parallel composition. `Terminality' unifies normalisation of
pure states, trace-preservation of CP-maps, and adding up to identity of
positive operators in quantum theory, and generalises this to arbitrary process
theories. We show that terminality and non-signalling coincide in any process
theory, provided one makes causal structure explicit. In fact, making causal
structure explicit is necessary to even make sense of non-signalling in process
theories. We conclude that because of its much simpler mathematical form,
terminality should be taken to be a more fundamental notion than
non-signalling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3694</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3694</id><created>2014-05-14</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Kaminski</keyname><forenames>Roland</forenames></author><author><keyname>Kaufmann</keyname><forenames>Benjamin</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Clingo = ASP + Control: Preliminary Report</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the new ASP system clingo 4. Unlike its predecessors, being mere
monolithic combinations of the grounder gringo with the solver clasp, the new
clingo 4 series offers high-level constructs for realizing complex reasoning
processes. Among others, such processes feature advanced forms of search, as in
optimization or theory solving, or even interact with an environment, as in
robotics or query-answering. Common to them is that the problem specification
evolves during the reasoning process, either because data or constraints are
added, deleted, or replaced. In fact, clingo 4 carries out such complex
reasoning within a single integrated ASP grounding and solving process. This
avoids redundancies in relaunching grounder and solver programs and benefits
from the solver's learning capacities. clingo 4 accomplishes this by
complementing ASP's declarative input language by control capacities expressed
via the embedded scripting languages lua and python. On the declarative side,
clingo 4 offers a new directive that allows for structuring logic programs into
named and parameterizable subprograms. The grounding and integration of these
subprograms into the solving process is completely modular and fully
controllable from the procedural side, viz. the scripting languages. By
strictly separating logic and control programs, clingo 4 also abolishes the
need for dedicated systems for incremental and reactive reasoning, like iclingo
and oclingo, respectively, and its flexibility goes well beyond the advanced
yet still rigid solving processes of the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3697</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3697</id><created>2014-05-14</created><authors><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Throughput-Smoothness Trade-offs in Multicasting of an Ordered Packet
  Stream</title><categories>cs.IT math.IT</categories><comments>Accepted to NetCod 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of streaming applications need packets to be strictly
in-order at the receiver. This paper provides a framework for analyzing
in-order packet delivery in such applications. We consider the problem of
multicasting an ordered stream of packets to two users over independent erasure
channels with instantaneous feedback to the source. Depending upon the channel
erasures, a packet which is in-order for one user, may be redundant for the
other. Thus there is an inter-dependence between throughput and the smoothness
of in-order packet delivery to the two users. We use a Markov chain model of
packet decoding to analyze these throughput-smoothness trade-offs of the users,
and propose coding schemes that can span different points on each trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3710</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3710</id><created>2014-05-14</created><updated>2014-06-03</updated><authors><author><keyname>Calimeri</keyname><forenames>Francesco</forenames></author><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Maratea</keyname><forenames>Marco</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>The Design of the Fifth Answer Set Programming Competition</title><categories>cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a well-established paradigm of declarative
programming that has been developed in the field of logic programming and
nonmonotonic reasoning. Advances in ASP solving technology are customarily
assessed in competition events, as it happens for other closely-related
problem-solving technologies like SAT/SMT, QBF, Planning and Scheduling. ASP
Competitions are (usually) biennial events; however, the Fifth ASP Competition
departs from tradition, in order to join the FLoC Olympic Games at the Vienna
Summer of Logic 2014, which is expected to be the largest event in the history
of logic. This edition of the ASP Competition series is jointly organized by
the University of Calabria (Italy), the Aalto University (Finland), and the
University of Genova (Italy), and is affiliated with the 30th International
Conference on Logic Programming (ICLP 2014). It features a completely
re-designed setup, with novelties involving the design of tracks, the scoring
schema, and the adherence to a fixed modeling language in order to push the
adoption of the ASP-Core-2 standard. Benchmark domains are taken from past
editions, and best system packages submitted in 2013 are compared with new
versions and solvers.
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3713</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3713</id><created>2014-05-14</created><updated>2014-05-15</updated><authors><author><keyname>Pereira</keyname><forenames>Lu&#xed;s Moniz</forenames></author><author><keyname>Dietz</keyname><forenames>Emmanuelle-Anna</forenames></author><author><keyname>H&#xf6;lldobler</keyname><forenames>Steffen</forenames></author></authors><title>Contextual Abductive Reasoning with Side-Effects</title><categories>cs.AI</categories><comments>14 pages, no figures, 1 table</comments><doi>10.1017/S1471068414000258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The belief bias effect is a phenomenon which occurs when we think that we
judge an argument based on our reasoning, but are actually influenced by our
beliefs and prior knowledge. Evans, Barston and Pollard carried out a
psychological syllogistic reasoning task to prove this effect. Participants
were asked whether they would accept or reject a given syllogism. We discuss
one specific case which is commonly assumed to be believable but which is
actually not logically valid. By introducing abnormalities, abduction and
background knowledge, we adequately model this case under the weak completion
semantics. Our formalization reveals new questions about possible extensions in
abductive reasoning. For instance, observations and their explanations might
include some relevant prior abductive contextual information concerning some
side-effect or leading to a contestable or refutable side-effect. A weaker
notion indicates the support of some relevant consequences by a prior abductive
context. Yet another definition describes jointly supported relevant
consequences, which captures the idea of two observations containing mutually
supportive side-effects. Though motivated with and exemplified by the running
psychology application, the various new general abductive context definitions
are introduced here and given a declarative semantics for the first time, and
have a much wider scope of application. Inspection points, a concept introduced
by Pereira and Pinto, allows us to express these definitions syntactically and
intertwine them into an operational semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3724</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3724</id><created>2014-05-14</created><authors><author><keyname>Chandio</keyname><forenames>Aftab Ahmed</forenames></author><author><keyname>Zhu</keyname><forenames>Dingju</forenames></author><author><keyname>Sodhro</keyname><forenames>Ali Hassan</forenames></author></authors><title>Integration of Inter-Connectivity of Information System (i3) using Web
  Services</title><categories>cs.SE</categories><comments>5 pages, 5 figures, submitted in the International MultiConference of
  Engineers and Computer Scientists (IMECS) 2012 March 12-16 Hong Kong</comments><journal-ref>Lecture Notes in Engineering and Computer Science, Vol. 2195(1),
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth and development of institutions day by day especially universities
are increasing very large number of peoples including employees as well as
students, working on different domain in distance between multiple integrated
institutes, and degrading the overall efficiency of system that cannot
facilitate better interoperability and automated integration. In many
universities, exist system does not have yet set of connections, in spite of
the fact they have different systems running on different platforms in
different administrative units or departments. University of Sindh (UoS) is the
one of them. In this paper we discuss about Integration of Inter-Connectivity
of Information System (i3) in the UoS which is based on Service Oriented
Architecture with web services. The proposed system (i3) can monitor and
exchange students information in support of verification along heterogeneous
and decentralized nature, and provide capability of interoperability in the
place of already deployed system in the UoS using different Languages as well
as Databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3725</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3725</id><created>2014-05-14</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Improving Physical-Layer Security in Wireless Communications Using
  Diversity Techniques</title><categories>cs.IT math.IT</categories><comments>IEEE Network Magazine, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the broadcast nature of radio propagation, the wireless transmission
can be readily overheard by unauthorized users for interception purposes and is
thus highly vulnerable to eavesdropping attacks. To this end, physical-layer
security is emerging as a promising paradigm to protect the wireless
communications against eavesdropping attacks by exploiting the physical
characteristics of wireless channels. This article is focused on the
investigation of diversity techniques to improve the physical-layer security,
differing from the conventional artificial noise generation and beamforming
techniques which typically consume additional power for generating artificial
noise and exhibit high implementation complexity for beamformer design. We
present several diversity approaches to improve the wireless physical-layer
security, including the multiple-input multiple-output (MIMO), multiuser
diversity, and cooperative diversity. To illustrate the security improvement
through diversity, we propose a case study of exploiting cooperative relays to
assist the signal transmission from source to destination while defending
against eavesdropping attacks. We evaluate the security performance of
cooperative relay transmission in Rayleigh fading environments in terms of
secrecy capacity and intercept probability. It is shown that as the number of
relays increases, the secrecy capacity and intercept probability of the
cooperative relay transmission both improve significantly, implying the
advantage of exploiting cooperative diversity to improve the physical-layer
security against eavesdropping attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3726</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3726</id><created>2014-05-14</created><authors><author><keyname>Qiu</keyname><forenames>Xi</forenames></author><author><keyname>Stewart</keyname><forenames>Christopher</forenames></author></authors><title>Topic words analysis based on LDA model</title><categories>cs.SI cs.DC cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network analysis (SNA), which is a research field describing and
modeling the social connection of a certain group of people, is popular among
network services. Our topic words analysis project is a SNA method to visualize
the topic words among emails from Obama.com to accounts registered in Columbus,
Ohio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model
of SNA, our project characterizes the preference of senders for target group of
receptors. Gibbs sampling is used to estimate topic and word distribution. Our
training and testing data are emails from the carbon-free server
Datagreening.com. We use parallel computing tool BashReduce for word processing
and generate related words under each latent topic to discovers typical
information of political news sending specially to local Columbus receptors.
Running on two instances using paralleling tool BashReduce, our project
contributes almost 30% speedup processing the raw contents, comparing with
processing contents on one instance locally. Also, the experimental result
shows that the LDA model applied in our project provides precision rate 53.96%
higher than TF-IDF model finding target words, on the condition that
appropriate size of topic words list is selected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3727</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3727</id><created>2014-05-14</created><authors><author><keyname>Rai</keyname><forenames>Sweta</forenames></author></authors><title>Student Dropout Risk Assessment in Undergraduate Course at Residential
  University</title><categories>cs.CY cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1202.4815, arXiv:1203.3832,
  arXiv:1002.1144 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Student dropout prediction is an indispensable for numerous intelligent
systems to measure the education system and success rate of any university as
well as throughout the university in the world. Therefore, it becomes essential
to develop efficient methods for prediction of the students at risk of dropping
out, enabling the adoption of proactive process to minimize the situation.
Thus, this research work propose a prototype machine learning tool which can
automatically recognize whether the student will continue their study or drop
their study using classification technique based on decision tree and extract
hidden information from large data about what factors are responsible for
dropout student. Further the contribution of factors responsible for dropout
risk was studied using discriminant analysis and to extract interesting
correlations, frequent patterns, associations or casual structures among
significant datasets, Association rule mining was applied. In this study, the
descriptive statistics analysis was carried out to measure the quality of data
using SPSS 20.0 statistical software and application of decision tree and
association rule were carried out by using WEKA data mining tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3729</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3729</id><created>2014-05-14</created><authors><author><keyname>Saini</keyname><forenames>Priyanka</forenames></author></authors><title>Building a Classification Model for Enrollment In Higher Educational
  Courses using Data Mining Techniques</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Mining is the process of extracting useful patterns from the huge amount
of database and many data mining techniques are used for mining these patterns.
Recently, one of the remarkable facts in higher educational institute is the
rapid growth data and this educational data is expanding quickly without any
advantage to the educational management. The main aim of the management is to
refine the education standard; therefore by applying the various data mining
techniques on this data one can get valuable information. This research study
proposed the &quot;classification model for the student's enrollment process in
higher educational courses using data mining techniques&quot;. Additionally, this
study contributes to finding some patterns that are meaningful to management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3739</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3739</id><created>2014-05-15</created><authors><author><keyname>Demaine</keyname><forenames>Erik</forenames></author><author><keyname>Pinsker</keyname><forenames>Nathan</forenames></author><author><keyname>Schneider</keyname><forenames>Jon</forenames></author></authors><title>Fast Dynamic Pointer Following via Link-Cut Trees</title><categories>cs.DS</categories><comments>7 pages</comments><msc-class>68P05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of fast dynamic pointer following: given
a directed graph $G$ where each vertex has outdegree $1$, efficiently support
the operations of i) changing the outgoing edge of any vertex, and ii) find the
vertex $k$ vertices `after' a given vertex. We exhibit a solution to this
problem based on link-cut trees that requires $O(\lg n)$ time per operation,
and prove that this is optimal in the cell-probe complexity model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3750</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3750</id><created>2014-05-15</created><updated>2014-07-12</updated><authors><author><keyname>Lee</keyname><forenames>Kyumin</forenames></author><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author><author><keyname>Chen</keyname><forenames>Jilin</forenames></author><author><keyname>Zhou</keyname><forenames>Michelle</forenames></author><author><keyname>Nichols</keyname><forenames>Jeffrey</forenames></author></authors><title>Who Will Retweet This? Automatically Identifying and Engaging Strangers
  on Twitter to Spread Information</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much effort on studying how social media sites, such as
Twitter, help propagate information in different situations, including
spreading alerts and SOS messages in an emergency. However, existing work has
not addressed how to actively identify and engage the right strangers at the
right time on social media to help effectively propagate intended information
within a desired time frame. To address this problem, we have developed two
models: (i) a feature-based model that leverages peoples' exhibited social
behavior, including the content of their tweets and social interactions, to
characterize their willingness and readiness to propagate information on
Twitter via the act of retweeting; and (ii) a wait-time model based on a user's
previous retweeting wait times to predict her next retweeting time when asked.
Based on these two models, we build a recommender system that predicts the
likelihood of a stranger to retweet information when asked, within a specific
time window, and recommends the top-N qualified strangers to engage with. Our
experiments, including live studies in the real world, demonstrate the
effectiveness of our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3758</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3758</id><created>2014-05-15</created><authors><author><keyname>Kohlhase</keyname><forenames>Andrea</forenames></author></authors><title>Search Interfaces for Mathematicians</title><categories>cs.HC cs.DL cs.MS</categories><comments>conference article &quot;CICM'14: International Conference on Computer
  Mathematics 2014&quot;, DML-Track: Digital Math Libraries 17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access to mathematical knowledge has changed dramatically in recent years,
therefore changing mathematical search practices. Our aim with this study is to
scrutinize professional mathematicians' search behavior. With this
understanding we want to be able to reason why mathematicians use which tool
for what search problem in what phase of the search process. To gain these
insights we conducted 24 repertory grid interviews with mathematically inclined
people (ranging from senior professional mathematicians to non-mathematicians).
From the interview data we elicited patterns for the user group
&quot;mathematicians&quot; that can be applied when understanding design issues or
creating new designs for mathematical search interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3760</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3760</id><created>2014-05-15</created><authors><author><keyname>Spreitzer</keyname><forenames>Raphael</forenames></author></authors><title>PIN Skimming: Exploiting the Ambient-Light Sensor in Mobile Devices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new type of side channel which is based on the
ambient-light sensor employed in today's mobile devices. The pervasive usage of
mobile devices, i.e., smartphones and tablet computers and their vast amount of
sensors represent a plethora of side channels posing a serious threat to the
user's privacy and security. While recent advances in this area of research
focused on the employed motion sensors and the camera as well as the sound, we
investigate a less obvious source of information leakage, namely the ambient
light. We successfully demonstrate that minor tilts and turns of mobile devices
cause variations of the ambient-light sensor information. Thus, we are the
first to show that this sensor leaks sensitive information. Furthermore, we
demonstrate that these variations leak enough information to infer a user's
personal identification number (PIN) input based on a set of known PINs. Our
results show that we are able to determine the correct PIN---out of a set of 50
random PINs---within the first ten guesses about 80% of the time. In contrast,
the chance of finding the right PIN by randomly guessing ten PINs would be 20%.
Since the data required to perform such an attack can be gathered without any
specific permissions or privileges, the presented side channel seriously
jeopardizes the security and privacy of mobile-device owners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3772</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3772</id><created>2014-05-15</created><authors><author><keyname>Haralambous</keyname><forenames>Yannis</forenames></author><author><keyname>Sauvage-Vincent</keyname><forenames>Julie</forenames></author><author><keyname>Puentes</keyname><forenames>John</forenames></author></authors><title>INAUT, a Controlled Language for the French Coast Pilot Books
  Instructions nautiques</title><categories>cs.CL</categories><comments>10 pages, 3 figures, accepted for publication at Fourth Workshop on
  Controlled Natural Language (CNL 2014), 20-22 August 2014, Galway, Ireland</comments><msc-class>68T30, 68T50, 97G40</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe INAUT, a controlled natural language dedicated to collaborative
update of a knowledge base on maritime navigation and to automatic generation
of coast pilot books (Instructions nautiques) of the French National
Hydrographic and Oceanographic Service SHOM. INAUT is based on French language
and abundantly uses georeferenced entities. After describing the structure of
the overall system, giving details on the language and on its generation, and
discussing the three major applications of INAUT (document production,
interaction with ENCs and collaborative updates of the knowledge base), we
conclude with future extensions and open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3775</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3775</id><created>2014-05-15</created><authors><author><keyname>Gholami</keyname><forenames>Mohammad</forenames></author><author><keyname>Samadieh</keyname><forenames>Mehdi</forenames></author></authors><title>Quasi Cyclic LDPC Codes Based on Finite Set Systems</title><categories>cs.IT cs.CR cs.DM math.IT</categories><comments>21 page,6 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite set system (FSS) is a pair (V; B) where V is a finite set whose
members are called points, equipped with a finite collection of its subsets B
whose members are called blocks. In this paper, finite set systems are used to
define a class of Quasi-cyclic low- density parity-check (LDPC) codes, called
FSS codes, such that the constructed codes possess large girth and arbitrary
column-weight distributions. Especially, the constructed column weight-2 FSS
codes have higher rates than the column weight-2 geometric and cylinder-type
codes with the same girths. To find the maximum girth of FSS codes based on (V;
B), inevitable walks are defined in B such that the maximum girth is determined
by the smallest length of the inevitable walks in B. Simulation results show
that the constructed FSS codes have very good performance over the AWGN channel
with iterative decoding and achieve significantly large coding gains compared
to the random-like LDPC codes of the same lengths and rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3786</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3786</id><created>2014-05-15</created><authors><author><keyname>Margan</keyname><forenames>Domagoj</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author></authors><title>Complex Networks Measures for Differentiation between Normal and
  Shuffled Croatian Texts</title><categories>cs.CL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the properties of the Croatian texts via complex networks.
We present network properties of normal and shuffled Croatian texts for
different shuffling principles: on the sentence level and on the text level. In
both experiments we preserved the vocabulary size, word and sentence frequency
distributions. Additionally, in the first shuffling approach we preserved the
sentence structure of the text and the number of words per sentence. Obtained
results showed that degree rank distributions exhibit no substantial deviation
in shuffled networks, and strength rank distributions are preserved due to the
same word frequencies. Therefore, standard approach to study the structure of
linguistic co-occurrence networks showed no clear difference among the
topologies of normal and shuffled texts. Finally, we showed that the in- and
out- selectivity values from shuffled texts are constantly below selectivity
values calculated from normal texts. Our results corroborate that the node
selectivity measure can capture structural differences between original and
shuffled Croatian texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3790</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3790</id><created>2014-05-15</created><authors><author><keyname>Gomes</keyname><forenames>Ana Sofia</forenames></author><author><keyname>Alferes</keyname><forenames>Jos&#xe9; J&#xfa;lio</forenames></author></authors><title>Transaction Logic with (Complex) Events</title><categories>cs.AI cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with the problem of combining reactive features, such as the
ability to respond to events and define complex events, with the execution of
transactions over general Knowledge Bases (KBs).
  With this as goal, we build on Transaction Logic (TR), a logic precisely
designed to model and execute transactions in KBs defined by arbitrary logic
theories. In it, transactions are written in a logic-programming style, by
combining primitive update operations over a general KB, with the usual logic
programming connectives and some additional connectives e.g. to express
sequence of actions. While TR is a natural choice to deal with transactions, it
remains the question whether TR can be used to express complex events, but also
to deal simultaneously with the detection of complex events and the execution
of transactions. In this paper we show that the former is possible while the
latter is not. For that, we start by illustrating how TR can express complex
events, and in particular, how SNOOP event expressions can be translated in the
logic. Afterwards, we show why TR fails to deal with the two issues together,
and to solve the intended problem propose Transaction Logic with Events, its
syntax, model theory and executional semantics. The achieved solution is a
non-monotonic extension of TR, which guarantees that every complex event
detected in a transaction is necessarily responded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3791</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3791</id><created>2014-05-15</created><authors><author><keyname>Banu-Demergian</keyname><forenames>Iulia Teodora</forenames></author><author><keyname>Stefanescu</keyname><forenames>Gheorghe</forenames></author></authors><title>On contour representation of two dimensional patterns</title><categories>cs.PL</categories><comments>To appear in: Carpathian J. Math., 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-dimensional patterns are used in many research areas in computer science,
ranging from image processing to specification and verification of complex
software systems (via scenarios). The contribution of this paper is twofold.
First, we present the basis of a new formal representation of two-dimensional
patterns based on contours and their compositions. Then, we present efficient
algorithms to verify correctness of the contour-representation. Finally, we
briefly discuss possible applications, in particular using them as a basic
instrument in developing software tools for handling two dimensional words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3792</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3792</id><created>2014-05-15</created><authors><author><keyname>Charalambidis</keyname><forenames>Angelos</forenames></author><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Rondogiannis</keyname><forenames>Panos</forenames></author></authors><title>Minimum Model Semantics for Extensional Higher-order Logic Programming
  with Negation</title><categories>cs.PL cs.AI cs.LO</categories><doi>10.1017/S1471068414000313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensional higher-order logic programming has been introduced as a
generalization of classical logic programming. An important characteristic of
this paradigm is that it preserves all the well-known properties of traditional
logic programming. In this paper we consider the semantics of negation in the
context of the new paradigm. Using some recent results from non-monotonic
fixed-point theory, we demonstrate that every higher-order logic program with
negation has a unique minimum infinite-valued model. In this way we obtain the
first purely model-theoretic semantics for negation in extensional higher-order
logic programming. Using our approach, we resolve an old paradox that was
introduced by W. W. Wadge in order to demonstrate the semantic difficulties of
higher-order logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3793</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3793</id><created>2014-05-15</created><authors><author><keyname>Sharaf</keyname><forenames>Nada</forenames></author><author><keyname>Abdennadher</keyname><forenames>Slim</forenames></author><author><keyname>Fruehwirth</keyname><forenames>Thom</forenames></author></authors><title>Visualization of Constraint Handling Rules</title><categories>cs.PL</categories><comments>An extended abstract / full version of a paper accepted to be
  presented at the Doctoral Consortium of the 30th International Conference on
  Logic Programming (ICLP 2014), July 19-22, Vienna, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint Handling Rules (CHR) has matured into a general purpose language
over the past two decades. Any general purpose language requires its own
development tools. Visualization tools, in particular, facilitate many tasks
for programmers as well as beginners to the language. The article presents
on-going work towards the visualization of CHR programs. The process is done
through source-to-source transformation. It aims towards reaching a generic
transformer to visualize different algorithms implemented in CHR. Note: An
extended abstract / full version of a paper accepted to be presented at the
Doctoral Consortium of the 30th International Conference on Logic Programming
(ICLP 2014), July 19-22, Vienna, Austria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3795</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3795</id><created>2014-05-15</created><authors><author><keyname>Ja&#x15b;kiewicz</keyname><forenames>Grzegorz</forenames></author></authors><title>Logic Programming as Scripting Language for Bots in Computer Games --
  Research Overview</title><categories>cs.PL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This publication is to present a summary of research (referred as Klabs -
http://www.kappalabs.org) carried out in author's Ph.D studies on topic of
application of Logic Programming as scripting language for virtual character
behavior control in First Person Shooter (FPS) games. The research goal is to
apply reasoning and knowledge representation techniques to create character
behavior, which results in increased players' engagement. An extended abstract
/ full version of a paper accepted to be presented at the Doctoral Consortium
of the 30th International Conference on Logic Programming (ICLP 2014), July
19-22, Vienna, Austria
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3805</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3805</id><created>2014-05-15</created><authors><author><keyname>Ervik</keyname><forenames>&#xc5;smund</forenames></author><author><keyname>Munkejord</keyname><forenames>Svend Tollak</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Bernhard</forenames></author></authors><title>Extending a serial 3D two-phase CFD code to parallel execution over MPI
  by using the PETSc library for domain decomposition</title><categories>physics.comp-ph cs.DC physics.flu-dyn</categories><comments>8 pages, 6 figures, final version for to the CFD 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To leverage the last two decades' transition in High-Performance Computing
(HPC) towards clusters of compute nodes bound together with fast interconnects,
a modern scalable CFD code must be able to efficiently distribute work amongst
several nodes using the Message Passing Interface (MPI). MPI can enable very
large simulations running on very large clusters, but it is necessary that the
bulk of the CFD code be written with MPI in mind, an obstacle to parallelizing
an existing serial code.
  In this work we present the results of extending an existing two-phase 3D
Navier-Stokes solver, which was completely serial, to a parallel execution
model using MPI. The 3D Navier-Stokes equations for two immiscible
incompressible fluids are solved by the continuum surface force method, while
the location of the interface is determined by the level-set method.
  We employ the Portable Extensible Toolkit for Scientific Computing (PETSc)
for domain decomposition (DD) in a framework where only a fraction of the code
needs to be altered. We study the strong and weak scaling of the resulting
code. Cases are studied that are relevant to the fundamental understanding of
oil/water separation in electrocoalescers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3806</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3806</id><created>2014-05-15</created><authors><author><keyname>Papadopoulou</keyname><forenames>Evanthia</forenames></author><author><keyname>Zavershynskyi</keyname><forenames>Maksym</forenames></author></authors><title>The Higher-Order Voronoi Diagram of Line Segments</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surprisingly, the order-$k$ Voronoi diagram of line segments had received no
attention in the computational-geometry literature. It illustrates properties
surprisingly different from its counterpart for points; for example, a single
order-$k$ Voronoi region may consist of $\Omega(n)$ disjoint faces. We analyze
the structural properties of this diagram and show that its combinatorial
complexity for $n$ non-crossing line segments is $O(k(n-k))$, despite the
disconnected regions. The same bound holds for $n$ intersecting line segments,
when $k\geq n/2$. We also consider the order-$k$ Voronoi diagram of line
segments that form a planar straight-line graph, and augment the definition of
an order-$k$ Voronoi diagram to cover non-disjoint sites, addressing the issue
of non-uniqueness for $k$-nearest sites. Furthermore, we enhance the iterative
approach to construct this diagram. All bounds are valid in the general $L_p$
metric, $1\leq p\leq \infty$. For non-crossing segments in the $L_\infty$ and
$L_1$ metrics, we show a tighter $O((n-k)^2)$ bound for $k&gt;n/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3817</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3817</id><created>2014-05-15</created><updated>2014-08-28</updated><authors><author><keyname>Favrholdt</keyname><forenames>Lene M.</forenames></author><author><keyname>Mikkelsen</keyname><forenames>Jesper W.</forenames></author></authors><title>Online Dual Edge Coloring of Paths and Trees</title><categories>cs.DS</categories><comments>WAOA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a dual version of online edge coloring, where the goal is to color
as many edges as possible using only a given number, $k$, of available colors.
All of our results are with regard to competitive analysis. For paths, we
consider $k=2$, and for trees, we consider any $k \geq 2$. We prove that a
natural greedy algorithm called First-Fit is optimal among deterministic
algorithms on paths as well as trees. This is the first time that an optimal
algorithm for online dual edge coloring has been identified for a class of
graphs. For paths, we give a randomized algorithm, which is optimal and better
than the best possible deterministic algorithm. Again, it is the first time
that this has been done for a class of graphs. For trees, we also show that
even randomized algorithms cannot be much better than First-Fit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3824</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3824</id><created>2014-05-15</created><authors><author><keyname>Gavanelli</keyname><forenames>Marco</forenames></author><author><keyname>Bragaglia</keyname><forenames>Stefano</forenames></author><author><keyname>Milano</keyname><forenames>Michela</forenames></author><author><keyname>Chesani</keyname><forenames>Federico</forenames></author><author><keyname>Marengo</keyname><forenames>Elisa</forenames></author><author><keyname>Cagnoli</keyname><forenames>Paolo</forenames></author></authors><title>Multi-Criteria Optimal Planning for Energy Policies in CLP</title><categories>cs.AI</categories><comments>Accepted at ICLP2014 Conference as Technical Communication, due to
  appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the policy making process a number of disparate and diverse issues such as
economic development, environmental aspects, as well as the social acceptance
of the policy, need to be considered. A single person might not have all the
required expertises, and decision support systems featuring optimization
components can help to assess policies. Leveraging on previous work on
Strategic Environmental Assessment, we developed a fully-fledged system that is
able to provide optimal plans with respect to a given objective, to perform
multi-objective optimization and provide sets of Pareto optimal plans, and to
visually compare them. Each plan is environmentally assessed and its footprint
is evaluated. The heart of the system is an application developed in a popular
Constraint Logic Programming system on the Reals sort. It has been equipped
with a web service module that can be queried through standard interfaces, and
an intuitive graphic user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3826</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3826</id><created>2014-05-15</created><authors><author><keyname>Stephan</keyname><forenames>Heike</forenames></author></authors><title>Application of Methods for Syntax Analysis of Context-Free Languages to
  Query Evaluation of Logic Programs</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  My research goal is to employ a parser generation algorithm based on the
Earley parsing algorithm to the evaluation and compilation of queries to logic
programs, especially to deductive databases. By means of partial deduction,
from a query to a logic program a parameterized automaton is to be generated
that models the evaluation of this query. This automaton can be compiled to
executable code; thus we expect a speedup in runtime of query evaluation. An
extended abstract/ full version of a paper accepted to be presented at the
Doctoral Consortium of the 30th International Conference on Logic Programming
(ICLP 2014), July 19-22, Vienna, Austria
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3843</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3843</id><created>2014-05-15</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Levy</keyname><forenames>Kfir Y.</forenames></author></authors><title>Logistic Regression: Tight Bounds for Stochastic and Online Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logistic loss function is often advocated in machine learning and
statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this
paper we investigate the question of whether these smoothness and convexity
properties make the logistic loss preferable to other widely considered options
such as the hinge loss. We show that in contrast to known asymptotic bounds, as
long as the number of prediction/optimization iterations is sub exponential,
the logistic loss provides no improvement over a generic non-smooth loss
function such as the hinge loss. In particular we show that the convergence
rate of stochastic logistic optimization is bounded from below by a polynomial
in the diameter of the decision set and the number of prediction iterations,
and provide a matching tight upper bound. This resolves the COLT open problem
of McMahan and Streeter (2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3848</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3848</id><created>2014-05-15</created><updated>2015-02-02</updated><authors><author><keyname>Joswig</keyname><forenames>Michael</forenames></author><author><keyname>Lutz</keyname><forenames>Frank H.</forenames></author><author><keyname>Tsuruga</keyname><forenames>Mimi</forenames></author></authors><title>Sphere Recognition: Heuristics and Examples</title><categories>math.GT cs.CG math.AT math.MG</categories><comments>30 pages, full paper replacing extended abstract</comments><report-no>CPH-SYM-DNRF92</report-no><msc-class>57Q15, 57Q05, 57M05, 57M40, 57N12, 52B70, 52B05, 52B22, 55N35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heuristic techniques for recognizing PL spheres using the topological
software polymake are presented. These methods have been successful very often
despite sphere recognition being known to be hard (for dimensions $d \ge 3$) or
even undecidable (for $d \ge 5$). A deeper look into the simplicial complexes
for which the heuristics failed uncovered a trove of examples having
interesting topological and combinatorial properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3860</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3860</id><created>2014-05-15</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Spatial Spectrum Access Game</title><categories>cs.NI cs.GT</categories><comments>The paper has been accepted by IEEE Transactions on Mobile Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key feature of wireless communications is the spatial reuse. However, the
spatial aspect is not yet well understood for the purpose of designing
efficient spectrum sharing mechanisms. In this paper, we propose a framework of
spatial spectrum access games on directed interference graphs, which can model
quite general interference relationship with spatial reuse in wireless
networks. We show that a pure Nash equilibrium exists for the two classes of
games: (1) any spatial spectrum access games on directed acyclic graphs, and
(2) any games satisfying the congestion property on directed trees and directed
forests. Under mild technical conditions, the spatial spectrum access games
with random backoff and Aloha channel contention mechanisms on undirected
graphs also have a pure Nash equilibrium. We also quantify the price of anarchy
of the spatial spectrum access game. We then propose a distributed learning
algorithm, which only utilizes users' local observations to adaptively adjust
the spectrum access strategies. We show that the distributed learning algorithm
can converge to an approximate mixed-strategy Nash equilibrium for any spatial
spectrum access games. Numerical results demonstrate that the distributed
learning algorithm achieves up to superior performance improvement over a
random access algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3865</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3865</id><created>2014-05-15</created><updated>2015-02-15</updated><authors><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Kernelization Complexity of Possible Winner and Coalitional Manipulation
  Problems in Voting</title><categories>cs.GT</categories><comments>Accepted in AAMAS 2015</comments><acm-class>F.2; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Possible Winner problem in computational social choice theory, we are
given a set of partial preferences and the question is whether a distinguished
candidate could be made winner by extending the partial preferences to linear
preferences. Previous work has provided, for many common voting rules, fixed
parameter tractable algorithms for the Possible Winner problem, with number of
candidates as the parameter. However, the corresponding kernelization question
is still open and in fact, has been mentioned as a key research challenge. In
this paper, we settle this open question for many common voting rules.
  We show that the Possible Winner problem for maximin, Copeland, Bucklin,
ranked pairs, and a class of scoring rules that include the Borda voting rule
do not admit a polynomial kernel with the number of candidates as the
parameter. We show however that the Coalitional Manipulation problem which is
an important special case of the Possible Winner problem does admit a
polynomial kernel for maximin, Copeland, ranked pairs, and a class of scoring
rules that includes the Borda voting rule, when the number of manipulators is
polynomial in the number of candidates. A significant conclusion of our work is
that the Possible Winner problem is harder than the Coalitional Manipulation
problem since the Coalitional Manipulation problem admits a polynomial kernel
whereas the Possible Winner problem does not admit a polynomial kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3866</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3866</id><created>2014-05-15</created><authors><author><keyname>Jaderberg</keyname><forenames>Max</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Speeding up Convolutional Neural Networks with Low Rank Expansions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is speeding up the evaluation of convolutional neural
networks. While delivering impressive results across a range of computer vision
and machine learning tasks, these networks are computationally demanding,
limiting their deployability. Convolutional layers generally consume the bulk
of the processing time, and so in this work we present two simple schemes for
drastically speeding up these layers. This is achieved by exploiting
cross-channel or filter redundancy to construct a low rank basis of filters
that are rank-1 in the spatial domain. Our methods are architecture agnostic,
and can be easily applied to existing CPU and GPU convolutional frameworks for
tuneable speedup performance. We demonstrate this with a real world network
designed for scene text character recognition, showing a possible 2.5x speedup
with no loss in accuracy, and 4.5x speedup with less than 1% drop in accuracy,
still achieving state-of-the-art on standard benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3869</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3869</id><created>2014-05-15</created><authors><author><keyname>Bendaoud</keyname><forenames>Fayssal</forenames></author><author><keyname>Abdennebi</keyname><forenames>Marwen</forenames></author><author><keyname>Didi</keyname><forenames>Fedoua</forenames></author></authors><title>Allocation des ressources radio en LTE</title><categories>cs.NI</categories><comments>in French. International Congress on Telecommunication and
  Application'14 University of A.MIRA Bejaia, Algeria, 23-24 APRIL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate several approaches and algorithms that have been
proposed in the literature to address the need (allocate resources
efficiently), this diversity and multitude of algorithms is related the factors
considered for the optimal management of requested radio traffic type resource,
specifically the requested QoS by the EU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3883</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3883</id><created>2014-05-15</created><authors><author><keyname>Gallagher</keyname><forenames>John P.</forenames></author><author><keyname>Kafle</keyname><forenames>Bishoksan</forenames></author></authors><title>Analysis and Transformation Tools for Constrained Horn Clause
  Verification</title><categories>cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several techniques and tools have been developed for verification of
properties expressed as Horn clauses with constraints over a background theory
(CHC). Current CHC verification tools implement intricate algorithms and are
often limited to certain subclasses of CHC problems. Our aim in this work is to
investigate the use of a combination of off-the-shelf techniques from the
literature in analysis and transformation of Constraint Logic Programs (CLPs)
to solve challenging CHC verification problems. We find that many problems can
be solved using a combination of tools based on well-known techniques from
abstract interpretation, semantics-preserving transformations, program
specialisation and query-answer transformations. This gives insights into the
design of automatic, more general CHC verification tools based on a library of
components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3896</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3896</id><created>2014-05-15</created><authors><author><keyname>Abrantes</keyname><forenames>M&#xe1;rio</forenames></author><author><keyname>Pereira</keyname><forenames>Lu&#xed;s Moniz</forenames></author></authors><title>Properties of Stable Model Semantics Extensions</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP), 10
  pages plus appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stable model (SM) semantics lacks the properties of existence, relevance
and cumulativity. If we prospectively consider the class of conservative
extensions of SM semantics (i.e., semantics that for each normal logic program
P retrieve a superset of the set of stable models of P), one may wander how do
the semantics of this class behave in what concerns the aforementioned
properties. That is the type of issue dealt with in this paper. We define a
large class of conservative extensions of the SM semantics, dubbed affix stable
model semantics, ASM, and study the above referred properties into two
non-disjoint subfamilies of the class ASM, here dubbed ASMh and ASMm. From this
study a number of results stem which facilitate the assessment of semantics in
the class ASMh U ASMm with respect to the properties of existence, relevance
and cumulativity, whilst unveiling relations among these properties. As a
result of the approach taken in our work, light is shed on the characterization
of the SM semantics, as we show that the properties of (lack of) existence and
(lack of) cautious monotony are equivalent, which opposes statements on this
issue that may be found in the literature; we also characterize the relevance
failure of SM semantics in a more clear way than usually stated in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3899</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3899</id><created>2014-05-15</created><updated>2014-06-01</updated><authors><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Zhang</keyname><forenames>Tianxian</forenames></author><author><keyname>Kong</keyname><forenames>Lingjiang</forenames></author></authors><title>MIMO OFDM Radar IRCI Free Range Reconstruction with Sufficient Cyclic
  Prefix</title><categories>cs.IT math.IT</categories><comments>36 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we propose MIMO OFDM radar with sufficient cyclic prefix (CP),
where all OFDM pulses transmitted from different transmitters share the same
frequency band and are orthogonal to each other for every subcarrier in the
discrete frequency domain. The orthogonality is not affected by time delays
from transmitters. Thus, our proposed MIMO OFDM radar has the same range
resolution as single transmitter radar and achieves full spatial diversity.
Orthogonal designs are used to achieve this orthogonality across the
transmitters, with which it is only needed to design OFDM pulses for the first
transmitter. We also propose a joint pulse compression and pulse coherent
integration for range reconstruction. In order to achieve the optimal SNR for
the range reconstruction, we apply the paraunitary filterbank theory to design
the OFDM pulses. We then propose a modified iterative clipping and filtering
(MICF) algorithm for the designs of OFDM pulses jointly, when other important
factors, such as peak-to-average power ratio (PAPR) in time domain, are also
considered. With our proposed MIMO OFDM radar, there is no interference for the
range reconstruction not only across the transmitters but also across the range
cells in a swath called inter-range-cell interference (IRCI) free that is
similar to our previously proposed CP based OFDM radar for single transmitter.
Simulations are presented to illustrate our proposed theory and show that the
CP based MIMO OFDM radar outperforms the existing frequency-band shared MIMO
radar with polyphase codes and also frequency division MIMO radar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3906</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3906</id><created>2014-05-15</created><authors><author><keyname>Gauthier</keyname><forenames>Thibault</forenames></author><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author></authors><title>Matching concepts across HOL libraries</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many proof assistant libraries contain formalizations of the same
mathematical concepts. The concepts are often introduced (defined) in different
ways, but the properties that they have, and are in turn formalized, are the
same. For the basic concepts, like natural numbers, matching them between
libraries is often straightforward, because of mathematical naming conventions.
However, for more advanced concepts, finding similar formalizations in
different libraries is a non-trivial task even for an expert.
  In this paper we investigate automatic discovery of similar concepts across
libraries of proof assistants. We propose an approach for normalizing
properties of concepts in formal libraries and a number of similarity measures.
We evaluate the approach on HOL based proof assistants HOL4, HOL Light and
Isabelle/HOL, discovering 398 pairs of isomorphic constants and types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3912</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3912</id><created>2014-05-15</created><authors><author><keyname>Jichkar</keyname><forenames>Mr. Rahul A</forenames></author><author><keyname>Chandak</keyname><forenames>Dr. M. B.</forenames></author></authors><title>An implementation on detection of trusted service provider in mobile
  ad-hoc networks</title><categories>cs.CR cs.NI</categories><comments>12 pages, 30 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V11(2),64-74 May 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V11P213</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The mobile ad-hoc network consists of energy constraint devices called nodes
communicating through radio signals forming a temporary network i.e. nodes are
continuously switching from one network to another. To minimize the power
consumption, we form the clusters with an elected cluster head (Service
Provider) based on any cluster head selection strategy.To establish a trusted
link amongst newly entered node and CH we have adopted an indirect trust
computation technique based on recommendations, which form an important
component in trust-based access control models for pervasive environment.In
this paper, we shall present some existing indirect trust based techniques and
subsequently discuss our proposal along with its merits and future scope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3925</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3925</id><created>2014-05-15</created><authors><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>IDSL, INRIA Saclay - Ile de France, CMB</affiliation></author><author><keyname>Witt</keyname><forenames>Andreas</forenames><affiliation>IDS</affiliation></author></authors><title>M\'ethodes pour la repr\'esentation informatis\'ee de donn\'ees
  lexicales / Methoden der Speicherung lexikalischer Daten</title><categories>cs.CL</categories><comments>This text comprises both a French and a German version</comments><proxy>ccsd</proxy><journal-ref>Lexicographica 30 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, new developments in the area of lexicography have altered
not only the management, processing and publishing of lexicographical data, but
also created new types of products such as electronic dictionaries and
thesauri. These expand the range of possible uses of lexical data and support
users with more flexibility, for instance in assisting human translation. In
this article, we give a short and easy-to-understand introduction to the
problematic nature of the storage, display and interpretation of lexical data.
We then describe the main methods and specifications used to build and
represent lexical data. This paper is targeted for the following groups of
people: linguists, lexicographers, IT specialists, computer linguists and all
others who wish to learn more about the modelling, representation and
visualization of lexical knowledge. This paper is written in two languages:
French and German.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3939</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3939</id><created>2014-05-12</created><authors><author><keyname>A</keyname><forenames>Aadhityan</forenames></author></authors><title>A Novel Method for Developing Robotics via Artificial Intelligence and
  Internet of Things</title><categories>cs.RO cs.AI cs.CY</categories><journal-ref>IJCA Proceedings on National Conference on Future Computing 2014
  NCFC 2014(1):1-4, January 2014. Published by Foundation of Computer Science,
  New York, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describe about a new methodology for developing and improving the
robotics field via artificial intelligence and internet of things. Now a day,
we can say Artificial Intelligence take the world into robotics. Almost all
industries use robots for lot of works. They are use co-operative robots to
make different kind of works. But there was some problem to make robot for
multi tasks. So there was a necessary new methodology to made multi tasking
robots. It will be done only by artificial intelligence and internet of things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3941</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3941</id><created>2014-05-15</created><authors><author><keyname>Kandeepan</keyname><forenames>Sithamparanathan</forenames></author><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Reynaud</keyname><forenames>Laurent</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author></authors><title>Aerial-Terrestrial Communications: Terrestrial Cooperation and
  Energy-Efficient Transmissions to Aerial-Base Stations</title><categories>cs.NI</categories><comments>To Appear In IEEE Transactions On Aerospace And Electronic Systems,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid aerial-terrestrial communication networks based on Low Altitude
Platforms (LAPs) are expected to optimally meet the urgent communication needs
of emergency relief and recovery operations for tackling large scale natural
disasters. The energy-efficient operation of such networks is important given
the fact that the entire network infrastructure, including the battery operated
ground terminals, exhibits requirements to operate under power-constrained
situations. In this paper, we discuss the design and evaluation of an adaptive
cooperative scheme intended to extend the survivability of the battery operated
aerial-terrestrial communication links. We propose and evaluate a real-time
adaptive cooperative transmission strategy for dynamic selection between direct
and cooperative links based on the channel conditions for improved energy
efficiency. We show that the cooperation between mobile terrestrial terminals
on the ground could improve the energy efficiency in the uplink depending on
the temporal behavior of the terrestrial and the aerial uplink channels. The
corresponding delay in having cooperative (relay-based) communications with
relay selection is also addressed. The simulation analysis corroborates that
the adaptive transmission technique improves the overall energy efficiency of
the network whilst maintaining low latency enabling real time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3953</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3953</id><created>2014-05-14</created><authors><author><keyname>Lager</keyname><forenames>Torbj&#xf6;rn</forenames></author><author><keyname>Wielemaker</keyname><forenames>Jan</forenames></author></authors><title>Pengines: Web Logic Programming Made Easy</title><categories>cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming</comments><doi>10.1017/S1471068414000192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When developing a (web) interface for a deductive database, functionality
required by the client is provided by means of HTTP handlers that wrap the
logical data access predicates. These handlers are responsible for converting
between client and server data representations and typically include options
for paginating results. Designing the web accessible API is difficult because
it is hard to predict the exact requirements of clients. Pengines changes this
picture. The client provides a Prolog program that selects the required data by
accessing the logical API of the server. The pengine infrastructure provides
general mechanisms for converting Prolog data and handling Prolog
non-determinism. The Pengines library is small (2000 lines Prolog, 150 lines
JavaScript). It greatly simplifies defining an AJAX based client for a Prolog
program and provides non-deterministic RPC between Prolog processes as well as
interaction with Prolog engines similar to Paul Tarau's engines. Pengines are
available as a standard package for SWI-Prolog 7.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3955</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3955</id><created>2014-05-14</created><authors><author><keyname>Majkic</keyname><forenames>Zoran</forenames></author></authors><title>Saturation of the morphisms in the database category</title><categories>cs.LO</categories><comments>17 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the problem of saturation of a given morphism in the
database category DB, which is the base category for the functiorial semantics
of the database schema mapping systems used in Data Integration theory. This
phenomena appears in the case when we are using the Second-Order
tuple-generating dependencies (SOtgd) with existentially quantified
non-built-in functions, for the database schema mappings. We provide the
algorithm of the saturation for a given morphism, which represents a mapping
between two relational databases, and show that the original morphism in DB can
be equivalently substituted by its more powerful saturated version in any
commutative diagram in DB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.3980</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.3980</id><created>2014-05-15</created><updated>2015-03-13</updated><authors><author><keyname>Mohammadi</keyname><forenames>Elaheh</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Sampling and Distortion Tradeoffs for Bandlimited Periodic Signals</title><categories>cs.IT math.IT</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the optimal sampling strategies (uniform or nonuniform) and
distortion tradeoffs for stationary Gaussian bandlimited periodic signals with
additive white Gaussian noise are studied. Unlike the previous works that
commonly consider the average distortion as the performance criterion, we
justify and use both the average and variance of distortion as the performance
criteria. To compute the optimal distortion, one needs to find the optimal
sampling locations, as well as the optimal pre-sampling filter. A complete
characterization of optimal distortion for the rates lower than half the Landau
rate is provided. It is shown that nonuniform sampling outperforms uniform
sampling. In addition, this nonuniform sampling is robust with respect to
missing sampling values. Next, for the rates higher than half the Landau rate,
we find bounds that are shown to be tight for some special cases. An extension
of the results for random discrete periodic signals is discussed, with
simulation results indicating that the intuitions from the continuous domain
carry over to the discrete domain. Sparse signals are also considered where it
is shown that uniform sampling is optimal above the Nyquist rate. Finally, we
consider a sampling/quantization scheme for compressing the signal. Here, we
show that the total distortion can be expressed as the sum of sampling and
quantization distortions. This implies a lower bound on the distortion via
Shannon's rate distortion theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4008</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4008</id><created>2014-05-15</created><authors><author><keyname>Saad</keyname><forenames>Aya</forenames></author></authors><title>CDF-Intervals: A Reliable Framework to Reason about Data with
  Uncertainty</title><categories>cs.LO cs.AI</categories><comments>10 pages, 15 Postscript figures, uses new_tlp.cls and acmtrans.bst,
  full version of a paper accepted to be presented at the Doctoral Consortium
  of the 30th International Conference on Logic Programming (ICLP 2014), July
  19-22, Vienna, Austria. arXiv admin note: substantial text overlap with
  arXiv:1405.2801</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research introduces a new constraint domain for reasoning about data
with uncertainty. It extends convex modeling with the notion of p-box to gain
additional quantifiable information on the data whereabouts. Unlike existing
approaches, the p-box envelops an unknown probability instead of approximating
its representation. The p-box bounds are uniform cumulative distribution
functions (cdf) in order to employ linear computations in the probabilistic
domain. The reasoning by means of p-box cdf-intervals is an interval
computation which is exerted on the real domain then it is projected onto the
cdf domain. This operation conveys additional knowledge represented by the
obtained probabilistic bounds. Empirical evaluation shows that, with minimal
overhead, the output solution set realizes a full enclosure of the data along
with tighter bounds on its probabilistic distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4013</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4013</id><created>2014-05-15</created><authors><author><keyname>Bhardwaj</keyname><forenames>Anurag</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Di</keyname><forenames>Wei</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author><author><keyname>Churchill</keyname><forenames>Elizabeth</forenames></author></authors><title>Enhancing Visual Fashion Recommendations with Users in the Loop</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a completely automated large scale visual recommendation system
for fashion. Existing approaches have primarily relied on purely computational
models to solving this problem that ignore the role of users in the system. In
this paper, we propose to overcome this limitation by incorporating a
user-centric design of visual fashion recommendations. Specifically, we propose
a technique that augments 'user preferences' in models by exploiting elasticity
in fashion choices. We further design a user study on these choices and gather
results from the 'wisdom of crowd' for deeper analysis. Our key insights learnt
through these results suggest that fashion preferences when constrained to a
particular class, contain important behavioral signals that are often ignored
in recommendation design. Further, presence of such classes also reflect strong
correlations to visual perception which can be utilized to provide
aesthetically pleasing user experiences. Finally, we illustrate that user
approval of visual fashion recommendations can be substantially improved by
carefully incorporating these user-centric feedback into the system framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4021</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4021</id><created>2014-05-15</created><authors><author><keyname>Brass</keyname><forenames>Stefan</forenames></author></authors><title>A Framework for Bottom-Up Simulation of SLD-Resolution</title><categories>cs.LO</categories><comments>ICLP 2014 Technical Communication. To appear in Theory and Practice
  of Logic Programming (TPLP). 10 pages</comments><acm-class>D.1.6; I.2.3; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a framework for the bottom-up simulation of
SLD-resolution based on partial evaluation. The main idea is to use database
facts to represent a set of SLD goals. For deductive databases it is natural to
assume that the rules defining derived predicates are known at &quot;compile time&quot;,
whereas the database predicates are known only later at runtime. The framework
is inspired by the author's own SLDMagic method, and a variant of Earley
deduction recently introduced by Heike Stephan and the author. However, it
opens a much broader perspective. [To appear in Theory and Practice of Logic
Programming (TPLP)]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4027</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4027</id><created>2014-05-15</created><authors><author><keyname>Kimmett</keyname><forenames>Ben</forenames></author><author><keyname>Thomo</keyname><forenames>Alex</forenames></author><author><keyname>Venkatesh</keyname><forenames>S.</forenames></author></authors><title>Three-Way Joins on MapReduce: An Experimental Study</title><categories>cs.DB cs.DC</categories><comments>6 pages</comments><msc-class>68W15</msc-class><acm-class>H.2.4; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study three-way joins on MapReduce. Joins are very useful in a multitude
of applications from data integration and traversing social networks, to mining
graphs and automata-based constructions. However, joins are expensive, even for
moderate data sets; we need efficient algorithms to perform distributed
computation of joins using clusters of many machines. MapReduce has become an
increasingly popular distributed computing system and programming paradigm. We
consider a state-of-the-art MapReduce multi-way join algorithm by Afrati and
Ullman and show when it is appropriate for use on very large data sets. By
providing a detailed experimental study, we demonstrate that this algorithm
scales much better than what is suggested by the original paper. However, if
the join result needs to be summarized or aggregated, as opposed to being only
enumerated, then the aggregation step can be integrated into a cascade of
two-way joins, making it more efficient than the other algorithm, and thus
becomes the preferred solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4028</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4028</id><created>2014-05-15</created><updated>2014-05-25</updated><authors><author><keyname>Komuravelli</keyname><forenames>Anvesh</forenames></author><author><keyname>Gurfinkel</keyname><forenames>Arie</forenames></author><author><keyname>Chaki</keyname><forenames>Sagar</forenames></author></authors><title>SMT-based Model Checking for Recursive Programs</title><categories>cs.LO</categories><comments>originally published as part of the proceedings of CAV 2014; fixed
  typos, better wording at some places</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an SMT-based symbolic model checking algorithm for safety
verification of recursive programs. The algorithm is modular and analyzes
procedures individually. Unlike other SMT-based approaches, it maintains both
&quot;over-&quot; and &quot;under-approximations&quot; of procedure summaries. Under-approximations
are used to analyze procedure calls without inlining. Over-approximations are
used to block infeasible counterexamples and detect convergence to a proof. We
show that for programs and properties over a decidable theory, the algorithm is
guaranteed to find a counterexample, if one exists. However, efficiency depends
on an oracle for quantifier elimination (QE). For Boolean Programs, the
algorithm is a polynomial decision procedure, matching the worst-case bounds of
the best BDD-based algorithms. For Linear Arithmetic (integers and rationals),
we give an efficient instantiation of the algorithm by applying QE &quot;lazily&quot;. We
use existing interpolation techniques to over-approximate QE and introduce
&quot;Model Based Projection&quot; to under-approximate QE. Empirical evaluation on
SV-COMP benchmarks shows that our algorithm improves significantly on the
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4034</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4034</id><created>2014-05-15</created><authors><author><keyname>Khan-Afshar</keyname><forenames>Sanaz</forenames></author><author><keyname>Aravantinos</keyname><forenames>Vincent</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author><author><keyname>Tahar</keyname><forenames>Sofiene</forenames></author></authors><title>Formalization of Complex Vectors in Higher-Order Logic</title><categories>cs.LO</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex vector analysis is widely used to analyze continuous systems in many
disciplines, including physics and engineering. In this paper, we present a
higher-order-logic formalization of the complex vector space to facilitate
conducting this analysis within the sound core of a theorem prover: HOL Light.
Our definition of complex vector builds upon the definitions of complex numbers
and real vectors. This extension allows us to extensively benefit from the
already verified theorems based on complex analysis and real vector analysis.
To show the practical usefulness of our library we adopt it to formalize
electromagnetic fields and to prove the law of reflection for the planar waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4041</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4041</id><created>2014-05-15</created><authors><author><keyname>Jackson</keyname><forenames>Ethan K.</forenames></author></authors><title>A Module System for Domain-Specific Languages</title><categories>cs.PL</categories><comments>Appearing in International Conference on Logic Programming (ICLP)
  2014</comments><doi>10.1017/S1471068414000337</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain-specific languages (DSLs) are routinely created to simplify difficult
or specialized programming tasks. They expose useful abstractions and design
patterns in the form of language constructs, provide static semantics to
eagerly detect misuse of these constructs, and dynamic semantics to completely
define how language constructs interact. However, implementing and composing
DSLs is a non-trivial task, and there is a lack of tools and techniques.
  We address this problem by presenting a complete module system over LP for
DSL construction, reuse, and composition. LP is already useful for DSL design,
because it supports executable language specifications using notations familiar
to language designers. We extend LP with a module system that is simple (with a
few concepts), succinct (for key DSL specification scenarios), and composable
(on the level of languages, compilers, and programs). These design choices
reflect our use of LP for industrial DSL design. Our module system has been
implemented in the FORMULA language, and was used to build key Windows 8 device
drivers via DSLs. Though we present our module system as it actually appears in
our FORMULA language, our emphasis is on concepts adaptable to other LP
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4044</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4044</id><created>2014-05-15</created><authors><author><keyname>Guar&#xed;n-Zapata</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Juan</forenames></author><author><keyname>Jaramillo</keyname><forenames>Juan</forenames></author></authors><title>Seismic Wave Scattering Through a Compressed Hybrid BEM/FEM Method</title><categories>cs.CE math.NA physics.comp-ph</categories><comments>19 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Approximated numerical techniques, for the solution of the elastic wave
scattering problem over semi-infinite domains are reviewed. The approximations
involve the representation of the half-space by a boundary condition described
in terms of 2D boundary element discretizations. The classical BEM matrices are
initially re-written into the form of a dense dynamic stiffness matrix and
later approximated to a banded matrix. The resulting final banded matrix is
then used like a standard finite element to solve the wave scattering problem
at lower memory requirements. The accuracy of the reviewed methods is
benchmarked against the classical problems of a semi-circular and a rectangular
canyon. Results are presented in the time and frequency domain, as well as in
terms of relative errors in the considered approximations. The main goal of the
paper is to give the analyst a method that can be used at the practising level
where an approximate solution is enough in order to support engineering
decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4047</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4047</id><created>2014-05-15</created><updated>2014-10-01</updated><authors><author><keyname>Ustun</keyname><forenames>Berk</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Methods and Models for Interpretable Linear Classification</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an integer programming framework to build accurate and
interpretable discrete linear classification models. Unlike existing
approaches, our framework is designed to provide practitioners with the control
and flexibility they need to tailor accurate and interpretable models for a
domain of choice. To this end, our framework can produce models that are fully
optimized for accuracy, by minimizing the 0--1 classification loss, and that
address multiple aspects of interpretability, by incorporating a range of
discrete constraints and penalty functions. We use our framework to produce
models that are difficult to create with existing methods, such as scoring
systems and M-of-N rule tables. In addition, we propose specially designed
optimization methods to improve the scalability of our framework through
decomposition and data reduction. We show that discrete linear classifiers can
attain the training accuracy of any other linear classifier, and provide an
Occam's Razor type argument as to why the use of small discrete coefficients
can provide better generalization. We demonstrate the performance and
flexibility of our framework through numerical experiments and a case study in
which we construct a highly tailored clinical tool for sleep apnea diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4051</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4051</id><created>2014-05-15</created><updated>2015-06-25</updated><authors><author><keyname>Gong</keyname><forenames>Shi-nan</forenames></author><author><keyname>Chen</keyname><forenames>Duan-bing</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Guan-nan</forenames></author><author><keyname>Wang</keyname><forenames>Liang-wei</forenames></author></authors><title>An approximation algorithm for shortest path based on the hierarchy
  networks</title><categories>cs.SI physics.soc-ph</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a critical issue to compute the shortest paths between nodes in
networks. Exact algorithms for shortest paths are usually inapplicable for
large scale networks due to the high computational complexity. In this paper,
we propose a novel algorithm that is applicable for large networks with high
efficiency and accuracy. The basic idea of our algorithm is to iteratively
construct higher level hierarchy networks by condensing the central nodes and
their neighbors into super nodes until the scale of the top level network is
very small. Then the algorithm approximates the distances of the shortest paths
in the original network with the help of super nodes in the higher level
hierarchy networks. The experiment results show that our algorithm achieves
both good efficiency and high accuracy compared with other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4053</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4053</id><created>2014-05-16</created><updated>2014-05-22</updated><authors><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author></authors><title>Distributed Representations of Sentences and Documents</title><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning algorithms require the input to be represented as a
fixed-length feature vector. When it comes to texts, one of the most common
fixed-length features is bag-of-words. Despite their popularity, bag-of-words
features have two major weaknesses: they lose the ordering of the words and
they also ignore semantics of the words. For example, &quot;powerful,&quot; &quot;strong&quot; and
&quot;Paris&quot; are equally distant. In this paper, we propose Paragraph Vector, an
unsupervised algorithm that learns fixed-length feature representations from
variable-length pieces of texts, such as sentences, paragraphs, and documents.
Our algorithm represents each document by a dense vector which is trained to
predict words in the document. Its construction gives our algorithm the
potential to overcome the weaknesses of bag-of-words models. Empirical results
show that Paragraph Vectors outperform bag-of-words models as well as other
techniques for text representations. Finally, we achieve new state-of-the-art
results on several text classification and sentiment analysis tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4054</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4054</id><created>2014-05-15</created><authors><author><keyname>Wang</keyname><forenames>Jianfeng</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Song</keyname><forenames>Jingkuan</forenames></author><author><keyname>Xu</keyname><forenames>Xin-Shun</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author></authors><title>Optimized Cartesian $K$-Means</title><categories>cs.CV</categories><comments>to appear in IEEE TKDE, accepted in Apr. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product quantization-based approaches are effective to encode
high-dimensional data points for approximate nearest neighbor search. The space
is decomposed into a Cartesian product of low-dimensional subspaces, each of
which generates a sub codebook. Data points are encoded as compact binary codes
using these sub codebooks, and the distance between two data points can be
approximated efficiently from their codes by the precomputed lookup tables.
Traditionally, to encode a subvector of a data point in a subspace, only one
sub codeword in the corresponding sub codebook is selected, which may impose
strict restrictions on the search accuracy. In this paper, we propose a novel
approach, named Optimized Cartesian $K$-Means (OCKM), to better encode the data
points for more accurate approximate nearest neighbor search. In OCKM, multiple
sub codewords are used to encode the subvector of a data point in a subspace.
Each sub codeword stems from different sub codebooks in each subspace, which
are optimally generated with regards to the minimization of the distortion
errors. The high-dimensional data point is then encoded as the concatenation of
the indices of multiple sub codewords from all the subspaces. This can provide
more flexibility and lower distortion errors than traditional methods.
Experimental results on the standard real-life datasets demonstrate the
superiority over state-of-the-art approaches for approximate nearest neighbor
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4070</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4070</id><created>2014-05-16</created><authors><author><keyname>Pavlic</keyname><forenames>Theodore P.</forenames></author><author><keyname>Adams</keyname><forenames>Alyssa M.</forenames></author><author><keyname>Davies</keyname><forenames>Paul C. W.</forenames></author><author><keyname>Walker</keyname><forenames>Sara Imari</forenames></author></authors><title>Self-referencing cellular automata: A model of the evolution of
  information control in biological systems</title><categories>nlin.CG cs.FL nlin.CD nlin.PS q-bio.PE</categories><comments>Accepted to ALIFE 2014. 8 pages, 9 figures (20 subfigures), 2 tables</comments><msc-class>03D10, 18B20, 20M35, 37B15, 37F99, 68Q70, 68Q05, 68Q80,</msc-class><acm-class>F.1.1; G.2.1</acm-class><doi>10.7551/978-0-262-32621-6-ch083</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cellular automata have been useful artificial models for exploring how
relatively simple rules combined with spatial memory can give rise to complex
emergent patterns. Moreover, studying the dynamics of how rules emerge under
artificial selection for function has recently become a powerful tool for
understanding how evolution can innovate within its genetic rule space.
However, conventional cellular automata lack the kind of state feedback that is
surely present in natural evolving systems. Each new generation of a population
leaves an indelible mark on its environment and thus affects the selective
pressures that shape future generations of that population. To model this
phenomenon, we have augmented traditional cellular automata with
state-dependent feedback. Rather than generating automata executions from an
initial condition and a static rule, we introduce mappings which generate
iteration rules from the cellular automaton itself. We show that these new
automata contain disconnected regions which locally act like conventional
automata, thus encapsulating multiple functions into one structure.
Consequently, we have provided a new model for processes like cell
differentiation. Finally, by studying the size of these regions, we provide
additional evidence that the dynamics of self-reference may be critical to
understanding the evolution of natural language. In particular, the rules of
elementary cellular automata appear to be distributed in the same way as words
in the corpus of a natural language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4085</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4085</id><created>2014-05-16</created><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author></authors><title>On the Topology Maintenance of Dynamic P2P Overlays through Self-Healing
  Local Interactions</title><categories>cs.NI cs.DC</categories><comments>A revised version of the paper appears in Proc. of the IFIP
  Networking 2014 Conference, IEEE, Trondheim, (Norway), June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the use of self-organizing protocols to improve the
reliability of dynamic Peer-to-Peer (P2P) overlay networks. We present two
approaches, that employ local knowledge of the 2nd neighborhood of nodes. The
first scheme is a simple protocol requiring interactions among nodes and their
direct neighbors. The second scheme extends this approach by resorting to the
Edge Clustering Coefficient (ECC), a local measure that allows to identify
those edges that connect different clusters in an overlay. A simulation
assessment is presented, which evaluates these protocols over uniform networks,
clustered networks and scale-free networks. Different failure modes are
considered. Results demonstrate the viability of the proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4092</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4092</id><created>2014-05-16</created><authors><author><keyname>Alexander</keyname><forenames>Rukshan</forenames></author><author><keyname>Alexander</keyname><forenames>Miroshan</forenames></author></authors><title>An ICT-Based Real-Time Surveillance System for Controlling Dengue in Sri
  Lanka</title><categories>cs.CY</categories><comments>10 pages, 7 figures, 1 tables, Proceedings of the International
  Conference on Contemporary Management (ICCM), 2014, Faculty of Management
  Studies &amp; Commerce, University of Jaffna, Sri Lanka. 14th &amp; 15rd March, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dengue is a notifiable communicable disease in Sri Lanka since 1996. Dengue
fever spread rapidly among people living in most of the districts of Sri Lanka.
The present notification system of dengue communicable diseases which is
enforced by law is a passive surveillance system carried out by the public
health care professionals. The present notification of communicable disease
system is manual, slow, inefficient, and repetitive all of these lead to handle
the dengue related health problems ineffectively. Thus it is less effective in
preventing a spreading epidemic, public health care professionals and others
require an operational support system to help for managing day-to-day public
health responsibilities as well as a method to effectively detect and manage
health problems such as Dengue. On the other hand the Information and
Communication Technology (ICT) in medical world has been widely used. To give
the information technology touch, a complementary web based open source
software application environment has been developed with minimum implementation
and recurrent costs critical for developing countries like Sri Lanka and named
as eDCS: e Dengue Control System based on the same principles of manual disease
surveillance system while taking steps to provide timely, accurate information
in a reliable and useable manner. The eDCS helps to manage outbreaks through
early detection, rapid verification, and appropriate response to Dengue. It
allows health care professionals and citizens to get early awareness about the
dengue disease via Internet or mobile phone and bring them for performing
Dengue prevention and controlling operation through the social media
acceleration. The system is initially limited to dengue communicable disease.
It can be easily expanded to other communicable diseases, and non communicable
disease surveillance in future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4095</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4095</id><created>2014-05-16</created><authors><author><keyname>Zhu</keyname><forenames>Xuzhen</forenames></author><author><keyname>Tian</keyname><forenames>Hui</forenames></author><author><keyname>Cai</keyname><forenames>Shimin</forenames></author></authors><title>Personalized recommendation with corrected similarity</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>13 pages, 2 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:0805.4127 by other authors</comments><doi>10.1088/1742-5468/2014/07/P07004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalized recommendation attracts a surge of interdisciplinary researches.
Especially, similarity based methods in applications of real recommendation
systems achieve great success. However, the computations of similarities are
overestimated or underestimated outstandingly due to the defective strategy of
unidirectional similarity estimation. In this paper, we solve this drawback by
leveraging mutual correction of forward and backward similarity estimations,
and propose a new personalized recommendation index, i.e., corrected similarity
based inference (CSI). Through extensive experiments on four benchmark
datasets, the results show a greater improvement of CSI in comparison with
these mainstream baselines. And the detailed analysis is presented to unveil
and understand the origin of such difference between CSI and mainstream
indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4097</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4097</id><created>2014-05-16</created><updated>2014-07-17</updated><authors><author><keyname>Ban</keyname><forenames>Kristina</forenames></author><author><keyname>Ivaki&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author></authors><title>A preliminary study of Croatian Language Syllable Networks</title><categories>cs.CL</categories><comments>in Proceedings MIPRO junior - Student Papers</comments><journal-ref>IEEE 36th International Convention on Information and
  Communication Technology, Electronics and Microelectronics (MIPRO 2013), pp.
  1004-1008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents preliminary results of Croatian syllable networks
analysis. Syllable network is a network in which nodes are syllables and links
between them are constructed according to their connections within words. In
this paper we analyze networks of syllables generated from texts collected from
the Croatian Wikipedia and Blogs. As a main tool we use complex network
analysis methods which provide mechanisms that can reveal new patterns in a
language structure. We aim to show that syllable networks have much higher
clustering coefficient in comparison to Erd\&quot;os-Renyi random networks. The
results indicate that Croatian syllable networks exhibit certain properties of
a small world networks. Furthermore, we compared Croatian syllable networks
with Portuguese and Chinese syllable networks and we showed that they have
similar properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4098</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4098</id><created>2014-05-16</created><updated>2014-07-15</updated><authors><author><keyname>Cohen</keyname><forenames>Kobi</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author></authors><title>Optimal Index Policies for Anomaly Localization in Resource-Constrained
  Cyber Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>30 pages, 4 figures, accepted for publication in the IEEE
  Transactions on Signal Processing, part of this work was presented at the
  IEEE GlobalSIP 2013</comments><doi>10.1109/TSP.2014.2332982</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of anomaly localization in a resource-constrained cyber system is
considered. Each anomalous component of the system incurs a cost per unit time
until its anomaly is identified and fixed. Different anomalous components may
incur different costs depending on their criticality to the system. Due to
resource constraints, only one component can be probed at each given time. The
observations from a probed component are realizations drawn from two different
distributions depending on whether the component is normal or anomalous. The
objective is a probing strategy that minimizes the total expected cost,
incurred by all the components during the detection process, under reliability
constraints. We consider both independent and exclusive models. In the former,
each component can be abnormal with a certain probability independent of other
components. In the latter, one and only one component is abnormal. We develop
optimal simple index policies under both models. The proposed index policies
apply to a more general case where a subset (more than one) of the components
can be probed simultaneously and have strong performance as demonstrated by
simulation examples. The problem under study also finds applications in
spectrum scanning in cognitive radio networks and event detection in sensor
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4100</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4100</id><created>2014-05-16</created><authors><author><keyname>Prisacariu</keyname><forenames>Cristian</forenames></author></authors><title>Higher Dimensional Modal Logic</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Higher dimensional automata (HDA) are a model of concurrency that can express
most of the traditional partial order models like Mazurkiewicz traces, pomsets,
event structures, or Petri nets. Modal logics, interpreted over Kripke
structures, are the logics for reasoning about sequential behavior and
interleaved concurrency. Modal logic is a well behaved subset of first-order
logic; many variants of modal logic are decidable. However, there are no
modal-like logics for the more expressive HDA models. In this paper we
introduce and investigate a modal logic over HDAs which incorporates two
modalities for reasoning about &quot;during&quot; and &quot;after&quot;. We prove that this general
higher dimensional modal logic (HDML) is decidable and we define an axiomatic
system for it. We also show how, when the HDA model is restricted to Kripke
structures, a syntactic restriction of HDML becomes the standard modal logic.
Then we isolate the class of HDAs that encode Mazurkiewicz traces and show how
HDML, with natural definitions of corresponding Until operators, can be
restricted to LTrL (the linear time temporal logic over Mazurkiewicz traces) or
the branching time ISTL. We also study the expressiveness of the basic HDML
language wrt. bisimulations and conclude that HDML captures the
split-bisimulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4118</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4118</id><created>2014-05-16</created><authors><author><keyname>Gupta</keyname><forenames>Shikhar Kumar</forenames></author><author><keyname>Joshi</keyname><forenames>Foram</forenames></author><author><keyname>Limbachiya</keyname><forenames>Dixita</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K</forenames></author></authors><title>3DNA: A Tool for DNA Sculpting</title><categories>cs.ET</categories><comments>5 pages, 11 figures, appeared as a poster paper in FNANO 2014
  conference, Software available at http://www.guptalab.org/3dna</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA self-assembly is a robust and programmable approach for building
structures at nanoscale. Researchers around the world have proposed and
implemented different techniques to build two dimensional and three dimensional
nano structures. One such technique involves the implementation of DNA Bricks
proposed by Ke et al., 2012 to create complex three-dimensional (3D)
structures. Modeling these DNA nano structures can prove to be a cumbersome and
tedious task. Exploiting the programmability of base-pairing to produce
self-assembling custom shapes, we present a software suite 3DNA, which can be
used for modeling, editing and visualizing such complex structures. 3DNA is an
open source software which works on the simple and modular self assembly of DNA
Bricks, offering a more intuitive better approach for constructing 3D shapes.
Apart from modeling and envisaging shapes through a simple graphical user
interface, 3DNA also supports an integrated random sequence generator that
generates DNA sequences corresponding to the designed model. The software is
available at www.guptalab.org/3dna
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4120</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4120</id><created>2014-05-16</created><authors><author><keyname>Utkovski</keyname><forenames>Zoran</forenames></author><author><keyname>Gajduk</keyname><forenames>Andrej</forenames></author><author><keyname>Basnarkov</keyname><forenames>Lasko</forenames></author><author><keyname>Bosnakovski</keyname><forenames>Darko</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>On Energy-efficiency in Wireless Networks: A Game-theoretic Approach to
  Cooperation Inspired by Evolutionary Biology</title><categories>cs.NI</categories><comments>This work has been submitted to the IEEE/ACM Transactions on
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a game-theoretic framework to investigate the effect of
cooperation on the energy efficiency in wireless networks. We address two
examples of network architectures, resembling ad-hoc network and network with
central infrastructure node. Most present approaches address the issue of
energy efficiency in communication networks by using complex algorithms to
enforce cooperation in the network, followed by extensive signal processing at
the network nodes. Instead, we address cooperative communication scenarios
which are governed by simple, evolutionary-like, local rules, and do not
require strategic complexity of the network nodes. The approach is motivated by
recent results in evolutionary biology which suggest that cooperation can
emerge in Nature by evolution, i. e. can be favoured by natural selection, if
certain mechanism is at work. As result, we are able to show by experiments
that cooperative behavior can indeed emerge and persist in wireless networks,
even if the behavior of the individual nodes is driven by selfish decision
making. The results from this work indicate that uncomplicated local rules,
followed by simple fitness evaluation, can promote cooperation and generate
network behavior which yields global energy efficiency in certain wireless
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4127</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4127</id><created>2014-05-16</created><authors><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Coded Random Access: How Coding Theory Helps to Build Random Access
  Protocols</title><categories>cs.NI cs.IT math.IT</categories><comments>9 pages, 4 figures; submitted to IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of machine-to-machine communications has rekindled the interest in
random access protocols and their use to support a massive number of
uncoordinatedly transmitting devices. The classic ALOHA approach is developed
under a collision model, where slots that contain collided packets are
considered as waste. However, if the common receiver (e.g., base station) is
capable to store the collision slots and use them in a transmission recovery
process based on successive interference cancellation, the design space for
access protocols is radically expanded. We present the paradigm of coded random
access, in which the structure of the access protocol can be mapped to a
structure of an erasure-correcting code defined on graph. This opens the
possibility to use coding theory and tools for designing efficient random
access protocols, offering markedly better performance than ALOHA. Several
instances of coded random access protocols are described, as well as a case
study on how to upgrade a legacy ALOHA wireless system using the ideas of coded
random access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4138</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4138</id><created>2014-05-16</created><authors><author><keyname>Azizi</keyname><forenames>Reza</forenames></author></authors><title>Empirical Study of Artificial Fish Swarm Algorithm</title><categories>cs.AI</categories><journal-ref>International Journal of Computing, Communications and Networking
  (IJCCN) , Volume 3, No.1, Pages 01-07, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial fish swarm algorithm (AFSA) is one of the swarm intelligence
optimization algorithms that works based on population and stochastic search.
In order to achieve acceptable result, there are many parameters needs to be
adjusted in AFSA. Among these parameters, visual and step are very significant
in view of the fact that artificial fish basically move based on these
parameters. In standard AFSA, these two parameters remain constant until the
algorithm termination. Large values of these parameters increase the capability
of algorithm in global search, while small values improve the local search
ability of the algorithm. In this paper, we empirically study the performance
of the AFSA and different approaches to balance between local and global
exploration have been tested based on the adaptive modification of visual and
step during algorithm execution. The proposed approaches have been evaluated
based on the four well-known benchmark functions. Experimental results show
considerable positive impact on the performance of AFSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4180</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4180</id><created>2014-05-16</created><authors><author><keyname>Chang</keyname><forenames>Liang</forenames></author><author><keyname>Sattler</keyname><forenames>Uli</forenames></author><author><keyname>Gu</keyname><forenames>Tianlong</forenames></author></authors><title>Algorithm for Adapting Cases Represented in a Tractable Description
  Logic</title><categories>cs.AI</categories><comments>21 pages. ICCBR 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Case-based reasoning (CBR) based on description logics (DLs) has gained a lot
of attention lately. Adaptation is a basic task in the CBR inference that can
be modeled as the knowledge base revision problem and solved in propositional
logic. However, in DLs, it is still a challenge problem since existing revision
operators only work well for strictly restricted DLs of the \emph{DL-Lite}
family, and it is difficult to design a revision algorithm which is
syntax-independent and fine-grained. In this paper, we present a new method for
adaptation based on the DL $\mathcal{EL_{\bot}}$. Following the idea of
adaptation as revision, we firstly extend the logical basis for describing
cases from propositional logic to the DL $\mathcal{EL_{\bot}}$, and present a
formalism for adaptation based on $\mathcal{EL_{\bot}}$. Then we present an
adaptation algorithm for this formalism and demonstrate that our algorithm is
syntax-independent and fine-grained. Our work provides a logical basis for
adaptation in CBR systems where cases and domain knowledge are described by the
tractable DL $\mathcal{EL_{\bot}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4189</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4189</id><created>2014-05-16</created><authors><author><keyname>Heizmann</keyname><forenames>Matthias</forenames></author><author><keyname>Hoenicke</keyname><forenames>Jochen</forenames></author><author><keyname>Podelski</keyname><forenames>Andreas</forenames></author></authors><title>Termination Analysis by Learning Terminating Programs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to termination analysis. In a first step, the
analysis uses a program as a black-box which exhibits only a finite set of
sample traces. Each sample trace is infinite but can be represented by a finite
lasso. The analysis can &quot;learn&quot; a program from a termination proof for the
lasso, a program that is terminating by construction. In a second step, the
analysis checks that the set of sample traces is representative in a sense that
we can make formal. An experimental evaluation indicates that the approach is a
potentially useful addition to the portfolio of existing approaches to
termination analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4197</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4197</id><created>2014-05-13</created><authors><author><keyname>Sharma</keyname><forenames>Parul</forenames></author><author><keyname>Girdhar</keyname><forenames>Dr. Anup</forenames></author></authors><title>Security concerns of ipv6-slaac and security policies to diminish the
  risk associated with them</title><categories>cs.CR</categories><comments>Paper presented at National Conference IPGCon-2014, Published by
  Cyber Times International Journal of Technology &amp; Management</comments><journal-ref>Cyber Times International Journal of Technology &amp; Management, Vol
  7, Issue 1, March 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Though, IPv6 has presented a long list of features that will actually change
the entire networking environment, it still represents a very small proportion
of Internet traffic. One of the major reasons behind its partial acceptance is
its security, as these features were incorporated in IPv6 just to enhance its
overall performance and quality of service, thereby neglecting its security
part. One such feature is Stateless Address Auto Configuration or simply SLAAC,
which have undoubtedly reduced a lot of overhead in configuring networks, but
on the same time it has challenged the security of IPv6 networks. This research
work, therefore targets at proposing some security policies to mitigate the
risks associated with SLAAC, by understanding its working scenario and trying
to create the possible attack vector to highlight its security concerns. Hence,
the implementation of these policies will directly add security to networks and
make them resistant to SLAAC based attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4200</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4200</id><created>2014-05-16</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Pa&#x161;kauskas</keyname><forenames>Rytis</forenames></author></authors><title>Mean-Field approximation and Quasi-Equilibrium reduction of Markov
  Population Models</title><categories>cs.SY cs.PF q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Population Model is a commonly used framework to describe stochastic
systems. Their exact analysis is unfeasible in most cases because of the state
space explosion. Approximations are usually sought, often with the goal of
reducing the number of variables. Among them, the mean field limit and the
quasi-equilibrium approximations stand out. We view them as techniques that are
rooted in independent basic principles. At the basis of the mean field limit is
the law of large numbers. The principle of the quasi-equilibrium reduction is
the separation of temporal scales. It is common practice to apply both limits
to an MPM yielding a fully reduced model. Although the two limits should be
viewed as completely independent options, they are applied almost invariably in
a fixed sequence: MF limit first, QE-reduction second. We present a framework
that makes explicit the distinction of the two reductions, and allows an
arbitrary order of their application. By inverting the sequence, we show that
the double limit does not commute in general: the mean field limit of a
time-scale reduced model is not the same as the time-scale reduced limit of a
mean field model. An example is provided to demonstrate this phenomenon.
Sufficient conditions for the two operations to be freely exchangeable are also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4201</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4201</id><created>2014-05-16</created><updated>2014-05-29</updated><authors><author><keyname>Polania</keyname><forenames>Luisa F.</forenames></author><author><keyname>Carrillo</keyname><forenames>Rafael E.</forenames></author><author><keyname>Blanco-Velasco</keyname><forenames>Manuel</forenames></author><author><keyname>Barner</keyname><forenames>Kenneth E.</forenames></author></authors><title>Exploiting Prior Knowledge in Compressed Sensing Wireless ECG Systems</title><categories>cs.CE</categories><comments>Accepted for publication at IEEE Journal of Biomedical and Health
  Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results in telecardiology show that compressed sensing (CS) is a
promising tool to lower energy consumption in wireless body area networks for
electrocardiogram (ECG) monitoring. However, the performance of current
CS-based algorithms, in terms of compression rate and reconstruction quality of
the ECG, still falls short of the performance attained by state-of-the-art
wavelet based algorithms. In this paper, we propose to exploit the structure of
the wavelet representation of the ECG signal to boost the performance of
CS-based methods for compression and reconstruction of ECG signals. More
precisely, we incorporate prior information about the wavelet dependencies
across scales into the reconstruction algorithms and exploit the high fraction
of common support of the wavelet coefficients of consecutive ECG segments.
Experimental results utilizing the MIT-BIH Arrhythmia Database show that
significant performance gains, in terms of compression rate and reconstruction
quality, can be obtained by the proposed algorithms compared to current
CS-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4206</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4206</id><created>2014-05-16</created><authors><author><keyname>Jansen</keyname><forenames>Joachim</forenames></author></authors><title>Model revision inference for extensions of first order logic</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I am Joachim Jansen and this is my research summary, part of my application
to the Doctoral Consortium at ICLP'14. I am a PhD student in the Knowledge
Representation and Reasoning (KRR) research group, a subgroup of the
Declarative Languages and Artificial Intelligence (DTAI) group at the
department of Computer Science at KU Leuven. I started my PhD in September
2012. My promotor is prof. dr. ir. Gerda Janssens and my co-promotor is prof.
dr. Marc Denecker. I can be contacted at joachim.jansen@cs.kuleuven.be or at:
Room 01.167 Celestijnenlaan 200A 3001 Heverlee Belgium An extended abstract /
full version of a paper accepted to be presented at the Doctoral Consortium of
the 30th International Conference on Logic Programming (ICLP 2014), July 19-22,
Vienna, Austria
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4211</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4211</id><created>2014-05-16</created><authors><author><keyname>Fish</keyname><forenames>Andrew</forenames></author><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author></authors><title>Detecting unknots via equational reasoning, I: Exploration</title><categories>cs.LO cs.DM math.GT</categories><comments>To appear in Proceedings of CICM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the application of automated reasoning techniques to unknot
detection, a classical problem of computational topology. We adopt a
two-pronged experimental approach, using a theorem prover to try to establish a
positive result (i.e. that a knot is the unknot), whilst simultaneously using a
model finder to try to establish a negative result (i.e. that the knot is not
the unknot). The theorem proving approach utilises equational reasoning, whilst
the model finder searches for a minimal size counter-model. We present and
compare experimental data using the involutary quandle of the knot, as well as
comparing with alternative approaches, highlighting instances of interest.
Furthermore, we present theoretical connections of the minimal countermodels
obtained with existing knot invariants, for all prime knots of up to 10
crossings: this may be useful for developing advanced search strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4217</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4217</id><created>2014-05-16</created><authors><author><keyname>Zhang</keyname><forenames>Qizhi</forenames></author><author><keyname>Liu</keyname><forenames>Deping</forenames></author></authors><title>On the hopping pattern design for D2D Discovery</title><categories>cs.NI math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hopping pattern for D2D Discovery are investi- gated. We propose three
metrics for hopping pattern performance evaluation: column period, maximal
collision ratio, maximal con- tinual collision number. A class of hopping
patterns is constructed based on the metrics, and through simulation the
patterns show better discovery performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4228</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4228</id><created>2014-05-16</created><updated>2014-06-28</updated><authors><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Salimi</keyname><forenames>Babak</forenames></author></authors><title>Unifying Causality, Diagnosis, Repairs and View-Updates in Databases</title><categories>cs.DB</categories><comments>On-line Proc. First International Workshop on Big Uncertain Data
  (BUDA 2014). Co-located with ACM PODS 2014. arXiv admin note: text overlap
  with arXiv:1404.6857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we establish and point out connections between the notion of
query-answer causality in databases and database repairs, model-based diagnosis
in its consistency-based and abductive versions, and database updates through
views. The mutual relationships among these areas of data management and
knowledge representation shed light on each of them and help to share notions
and results they have in common. In one way or another, these are all
approaches to uncertainty management, which becomes even more relevant in the
context of big data that have to be made sense of.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4232</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4232</id><created>2014-05-16</created><updated>2014-05-20</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author></authors><title>Architectural Design of a RAM Arbiter</title><categories>cs.AR</categories><comments>121 pages, 87 figures, 1 table. Mini Project Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard memory modules to store (and access) data are designed for use with
a single system accessing it. More complicated memory modules would be accessed
through a memory controller, which are also designed for one system. For
multiple systems to access a single memory module there must be some
facilitation that allows them to access the memory without overriding or
corrupting the access from the others. This was done with the use of a memory
arbiter, which controls the flow of traffic into the memory controller. The
arbiter has a set of rules to abide to in order to choose which system gets
through to the memory controller. In this project, a regular RAM module is
designed for use with one system. Furthermore, a memory arbiter is also
designed in Verilog that allows for more than one system to use a single RAM
module in a controlled and synchronized manner. The arbiter uses a fixed
priority scheme to avoid starvation of the system. In addition one of the major
problems associated with such systems i.e. The Address Clash Problem has been
nicely tackled and solved. The design is verified in simulation and validated
on a Xilinx ML605 evaluation board with a Virtex 6 FPGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4248</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4248</id><created>2014-05-16</created><authors><author><keyname>Haralambous</keyname><forenames>Yannis</forenames></author></authors><title>Les math\'ematiques de la langue : l'approche formelle de Montague</title><categories>cs.CL</categories><comments>14 pages, in French. Will appear in the journal Quadrature
  (http://www.quadrature.info) in 2015</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a natural language modelization method which is strongely relying
on mathematics. This method, called &quot;Formal Semantics,&quot; has been initiated by
the American linguist Richard M. Montague in the 1970's. It uses mathematical
tools such as formal languages and grammars, first-order logic, type theory and
$\lambda$-calculus. Our goal is to have the reader discover both Montagovian
formal semantics and the mathematical tools that he used in his method.
  -----
  Nous pr\'esentons une m\'ethode de mod\'elisation de la langue naturelle qui
est fortement bas\'ee sur les math\'ematiques. Cette m\'ethode, appel\'ee
{\guillemotleft}s\'emantique formelle{\guillemotright}, a \'et\'e initi\'ee par
le linguiste am\'ericain Richard M. Montague dans les ann\'ees 1970. Elle
utilise des outils math\'ematiques tels que les langages et grammaires formels,
la logique du 1er ordre, la th\'eorie de types et le $\lambda$-calcul. Nous
nous proposons de faire d\'ecouvrir au lecteur tant la s\'emantique formelle de
Montague que les outils math\'ematiques dont il s'est servi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4256</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4256</id><created>2014-05-16</created><authors><author><keyname>Serrano</keyname><forenames>Alejandro</forenames></author><author><keyname>Lopez-Garcia</keyname><forenames>Pedro</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Resource Usage Analysis of Logic Programs via Abstract Interpretation
  Using Sized Types</title><categories>cs.PL cs.DC</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP),
  improved version of arXiv:1308.3940 (which was conference version)</comments><doi>10.1017/S147106841400057X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel general resource analysis for logic programs based on
sized types. Sized types are representations that incorporate structural
(shape) information and allow expressing both lower and upper bounds on the
size of a set of terms and their subterms at any position and depth. They also
allow relating the sizes of terms and subterms occurring at different argument
positions in logic predicates. Using these sized types, the resource analysis
can infer both lower and upper bounds on the resources used by all the
procedures in a program as functions on input term (and subterm) sizes,
overcoming limitations of existing resource analyses and enhancing their
precision. Our new resource analysis has been developed within the abstract
interpretation framework, as an extension of the sized types abstract domain,
and has been integrated into the Ciao preprocessor, CiaoPP. The abstract domain
operations are integrated with the setting up and solving of recurrence
equations for inferring both size and resource usage functions. We show that
the analysis is an improvement over the previous resource analysis present in
CiaoPP and compares well in power to state of the art systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4272</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4272</id><created>2014-05-16</created><updated>2014-08-01</updated><authors><author><keyname>Taheri</keyname><forenames>Mina</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Multi-power-level Energy Saving Management for Passive Optical Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environmental concerns have motivated network designers to further reduce
energy consumption of access networks. This paper focuses on reducing energy
consumption of Ethernet passive optical network (EPON) as one of the most
efficient transmission technologies for broadband access. In EPON, the
downstream traffic is sent from the optical line terminal (OLT) located at the
central office to all optical network units (ONUs). Each ONU checks all arrival
downstream packets and selects the downstream packets destined to itself.
Therefore, receivers at ONUs have to always stay awake, thus consuming a large
amount of energy. Contrariwise, an ONU transmitter can be triggered by the
arrival of the upstream traffic and so it can go to the low power mode when no
traffic is observed. Putting ONUs into the low power mode during light traffic
is a known strategy for energy saving. In this article, we address the
downstream challenge and also improve the ONU transmitter sleep time by
proposing a simple sleep control scheme. We also propose an upstream and a
downstream sleep-aware traffic scheduling scheme to avoid missing the packets
during the sleep states. The proposed sleep control scheme does not need the
handshake process and is based on the mutual inference at OLT and ONU.
Simulation results show that the proposed scheme can save energy as much as 60%
when the network traffic is light.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4273</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4273</id><created>2014-05-16</created><authors><author><keyname>Botha</keyname><forenames>Jan A.</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Compositional Morphology for Word Representations and Language Modelling</title><categories>cs.CL</categories><comments>Proceedings of the 31st International Conference on Machine Learning
  (ICML)</comments><msc-class>68T50</msc-class><acm-class>I.2.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a scalable method for integrating compositional
morphological representations into a vector-based probabilistic language model.
Our approach is evaluated in the context of log-bilinear language models,
rendered suitably efficient for implementation inside a machine translation
decoder by factoring the vocabulary. We perform both intrinsic and extrinsic
evaluations, presenting results on a range of languages which demonstrate that
our model learns morphological representations that both perform well on word
similarity tasks and lead to substantial reductions in perplexity. When used
for translation into morphologically rich languages with large vocabularies,
our models obtain improvements of up to 1.2 BLEU points relative to a baseline
system using back-off n-gram models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4286</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4286</id><created>2014-05-16</created><authors><author><keyname>Chaphekar</keyname><forenames>Pratik P.</forenames></author></authors><title>Survey of Key Distribution Schemes for Wireless Sensor Networks</title><categories>cs.CR</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a survey of key distribution schemes for wireless sensor
networks. This survey includes the new approach of key distribution using the
piggy bank method. Different Network architectures and different key
pre-distribution schemes are described. The survey includes the use of the
piggy bank approach to cryptography in which part of the key is pre-distributed
and the remainder is varied in the application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4298</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4298</id><created>2014-05-16</created><authors><author><keyname>Sornette</keyname><forenames>Didier</forenames></author><author><keyname>Maillart</keyname><forenames>Thomas</forenames></author><author><keyname>Ghezzi</keyname><forenames>Giacomo</forenames></author></authors><title>How Much is the Whole Really More than the Sum of its Parts? 1 + 1 =
  2.5: Superlinear Productivity in Collective Group Actions</title><categories>physics.soc-ph cs.SI</categories><comments>29 pages, 8 figures</comments><doi>10.1371/journal.pone.0103023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of open source software projects, we document a superlinear
growth of production ($R \sim c^\beta$) as a function of the number of active
developers $c$, with $\beta \simeq 4/3$ with large dispersions. For a typical
project in this class, doubling of the group size multiplies typically the
output by a factor $2^\beta=2.5$, explaining the title. This superlinear law is
found to hold for group sizes ranging from 5 to a few hundred developers. We
propose two classes of mechanisms, {\it interaction-based} and {\it large
deviation}, along with a cascade model of productive activity, which unifies
them. In this common framework, superlinear productivity requires that the
involved social groups function at or close to criticality, in the sense of a
subtle balance between order and disorder. We report the first empirical test
of the renormalization of the exponent of the distribution of the sizes of
first generation events into the renormalized exponent of the distribution of
clusters resulting from the cascade of triggering over all generation in a
critical branching process in the non-meanfield regime. Finally, we document a
size effect in the strength and variability of the superlinear effect, with
smaller groups exhibiting widely distributed superlinear exponents, some of
them characterizing highly productive teams. In contrast, large groups tend to
have a smaller superlinearity and less variability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4301</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4301</id><created>2014-05-16</created><authors><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Sitko</keyname><forenames>Izabela</forenames></author><author><keyname>Grauwin</keyname><forenames>Sebastian</forenames></author><author><keyname>Combes</keyname><forenames>Remi Tachet des</forenames></author><author><keyname>Hawelka</keyname><forenames>Bartosz</forenames></author><author><keyname>Arias</keyname><forenames>Juan Murillo</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Mining Urban Performance: Scale-Independent Classification of Cities
  Based on Individual Economic Transactions</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>10 pages, 7 figures, to be published in the proceedings of ASE
  BigDataScience 2014 conference</comments><msc-class>62-07, 68U01</msc-class><acm-class>H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intensive development of urban systems creates a number of challenges for
urban planners and policy makers in order to maintain sustainable growth.
Running efficient urban policies requires meaningful urban metrics, which could
quantify important urban characteristics including various aspects of an actual
human behavior. Since a city size is known to have a major, yet often
nonlinear, impact on the human activity, it also becomes important to develop
scale-free metrics that capture qualitative city properties, beyond the effects
of scale. Recent availability of extensive datasets created by human activity
involving digital technologies creates new opportunities in this area. In this
paper we propose a novel approach of city scoring and classification based on
quantitative scale-free metrics related to economic activity of city residents,
as well as domestic and foreign visitors. It is demonstrated on the example of
Spain, but the proposed methodology is of a general character. We employ a new
source of large-scale ubiquitous data, which consists of anonymized countrywide
records of bank card transactions collected by one of the largest Spanish
banks. Different aspects of the classification reveal important properties of
Spanish cities, which significantly complement the pattern that might be
discovered with the official socioeconomic statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4308</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4308</id><created>2014-05-16</created><authors><author><keyname>Liu</keyname><forenames>Meizhu</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Ye</keyname><forenames>Xiaojing</forenames></author><author><keyname>Yu</keyname><forenames>Shipeng</forenames></author></authors><title>Coarse-to-Fine Classification via Parametric and Nonparametric Models
  for Computer-Aided Diagnosis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification is one of the core problems in Computer-Aided Diagnosis (CAD),
targeting for early cancer detection using 3D medical imaging interpretation.
High detection sensitivity with desirably low false positive (FP) rate is
critical for a CAD system to be accepted as a valuable or even indispensable
tool in radiologists' workflow. Given various spurious imagery noises which
cause observation uncertainties, this remains a very challenging task. In this
paper, we propose a novel, two-tiered coarse-to-fine (CTF) classification
cascade framework to tackle this problem. We first obtain
classification-critical data samples (e.g., samples on the decision boundary)
extracted from the holistic data distributions using a robust parametric model
(e.g., \cite{Raykar08}); then we build a graph-embedding based nonparametric
classifier on sampled data, which can more accurately preserve or formulate the
complex classification boundary. These two steps can also be considered as
effective &quot;sample pruning&quot; and &quot;feature pursuing + $k$NN/template matching&quot;,
respectively. Our approach is validated comprehensively in colorectal polyp
detection and lung nodule detection CAD systems, as the top two deadly cancers,
using hospital scale, multi-site clinical datasets. The results show that our
method achieves overall better classification/detection performance than
existing state-of-the-art algorithms using single-layer classifiers, such as
the support vector machine variants \cite{Wang08}, boosting \cite{Slabaugh10},
logistic regression \cite{Ravesteijn10}, relevance vector machine
\cite{Raykar08}, $k$-nearest neighbor \cite{Murphy09} or spectral projections
on graph \cite{Cai08}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4322</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4322</id><created>2014-05-16</created><authors><author><keyname>Knoester</keyname><forenames>David B.</forenames></author><author><keyname>Goldsby</keyname><forenames>Heather J.</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>Leveraging Evolutionary Search to Discover Self-Adaptive and
  Self-Organizing Cellular Automata</title><categories>cs.NE nlin.CG</categories><comments>10 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building self-adaptive and self-organizing (SASO) systems is a challenging
problem, in part because SASO principles are not yet well understood and few
platforms exist for exploring them. Cellular automata (CA) are a well-studied
approach to exploring the principles underlying self-organization. A CA
comprises a lattice of cells whose states change over time based on a discrete
update function. One challenge to developing CA is that the relationship of an
update function, which describes the local behavior of each cell, to the global
behavior of the entire CA is often unclear. As a result, many researchers have
used stochastic search techniques, such as evolutionary algorithms, to
automatically discover update functions that produce a desired global behavior.
However, these update functions are typically defined in a way that does not
provide for self-adaptation. Here we describe an approach to discovering CA
update functions that are both self-adaptive and self-organizing. Specifically,
we use a novel evolutionary algorithm-based approach to discover finite state
machines (FSMs) that implement update functions for CA. We show how this
approach is able to evolve FSM-based update functions that perform well on the
density classification task for 1-, 2-, and 3-dimensional CA. Moreover, we show
that these FSMs are self-adaptive, self-organizing, and highly scalable, often
performing well on CA that are orders of magnitude larger than those used to
evaluate performance during the evolutionary search. These results demonstrate
that CA are a viable platform for studying the integration of self-adaptation
and self-organization, and strengthen the case for using evolutionary
algorithms as a component of SASO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4324</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4324</id><created>2014-05-16</created><authors><author><keyname>Gadde</keyname><forenames>Akshay</forenames></author><author><keyname>Anis</keyname><forenames>Aamir</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Active Semi-Supervised Learning Using Sampling Theory for Graph Signals</title><categories>cs.LG stat.ML</categories><comments>10 pages, 6 figures, To appear in KDD'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of offline, pool-based active semi-supervised
learning on graphs. This problem is important when the labeled data is scarce
and expensive whereas unlabeled data is easily available. The data points are
represented by the vertices of an undirected graph with the similarity between
them captured by the edge weights. Given a target number of nodes to label, the
goal is to choose those nodes that are most informative and then predict the
unknown labels. We propose a novel framework for this problem based on our
recent results on sampling theory for graph signals. A graph signal is a
real-valued function defined on each node of the graph. A notion of frequency
for such signals can be defined using the spectrum of the graph Laplacian
matrix. The sampling theory for graph signals aims to extend the traditional
Nyquist-Shannon sampling theory by allowing us to identify the class of graph
signals that can be reconstructed from their values on a subset of vertices.
This approach allows us to define a criterion for active learning based on
sampling set selection which aims at maximizing the frequency of the signals
that can be reconstructed from their samples on the set. Experiments show the
effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4329</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4329</id><created>2014-05-16</created><updated>2014-12-04</updated><authors><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author><author><keyname>Sharma</keyname><forenames>Rajesh</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Siyari</keyname><forenames>Payam</forenames></author><author><keyname>Montesi</keyname><forenames>Danilo</forenames></author></authors><title>Spreading processes in Multilayer Networks</title><categories>cs.SI physics.soc-ph</categories><comments>21 pages, 3 figures, 4 tables</comments><journal-ref>IEEE Transactions on Network Science and Engineering (TNSE), 2015</journal-ref><doi>10.1109/TNSE.2015.2425961</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several systems can be modeled as sets of interconnected networks or networks
with multiple types of connections, here generally called multilayer networks.
Spreading processes such as information propagation among users of an online
social networks, or the diffusion of pathogens among individuals through their
contact network, are fundamental phenomena occurring in these networks.
However, while information diffusion in single networks has received
considerable attention from various disciplines for over a decade, spreading
processes in multilayer networks is still a young research area presenting many
challenging research issues. In this paper we review the main models, results
and applications of multilayer spreading processes and discuss some promising
research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4332</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4332</id><created>2014-05-16</created><authors><author><keyname>Smith</keyname><forenames>Laura M.</forenames></author><author><keyname>Zhu</keyname><forenames>Linhong</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>Partitioning Networks with Node Attributes by Compressing Information
  Flow</title><categories>cs.SI cs.CY cs.IT math.IT physics.soc-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world networks are often organized as modules or communities of similar
nodes that serve as functional units. These networks are also rich in content,
with nodes having distinguishing features or attributes. In order to discover a
network's modular structure, it is necessary to take into account not only its
links but also node attributes. We describe an information-theoretic method
that identifies modules by compressing descriptions of information flow on a
network. Our formulation introduces node content into the description of
information flow, which we then minimize to discover groups of nodes with
similar attributes that also tend to trap the flow of information. The method
has several advantages: it is conceptually simple and does not require ad-hoc
parameters to specify the number of modules or to control the relative
contribution of links and node attributes to network structure. We apply the
proposed method to partition real-world networks with known community
structure. We demonstrate that adding node attributes helps recover the
underlying community structure in content-rich networks more effectively than
using links alone. In addition, we show that our method is faster and more
accurate than alternative state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4335</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4335</id><created>2014-05-16</created><authors><author><keyname>Saini</keyname><forenames>Hemant Kumar</forenames></author><author><keyname>Kushwaha</keyname><forenames>Satpal Singh</forenames></author><author><keyname>Krishna</keyname><forenames>C. Rama</forenames></author></authors><title>Compressing the Data Densely by New Geflochtener to Accelerate Web</title><categories>cs.IT math.IT</categories><journal-ref>International Journal of Computer Applications, 2014</journal-ref><doi>10.5120/16377-5867 10.5120/16377-5867 10.5120/16377-5867</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the present scenario of the internet, there exist many optimization
techniques to improve the Web speed but almost expensive in terms of bandwidth.
So after a long investigation on different techniques to compress the data
without any loss, a new algorithm is proposed based on L Z 77 family which
selectively models the references with backward movement and encodes the
longest matches through greedy parsing with the shortest path technique to
compresses the data with high density. This idea seems to be useful since the
single Web Page contains many repetitive words which create havoc in consuming
space, so let it removes such unnecessary redundancies with 70% efficiency and
compress the pages with 23.75 - 35% compression ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4341</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4341</id><created>2014-05-16</created><updated>2014-09-16</updated><authors><author><keyname>Tan</keyname><forenames>Fei</forenames></author><author><keyname>Xia</keyname><forenames>Yongxiang</forenames></author><author><keyname>Zhu</keyname><forenames>Boyao</forenames></author></authors><title>Link Prediction in Complex Networks: A Mutual Information Perspective</title><categories>cs.SI physics.soc-ph</categories><comments>16 pages</comments><journal-ref>PLOS ONE 2014</journal-ref><doi>10.1371/journal.pone.0107056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological properties of networks are widely applied to study the
link-prediction problem recently. Common Neighbors, for example, is a natural
yet efficient framework. Many variants of Common Neighbors have been thus
proposed to further boost the discriminative resolution of candidate links. In
this paper, we reexamine the role of network topology in predicting missing
links from the perspective of information theory, and present a practical
approach based on the mutual information of network structures. It not only can
improve the prediction accuracy substantially, but also experiences reasonable
computing complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4342</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4342</id><created>2014-05-16</created><authors><author><keyname>Tan</keyname><forenames>Fei</forenames></author><author><keyname>Xia</keyname><forenames>Yongxiang</forenames></author></authors><title>The robust-yet-fragile nature of interdependent networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages</comments><journal-ref>Phys. Rev. E 91, 052809 (2015)</journal-ref><doi>10.1103/PhysRevE.91.052809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interdependent networks have been shown to be extremely vulnerable based on
the percolation model. Parshani et. al further indicated that the more
inter-similar networks are, the more robust they are to random failure. Our
understanding of how coupling patterns shape and impact the cascading failures
of loads in interdependent networks is limited, but is essential for the design
and optimization of the real-world interdependent networked systems. This
question, however, is largely unexplored. In this paper, we address this
question by investigating the robustness of interdependent ER random graphs and
BA scale-free networks under both random failure and intentional attack. It is
found that interdependent ER random graphs are robust-yet-fragile under both
random failures and intentional attack. Interdependent BA scale-free networks,
however, are only robust-yet-fragile under random failure but fragile under
intentional attack. These results advance our understanding of the robustness
of interdependent networks significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4345</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4345</id><created>2014-05-16</created><updated>2014-07-26</updated><authors><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Dai</keyname><forenames>Liyi</forenames></author></authors><title>Wiener Filters in Gaussian Mixture Signal Estimation with Infinity-Norm
  Error</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the estimation of a signal ${\bf x}\in\mathbb{R}^N$ from noisy
observations ${\bf r=x+z}$, where the input~${\bf x}$ is generated by an
independent and identically distributed (i.i.d.) Gaussian mixture source, and
${\bf z}$ is additive white Gaussian noise (AWGN) in parallel Gaussian
channels. Typically, the $\ell_2$-norm error (squared error) is used to
quantify the performance of the estimation process. In contrast, we consider
the $\ell_\infty$-norm error (worst case error). For this error metric, we
prove that, in an asymptotic setting where the signal dimension $N\to\infty$,
the $\ell_\infty$-norm error always comes from the Gaussian component that has
the largest variance, and the Wiener filter asymptotically achieves the optimal
expected $\ell_\infty$-norm error. The i.i.d. Gaussian mixture case is easily
applicable to i.i.d. Bernoulli-Gaussian distributions, which are often used to
model sparse signals. Finally, our results can be extended to linear mixing
systems with i.i.d. Gaussian mixture inputs, in settings where a linear mixing
system can be decoupled to parallel Gaussian channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4347</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4347</id><created>2014-05-16</created><updated>2015-06-11</updated><authors><author><keyname>Haugh</keyname><forenames>Martin</forenames></author><author><keyname>Wang</keyname><forenames>Chun</forenames></author></authors><title>Information Relaxations and Dynamic Zero-Sum Games</title><categories>cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic zero-sum games are an important class of problems with applications
ranging from evasion-pursuit and heads-up poker to certain adversarial versions
of control problems such as multi-armed bandit and multiclass queuing problems.
These games are generally very difficult to solve even when one player's
strategy is fixed, and so constructing and evaluating good sub-optimal policies
for each player is an important practical problem. In this paper, we propose
the use of information relaxations to construct dual lower and upper bounds on
the optimal value of the game. We note that the information relaxation
approach, which has been developed and applied successfully to many large-scale
dynamic programming problems, applies immediately to zero-sum game problems. We
provide some simple numerical examples and identify interesting issues and
complications that arise in the context of zero-sum games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4354</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4354</id><created>2014-05-17</created><authors><author><keyname>Sasao</keyname><forenames>Tomoyo</forenames></author><author><keyname>Konomi</keyname><forenames>Shin'ichi</forenames></author><author><keyname>Arikawa</keyname><forenames>Masatoshi</forenames></author><author><keyname>Fujita</keyname><forenames>Hideyuki</forenames></author></authors><title>Touch Survey: Comparison with Paper and Web Questionnaires</title><categories>cs.HC</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a prototype of touch-based survey tool for tablets and conducted
an experiment to compare interaction patterns of touch-based, PC-based, and
paper-based questionnaires. Our findings suggest that a touch-based interface
allows users to complete ranking questions easily, quickly, and accurately
although it can increase the time to complete a location input task for
well-known, prominent locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4356</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4356</id><created>2014-05-17</created><updated>2014-06-19</updated><authors><author><keyname>Hegeman</keyname><forenames>James W.</forenames></author><author><keyname>Pemmaraju</keyname><forenames>Sriram V.</forenames></author></authors><title>Lessons from the Congested Clique Applied to MapReduce</title><categories>cs.DC cs.DS</categories><comments>15 pages</comments><acm-class>F.2.2; G.2.1; G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main results of this paper are (I) a simulation algorithm which, under
quite general constraints, transforms algorithms running on the Congested
Clique into algorithms running in the MapReduce model, and (II) a distributed
$O(\Delta)$-coloring algorithm running on the Congested Clique which has an
expected running time of (i) $O(1)$ rounds, if $\Delta \geq \Theta(\log^4 n)$;
and (ii) $O(\log \log n)$ rounds otherwise. Applying the simulation theorem to
the Congested-Clique $O(\Delta)$-coloring algorithm yields an $O(1)$-round
$O(\Delta)$-coloring algorithm in the MapReduce model.
  Our simulation algorithm illustrates a natural correspondence between
per-node bandwidth in the Congested Clique model and memory per machine in the
MapReduce model. In the Congested Clique (and more generally, any network in
the $\mathcal{CONGEST}$ model), the major impediment to constructing fast
algorithms is the $O(\log n)$ restriction on message sizes. Similarly, in the
MapReduce model, the combined restrictions on memory per machine and total
system memory have a dominant effect on algorithm design. In showing a fairly
general simulation algorithm, we highlight the similarities and differences
between these models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4362</identifier>
 <datestamp>2014-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4362</id><created>2014-05-17</created><authors><author><keyname>Semwal</keyname><forenames>Vijay Bhaskar</forenames></author><author><keyname>Katiyar</keyname><forenames>S. A.</forenames></author><author><keyname>Chakraborty</keyname><forenames>P.</forenames></author><author><keyname>Nandi</keyname><forenames>G. C.</forenames></author></authors><title>Bipedal Model Based on Human Gait Pattern Parameters for Sagittal Plane
  Movement</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present research as described in this paper tries to impart how imitation
based learning for behavior-based programming can be used to teach the robot.
This development is a big step in way to prove that push recovery is a software
engineering problem and not hardware engineering problem. The walking algorithm
used here aims to select a subset of push recovery problem i.e. disturbance
from environment. We applied the physics at each joint of Halo with some degree
of freedom. The proposed model, Halo is different from other models as
previously developed model were inconsistent with data for different persons.
This would lead to development of the generalized biped model in future and
will bridge the gap between performance and inconsistency. In this paper the
proposed model is applied to data of different persons. Accuracy of model,
performance and result is measured using the behavior negotiation capability of
model developed. In order to improve the performance, proposed model gives the
freedom to handle each joint independently based on the belongingness value for
each joint. The development can be considered as important development for
future world of robotics. The accuracy of model is 70% in one go.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4364</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4364</id><created>2014-05-17</created><authors><author><keyname>Haralambous</keyname><forenames>Yannis</forenames></author><author><keyname>Klyuev</keyname><forenames>Vitaly</forenames></author></authors><title>Thematically Reinforced Explicit Semantic Analysis</title><categories>cs.CL</categories><comments>13 pages, 2 figures, presented at CICLing 2013</comments><msc-class>68T50</msc-class><journal-ref>IJCLA vol. 4, no. 1, Jan.-Jun. 2013, pp. 79--94</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extended, thematically reinforced version of Gabrilovich and
Markovitch's Explicit Semantic Analysis (ESA), where we obtain thematic
information through the category structure of Wikipedia. For this we first
define a notion of categorical tfidf which measures the relevance of terms in
categories. Using this measure as a weight we calculate a maximal spanning tree
of the Wikipedia corpus considered as a directed graph of pages and categories.
This tree provides us with a unique path of &quot;most related categories&quot; between
each page and the top of the hierarchy. We reinforce tfidf of words in a page
by aggregating it with categorical tfidfs of the nodes of these paths, and
define a thematically reinforced ESA semantic relatedness measure which is more
robust than standard ESA and less sensitive to noise caused by out-of-context
words. We apply our method to the French Wikipedia corpus, evaluate it through
a text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a
precision increase of 9-10% compared with standard ESA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4372</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4372</id><created>2014-05-17</created><updated>2015-12-21</updated><authors><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Shen</keyname><forenames>Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao-Ping</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author><author><keyname>Meng</keyname><forenames>Huadong</forenames></author></authors><title>Performance Limits and Geometric Properties of Array Localization</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location-aware networks are of great importance and interest in both civil
and military applications. This paper determines the localization accuracy of
an agent, which is equipped with an antenna array and localizes itself using
wireless measurements with anchor nodes, in a far-field environment. In view of
the Cram\'er-Rao bound, we first derive the localization information for static
scenarios and demonstrate that such information is a weighed sum of Fisher
information matrices from each anchor-antenna measurement pair. Each matrix can
be further decomposed into two parts: a distance part with intensity
proportional to the squared baseband effective bandwidth of the transmitted
signal and a direction part with intensity associated with the normalized
anchor-antenna visual angle. Moreover, in dynamic scenarios, we show that the
Doppler shift contributes additional direction information, with intensity
determined by the agent velocity and the root mean squared time duration of the
transmitted signal. In addition, two measures are proposed to evaluate the
localization performance of wireless networks with different anchor-agent and
array-antenna geometries, and both formulae and simulations are provided for
typical anchor deployments and antenna arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4375</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4375</id><created>2014-05-17</created><authors><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Lu</keyname><forenames>Hsiao-feng</forenames></author></authors><title>Algebraic Codes and a New Physical Layer Transmission Protocol for
  Wireless Distributed Storage Systems</title><categories>cs.IT math.IT math.NT</categories><comments>5 pages. MTNS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wireless storage system, having to communicate over a fading channel
makes repair transmissions prone to physical layer errors. The first approach
to combat fading is to utilize the existing optimal space-time codes. However,
it was recently pointed out that such codes are in general too complex to
decode when the number of helper nodes is bigger than the number of antennas at
the newcomer or data collector. In this paper, a novel protocol for wireless
storage transmissions based on algebraic space-time codes is presented in order
to improve the system reliability while enabling feasible decoding. The
diversity-multiplexing gain tradeoff (DMT) of the system together with
sphere-decodability even with low number of antennas are used as the main
design criteria, thus naturally establishing a DMT-complexity tradeoff. It is
shown that the proposed protocol outperforms the simple time-division multiple
access (TDMA) protocol, while still falling behind the optimal DMT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4378</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4378</id><created>2014-05-17</created><updated>2016-03-02</updated><authors><author><keyname>Alsheikh</keyname><forenames>Mohammad Abu</forenames></author><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author><author><keyname>Tan</keyname><forenames>Hwee-Pink</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author></authors><title>Area Coverage Under Low Sensor Density</title><categories>cs.NI</categories><doi>10.1109/SAHCN.2014.6990347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a solution to the problem of monitoring a region of
interest (RoI) using a set of nodes that is not sufficient to achieve the
required degree of monitoring coverage. In particular, sensing coverage of
wireless sensor networks (WSNs) is a crucial issue in projects due to failure
of sensors. The lack of sensor equipment resources hinders the traditional
method of using mobile robots to move around the RoI to collect readings.
Instead, our solution employs supervised neural networks to produce the values
of the uncovered locations by extracting the non-linear relation among randomly
deployed sensor nodes throughout the area. Moreover, we apply a hybrid
backpropagation method to accelerate the learning convergence speed to a local
minimum solution. We use a real-world data set from meteorological deployment
for experimental validation and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4380</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4380</id><created>2014-05-17</created><authors><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Xiaofeng</forenames></author></authors><title>Transport Capacity of Distributed Wireless CSMA Networks</title><categories>cs.NI</categories><comments>9 figures, accepted to appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the transport capacity of large multi-hop wireless
CSMA networks. Different from previous studies which rely on the use of
centralized scheduling algorithm and/or centralized routing algorithm to
achieve the optimal capacity scaling law, we show that the optimal capacity
scaling law can be achieved using entirely distributed routing and scheduling
algorithms. Specifically, we consider a network with nodes Poissonly
distributed with unit intensity on a $\sqrt{n}\times\sqrt{n}$ square
$B_{n}\subset\Re^{2}$. Furthermore, each node chooses its destination randomly
and independently and transmits following a CSMA protocol. By resorting to the
percolation theory and by carefully tuning the three controllable parameters in
CSMA protocols, i.e. transmission power, carrier-sensing threshold and
count-down timer, we show that a throughput of
$\Theta\left(\frac{1}{\sqrt{n}}\right)$ is achievable in distributed CSMA
networks. Furthermore, we derive the pre-constant preceding the order of the
transport capacity by giving an upper and a lower bound of the transport
capacity. The tightness of the bounds is validated using simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4389</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4389</id><created>2014-05-17</created><authors><author><keyname>Mehta</keyname><forenames>Shraddha</forenames></author><author><keyname>Kalariya</keyname><forenames>Vaishali</forenames></author></authors><title>Efficient Tracking of a Moving Object using Inter-Frame Coding</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video surveillance has long been in use to monitor security sensitive areas
such as banks, department stores, highways, crowded public places and
borders.The advance in computing power, availability of large-capacity storage
devices and high speed network infrastructure paved the way for cheaper,
multi-sensor video surveillance systems.Traditionally, the video outputs are
processed online by human operators and are usually saved to tapes for later
use only after a forensic event.The increase in the number of cameras in
ordinary surveillance systems overloaded both the human operators and the
storage devices with high volumes of data and made it in-feasible to ensure
proper monitoring of sensitive areas for long times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4390</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4390</id><created>2014-05-17</created><authors><author><keyname>Mehta</keyname><forenames>Shraddha</forenames></author><author><keyname>Kalariya</keyname><forenames>Vaishali</forenames></author></authors><title>Real Time Object Tracking Based on Inter-frame Coding: A Review</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-frame Coding plays significant role for video Compression and Computer
Vision. Computer vision systems have been incorporated in many real life
applications (e.g. surveillance systems, medical imaging, robot navigation and
identity verification systems). Object tracking is a key computer vision topic,
which aims at detecting the position of a moving object from a video sequence.
The application of Inter-frame Coding for low frame rate video, as well as for
low resolution video. Various methods based on Top-down approach just like
kernel based or mean shift technique are used to track the object for video,
So, Inter-frame Coding algorithms are widely adopted by video coding standards,
mainly due to their simplicity and good distortion performance for object
tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4392</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4392</id><created>2014-05-17</created><authors><author><keyname>Mitra</keyname><forenames>Sunny</forenames></author><author><keyname>Mitra</keyname><forenames>Ritwik</forenames></author><author><keyname>Riedl</keyname><forenames>Martin</forenames></author><author><keyname>Biemann</keyname><forenames>Chris</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author><author><keyname>Goyal</keyname><forenames>Pawan</forenames></author></authors><title>That's sick dude!: Automatic identification of word sense change across
  different timescales</title><categories>cs.CL cs.AI</categories><comments>10 pages, 2 figures, ACL-2014</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an unsupervised method to identify noun sense
changes based on rigorous analysis of time-varying text data available in the
form of millions of digitized books. We construct distributional thesauri based
networks from data at different time points and cluster each of them separately
to obtain word-centric sense clusters corresponding to the different time
points. Subsequently, we compare these sense clusters of two different time
points to find if (i) there is birth of a new sense or (ii) if an older sense
has got split into more than one sense or (iii) if a newer sense has been
formed from the joining of older senses or (iv) if a particular sense has died.
We conduct a thorough evaluation of the proposed methodology both manually as
well as through comparison with WordNet. Manual evaluation indicates that the
algorithm could correctly identify 60.4% birth cases from a set of 48 randomly
picked samples and 57% split/join cases from a set of 21 randomly picked
samples. Remarkably, in 44% cases the birth of a novel sense is attested by
WordNet, while in 46% cases and 43% cases split and join are respectively
confirmed by WordNet. Our approach can be applied for lexicography, as well as
for applications like word sense disambiguation or semantic search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4394</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4394</id><created>2014-05-17</created><authors><author><keyname>Stock</keyname><forenames>Michiel</forenames></author><author><keyname>Fober</keyname><forenames>Thomas</forenames></author><author><keyname>H&#xfc;llermeier</keyname><forenames>Eyke</forenames></author><author><keyname>Glinca</keyname><forenames>Serghei</forenames></author><author><keyname>Klebe</keyname><forenames>Gerhard</forenames></author><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author></authors><title>Identification of functionally related enzymes by learning-to-rank
  methods</title><categories>cs.LG cs.CE q-bio.QM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enzyme sequences and structures are routinely used in the biological sciences
as queries to search for functionally related enzymes in online databases. To
this end, one usually departs from some notion of similarity, comparing two
enzymes by looking for correspondences in their sequences, structures or
surfaces. For a given query, the search operation results in a ranking of the
enzymes in the database, from very similar to dissimilar enzymes, while
information about the biological function of annotated database enzymes is
ignored.
  In this work we show that rankings of that kind can be substantially improved
by applying kernel-based learning algorithms. This approach enables the
detection of statistical dependencies between similarities of the active cleft
and the biological function of annotated enzymes. This is in contrast to
search-based approaches, which do not take annotated training data into
account. Similarity measures based on the active cleft are known to outperform
sequence-based or structure-based measures under certain conditions. We
consider the Enzyme Commission (EC) classification hierarchy for obtaining
annotated enzymes during the training phase. The results of a set of sizeable
experiments indicate a consistent and significant improvement for a set of
similarity measures that exploit information about small cavities in the
surface of enzymes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4395</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4395</id><created>2014-05-17</created><authors><author><keyname>Li</keyname><forenames>Chang</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>User-Centric Intercell Interference Nulling for Downlink Small Cell
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cell networks are regarded as a promising candidate to meet the
exponential growth of mobile data traffic in cellular networks. With a dense
deployment of access points, spatial reuse will be improved, and uniform
coverage can be provided. However, such performance gains cannot be achieved
without effective intercell interference management. In this paper, a novel
interference coordination strategy, called user-centric intercell interference
nulling, is proposed for small cell networks. A main merit of the proposed
strategy is its ability to effectively identify and mitigate the dominant
interference for each user. Different from existing works, each user selects
the coordinating base stations (BSs) based on the relative distance between the
home BS and the interfering BSs, called the interference nulling (IN) range,
and thus interference nulling adapts to each user's own interference situation.
By adopting a random spatial network model, we derive an approximate expression
of the successful transmission probability to the typical user, which is then
used to determine the optimal IN range. Simulation results shall confirm the
tightness of the approximation, and demonstrate significant performance gains
(about 35%-40%) of the proposed coordination strategy, compared with the
non-coordination case. Moreover, it is shown that the proposed strategy
outperforms other interference nulling methods. Finally, the effect of
imperfect channel state information (CSI) is investigated, where CSI is assumed
to be obtained via limited feedback. It is shown that the proposed coordination
strategy still provides significant performance gains even with a moderate
number of feedback bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4399</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4399</id><created>2014-05-17</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Alanazi</keyname><forenames>Mohammad N.</forenames></author></authors><title>An Efficient Binary Technique for Trace Simplifications of Concurrent
  Programs</title><categories>cs.SE</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Execution of concurrent programs implies frequent switching between different
thread contexts. This property perplexes analyzing and reasoning about
concurrent programs. Trace simplification is a technique that aims at
alleviating this problem via transforming a concurrent program trace
(execution) into a semantically equivalent one. The resulted trace typically
includes less number of context switches than that in the original trace.
  This paper presents a new static approach for trace simplification. This
approach is based on a connectivity analysis that calculates for each
trace-point connectivity and context-switching information. The paper also
presents a novel operational semantics for concurrent programs. The semantics
is used to prove the correctness and efficiency of the proposed techniques for
connectivity analysis and trace simplification. The results of experiments
testing the proposed technique on problems treated by previous work for trace
simplification are also shown in the paper. The results prove the efficiency
and effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4401</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4401</id><created>2014-05-17</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Alanazi</keyname><forenames>Mohammad N.</forenames></author></authors><title>Probabilistic Alias Analysis for Parallel Programming in SSA Forms</title><categories>cs.PL cs.SE</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static alias analysis of different type of programming languages has been
drawing researcher attention. However most of the results of existing
techniques for alias analysis are not precise enough compared to needs of
modern compilers. Probabilistic versions of these results, in which result
elements are associated with occurrence probabilities, are required in
optimizations techniques of modern compilers.
  This paper presents a new probabilistic approach for alias analysis of
parallel programs. The treated parallelism model is that of SPMD where in SPMD,
a program is executed using a fixed number of program threads running on
distributed machines on different data. The analyzed programs are assumed to be
in the static single assignment (SSA) form which is a program representation
form facilitating program analysis. The proposed technique has the form of
simply-strictured system of inference rules. This enables using the system in
applications like Proof-Carrying Code (PPC) which is a general technique for
proving the safety characteristics of modern programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4402</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4402</id><created>2014-05-17</created><updated>2014-12-06</updated><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Xuemin</forenames></author><author><keyname>Sun</keyname><forenames>Zhenlong</forenames></author><author><keyname>Yan</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Lifeng</forenames></author><author><keyname>Jin</keyname><forenames>Zhihui</forenames></author><author><keyname>Wang</keyname><forenames>Liubin</forenames></author><author><keyname>Gao</keyname><forenames>Yang</forenames></author><author><keyname>Law</keyname><forenames>Ching</forenames></author><author><keyname>Zeng</keyname><forenames>Jia</forenames></author></authors><title>Peacock: Learning Long-Tail Topic Features for Industrial Applications</title><categories>cs.IR cs.DC</categories><comments>23 pages, 11 figures, ACM Transactions on Intelligent Systems and
  Technology, 2015</comments><journal-ref>ACM Transactions on Intelligent Systems and Technology, Vol. 6,
  No. 4, Article 47, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet allocation (LDA) is a popular topic modeling technique in
academia but less so in industry, especially in large-scale applications
involving search engine and online advertising systems. A main underlying
reason is that the topic models used have been too small in scale to be useful;
for example, some of the largest LDA models reported in literature have up to
$10^3$ topics, which cover difficultly the long-tail semantic word sets. In
this paper, we show that the number of topics is a key factor that can
significantly boost the utility of topic-modeling systems. In particular, we
show that a &quot;big&quot; LDA model with at least $10^5$ topics inferred from $10^9$
search queries can achieve a significant improvement on industrial search
engine and online advertising systems, both of which serving hundreds of
millions of users. We develop a novel distributed system called Peacock to
learn big LDA models from big data. The main features of Peacock include
hierarchical distributed architecture, real-time prediction and topic
de-duplication. We empirically demonstrate that the Peacock system is capable
of providing significant benefits via highly scalable LDA topic models for
several industrial applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4413</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4413</id><created>2014-05-17</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Heizmann</keyname><forenames>Matthias</forenames></author></authors><title>Geometric Series as Nontermination Arguments for Linear Lasso Programs</title><categories>cs.LO</categories><comments>WST 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a new kind of nontermination argument for linear lasso programs,
called geometric nontermination argument. A geometric nontermination argument
is a finite representation of an infinite execution of the form $(\vec{x} +
\sum_{i=0}^t \lambda^i \vec{y})_{t \geq 0}$. The existence of this
nontermination argument can be stated as a set of nonlinear algebraic
constraints. We show that every linear loop program that has a bounded infinite
execution also has a geometric nontermination argument. Furthermore, we discuss
nonterminating programs that do not have a geometric nontermination argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4422</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4422</id><created>2014-05-17</created><updated>2014-06-20</updated><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author></authors><title>Software developers, moods, emotions, and performance</title><categories>cs.SE</categories><comments>6 pages. Accepted (before copyediting and typesetting) version</comments><journal-ref>IEEE Software, vol. 31, no. 4, pp. 24-27, 2014</journal-ref><doi>10.1109/MS.2014.94</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies show that software developers' happiness pays off when it comes to
productivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4423</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4423</id><created>2014-05-17</created><authors><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author><author><keyname>Stock</keyname><forenames>Michiel</forenames></author><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>Aittokallio</keyname><forenames>Tero</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author></authors><title>A two-step learning approach for solving full and almost full cold start
  problems in dyadic prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dyadic prediction methods operate on pairs of objects (dyads), aiming to
infer labels for out-of-sample dyads. We consider the full and almost full cold
start problem in dyadic prediction, a setting that occurs when both objects in
an out-of-sample dyad have not been observed during training, or if one of them
has been observed, but very few times. A popular approach for addressing this
problem is to train a model that makes predictions based on a pairwise feature
representation of the dyads, or, in case of kernel methods, based on a tensor
product pairwise kernel. As an alternative to such a kernel approach, we
introduce a novel two-step learning algorithm that borrows ideas from the
fields of pairwise learning and spectral filtering. We show theoretically that
the two-step method is very closely related to the tensor product kernel
approach, and experimentally that it yields a slightly better predictive
performance. Moreover, unlike existing tensor product kernel methods, the
two-step method allows closed-form solutions for training and parameter
selection via cross-validation estimates both in the full and almost full cold
start settings, making the approach much more efficient and straightforward to
implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4429</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4429</id><created>2014-05-17</created><updated>2015-02-13</updated><authors><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Compressive Imaging via Approximate Message Passing with Image Denoising</title><categories>cs.IT math.IT</categories><comments>15 pages; 2 tables; 7 figures; to appear in IEEE Trans. Signal
  Process</comments><doi>10.1109/TSP.2015.2408558</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider compressive imaging problems, where images are reconstructed from
a reduced number of linear measurements. Our objective is to improve over
existing compressive imaging algorithms in terms of both reconstruction error
and runtime. To pursue our objective, we propose compressive imaging algorithms
that employ the approximate message passing (AMP) framework. AMP is an
iterative signal reconstruction algorithm that performs scalar denoising at
each iteration; in order for AMP to reconstruct the original input signal well,
a good denoiser must be used. We apply two wavelet based image denoisers within
AMP. The first denoiser is the &quot;amplitude-scaleinvariant Bayes estimator&quot;
(ABE), and the second is an adaptive Wiener filter; we call our AMP based
algorithms for compressive imaging AMP-ABE and AMP-Wiener. Numerical results
show that both AMP-ABE and AMP-Wiener significantly improve over the state of
the art in terms of runtime. In terms of reconstruction quality, AMP-Wiener
offers lower mean square error (MSE) than existing compressive imaging
algorithms. In contrast, AMP-ABE has higher MSE, because ABE does not denoise
as well as the adaptive Wiener filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4433</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4433</id><created>2014-05-17</created><authors><author><keyname>Margan</keyname><forenames>Domagoj</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author></authors><title>Preliminary Report on the Structure of Croatian Linguistic Co-occurrence
  Networks</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we investigate the structure of Croatian linguistic
co-occurrence networks. We examine the change of network structure properties
by systematically varying the co-occurrence window sizes, the corpus sizes and
removing stopwords. In a co-occurrence window of size $n$ we establish a link
between the current word and $n-1$ subsequent words. The results point out that
the increase of the co-occurrence window size is followed by a decrease in
diameter, average path shortening and expectedly condensing the average
clustering coefficient. The same can be noticed for the removal of the
stopwords. Finally, since the size of texts is reflected in the network
properties, our results suggest that the corpus influence can be reduced by
increasing the co-occurrence window size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4443</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4443</id><created>2014-05-17</created><updated>2014-08-06</updated><authors><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Quantum Recursion and Second Quantisation</title><categories>quant-ph cs.LO cs.PL</categories><comments>talk at Tsinghua Software Day 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new notion of quantum recursion of which the control
flow of the computation is quantum rather than classical as in the notions of
recursion considered in the previous studies of quantum programming. A typical
example is recursive quantum walks, which are obtained by slightly modifying
the construction of the ordinary quantum walks. The operational and
denotational semantics of quantum recursions are defined by employing the
second quantisation method, and they are proved to be equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4450</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4450</id><created>2014-05-17</created><authors><author><keyname>Semwal</keyname><forenames>Vijay Bhaskar</forenames></author><author><keyname>Nandi</keyname><forenames>G. C.</forenames></author></authors><title>Study of Humanoid Push Recovery Based on Experiments</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human can negotiate and recovers from Push up to certain extent. The push
recovery capability grows with age (a child has poor push recovery than an
adult) and it is based on learning. A wrestler, for example, has better push
recovery than an ordinary man. However, the mechanism of reactive push recovery
is not known to us. We tried to understand the human learning mechanism by
conducting several experiments. The subjects for the experiments were selected
both as right handed and left handed. Pushes were induced from the behind with
close eyes to observe the motor action as well as with open eyes to observe
learning based reactive behaviors. Important observations show that the left
handed and right handed persons negotiate pushes differently (in opposite
manner). The present research describes some details about the experiments and
the analyses of the results mainly obtained from the joint angle variations
(both for ankle and hip joints) as the manifestation of perturbation. After
smoothening the captured data through higher order polynomials, we feed them to
our model which was developed exploiting the physics of an inverted pendulum
and configured it as a representative of the subjects in the Webot simulation
framework available in our laboratory. In each cases the model also could
recover from the push for the same rage of perturbation which proves the
correctness of the model. Hence the model now can provide greater insight to
push recovery mechanism and can be used for determining push recovery strategy
for humanoid robots. The paper claimed the push recovery is software
engineering problem rather than hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4459</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4459</id><created>2014-05-18</created><authors><author><keyname>Ferrante</keyname><forenames>Guido Carlo</forenames></author><author><keyname>Fiorina</keyname><forenames>Jocelyn</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Maria-Gabriella</forenames></author></authors><title>Robustness of Time Reversal vs. All-Rake Transceivers in Multiple Access
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time reversal, that is prefiltering of transmitted signals with time reversed
channel impulse responses, may be used in single user communications in order
to move complexity from the receiver to the transmitter, and in multiuser
communications to also modify statistical properties of multiuser interference.
Imperfect channel estimation may, however, affect pre- vs. post- filtering
schemes in a different way. This issue is the object of this paper; Robustness
of time reversal vs. All-Rake (AR) transceivers, in multiple access
communications, with respect to channel estimation errors, is investigated.
Results of performance analyses in terms of symbol error probability and
spectral efficiency when the receiver is structured either by a bank of matched
filters or by 1Rake, followed by independent decoders, indicate that AR is
slightly more robust than time reversal but requires in practice more complex
channel estimation procedures since all channels must be simultaneously
inferred in the multiuser communication setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4463</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4463</id><created>2014-05-18</created><updated>2015-03-19</updated><authors><author><keyname>Alsheikh</keyname><forenames>Mohammad Abu</forenames></author><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Tan</keyname><forenames>Hwee-Pink</forenames></author></authors><title>Machine Learning in Wireless Sensor Networks: Algorithms, Strategies,
  and Applications</title><categories>cs.NI cs.LG</categories><comments>Accepted for publication in IEEE Communications Surveys and Tutorials</comments><doi>10.1109/COMST.2014.2320099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks monitor dynamic environments that change rapidly
over time. This dynamic behavior is either caused by external factors or
initiated by the system designers themselves. To adapt to such conditions,
sensor networks often adopt machine learning techniques to eliminate the need
for unnecessary redesign. Machine learning also inspires many practical
solutions that maximize resource utilization and prolong the lifespan of the
network. In this paper, we present an extensive literature review over the
period 2002-2013 of machine learning methods that were used to address common
issues in wireless sensor networks (WSNs). The advantages and disadvantages of
each proposed algorithm are evaluated against the corresponding problem. We
also provide a comparative guide to aid WSN designers in developing suitable
machine learning solutions for their specific application challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4464</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4464</id><created>2014-05-18</created><updated>2014-07-21</updated><authors><author><keyname>Shi</keyname><forenames>Justin</forenames></author></authors><title>Seeking the Principles of Sustainable Software Engineering</title><categories>cs.DC</categories><comments>WSSSPE 2014</comments><acm-class>C.1.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Like other engineering disciplines, software engineering should also have
principles to guide the construction of sustainable computer applications.
Tangible properties include a) unlimited scalability, b) maximal
reproducibility, and c) optimizable energy efficiency. In practice, we expect a
sustainable scientific application should be written once and execute many
times on multiple different processing platforms of different scales with
optimized performance and energy efficiency. For more than two decades,
explicit parallel programming/processing paradigms only focused on performance.
Practices showed that the rigid program-data binding prohibited dynamic runtime
resource optimization and fault isolation, making it difficult to reproduce
applications in scale. This paper reports our practice and experiences in
search of the first principles of sustainable software engineering for compute
and data intensive applications. Specifically, we report our practice and
experiences using implicit parallel programming/processing paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4471</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4471</id><created>2014-05-18</created><authors><author><keyname>Dekel</keyname><forenames>Ofer</forenames></author><author><keyname>Ding</keyname><forenames>Jian</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Online Learning with Composite Loss Functions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new class of online learning problems where each of the online
algorithm's actions is assigned an adversarial value, and the loss of the
algorithm at each step is a known and deterministic function of the values
assigned to its recent actions. This class includes problems where the
algorithm's loss is the minimum over the recent adversarial values, the maximum
over the recent values, or a linear combination of the recent values. We
analyze the minimax regret of this class of problems when the algorithm
receives bandit feedback, and prove that when the minimum or maximum functions
are used, the minimax regret is $\tilde \Omega(T^{2/3})$ (so called hard online
learning problems), and when a linear function is used, the minimax regret is
$\tilde O(\sqrt{T})$ (so called easy learning problems). Previously, the only
online learning problem that was known to be provably hard was the multi-armed
bandit with switching costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4472</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4472</id><created>2014-05-18</created><updated>2014-09-23</updated><authors><author><keyname>Dell</keyname><forenames>Holger</forenames></author></authors><title>AND-compression of NP-complete problems: Streamlined proof and minor
  observations</title><categories>cs.CC cs.DS</categories><comments>extended abstract appears in the Proceedings of the 9th International
  Symposium on Parameterized and Exact Computation (IPEC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drucker (2012) proved the following result: Unless the unlikely
complexity-theoretic collapse coNP is in NP/poly occurs, there is no
AND-compression for SAT. The result has implications for the compressibility
and kernelizability of a whole range of NP-complete parameterized problems. We
present a streamlined proof of Drucker's theorem.
  An AND-compression is a deterministic polynomial-time algorithm that maps a
set of SAT-instances $x_1,\dots,x_t$ to a single SAT-instance $y$ of size
poly(max $|x_i|$) such that $y$ is satisfiable if and only if all $x_i$ are
satisfiable. The &quot;AND&quot; in the name stems from the fact that the predicate &quot;$y$
is satisfiable&quot; can be written as the AND of all predicates &quot;$x_i$ is
satisfiable&quot;. Drucker's result complements the result by Bodlaender et al.
(2009) and Fortnow and Santhanam (2010), who proved the analogous statement for
OR-compressions, and Drucker's proof not only subsumes that result but also
extends it to randomized compression algorithms that are allowed to have a
certain probability of failure.
  Drucker (2012) presented two proofs: The first uses information theory and
the minimax theorem from game theory, and the second is an elementary,
iterative proof that is not as general. In our proof, we realize the iterative
structure as a generalization of the arguments of Ko (1983) for P-selective
sets, which use the fact that tournaments have dominating sets of logarithmic
size. We generalize this fact to hypergraph tournaments. Our proof achieves the
full generality of Drucker's theorem, avoids the minimax theorem, and restricts
the use of information theory to a single, intuitive lemma about the average
noise sensitivity of compressive maps. To prove this lemma, we use the same
information-theoretic inequalities as Drucker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4480</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4480</id><created>2014-05-18</created><authors><author><keyname>Granell</keyname><forenames>Clara</forenames></author><author><keyname>Gomez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Competing spreading processes on multiplex networks: awareness and
  epidemics</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>7 pages, 7 figures</comments><journal-ref>Physical Review E 90 (2014) 012808</journal-ref><doi>10.1103/PhysRevE.90.012808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic-like spreading processes on top of multilayered interconnected
complex networks reveal a rich phase diagram of intertwined competition
effects. A recent study by the authors [Granell et al. Phys. Rev. Lett. 111,
128701 (2013)] presented the analysis of the interrelation between two
processes accounting for the spreading of an epidemics, and the spreading of
information awareness to prevent its infection, on top of multiplex networks.
The results in the case in which awareness implies total immunization to the
disease, revealed the existence of a metacritical point at which the critical
onset of the epidemics starts depending on the reaching of the awareness
process. Here we present a full analysis of these critical properties in the
more general scenario where the awareness spreading does not imply total
immunization, and where infection does not imply immediate awareness of it. We
find the critical relation between both competing processes for a wide spectrum
of parameters representing the interaction between them. We also analyze the
consequences of a massive broadcast of awareness (mass media) on the final
outcome of the epidemic incidence. Importantly enough, the mass media makes the
metacritical point to disappear. The results reveal that the main finding i.e.
existence of a metacritical point, is rooted on the competition principle and
holds for a large set of scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4487</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4487</id><created>2014-05-18</created><updated>2014-12-10</updated><authors><author><keyname>Mu&#xf1;oz</keyname><forenames>Olga</forenames></author><author><keyname>Pascual-Iserte</keyname><forenames>Antonio</forenames></author><author><keyname>Vidal</keyname><forenames>Josep</forenames></author></authors><title>Optimization of Radio and Computational Resources for Energy Efficiency
  in Latency-Constrained Application Offloading</title><categories>cs.IT math.IT</categories><comments>Accepted to be published at IEEE Transactions on Vehicular Technology
  (acceptance: November 2014)</comments><doi>10.1109/TVT.2014.2372852</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing femto-access points (FAPs) with computational capabilities will
allow (either total or partial) offloading of highly demanding applications
from smart-phones to the so called femto-cloud. Such offloading promises to be
beneficial in terms of battery saving at the mobile terminal (MT) and/or
latency reduction in the execution of applications, whenever the energy and/or
time required for the communication process are compensated by the energy
and/or time savings that result from the remote computation at the FAPs. For
this problem, we provide in this paper a framework for the joint optimization
of the radio and computational resource usage exploiting the tradeoff between
energy consumption and latency, and assuming that multiple antennas are
available at the MT and the serving FAP. As a result of the optimization, the
optimal communication strategy (e.g., transmission power, rate, precoder) is
obtained, as well as the optimal distribution of the computational load between
the handset and the serving FAP. The paper also establishes the conditions
under which total or no offloading are optimal, determines which is the minimum
affordable latency in the execution of the application, and analyzes as a
particular case the minimization of the total consumed energy without latency
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4491</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4491</id><created>2014-05-18</created><updated>2014-06-18</updated><authors><author><keyname>Walter</keyname><forenames>Hermann K. -G.</forenames><affiliation>FB Informatik-TU Darmstadt</affiliation></author><author><keyname>Brandt</keyname><forenames>Ulrike</forenames><affiliation>FB Informatik-TU Darmstadt</affiliation></author></authors><title>Unsolvability Cores in Classification Problems</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 19,
  2014) lmcs:1021</journal-ref><doi>10.2168/LMCS-10(2:12)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification problems have been introduced by M. Ziegler as a
generalization of promise problems. In this paper we are concerned with
solvability and unsolvability questions with respect to a given set or language
family, especially with cores of unsolvability. We generalize the results about
unsolvability cores in promise problems to classification problems. Our main
results are a characterization of unsolvability cores via cohesiveness and
existence theorems for such cores in unsolvable classification problems. In
contrast to promise problems we have to strengthen the conditions to assert the
existence of such cores. In general unsolvable classification problems with
more than two components exist, which possess no cores, even if the set family
under consideration satisfies the assumptions which are necessary to prove the
existence of cores in unsolvable promise problems. But, if one of the
components is fixed we can use the results on unsolvability cores in promise
problems, to assert the existence of such cores in general. In this case we
speak of conditional classification problems and conditional cores. The
existence of conditional cores can be related to complexity cores. Using this
connection we can prove for language families, that conditional cores with
recursive components exist, provided that this family admits an uniform
solution for the word problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4506</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4506</id><created>2014-05-18</created><authors><author><keyname>Peng</keyname><forenames>Xiaojiang</forenames></author><author><keyname>Wang</keyname><forenames>Limin</forenames></author><author><keyname>Wang</keyname><forenames>Xingxing</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Bag of Visual Words and Fusion Methods for Action Recognition:
  Comprehensive Study and Good Practice</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video based action recognition is one of the important and challenging
problems in computer vision research. Bag of Visual Words model (BoVW) with
local features has become the most popular method and obtained the
state-of-the-art performance on several realistic datasets, such as the HMDB51,
UCF50, and UCF101. BoVW is a general pipeline to construct a global
representation from a set of local features, which is mainly composed of five
steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook
generation, (iv) feature encoding, and (v) pooling and normalization. Many
efforts have been made in each step independently in different scenarios and
their effect on action recognition is still unknown. Meanwhile, video data
exhibits different views of visual pattern, such as static appearance and
motion dynamics. Multiple descriptors are usually extracted to represent these
different views. Many feature fusion methods have been developed in other areas
and their influence on action recognition has never been investigated before.
This paper aims to provide a comprehensive study of all steps in BoVW and
different fusion methods, and uncover some good practice to produce a
state-of-the-art action recognition system. Specifically, we explore two kinds
of local features, ten kinds of encoding methods, eight kinds of pooling and
normalization strategies, and three kinds of fusion methods. We conclude that
every step is crucial for contributing to the final recognition rate.
Furthermore, based on our comprehensive study, we propose a simple yet
effective representation, called hybrid representation, by exploring the
complementarity of different BoVW frameworks and local descriptors. Using this
representation, we obtain the state-of-the-art on the three challenging
datasets: HMDB51 (61.1%), UCF50 (92.3%), and UCF101 (87.9%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4507</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4507</id><created>2014-05-18</created><authors><author><keyname>Ye</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Lu</keyname><forenames>Zhipeng</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>A Multi-parent Memetic Algorithm for the Linear Ordering Problem</title><categories>cs.NE math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a multi-parent memetic algorithm (denoted by MPM)
for solving the classic Linear Ordering Problem (LOP). The MPM algorithm
integrates in particular a multi-parent recombination operator for generating
offspring solutions and a distance-and-quality based criterion for pool
updating. Our MPM algorithm is assessed on 8 sets of 484 widely used LOP
instances and compared with several state-of-the-art algorithms in the
literature, showing the efficacy of the MPM algorithm. Specifically, for the
255 instances whose optimal solutions are unknown, the MPM is able to detect
better solutions than the previous best-known ones for 66 instances, while
matching the previous best-known results for 163 instances. Furthermore, some
additional experiments are carried out to analyze the key elements and
important parameters of MPM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4510</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4510</id><created>2014-05-18</created><authors><author><keyname>Ye</keyname><forenames>Tao</forenames></author><author><keyname>Zhou</keyname><forenames>Kan</forenames></author><author><keyname>Lu</keyname><forenames>Zhipeng</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>A Memetic Algorithm for the Linear Ordering Problem with Cumulative
  Costs</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an effective memetic algorithm for the linear ordering
problem with cumulative costs. The proposed algorithm combines an order-based
recombination operator with an improved forward-backward local search procedure
and employs a solution quality based replacement criterion for pool updating.
Extensive experiments on 118 well-known benchmark instances show that the
proposed algorithm achieves competitive results by identifying 46 new upper
bounds. Furthermore, some critical ingredients of our algorithm are analyzed to
understand the source of its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4522</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4522</id><created>2014-05-18</created><authors><author><keyname>Sarma</keyname><forenames>Siddhartha</forenames></author><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Secure Transmission in Amplify and Forward Networks for Multiple
  Degraded Eavesdroppers</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have evaluated the optimal secrecy rate for Amplify-and-Forward (AF) relay
networks with multiple eavesdroppers. Assuming i.i.d. Gaussian noise at the
destination and the eavesdroppers, we have devised technique to calculate
optimal scaling factor for relay nodes to obtain optimal secrecy rate under
both sum power constraint and individual power constraint. Initially, we have
considered special channel conditions for both destination and eavesdroppers,
which led us to analytical solution of the problem. Contrarily, the general
scenario being a non-convex optimization problem, not only lacks an analytical
solution, but also is hard to solve. Therefore, we have proposed an efficiently
solvable quadratic program (QP) which provides a sub-optimal solution to the
original problem. Then, we have devised an iterative scheme for calculating
optimal scaling factor efficiently for both the sum power and individual power
constraint scenario. Necessary figures are provided in result section to affirm
the validity of our proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4534</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4534</id><created>2014-05-18</created><authors><author><keyname>S</keyname><forenames>Lakshmi Prabha</forenames></author><author><keyname>Janakiraman</keyname><forenames>T. N.</forenames></author></authors><title>Polynomial-time Approximation Algorithm for finding Highly Comfortable
  Team in any given Social Network</title><categories>cs.DS cs.SI</categories><comments>The manuscript contains 38 pages and 9 Figures</comments><msc-class>91D30, 05C82, 05C85, 05C69, 05C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many indexes (measures or metrics) in Social Network Analysis
(SNA), like density, cohesion, etc. In this paper, we define a new SNA index
called &quot;comfortability&quot;. One among the lack of many factors, which affect the
effectiveness of a group, is &quot;comfortability&quot;. So, comfortability is one of the
important attributes (characteristics) for a successful team work. It is
important to find a comfortable and successful team in any given social
network. In this paper, comfortable team, better comfortable team and highly
comfortable team of a social network are defined based on \textbf{graph
theoretic concepts} and some of their structural properties are analyzed.
  It is proved that forming better comfortable team or highly comfortable team
in any connected network are NP-Complete using the concepts of domination in
graph theory. Next, we give a polynomial-time approximation algorithm for
finding such a highly comfortable team in any given network with performance
ratio O(\ln \Delta), where \Delta is the maximum degree of a given network
(graph). The time complexity of the algorithm is proved to be O(n^{3}), where n
is the number of persons (vertices) in the network (graph). It is also proved
that our algorithm has reasonably reduced the dispersion rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4535</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4535</id><created>2014-05-18</created><authors><author><keyname>Baoxin</keyname><forenames>Xiu</forenames></author><author><keyname>Fan</keyname><forenames>Changjun</forenames></author><author><keyname>Liang</keyname><forenames>Meilian</forenames></author></authors><title>On Disjoint Golomb Rulers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set $\{a_i\:|\: 1\leq i \leq k\}$ of non-negative integers is a Golomb
ruler if differences $a_i-a_j$, for any $i \neq j$, are all distinct. A set of
$I$ disjoint Golomb rulers (DGR) each being a $J$-subset of $\{1,2,\cdots, n\}$
is called an $(I,J,n)-DGR$. Let $H(I, J)$ be the least positive $n$ such that
there is an $(I,J,n)-DGR$. In this paper, we propose a series of conjectures on
the constructions and structures of DGR. The main conjecture states that if $A$
is any set of positive integers such that $|A| = H(I, J)$, then there are $I$
disjoint Golomb rulers, each being a $J$-subset of $A$, which generalizes the
conjecture proposed by Koml{\'o}s, Sulyok and Szemer{\'e}di in 1975 on the
special case $I = 1$. These conjectures are computationally verified for some
values of $I$ and $J$ through modest computation. Eighteen exact values of
$H(I,J)$ and ten upper bounds on $H(I,J)$ are obtained by computer search for
$7 \leq I \leq 13$ and $10 \leq J \leq 13$. Moveover for $I &gt; 13$ and $10 \leq
J \leq 13$, $H(I,J)=IJ$ are determined without difficulty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4543</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4543</id><created>2014-05-18</created><authors><author><keyname>Mahajan</keyname><forenames>Dhruv</forenames></author><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author><author><keyname>Sundararajan</keyname><forenames>S.</forenames></author></authors><title>A Distributed Algorithm for Training Nonlinear Kernel Machines</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the distributed training of nonlinear kernel machines on
Map-Reduce. We show that a re-formulation of Nystr\&quot;om approximation based
solution which is solved using gradient based techniques is well suited for
this, especially when it is necessary to work with a large number of basis
points. The main advantages of this approach are: avoidance of computing the
pseudo-inverse of the kernel sub-matrix corresponding to the basis points;
simplicity and efficiency of the distributed part of the computations; and,
friendliness to stage-wise addition of basis points. We implement the method
using an AllReduce tree on Hadoop and demonstrate its value on a few large
benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4544</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4544</id><created>2014-05-18</created><updated>2015-03-16</updated><authors><author><keyname>Mahajan</keyname><forenames>Dhruv</forenames></author><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author><author><keyname>Sundararajan</keyname><forenames>S.</forenames></author></authors><title>A distributed block coordinate descent method for training $l_1$
  regularized linear classifiers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed training of $l_1$ regularized classifiers has received great
attention recently. Most existing methods approach this problem by taking steps
obtained from approximating the objective by a quadratic approximation that is
decoupled at the individual variable level. These methods are designed for
multicore and MPI platforms where communication costs are low. They are
ineffi?cient on systems such as Hadoop running on a cluster of commodity
machines where communication costs are substantial. In this paper we design a
distributed algorithm for $l_1$ regularization that is much better suited for
such systems than existing algorithms. A careful cost analysis is used to
support these points and motivate our method. The main idea of our algorithm is
to do block optimization of many variables on the actual objective function
within each computing node; this increases the computational cost per step that
is matched with the communication cost, and decreases the number of outer
iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and
Gauss-Southwell greedy schemes are used for choosing variables to update in
each step. We establish global convergence theory for our algorithm, including
Q-linear rate of convergence. Experiments on two benchmark problems show our
method to be much faster than existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4560</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4560</id><created>2014-05-18</created><authors><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author><author><keyname>Lenhardt</keyname><forenames>Rastislav</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Model Checking Markov Chains Against Unambiguous Buchi Automata</title><categories>cs.LO cs.FL</categories><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial-time algorithm for model checking finite Markov chains
against omega-regular specifications given as unambiguous Buchi automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4565</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4565</id><created>2014-05-18</created><updated>2015-07-16</updated><authors><author><keyname>Grech</keyname><forenames>Neville</forenames></author><author><keyname>Georgiou</keyname><forenames>Kyriakos</forenames></author><author><keyname>Pallister</keyname><forenames>James</forenames></author><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author><author><keyname>Morse</keyname><forenames>Jeremy</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Static analysis of energy consumption for LLVM IR programs</title><categories>cs.PL</categories><doi>10.1145/2764967.2764974</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy models can be constructed by characterizing the energy consumed by
executing each instruction in a processor's instruction set. This can be used
to determine how much energy is required to execute a sequence of assembly
instructions, without the need to instrument or measure hardware.
  However, statically analyzing low-level program structures is hard, and the
gap between the high-level program structure and the low-level energy models
needs to be bridged. We have developed techniques for performing a static
analysis on the intermediate compiler representations of a program.
Specifically, we target LLVM IR, a representation used by modern compilers,
including Clang. Using these techniques we can automatically infer an estimate
of the energy consumed when running a function under different platforms, using
different compilers.
  One of the challenges in doing so is that of determining an energy cost of
executing LLVM IR program segments, for which we have developed two different
approaches. When this information is used in conjunction with our analysis, we
are able to infer energy formulae that characterize the energy consumption for
a particular program. This approach can be applied to any languages targeting
the LLVM toolchain, including C and XC or architectures such as ARM Cortex-M or
XMOS xCORE, with a focus towards embedded platforms. Our techniques are
validated on these platforms by comparing the static analysis results to the
physical measurements taken from the hardware. Static energy consumption
estimation enables energy-aware software development, without requiring
hardware knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4571</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4571</id><created>2014-05-18</created><authors><author><keyname>Peng</keyname><forenames>Tong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Delay-Tolerant Distributed Space-Time Coding Based on
  Adjustable Code Matrices for Cooperative MIMO Relaying Systems</title><categories>cs.IT math.IT</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive delay-tolerant distributed space-time coding (DSTC) scheme that
exploits feedback is proposed for two-hop cooperative MIMO networks. Maximum
likelihood (ML) receivers and adjustable code matrices are considered subject
to a power constraint with a decode-and-forward (DF) cooperation strategy. In
the proposed delay-tolerant DSTC scheme, an adjustable code matrix is employed
to transform the space-time coded matrix at the relay nodes. Least-squares (LS)
algorithms are developed with reduced computational complexity to adjust the
parameters of the codes. Simulation results show that the proposed algorithms
obtain significant performance gains and address the delay issue for
cooperative MIMO systems as compared to existing delay-tolerant DSTC schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4572</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4572</id><created>2014-05-18</created><authors><author><keyname>Tobin</keyname><forenames>R. Joshua</forenames></author><author><keyname>Houghton</keyname><forenames>Conor J.</forenames></author></authors><title>A Kernel-Based Calculation of Information on a Metric Space</title><categories>cs.IT math.IT</categories><journal-ref>Entropy 2013, 15(10), 4540-4552</journal-ref><doi>10.3390/e15104540</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel density estimation is a technique for approximating probability
distributions. Here, it is applied to the calculation of mutual information on
a metric space. This is motivated by the problem in neuroscience of calculating
the mutual information between stimuli and spiking responses; the space of
these responses is a metric space. It is shown that kernel density estimation
on a metric space resembles the k-nearest-neighbor approach. This approach is
applied to a toy dataset designed to mimic electrophysiological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4574</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4574</id><created>2014-05-18</created><authors><author><keyname>Greenewald</keyname><forenames>Kristjan H.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Kronecker PCA Based Spatio-Temporal Modeling of Video for Dismount
  Classification</title><categories>cs.CV stat.ME</categories><comments>8 pages. To appear in Proceeding of SPIE DSS. arXiv admin note: text
  overlap with arXiv:1402.5568</comments><doi>10.1117/12.2050184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the application of KronPCA spatio-temporal modeling techniques
[Greenewald et al 2013, Tsiligkaridis et al 2013] to the extraction of
spatiotemporal features for video dismount classification. KronPCA performs a
low-rank type of dimensionality reduction that is adapted to spatio-temporal
data and is characterized by the T frame multiframe mean and covariance of p
spatial features. For further regularization and improved inverse estimation,
we also use the diagonally corrected KronPCA shrinkage methods we presented in
[Greenewald et al 2013]. We apply this very general method to the modeling of
the multivariate temporal behavior of HOG features extracted from pedestrian
bounding boxes in video, with gender classification in a challenging dataset
chosen as a specific application. The learned covariances for each class are
used to extract spatiotemporal features which are then classified, achieving
competitive classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4583</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4583</id><created>2014-05-18</created><authors><author><keyname>Feng</keyname><forenames>Wei</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author></authors><title>ESSP: An Efficient Approach to Minimizing Dense and Nonsubmodular Energy
  Functions</title><categories>cs.CV cs.LG</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many recent advances in computer vision have demonstrated the impressive
power of dense and nonsubmodular energy functions in solving visual labeling
problems. However, minimizing such energies is challenging. None of existing
techniques (such as s-t graph cut, QPBO, BP and TRW-S) can individually do this
well. In this paper, we present an efficient method, namely ESSP, to optimize
binary MRFs with arbitrary pairwise potentials, which could be nonsubmodular
and with dense connectivity. We also provide a comparative study of our
approach and several recent promising methods. From our study, we make some
reasonable recommendations of combining existing methods that perform the best
in different situations for this challenging problem. Experimental results
validate that for dense and nonsubmodular energy functions, the proposed
approach can usually obtain lower energies than the best combination of other
techniques using comparably reasonable time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4589</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4589</id><created>2014-05-18</created><updated>2014-05-20</updated><authors><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Mei</keyname><forenames>Hong-cen</forenames></author><author><keyname>Yang</keyname><forenames>Hao</forenames></author></authors><title>A Parallel Way to Select the Parameters of SVM Based on the Ant
  Optimization Algorithm</title><categories>cs.NE cs.LG</categories><comments>3 pages, 2 figures, 2 tables</comments><msc-class>13-XX 13Pxx 13P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of experimental data shows that Support Vector Machine (SVM)
algorithm has obvious advantages in text classification, handwriting
recognition, image classification, bioinformatics, and some other fields. To
some degree, the optimization of SVM depends on its kernel function and Slack
variable, the determinant of which is its parameters $\delta$ and c in the
classification function. That is to say,to optimize the SVM algorithm, the
optimization of the two parameters play a huge role. Ant Colony Optimization
(ACO) is optimization algorithm which simulate ants to find the optimal path.In
the available literature, we mix the ACO algorithm and Parallel algorithm
together to find a well parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4592</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4592</id><created>2014-05-18</created><authors><author><keyname>Xie</keyname><forenames>Hu</forenames></author><author><keyname>Feng</keyname><forenames>Da-Zheng</forenames></author><author><keyname>Yuan</keyname><forenames>Ming-Dong</forenames></author></authors><title>Fast Adaptive Beamforming based on kernel method under Small Sample
  Support</title><categories>cs.IT math.IT</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the high computational complexity and the insufficient
samples in large-scale array signal processing restrict the real-world
applications of the conventional full-dimensional adaptive beamforming (sample
matrix inversion) algorithms. In this paper, we propose a computationally
efficient and fast adaptive beamforming algorithm under small sample support.
The proposed method is implemented by formulating the adaptive weight vector as
a linear combination of training samples plus a signal steering vector, on the
basis of the fact that the adaptive weight vector lies in the
signal-plus-interference subspace. Consequently, by using the well-known linear
kernel methods with very good small-sample performance, only a low-dimension
combination vector needs to be computed instead of the high-dimension adaptive
weight vector itself, which remarkably reduces the degree of freedom and the
computational complexity. Experimental results validate the good performance
and the computational effectiveness of the proposed methods for small samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4596</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4596</id><created>2014-05-19</created><updated>2015-02-05</updated><authors><author><keyname>Huang</keyname><forenames>Zhenyu</forenames></author><author><keyname>Sun</keyname><forenames>Yao</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>On the Efficiency of Solving Boolean Polynomial Systems with the
  Characteristic Set Method</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improved characteristic set algorithm for solving Boolean polynomial
systems is proposed. This algorithm is based on the idea of converting all the
polynomials into monic ones by zero decomposition, and using additions to
obtain pseudo-remainders. To improve the efficiency, two important techniques
are applied in the algorithm. One is eliminating variables by new generated
linear polynomials, and the other is optimizing the order of choosing
polynomial for zero decomposition. We present some complexity bounds of the
algorithm by analyzing the structure of the zero decomposition tree. Some
experimental results show that this new algorithm is more efficient than
precious characteristic set algorithms for solving Boolean polynomial systems,
and in some cases the running time of this algorithm can be well predicted by
analyzing a part of branches in the zero decomposition tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4597</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4597</id><created>2014-05-19</created><authors><author><keyname>Naderinejad</keyname><forenames>Marjan</forenames></author><author><keyname>Tarokh</keyname><forenames>Mohammad Jafar</forenames></author><author><keyname>Poorebrahimi</keyname><forenames>Alireza</forenames></author></authors><title>Recognition and Ranking Critical Success Factors of Business
  Intelligence in Hospitals -- Case Study: Hasheminejad Hospital</title><categories>cs.OH</categories><comments>http://airccse.org/journal/ijcsit2014_curr.html</comments><doi>10.5121/ijcsit.2014.6208</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Business Intelligence, not as a tool of a product but as a new approach is
propounded in organizations to make tough decisions in business as shortly as
possible. Hospital managers often need business intelligence in their fiscal,
operational, and clinical reports and indices. The main goal of recognition and
ranking CSF is implementation of a business intelligent system in hospitals to
increase success factor of application of business intelligence in health and
treatment sector. This paper is an application and descriptive-analytical one,
in which we use questionnaires to gather data and we used SPSS and LISREL to
analyze them. Its statistical society is managers and personnel of Hasheminejad
hospital and case studies are selected by Cochran formula. The findings show
that all three organizational, process, and technological factors equally
affect implementation of business intelligence based on Yeoh &amp; Koronis
approach, where the assumptions are based upon it. The proposed model for CSFs
of business intelligence in hospitals include: declaring perspective, goals and
strategies, development of human and financial resources, clarification of
organizational culture, documentation and process mature, management support,
etc. Business intelligence implementation is affected by different components.
Center of Hasheminejad hospital BI system as a leader in providing quality
health care, partially succeeded to take advantage of the benefits the
organization in passing the information revolution but the development of this
system to achieve intelligent hospital and its certainty is a high priority,
thus it can`t be said that the hospital-wide BI system is quite favorable. In
this regard, it can be concluded that Hasheminejad hospital requires practical
model for business intelligence systems development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4599</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4599</id><created>2014-05-19</created><authors><author><keyname>Wu</keyname><forenames>Dalei</forenames></author><author><keyname>Wu</keyname><forenames>Haiqing</forenames></author></authors><title>Modelling Data Dispersion Degree in Automatic Robust Estimation for
  Multivariate Gaussian Mixture Models with an Application to Noisy Speech
  Processing</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The trimming scheme with a prefixed cutoff portion is known as a method of
improving the robustness of statistical models such as multivariate Gaussian
mixture models (MG- MMs) in small scale tests by alleviating the impacts of
outliers. However, when this method is applied to real- world data, such as
noisy speech processing, it is hard to know the optimal cut-off portion to
remove the outliers and sometimes removes useful data samples as well. In this
paper, we propose a new method based on measuring the dispersion degree (DD) of
the training data to avoid this problem, so as to realise automatic robust
estimation for MGMMs. The DD model is studied by using two different measures.
For each one, we theoretically prove that the DD of the data samples in a
context of MGMMs approximately obeys a specific (chi or chi-square)
distribution. The proposed method is evaluated on a real-world application with
a moderately-sized speaker recognition task. Experiments show that the proposed
method can significantly improve the robustness of the conventional training
method of GMMs for speaker recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4604</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4604</id><created>2014-05-19</created><updated>2014-05-27</updated><authors><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Dauphin</keyname><forenames>Yann N.</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the saddle point problem for non-convex optimization</title><categories>cs.LG cs.NE</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central challenge to many fields of science and engineering involves
minimizing non-convex error functions over continuous, high dimensional spaces.
Gradient descent or quasi-Newton methods are almost ubiquitously used to
perform such minimizations, and it is often thought that a main source of
difficulty for the ability of these local methods to find the global minimum is
the proliferation of local minima with much higher error than the global
minimum. Here we argue, based on results from statistical physics, random
matrix theory, and neural network theory, that a deeper and more profound
difficulty originates from the proliferation of saddle points, not local
minima, especially in high dimensional problems of practical interest. Such
saddle points are surrounded by high error plateaus that can dramatically slow
down learning, and give the illusory impression of the existence of a local
minimum. Motivated by these arguments, we propose a new algorithm, the
saddle-free Newton method, that can rapidly escape high dimensional saddle
points, unlike gradient descent and quasi-Newton methods. We apply this
algorithm to deep neural network training, and provide preliminary numerical
evidence for its superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4607</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4607</id><created>2014-05-19</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bernardo</forenames></author><author><keyname>Porto</keyname><forenames>Fabio</forenames></author></authors><title>$\Upsilon$-DB: Managing scientific hypotheses as uncertain data</title><categories>cs.DB cs.CE</categories><comments>To appear in PVLDB 2014</comments><journal-ref>PVLDB 7(11):959-62, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In view of the paradigm shift that makes science ever more data-driven, we
consider deterministic scientific hypotheses as uncertain data. This vision
comprises a probabilistic database (p-DB) design methodology for the systematic
construction and management of U-relational hypothesis DBs, viz.,
$\Upsilon$-DBs. It introduces hypothesis management as a promising new class of
applications for p-DBs. We illustrate the potential of $\Upsilon$-DB as a tool
for deep predictive analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4608</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4608</id><created>2014-05-19</created><updated>2014-05-20</updated><authors><author><keyname>Chen</keyname><forenames>Junting</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Two-Tier Precoding for FDD Multi-cell Massive MIMO Time-Varying
  Interference Networks (Full Version)</title><categories>cs.IT cs.NI math.IT</categories><doi>10.1109/JSAC.2014.2328391</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a promising technology in future wireless communication
networks. However, it raises a lot of implementation challenges, for example,
the huge pilot symbols and feedback overhead, requirement of real-time global
CSI, large number of RF chains needed and high computational complexity. We
consider a two-tier precoding strategy for multi-cell massive MIMO interference
networks, with an outer precoder for inter-cell/inter-cluster interference
cancellation, and an inner precoder for intra-cell multiplexing. In particular,
to combat with the computational complexity issue of the outer precoding, we
propose a low complexity online iterative algorithm to track the outer precoder
under time-varying channels. We follow an optimization technique and formulate
the problem on the Grassmann manifold. We develop a low complexity iterative
algorithm, which converges to the global optimal solution under static
channels. In time-varying channels, we propose a compensation technique to
offset the variation of the time-varying optimal solution. We show with our
theoretical result that, under some mild conditions, perfect tracking of the
target outer precoder using the proposed algorithm is possible. Numerical
results demonstrate that the two-tier precoding with the proposed iterative
compensation algorithm can achieve a good performance with a significant
complexity reduction compared with the conventional two-tier precoding
techniques in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4618</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4618</id><created>2014-05-19</created><authors><author><keyname>Shu</keyname><forenames>Wanneng</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Yunji</forenames></author></authors><title>A novel energy-efficient resource allocation algorithm based on immune
  clonal optimization for green cloud computing</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1006.0308 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cloud computing is a style of computing in which dynamically scalable and
other virtualized resources are provided as a service over the Internet. The
energy consumption and makespan associated with the resources allocated should
be taken into account. This paper proposes an improved clonal selection
algorithm based on time cost and energy consumption models in cloud computing
environment. We have analyzed the performance of our approach using the
CloudSim toolkit. The experimental results show that our approach has immense
potential as it offers significant improvement in the aspects of response time
and makespan, demonstrates high potential for the improvement in energy
efficiency of the data center, and can effectively meet the service level
agreement requested by the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4623</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4623</id><created>2014-05-19</created><updated>2014-10-18</updated><authors><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author></authors><title>Training-Based SWIPT: Optimal Power Splitting at the Receiver</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted for publication in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a point-to-point system with simultaneous wireless information
and power transfer (SWIPT) over a block fading channel. Each transmission block
consists of a training phase and a data transmission phase. Pilot symbols are
transmitted during the training phase for channel estimation at the receiver.
To enable SWIPT, the receiver adopts a power-splitting design, such that a
portion of the received signal is used for channel estimation or data
detection, while the remaining is used for energy harvesting. We optimally
design the power-splitting ratios for both training and data phases to achieve
the best ergodic capacity performance while maintaining a required energy
harvesting rate. Our result shows how a power-splitting receiver can make the
best use of the received pilot and data signals to obtain the optimal SWIPT
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4626</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4626</id><created>2014-05-19</created><authors><author><keyname>Yang</keyname><forenames>Junjie</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Yu</keyname><forenames>Rong</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>A Semiblind Two-Way Training Method for Discriminatory Channel
  Estimation in MIMO Systems</title><categories>cs.IT cs.CR math.IT</categories><comments>accepted for publication in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discriminatory channel estimation (DCE) is a recently developed strategy to
enlarge the performance difference between a legitimate receiver (LR) and an
unauthorized receiver (UR) in a multiple-input multiple-output (MIMO) wireless
system. Specifically, it makes use of properly designed training signals to
degrade channel estimation at the UR which in turn limits the UR's
eavesdropping capability during data transmission. In this paper, we propose a
new two-way training scheme for DCE through exploiting a whitening-rotation
(WR) based semiblind method. To characterize the performance of DCE, a
closed-form expression of the normalized mean squared error (NMSE) of the
channel estimation is derived for both the LR and the UR. Furthermore, the
developed analytical results on NMSE are utilized to perform optimal power
allocation between the training signal and artificial noise (AN). The
advantages of our proposed DCE scheme are two folds: 1) compared to the
existing DCE scheme based on the linear minimum mean square error (LMMSE)
channel estimator, the proposed scheme adopts a semiblind approach and achieves
better DCE performance; 2) the proposed scheme is robust against active
eavesdropping with the pilot contamination attack, whereas the existing scheme
fails under such an attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4628</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4628</id><created>2014-05-19</created><authors><author><keyname>Chou</keyname><forenames>Evan</forenames></author><author><keyname>G&#xfc;nt&#xfc;rk</keyname><forenames>Sinan</forenames></author></authors><title>Distributed noise-shaping quantization: I. Beta duals of finite frames
  and near-optimal quantization of random measurements</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new algorithm for the so-called &quot;Analysis Problem&quot; in
quantization of finite frame representations which provides a near-optimal
solution in the case of random measurements. The main contributions include the
development of a general quantization framework called {\em distributed
noise-shaping}, and in particular, {\em beta duals} of frames, as well as the
performance analysis of beta duals in both deterministic and probabilistic
settings. It is shown that for Gaussian random frames, using beta duals result
in near-optimally accurate reconstructions with respect to both the frame
redundancy and the number of levels that the frame coefficients are quantized
at. More specifically, if $L$ quantization levels per measurement are used to
encode the unit ball in $\mathbb{R}^k$ via a Gaussian frame of $m$ vectors,
then with overwhelming probability the beta-dual reconstruction error is shown
to be bounded by $\sqrt{k}L^{-(1-\eta)m/k}$ where $\eta$ is arbitrarily small
for sufficiently large problems. Additional features of the proposed algorithm
include low computational cost and parallel implementability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4642</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4642</id><created>2014-05-19</created><authors><author><keyname>Hongbin</keyname><forenames>Chen</forenames></author><author><keyname>Fangfang</keyname><forenames>Zhou</forenames></author><author><keyname>Jun</keyname><forenames>Cai</forenames></author><author><keyname>Feng</keyname><forenames>Zhao</forenames></author><author><keyname>Qian</keyname><forenames>He</forenames></author></authors><title>Geometric projection-based switching policy for multiple energy
  harvesting transmitters</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmitter switching can provide resiliency and robustness to a
communication system with multiple energy harvesting transmitters. However,
excessive transmitter switching will bring heavy control overhead. In this
paper, a geometric projection-based transmitter switching policy is proposed
for a communication system with multiple energy harvesting transmitters and one
receiver, which can reduce the number of switches. The results show that the
proposed transmitter switching policy outperforms several heuristic ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4644</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4644</id><created>2014-05-19</created><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Malossi</keyname><forenames>A. Cristiano I.</forenames></author><author><keyname>Bekas</keyname><forenames>Constantin</forenames></author><author><keyname>Curioni</keyname><forenames>Alessandro</forenames></author></authors><title>Changing Computing Paradigms Towards Power Efficiency</title><categories>cs.MS cs.NA</categories><journal-ref>Philosophical Transactions of the Royal Society A: Physical,
  Mathematical and Engineering Sciences. 372(2018)</journal-ref><doi>10.1098/rsta.2013.0278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power awareness is fast becoming immensely important in computing, ranging
from the traditional High Performance Computing applications, to the new
generation of data centric workloads.
  In this work we describe our efforts towards a power efficient computing
paradigm that combines low precision and high precision arithmetic. We showcase
our ideas for the widely used kernel of solving systems of linear equations
that finds numerous applications in scientific and engineering disciplines as
well as in large scale data analytics, statistics and machine learning.
  Towards this goal we developed tools for the seamless power profiling of
applications at a fine grain level. In addition, we verify here previous work
on post FLOPS/Watt metrics and show that these can shed much more light in the
power/energy profile of important applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4647</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4647</id><created>2014-05-19</created><authors><author><keyname>Mallat</keyname><forenames>Achraf</forenames></author><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author><author><keyname>Dardari</keyname><forenames>Davide</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Statistics of the MLE and Approximate Upper and Lower Bounds - Part 2:
  Threshold Computation and Optimal Signal Design</title><categories>stat.AP cs.IT math.IT</categories><doi>10.1109/TSP.2014.2355776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threshold and ambiguity phenomena are studied in Part 1 of this work where
approximations for the mean-squared-error (MSE) of the maximum likelihood
estimator are proposed using the method of interval estimation (MIE), and where
approximate upper and lower bounds are derived. In this part we consider
time-of-arrival estimation and we employ the MIE to derive closed-form
expressions of the begin-ambiguity, end-ambiguity and asymptotic
signal-to-noise ratio (SNR) thresholds with respect to some features of the
transmitted signal. Both baseband and passband pulses are considered. We prove
that the begin-ambiguity threshold depends only on the shape of the envelope of
the ACR, whereas the end-ambiguity and asymptotic thresholds only on the shape
of the ACR. We exploit the results on the begin-ambiguity and asymptotic
thresholds to optimize, with respect to the available SNR, the pulse that
achieves the minimum attainable MSE. The results of this paper are valid for
various estimation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4657</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4657</id><created>2014-05-19</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Jagannathan</keyname><forenames>Krishna</forenames></author></authors><title>Finite-Horizon Optimal Transmission Policies for Energy Harvesting
  Sensors</title><categories>cs.IT math.IT</categories><comments>Appeared in IEEE ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive optimal transmission policies for energy harvesting
sensors to maximize the utility obtained over a finite horizon. First, we
consider a single energy harvesting sensor, with discrete energy arrival
process, and a discrete energy consumption policy. Under this model, we show
that the optimal finite horizon policy is a threshold policy, and explicitly
characterize the thresholds, and the thresholds can be precomputed using a
recursion. Next, we address the case of multiple sensors, with only one of them
allowed to transmit at any given time to avoid interference, and derive an
explicit optimal policy for this scenario as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4659</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4659</id><created>2014-05-19</created><updated>2014-09-21</updated><authors><author><keyname>Cohen</keyname><forenames>Kobi</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Asymptotically Optimal Anomaly Detection via Sequential Testing</title><categories>cs.IT math.IT</categories><comments>28 pages, 5 figures, part of this work will be presented at the 52nd
  Annual Allerton Conference on Communication, Control, and Computing, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential detection of independent anomalous processes among K processes is
considered. At each time, only M processes can be observed, and the
observations from each chosen process follow two different distributions,
depending on whether the process is normal or abnormal. Each anomalous process
incurs a cost per unit time until its anomaly is identified and fixed.
Switching across processes and state declarations are allowed at all times,
while decisions are based on all past observations and actions. The objective
is a sequential search strategy that minimizes the total expected cost incurred
by all the processes during the detection process under reliability
constraints. Low-complexity algorithms are established to achieve
asymptotically optimal performance as the error constraints approach zero.
Simulation results demonstrate strong performance in the finite regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4681</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4681</id><created>2014-05-19</created><authors><author><keyname>Zheng</keyname><forenames>Yuanshi</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Containment control of multi-agent systems with measurement noises</title><categories>cs.SY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, containment control of multi-agent systems with measurement
noises is studied under directed networks. When the leaders are stationary, a
stochastic approximation type protocol is employed to solve the containment
control of multi-agent systems. By using stochastic analysis tools and
algebraic graph theory, some necessary and sufficient criteria are established
to ensure the followers converge to the convex hull spanned by the leaders in
the sense of mean square and probability 1. When the leasers are dynamic, a
stochastic approximation type protocol with distributed estimators is developed
and necessary and sufficient conditions are also obtained for solving the
containment control problem. Simulations are provided to illustrate the
effectiveness of the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4691</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4691</id><created>2014-05-19</created><updated>2014-07-14</updated><authors><author><keyname>Cheng</keyname><forenames>Siu-Wing</forenames></author><author><keyname>Mencel</keyname><forenames>Liam</forenames></author><author><keyname>Vigneron</keyname><forenames>Antoine</forenames></author></authors><title>A Faster Algorithm for Computing Straight Skeletons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for computing the straight skeleton of a polygon.
For a polygon with $n$ vertices, among which $r$ are reflex vertices, we give a
deterministic algorithm that reduces the straight skeleton computation to a
motorcycle graph computation in $O(n (\log n)\log r)$ time. It improves on the
previously best known algorithm for this reduction, which is randomized, and
runs in expected $O(n \sqrt{h+1}\log^2 n)$ time for a polygon with $h$ holes.
Using known motorcycle graph algorithms, our result yields improved time bounds
for computing straight skeletons. In particular, we can compute the straight
skeleton of a non-degenerate polygon in $O(n (\log n) \log r +
r^{4/3+\varepsilon})$ time for any $\varepsilon&gt;0$. On degenerate input, our
time bound increases to $O(n (\log n) \log r + r^{17/11+\varepsilon})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4697</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4697</id><created>2014-05-19</created><updated>2014-08-26</updated><authors><author><keyname>Yu</keyname><forenames>Ye</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author></authors><title>Space Shuffle: A Scalable, Flexible, and High-Bandwidth Data Center
  Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data center applications require the network to be scalable and
bandwidth-rich. Current data center network architectures often use rigid
topologies to increase network bandwidth. A major limitation is that they can
hardly support incremental network growth. Recent work proposes to use random
interconnects to provide growth flexibility. However routing on a random
topology suffers from control and data plane scalability problems, because
routing decisions require global information and forwarding state cannot be
aggregated. In this paper we design a novel flexible data center network
architecture, Space Shuffle (S2), which applies greedy routing on multiple ring
spaces to achieve high-throughput, scalability, and flexibility. The proposed
greedy routing protocol of S2 effectively exploits the path diversity of
densely connected topologies and enables key-based routing. Extensive
experimental studies show that S2 provides high bisectional bandwidth and
throughput, near-optimal routing path lengths, extremely small forwarding
state, fairness among concurrent data flows, and resiliency to network
failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4699</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4699</id><created>2014-05-19</created><authors><author><keyname>Naskos</keyname><forenames>Athanasios</forenames></author><author><keyname>Stachtiari</keyname><forenames>Emmanouela</forenames></author><author><keyname>Gounaris</keyname><forenames>Anastasios</forenames></author><author><keyname>Katsaros</keyname><forenames>Panagiotis</forenames></author><author><keyname>Tsoumakos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Konstantinou</keyname><forenames>Ioannis</forenames></author><author><keyname>Sioutas</keyname><forenames>Spyros</forenames></author></authors><title>Cloud elasticity using probabilistic model checking</title><categories>cs.DC cs.LO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has become the leading paradigm for deploying large-scale
infrastructures and running big data applications, due to its capacity of
achieving economies of scale. In this work, we focus on one of the most
prominent advantages of cloud computing, namely the on-demand resource
provisioning, which is commonly referred to as elasticity. Although a lot of
effort has been invested in developing systems and mechanisms that enable
elasticity, the elasticity decision policies tend to be designed without
guaranteeing or quantifying the quality of their operation. This work aims to
make the development of elasticity policies more formalized and dependable. We
make two distinct contributions. First, we propose an extensible approach to
enforcing elasticity through the dynamic instantiation and online quantitative
verification of Markov Decision Processes (MDP) using probabilistic model
checking. Second, we propose concrete elasticity models and related elasticity
policies. We evaluate our decision policies using both real and synthetic
datasets in clusters of NoSQL databases. According to the experimental results,
our approach improves upon the state-of-the-art in significantly increasing
user-defined utility values and decreasing user-defined threshold violations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4709</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4709</id><created>2014-05-19</created><authors><author><keyname>Gomez</keyname><forenames>Gerardo</forenames></author><author><keyname>Hortiguela</keyname><forenames>Lorenzo</forenames></author><author><keyname>Perez</keyname><forenames>Quiliano</forenames></author><author><keyname>Lorca</keyname><forenames>Javier</forenames></author><author><keyname>Garcia</keyname><forenames>Raquel</forenames></author><author><keyname>Aguayo-Torres</keyname><forenames>Mari Carmen</forenames></author></authors><title>YouTube QoE Evaluation Tool for Android Wireless Terminals</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an Android application which is able to evaluate
and analyze the perceived Quality of Experience (QoE) for YouTube service in
wireless terminals. To achieve this goal, the application carries out
measurements of objective Quality of Service (QoS) parameters, which are then
mapped onto subjective QoE (in terms of Mean Opinion Score, MOS) by means of a
utility function. Our application also informs the user about potential causes
that lead to a low MOS as well as provides some hints to improve it. After each
YouTube session, the users may optionally qualify the session through an online
opinion survey. This information has been used in a pilot experience to
correlate the theoretical QoE model with real user feedback. Results from such
an experience have shown that the theoretical model (taken from the literature)
provides slightly more pessimistic results compared to user feedback. Users
seem to be more indulgent with wireless connections, increasing the MOS from
the opinion survey in about 20% compared to the theoretical model, which was
obtained from wired scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4713</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4713</id><created>2014-05-19</created><updated>2016-02-01</updated><authors><author><keyname>Yi</keyname><forenames>Huiyue</forenames></author></authors><title>Signal-noise search RMT estimator with adaptive decision criterion for
  estimating the number of signals based on random matrix theory</title><categories>cs.IT math.IT stat.ME</categories><comments>16 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the number of signals is a fundamental problem in many scientific
and engineering fields. As a well-known estimator based on random matrix theory
(RMT), the RMT estimator estimates the number of signals via sequentially
testing the likelihood of an eigenvalue as arising from a signal or from noise.
However, the RMT estimator tends to down-estimate the number of signals as some
signals will be buried in the interaction term among eigenvalues. In order to
overcome this problem, we focus on developing novel RMT estimators by
incorporating the Lawley theory into random matrix theory. Firstly, we derive a
novel decision statistics for signal number estimation by incorporating the
Lawley theory into random matrix theory, and then propose a signal-search RMT
estimator for signal number estimation. Secondly, we analyze the effect of the
interaction term among eigenvalues on the estimation performance of the
signal-search RMT estimator and the RMT estimator. It shows that the
signal-search RMT estimator has better detection performance than the RMT
estimator when some signals are buried in the interaction term among
eigenvalues, but has larger over-estimation probability than the RMT estimator
when all signals are strong enough to be detected by the RMT estimator.
Thirdly, in order to overcome the individual drawbacks of these two estimators,
we derive the over-estimation probability of the signal-search RMT estimator
and the down-estimation probability of the RMT estimator, and propose a
signal-noise search RMT estimator which can adaptively select its decision
criterion between the RMT estimator and the signal-search RMT estimator.
Finally, simulation results show that the signal-noise search RMT estimator
significantly outperforms the existing estimators including the RMT estimator,
the classic AIC and MDL estimators, and the modified AIC estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4721</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4721</id><created>2014-05-16</created><authors><author><keyname>Cuevas</keyname><forenames>Erik</forenames></author><author><keyname>Zaldivar</keyname><forenames>Daniel</forenames></author><author><keyname>Perez-Cisneros</keyname><forenames>Marco</forenames></author><author><keyname>Oliva</keyname><forenames>Diego</forenames></author></authors><title>Block matching algorithm based on Differential Evolution for motion
  estimation</title><categories>cs.MM</categories><comments>19 pages</comments><journal-ref>Engineering Applications of Artificial Intelligence, 26 (1) ,
  (2013), pp. 488-498</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion estimation is one of the major problems in developing video coding
applications. Among all motion estimation approaches, Block matching (BM)
algorithms are the most popular methods due to their effectiveness and
simplicity for both software and hardware implementations. A BM approach
assumes that the movement of pixels within a defined region of the current
frame (Macro-Block, MB) can be modeled as a translation of pixels contained in
the previous frame. In this procedure, the motion vector is obtained by
minimizing the sum of absolute differences (SAD) produced by the MB of the
current frame over a determined search window from the previous frame. The SAD
evaluation is computationally expensive and represents the most consuming
operation in the BM process. The most straightforward BM method is the full
search algorithm (FSA) which finds the most accurate motion vector, calculating
exhaustively the SAD values for all elements of the search window. Over this
decade, several fast BM algorithms have been proposed to reduce the number of
SAD operations by calculating only a fixed subset of search locations at the
price of a poor accuracy. In this paper, a new algorithm based on Differential
Evolution (DE) is proposed to reduce the number of search locations in the BM
process. In order to avoid computing several search locations, the algorithm
estimates the SAD values (fitness) for some locations using the SAD values of
previously calculated neighboring positions. Since the proposed algorithm does
not consider any fixed search pattern or other different assumption, a high
probability for finding the true minimum (accurate motion vector) is expected.
In comparison to other fast BM algorithms, the proposed method deploys more
accurate motion vectors yet delivering competitive time rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4733</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4733</id><created>2014-05-19</created><updated>2014-12-03</updated><authors><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames></author></authors><title>Multiple-Environment Markov Decision Processes</title><categories>cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Multi-Environment Markov Decision Processes (MEMDPs) which are
MDPs with a set of probabilistic transition functions. The goal in a MEMDP is
to synthesize a single controller with guaranteed performances against all
environments even though the environment is unknown a priori. While MEMDPs can
be seen as a special class of partially observable MDPs, we show that several
verification problems that are undecidable for partially observable MDPs, are
decidable for MEMDPs and sometimes have even efficient solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4734</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4734</id><created>2014-04-30</created><authors><author><keyname>Solomon</keyname><forenames>Justin</forenames></author><author><keyname>Crane</keyname><forenames>Keenan</forenames></author><author><keyname>Butscher</keyname><forenames>Adrian</forenames></author><author><keyname>Wojtan</keyname><forenames>Chris</forenames></author></authors><title>A General Framework for Bilateral and Mean Shift Filtering</title><categories>cs.GR</categories><comments>11 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalization of the bilateral filter that can be applied to
feature-preserving smoothing of signals on images, meshes, and other domains
within a single unified framework. Our discretization is competitive with
state-of-the-art smoothing techniques in terms of both accuracy and speed, is
easy to implement, and has parameters that are straightforward to understand.
Unlike previous bilateral filters developed for meshes and other irregular
domains, our construction reduces exactly to the image bilateral on rectangular
domains and comes with a rigorous foundation in both the smooth and discrete
settings. These guarantees allow us to construct unconditionally convergent
mean-shift schemes that handle a variety of extremely noisy signals. We also
apply our framework to geometric edge-preserving effects like feature
enhancement and show how it is related to local histogram techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4740</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4740</id><created>2014-05-19</created><authors><author><keyname>Bouzidi</keyname><forenames>Yacine</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Lazard</keyname><forenames>Sylvain</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Pouget</keyname><forenames>Marc</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>IMJ, INRIA Paris-Rocquencourt</affiliation></author></authors><title>Improved algorithm for computing separating linear forms for bivariate
  systems</title><categories>cs.CG cs.SC</categories><comments>ISSAC - 39th International Symposium on Symbolic and Algebraic
  Computation (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of computing a linear separating form of a system of
two bivariate polynomials with integer coefficients, that is a linear
combination of the variables that takes different values when evaluated at the
distinct solutions of the system. The computation of such linear forms is at
the core of most algorithms that solve algebraic systems by computing rational
parameterizations of the solutions and this is the bottleneck of these
algorithms in terms of worst-case bit complexity. We present for this problem a
new algorithm of worst-case bit complexity $\sOB(d^7+d^6\tau)$ where $d$ and
$\tau$ denote respectively the maximum degree and bitsize of the input (and
where $\sO$ refers to the complexity where polylogarithmic factors are omitted
and $O_B$ refers to the bit complexity). This algorithm simplifies and
decreases by a factor $d$ the worst-case bit complexity presented for this
problem by Bouzidi et al. \cite{bouzidiJSC2014a}. This algorithm also yields,
for this problem, a probabilistic Las-Vegas algorithm of expected bit
complexity $\sOB(d^5+d^4\tau)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4758</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4758</id><created>2014-05-19</created><authors><author><keyname>Magureanu</keyname><forenames>Stefan</forenames></author><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms</title><categories>cs.LG</categories><comments>COLT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic multi-armed bandit problems where the expected reward
is a Lipschitz function of the arm, and where the set of arms is either
discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic
problem specific lower bounds for the regret satisfied by any algorithm, and
propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz
structure of the problem. In fact, we prove that OSLB is asymptotically
optimal, as its asymptotic regret matches the lower bound. The regret analysis
of our algorithms relies on a new concentration inequality for weighted sums of
KL divergences between the empirical distributions of rewards and their true
distributions. For continuous Lipschitz bandits, we propose to first discretize
the action space, and then apply OSLB or CKL-UCB, algorithms that provably
exploit the structure efficiently. This approach is shown, through numerical
experiments, to significantly outperform existing algorithms that directly deal
with the continuous set of arms. Finally the results and algorithms are
extended to contextual bandits with similarities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4764</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4764</id><created>2014-05-19</created><authors><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author><author><keyname>Tsitsiklis</keyname><forenames>John. N.</forenames></author><author><keyname>Zhong</keyname><forenames>Yuan</forenames></author></authors><title>On Queue-Size Scaling for Input-Queued Switches</title><categories>cs.NI math.OC</categories><comments>21 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal scaling of the expected total queue size in an $n\times
n$ input-queued switch, as a function of the number of ports $n$ and the load
factor $\rho$, which has been conjectured to be $\Theta (n/(1-\rho))$. In a
recent work, the validity of this conjecture has been established for the
regime where $1-\rho = O(1/n^2)$. In this paper, we make further progress in
the direction of this conjecture. We provide a new class of scheduling policies
under which the expected total queue size scales as
$O(n^{1.5}(1-\rho)^{-1}\log(1/(1-\rho)))$ when $1-\rho = O(1/n)$. This is an
improvement over the state of the art; for example, for $\rho = 1 - 1/n$ the
best known bound was $O(n^3)$, while ours is $O(n^{2.5}\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4779</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4779</id><created>2014-05-19</created><authors><author><keyname>Valerdi</keyname><forenames>Juan Luis</forenames></author><author><keyname>Rodriguez</keyname><forenames>Fernando Raul</forenames></author></authors><title>Una metodolog\'ia para realizar Diferenciaci\'on Autom\'atica Anidada</title><categories>cs.SC</categories><comments>11 pages, in Spanish</comments><acm-class>G.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  En este trabajo se presenta una propuesta para realizar Diferenciaci\'on
Autom\'atica Anidada utilizando cualquier biblioteca de Diferenciaci\'on
Autom\'atica que permita sobrecarga de operadores. Para calcular las derivadas
anidadas en una misma evaluaci\'on de la funci\'on, la cual se asume que sea
anal\'itica, se trabaja con el modo forward utilizando una nueva estructura
llamada SuperAdouble, que garantiza que se aplique correctamente la
Diferenciaci\'on Autom\'atica y se calculen el valor y la derivada que se
requiera.
  This paper proposes a framework to apply Nested Automatic Differentiation
using any library of Automatic Differentiation which allows operator
overloading. To compute nested derivatives of a function while it is being
evaluated, which is assumed to be analytic, a new structure called SuperAdouble
is used in the forward mode. This new class guarantees the correct application
of Automatic Differentiation to calculate the value and derivative of a
function where is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4802</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4802</id><created>2014-05-19</created><updated>2014-10-11</updated><authors><author><keyname>Parmar</keyname><forenames>Paritosh</forenames></author></authors><title>Use of Computer Vision to Detect Tangles in Tangled Objects</title><categories>cs.CV</categories><comments>IEEE International Conference on Image Information Processing;
  untangle; untangling; computer vision; robotic vision; untangling by robot;
  Tangled-100 dataset; tangled linear deformable objects; personal robotics;
  image processing</comments><doi>10.1109/ICIIP.2013.6707551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Untangling of structures like ropes and wires by autonomous robots can be
useful in areas such as personal robotics, industries and electrical wiring &amp;
repairing by robots. This problem can be tackled by using computer vision
system in robot. This paper proposes a computer vision based method for
analyzing visual data acquired from camera for perceiving the overlap of wires,
ropes, hoses i.e. detecting tangles. Information obtained after processing
image according to the proposed method comprises of position of tangles in
tangled object and which wire passes over which wire. This information can then
be used to guide robot to untangle wire/s. Given an image, preprocessing is
done to remove noise. Then edges of wire are detected. After that, the image is
divided into smaller blocks and each block is checked for wire overlap/s and
finding other relevant information. TANGLED-100 dataset was introduced, which
consists of images of tangled linear deformable objects. Method discussed in
here was tested on the TANGLED-100 dataset. Accuracy achieved during
experiments was found to be 74.9%. Robotic simulations were carried out to
demonstrate the use of the proposed method in applications of robot. Proposed
method is a general method that can be used by robots working in different
situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4806</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4806</id><created>2014-05-19</created><updated>2015-07-23</updated><authors><author><keyname>Lin</keyname><forenames>T.</forenames></author></authors><title>Undecidability of model-checking branching-time properties of stateless
  probabilistic pushdown process</title><categories>cs.LO cs.FL</categories><comments>minor revision</comments><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we settle a problem in probabilistic verification of
infinite--state process (specifically, {\it probabilistic pushdown process}).
We show that model checking {\it stateless probabilistic pushdown process}
(pBPA) against {\it probabilistic computational tree logic} (PCTL) is
undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4807</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4807</id><created>2014-05-19</created><authors><author><keyname>Huang</keyname><forenames>Qixing</forenames></author><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author></authors><title>Scalable Semidefinite Relaxation for Maximum A Posterior Estimation</title><categories>cs.LG cs.CV cs.IT math.IT math.OC stat.ML</categories><comments>accepted to International Conference on Machine Learning (ICML 2014)</comments><journal-ref>International Conference on Machine Learning (ICML), vol. 32, pp.
  64-72, June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum a posteriori (MAP) inference over discrete Markov random fields is a
fundamental task spanning a wide spectrum of real-world applications, which is
known to be NP-hard for general graphs. In this paper, we propose a novel
semidefinite relaxation formulation (referred to as SDR) to estimate the MAP
assignment. Algorithmically, we develop an accelerated variant of the
alternating direction method of multipliers (referred to as SDPAD-LR) that can
effectively exploit the special structure of the new relaxation. Encouragingly,
the proposed procedure allows solving SDR for large-scale problems, e.g.,
problems on a grid graph comprising hundreds of thousands of variables with
multiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable
of attaining comparable accuracy while exhibiting remarkably improved
scalability, in contrast to the commonly held belief that semidefinite
relaxation can only been applied on small-scale MRF problems. We have evaluated
the performance of SDR on various benchmark datasets including OPENGM2 and PIC
in terms of both the quality of the solutions and computation time.
Experimental results demonstrate that for a broad class of problems, SDPAD-LR
outperforms state-of-the-art algorithms in producing better MAP assignment in
an efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4819</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4819</id><created>2014-05-19</created><updated>2014-10-13</updated><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>A Reduced Latency List Decoding Algorithm for Polar Codes</title><categories>cs.IT math.IT</categories><comments>7 pages, accepted by 2014 IEEE International Workshop on Signal
  Processing Systems (SiPS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long polar codes can achieve the capacity of arbitrary binary-input discrete
memoryless channels under a low complexity successive cancelation (SC) decoding
algorithm. But for polar codes with short and moderate code length, the
decoding performance of the SC decoding algorithm is inferior. The cyclic
redundancy check (CRC) aided successive cancelation list (SCL) decoding
algorithm has better error performance than the SC decoding algorithm for short
or moderate polar codes. However, the CRC aided SCL (CA-SCL) decoding algorithm
still suffer from long decoding latency. In this paper, a reduced latency list
decoding (RLLD) algorithm for polar codes is proposed. For the proposed RLLD
algorithm, all rate-0 nodes and part of rate-1 nodes are decoded instantly
without traversing the corresponding subtree. A list maximum-likelihood
decoding (LMLD) algorithm is proposed to decode the maximum likelihood (ML)
nodes and the remaining rate-1 nodes. Moreover, a simplified LMLD (SLMLD)
algorithm is also proposed to reduce the computational complexity of the LMLD
algorithm. Suppose a partial parallel list decoder architecture with list size
$L=4$ is used, for an (8192, 4096) polar code, the proposed RLLD algorithm can
reduce the number of decoding clock cycles and decoding latency by 6.97 and
6.77 times, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4820</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4820</id><created>2014-05-19</created><authors><author><keyname>Dewan</keyname><forenames>Prateek</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>It Doesn't Break Just on Twitter. Characterizing Facebook content During
  Real World Events</title><categories>cs.SI physics.soc-ph</categories><comments>One of the first attempts at characterizing Facebook content, and
  comparing it with Twitter content during real world events</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple studies in the past have analyzed the role and dynamics of the
Twitter social network during real world events. However, little work has
explored the content of other social media services, or compared content across
two networks during real world events. We believe that social media platforms
like Facebook also play a vital role in disseminating information on the
Internet during real world events. In this work, we study and characterize the
content posted on the world's biggest social network, Facebook, and present a
comparative analysis of Facebook and Twitter content posted during 16 real
world events. Contrary to existing notion that Facebook is used mostly as a
private network, our findings reveal that more than 30% of public content that
was present on Facebook during these events, was also present on Twitter. We
then performed qualitative analysis on the content spread by the most active
users during these events, and found that over 10% of the most active users on
both networks post spam content. We used stylometric features from Facebook
posts and tweet text to classify this spam content, and were able to achieve an
accuracy of over 99% for Facebook, and over 98% for Twitter. This work is aimed
at providing researchers with an overview of Facebook content during real world
events, and serve as basis for more in-depth exploration of its various aspects
like information quality, and credibility during real world events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4824</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4824</id><created>2014-05-19</created><authors><author><keyname>Khan</keyname><forenames>Parvez Mahmood</forenames></author><author><keyname>Beg</keyname><forenames>M. M. Sufyan</forenames></author></authors><title>Measuring Cost of Quality (CoQ) on SDLC Projects is Indispensible for
  Effective Software Quality Assurance</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1202.2506 by other authors
  without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known fact that was phrased by famous quality scholar P.B. Crosby
that it is always cheaper to do the job right the first time. However, this
statement must be reconsidered with respect to software development projects,
because the concept of quality and associated costs measurements in software
engineering discipline is not as matured as in manufacturing and other fields
of the industry. Post delivery defects (i.e. software bugs) are very common and
integral part of software industry. While the process of measuring and
classifying quality cost components is visible, obvious and institutionalized
in manufacturing industry, it is still evolving in software industry. In
addition to this, the recommendations of British standard BS-6143-2:1990 for
classifying quality-related costs into prevention costs, appraisal costs, and
failure costs have been successfully adopted by many industries, by identifying
the activities carried out within each of these categories, and measuring the
costs connected with them, software industry has a long-way to go to have the
same level of adoption and institutionalization of cost of quality measurements
and visibility. Cost of Quality for software isn't the price of creating a
quality software product or IT-service. It's actually the cost of NOT creating
a quality software product or IT-service. The chronic affliction of majority of
software development projects that are frequently found bleeding with cost
overruns, schedule slippage, scope creep and poor quality of deliverables in
the global IT industry, was the trigger for this research work. Lessons learnt
from this study offer valuable prescriptive guidance for small and medium
software businesses, who can benefit from this study by applying the same for
their quality improvement initiatives using CoQ-metric, to enhance the
capability and maturity of their SDLC-project performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4828</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4828</id><created>2014-05-19</created><authors><author><keyname>Hamdare</keyname><forenames>Safa</forenames></author><author><keyname>Nagpurkar</keyname><forenames>Varsha</forenames></author><author><keyname>Mittal</keyname><forenames>Jayashri</forenames></author></authors><title>Securing SMS Based One Time Password Technique from Man in the Middle
  Attack</title><categories>cs.CR cs.SI</categories><comments>5 pages, 5 figures, 3 Tables, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>IJETT 11(3):154-158, May 2014</journal-ref><doi>10.14445/22315381/IJETT-V11P230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of financial transaction in e-commerce is difficult to implement and
there is a risk that users confidential data over the internet may be accessed
by hackers. Unfortunately, interacting with an online service such as a banking
web application often requires certain degree of technical sophistication that
not all Internet users possess. For the last couple of years such naive users
have been increasingly targeted by phishing attacks that are launched by
miscreants who are aiming to make an easy profit by means of illegal financial
transactions. In this paper, we have proposed an idea for securing e-commerce
transaction from phishing attack. An approach already exists where phishing
attack is prevented using one time password which is sent on users registered
mobile via SMS for authentication.But this method can be counter attacked by
man in the middle.In our paper, a new idea is proposed which is more secure
compared to the existing online payment system using OTP. In this mechanism,
OTP is combined with the secure key and is then passed through RSA algorithm to
generate the Transaction password. A copy of this password is maintained at the
server side and is being generated at the user side using a mobile
application.So that it is not transferred over the insecure network leading to
a fraudulent transaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4841</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4841</id><created>2014-05-19</created><authors><author><keyname>Moss</keyname><forenames>Aaron</forenames></author></authors><title>Derivatives of Parsing Expression Grammars</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new memoized derivative parsing algorithm for
recognition of parsing expression grammars. The algorithm runs in worst case
quartic time and cubic space. However, existing research suggests that due to
the limited amount of backtracking and recursion in real-world grammars and
input, practical performance may be closer to linear time and constant space;
experimental validation of this conjecture is in progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1405.4843</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1405.4843</id><created>2014-05-19</created><authors><author><keyname>Atkins</keyname><forenames>Joshua</forenames></author><author><keyname>Giacobello</keyname><forenames>Daniele</forenames></author></authors><title>Trends and Perspectives for Signal Processing in Consumer Audio</title><categories>cs.SD cs.MM</categories><comments>IEEE Audio and Acoustic Signal Processing Technical Committee
  Newsletter, May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The trend in media consumption towards streaming and portability offers new
challenges and opportunities for signal processing in audio and acoustics. The
most significant embodiment of this trend is that most music consumption now
happens on-the-go which has recently led to an explosion in headphone sales and
small portable speakers. In particular, premium headphones offer a gateway for
a younger generation to experience high quality sound. Additionally, through
technologies incorporating head-related transfer functions headphones can also
offer unique new experiences in gaming, augmented reality, and surround sound
listening. Home audio has also seen a transition to smaller sound systems in
the form of sound bars. This speaker configuration offers many exciting
challenges for surround sound reproduction which has traditionally used five
speakers surrounding the listener. Furthermore, modern home entertainment
systems offer more than just content delivery; users now expect wireless and
connected smart devices with video conferencing, gaming, and other interactive
capabilities. With this comes challenges for voice interaction at a distance
and in demanding conditions, e.g., during content playback, and opportunities
for new smart interactive experiences based on awareness of environment and
user biometrics.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="60000" completeListSize="102538">1122234|61001</resumptionToken>
</ListRecords>
</OAI-PMH>
