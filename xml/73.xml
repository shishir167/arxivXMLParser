<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:42:57Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|72001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06625</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06625</id><created>2015-01-26</created><updated>2015-06-12</updated><authors><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author><author><keyname>Yu</keyname><forenames>Xiangcheng</forenames></author></authors><title>Accelerating Polynomial Homotopy Continuation on a Graphics Processing
  Unit with Double Double and Quad Double Arithmetic</title><categories>cs.MS cs.DC cs.NA math.AG math.NA</categories><comments>Accepted for publication in the Proceedings of the 7th International
  Workshop on Parallel Symbolic Computation (PASCO 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical continuation methods track a solution path defined by a homotopy.
The systems we consider are defined by polynomials in several variables with
complex coefficients. For larger dimensions and degrees, the numerical
conditioning worsens and hardware double precision becomes often insufficient
to reach the end of the solution path. With double double and quad double
arithmetic, we can solve larger problems that we could not solve with hardware
double arithmetic, but at a higher computational cost. This cost overhead can
be compensated by acceleration on a Graphics Processing Unit (GPU). We describe
our implementation and report on computational results on benchmark polynomial
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06626</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06626</id><created>2015-01-26</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author><author><keyname>Mattei</keyname><forenames>Nicholas</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Manipulating the Probabilistic Serial Rule</title><categories>cs.GT cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.6523</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic serial (PS) rule is one of the most prominent randomized
rules for the assignment problem. It is well-known for its superior fairness
and welfare properties. However, PS is not immune to manipulative behaviour by
the agents. We initiate the study of the computational complexity of an agent
manipulating the PS rule. We show that computing an expected utility better
response is NP- hard. On the other hand, we present a polynomial-time algorithm
to compute a lexicographic best response. For the case of two agents, we show
that even an expected utility best response can be computed in polynomial time.
Our result for the case of two agents relies on an interesting connection with
sequential allocation of discrete objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06627</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06627</id><created>2015-01-26</created><updated>2015-05-20</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Competitive Equilibrium with Equal Incomes for Allocation of Indivisible
  Objects</title><categories>cs.GT</categories><comments>6 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In AAMAS 2014, Bouveret and Lemaitre (2014) presented a hierarchy of fairness
concepts for allocation of indivisible objects. Among them CEEI (Competitive
Equilibrium with Equal Incomes) was the strongest. In this note, we settle the
complexity of computing a discrete CEEI assignment by showing it is strongly
NP-hard. We then highlight a fairness notion (CEEI-FRAC) that is even stronger
than CEEI for discrete assignments, is always Pareto optimal, and can be
verified in polynomial time. We also show that computing a CEEI-FRAC discrete
assignment is strongly NP-hard in general but polynomial-time computable if the
utilities are zero or one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06633</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06633</id><created>2015-01-26</created><updated>2015-01-30</updated><authors><author><keyname>Lavin</keyname><forenames>Andrew</forenames></author></authors><title>maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell
  GPUs</title><categories>cs.NE cs.DC cs.LG</categories><comments>7 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes maxDNN, a computationally efficient convolution kernel
for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%
computational efficiency on typical deep learning network architectures. The
design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We
only address forward propagation (FPROP) operation of the network, but we
believe that the same techniques used here will be effective for backward
propagation (BPROP) as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06647</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06647</id><created>2015-01-26</created><authors><author><keyname>Zhang</keyname><forenames>Zijie</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Anderson</keyname><forenames>Brian</forenames></author></authors><title>Energy Efficient Broadcast in Mobile Networks Subject to Channel
  Randomness</title><categories>cs.NI</categories><comments>accepted to appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication in a network of mobile devices is a challenging and
resource demanding task, due to the highly dynamic network topology and the
wireless channel randomness. This paper investigates information broadcast
schemes in 2D mobile ad-hoc networks where nodes are initially randomly
distributed and then move following a random direction mobility model. Based on
an in-depth analysis of the popular Susceptible-Infectious-Recovered epidemic
broadcast scheme, this paper proposes a novel energy and bandwidth efficient
broadcast scheme, named the energy-efficient broadcast scheme, which is able to
adapt to fast-changing network topology and channel randomness. Analytical
results are provided to characterize the performance of the proposed scheme,
including the fraction of nodes that can receive the information and the delay
of the information dissemination process. The accuracy of analytical results is
verified using simulations driven by both the random direction mobility model
and a real world trace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06654</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06654</id><created>2015-01-27</created><authors><author><keyname>Ahmed</keyname><forenames>Ali</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Compressive Sampling of Ensembles of Correlated Signals</title><categories>cs.IT math.IT</categories><comments>40 pages, 10 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose several sampling architectures for the efficient acquisition of an
ensemble of correlated signals. We show that without prior knowledge of the
correlation structure, each of our architectures (under different sets of
assumptions) can acquire the ensemble at a sub-Nyquist rate. Prior to sampling,
the analog signals are diversified using simple, implementable components. The
diversification is achieved by injecting types of &quot;structured randomness&quot; into
the ensemble, the result of which is subsampled. For reconstruction, the
ensemble is modeled as a low-rank matrix that we have observed through an
(undetermined) set of linear equations. Our main results show that this matrix
can be recovered using standard convex programming techniques when the total
number of samples is on the order of the intrinsic degree of freedom of the
ensemble --- the more heavily correlated the ensemble, the fewer samples are
needed.
  To motivate this study, we discuss how such ensembles arise in the context of
array processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06661</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06661</id><created>2015-01-27</created><authors><author><keyname>Naidu</keyname><forenames>R. Ramu</forenames></author><author><keyname>Sastry</keyname><forenames>C. S.</forenames></author><author><keyname>Jampana</keyname><forenames>Phanindra</forenames></author></authors><title>Deterministic compressed sensing matrices: Construction via Euler
  Squares and applications</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Compressed Sensing the matrices that satisfy the Restricted Isometry
Property (RIP) play an important role. But to date, very few results for
designing such matrices are available. For applications such as multiplier-less
data compression, binary sensing matrices are of interest. The present work
constructs deterministic and binary sensing matrices using Euler Squares. In
particular, given a positive integer $m$ different from $p, p^2$ for a prime
$p$, we show that it is possible to construct a binary sensing matrix of size
$m \times c (m\mu)^2$, where $\mu$ is the coherence parameter of the matrix and
$c \in [1,2)$. The matrices that we construct have smaller density (that is,
percentage of nonzero entries in the matrix is small) with no function
evaluation in their construction, which support algorithms with low
computational complexity. Through experimental work, we show that our binary
sensing matrices can be used for such applications as content based image
retrieval. Our simulation results demonstrate that the Euler Square based CS
matrices give better performance than their Gaussian counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06662</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06662</id><created>2015-01-27</created><authors><author><keyname>Sasidharan</keyname><forenames>Birenjith</forenames></author><author><keyname>Agarwal</keyname><forenames>Gaurav Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>A High-Rate MSR Code With Polynomial Sub-Packetization Level</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a high-rate $(n,k,d=n-1)$-MSR code with a sub-packetization level
that is polynomial in the dimension $k$ of the code. While polynomial
sub-packetization level was achieved earlier for vector MDS codes that repair
systematic nodes optimally, no such MSR code construction is known. In the
low-rate regime (i. e., rates less than one-half), MSR code constructions with
a linear sub-packetization level are available. But in the high-rate regime (i.
e., rates greater than one-half), the known MSR code constructions required a
sub-packetization level that is exponential in $k$. In the present paper, we
construct an MSR code for $d=n-1$ with a fixed rate $R=\frac{t-1}{t}, \ t \geq
2,$ achieveing a sub-packetization level $\alpha = O(k^t)$. The code allows
help-by-transfer repair, i. e., no computations are needed at the helper nodes
during repair of a failed node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06663</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06663</id><created>2015-01-27</created><authors><author><keyname>Tian</keyname><forenames>Yun</forenames></author><author><keyname>Xu</keyname><forenames>Bojian</forenames></author></authors><title>On Longest Repeat Queries Using GPU</title><categories>cs.DC</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repeat finding in strings has important applications in subfields such as
computational biology. The challenge of finding the longest repeats covering
particular string positions was recently proposed and solved by \.{I}leri et
al., using a total of the optimal $O(n)$ time and space, where $n$ is the
string size. However, their solution can only find the \emph{leftmost} longest
repeat for each of the $n$ string position. It is also not known how to
parallelize their solution. In this paper, we propose a new solution for
longest repeat finding, which although is theoretically suboptimal in time but
is conceptually simpler and works faster and uses less memory space in practice
than the optimal solution. Further, our solution can find \emph{all} longest
repeats of every string position, while still maintaining a faster processing
speed and less memory space usage. Moreover, our solution is
\emph{parallelizable} in the shared memory architecture (SMA), enabling it to
take advantage of the modern multi-processor computing platforms such as the
general-purpose graphics processing units (GPU). We have implemented both the
sequential and parallel versions of our solution. Experiments with both
biological and non-biological data show that our sequential and parallel
solutions are faster than the optimal solution by a factor of 2--3.5 and 6--14,
respectively, and use less memory space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06671</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06671</id><created>2015-01-27</created><updated>2015-04-15</updated><authors><author><keyname>Ben-Yishai</keyname><forenames>Assaf</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>The Gaussian Channel with Noisy Feedback: Improving Reliability via
  Interaction</title><categories>cs.IT math.IT</categories><comments>Accepted for ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a pair of terminals connected by two independent (feedforward and
feedback) Additive White Gaussian Noise (AWGN) channels, and limited by
individual power constraints. The first terminal would like to reliably send
information to the second terminal at a given rate. While the reliability in
the cases of no feedback and of noiseless feedback is well studied, not much is
known about the case of noisy feedback. In this work, we present an interactive
scheme that significantly improves the reliability relative to the no-feedback
setting, whenever the feedback Signal to Noise Ratio (SNR) is sufficiently
larger than the feedforward SNR. The scheme combines Schalkwijk-Kailath (S-K)
coding and modulo--lattice analog transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06678</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06678</id><created>2015-01-27</created><updated>2016-01-28</updated><authors><author><keyname>Zeng</keyname><forenames>Zhiwen</forenames></author><author><keyname>Wang</keyname><forenames>Xiangke</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiqiang</forenames></author></authors><title>Edge Agreement of Multi-agent System with Quantized Measurements via the
  Directed Edge Laplacian</title><categories>cs.SY cs.MA</categories><comments>16 pages, 10 figures; Round2, revised to IET Control Theory &amp;
  Applications, 2016</comments><msc-class>93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores the edge agreement problem of second-order nonlinear
multi-agent system under quantized measurements. Under the edge agreement
framework, we introduce an important concept about the \emph{essential edge
Laplacian} and also obtain a reduced model of the edge agreement dynamics based
on the spanning tree subgraph. The quantized edge agreement problem of
second-order nonlinear multi-agent system is studied, in which both uniform and
logarithmic quantizers are considered. We do not only guarantee the stability
of the proposed quantized control law, but also reveal the explicit
mathematical connection of the quantized interval and the convergence
properties for both uniform and logarithmic quantizers, which has not been
addressed before. Particularly, for uniform quantizers, we provide the upper
bound of the radius of the agreement neighborhood and indicate that the radius
increases with the quantization interval. While for logarithmic quantizers, the
agents converge exponentially to the desired agreement equilibrium. In
addition, we figure out the relationship of the quantization interval and the
convergence speed and also provide the estimates of the convergence rate.
Finally, simulation results are given to verify the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06683</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06683</id><created>2015-01-27</created><authors><author><keyname>Sasidharan</keyname><forenames>Birenjith</forenames></author><author><keyname>Agarwal</keyname><forenames>Gaurav Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>Codes With Hierarchical Locality</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the notion of {\em codes with hierarchical locality}
that is identified as another approach to local recovery from multiple
erasures. The well-known class of {\em codes with locality} is said to possess
hierarchical locality with a single level. In a {\em code with two-level
hierarchical locality}, every symbol is protected by an inner-most local code,
and another middle-level code of larger dimension containing the local code. We
first consider codes with two levels of hierarchical locality, derive an upper
bound on the minimum distance, and provide optimal code constructions of low
field-size under certain parameter sets. Subsequently, we generalize both the
bound and the constructions to hierarchical locality of arbitrary levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06686</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06686</id><created>2015-01-27</created><authors><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author></authors><title>Reduced Complexity Decoding of n x n Algebraic Space-Time Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to Workshop on Coding and Cryptography (WCC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic space-time coding allows for reliable data exchange across fading
multiple-input multiple-output channels. A powerful technique for decoding
space-time codes in Maximum-Likelihood (ML) decoding, but well-performing and
widely-used codes such as the Golden code often suffer from high ML-decoding
complexity. In this article, a recursive algorithm for decoding general
algebraic space-time codes of arbitrary dimension is proposed, which reduces
the worst-case decoding complexity from $O(|S|^{n^2})$ to $O(|S|^n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06689</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06689</id><created>2015-01-27</created><authors><author><keyname>Zinn</keyname><forenames>Daniel</forenames></author></authors><title>General-Purpose Join Algorithms for Listing Triangles in Large Graphs</title><categories>cs.DB</categories><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate applying general-purpose join algorithms to the triangle
listing problem in an out-of-core context. In particular, we focus on Leapfrog
Triejoin (LFTJ) by Veldhuizen 2014, a recently proposed, worst-case optimal
algorithm. We present &quot;boxing&quot;: a novel, yet conceptually simple, approach for
feeding input data to LFTJ. Our extensive analysis shows that this approach is
I/O efficient, being worst-case optimal (in a certain sense). Furthermore, if
input data is only a constant factor larger than the available memory, then a
boxed LFTJ essentially maintains the CPU data-complexity of the vanilla LFTJ.
Next, focusing on LFTJ applied to the triangle query, we show that for many
graphs boxed LFTJ matches the I/O complexity of the recently by Hu, Tao and
Yufei proposed specialized algorithm MGT for listing tiangles in an out-of-core
setting. We also strengthen the analysis of LFTJ's computational complexity for
the triangle query by considering families of input graphs that are
characterized not only by the number of edges but also by a measure of their
density. E.g., we show that LFTJ achieves a CPU complexity of O(|E|log|E|) for
planar graphs, while on general graphs, no algorithm can be faster than
O(|E|^{1.5}). Finally, we perform an experimental evaluation for the triangle
listing problem confirming our theoretical results and showing the overall
effectiveness of our approach. On all our real-world and synthetic data sets
(some of which containing more than 1.2 billion edges) LFTJ in single-threaded
mode is within a factor of 3 of the specialized MGT; a penalty that---as we
demonstrate---can be alleviated by parallelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06698</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06698</id><created>2015-01-27</created><authors><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>M&#xfc;elich</keyname><forenames>Sven</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author><author><keyname>Hiller</keyname><forenames>Matthias</forenames></author><author><keyname>Sigl</keyname><forenames>Georg</forenames></author></authors><title>On Error Correction for Physical Unclonable Functions</title><categories>cs.IT math.IT</categories><comments>6 pages, accepted at 10th International ITG Conference on Systems,
  Communications and Coding, Hamburg, Germany, February 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical Unclonable Functions evaluate manufacturing variations to generate
secure cryptographic keys for embedded systems without secure key storage. It
is explained how methods from coding theory are applied in order to ensure
reliable key reproduction. We show how better results can be obtained using
code classes and decoding principles not used for this scenario before. These
methods are exemplified by specific code constructions which improve existing
codes with respect to error probability, decoding complexity and codeword
length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06705</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06705</id><created>2015-01-27</created><authors><author><keyname>Attiaoui</keyname><forenames>Dorra</forenames><affiliation>IRISA</affiliation></author><author><keyname>Dor&#xe9;</keyname><forenames>Pierre-Emmanuel</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Inclusion within Continuous Belief Functions</title><categories>cs.AI</categories><comments>International Conference on Information Fusion - (FUSION 2013), Jul
  2013, Istanbul, Turkey</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defining and modeling the relation of inclusion between continuous belief
function may be considered as an important operation in order to study their
behaviors. Within this paper we will propose and present two forms of
inclusion: The strict and the partial one. In order to develop this relation,
we will study the case of consonant belief function. To do so, we will simulate
normal distributions allowing us to model and analyze these relations. Based on
that, we will determine the parameters influencing and characterizing the two
forms of inclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06715</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06715</id><created>2015-01-27</created><authors><author><keyname>De Maio</keyname><forenames>Carmen</forenames></author><author><keyname>Fenza</keyname><forenames>Giuseppe</forenames></author><author><keyname>Loia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Parente</keyname><forenames>Mimmo</forenames></author></authors><title>Time Aware Knowledge Extraction for Microblog Summarization on Twitter</title><categories>cs.IR</categories><comments>33 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microblogging services like Twitter and Facebook collect millions of user
generated content every moment about trending news, occurring events, and so
on. Nevertheless, it is really a nightmare to find information of interest
through the huge amount of available posts that are often noise and redundant.
In general, social media analytics services have caught increasing attention
from both side research and industry. Specifically, the dynamic context of
microblogging requires to manage not only meaning of information but also the
evolution of knowledge over the timeline. This work defines Time Aware
Knowledge Extraction (briefly TAKE) methodology that relies on temporal
extension of Fuzzy Formal Concept Analysis. In particular, a microblog
summarization algorithm has been defined filtering the concepts organized by
TAKE in a time-dependent hierarchy. The algorithm addresses topic-based
summarization on Twitter. Besides considering the timing of the concepts,
another distinguish feature of the proposed microblog summarization framework
is the possibility to have more or less detailed summary, according to the
user's needs, with good levels of quality and completeness as highlighted in
the experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06716</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06716</id><created>2015-01-27</created><authors><author><keyname>Kushnir</keyname><forenames>Maria</forenames></author><author><keyname>Shimshoni</keyname><forenames>Ilan</forenames></author></authors><title>A General Preprocessing Method for Improved Performance of Epipolar
  Geometry Estimation Algorithms</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a deterministic preprocessing algorithm is presented, whose
output can be given as input to most state-of-the-art epipolar geometry
estimation algorithms, improving their results considerably. They are now able
to succeed on hard cases for which they failed before. The algorithm consists
of three steps, whose scope changes from local to global. In the local step it
extracts from a pair of images local features (e.g. SIFT). Similar features
from each image are clustered and the clusters are matched yielding a large
number of putative matches. In the second step pairs of spatially close
features (called 2keypoints) are matched and ranked by a classifier. The
2keypoint matches with the highest ranks are selected. In the global step, from
each two 2keypoint matches a fundamental matrix is computed. As quite a few of
the matrices are generated from correct matches they are used to rank the
putative matches found in the first step. For each match the number of
fundamental matrices, for which it approximately satisfies the epipolar
constraint, is calculated. This set of matches is combined with the putative
matches generated by standard methods and their probabilities to be correct are
estimated by a classifier. These are then given as input to state-of-the-art
epipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yielding
much better results than the original algorithms. This was shown in extensive
testing performed on almost 900 image pairs from six publicly available
data-sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06721</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06721</id><created>2015-01-27</created><updated>2015-08-12</updated><authors><author><keyname>Krzywicki</keyname><forenames>D.</forenames></author><author><keyname>Turek</keyname><forenames>W.</forenames></author><author><keyname>Byrski</keyname><forenames>A.</forenames></author><author><keyname>Kisiel-Dorohinicki</keyname><forenames>M.</forenames></author></authors><title>Massively-concurrent Agent-based Evolutionary Computing</title><categories>cs.MA cs.NE</categories><comments>Journal of Computational Science, Available online 29 July 2015</comments><doi>10.1016/j.jocs.2015.07.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fusion of the multi-agent paradigm with evolutionary computation yielded
promising results in many optimization problems. Evolutionary multi-agent
system (EMAS) are more similar to biological evolution than classical
evolutionary algorithms. However, technological limitations prevented the use
of fully asynchronous agents in previous EMAS implementations. In this paper we
present a new algorithm for agent-based evolutionary computations. The
individuals are represented as fully autonomous and asynchronous agents. An
efficient implementation of this algorithm was possible through the use of
modern technologies based on functional languages (namely Erlang and Scala),
which natively support lightweight processes and asynchronous communication.
Our experiments show that such an asynchronous approach is both faster and more
efficient in solving common optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06722</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06722</id><created>2015-01-27</created><authors><author><keyname>Popa</keyname><forenames>Alin-Ionut</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Parametric Image Segmentation of Humans with Structural Shape Priors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The figure-ground segmentation of humans in images captured in natural
environments is an outstanding open problem due to the presence of complex
backgrounds, articulation, varying body proportions, partial views and
viewpoint changes. In this work we propose class-specific segmentation models
that leverage parametric max-flow image segmentation and a large dataset of
human shapes. Our contributions are as follows: (1) formulation of a
sub-modular energy model that combines class-specific structural constraints
and data-driven shape priors, within a parametric max-flow optimization
methodology that systematically computes all breakpoints of the model in
polynomial time; (2) design of a data-driven class-specific fusion methodology,
based on matching against a large training set of exemplar human shapes
(100,000 in our experiments), that allows the shape prior to be constructed
on-the-fly, for arbitrary viewpoints and partial views. (3) demonstration of
state of the art results, in two challenging datasets, H3D and MPII (where
figure-ground segmentation annotations have been added by us), where we
substantially improve on the first ranked hypothesis estimates of mid-level
segmentation methods, by 20%, with hypothesis set sizes that are up to one
order of magnitude smaller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06727</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06727</id><created>2015-01-27</created><updated>2015-11-18</updated><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Jose M.</forenames></author></authors><title>Factorization, Inference and Parameter Learning in Discrete AMP Chain
  Graphs</title><categories>stat.ML cs.AI</categories><journal-ref>Proceedings of the 13th European Conference on Symbolic and
  Quantitative Approaches to Reasoning under Uncertainty (ECSQARU 2015),
  Lecture Notes in Artificial Intelligence 9161, 335-345</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address some computational issues that may hinder the use of AMP chain
graphs in practice. Specifically, we show how a discrete probability
distribution that satisfies all the independencies represented by an AMP chain
graph factorizes according to it. We show how this factorization makes it
possible to perform inference and parameter learning efficiently, by adapting
existing algorithms for Markov and Bayesian networks. Finally, we turn our
attention to another issue that may hinder the use of AMP CGs, namely the lack
of an intuitive interpretation of their edges. We provide one such
interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06729</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06729</id><created>2015-01-27</created><updated>2015-09-30</updated><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>The Complexity of the Kth Largest Subset Problem and Related Problems</title><categories>cs.CC</categories><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Kth largest subset problem and the Kth largest m-tuple
problem are in PP and hard for PP under polynomial-time Turing reductions.
Several problems from the literature were previously shown NP-hard via
reductions from those two problems, and by our main result they become PP-hard
as well. We also provide complementary PP-upper bounds for some of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06736</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06736</id><created>2015-01-27</created><authors><author><keyname>Fukushima</keyname><forenames>Masaru</forenames></author><author><keyname>Okazaki</keyname><forenames>Takuya</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author></authors><title>Spatially-Coupled MacKay-Neal Codes Universally Achieve the Symmetric
  Information Rate of Arbitrary Generalized Erasure Channels with Memory</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the belief propagation decoding of spatially-coupled
MacKay-Neal (SC-MN) codes over erasure channels with memory. We show that SC-MN
codes with bounded degree universally achieve the symmetric information rate
(SIR) of arbitrary erasure channels with memory. We mean by universality the
following sense: the sender does not need to know the whole channel statistics
but needs to know only the SIR, while the receiver estimates the transmitted
codewords from channel statistics and received words. The proof is based on
potential function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06741</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06741</id><created>2015-01-27</created><authors><author><keyname>Beer</keyname><forenames>Gernot</forenames></author><author><keyname>Marussig</keyname><forenames>Benjamin</forenames></author><author><keyname>Zechner</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>A simple approach to the numerical simulation with trimmed CAD surfaces</title><categories>cs.NA math.NA</categories><comments>20 pages and 16 figures</comments><journal-ref>Computer Methods in Applied Mechanics and Engineering, Volume 285,
  1 March 2015, Pages 776-790</journal-ref><doi>10.1016/j.cma.2014.12.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work a novel method for the analysis with trimmed CAD surfaces is
presented. The method involves an additional mapping step and the attraction
stems from its sim- plicity and ease of implementation into existing Finite
Element (FEM) or Boundary Element (BEM) software. The method is first verified
with classical test examples in structural mechanics. Then two practical
applications are presented one using the FEM, the other the BEM, that show the
applicability of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06743</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06743</id><created>2015-01-27</created><authors><author><keyname>Alwan</keyname><forenames>Esraa</forenames></author><author><keyname>Fitch</keyname><forenames>John</forenames></author><author><keyname>Padget</keyname><forenames>Julian</forenames></author></authors><title>Enhancing the performance of Decoupled Software Pipeline through
  Backward Slicing</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapidly increasing number of cores available in multicore processors does
not necessarily lead directly to a commensurate increase in performance:
programs written in conventional languages, such as C, need careful
restructuring, preferably automatically, before the benefits can be observed in
improved run-times. Even then, much depends upon the intrinsic capacity of the
original program for concurrent execution. The subject of this paper is the
performance gains from the combined effect of the complementary techniques of
the Decoupled Software Pipeline (DSWP) and (backward) slicing. DSWP extracts
threadlevel parallelism from the body of a loop by breaking it into stages
which are then executed pipeline style: in effect cutting across the control
chain. Slicing, on the other hand, cuts the program along the control chain,
teasing out finer threads that depend on different variables (or locations).
parts that depend on different variables. The main contribution of this paper
is to demonstrate that the application of DSWP, followed by slicing offers
notable improvements over DSWP alone, especially when there is a loop-carried
dependence that prevents the application of the simpler DOALL optimization.
Experimental results show an improvement of a factor of ?1.6 for DSWP + slicing
over DSWP alone and a factor of ?2.4 for DSWP + slicing over the original
sequential code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06751</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06751</id><created>2015-01-27</created><authors><author><keyname>Ginzburg</keyname><forenames>Chaim</forenames></author><author><keyname>Raphael</keyname><forenames>Amit</forenames></author><author><keyname>Weinshall</keyname><forenames>Daphna</forenames></author></authors><title>A Cheap System for Vehicle Speed Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reliable detection of speed of moving vehicles is considered key to
traffic law enforcement in most countries, and is seen by many as an important
tool to reduce the number of traffic accidents and fatalities. Many automatic
systems and different methods are employed in different countries, but as a
rule they tend to be expensive and/or labor intensive, often employing outdated
technology due to the long development time. Here we describe a speed detection
system that relies on simple everyday equipment - a laptop and a consumer web
camera. Our method is based on tracking the license plates of cars, which gives
the relative movement of the cars in the image. This image displacement is
translated to actual motion by using the method of projection to a reference
plane, where the reference plane is the road itself. However, since license
plates do not touch the road, we must compensate for the entailed distortion in
speed measurement. We show how to compute the compensation factor using
knowledge of the license plate standard dimensions. Consequently our system
computes the true speed of moving vehicles fast and accurately. We show
promising results on videos obtained in a number of scenes and with different
car models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06758</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06758</id><created>2015-01-27</created><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Korach</keyname><forenames>Ephraim</forenames></author><author><keyname>Sharma</keyname><forenames>Shantanu</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author></authors><title>Assignment of Different-Sized Inputs in MapReduce</title><categories>cs.DB</categories><comments>Brief announcement in International Symposium on Distributed
  Computing (DISC), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A MapReduce algorithm can be described by a mapping schema, which assigns
inputs to a set of reducers, such that for each required output there exists a
reducer that receives all the inputs that participate in the computation of
this output. Reducers have a capacity, which limits the sets of inputs that
they can be assigned. However, individual inputs may vary in terms of size. We
consider, for the first time, mapping schemas where input sizes are part of the
considerations and restrictions. One of the significant parameters to optimize
in any MapReduce job is communication cost between the map and reduce phases.
The communication cost can be optimized by minimizing the number of copies of
inputs sent to the reducers. The communication cost is closely related to the
number of reducers of constrained capacity that are used to accommodate
appropriately the inputs, so that the requirement of how the inputs must meet
in a reducer is satisfied. In this work, we consider a family of problems where
it is required that each input meets with each other input in at least one
reducer. We also consider a slightly different family of problems in which,
each input of a set, X, is required to meet each input of another set, Y, in at
least one reducer. We prove that finding an optimal mapping schema for these
families of problem is NP-hard, and present several approximation algorithms
for finding a near optimal mapping schema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06769</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06769</id><created>2015-01-27</created><updated>2015-02-09</updated><authors><author><keyname>van de Meent</keyname><forenames>Jan-Willem</forenames></author><author><keyname>Yang</keyname><forenames>Hongseok</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Particle Gibbs with Ancestor Sampling for Probabilistic Programs</title><categories>stat.ML cs.AI cs.PL</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle Markov chain Monte Carlo techniques rank among current
state-of-the-art methods for probabilistic program inference. A drawback of
these techniques is that they rely on importance resampling, which results in
degenerate particle trajectories and a low effective sample size for variables
sampled early in a program. We here develop a formalism to adapt ancestor
resampling, a technique that mitigates particle degeneracy, to the
probabilistic programming setting. We present empirical results that
demonstrate nontrivial performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06774</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06774</id><created>2015-01-27</created><authors><author><keyname>Lins</keyname><forenames>Lauro</forenames></author><author><keyname>Ferreira</keyname><forenames>Nivan</forenames></author><author><keyname>Freire</keyname><forenames>Juliana</forenames></author><author><keyname>Silva</keyname><forenames>Claudio</forenames></author></authors><title>Maximum Common Subelement Metrics and its Applications to Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we characterize a mathematical model called Maximum Common
Subelement (MCS) Model and prove the existence of four different metrics on
such model. We generalize metrics on graphs previously proposed in the
literature and identify new ones by showing three different examples of MCS
Models on graphs based on (1) subgraphs, (2) induced subgraphs and (3) an
extended notion of subgraphs. This latter example can be used to model graphs
with complex labels (e.g., graphs whose labels are other graphs), and hence to
derive metrics on them. Furthermore, we also use (3) to show that graph edit
distance, when a metric, is related to a maximum common subelement in a
corresponding MCS Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06781</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06781</id><created>2015-01-27</created><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Error and Erasure Exponents for the Broadcast Channel with Degraded
  Message Sets</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error and erasure exponents for the broadcast channel with degraded message
sets are analyzed. The focus of our error probability analysis is on the main
receiver where, nominally, both messages are to be decoded. A two-step decoding
algorithm is proposed and analyzed. This receiver first attempts to decode both
messages, failing which, it attempts to decode only the message representing
the coarser information, i.e., the cloud center. This algorithm reflects the
intuition that we should decode both messages only if we have confidence in the
estimates; otherwise one should only decode the coarser information. The
resulting error and erasure exponents, derived using the method of types, are
expressed in terms of a penalized form of the modified random coding error
exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06783</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06783</id><created>2015-01-27</created><updated>2015-04-23</updated><authors><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment L.</forenames></author></authors><title>Big Data on the Rise: Testing monotonicity of distributions</title><categories>cs.DS cs.DM math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of property testing of probability distributions, or distribution
testing, aims to provide fast and (most likely) correct answers to questions
pertaining to specific aspects of very large datasets. In this work, we
consider a property of particular interest, monotonicity of distributions. We
focus on the complexity of monotonicity testing across different models of
access to the distributions; and obtain results in these new settings that
differ significantly from the known bounds in the standard sampling model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06789</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06789</id><created>2015-01-27</created><authors><author><keyname>Wagner</keyname><forenames>Caroline S.</forenames></author><author><keyname>Horlings</keyname><forenames>Edwin</forenames></author><author><keyname>Dutta</keyname><forenames>Arindum</forenames></author></authors><title>Can Science and Technology Capacity be Measured?</title><categories>stat.AP cs.DL physics.soc-ph</categories><comments>Updates a RAND 2001 report: Science and Technology Collaboration:
  Building Capacity in Developing Countries? MR-135, Santa Monica, CA</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The ability of a nation to participate in the global knowledge economy
depends to some extent on its capacities in science and technology. In an
effort to assess the capacity of different countries in science and technology,
this article updates a classification scheme developed by RAND to measure
science and technology capacity for 150 countries of the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06794</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06794</id><created>2015-01-27</created><authors><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Muandet</keyname><forenames>Krikamol</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author><author><keyname>Peters</keyname><forenames>Jonas</forenames></author></authors><title>Computing Functions of Random Variables via Reproducing Kernel Hilbert
  Space Representations</title><categories>stat.ML cs.DS cs.LG</categories><acm-class>G.3; I.2.6; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method to perform functional operations on probability
distributions of random variables. The method uses reproducing kernel Hilbert
space representations of probability distributions, and it is applicable to all
operations which can be applied to points drawn from the respective
distributions. We refer to our approach as {\em kernel probabilistic
programming}. We illustrate it on synthetic data, and show how it can be used
for nonparametric structural equation models, with an application to causal
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06802</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06802</id><created>2015-01-27</created><authors><author><keyname>Said</keyname><forenames>Gamal Abd El-Nasser A.</forenames></author><author><keyname>El-Horbaty</keyname><forenames>El-Sayed M.</forenames></author></authors><title>A Simulation Modeling Approach for Optimization of Storage Space
  Allocation in Container Terminal</title><categories>cs.OH</categories><comments>International Journal of Computer, Information, Systems and Control
  Engineering Vol:9 No:1, 2015</comments><journal-ref>Information, Systems and Control Engineering Vol. 9, No. 1, 2015,
  pp. 168-173</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Container handling problems at container terminals are NP-hard problems. This
paper presents an approach using discrete-event simulation modeling to optimize
solution for storage space allocation problem, taking into account all various
interrelated container terminal handling activities. The proposed approach is
applied on a real case study data of container terminal at Alexandria port. The
computational results show the effectiveness of the proposed model for
optimization of storage space allocation in container terminal where 54%
reduction in containers handling time in port is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06809</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06809</id><created>2015-01-27</created><updated>2015-06-07</updated><authors><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Ba&#xf1;os</keyname><forenames>Raquel A.</forenames></author><author><keyname>Gracia-L&#xe1;zaro</keyname><forenames>Carlos</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>The nested assembly of collective attention in online social systems</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>Main text + Supplementary Information (merged)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current state-of-the-art approaches to collective attention and information
life-cycle are aimed to prove how topics arise and fade in well-defined
dynamical classes, with either the accent placed on human temporal dynamics or
on semiotic dynamics. Both frameworks, however, neglect agent-symbol (memes,
hashtags) interactions: the originally rich bipartite representation of data is
collapsed onto one of the dimensions of the problem, providing a partial
understanding of how attention consensus builds up around a certain topic. On
the other hand, recent works have highlighted the competitive nature of
information systems, in which humans and memes strive for scarce resources
--visibility and attention, respectively. Here, we apply an ecosystems approach
to show that such a competitive environment drives the emergence of
modular-to-nested architectures. Specifically, we consider information-driven
systems, such as online social platforms, as mutualistic networks, in which
users and memes (the nodes of the bipartite graph) compete for resources within
their respective classes, and cooperate without. This setting yields
longitudinal observation of initially modular, then nested structures for
different types of topics from large, public collections of online time-stamped
microblogging data. More importantly, intensive numerical simulations allow us
to explain the driving force underlying the observed modular-to-nested
transition, under the logic of a dynamical model of mutualistic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06813</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06813</id><created>2015-01-27</created><authors><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Staals</keyname><forenames>Frank</forenames></author></authors><title>Mixed Map Labeling</title><categories>cs.CG</categories><comments>Full version for the paper accepted at CIAC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point feature map labeling is a geometric problem, in which a set of input
points must be labeled with a set of disjoint rectangles (the bounding boxes of
the label texts). Typically, labeling models either use internal labels, which
must touch their feature point, or external (boundary) labels, which are placed
on one of the four sides of the input points' bounding box and which are
connected to their feature points by crossing-free leader lines. In this paper
we study polynomial-time algorithms for maximizing the number of internal
labels in a mixed labeling model that combines internal and external labels.
The model requires that all leaders are parallel to a given orientation $\theta
\in [0,2\pi)$, whose value influences the geometric properties and hence the
running times of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06814</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06814</id><created>2015-01-27</created><updated>2015-07-17</updated><authors><author><keyname>Rossi</keyname><forenames>Luca</forenames></author><author><keyname>Walker</keyname><forenames>James</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Spatio-Temporal Techniques for User Identification by means of GPS
  Mobility Data</title><categories>cs.CR cs.CY physics.data-an</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the greatest concerns related to the popularity of GPS-enabled devices
and applications is the increasing availability of the personal location
information generated by them and shared with application and service
providers. Moreover, people tend to have regular routines and be characterized
by a set of &quot;significant places&quot;, thus making it possible to identify a user
from his/her mobility data.
  In this paper we present a series of techniques for identifying individuals
from their GPS movements. More specifically, we study the uniqueness of GPS
information for three popular datasets, and we provide a detailed analysis of
the discriminatory power of speed, direction and distance of travel. Most
importantly, we present a simple yet effective technique for the identification
of users from location information that are not included in the original
dataset used for training, thus raising important privacy concerns for the
management of location datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06831</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06831</id><created>2015-01-27</created><updated>2015-07-04</updated><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>CARTE</affiliation></author></authors><title>Aperiodic Subshifts of Finite Type on Groups</title><categories>math.GR cs.DM math.DS</categories><comments>New version. Adding results about monster groups</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove the following results:
  $\bullet$ If a finitely presented group $G$ admits a strongly aperiodic SFT,
then $G$ has decidable word problem. More generally, for f.g. groups that are
not recursively presented, there exists a computable obstruction for them to
admit strongly aperiodic SFTs.
  $\bullet$ On the positive side, we build strongly aperiodic SFTs on some new
classes of groups. We show in particular that some particular monster groups
admits strongly aperiodic SFTs for trivial reasons. Then, for a large class of
group $G$, we show how to build strongly aperiodic SFTs over $\mathbb{Z}\times
G$. In particular, this is true for the free group with 2 generators,
Thompson's groups $T$ and $V$, $PSL_2(\mathbb{Z})$ and any f.g. group of
rational matrices which is bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06835</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06835</id><created>2015-01-27</created><authors><author><keyname>Zuev</keyname><forenames>Konstantin</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Emergence of Soft Communities from Geometric Preferential Attachment</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>10 pages, 6 figures</comments><journal-ref>Nature Scientific Reports, v.5, p.9421, 2015</journal-ref><doi>10.1038/srep09421</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All real networks are different, but many have some structural properties in
common. There seems to be no consensus on what the most common properties are,
but scale-free degree distributions, strong clustering, and community structure
are frequently mentioned without question. Surprisingly, there exists no simple
generative mechanism explaining all the three properties at once in growing
networks. Here we show how latent network geometry coupled with preferential
attachment of nodes to this geometry fills this gap. We call this mechanism
geometric preferential attachment (GPA), and validate it against the Internet.
GPA gives rise to soft communities that provide a different perspective on the
community structure in networks. The connections between GPA and cosmological
models, including inflation, are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06841</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06841</id><created>2015-01-27</created><updated>2015-04-24</updated><authors><author><keyname>Balle</keyname><forenames>Borja</forenames></author><author><keyname>Panangaden</keyname><forenames>Prakash</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>A Canonical Form for Weighted Automata and Applications to Approximate
  Minimization</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing approximations to a weighted automaton.
Weighted finite automata (WFA) are closely related to the theory of rational
series. A rational series is a function from strings to real numbers that can
be computed by a finite WFA. Among others, this includes probability
distributions generated by hidden Markov models and probabilistic automata. The
relationship between rational series and WFA is analogous to the relationship
between regular languages and ordinary automata. Associated with such rational
series are infinite matrices called Hankel matrices which play a fundamental
role in the theory of minimal WFA. Our contributions are: (1) an effective
procedure for computing the singular value decomposition (SVD) of such infinite
Hankel matrices based on their representation in terms of finite WFA; (2) a new
canonical form for finite WFA based on this SVD decomposition; and, (3) an
algorithm to construct approximate minimizations of a given WFA. The goal of
our approximate minimization algorithm is to start from a minimal WFA and
produce a smaller WFA that is close to the given one in a certain sense. The
desired size of the approximating automaton is given as input. We give bounds
describing how well the approximation emulates the behavior of the original
WFA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06845</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06845</id><created>2015-01-26</created><updated>2015-05-07</updated><authors><author><keyname>Chen</keyname><forenames>Yu</forenames></author></authors><title>Exact Solution for One Type of Lindley's Equation for Queueing Theory
  and Network Calculus</title><categories>stat.AP cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lindley's equation is an important relation in queueing theory and network
calculus. In this paper, we develop a new method to solve one type of Lindley's
equation, i.e., the equation V(s)T(-s)-1=0 only has finite negative real roots.
V(s) and T(-s) are the Laplace transforms of service time's probability density
function (PDF) and interarrival time's PDF (evaluated at -s). For queueing
theory, we use this method to derive the exact M/M/1, M/H2/1 and M/E2/1
waiting-time distributions, and for the first time find the exact D/M/1
waiting-time distribution. For network calculus, we use two examples to compare
our method with the effective bandwidth model and its dual, the effective
capacity model, respectively. We observe that the distribution function of
backlog size in the first example can be obtained exactly by our method and
partially by the effective bandwidth model; however, such a distribution
function in the second example cannot be obtained by our method but can be
approximated by the effective capacity model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06851</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06851</id><created>2015-01-27</created><authors><author><keyname>Ahmad</keyname><forenames>Syed Amaar</forenames></author><author><keyname>Datla</keyname><forenames>Dinesh</forenames></author></authors><title>Distributed Power Allocations in Heterogeneous Networks with Dual
  Connectivity using Backhaul State Information</title><categories>cs.IT math.IT</categories><comments>9 pages 9 figures; submitted to IEEE Transactions on Wireless
  Communications; Key Terms: Waterfilling, Energy-efficiency, HetNets, Small
  cells, Gaussian interference channel, Cross-layer design</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE release 12 proposes the use of dual connectivity in heterogeneous
cellular networks, where a user equipment (UE) maintains parallel connections
to a macro-cell node (base station) and to a low-tier node (pico base station
or relay). In this paper, we propose a distributed multi-objective power
control scheme where each UE independently adapts its transmit power on its
dual connections, possibly of unequal bandwidth, with non-ideal backhaul links.
In the proposed scheme, the UEs can dynamically switch their objectives between
data rate maximization and transmit power minimization as the backhaul load
varies. Given the coupling between interference and the backhaul load, we
propose a low-overhead convergence mechanism which does not require explicit
coordination between autonomous nodes and also derive a closed-form expression
of the transmit power levels at equilibrium. We illustrate a higher aggregate
end-to-end data rate and significant power saving for our scheme over when the
optimization is implemented through a greedy algorithm or when UEs only perform
waterfilling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06857</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06857</id><created>2015-01-27</created><authors><author><keyname>Merelo</keyname><forenames>J. J.</forenames></author><author><keyname>Rico</keyname><forenames>Nuria</forenames></author><author><keyname>Blancas</keyname><forenames>Israel</forenames></author><author><keyname>Arenas</keyname><forenames>M. G.</forenames></author><author><keyname>Tricas</keyname><forenames>Fernando</forenames></author><author><keyname>Vacas</keyname><forenames>Jos&#xe9; Antonio</forenames></author></authors><title>Measuring the local GitHub developer community</title><categories>cs.SI cs.SE</categories><comments>Paper supporting presentation at Floss Community Metrics conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating rankings might seem like a vain exercise in belly-button gazing,
even more so for people so unlike that kind of things as programmers. However,
in this paper we will try to prove how creating city (or province) based
rankings in Spain has led to all kind of interesting effects, including
increased productivity and community building. We describe the methodology we
have used to search for programmers residing in a particular province focusing
on those where most population is concentrated and apply different measures to
show how these communities differ in structure, number and productivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06862</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06862</id><created>2015-01-27</created><updated>2015-06-29</updated><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Rad</keyname><forenames>Tudor-Dan</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>Factorization of Rational Motions: A Survey with Examples and
  Applications</title><categories>cs.RO math.MG</categories><comments>To be published in the proceedings of The 14th IFToMM World Congress,
  Taipei, 2015</comments><msc-class>12D05, 70B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its introduction in 2012, the factorization theory for rational motions
quickly evolved and found applications in theoretical and applied mechanism
science. We provide an accessible introduction to motion factorization with
many examples, summarize recent developments and hint at some new applications.
In particular, we provide pseudo-code for the generic factorization algorithm,
demonstrate how to find a replacement linkage for a special case in the
synthesis of Bennett mechanisms and, as an example of non-generic
factorization, synthesize open chains for circular and elliptic translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06864</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06864</id><created>2015-01-27</created><updated>2015-08-20</updated><authors><author><keyname>Ling</keyname><forenames>Shuyang</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author></authors><title>Self-Calibration and Biconvex Compressive Sensing</title><categories>cs.IT math.IT</categories><doi>10.1088/0266-5611/31/11/115002</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The design of high-precision sensing devises becomes ever more difficult and
expensive. At the same time, the need for precise calibration of these devices
(ranging from tiny sensors to space telescopes) manifests itself as a major
roadblock in many scientific and technological endeavors. To achieve optimal
performance of advanced high-performance sensors one must carefully calibrate
them, which is often difficult or even impossible to do in practice. In this
work we bring together three seemingly unrelated concepts, namely
Self-Calibration, Compressive Sensing, and Biconvex Optimization. The idea
behind self-calibration is to equip a hardware device with a smart algorithm
that can compensate automatically for the lack of calibration. We show how
several self-calibration problems can be treated efficiently within the
framework of biconvex compressive sensing via a new method called SparseLift.
More specifically, we consider a linear system of equations y = DAx, where both
x and the diagonal matrix D (which models the calibration error) are unknown.
By &quot;lifting&quot; this biconvex inverse problem we arrive at a convex optimization
problem. By exploiting sparsity in the signal model, we derive explicit
theoretical guarantees under which both x and D can be recovered exactly,
robustly, and numerically efficiently via linear programming. Applications in
array calibration and wireless communications are discussed and numerical
simulations are presented, confirming and complementing our theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06873</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06873</id><created>2015-01-27</created><authors><author><keyname>M&#xed;nguez</keyname><forenames>R.</forenames></author><author><keyname>Castillo</keyname><forenames>E.</forenames></author><author><keyname>Pruneda</keyname><forenames>R.</forenames></author><author><keyname>Solares</keyname><forenames>C.</forenames></author></authors><title>Truss Analysis Discussion and Interpretation Using Linear Systems of
  Equalities and Inequalities</title><categories>cs.CE</categories><comments>39 pages and 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows the complementary roles of mathematical and engineering
points of view when dealing with truss analysis problems involving systems of
linear equations and inequalities. After the compatibility condition and the
mathematical structure of the general solution of a system of linear equations
is discussed, the truss analysis problem is used to illustrate its mathematical
and engineering multiple aspects, including an analysis of the compatibility
conditions and a physical interpretation of the general solution, and the
generators of the resulting affine space. Next, the compatibility and the
mathematical structure of the general solution of linear systems of
inequalities are analyzed and the truss analysis problem revisited adding some
inequality constraints, and discussing how they affect the resulting general
solution and many other aspects of it. Finally, some conclusions are drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06907</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06907</id><created>2015-01-26</created><authors><author><keyname>Brown</keyname><forenames>David K.</forenames></author><author><keyname>Musyoka</keyname><forenames>Thommas M.</forenames></author><author><keyname>Penkler</keyname><forenames>David L.</forenames></author><author><keyname>Bishop</keyname><forenames>&#xd6;zlem Tastan</forenames></author></authors><title>JMS: A workflow management system and web-based cluster front-end for
  the Torque resource manager</title><categories>cs.SE q-bio.BM</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Complex computational pipelines are becoming a staple of modern
scientific research. Often these pipelines are resource intensive and require
days of computing time. In such cases, it makes sense to run them over
distributed computer clusters where they can take advantage of the aggregated
resources of many powerful computers. In addition to this, researchers often
want to integrate their workflows into their own web servers. In these cases,
software is needed to manage the submission of jobs from the web interface to
the cluster and then return the results once the job has finished executing.
  Results: We have developed the Job Management System (JMS), a workflow
management system and interface for the Torque resource manager. The JMS
provides users with a user-friendly interface for creating complex workflows
with multiple stages. It integrates this workflow functionality with Torque, a
tool that is used to control and manage batch jobs on distributed computing
clusters. The JMS can be used by researchers to build and run complex
computational pipelines and provides functionality to include these pipelines
in external interfaces. The JMS is currently being used to house a number of
structural bioinformatics pipelines at the Research Unit in Bioinformatics
(RUBi) at Rhodes University.
  Availability: The JMS is an open-source project and is freely available at
https://github.com/RUBi-ZA/JMS
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06929</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06929</id><created>2015-01-27</created><authors><author><keyname>Fernandez-Bes</keyname><forenames>Jesus</forenames></author><author><keyname>Elvira</keyname><forenames>V&#xed;ctor</forenames></author><author><keyname>Van Vaerenbergh</keyname><forenames>Steven</forenames></author></authors><title>A Probabilistic Least-Mean-Squares Filter</title><categories>stat.ML cs.SY stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a probabilistic approach to the LMS filter. By means of an
efficient approximation, this approach provides an adaptable step-size LMS
algorithm together with a measure of uncertainty about the estimation. In
addition, the proposed approximation preserves the linear complexity of the
standard LMS. Numerical results show the improved performance of the algorithm
with respect to standard LMS and state-of-the-art algorithms with similar
complexity. The goal of this work, therefore, is to open the door to bring some
more Bayesian machine learning techniques to adaptive filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06941</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06941</id><created>2015-01-27</created><authors><author><keyname>Tagiew</keyname><forenames>Rustam</forenames></author></authors><title>The Economy of Internet-Based Hospitality Exchange</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 8 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze and compare general development and individual
behavior on two non-profit internet-based hospitality exchange services --
bewelcome.org and warmshowers.org. We measure the effort needed to achieve a
real-life interaction, whereby the advantages of mutual altruism arise. The
effort needed is the communication quantified in units of time. Since the
amount of effort is not obvious to individual users, the development of the
effort investing strategy is investigated. The impact of individual behavior on
general development is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06946</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06946</id><created>2015-01-27</created><authors><author><keyname>Ehlers</keyname><forenames>Thorsten</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Mike</forenames></author></authors><title>New Bounds on Optimal Sorting Networks</title><categories>cs.DM cs.DS</categories><comments>Submitted to CiE. arXiv admin note: text overlap with arXiv:1410.2736</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new parallel sorting networks for $17$ to $20$ inputs. For $17,
19,$ and $20$ inputs these new networks are faster (i.e., they require less
computation steps) than the previously known best networks. Therefore, we
improve upon the known upper bounds for minimal depth sorting networks on $17,
19,$ and $20$ channels. Furthermore, we show that our sorting network for $17$
inputs is optimal in the sense that no sorting network using less layers
exists. This solves the main open problem of [D. Bundala &amp; J. Za\'vodn\'y.
Optimal sorting networks, Proc. LATA 2014].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06954</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06954</id><created>2015-01-27</created><updated>2015-07-27</updated><authors><author><keyname>Ibrahim</keyname><forenames>Abdelrahman M.</forenames></author><author><keyname>Ercetin</keyname><forenames>Ozgur</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author></authors><title>Stability Analysis of Slotted Aloha with Opportunistic RF Energy
  Harvesting</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) is a promising technology for realizing energy
efficient wireless networks. In this paper, we utilize the ambient RF energy,
particularly interference from neighboring transmissions, to replenish the
batteries of the EH enabled nodes. However, RF energy harvesting imposes new
challenges into the analysis of wireless networks. Our objective in this work
is to investigate the performance of a slotted Aloha random access wireless
network consisting of two types of nodes, namely Type I which has unlimited
energy supply and Type II which is solely powered by an RF energy harvesting
circuit. The transmissions of a Type I node are recycled by a Type II node to
replenish its battery. We characterize an inner bound on the stable throughput
region under half-duplex and full-duplex energy harvesting paradigms as well as
for the finite capacity battery case. We present numerical results that
validate our analytical results, and demonstrate their utility for the analysis
of the exact system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06957</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06957</id><created>2015-01-27</created><updated>2015-07-06</updated><authors><author><keyname>Carvalho</keyname><forenames>Rui</forenames></author><author><keyname>Buzna</keyname><forenames>Lubos</forenames></author><author><keyname>Gibbens</keyname><forenames>Richard</forenames></author><author><keyname>Kelly</keyname><forenames>Frank</forenames></author></authors><title>Critical behaviour in charging of electric vehicles</title><categories>math.OC cs.SY physics.soc-ph</categories><comments>19 pages, 6 figures</comments><doi>10.1088/1367-2630/17/9/095001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing penetration of electric vehicles over the coming decades,
taken together with the high cost to upgrade local distribution networks and
consumer demand for home charging, suggest that managing congestion on low
voltage networks will be a crucial component of the electric vehicle revolution
and the move away from fossil fuels in transportation. Here, we model the
max-flow and proportional fairness protocols for the control of congestion
caused by a fleet of vehicles charging on two real-world distribution networks.
We show that the system undergoes a continuous phase transition to a congested
state as a function of the rate of vehicles plugging to the network to charge.
We focus on the order parameter and its fluctuations close to the phase
transition, and show that the critical point depends on the choice of
congestion protocol. Finally, we analyse the inequality in the charging times
as the vehicle arrival rate increases, and show that charging times are
considerably more equitable in proportional fairness than in max-flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06964</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06964</id><created>2015-01-27</created><authors><author><keyname>Keshavamurthy</keyname><forenames>Usha</forenames></author><author><keyname>Guruprasad</keyname><forenames>H. S.</forenames></author></authors><title>Learning Analytics: A Survey</title><categories>cs.DB</categories><acm-class>E.0</acm-class><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  Volume 18 Number 6 Dec 2014 Page 260 - 264</journal-ref><doi>10.14445/22312803/IJCTT-V18P155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning analytics is a research topic that is gaining increasing popularity
in recent time. It analyzes the learning data available in order to make aware
or improvise the process itself and/or the outcome such as student performance.
In this survey paper, we look at the recent research work that has been
conducted around learning analytics, framework and integrated models, and
application of various models and data mining techniques to identify students
at risk and to predict student performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06988</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06988</id><created>2015-01-28</created><updated>2015-04-13</updated><authors><author><keyname>Zhao</keyname><forenames>Jian</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Lei</keyname><forenames>Zhongding</forenames></author></authors><title>Heterogeneous Cellular Networks Using Wireless Backhaul: Fast Admission
  Control and Large System Analysis</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Journal on Selected Areas in Communications, under
  2nd round review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a heterogeneous cellular network with densely underlaid small
cell access points (SAPs). Wireless backhaul provides the data connection from
the core network to SAPs. To serve as many SAPs and their corresponding users
as possible with guaranteed data rates, admission control of SAPs needs to be
performed in wireless backhaul. Such a problem involves joint design of
transmit beamformers, power control, and selection of SAPs. In order to tackle
such a difficult problem, we apply $\ell_1$-relaxation and propose an iterative
algorithm for the $\ell_1$-relaxed problem. The selection of SAPs is made based
on the outputs of the iterative algorithm. This algorithm is fast and enjoys
low complexity for small-to-medium sized systems. However, its solution depends
on the actual channel state information, and resuming the algorithm for each
new channel realization may be unrealistic for large systems. Therefore, we
make use of random matrix theory and also propose an iterative algorithm for
large systems. Such a large system iterative algorithm produces asymptotically
optimum solution for the $\ell_1$-relaxed problem, which only requires
large-scale channel coefficients irrespective of the actual channel
realization. Near optimum results are achieved by our proposed algorithms in
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.06993</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.06993</id><created>2015-01-28</created><authors><author><keyname>Zhou</keyname><forenames>Youjie</forenames></author><author><keyname>Yu</keyname><forenames>Hongkai</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author></authors><title>Feature Sampling Strategies for Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although dense local spatial-temporal features with bag-of-features
representation achieve state-of-the-art performance for action recognition, the
huge feature number and feature size prevent current methods from scaling up to
real size problems. In this work, we investigate different types of feature
sampling strategies for action recognition, namely dense sampling, uniformly
random sampling and selective sampling. We propose two effective selective
sampling methods using object proposal techniques. Experiments conducted on a
large video dataset show that we are able to achieve better average recognition
accuracy using 25% less features, through one of proposed selective sampling
methods, and even remain comparable accuracy while discarding 70% features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07005</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07005</id><created>2015-01-28</created><authors><author><keyname>Makwana</keyname><forenames>Monika T.</forenames></author><author><keyname>Vegda</keyname><forenames>Deepak C.</forenames></author></authors><title>Survey:Natural Language Parsing For Indian Languages</title><categories>cs.CL</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syntactic parsing is a necessary task which is required for NLP applications
including machine translation. It is a challenging task to develop a
qualitative parser for morphological rich and agglutinative languages.
Syntactic analysis is used to understand the grammatical structure of a natural
language sentence. It outputs all the grammatical information of each word and
its constituent. Also issues related to it help us to understand the language
in a more detailed way. This literature survey is groundwork to understand the
different parser development for Indian languages and various approaches that
are used to develop such tools and techniques. This paper provides a survey of
research papers from well known journals and conferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07008</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07008</id><created>2015-01-28</created><authors><author><keyname>Essaid</keyname><forenames>Amira</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Smits</keyname><forenames>Gr&#xe9;gory</forenames></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>A Distance-Based Decision in the Credal Level</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Artificial Intelligence and Symbolic
  Computation (AISC 2014), Dec 2014, Sevilla, Spain. pp.147 - 156</journal-ref><doi>10.1007/978-3-319-13770-4_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief function theory provides a flexible way to combine information
provided by different sources. This combination is usually followed by a
decision making which can be handled by a range of decision rules. Some rules
help to choose the most likely hypothesis. Others allow that a decision is made
on a set of hypotheses. In [6], we proposed a decision rule based on a distance
measure. First, in this paper, we aim to demonstrate that our proposed decision
rule is a particular case of the rule proposed in [4]. Second, we give
experiments showing that our rule is able to decide on a set of hypotheses.
Some experiments are handled on a set of mass functions generated randomly,
others on real databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07019</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07019</id><created>2015-01-28</created><authors><author><keyname>Fowler</keyname><forenames>Alex</forenames></author><author><keyname>Galbraith</keyname><forenames>Steven</forenames></author></authors><title>Kangaroo Methods for Solving the Interval Discrete Logarithm Problem</title><categories>math.NT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interval discrete logarithm problem is defined as follows: Given some
$g,h$ in a group $G$, and some $N \in \mathbb{N}$ such that $g^z=h$ for some
$z$ where $0 \leq z &lt; N$, find $z$. At the moment, kangaroo methods are the
best low memory algorithm to solve the interval discrete logarithm problem. The
fastest non parallelised kangaroo methods to solve this problem are the three
kangaroo method, and the four kangaroo method. These respectively have expected
average running times of $\big(1.818+o(1)\big)\sqrt{N}$, and $\big(1.714 +
o(1)\big)\sqrt{N}$ group operations. It is currently an open question as to
whether it is possible to improve kangaroo methods by using more than four
kangaroos. Before this dissertation, the fastest kangaroo method that used more
than four kangaroos required at least $2\sqrt{N}$ group operations to solve the
interval discrete logarithm problem. In this thesis, I improve the running time
of methods that use more than four kangaroos significantly, and almost beat the
fastest kangaroo algorithm, by presenting a seven kangaroo method with an
expected average running time of $\big(1.7195 + o(1)\big)\sqrt{N} \pm O(1)$
group operations. The question, 'Are five kangaroos worse than three?' is also
answered in this thesis, as I propose a five kangaroo algorithm that requires
on average $\big(1.737+o(1)\big)\sqrt{N}$ group operations to solve the
interval discrete logarithm problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07020</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07020</id><created>2015-01-28</created><authors><author><keyname>Khoshkbarforoushha</keyname><forenames>Alireza</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Gaire</keyname><forenames>Raj</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem P.</forenames></author><author><keyname>Hosking</keyname><forenames>John</forenames></author><author><keyname>Abbasnejad</keyname><forenames>Ehsan</forenames></author></authors><title>Resource Usage Estimation of Data Stream Processing Workloads in
  Datacenter Clouds</title><categories>cs.DB</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time computation of data streams over affordable virtualized
infrastructure resources is an important form of data in motion processing
architecture. However, processing such data streams while ensuring strict
guarantees on quality of services is problematic due to: (i) uncertain stream
arrival pattern; (ii) need of processing different types of continuous queries;
and (iii) variable resource consumption behavior of continuous queries. Recent
work has explored the use of statistical techniques for resource estimation of
SQL queries and OLTP workloads. All these techniques approximate resource usage
for each query as a single point value. However, in data stream processing
workloads in which data flows through the graph of operators endlessly and
poses performance and resource demand fluctuations, the single point resource
estimation is inadequate. Because it is neither expressive enough nor does it
capture the multi-modal nature of the target data. To this end, we present a
novel technique which uses mixture density networks, a combined structure of
neural networks and mixture models, to estimate the whole spectrum of resource
usage as probability density functions. The proposed approach is a flexible and
convenient means of modeling unknown distribution models. We have validated the
models using both the linear road benchmark and the TPC-H, observing high
accuracy under a number of error metrics: mean-square error, continuous ranked
probability score, and negative log predictive density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07033</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07033</id><created>2015-01-28</created><authors><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>Cyran</keyname><forenames>Michael</forenames></author><author><keyname>Fischer</keyname><forenames>Robert F. H.</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Error Correction for Differential Linear Network Coding in
  Slowly-Varying Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, accepted at 10th International ITG Conference on Systems,
  Communications and Coding, Hamburg, Germany, February 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential linear network coding (DLNC) is a precoding scheme for
information transmission over random linear networks. By using differential
encoding and decoding, the conventional approach of lifting, required for
inherent channel sounding, can be omitted and in turn higher transmission rates
are supported. However, the scheme is sensitive to variations in the network
topology. In this paper, we derive an extended DLNC channel model which
includes slow network changes. Based on this, we propose and analyze a suitable
channel coding scheme matched to the situation at hand using rank-metric
convolutional codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07034</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07034</id><created>2015-01-28</created><authors><author><keyname>Gorbachev</keyname><forenames>V. N.</forenames></author><author><keyname>Denisov</keyname><forenames>L. A.</forenames></author><author><keyname>Kainarova</keyname><forenames>E. M.</forenames></author></authors><title>Embedding of binary image in the Gray planes</title><categories>cs.MM</categories><comments>7 pages, 3 figures, Proceeding of 24rd International Conference on
  Computer Graphics and Vision GraphiCon'2014, Sept.30 - Oct.3,2014,
  Rostov-on-Don, Russia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For watermarking of the digital grayscale image its Gray planes have been
used. With the help of the introduced representation over Gray planes the LSB
embedding method and detection have been discussed. It found that data, a
binary image, hidden in the Gray planes is more robust to JPEG lossy
compression than in the bit planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07053</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07053</id><created>2015-01-28</created><updated>2015-01-29</updated><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Backurs</keyname><forenames>Arturs</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author></authors><title>Quadratic-Time Hardness of LCS and other Sequence Similarity Measures</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two important similarity measures between sequences are the longest common
subsequence (LCS) and the dynamic time warping distance (DTWD). The
computations of these measures for two given sequences are central tasks in a
variety of applications. Simple dynamic programming algorithms solve these
tasks in $O(n^2)$ time, and despite an extensive amount of research, no
algorithms with significantly better worst case upper bounds are known.
  In this paper, we show that an $O(n^{2-\epsilon})$ time algorithm, for some
$\epsilon&gt;0$, for computing the LCS or the DTWD of two sequences of length $n$
over a constant size alphabet, refutes the popular Strong Exponential Time
Hypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over
an alphabet of size $O(k)$ cannot be done in $O(n^{k-\epsilon})$ time, for any
$\epsilon&gt;0$, under SETH. Finally, we also address the time complexity of
approximating the DTWD of two strings in truly subquadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07056</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07056</id><created>2015-01-28</created><authors><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames></author><author><keyname>Sethi</keyname><forenames>Shuchi</forenames></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author></authors><title>An Effective Framework for Managing University Data using a Cloud based
  Environment</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Management of data in education sector particularly management of data for
big universities with several employees, departments and students is a very
challenging task. There are also problems such as lack of proper funds and
manpower for management of such data in universities. Education sector can
easily and effectively take advantage of cloud computing skills for management
of data. It can enhance the learning experience as a whole and can add entirely
new dimensions to the way in which education is imbibed. Several benefits of
Cloud computing such as monetary benefits, environmental benefits and remote
data access for management of data such as university database can be used in
education sector. Therefore, in this paper we have proposed an effective
framework for managing university data using a cloud based environment. We have
also proposed cloud data management simulator: a new simulation framework which
demonstrates the applicability of cloud in the current education sector. The
framework consists of a cloud developed for processing a universities database
which consists of staff and students. It has the following features (i) support
for modeling cloud computing infrastructure, which includes data centers
containing university database; (ii) a user friendly interface; (iii)
flexibility to switch between the different types of users; and (iv)
virtualized access to cloud data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07072</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07072</id><created>2015-01-28</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>On the stability of asynchronous Random Access Schemes</title><categories>cs.IT cs.NI math.IT</categories><comments>9th International Wireless Communications and Mobile Computing
  Conference (IWCMC), 2013. Awarded as Best Paper at IWCMC 2013; IEEE
  proceedings, 2013</comments><doi>10.1109/IWCMC.2013.6583667</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slotted Aloha-based Random Access (RA) techniques have recently regained
attention in light of the use of Interference Cancellation (IC) as a mean to
exploit diversity created through the transmission of multiple burst copies per
packet content (CRDSA). Subsequently, the same concept has been extended to
pure ALOHA-based techniques in order to boost the performance also in case of
asynchronous RA schemes. In this paper, throughput as well as packet delay and
related stability for asynchronous ALOHA techniques under geometrically
distributed retransmissions are analyzed both in case of finite and infinite
population size. Moreover, a comparison between pure ALOHA, its evolution
(known as CRA) and CRDSA techniques is presented, in order to give a measure of
the achievable gain that can be reached in a closed-loop scenario with respect
to the previous state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07079</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07079</id><created>2015-01-28</created><authors><author><keyname>Licht</keyname><forenames>Fabio</forenames></author><author><keyname>Schulze</keyname><forenames>Bruno</forenames></author><author><keyname>Bona</keyname><forenames>Luis E.</forenames></author><author><keyname>Mury</keyname><forenames>Antonio R.</forenames></author></authors><title>The Affinity Effects of Parallelized Libraries in Concurrent
  Environments</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of cloud computing grows as it appears to be an additional resource
for High-Performance Parallel and Distributed Computing (HPDC), especially with
respect to its use in support of scientific applications. Many studies have
been devoted to determining the effect of the virtualization layer on the
performance, but most of the studies conducted so far lack insight into the
joint effects between application type, virtualization layer and parallelized
libraries in applications. This work introduces the concept of affinity with
regard to the combined effects of the virtualization layer, class of
application and parallelized libraries used in these applications. Affinity is
here defined as the degree of influence that one application has on other
applications when running concurrently in virtual environments hosted on the
same real server. The results presented here show how parallel libraries used
in application implementation have a significant influence and how the
combinations between these types of libraries and classes of applications could
significantly influence the performance of the environment. In this context,
the concept of affinity is then used to evaluate these impacts to contribute to
better stability and performance in the computational environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07080</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07080</id><created>2015-01-28</created><authors><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>On the genetic optimization of APSK constellations for satellite
  broadcasting</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE International Symposium on Broadband Multimedia Systems and
  Broadcasting (BMSB), 2014; IEEE proceedings, 2014</comments><doi>10.1109/BMSB.2014.6873465</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both satellite transmissions and DVB applications over satellite present
peculiar characteristics that could be taken into consideration in order to
further exploit the optimality of the transmission. In this paper, starting
from the state-of-the-art, the optimization of the APSK constellation through
asymmetric symbols arrangement is investigated for its use in satellite
communications. In particular, the optimization problem is tackled by means of
Genetic Algorithms that have already been demonstrated to work nicely with
complex non-linear optimization problems like the one presented hereinafter.
This work aims at studying the various parameters involved in the optimization
routine in order to establish those that best fit this case, thus further
enhancing the constellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07082</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07082</id><created>2015-01-28</created><authors><author><keyname>Hadzihasanovic</keyname><forenames>Amar</forenames></author></authors><title>A Diagrammatic Axiomatisation for Qubit Entanglement</title><categories>cs.LO math.CT quant-ph</categories><comments>12 pages</comments><msc-class>81P45 (Primary) 18D10, 16T05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diagrammatic techniques for reasoning about monoidal categories provide an
intuitive understanding of the symmetries and connections of interacting
computational processes. In the context of categorical quantum mechanics,
Coecke and Kissinger suggested that two 3-qubit states, GHZ and W, may be used
as the building blocks of a new graphical calculus, aimed at a diagrammatic
classification of multipartite qubit entanglement that would highlight the
communicational properties of quantum states, and their potential uses in
cryptographic schemes.
  In this paper, we present a full graphical axiomatisation of the relations
between GHZ and W: the ZW calculus. This refines a version of the preexisting
ZX calculus, while keeping its most desirable characteristics: undirectedness,
a large degree of symmetry, and an algebraic underpinning. We prove that the ZW
calculus is complete for the category of free abelian groups on a power of two
generators - &quot;qubits with integer coefficients&quot; - and provide an explicit
normalisation procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07084</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07084</id><created>2015-01-28</created><updated>2015-09-14</updated><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author><author><keyname>Huang</keyname><forenames>Wen-Hung</forenames></author><author><keyname>Liu</keyname><forenames>Cong</forenames></author></authors><title>k2U: A General Framework from k-Point Effective Schedulability Analysis
  to Utilization-Based Tests</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To deal with a large variety of workloads in different application domains in
real-time embedded systems, a number of expressive task models have been
developed. For each individual task model, researchers tend to develop
different types of techniques for deriving schedulability tests with different
computation complexity and performance. In this paper, we present a general
schedulability analysis framework, namely the k2U framework, that can be
potentially applied to analyze a large set of real-time task models under any
fixed-priority scheduling algorithm, on both uniprocessor and multiprocessor
scheduling. The key to k2U is a k-point effective schedulability test, which
can be viewed as a &quot;blackbox&quot; interface. For any task model, if a corresponding
k-point effective schedulability test can be constructed, then a sufficient
utilization-based test can be automatically derived. We show the generality of
k2U by applying it to different task models, which results in new and improved
tests compared to the state-of-the-art.
  Analogously, a similar concept by testing only k points with a different
formulation has been studied by us in another framework, called k2Q, which
provides quadratic bounds or utilization bounds based on a different
formulation of schedulability test. With the quadratic and hyperbolic forms,
k2Q and k2U frameworks can be used to provide many quantitive features to be
measured, like the total utilization bounds, speed-up factors, etc., not only
for uniprocessor scheduling but also for multiprocessor scheduling. These
frameworks can be viewed as a &quot;blackbox&quot; interface for schedulability tests and
response-time analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07093</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07093</id><created>2015-01-28</created><updated>2015-01-30</updated><authors><author><keyname>Rao</keyname><forenames>V. Sree Hari</forenames></author><author><keyname>Kumar</keyname><forenames>M. Naresh</forenames></author></authors><title>Novel Approaches for Predicting Risk Factors of Atherosclerosis</title><categories>cs.LG</categories><comments>7 pages, 2 figures</comments><journal-ref>Biomedical and Health Informatics, IEEE Journal of , vol.17, no.1,
  pp.183,189, Jan. 2013</journal-ref><doi>10.1109/TITB.2012.2227271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coronary heart disease (CHD) caused by hardening of artery walls due to
cholesterol known as atherosclerosis is responsible for large number of deaths
world-wide. The disease progression is slow, asymptomatic and may lead to
sudden cardiac arrest, stroke or myocardial infraction. Presently, imaging
techniques are being employed to understand the molecular and metabolic
activity of atherosclerotic plaques to estimate the risk. Though imaging
methods are able to provide some information on plaque metabolism they lack the
required resolution and sensitivity for detection. In this paper we consider
the clinical observations and habits of individuals for predicting the risk
factors of CHD. The identification of risk factors helps in stratifying
patients for further intensive tests such as nuclear imaging or coronary
angiography. We present a novel approach for predicting the risk factors of
atherosclerosis with an in-built imputation algorithm and particle swarm
optimization (PSO). We compare the performance of our methodology with other
machine learning techniques on STULONG dataset which is based on longitudinal
study of middle aged individuals lasting for twenty years. Our methodology
powered by PSO search has identified physical inactivity as one of the risk
factor for the onset of atherosclerosis in addition to other already known
factors. The decision rules extracted by our methodology are able to predict
the risk factors with an accuracy of $99.73%$ which is higher than the
accuracies obtained by application of the state-of-the-art machine learning
techniques presently being employed in the identification of atherosclerosis
risk studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07106</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07106</id><created>2015-01-28</created><authors><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Planarity of Streamed Graphs</title><categories>cs.DS</categories><comments>21 pages, 9 figures, extended version of &quot;Planarity of Streamed
  Graphs&quot; (9th International Conference on Algorithms and Complexity, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a notion of planarity for graphs that are
presented in a streaming fashion. A $\textit{streamed graph}$ is a stream of
edges $e_1,e_2,...,e_m$ on a vertex set $V$. A streamed graph is
$\omega$-$\textit{stream planar}$ with respect to a positive integer window
size $\omega$ if there exists a sequence of planar topological drawings
$\Gamma_i$ of the graphs $G_i=(V,\{e_j \mid i\leq j &lt; i+\omega\})$ such that
the common graph $G^{i}_\cap=G_i\cap G_{i+1}$ is drawn the same in $\Gamma_i$
and in $\Gamma_{i+1}$, for $1\leq i &lt; m-\omega$. The $\textit{Stream
Planarity}$ Problem with window size $\omega$ asks whether a given streamed
graph is $\omega$-stream planar. We also consider a generalization, where there
is an additional $\textit{backbone graph}$ whose edges have to be present
during each time step. These problems are related to several well-studied
planarity problems.
  We show that the $\textit{Stream Planarity}$ Problem is NP-complete even when
the window size is a constant and that the variant with a backbone graph is
NP-complete for all $\omega \ge 2$. On the positive side, we provide
$O(n+\omega{}m)$-time algorithms for (i) the case $\omega = 1$ and (ii) all
values of $\omega$ provided the backbone graph consists of one $2$-connected
component plus isolated vertices and no stream edge connects two isolated
vertices. Our results improve on the Hanani-Tutte-style $O((nm)^3)$-time
algorithm proposed by Schaefer [GD'14] for $\omega=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07107</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07107</id><created>2015-01-28</created><authors><author><keyname>Liu</keyname><forenames>Beiyi</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Shimoi</keyname><forenames>Nobuhiro</forenames></author></authors><title>Iterative-Promoting Variable Step Size Least Mean Square Algorithm for
  Accelerating Adaptive Channel Estimation</title><categories>cs.IT math.IT</categories><comments>6 pages, 8 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invariable step size based least-mean-square error (ISS-LMS) was considered
as a very simple adaptive filtering algorithm and hence it has been widely
utilized in many applications, such as adaptive channel estimation. It is well
known that the convergence speed of ISS-LMS is fixed by the initial step-size.
In the channel estimation scenarios, it is very hard to make tradeoff between
convergence speed and estimation performance. In this paper, we propose an
iterative-promoting variable step size based least-mean-square error (VSS-LMS)
algorithm to control the convergence speed as well as to improve the estimation
performance. Simulation results show that the proposed algorithm can achieve
better estimation performance than previous ISS-LMS while without sacrificing
convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07114</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07114</id><created>2015-01-28</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author></authors><title>Optimal Binary Locally Repairable Codes via Anticodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a construction for several families of optimal binary
locally repairable codes (LRCs) with small locality (2 and 3). This
construction is based on various anticodes. It provides binary LRCs which
attain the Cadambe-Mazumdar bound. Moreover, most of these codes are optimal
with respect to the Griesmer bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07130</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07130</id><created>2015-01-28</created><authors><author><keyname>Balaji</keyname><forenames>S. B.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>On Partial Maximally-Recoverable and Maximally-Recoverable Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An [n, k] linear code C that is subject to locality constraints imposed by a
parity check matrix H0 is said to be a maximally recoverable (MR) code if it
can recover from any erasure pattern that some k-dimensional subcode of the
null space of H0 can recover from. The focus in this paper is on MR codes
constrained to have all-symbol locality r. Given that it is challenging to
construct MR codes having small field size, we present results in two
directions. In the first, we relax the MR constraint and require only that
apart from the requirement of being an optimum all-symbol locality code, the
code must yield an MDS code when punctured in a single, specific pattern which
ensures that each local code is punctured in precisely one coordinate and that
no two local codes share the same punctured coordinate. We term these codes as
partially maximally recoverable (PMR) codes. We provide a simple construction
for high-rate PMR codes and then provide a general, promising approach that
needs further investigation. In the second direction, we present three
constructions of MR codes with improved parameters, primarily the size of the
finite field employed in the construction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07131</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07131</id><created>2015-01-28</created><updated>2015-07-28</updated><authors><author><keyname>Berwanger</keyname><forenames>Dietmar</forenames></author><author><keyname>Bogaard</keyname><forenames>Marie van den</forenames></author></authors><title>Consensus Game Acceptors</title><categories>cs.FL cs.GT cs.LO</categories><comments>12 pages; presented at DLT 2015</comments><acm-class>F.4.3; F.1.2; I.2.11</acm-class><doi>10.1007/978-3-319-21500-6_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a game for recognising formal languages in which two players with
imperfect information need to coordinate on a common decision, given private
input strings correlated by a finite graph. The players have a joint objective
to avoid an inadmissible decision, in spite of the uncertainty induced by the
input.
  We show that the acceptor model based on consensus games characterises
context-sensitive languages and conversely, that winning strategies in such
games can be described by context-sensitive languages. We also discuss
consensus game acceptors with a restricted observation pattern that describe
nondeterministic linear-time languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07132</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07132</id><created>2015-01-27</created><authors><author><keyname>Chang</keyname><forenames>Lubin</forenames></author></authors><title>On Kalman-Like Finite Impulse Response Filters</title><categories>cs.SY</categories><comments>This note is my personal understanding of the Kalman-Like Finite
  Impulse Response Filters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note reveals an explicit relationship between two representative finite
impulse response (FIR) filters, i.e. the newly derived and popularized
Kalman-Like unbiased FIR filter (UFIR) and the receding horizon Kalman FIR
filter (RHKF). It is pointed out that the only difference of the two algorithms
lies in the noise statistics ignorance and appropriate initial condition
construction strategy in UFIR. The revelation can benefit the performance
improvement of one by drawing lessons from the other. Some interesting
conclusions have also been drawn and discussed from this revelation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07133</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07133</id><created>2015-01-28</created><authors><author><keyname>Dhameliya</keyname><forenames>Vijay</forenames></author><author><keyname>Limbachiya</keyname><forenames>Dixita</forenames></author><author><keyname>Khakhar</keyname><forenames>Madhav</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K</forenames></author></authors><title>On Optimal Family of Codes for DNA Storage</title><categories>cs.IT math.IT</categories><comments>Supplementary file and the software DNA Cloud 2.0 is available at
  http://www.guptalab.org/dnacloud</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancement in the technology has generated immense data which has become a
burning issue for data storage experts. To accommodate the data and cope up
with demand, computer scientist are striving to produce the improved, dense and
reliable data storage medium. The main challenge for the development of optimum
data storage medium is enhancement in storage capacity, reliability and
security. Various applications of DNA in computing technologies and its dense,
stable, reliable nature enticed the researcher to use DNA as storage medium.
The idea of using DNA as storage medium has many success stories but the main
challenges to deal with are error correction and cost associated with the DNA
sequencing and synthesis. In this work, we have developed an efficient
technique to encode the data into DNA by using non-linear family of ternary
codes. This gives us significant reduction in file size for storing data on DNA
from previous developed methods. Using our method one can store 1.15 ExaBytes
(EB) of information in one gram of DNA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07135</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07135</id><created>2015-01-28</created><authors><author><keyname>Khan</keyname><forenames>Imran</forenames></author><author><keyname>Belqasmi</keyname><forenames>Fatna</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author><author><keyname>Crespi</keyname><forenames>Noel</forenames></author><author><keyname>Morrow</keyname><forenames>Monique</forenames></author><author><keyname>Polakos</keyname><forenames>Paul</forenames></author></authors><title>Wireless Sensor Network Virtualization: Early Architecture and Research
  Perspectives</title><categories>cs.NI</categories><comments>Accepted for publication on June 27th 2014 in Forthcoming issue of
  IEEE Network Magazine (May/June 2015 issue)</comments><journal-ref>Network, IEEE , vol.29, no.3, pp.104-112, May-June 2015</journal-ref><doi>10.1109/MNET.2015.7113233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) have become pervasive and are used in many
applications and services. Usually the deployments of WSNs are task oriented
and domain specific; thereby precluding re-use when other applications and
services are contemplated. This inevitably leads to the proliferation of
redundant WSN deployments. Virtualization is a technology that can aid in
tackling this issue, as it enables the sharing of resources/infrastructure by
multiple independent entities. In this paper we critically review the state of
the art and propose a novel architecture for WSN virtualization. The proposed
architecture has four layers (physical layer, virtual sensor layer, virtual
sensor access layer and overlay layer) and relies on the constrained
application protocol (CoAP). We illustrate its potential by using it in a
scenario where a single WSN is shared by multiple applications; one of which is
a fire monitoring application. We present the proof-of-concept prototype we
have built along with the performance measurements, and discuss future research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07139</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07139</id><created>2015-01-28</created><authors><author><keyname>Khan</keyname><forenames>Imran</forenames></author><author><keyname>Jafrin</keyname><forenames>Rifat</forenames></author><author><keyname>Errounda</keyname><forenames>Fatima Zahra</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author><author><keyname>Crespi</keyname><forenames>Noel</forenames></author><author><keyname>Morrow</keyname><forenames>Monique</forenames></author><author><keyname>Polako</keyname><forenames>Paul</forenames></author></authors><title>A Data Annotation Architecture for Semantic Applications in Virtualized
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>This paper has been accepted for presentation in main technical
  session of 14th IFIP/IEEE Symposium on Integrated Network and Service
  Management (IM 2015) to be held on 11-15 May, 2015, Ottawa, Canada</comments><doi>10.1109/INM.2015.7140273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) have become very popular and are being used
in many application domains (e.g. smart cities, security, gaming and
agriculture). Virtualized WSNs allow the same WSN to be shared by multiple
applications. Semantic applications are situation-aware and can potentially
play a critical role in virtualized WSNs. However, provisioning them in such
settings remains a challenge. The key reason is that semantic applications
provisioning mandates data annotation. Unfortunately it is no easy task to
annotate data collected in virtualized WSNs. This paper proposes a data
annotation architecture for semantic applications in virtualized heterogeneous
WSNs. The architecture uses overlays as the cornerstone, and we have built a
prototype in the cloud environment using Google App Engine. The early
performance measurements are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07174</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07174</id><created>2015-01-28</created><authors><author><keyname>Jia</keyname><forenames>Xiangyang</forenames></author><author><keyname>Ghezzi</keyname><forenames>Carlo</forenames></author><author><keyname>Ying</keyname><forenames>Shi</forenames></author></authors><title>Enhancing Reuse of Constraint Solutions to Improve Symbolic Execution</title><categories>cs.SE</categories><comments>this paper has been submitted to conference ISSTA 2015</comments><acm-class>D.2.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint solution reuse is an effective approach to save the time of
constraint solving in symbolic execution. Most of the existing reuse approaches
are based on syntactic or semantic equivalence of constraints; e.g. the Green
framework is able to reuse constraints which have different representations but
are semantically equivalent, through canonizing constraints into syntactically
equivalent normal forms. However, syntactic/semantic equivalence is not a
necessary condition for reuse--some constraints are not syntactically or
semantically equivalent, but their solutions still have potential for reuse.
Existing approaches are unable to recognize and reuse such constraints.
  In this paper, we present GreenTrie, an extension to the Green framework,
which supports constraint reuse based on the logical implication relations
among constraints. GreenTrie provides a component, called L-Trie, which stores
constraints and solutions into tries, indexed by an implication partial order
graph of constraints. L-Trie is able to carry out logical reduction and logical
subset and superset querying for given constraints, to check for reuse of
previously solved constraints. We report the results of an experimental
assessment of GreenTrie against the original Green framework, which shows that
our extension achieves better reuse of constraint solving result and saves
significant symbolic execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07180</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07180</id><created>2015-01-28</created><updated>2015-04-11</updated><authors><author><keyname>Zhang</keyname><forenames>Liliang</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wu</keyname><forenames>Xian</forenames></author><author><keyname>Ding</keyname><forenames>Shengyong</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>End-to-End Photo-Sketch Generation via Fully Convolutional
  Representation Learning</title><categories>cs.CV</categories><comments>8 pages, 6 figures. Proceeding in ACM International Conference on
  Multimedia Retrieval (ICMR), 2015</comments><acm-class>I.2.10</acm-class><doi>10.1145/2671188.2749321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sketch-based face recognition is an interesting task in vision and multimedia
research, yet it is quite challenging due to the great difference between face
photos and sketches. In this paper, we propose a novel approach for
photo-sketch generation, aiming to automatically transform face photos into
detail-preserving personal sketches. Unlike the traditional models synthesizing
sketches based on a dictionary of exemplars, we develop a fully convolutional
network to learn the end-to-end photo-sketch mapping. Our approach takes whole
face photos as inputs and directly generates the corresponding sketch images
with efficient inference and learning, in which the architecture are stacked by
only convolutional kernels of very small sizes. To well capture the person
identity during the photo-sketch transformation, we define our optimization
objective in the form of joint generative-discriminative minimization. In
particular, a discriminative regularization term is incorporated into the
photo-sketch generation, enhancing the discriminability of the generated person
sketches against other individuals. Extensive experiments on several standard
benchmarks suggest that our approach outperforms other state-of-the-art methods
in both photo-sketch generation and face sketch verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07184</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07184</id><created>2015-01-28</created><authors><author><keyname>Qiao</keyname><forenames>Shi</forenames></author><author><keyname>Ozsoyoglu</keyname><forenames>Z. Meral</forenames></author></authors><title>One Size Does not Fit All: When to Use Signature-based Pruning to
  Improve Template Matching for RDF graphs</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signature-based pruning is broadly accepted as an effective way to improve
query performance of graph template matching on general labeled graphs. Most
existing techniques which utilize signature-based pruning claim its benefits on
all datasets and queries. However, the effectiveness of signature-based pruning
varies greatly among different RDF datasets and highly related with their
dataset characteristics. We observe that the performance benefits from
signature-based pruning depend not only on the size of the RDF graphs, but also
the underlying graph structure and the complexity of queries. This motivates us
to propose a flexible RDF querying framework, called RDF-h, which selectively
utilizes signature-based pruning by evaluating the characteristics of RDF
datasets and query templates. Scalability and efficiency of RDF-h is
demonstrated in experimental results using both real and synthetic datasets.
  Keywords: RDF, Graph Template Matching, Signature-based Pruning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07188</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07188</id><created>2015-01-28</created><authors><author><keyname>Gemsa</keyname><forenames>Andreas</forenames></author><author><keyname>Niedermann</keyname><forenames>Benjamin</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author></authors><title>Label Placement in Road Maps</title><categories>cs.CG cs.DS</categories><comments>extended version of a CIAC 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A road map can be interpreted as a graph embedded in the plane, in which each
vertex corresponds to a road junction and each edge to a particular road
section. We consider the cartographic problem to place non-overlapping road
labels along the edges so that as many road sections as possible are identified
by their name, i.e., covered by a label. We show that this is NP-hard in
general, but the problem can be solved in polynomial time if the road map is an
embedded tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07195</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07195</id><created>2015-01-28</created><updated>2016-01-19</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>The Logic of Counting Query Answers</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of counting the number of answers to a first-order
formula on a finite structure. We present and study an extension of first-order
logic in which algorithms for this counting problem can be naturally and
conveniently expressed, in senses that are made precise and that are motivated
by the wish to understand tractable cases of the counting problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07201</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07201</id><created>2015-01-28</created><updated>2015-02-02</updated><authors><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Petroni</keyname><forenames>Fabio</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Everyday the Same Picture: Popularity and Content Diversity</title><categories>cs.SI cs.HC physics.data-an physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Facebook is flooded by diverse and heterogeneous content, from kittens up to
music and news, passing through satirical and funny stories. Each piece of that
corpus reflects the heterogeneity of the underlying social background. In the
Italian Facebook we have found an interesting case: a page having more than
$40K$ followers that every day posts the same picture of a popular Italian
singer. In this work, we use such a page as a control to study and model the
relationship between content heterogeneity on popularity. In particular, we use
that page for a comparative analysis of information consumption patterns with
respect to pages posting science and conspiracy news. In total, we analyze
about $2M$ likes and $190K$ comments, made by approximately $340K$ and $65K$
users, respectively. We conclude the paper by introducing a model mimicking
users selection preferences accounting for the heterogeneity of contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07203</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07203</id><created>2015-01-28</created><authors><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Structural Patterns of the Occupy Movement on Facebook</title><categories>cs.SI cs.HC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study a peculiar example of social organization on Facebook:
the Occupy Movement -- i.e., an international protest movement against social
and economic inequality organized online at a city level. We consider 179 US
Facebook public pages during the time period between September 2011 and
February 2013. The dataset includes 618K active users and 753K posts that
received about 5.2M likes and 1.1M comments. By labeling user according to
their interaction patterns on pages -- e.g., a user is considered to be
polarized if she has at least the 95% of her likes on a specific page -- we
find that activities are not locally coordinated by geographically close pages,
but are driven by pages linked to major US cities that act as hubs within the
various groups. Such a pattern is verified even by extracting the backbone
structure -- i.e., filtering statistically relevant weight heterogeneities --
for both the pages-reshares and the pages-common users networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07209</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07209</id><created>2015-01-28</created><authors><author><keyname>Voigt</keyname><forenames>Marco</forenames></author><author><keyname>Weidenbach</keyname><forenames>Christoph</forenames></author></authors><title>Bernays-Sch\&quot;onfinkel-Ramsey with Simple Bounds is NEXPTIME-complete</title><categories>cs.LO cs.CC</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear arithmetic extended with free predicate symbols is undecidable, in
general. We show that the restriction of linear arithmetic inequations to
simple bounds extended with the Bernays-Sch\&quot;onfinkel-Ramsey free first-order
fragment is decidable and NEXPTIME-complete. The result is almost tight because
the Bernays-Sch\&quot;onfinkel-Ramsey fragment is undecidable in combination with
linear difference inequations, simple additive inequations, quotient
inequations and multiplicative inequations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07215</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07215</id><created>2015-01-28</created><authors><author><keyname>Enqvist</keyname><forenames>Sebastian</forenames></author><author><keyname>Seifan</keyname><forenames>Fatemeh</forenames></author><author><keyname>Venema</keyname><forenames>Yde</forenames></author></authors><title>Monadic Second-Order Logic and Bisimulation Invariance for Coalgebras</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalizing standard monadic second-order logic for Kripke models, we
introduce monadic second-order logic interpreted over coalgebras for an
arbitrary set functor. Similar to well-known results for monadic second-order
logic over trees, we provide a translation of this logic into a class of
automata, relative to the class of coalgebras that admit a tree-like supporting
Kripke frame. We then consider invariance under behavioral equivalence of
formulas; more in particular, we investigate whether the coalgebraic
mu-calculus is the bisimulation-invariant fragment of monadic second-order
logic. Building on recent results by the third author we show that in order to
provide such a coalgebraic generalization of the Janin-Walukiewicz Theorem, it
suffices to find what we call an adequate uniform construction for the functor.
As applications of this result we obtain a partly new proof of the
Janin-Walukiewicz Theorem, and bisimulation invariance results for the bag
functor (graded modal logic) and all exponential polynomial functors.
  Finally, we consider in some detail the monotone neighborhood functor, which
provides coalgebraic semantics for monotone modal logic. It turns out that
there is no adequate uniform construction for this functor, whence the
automata-theoretic approach towards bisimulation invariance does not apply
directly. This problem can be overcome if we consider global bisimulations
between neighborhood models: one of our main technical results provides a
characterization of the monotone modal mu-calculus extended with the global
modalities, as the fragment of monadic second-order logic for the monotone
neighborhood functor that is invariant for global bisimulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07227</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07227</id><created>2015-01-28</created><updated>2016-02-10</updated><authors><author><keyname>Murphy</keyname><forenames>Robert A.</forenames></author></authors><title>A Neural Network Anomaly Detector Using the Random Cluster Model</title><categories>cs.LG cs.NE stat.ML</categories><comments>These writings are part of a longer writing which has been submitted
  for publication. I plan to replace this writing (and the other 2 writings)
  with the single writing that has been submitted for publication. The other
  writings to be withdrawn are 1503.03488 and 1412.4178</comments><msc-class>60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random cluster model is used to define an upper bound on a distance
measure as a function of the number of data points to be classified and the
expected value of the number of classes to form in a hybrid K-means and
regression classification methodology, with the intent of detecting anomalies.
Conditions are given for the identification of classes which contain anomalies
and individual anomalies within identified classes. A neural network model
describes the decision region-separating surface for offline storage and recall
in any new anomaly detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07242</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07242</id><created>2015-01-28</created><updated>2015-06-15</updated><authors><author><keyname>Belloni</keyname><forenames>Alexandre</forenames></author><author><keyname>Liang</keyname><forenames>Tengyuan</forenames></author><author><keyname>Narayanan</keyname><forenames>Hariharan</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author></authors><title>Escaping the Local Minima via Simulated Annealing: Optimization of
  Approximately Convex Functions</title><categories>cs.NA cs.LG math.OC</categories><comments>27 pages</comments><journal-ref>Journal of Machine Learning Research W&amp;CP vol 40: 240-265, 2015
  (COLT 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimizing an approximately convex function over a
bounded convex set in $\mathbb{R}^n$ using only function evaluations. The
problem is reduced to sampling from an \emph{approximately} log-concave
distribution using the Hit-and-Run method, which is shown to have the same
$\mathcal{O}^*$ complexity as sampling from log-concave distributions. In
addition to extend the analysis for log-concave distributions to approximate
log-concave distributions, the implementation of the 1-dimensional sampler of
the Hit-and-Run walk requires new methods and analysis. The algorithm then is
based on simulated annealing which does not relies on first order conditions
which makes it essentially immune to local minima.
  We then apply the method to different motivating problems. In the context of
zeroth order stochastic convex optimization, the proposed method produces an
$\epsilon$-minimizer after $\mathcal{O}^*(n^{7.5}\epsilon^{-2})$ noisy function
evaluations by inducing a $\mathcal{O}(\epsilon/n)$-approximately log concave
distribution. We also consider in detail the case when the &quot;amount of
non-convexity&quot; decays towards the optimum of the function. Other applications
of the method discussed in this work include private computation of empirical
risk minimizers, two-stage stochastic programming, and approximate dynamic
programming for online learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07250</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07250</id><created>2015-01-28</created><authors><author><keyname>Torre&#xf1;o</keyname><forenames>Alejandro</forenames></author><author><keyname>Onaindia</keyname><forenames>Eva</forenames></author><author><keyname>Sapena</keyname><forenames>&#xd3;scar</forenames></author></authors><title>FMAP: Distributed Cooperative Multi-Agent Planning</title><categories>cs.AI</categories><comments>21 pages, 11 figures</comments><msc-class>68T20, 68T42</msc-class><acm-class>I.2.8; I.2.11</acm-class><journal-ref>Applied Intelligence, Volume 41, Issue 2, pp. 606-626, Year 2014</journal-ref><doi>10.1007/s10489-014-0540-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes FMAP (Forward Multi-Agent Planning), a fully-distributed
multi-agent planning method that integrates planning and coordination. Although
FMAP is specifically aimed at solving problems that require cooperation among
agents, the flexibility of the domain-independent planning model allows FMAP to
tackle multi-agent planning tasks of any type. In FMAP, agents jointly explore
the plan space by building up refinement plans through a complete and flexible
forward-chaining partial-order planner. The search is guided by $h_{DTG}$, a
novel heuristic function that is based on the concepts of Domain Transition
Graph and frontier state and is optimized to evaluate plans in distributed
environments. Agents in FMAP apply an advanced privacy model that allows them
to adequately keep private information while communicating only the data of the
refinement plans that is relevant to each of the participating agents.
Experimental results show that FMAP is a general-purpose approach that
efficiently solves tightly-coupled domains that have specialized agents and
cooperative goals as well as loosely-coupled problems. Specifically, the
empirical evaluation shows that FMAP outperforms current MAP systems at solving
complex planning tasks that are adapted from the International Planning
Competition benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07255</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07255</id><created>2015-01-28</created><authors><author><keyname>Lira</keyname><forenames>M. M. S.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author></authors><title>Elliptic-cylindrical Wavelets: The Mathieu Wavelets</title><categories>stat.ME cs.NA math.NA stat.AP</categories><comments>10 pages, 2 figures</comments><journal-ref>IEEE Signal Processing Letters, vol. 11, issue 1, pp. 52-55, 2004</journal-ref><doi>10.1109/LSP.2003.819341</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note introduces a new family of wavelets and a multiresolution analysis,
which exploits the relationship between analysing filters and Floquet's
solution of Mathieu differential equations. The transfer function of both the
detail and the smoothing filter is related to the solution of a Mathieu
equation of odd characteristic exponent. The number of notches of these filters
can be easily designed. Wavelets derived by this method have potential
application in the fields of Optics and Electromagnetism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07256</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07256</id><created>2015-01-28</created><authors><author><keyname>Torre&#xf1;o</keyname><forenames>Alejandro</forenames></author><author><keyname>Onaindia</keyname><forenames>Eva</forenames></author><author><keyname>Sapena</keyname><forenames>&#xd3;scar</forenames></author></authors><title>An approach to multi-agent planning with incomplete information</title><categories>cs.AI</categories><comments>6 pages, 2 figures</comments><msc-class>68T20, 68T42</msc-class><acm-class>I.2.8; I.2.11</acm-class><journal-ref>20th European Conference of Artificial Intelligence (ECAI 2012),
  Volume 242, pp. 762-767, Year 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent planning (MAP) approaches have been typically conceived for
independent or loosely-coupled problems to enhance the benefits of distributed
planning between autonomous agents as solving this type of problems require
less coordination between the agents' sub-plans. However, when it comes to
tightly-coupled agents' tasks, MAP has been relegated in favour of centralized
approaches and little work has been done in this direction. In this paper, we
present a general-purpose MAP capable to efficiently handle planning problems
with any level of coupling between agents. We propose a cooperative refinement
planning approach, built upon the partial-order planning paradigm, that allows
agents to work with incomplete information and to have incomplete views of the
world, i.e. being ignorant of other agents' information, as well as maintaining
their own private information. We show various experiments to compare the
performance of our system with a distributed CSP-based MAP approach over a
suite of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07293</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07293</id><created>2015-01-25</created><authors><author><keyname>Zhu</keyname><forenames>Ru</forenames></author></authors><title>Accelerate micromagnetic simulations with GPU programming in MATLAB</title><categories>cs.CE</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite-difference Micromagnetic simulation code written in MATLAB is
presented with Graphics Processing Unit (GPU) acceleration. The high
performance of Graphics Processing Unit (GPU) is demonstrated compared to a
typical Central Processing Unit (CPU) based code. The speed-up of GPU to CPU is
shown to be greater than 30 for problems with larger sizes on a mid-end GPU in
single precision. The code is less than 200 lines and suitable for new
algorithm developing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07304</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07304</id><created>2015-01-28</created><authors><author><keyname>Redi</keyname><forenames>Miriam</forenames></author><author><keyname>Rasiwasia</keyname><forenames>Nikhil</forenames></author><author><keyname>Aggarwal</keyname><forenames>Gaurav</forenames></author><author><keyname>Jaimes</keyname><forenames>Alejandro</forenames></author></authors><title>The Beauty of Capturing Faces: Rating the Quality of Digital Portraits</title><categories>cs.CV cs.CY cs.MM</categories><comments>FG 2015, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital portrait photographs are everywhere, and while the number of face
pictures keeps growing, not much work has been done to on automatic portrait
beauty assessment. In this paper, we design a specific framework to
automatically evaluate the beauty of digital portraits. To this end, we procure
a large dataset of face images annotated not only with aesthetic scores but
also with information about the traits of the subject portrayed. We design a
set of visual features based on portrait photography literature, and
extensively analyze their relation with portrait beauty, exposing interesting
findings about what makes a portrait beautiful. We find that the beauty of a
portrait is linked to its artistic value, and independent from age, race and
gender of the subject. We also show that a classifier trained with our features
to separate beautiful portraits from non-beautiful portraits outperforms
generic aesthetic classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07315</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07315</id><created>2015-01-28</created><authors><author><keyname>Slavakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation</title><categories>cs.LG</categories><comments>Preliminary results of this work appear in the Proc. of the IEEE
  Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence,
  Italy, May 4-9, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications involving dictionary learning, non-negative matrix
factorization, subspace clustering, and parallel factor tensor decomposition
tasks motivate well algorithms for per-block-convex and non-smooth optimization
problems. By leveraging the stochastic approximation paradigm and first-order
acceleration schemes, this paper develops an online and modular learning
algorithm for a large class of non-convex data models, where convexity is
manifested only per-block of variables whenever the rest of them are held
fixed. The advocated algorithm incurs computational complexity that scales
linearly with the number of unknowns. Under minimal assumptions on the cost
functions of the composite optimization task, without bounding constraints on
the optimization variables, or any explicit information on bounds of Lipschitz
coefficients, the expected cost evaluated online at the resultant iterates is
provably convergent with quadratic rate to an accumulation point of the
(per-block) minima, while subgradients of the expected cost asymptotically
vanish in the mean-squared sense. The merits of the general approach are
demonstrated in two online learning setups: (i) Robust linear regression using
a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised
dictionary learning for network-wide link load tracking and imputation with
missing entries. Numerical tests on synthetic and real data highlight the
potential of the proposed framework for streaming data analytics by
demonstrating superior performance over block coordinate descent, and reduced
complexity relative to the popular alternating-direction method of multipliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07319</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07319</id><created>2015-01-28</created><updated>2016-01-07</updated><authors><author><keyname>Kim</keyname><forenames>Su Min</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Virtual Full-Duplex Buffer-Aided Relaying in the Presence of Inter-Relay
  Interference</title><categories>cs.IT math.IT</categories><comments>Accepted for publication to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study virtual full-duplex (FD) buffer-aided relaying to
recover the loss of multiplexing gain caused by half-duplex (HD) relaying in a
multiple relay network, where each relay is equipped with a buffer and multiple
antennas, through joint opportunistic relay selection (RS) and beamforming (BF)
design. The main idea of virtual FD buffer-aided relaying is that the source
and one of the relays simultaneously transmit their own information to another
relay and the destination, respectively. In such networks, inter-relay
interference (IRI) is a crucial problem which has to be resolved like
self-interference in the FD relaying. In contrast to previous work that
neglected IRI, we propose joint RS and BF schemes taking IRI into consideration
by using multiple antennas at the relays. In order to maximize average
end-to-end rate, we propose a weighted sum-rate maximization strategy assuming
that adaptive rate transmission is employed in both the source to relay and
relay to destination links. Then, we propose several BF schemes cancelling or
suppressing IRI in order to maximize the weighted sum-rate. Numerical results
show that our proposed optimal, zero forcing, and minimum mean square error
BF-based RS schemes asymptotically approach the ideal FD relaying upper bound
when increasing the number of antennas and/or the number of relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07320</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07320</id><created>2015-01-28</created><updated>2015-05-18</updated><authors><author><keyname>Kuleshov</keyname><forenames>Volodymyr</forenames></author><author><keyname>Chaganty</keyname><forenames>Arun Tejasvi</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Tensor Factorization via Matrix Factorization</title><categories>cs.LG stat.ML</categories><comments>Appearing in Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA.
  JMLR: W&amp;CP volume 38</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor factorization arises in many machine learning applications, such
knowledge base modeling and parameter estimation in latent variable models.
However, numerical methods for tensor factorization have not reached the level
of maturity of matrix factorization methods. In this paper, we propose a new
method for CP tensor factorization that uses random projections to reduce the
problem to simultaneous matrix diagonalization. Our method is conceptually
simple and also applies to non-orthogonal and asymmetric tensors of arbitrary
order. We prove that a small number random projections essentially preserves
the spectral information in the tensor, allowing us to remove the dependence on
the eigengap that plagued earlier tensor-to-matrix reductions. Experimentally,
our method outperforms existing tensor factorization methods on both simulated
data and two real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07323</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07323</id><created>2015-01-28</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Liang</keyname><forenames>Weifa</forenames></author><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Lin</keyname><forenames>Zhiyun</forenames></author></authors><title>Performance Analysis of Raptor Codes under Maximum-Likelihood (ML)
  Decoding</title><categories>cs.IT math.IT</categories><comments>28 pages and 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Raptor codes have been widely used in many multimedia broadcast/multicast
applications. However, our understanding of Raptor codes is still incomplete
due to the insufficient amount of theoretical work on the performance analysis
of Raptor codes, particularly under maximum-likelihood (ML) decoding, which
provides an optimal benchmark on the system performance for the other decoding
schemes to compare against. For the first time, this paper provides an upper
bound and a lower bound, on the packet error performance of Raptor codes under
ML decoding, which is measured by the probability that all source packets can
be successfully decoded by a receiver with a given number of successfully
received coded packets. Simulations are conducted to validate the accuracy of
the analysis. More specifically, Raptor codes with different degree
distribution and pre-coders, are evaluated using the derived bounds with high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07328</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07328</id><created>2015-01-28</created><authors><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Neil</keyname><forenames>Callum T.</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author></authors><title>On the Convergence of Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine convergence properties of massive MIMO systems with
the aim of determining the number of antennas required for massive MIMO gains.
We consider three characteristics of a channel matrix and study their
asymptotic behaviour. Furthermore, we derive ZF SNR and MF SINR for a scenario
of unequal receive powers. In our results we include the effects of spatial
correlation. We show that the rate of convergence of channel metrics is much
slower than that of the ZF/MF precoder properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07336</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07336</id><created>2015-01-28</created><authors><author><keyname>Emran</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Elsabrouty</keyname><forenames>Maha</forenames></author></authors><title>Generalized Simplified Variable-Scaled Min Sum LDPC decoder for
  irregular LDPC Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel low complexity scaling strategy of min-sum
decoding algorithm for irregular LDPC codes. In the proposed method, we
generalize our previously proposed simplified Variable Scaled Min-Sum
(SVS-min-sum) by replacing the sub-optimal starting value and heuristic update
for the scaling factor sequence by optimized values. Density evolution and
Nelder-Mead optimization are used offline, prior to the decoding, to obtain the
optimal starting point and per iteration updating step size for the scaling
factor sequence of the proposed scaling strategy. The optimization of these
parameters proves to be of noticeable positive impact on the decoding
performance. We used different DVB-T2 LDPC codes in our simulation. Simulation
results show the superior performance (in both WER and latency) of the proposed
algorithm to other Min-Sum based algorithms. In addition to that, generalized
SVS-min-sum algorithm has very close performance to LLR-SPA with much lower
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07338</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07338</id><created>2015-01-28</created><authors><author><keyname>Ren</keyname><forenames>Jimmy SJ.</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author></authors><title>On Vectorization of Deep Convolutional Neural Networks for Vision Tasks</title><categories>cs.CV</categories><comments>To appear in the 29th AAAI Conference on Artificial Intelligence
  (AAAI-15). Austin, Texas, USA, January 25-30, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recently have witnessed many ground-breaking results in machine learning
and computer vision, generated by using deep convolutional neural networks
(CNN). While the success mainly stems from the large volume of training data
and the deep network architectures, the vector processing hardware (e.g. GPU)
undisputedly plays a vital role in modern CNN implementations to support
massive computation. Though much attention was paid in the extent literature to
understand the algorithmic side of deep CNN, little research was dedicated to
the vectorization for scaling up CNNs. In this paper, we studied the
vectorization process of key building blocks in deep CNNs, in order to better
understand and facilitate parallel implementation. Key steps in training and
testing deep CNNs are abstracted as matrix and vector operators, upon which
parallelism can be easily achieved. We developed and compared six
implementations with various degrees of vectorization with which we illustrated
the impact of vectorization on the speed of model training and testing.
Besides, a unified CNN framework for both high-level and low-level vision tasks
is provided, along with a vectorized Matlab implementation with
state-of-the-art speed performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07340</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07340</id><created>2015-01-28</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Sequential Probability Assignment with Binary Alphabets and Large
  Classes of Experts</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of sequential probability assignment for binary
outcomes with side information and logarithmic loss, where regret---or,
redundancy---is measured with respect to a (possibly infinite) class of
experts. We provide upper and lower bounds for minimax regret in terms of
sequential complexities of the class. These complexities were recently shown to
give matching (up to logarithmic factors) upper and lower bounds for sequential
prediction with general convex Lipschitz loss functions (Rakhlin and Sridharan,
2015). To deal with unbounded gradients of the logarithmic loss, we present a
new analysis that employs a sequential chaining technique with a Bernstein-type
bound. The introduced complexities are intrinsic to the problem of sequential
probability assignment, as illustrated by our lower bound.
  We also consider an example of a large class of experts parametrized by
vectors in a high-dimensional Euclidean ball (or a Hilbert ball). The typical
discretization approach fails, while our techniques give a non-trivial bound.
For this problem we also present an algorithm based on regularization with a
self-concordant barrier. This algorithm is of an independent interest, as it
requires a bound on the function values rather than gradients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07348</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07348</id><created>2015-01-29</created><authors><author><keyname>Chen</keyname><forenames>Xujin</forenames></author><author><keyname>Hu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wang</keyname><forenames>Changjun</forenames></author></authors><title>Finding Connected Dense $k$-Subgraphs</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a connected graph $G$ on $n$ vertices and a positive integer $k\le n$,
a subgraph of $G$ on $k$ vertices is called a $k$-subgraph in $G$. We design
combinatorial approximation algorithms for finding a connected $k$-subgraph in
$G$ such that its density is at least a factor
$\Omega(\max\{n^{-2/5},k^2/n^2\})$ of the density of the densest $k$-subgraph
in $G$ (which is not necessarily connected). These particularly provide the
first non-trivial approximations for the densest connected $k$-subgraph problem
on general graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07349</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07349</id><created>2015-01-29</created><authors><author><keyname>Liu</keyname><forenames>Bo</forenames></author><author><keyname>Lu</keyname><forenames>Wenlian</forenames></author><author><keyname>Jiao</keyname><forenames>Licheng</forenames></author><author><keyname>Chen</keyname><forenames>Tianping</forenames></author></authors><title>Structure-Based Self-Triggered Consensus in Networks of Multiagents with
  Switching Topologies</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new self-triggered consensus algorithm in
networks of multi-agents. Different from existing works, which are based on the
observation of states, here, each agent determines its next update time based
on its coupling structure. Both centralized and distributed approaches of the
algorithms have been discussed. By transforming the algorithm to a proper
discrete-time systems without self delays, we established a new analysis
framework to prove the convergence of the algorithm. Then we extended the
algorithm to networks with switching topologies, especially stochastically
switching topologies. Compared to existing works, our algorithm is easier to
understand and implement. It explicitly provides positive lower and upper
bounds for the update time interval of each agent based on its coupling
structure, which can also be independently adjusted by each agent according to
its own situation. Our work reveals that the event/self triggered algorithms
are essentially discrete and more suitable to a discrete analysis framework.
Numerical simulations are also provided to illustrate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07350</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07350</id><created>2015-01-29</created><updated>2015-08-26</updated><authors><author><keyname>Duy</keyname><forenames>Truong Vinh Truong</forenames></author><author><keyname>Ozaki</keyname><forenames>Taisuke</forenames></author></authors><title>Performance Tuning of a Parallel 3-D FFT Package OpenFFT</title><categories>cs.MS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fast Fourier transform (FFT) is a primitive kernel in numerous fields of
science and engineering. OpenFFT is an open-source parallel package for 3-D
FFTs, built on a communication-optimal domain decomposition method for
achieving minimal volume of communication. In this paper, we analyze and tune
the performance of OpenFFT, paying a particular attention to tuning of
communication that dominates the run time of large-scale calculations. We first
analyze its performance on different machines for an understanding of the
behaviors of the package and machines. Based on the performance analysis, we
develop six communication methods for performing communication with the aim of
covering varied calculation scales on a variety of computational platforms.
OpenFFT is then augmented with an auto-tuning of communication to select the
best method in run time depending on their performance. Numerical results
demonstrate that the optimized OpenFFT is able to deliver relatively good
performance in comparison with other state-of-the-art packages at different
computational scales on a number of parallel machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07359</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07359</id><created>2015-01-29</created><updated>2015-09-27</updated><authors><author><keyname>Wu</keyname><forenames>Tianfu</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Learning And-Or Models to Represent Context and Occlusion for Car
  Detection and Viewpoint Estimation</title><categories>cs.CV</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for learning And-Or models to represent context
and occlusion for car detection and viewpoint estimation. The learned And-Or
model represents car-to-car context and occlusion configurations at three
levels: (i) spatially-aligned cars, (ii) single car under different occlusion
configurations, and (iii) a small number of parts. The And-Or model embeds a
grammar for representing large structural and appearance variations in a
reconfigurable hierarchy. The learning process consists of two stages in a
weakly supervised way (i.e., only bounding boxes of single cars are annotated).
Firstly, the structure of the And-Or model is learned with three components:
(a) mining multi-car contextual patterns based on layouts of annotated single
car bounding boxes, (b) mining occlusion configurations between single cars,
and (c) learning different combinations of part visibility based on car 3D CAD
simulation. The And-Or model is organized in a directed and acyclic graph which
can be inferred by Dynamic Programming. Secondly, the model parameters (for
appearance, deformation and bias) are jointly trained using Weak-Label
Structural SVM. In experiments, we test our model on four car detection
datasets --- the KITTI dataset \cite{Geiger12}, the PASCAL VOC2007 car
dataset~\cite{pascal}, and two self-collected car datasets, namely the
Street-Parking car dataset and the Parking-Lot car dataset, and three datasets
for car viewpoint estimation --- the PASCAL VOC2006 car dataset~\cite{pascal},
the 3D car dataset~\cite{savarese}, and the PASCAL3D+ car
dataset~\cite{xiang_wacv14}. Compared with state-of-the-art variants of
deformable part-based models and other methods, our model achieves significant
improvement consistently on the four detection datasets, and comparable
performance on car viewpoint estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07362</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07362</id><created>2015-01-29</created><authors><author><keyname>Alaoui-Fdili</keyname><forenames>Othmane</forenames><affiliation>IEMN, GSCM-LRIT</affiliation></author><author><keyname>CORLAY</keyname><forenames>Patrick</forenames><affiliation>IEMN</affiliation></author><author><keyname>Fakhri</keyname><forenames>Youssef</forenames><affiliation>GSCM-LRIT</affiliation></author><author><keyname>Coudoux</keyname><forenames>Fran&#xe7;ois-Xavier</forenames><affiliation>IEMN</affiliation></author><author><keyname>Aboutajdine</keyname><forenames>Driss</forenames><affiliation>GSCM-LRIT</affiliation></author></authors><title>A Cross-Layer Approach for Video Delivery over Wireless Video Sensor
  Networks</title><categories>cs.NI</categories><comments>The 7th International Symposium on signal, Image, Video and
  Communications (ISIVC 2014), Nov 2014, Marrakech, Morocco</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel cross-layer ap-proach for video delivery
over Wireless Video Sensor Networks (WVSN)s. We adopt an energy efficient and
adaptive video compression scheme dedicated to the WVSNs, based on the
H.264/AVC video compression standard. The encoder operates using two modes. In
the first mode, the nodes capture the scene following a low frame rate. When an
event is detected, the encoder switches to the second mode with a higher frame
rate and outputs two different types of macroblocks, referring to the region of
interest and the background respectively. Furthermore, we propose an Energy and
Queue Buffer Size Aware MMSPEED-based protocol for reliably and energy
efficiently routing both regions towards the destination. Simulations results
prove that the proposed approach is energy efficient and delivers good quality
video streams. In addition, the proposed routing protocol EQBSA-MMSPEED
outperforms its predecessors, the QBSA-MMSPEED and the MMSPEED, providing 33%
of lifetime extension and 3 dBs of video quality enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07365</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07365</id><created>2015-01-29</created><updated>2015-06-29</updated><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>7R Darboux Linkages by Factorization of Motion Polynomials</title><categories>cs.RO math.MG</categories><comments>To be published in the proceedings of the 14th IFToMM World Congress,
  Taipei, 2015</comments><msc-class>12D05, 70B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct two types of 7R closed single loop linkages by
combining different factorizations of a general (non-vertical) Darboux motion.
These factorizations are obtained by extensions of a factorization algorithm
for a generic rational motion. The first type of 7R linkages has several
one-dimensional configuration components and one of them corresponds to the
Darboux motion. The other type is a 7R linkage with two degrees of freedom and
without one-dimensional component. The Darboux motion is a curve in an
irreducible two dimensional configuration component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07379</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07379</id><created>2015-01-29</created><authors><author><keyname>Fuerst</keyname><forenames>Carlo</forenames></author><author><keyname>Pacut</keyname><forenames>Maciej</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>Hardness of Virtual Network Embedding with Replica Selection</title><categories>cs.DC</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient embedding virtual clusters in physical network is a challenging
problem. In this paper we consider a scenario where physical network has a
structure of a balanced tree. This assumption is justified by many real- world
implementations of datacenters. We consider an extension to virtual cluster
embedding by introducing replication among data chunks. In many real-world
applications, data is stored in distributed and redundant way. This assumption
introduces additional hardness in deciding what replica to process. By
reduction from classical NP-complete problem of Boolean Satisfia- bility, we
show limits of optimality of embedding. Our result holds even in trees of edge
height bounded by three. Also, we show that limiting repli- cation factor to
two replicas per chunk type does not make the problem simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07388</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07388</id><created>2015-01-29</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Rahn</keyname><forenames>Mona</forenames></author><author><keyname>Schaefer</keyname><forenames>Guido</forenames></author><author><keyname>Simon</keyname><forenames>Sunil</forenames></author></authors><title>Coordination Games on Graphs</title><categories>cs.GT</categories><comments>23 pages. An extended abstract of this paper appeared in the Proc.
  10th International Workshop on Internet and Network Economics (WINE 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce natural strategic games on graphs, which capture the idea of
coordination in a local setting. We show that these games have an exact
potential and have strong equilibria when the graph is a pseudoforest. We also
exhibit some other classes of graphs for which a strong equilibrium exists.
However, in general strong equilibria do not need to exist. Further, we study
the (strong) price of stability and anarchy. Finally, we consider the problems
of computing strong equilibria and of determining whether a joint strategy is a
strong equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07396</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07396</id><created>2015-01-29</created><updated>2015-03-02</updated><authors><author><keyname>Giglio</keyname><forenames>Davide</forenames></author></authors><title>A MILP model for single machine family scheduling with
  sequence-dependent batch setup and controllable processing times</title><categories>math.OC cs.SY</categories><comments>Technical Report, 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mathematical programming model for a class of single machine family
scheduling problem is described in this technical report, with the aim of
comparing the performance in solving the scheduling problem by means of
mathematical programming with the performance obtained when using optimal
control strategies, that can be derived from the application of a dynamic
programming-based methodology proposed by the Author. The scheduling problem is
characterized by the presence of sequence-dependent batch setup and
controllable processing times; moreover, the generalized due-date model is
adopted in the problem. Three mixed-integer linear programming (MILP) models
are proposed. The best one, from the performance point of view, is a model
which makes use of two sets of binary variables: the former to define the
relative position of jobs and the latter to define the exact sequence of jobs.
In addition, one of the model exploits a stage-based state space representation
which can be adopted to define the dynamics of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07399</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07399</id><created>2015-01-29</created><authors><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Arcos</keyname><forenames>Josep Lluis</forenames></author></authors><title>Particle swarm optimization for time series motif discovery</title><categories>cs.LG cs.NE</categories><comments>12 pages, 9 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficiently finding similar segments or motifs in time series data is a
fundamental task that, due to the ubiquity of these data, is present in a wide
range of domains and situations. Because of this, countless solutions have been
devised but, to date, none of them seems to be fully satisfactory and flexible.
In this article, we propose an innovative standpoint and present a solution
coming from it: an anytime multimodal optimization algorithm for time series
motif discovery based on particle swarms. By considering data from a variety of
domains, we show that this solution is extremely competitive when compared to
the state-of-the-art, obtaining comparable motifs in considerably less time
using minimal memory. In addition, we show that it is robust to different
implementation choices and see that it offers an unprecedented degree of
flexibility with regard to the task. All these qualities make the presented
solution stand out as one of the most prominent candidates for motif discovery
in long time series streams. Besides, we believe the proposed standpoint can be
exploited in further time series analysis and mining tasks, widening the scope
of research and potentially yielding novel effective solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07400</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07400</id><created>2015-01-29</created><authors><author><keyname>Huber</keyname><forenames>Markus</forenames></author><author><keyname>Gmeiner</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author><author><keyname>Wohlmuth</keyname><forenames>Barbara</forenames></author></authors><title>Resilience for Exascale Enabled Multigrid Methods</title><categories>cs.CE cs.DC</categories><msc-class>68W10, 68N30, 65N55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing number of components and further miniaturization the mean
time between faults in supercomputers will decrease. System level fault
tolerance techniques are expensive and cost energy, since they are often based
on redundancy. Also classical check-point-restart techniques reach their limits
when the time for storing the system state to backup memory becomes excessive.
Therefore, algorithm-based fault tolerance mechanisms can become an attractive
alternative. This article investigates the solution process for elliptic
partial differential equations that are discretized by finite elements. Faults
that occur in the parallel geometric multigrid solver are studied in various
model scenarios. In a standard domain partitioning approach, the impact of a
failure of a core or a node will affect one or several subdomains. Different
strategies are developed to compensate the effect of such a failure
algorithmically. The recovery is achieved by solving a local subproblem with
Dirichlet boundary conditions using local multigrid cycling algorithms.
Additionally, we propose a superman strategy where extra compute power is
employed to minimize the time of the recovery process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07417</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07417</id><created>2015-01-29</created><authors><author><keyname>Hirche</keyname><forenames>Christoph</forenames></author><author><keyname>Morgan</keyname><forenames>Ciara</forenames></author></authors><title>An improved rate region for the classical-quantum broadcast channel</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, double column, 1 figure, based on a result presented in the
  Master's thesis arXiv:1501.03737</comments><journal-ref>Proceedings of the 2015 IEEE International Symposium on
  Information Theory (ISIT 2015, Hong Kong), pages 2782 - 2786</journal-ref><doi>10.1109/ISIT.2015.7282963</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new achievable rate region for the two-user binary-input
classical-quantum broadcast channel. The result is a generalization of the
classical Marton-Gelfand-Pinsker region and is provably larger than the best
previously known rate region for classical-quantum broadcast channels. The
proof of achievability is based on the recently introduced polar coding scheme
and its generalization to quantum network information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07418</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07418</id><created>2015-01-29</created><updated>2015-05-13</updated><authors><author><keyname>Yu</keyname><forenames>Pengqian</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author></authors><title>Distributionally Robust Counterpart in Markov Decision Processes</title><categories>cs.SY math.OC</categories><comments>Added references. Corrected typos. Modified a mistake in Example 2
  (Variance). Provided more details of the simulation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies Markov Decision Processes under parameter uncertainty. We
adapt the distributionally robust optimization framework, and assume that the
uncertain parameters are random variables following an unknown distribution,
and seeks the strategy which maximizes the expected performance under the most
adversarial distribution. In particular, we generalize previous study
\cite{xu2012distributionally} which concentrates on distribution sets with very
special structure to much more generic class of distribution sets, and show
that the optimal strategy can be obtained efficiently under mild technical
condition. This significantly extends the applicability of distributionally
robust MDP to incorporate probabilistic information of uncertainty in a more
flexible way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07420</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07420</id><created>2015-01-29</created><authors><author><keyname>Sarangi</keyname><forenames>Smruti R.</forenames></author><author><keyname>Kalayappan</keyname><forenames>Rajshekar</forenames></author><author><keyname>Kallurkar</keyname><forenames>Prathmesh</forenames></author><author><keyname>Goel</keyname><forenames>Seep</forenames></author></authors><title>Tejas Simulator : Validation against Hardware</title><categories>cs.AR</categories><comments>3 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this report we show results that validate the Tejas architectural
simulator against native hardware. We report mean error rates of 11.45% and
18.77% for the SPEC2006 and Splash2 benchmark suites respectively. These error
rates are competitive and in most cases better than the numbers reported by
other contemporary simulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07422</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07422</id><created>2015-01-29</created><authors><author><keyname>Ishikawa</keyname><forenames>Kohta</forenames></author><author><keyname>Sato</keyname><forenames>Ikuro</forenames></author><author><keyname>Ambai</keyname><forenames>Mitsuru</forenames></author></authors><title>Pairwise Rotation Hashing for High-dimensional Features</title><categories>cs.CV stat.ML</categories><comments>16 pages, 8 figures, wrote at Mar 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary Hashing is widely used for effective approximate nearest neighbors
search. Even though various binary hashing methods have been proposed, very few
methods are feasible for extremely high-dimensional features often used in
visual tasks today. We propose a novel highly sparse linear hashing method
based on pairwise rotations. The encoding cost of the proposed algorithm is
$\mathrm{O}(n \log n)$ for n-dimensional features, whereas that of the existing
state-of-the-art method is typically $\mathrm{O}(n^2)$. The proposed method is
also remarkably faster in the learning phase. Along with the efficiency, the
retrieval accuracy is comparable to or slightly outperforming the
state-of-the-art. Pairwise rotations used in our method are formulated from an
analytical study of the trade-off relationship between quantization error and
entropy of binary codes. Although these hashing criteria are widely used in
previous researches, its analytical behavior is rarely studied. All building
blocks of our algorithm are based on the analytical solution, and it thus
provides a fairly simple and efficient procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07423</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07423</id><created>2015-01-29</created><authors><author><keyname>Torre&#xf1;o</keyname><forenames>Alejandro</forenames></author><author><keyname>Onaindia</keyname><forenames>Eva</forenames></author><author><keyname>Sapena</keyname><forenames>&#xd3;scar</forenames></author></authors><title>A Flexible Coupling Approach to Multi-Agent Planning under Incomplete
  Information</title><categories>cs.AI</categories><comments>40 pages, 10 figures</comments><msc-class>68T20, 68T42</msc-class><acm-class>I.2.8; I.2.11</acm-class><journal-ref>Knowledge and Information Systems, Volume 38, Issue 1, pp.
  141-178, Year 2014</journal-ref><doi>10.1007/s10115-012-0569-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent planning (MAP) approaches are typically oriented at solving
loosely-coupled problems, being ineffective to deal with more complex,
strongly-related problems. In most cases, agents work under complete
information, building complete knowledge bases. The present article introduces
a general-purpose MAP framework designed to tackle problems of any coupling
levels under incomplete information. Agents in our MAP model are partially
unaware of the information managed by the rest of agents and share only the
critical information that affects other agents, thus maintaining a distributed
vision of the task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07429</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07429</id><created>2015-01-29</created><updated>2015-05-06</updated><authors><author><keyname>Lefebvre</keyname><forenames>Nans</forenames></author></authors><title>Convergence law for hyper-graphs with prescribed degree sequences</title><categories>cs.LO</categories><comments>10 pages, 6 figures</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We view hyper-graphs as incidence graphs, i.e. bipartite graphs with a set of
nodes representing vertices and a set of nodes representing hyper-edges, with
two nodes being adjacent if the corresponding vertex belongs to the
corresponding hyper-edge. It defines a random hyper-multigraph specified by two
distributions, one for the degrees of the vertices, and one for the sizes of
the hyper-edges. We develop the logical analysis of this framework and first
prove a convergence law for first-order logic, then characterise the limit
first-order theories defined by a wide class of degree distributions.
Convergence laws of other models follow, and in particular for the classical
Erd\H{o}s-R\'enyi graphs and $k$-uniform hyper-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07430</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07430</id><created>2015-01-29</created><updated>2015-06-02</updated><authors><author><keyname>Lee</keyname><forenames>Juho</forenames></author><author><keyname>Choi</keyname><forenames>Seungjin</forenames></author></authors><title>Bayesian Hierarchical Clustering with Exponential Family: Small-Variance
  Asymptotics and Reducibility</title><categories>stat.ML cs.LG</categories><comments>10 pages, 2 figures, AISTATS-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian hierarchical clustering (BHC) is an agglomerative clustering method,
where a probabilistic model is defined and its marginal likelihoods are
evaluated to decide which clusters to merge. While BHC provides a few
advantages over traditional distance-based agglomerative clustering algorithms,
successive evaluation of marginal likelihoods and careful hyperparameter tuning
are cumbersome and limit the scalability. In this paper we relax BHC into a
non-probabilistic formulation, exploring small-variance asymptotics in
conjugate-exponential models. We develop a novel clustering algorithm, referred
to as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that
exhibits the scalability of distance-based agglomerative clustering algorithms
as well as the flexibility of Bayesian nonparametric models. We also
investigate the reducibility of the dissimilarity measure emerged from the
asymptotic limit of the BHC model, allowing us to use scalable algorithms such
as the nearest neighbor chain algorithm. Numerical experiments on both
synthetic and real-world datasets demonstrate the validity and high performance
of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07431</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07431</id><created>2015-01-29</created><authors><author><keyname>Ghosh</keyname><forenames>Bappaditya</forenames></author></authors><title>Negacyclic codes of odd length over the ring $\mathbb{F}_p[u,v]/\langle
  u^2,v^2,uv-vu\rangle$</title><categories>cs.IT math.IT</categories><msc-class>94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the structure of negacyclic codes of odd length over the ring
$\mathbb{F}_p[u, v]/ \langle u^2, v^2, uv-vu \rangle$. We find the unique
generating set, the rank and the minimum distance for these negacyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07438</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07438</id><created>2015-01-29</created><authors><author><keyname>Mineraud</keyname><forenames>Julien</forenames></author><author><keyname>Mazhelis</keyname><forenames>Oleksiy</forenames></author><author><keyname>Su</keyname><forenames>Xiang</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author></authors><title>Contemporary Internet of Things platforms</title><categories>cs.OH</categories><comments>6 pages, 0 figure, techreport, review of IoT platforms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document regroups a representative, but non-exhaustive, list of
contemporary IoT platforms. The platforms are ordered alphabetically. The aim
of this document is to provide the a quick review of current IoT platforms, as
well as relevant information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07439</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07439</id><created>2015-01-29</created><updated>2016-01-22</updated><authors><author><keyname>N&#xf6;tzel</keyname><forenames>Janis</forenames></author><author><keyname>Wiese</keyname><forenames>Moritz</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>The Arbitrarily Varying Wiretap Channel - Secret Randomness, Stability
  and Super-Activation</title><categories>cs.IT math.IT</categories><comments>Significant changes: now 48 pages, 2 figures. Changed naming
  convention of capacities. New Lemma 3 added and used in proof of Theorem 2.
  New insights regarding relations between different capacities on page 22</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the common randomness assisted capacity of an arbitrarily varying
channel (AVWC) when the Eavesdropper is kept ignorant about the common
randomness. We prove a multi-letter capacity formula for this model. We prove
that, if enough common randomness is used, the capacity formula can be given a
single-shot form again. We then consider the opposite extremal case, where no
common randomness is available. It is known that the capacity of the system can
be discontinuous under these circumstances. We prove here that it is still
stable in the sense that it is continuous around its positivity points. We
further prove that discontinuities can only arise if the legal link is
symmetrizable and characterize the points where it is positive. These results
shed new light on the design principles of communication systems with embedded
security features. At last we investigate the effect of super-activation of the
message transmission capacity of AVWCs under the average error criterion. We
give a complete characterization of those AVWCs that may be super-activated.
The effect is thereby also related to the (conjectured) super-activation of the
common randomness assisted capacity of AVWCs with an eavesdropper that gets to
know the common randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07440</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07440</id><created>2015-01-29</created><updated>2015-02-11</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Limits on Support Recovery with Probabilistic Models: An
  Information-Theoretic Framework</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>Shortened version submitted to ISIT 2015. (v2) Changed title;
  submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The support recovery problem consists of determining a sparse subset of a set
of variables that is relevant in generating a set of observations, and arises
in a diverse range of settings such as group testing, compressive sensing, and
subset selection in regression. In this paper, we take a unified approach to
support recovery problems, considering general probabilistic observation models
relating a sparse data vector to an observation vector. We study the
information-theoretic limits of both exact and partial support recovery, taking
a novel approach motivated by thresholding techniques in channel coding. We
provide general achievability and converse bounds characterizing the trade-off
between the error probability and number of measurements, and we specialize
these bounds to variants of models from group testing, linear regression, and
1-bit compressive sensing. In several cases, our bounds not only provide
matching scaling laws in the necessary and sufficient number of measurements,
but also sharp thresholds with matching constant factors. Our approach has
several advantages over previous approaches: For the achievability part, we
obtain sharp thresholds under broader scalings of the sparsity level and other
parameters (e.g. signal-to-noise ratio) compared to several previous works, and
for the converse part, we not only provide conditions under which the error
probability fails to vanish, but also conditions under which it tends to one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07460</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07460</id><created>2015-01-29</created><authors><author><keyname>Kotrbcik</keyname><forenames>Michal</forenames></author><author><keyname>Skoviera</keyname><forenames>Martin</forenames></author></authors><title>Simple greedy 2-approximation algorithm for the maximum genus of a graph</title><categories>math.CO cs.DM</categories><comments>6 pages</comments><msc-class>Primary: 05C10 Secondary: 05C85, 05C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum genus $\gamma_M(G)$ of a graph G is the largest genus of an
orientable surface into which G has a cellular embedding. Combinatorially, it
coincides with the maximum number of disjoint pairs of adjacent edges of G
whose removal results in a connected spanning subgraph of G. In this paper we
prove that removing pairs of adjacent edges from G arbitrarily while retaining
connectedness leads to at least $\gamma_M(G)/2$ pairs of edges removed. This
allows us to describe a greedy algorithm for the maximum genus of a graph; our
algorithm returns an integer k such that $\gamma_M(G)/2\le k \le \gamma_M(G)$,
providing a simple method to efficiently approximate maximum genus. As a
consequence of our approach we obtain a 2-approximate counterpart of Xuong's
combinatorial characterisation of maximum genus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07464</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07464</id><created>2015-01-29</created><authors><author><keyname>Charlier</keyname><forenames>Emilie</forenames></author><author><keyname>Harju</keyname><forenames>Tero</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca</forenames></author></authors><title>Abelian bordered factors and periodicity</title><categories>cs.FL cs.DM</categories><comments>14 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite word u is said to be bordered if u has a proper prefix which is also
a suffix of u, and unbordered otherwise. Ehrenfeucht and Silberger proved that
an infinite word is purely periodic if and only if it contains only finitely
many unbordered factors. We are interested in abelian and weak abelian
analogues of this result; namely, we investigate the following question(s): Let
w be an infinite word such that all sufficiently long factors are (weakly)
abelian bordered; is w (weakly) abelian periodic? In the process we answer a
question of Avgustinovich et al. concerning the abelian critical factorization
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07467</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07467</id><created>2015-01-29</created><authors><author><keyname>Zamani</keyname><forenames>Hamed</forenames></author><author><keyname>Shakery</keyname><forenames>Azadeh</forenames></author><author><keyname>Moradi</keyname><forenames>Pooya</forenames></author></authors><title>Regression and Learning to Rank Aggregation for User Engagement
  Evaluation</title><categories>cs.IR cs.LG</categories><comments>In Proceedings of the 2014 ACM Recommender Systems Challenge,
  RecSysChallenge '14</comments><acm-class>H.2.8; J.4</acm-class><doi>10.1145/2668067.2668077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User engagement refers to the amount of interaction an instance (e.g., tweet,
news, and forum post) achieves. Ranking the items in social media websites
based on the amount of user participation in them, can be used in different
applications, such as recommender systems. In this paper, we consider a tweet
containing a rating for a movie as an instance and focus on ranking the
instances of each user based on their engagement, i.e., the total number of
retweets and favorites it will gain.
  For this task, we define several features which can be extracted from the
meta-data of each tweet. The features are partitioned into three categories:
user-based, movie-based, and tweet-based. We show that in order to obtain good
results, features from all categories should be considered. We exploit
regression and learning to rank methods to rank the tweets and propose to
aggregate the results of regression and learning to rank methods to achieve
better performance. We have run our experiments on an extended version of
MovieTweeting dataset provided by ACM RecSys Challenge 2014. The results show
that learning to rank approach outperforms most of the regression models and
the combination can improve the performance significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07469</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07469</id><created>2015-01-29</created><updated>2015-05-12</updated><authors><author><keyname>Frieze</keyname><forenames>Alan</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>P&#xe9;rez-Gim&#xe9;nez</keyname><forenames>Xavier</forenames></author><author><keyname>Pra&#x142;at</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>On-line list colouring of random graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the on-line list colouring of binomial random graphs G(n,p) is
studied. We show that the on-line choice number of G(n,p) is asymptotically
almost surely asymptotic to the chromatic number of G(n,p), provided that the
average degree d=p(n-1) tends to infinity faster than (log log n)^1/3(log
n)^2n^(2/3). For sparser graphs, we are slightly less successful; we show that
if d&gt;(log n)^(2+epsilon) for some epsilon&gt;0, then the on-line choice number is
larger than the chromatic number by at most a multiplicative factor of C, where
C in [2,4], depending on the range of d. Also, for d=O(1), the on-line choice
number is by at most a multiplicative constant factor larger than the chromatic
number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07492</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07492</id><created>2015-01-29</created><updated>2015-09-08</updated><authors><author><keyname>Jiang</keyname><forenames>Huaizu</forenames></author></authors><title>Weakly Supervised Learning for Salient Object Detection</title><categories>cs.CV</categories><comments>technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in supervised salient object detection has resulted in
significant performance on benchmark datasets. Training such models, however,
requires expensive pixel-wise annotations of salient objects. Moreover, many
existing salient object detection models assume that at least one salient
object exists in the input image. Such an assumption often leads to less
appealing saliency maps on the background images, which contain no salient
object at all. To avoid the requirement of expensive pixel-wise salient region
annotations, in this paper, we study weakly supervised learning approaches for
salient object detection. Given a set of background images and salient object
images, we propose a solution toward jointly addressing the salient object
existence and detection tasks. We adopt the latent SVM framework and formulate
the two problems together in a single integrated objective function: saliency
labels of superpixels are modeled as hidden variables and involved in a
classification term conditioned to the salient object existence variable, which
in turn depends on both global image and regional saliency features and
saliency label assignment. Experimental results on benchmark datasets validate
the effectiveness of our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07496</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07496</id><created>2015-01-29</created><authors><author><keyname>Da Silva</keyname><forenames>E. L. F.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Implementation of an Automatic Syllabic Division Algorithm from Speech
  Files in Portuguese Language</title><categories>cs.SD cs.CL cs.DS</categories><comments>9 pages, 7 figures, 4 tables, conference: XIX Congresso Brasileiro de
  Automatica CBA, Campina Grande, Setembro, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07510</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07510</id><created>2015-01-29</created><updated>2015-06-12</updated><authors><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Throughput of a Cognitive Radio Network under Congestion Constraints: A
  Network-Level Study</title><categories>cs.NI cs.IT math.IT</categories><comments>Presented at CROWNCOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a cognitive radio network with one primary and one
secondary transmitter, in which the primary transmitter has bursty arrivals
while the secondary node is assumed to be saturated (i.e. always has a packet
waiting to be transmitted). The secondary node transmits in a cognitive way
such that it does not impede the performance of the primary node. We assume
that the receivers have multipacket reception (MPR) capabilities and that the
secondary node can take advantage of the MPR capability by transmitting
simultaneously with the primary under certain conditions. We obtain analytical
expressions for the stationary distribution of the primary node queue and we
also provide conditions for its stability. Finally, we provide expressions for
the aggregate throughput of the network as well as for the throughput at the
secondary node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07529</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07529</id><created>2015-01-29</created><updated>2015-05-01</updated><authors><author><keyname>Nandi</keyname><forenames>Kaushik</forenames></author><author><keyname>Paul</keyname><forenames>Goutam</forenames></author></authors><title>Quantum Information splitting using a pair of GHZ states</title><categories>quant-ph cs.CR</categories><journal-ref>In Quantum Information and Computation, pages 1041--1047, vol. 15,
  issue 11 &amp; 12, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a protocol for quantum information splitting (QIS) of a
restricted class of three-qubit states among three parties Alice, Bob and
Charlie, using a pair of GHZ states as the quantum channel. There are two
different forms of this three-qubit state that is used for QIS depending on the
distribution of the particles among the three parties. There is also a special
type of four-qubit state that can be used for QIS using the above channel. We
explicitly construct the quantum channel, Alice's measurement basis and the
analytic form of the unitary operations required by the receiver for such a
purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07539</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07539</id><created>2015-01-29</created><updated>2015-08-26</updated><authors><author><keyname>G&#xf6;bel</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author></authors><title>Counting Homomorphisms to Square-Free Graphs, Modulo 2</title><categories>cs.CC math.CO</categories><comments>32 pages, 8 figures (v4 adds Corollary 3.7 to fix a bug in the proof
  of Lemma 5.15; v3 is a minor update; v2 corrects a typo: we wrote &quot;dist&quot;
  instead of &quot;dom&quot; for the domain of a function in v1)</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem HomsTo$H$ of counting, modulo 2, the homomorphisms from
an input graph to a fixed undirected graph $H$. A characteristic feature of
modular counting is that cancellations make wider classes of instances
tractable than is the case for exact (non-modular) counting, so subtle
dichotomy theorems can arise. We show the following dichotomy: for any $H$ that
contains no 4-cycles, HomsTo$H$ is either in polynomial time or is $\oplus
P$-complete. This confirms a conjecture of Faben and Jerrum that was previously
only known to hold for trees and for a restricted class of treewidth-2 graphs
called cactus graphs. We confirm the conjecture for a rich class of graphs
including graphs of unbounded treewidth. In particular, we focus on square-free
graphs, which are graphs without 4-cycles. These graphs arise frequently in
combinatorics, for example in connection with the strong perfect graph theorem
and in certain graph algorithms. Previous dichotomy theorems required the graph
to be tree-like so that tree-like decompositions could be exploited in the
proof. We prove the conjecture for a much richer class of graphs by adopting a
much more general approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07544</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07544</id><created>2015-01-29</created><updated>2015-04-21</updated><authors><author><keyname>Naderializadeh</keyname><forenames>Navid</forenames></author><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>When Does an Ensemble of Matrices with Randomly Scaled Rows Lose Rank?</title><categories>cs.IT math.CO math.IT</categories><comments>submitted to IEEE Transactions on Information Theory; shorter version
  to appear at IEEE International Symposium on Information Theory (ISIT 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of determining rank loss conditions for a
concatenation of full-rank matrices, such that each row of the composing
matrices is scaled by a random coefficient. This problem has applications in
wireless interference management and recommendation systems. We determine
necessary and sufficient conditions for the design of each matrix, such that
the random ensemble will almost surely lose rank by a certain amount. The
result is proved by converting the problem to determining rank loss conditions
for the union of some specific matroids, and then using tools from matroid and
graph theories to derive the necessary and sufficient conditions. As an
application, we discuss how this result can be applied to the problem of
topological interference management, and characterize the linear symmetric
degrees of freedom for a class of network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07547</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07547</id><created>2015-01-29</created><updated>2015-02-25</updated><authors><author><keyname>Chen</keyname><forenames>Yanling</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Individual secrecy for broadcast channels with receiver side information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of secure communication over the broadcast
channel with receiver side information under the lens of individual secrecy
constraints. That is, the transmitter wants to send two independent messages to
two receivers which have, respectively, the desired message of the other
receiver as side information, while keeping the eavesdropper ignorant of each
message (i.e., the information leakage from each message to the eavesdropper is
made vanishing). Building upon one-time pad, secrecy coding, and broadcasting
schemes, achievable rate regions are investigated, and the capacity region for
special cases of either a weak or strong eavesdropper (compared to both
legitimate receivers) are characterized. Interestingly, the capacity region for
the former corresponds to a line and the latter corresponds to a square with
missing corners; a phenomenon occurring due to the coupling between user's
rates. Moreover, the individual secrecy capacity region is also fully
characterized for the case where the eavesdropper's channel is deterministic.
In addition to discrete memoryless setup, Gaussian scenarios are studied. For
the Gaussian model, in addition to the strong and weak eavesdropper cases, the
capacity region is characterized for the low and high SNR regimes when the
eavesdropper's channel is stronger than one receiver but weaker than the other.
Remarkably, positive secure transmission rates are always guaranteed under the
individual secrecy constraint, unlike the case of the joint secrecy constraint
(i.e., the information leakage from both messages to the eavesdropper is made
vanishing). Thus, this notion of secrecy serves as an appropriate candidate for
trading off secrecy level and transmission rate; making secrecy more affordable
but still acceptable to the end user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07556</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07556</id><created>2015-01-29</created><updated>2015-02-19</updated><authors><author><keyname>Halbawi</keyname><forenames>Wael</forenames></author><author><keyname>Thill</keyname><forenames>Matthew</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Coding with Constraints: Minimum Distance Bounds and Systematic
  Constructions</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine an error-correcting coding framework in which each coded symbol is
constrained to be a function of a fixed subset of the message symbols. With an
eye toward distributed storage applications, we seek to design systematic codes
with good minimum distance that can be decoded efficiently. On this note, we
provide theoretical bounds on the minimum distance of such a code based on the
coded symbol constraints. We refine these bounds in the case where we demand a
systematic linear code. Finally, we provide conditions under which each of
these bounds can be achieved by choosing our code to be a subcode of a
Reed-Solomon code, allowing for efficient decoding. This problem has been
considered in multisource multicast network error correction. The problem setup
is also reminiscent of locally repairable codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07584</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07584</id><created>2015-01-29</created><authors><author><keyname>Guo</keyname><forenames>Qi</forenames></author><author><keyname>Chen</keyname><forenames>Bo-Wei</forenames></author><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Ji</keyname><forenames>Xiangyang</forenames></author><author><keyname>Kung</keyname><forenames>Sun-Yuan</forenames></author></authors><title>Efficient Divide-And-Conquer Classification Based on Feature-Space
  Decomposition</title><categories>cs.LG</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a divide-and-conquer (DC) approach based on feature space
decomposition for classification. When large-scale datasets are present,
typical approaches usually employed truncated kernel methods on the feature
space or DC approaches on the sample space. However, this did not guarantee
separability between classes, owing to overfitting. To overcome such problems,
this work proposes a novel DC approach on feature spaces consisting of three
steps. Firstly, we divide the feature space into several subspaces using the
decomposition method proposed in this paper. Subsequently, these feature
subspaces are sent into individual local classifiers for training. Finally, the
outcomes of local classifiers are fused together to generate the final
classification results. Experiments on large-scale datasets are carried out for
performance evaluation. The results show that the error rates of the proposed
DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g.,
reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07586</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07586</id><created>2015-01-29</created><updated>2015-08-10</updated><authors><author><keyname>Pappas</keyname><forenames>Christos</forenames></author><author><keyname>Reischuk</keyname><forenames>Raphael M.</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author></authors><title>FAIR: Forwarding Accountability for Internet Reputability</title><categories>cs.NI</categories><comments>16 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents FAIR, a forwarding accountability mechanism that
incentivizes ISPs to apply stricter security policies to their customers. The
Autonomous System (AS) of the receiver specifies a traffic profile that the
sender AS must adhere to. Transit ASes on the path mark packets. In case of
traffic profile violations, the marked packets are used as a proof of
misbehavior.
  FAIR introduces low bandwidth overhead and requires no per-packet and no
per-flow state for forwarding. We describe integration with IP and demonstrate
a software switch running on commodity hardware that can switch packets at a
line rate of 120 Gbps, and can forward 140M minimum-sized packets per second,
limited by the hardware I/O subsystem.
  Moreover, this paper proposes a &quot;suspicious bit&quot; for packet headers - an
application that builds on top of FAIR's proofs of misbehavior and flags
packets to warn other entities in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07591</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07591</id><created>2015-01-29</created><authors><author><keyname>Krivulin</keyname><forenames>N.</forenames></author></authors><title>Direct algebraic solutions to constrained tropical optimization problems</title><categories>math.OC cs.SY</categories><comments>23 pages</comments><msc-class>65K10 (Primary), 15A80, 65K05, 90C48, 90B35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a new optimization problem formulated in the tropical mathematics
setting as an extension of certain known problems. The problem is to minimize a
nonlinear objective function, which is defined on vectors over an idempotent
semifield by using multiplicative conjugate transposition, subject to
inequality constraints. As compared to the known problems, the new one has a
more general objective function and additional constraints. We provide a
complete solution in an explicit form to the problem by using an approach that
introduces an additional variable to represent the values of the objective
function, and then reduces the initial problem to a parametrized vector
inequality. The minimum of the objective function is evaluated by applying the
existence conditions for the solution of this inequality. A complete solution
to the problem is given by the solutions of the inequality, provided the
parameter is set to the minimum value. As a consequence, we obtain solutions to
new special cases of the general problem. To illustrate the application of the
results, we solve a real-world problem drawn from project scheduling, and offer
a representative numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07594</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07594</id><created>2015-01-29</created><authors><author><keyname>Meier</keyname><forenames>Florian</forenames></author><author><keyname>Turau</keyname><forenames>Volker</forenames></author></authors><title>Analytical Model for IEEE 802.15.4 Multi-Hop Networks with Improved
  Handling of Acknowledgements and Retransmissions</title><categories>cs.NI</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 802.15.4 standard allows for the deployment of cost-effective and
energy-efficient multi-hop networks. This document features an in-depth
presentation of an analytical model for assessing the performance of such
networks. It considers a generic, static topology with Poisson distributed
data-collection as well as data-dissemination traffic. The unslotted CSMA/CA
MAC layer of IEEE 802.15.4 is closely modeled as well as an enhanced model of
the neighborhood allows for consideration of collisions of packets including
interferences with acknowledgements. The hidden node problem is taken into
account as well as a formerly disregarded effect of repeated collisions of
retransmissions. The model has been shown to be suitable to estimate the
capacity of large-scale multi-hop networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07621</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07621</id><created>2015-01-29</created><authors><author><keyname>von Csefalvay</keyname><forenames>Chris</forenames></author></authors><title>Ecological metrics of diversity in understanding social media</title><categories>cs.SI physics.soc-ph stat.AP</categories><msc-class>92D40</msc-class><acm-class>G.3; H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topical discussion networks (TDNs) are networks centered around a discourse
concerning a particular concept, whether in real life or online. This paper
analogises the population of such networks to populations encountered in
mathematical ecology, and seeks to evaluate whether three metrics of diversity
used in ecology - Shannon's $H'$, Simpson's $\lambda$ and $E_{var}$ proposed by
Smith and Wilson - give valuable information about the composition and
diversity of TDNs. It concludes that each metric has its particular use, and
the choice of metric is best understood in the context of the particular
research question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07627</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07627</id><created>2015-01-29</created><authors><author><keyname>Gallant</keyname><forenames>Stephen I.</forenames></author><author><keyname>Okaywe</keyname><forenames>T. Wendy</forenames></author></authors><title>Representing Objects, Relations, and Sequences</title><categories>cs.LG</categories><comments>41 pages</comments><journal-ref>Neural Computation 25, 2038-2078 (August 2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector Symbolic Architectures (VSAs) are high-dimensional vector
representations of objects (eg., words, image parts), relations (eg., sentence
structures), and sequences for use with machine learning algorithms. They
consist of a vector addition operator for representing a collection of
unordered objects, a Binding operator for associating groups of objects, and a
methodology for encoding complex structures.
  We first develop Constraints that machine learning imposes upon VSAs: for
example, similar structures must be represented by similar vectors. The
constraints suggest that current VSAs should represent phrases (&quot;The smart
Brazilian girl&quot;) by binding sums of terms, in addition to simply binding the
terms directly.
  We show that matrix multiplication can be used as the binding operator for a
VSA, and that matrix elements can be chosen at random. A consequence for living
systems is that binding is mathematically possible without the need to specify,
in advance, precise neuron-to-neuron connection properties for large numbers of
synapses.
  A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms),
is described that satisfies all Constraints.
  With respect to machine learning, for some types of problems appropriate VSA
representations permit us to prove learnability, rather than relying on
simulations. We also propose dividing machine (and neural) learning and
representation into three Stages, with differing roles for learning in each
stage.
  For neural modeling, we give &quot;representational reasons&quot; for nervous systems
to have many recurrent connections, as well as for the importance of phrases in
language processing.
  Sizing simulations and analyses suggest that VSAs in general, and MBAT in
particular, are ready for real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07637</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07637</id><created>2015-01-29</created><authors><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author><author><keyname>Weinberg</keyname><forenames>S. Matthew</forenames></author></authors><title>Simple Mechanisms for a Combinatorial Buyer and Applications to Revenue
  Monotonicity</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the revenue maximization problem of a seller with n heterogeneous
items for sale to a single buyer whose valuation function for sets of items is
unknown and drawn from some distribution D. We show that if D is a distribution
over subadditive valuations with independent items, then the better of pricing
each item separately or pricing only the grand bundle achieves a
constant-factor approximation to the revenue of the optimal mechanism. This
includes buyers who are k-demand, additive up to a matroid constraint, or
additive up to constraints of any downwards-closed set system (and whose values
for the individual items are sampled independently), as well as buyers who are
fractionally subadditive with item multipliers drawn independently. Our proof
makes use of the core-tail decomposition framework developed in prior work
showing similar results for the significantly simpler class of additive buyers
[LY13, BILW14].
  In the second part of the paper, we develop a connection between
approximately optimal simple mechanisms and approximate revenue monotonicity
with respect to buyers' valuations. Revenue non-monotonicity is the phenomenon
that sometimes strictly increasing buyers' values for every set can strictly
decrease the revenue of the optimal mechanism [HR12]. Using our main result, we
derive a bound on how bad this degradation can be (and dub such a bound a proof
of approximate revenue monotonicity); we further show that better bounds on
approximate monotonicity imply a better analysis of our simple mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07640</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07640</id><created>2015-01-29</created><authors><author><keyname>Kostina</keyname><forenames>Victoria</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Joint source-channel coding with feedback (extended)</title><categories>cs.IT math.IT</categories><comments>parts submitted to ISIT 2015 and ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper quantifies the fundamental limits of variable-length transmission
of a general (possibly analog) source over a memoryless channel with noiseless
feedback, under a distortion constraint. We consider excess distortion, average
distortion and guaranteed distortion (d-semifaithful codes). In contrast to the
asymptotic fundamental limit, a general conclusion is that allowing
variable-length codes and feedback leads to a sizable improvement in the
fundamental delay-distortion tradeoff. In addition, we investigate the minimum
energy required to reproduce k source samples with a given fidelity after
transmission over a memoryless Gaussian channel, and we show that the required
minimum energy is reduced with feedback and an average (rather than maximal)
power constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07645</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07645</id><created>2015-01-29</created><updated>2015-05-16</updated><authors><author><keyname>Talathi</keyname><forenames>Sachin S.</forenames></author></authors><title>Hyper-parameter optimization of Deep Convolutional Networks for object
  recognition</title><categories>cs.CV cs.LG</categories><comments>4 pages, 1 figure, 3 tables, Submitted to ICIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently sequential model based optimization (SMBO) has emerged as a
promising hyper-parameter optimization strategy in machine learning. In this
work, we investigate SMBO to identify architecture hyper-parameters of deep
convolution networks (DCNs) object recognition. We propose a simple SMBO
strategy that starts from a set of random initial DCN architectures to generate
new architectures, which on training perform well on a given dataset. Using the
proposed SMBO strategy we are able to identify a number of DCN architectures
that produce results that are comparable to state-of-the-art results on object
recognition benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07648</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07648</id><created>2015-01-29</created><authors><author><keyname>Ye</keyname><forenames>Chen</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Shimoi</keyname><forenames>Nobuhiro</forenames></author></authors><title>Improved Adaptive Sparse Channel Estimation Using Re-Weighted L1-norm
  Normalized Least Mean Fourth Algorithm</title><categories>cs.IT cs.SY math.IT</categories><comments>6 pages, 11 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In next-generation wireless communications systems, accurate sparse channel
estimation (SCE) is required for coherent detection. This paper studies SCE in
terms of adaptive filtering theory, which is often termed as adaptive channel
estimation (ACE). Theoretically, estimation accuracy could be improved by
either exploiting sparsity or adopting suitable error criterion. It motivates
us to develop effective adaptive sparse channel estimation (ASCE) methods to
improve estimation performance. In our previous research, two ASCE methods have
been proposed by combining forth-order error criterion based normalized least
mean fourth (NLMF) and L1-norm penalized functions, i.e., zero-attracting NLMF
(ZA-NLMF) algorithm and reweighted ZA-NLMF (RZA-NLMF) algorithm. Motivated by
compressive sensing theory, an improved ASCE method is proposed by using
reweighted L1-norm NLMF (RL1-NLMF) algorithm where RL1 can exploit more
sparsity information than ZA and RZA. Specifically, we construct the cost
function of RL1-NLMF and hereafter derive its update equation. In addition,
intuitive figure is also given to verify that RL1 is more efficient than
conventional two sparsity constraints. Finally, simulation results are provided
to confirm this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07658</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07658</id><created>2015-01-29</created><updated>2015-05-18</updated><authors><author><keyname>Zhao</keyname><forenames>Ming-Min</forenames></author><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author><author><keyname>Zhao</keyname><forenames>Min-Jian</forenames></author></authors><title>Robust Transceiver Design for MISO Interference Channel with Energy
  Harvesting</title><categories>cs.IT math.IT</categories><comments>13 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1407.0474 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we consider multiuser multiple-input single-output (MISO)
interference channel where the received signal is divided into two parts for
information decoding and energy harvesting (EH), respectively. The transmit
beamforming vectors and receive power splitting (PS) ratios are jointly
designed in order to minimize the total transmission power subject to both
signal-to-interference-plus-noise ratio (SINR) and EH constraints. Most joint
beamforming and power splitting (JBPS) designs assume that perfect channel
state information (CSI) is available; however CSI errors are inevitable in
practice. To overcome this limitation, we study the robust JBPS design problem
assuming a norm-bounded error (NBE) model for the CSI. Three different solution
approaches are proposed for the robust JBPS problem, each one leading to a
different computational algorithm. Firstly, an efficient semidefinite
relaxation (SDR)-based approach is presented to solve the highly non-convex
JBPS problem, where the latter can be formulated as a semidefinite programming
(SDP) problem. A rank-one recovery method is provided to recover a robust
feasible solution to the original problem. Secondly, based on second order cone
programming (SOCP) relaxation, we propose a low complexity approach with the
aid of a closed-form robust solution recovery method. Thirdly, a new iterative
method is also provided which can achieve near-optimal performance when the
SDR-based algorithm results in a higher-rank solution. We prove that this
iterative algorithm monotonically converges to a Karush-Kuhn-Tucker (KKT)
solution of the robust JBPS problem. Finally, simulation results are presented
to validate the robustness and efficiency of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07662</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07662</id><created>2015-01-29</created><authors><author><keyname>Bhandari</keyname><forenames>Ayush</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author></authors><title>Super-Resolution in Phase Space</title><categories>cs.IT math.IT</categories><comments>10 Pages, short paper in part accepted to ICASSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the problem of super-resolution. The goal is to resolve a
Dirac distribution from knowledge of its discrete, low-pass, Fourier
measurements. Classically, such problems have been dealt with parameter
estimation methods. Recently, it has been shown that convex-optimization based
formulations facilitate a continuous time solution to the super-resolution
problem. Here we treat super-resolution from low-pass measurements in Phase
Space. The Phase Space transformation parametrically generalizes a number of
well known unitary mappings such as the Fractional Fourier, Fresnel, Laplace
and Fourier transforms. Consequently, our work provides a general super-
resolution strategy which is backward compatible with the usual Fourier domain
result. We consider low-pass measurements of Dirac distributions in Phase Space
and show that the super-resolution problem can be cast as Total Variation
minimization. Remarkably, even though are setting is quite general, the bounds
on the minimum separation distance of Dirac distributions is comparable to
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07671</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07671</id><created>2015-01-30</created><authors><author><keyname>Bo&#xf1;golan</keyname><forenames>Vena Pearl</forenames></author><author><keyname>Baritua</keyname><forenames>Karessa Alexandra O.</forenames></author><author><keyname>Santos</keyname><forenames>Marie Junne</forenames></author></authors><title>Prioritizing the Components of Vulnerability in a Genetic Algorithms
  Minimization of Flood Risk</title><categories>cs.CE cs.CY</categories><comments>eight pages in pdf, with figures included</comments><acm-class>I.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare two prioritization schemes for the components of flooding
vulnerability: urbanized area ration, literacy rate, mortality rate, poverty,
radio/tv penetration, non-structural measures and structural measure. We
prioritize the components, giving each a weight. We then express the
vulnerability function as a weighted sum of its components. This weighted sum
serves as the fitness function in a genetic algorithm, which comes up with the
optimal design for a flood-resistant city.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07676</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07676</id><created>2015-01-30</created><authors><author><keyname>Atoum</keyname><forenames>Issa</forenames></author><author><keyname>Bong</keyname><forenames>Chih How</forenames></author><author><keyname>Kulathuramaiyer</keyname><forenames>Narayanan</forenames></author></authors><title>Towards Resolving Software Quality-in-Use Measurement Challenges</title><categories>cs.SE cs.CL</categories><comments>9 pages, 4 figures, Journal of Emerging Trends in Computing and
  Information Sciences, Vol. 5, No. 11, November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software quality-in-use comprehends the quality from user's perspectives. It
has gained its importance in e-learning applications, mobile service based
applications and project management tools. User's decisions on software
acquisitions are often ad hoc or based on preference due to difficulty in
quantitatively measure software quality-in-use. However, why quality-in-use
measurement is difficult? Although there are many software quality models to
our knowledge, no works surveys the challenges related to software
quality-in-use measurement. This paper has two main contributions; 1) presents
major issues and challenges in measuring software quality-in-use in the context
of the ISO SQuaRE series and related software quality models, 2) Presents a
novel framework that can be used to predict software quality-in-use, and 3)
presents preliminary results of quality-in-use topic prediction. Concisely, the
issues are related to the complexity of the current standard models and the
limitations and incompleteness of the customized software quality models. The
proposed framework employs sentiment analysis techniques to predict software
quality-in-use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07680</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07680</id><created>2015-01-30</created><updated>2016-01-20</updated><authors><author><keyname>Chakrabarti</keyname><forenames>Subit</forenames></author><author><keyname>Judge</keyname><forenames>Jasmeet</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author><author><keyname>Ranka</keyname><forenames>Sanjay</forenames></author></authors><title>Disaggregation of Remotely Sensed Soil Moisture in Heterogeneous
  Landscapes using Holistic Structure based Models</title><categories>cs.CV</categories><comments>28 pages, 14 figures, submitted to IEEE Transactions on Geoscience
  and Remote Sensing</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a novel machine learning algorithm is presented for
disaggregation of satellite soil moisture (SM) based on self-regularized
regressive models (SRRM) using high-resolution correlated information from
auxiliary sources. It includes regularized clustering that assigns soft
memberships to each pixel at fine-scale followed by a kernel regression that
computes the value of the desired variable at all pixels. Coarse-scale remotely
sensed SM were disaggregated from 10km to 1km using land cover, precipitation,
land surface temperature, leaf area index, and in-situ observations of SM. This
algorithm was evaluated using multi-scale synthetic observations in NC Florida
for heterogeneous agricultural land covers. It was found that the root mean
square error (RMSE) for 96% of the pixels was less than 0.02 $m^3/m^3$. The
clusters generated represented the data well and reduced the RMSE by upto 40%
during periods of high heterogeneity in land-cover and meteorological
conditions. The Kullback Leibler divergence (KLD) between the true SM and the
disaggregated estimates is close to 0, for both vegetated and baresoil
landcovers. The disaggregated estimates were compared to those generated by the
Principle of Relevant Information (PRI) method. The RMSE for the PRI
disaggregated estimates is higher than the RMSE for the SRRM on each day of the
season. The KLD of the disaggregated estimates generated by the SRRM is at
least four orders of magnitude lower than those for the PRI disaggregated
estimates, while the computational time needed was reduced by three times. The
results indicate that the SRRM can be used for disaggregating SM with complex
non-linear correlations on a grid with high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07681</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07681</id><created>2015-01-30</created><authors><author><keyname>Yang</keyname><forenames>Lan</forenames></author><author><keyname>Wang</keyname><forenames>Jingbin</forenames></author><author><keyname>Tu</keyname><forenames>Yujin</forenames></author><author><keyname>Mahapatra</keyname><forenames>Prarthana</forenames></author><author><keyname>Cardoso</keyname><forenames>Nelson</forenames></author></authors><title>Vector Quantization by Minimizing Kullback-Leibler Divergence</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method for vector quantization by minimizing the
Kullback-Leibler Divergence between the class label distributions over the
quantization inputs, which are original vectors, and the output, which is the
quantization subsets of the vector set. In this way, the vector quantization
output can keep as much information of the class label as possible. An
objective function is constructed and we also developed an iterative algorithm
to minimize it. The new method is evaluated on bag-of-features based image
classification problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07683</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07683</id><created>2015-01-30</created><authors><author><keyname>Chakrabarti</keyname><forenames>Subit</forenames></author><author><keyname>Judge</keyname><forenames>Jasmeet</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author><author><keyname>Ranka</keyname><forenames>Sanjay</forenames></author></authors><title>Downscaling Microwave Brightness Temperatures Using Self Regularized
  Regressive Models</title><categories>cs.CV</categories><comments>7 pages, 4 figures, submitted to be presented at the International
  Geoscience and Remote Sensing Conference 2015</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel algorithm is proposed to downscale microwave brightness temperatures
($\mathrm{T_B}$), at scales of 10-40 km such as those from the Soil Moisture
Active Passive mission to a resolution meaningful for hydrological and
agricultural applications. This algorithm, called Self-Regularized Regressive
Models (SRRM), uses auxiliary variables correlated to $\mathrm{T_B}$ along-with
a limited set of \textit{in-situ} SM observations, which are converted to high
resolution $\mathrm{T_B}$ observations using biophysical models. It includes an
information-theoretic clustering step based on all auxiliary variables to
identify areas of similarity, followed by a kernel regression step that
produces downscaled $\mathrm{T_B}$. This was implemented on a multi-scale
synthetic data-set over NC-Florida for one year. An RMSE of 5.76~K with
standard deviation of 2.8~k was achieved during the vegetated season and an
RMSE of 1.2~K with a standard deviation of 0.9~K during periods of no
vegetation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07686</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07686</id><created>2015-01-30</created><authors><author><keyname>Guellouma</keyname><forenames>Younes</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Cherroun</keyname><forenames>Hadda</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>Construction of rational expression from tree automata using a
  generalization of Arden's Lemma</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arden's Lemma is a classical result in language theory allowing the
computation of a rational expression denoting the language recognized by a
finite string automaton. In this paper we generalize this important lemma to
the rational tree languages. Moreover, we propose also a construction of a
rational tree expression which denotes the accepted tree language of a finite
tree automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07687</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07687</id><created>2015-01-30</created><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Optimistic-Conservative Bidding in Sequential Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider selling items using a sequential first price auction
mechanism. We generalize the assumption of conservative bidding to extensive
form games (henceforth optimistic conservative bidding), and show that for both
linear and unit demand valuations, the only pure subgame perfect equilibrium
where buyers are bidding in an optimistic conservative manner is the minimal
Walrasian equilibrium.
  In addition, we show examples where without the requirement of conservative
bidding, subgame perfect equilibria can admit a variety of unlikely
predictions, including high price of anarchy and low revenue in markets
composed of additive bidders, equilibria which elicit all the surplus as
revenue, and more. We also show that the order in which the items are sold can
influence the outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07692</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07692</id><created>2015-01-30</created><authors><author><keyname>Sottile</keyname><forenames>Matthew</forenames></author></authors><title>Blob indentation identification via curvature measurement</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method for identifying indentations on the
boundary of solid 2D shape. It uses the signed curvature at a set of points
along the boundary to identify indentations and provides one parameter for
tuning the selection mechanism for discriminating indentations from other
boundary irregularities. An efficient implementation is described based on the
Fourier transform for calculating curvature from a sequence of points obtained
from the boundary of a binary blob.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07695</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07695</id><created>2015-01-30</created><authors><author><keyname>Lauzier</keyname><forenames>Matthieu</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Risset</keyname><forenames>Tanguy</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Fraboulet</keyname><forenames>Antoine</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes, INSA Lyon</affiliation></author></authors><title>Live Group Detection for Mobile Wireless Sensor Networks</title><categories>cs.NI</categories><comments>BodyNets 2014 - 9th International Conference on Body Area Networks,
  Sep 2014, Londres, United Kingdom</comments><proxy>ccsd</proxy><doi>10.4108/icst.bodynets.2014.257026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with distributed algorithms for monitoring the topology of a
dynamic group of mobile wireless sensor networks. We propose two major
extensions of a distributed static group consensus algorithm and an
experimental implementation. Group consensus algorithms are exploited to let
each node obtain the knowledge of its connected com-ponents. The proposed
extensions provide a more accurate information about the proximity of nodes and
allow to deal with dynamic networks using a periodical reevaluation of the
group detection. We validate these algorithms by implementing them in an
original and challenging application scenario, in the context of a real bicycle
race. The real traces thus obtained and analyzed show the effectiveness of our
live group detection implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07701</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07701</id><created>2015-01-30</created><authors><author><keyname>Passerat-Palmbach</keyname><forenames>Jonathan</forenames><affiliation>UBP, LIMOS</affiliation></author><author><keyname>Mazel</keyname><forenames>Claude</forenames><affiliation>LIMOS, UBP</affiliation></author><author><keyname>Mahul</keyname><forenames>Antoine</forenames><affiliation>UBP, LIMOS</affiliation></author><author><keyname>Hill</keyname><forenames>David</forenames><affiliation>UBP, LIMOS</affiliation></author></authors><title>Reliable Initialization of GPU-enabled Parallel Stochastic Simulations
  Using Mersenne Twister for Graphics Processors</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>European Simulation and Modelling 2010, Oct 2010, Essen, Belgium.
  pp.187 - 195</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel stochastic simulations tend to exploit more and more computing power
and they are now also developed for General Purpose Graphics Process Units
(GP-GPUs). Conse-quently, they need reliable random sources to feed their
applications. We propose a survey of the current Pseudo Random Numbers
Generators (PRNG) available on GPU. We give a particular focus to the recent
Mersenne Twister for Graphics Processors (MTGP) that has just been released.
Our work provides empirically checked statuses designed to initialize a
particular configuration of this generator, in order to prevent any potential
bias introduced by the parallelization of the PRNG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07704</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07704</id><created>2015-01-30</created><authors><author><keyname>&#x10c;&#xe1;p</keyname><forenames>Michal</forenames></author><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Kleiner</keyname><forenames>Alexander</forenames></author></authors><title>Complete Decentralized Method for On-Line Multi-Robot Trajectory
  Planning in Valid Infrastructures</title><categories>cs.RO</categories><comments>Accepted to International Conference on Automated Planning and
  Scheduling 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a system consisting of multiple mobile robots in which the user
can at any time issue relocation tasks ordering one of the robots to move from
its current location to a given destination location. In this paper, we deal
with the problem of finding a trajectory for each such relocation task that
avoids collisions with other robots. The chosen robot plans its trajectory so
as to avoid collision with other robots executing tasks that were issued
earlier. We prove that if all possible destinations of the relocation tasks
satisfy so-called valid infrastructure property, then this mechanism is
guaranteed to always succeed and provide a trajectory for the robot that
reaches the destination without colliding with any other robot. The
time-complexity of the approach on a fixed space-time discretization is only
quadratic in the number of robots. We demonstrate the applicability of the
presented method on several real-world maps and compare its performance against
a popular reactive approach that attempts to solve the collisions locally.
Besides being dead-lock free, the presented approach generates trajectories
that are significantly faster (up to 48% improvement) than the trajectories
resulting from local collision avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07716</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07716</id><created>2015-01-30</created><authors><author><keyname>Seitlinger</keyname><forenames>Paul</forenames></author><author><keyname>Kowald</keyname><forenames>Dominik</forenames></author><author><keyname>Kopeinik</keyname><forenames>Simone</forenames></author><author><keyname>Hasani-Mavriqi</keyname><forenames>Ilire</forenames></author><author><keyname>Ley</keyname><forenames>Tobias</forenames></author><author><keyname>Lex</keyname><forenames>Elisabeth</forenames></author></authors><title>Attention Please! A Hybrid Resource Recommender Mimicking
  Attention-Interpretation Dynamics</title><categories>cs.IR</categories><comments>Submitted to WWW'15 WebScience Track</comments><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classic resource recommenders like Collaborative Filtering (CF) treat users
as being just another entity, neglecting non-linear user-resource dynamics
shaping attention and interpretation. In this paper, we propose a novel hybrid
recommendation strategy that refines CF by capturing these dynamics. The
evaluation results reveal that our approach substantially improves CF and,
depending on the dataset, successfully competes with a computationally much
more expensive Matrix Factorization variant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07719</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07719</id><created>2015-01-30</created><updated>2015-06-19</updated><authors><author><keyname>Perkins</keyname><forenames>Simon</forenames></author><author><keyname>Marais</keyname><forenames>Patrick</forenames></author><author><keyname>Zwart</keyname><forenames>Jonathan</forenames></author><author><keyname>Natarajan</keyname><forenames>Iniyan</forenames></author><author><keyname>Tasse</keyname><forenames>Cyril</forenames></author><author><keyname>Smirnov</keyname><forenames>Oleg</forenames></author></authors><title>Montblanc: GPU accelerated Radio Interferometer Measurement Equations in
  support of Bayesian Inference for Radio Observations</title><categories>cs.DC astro-ph.IM cs.CV</categories><comments>Submitted to Astronomy and Computing
  (http://www.journals.elsevier.com/astronomy-and-computing). The code is
  available online at https://github.com/ska-sa/montblanc. 29 pages long, with
  10 figures, 6 tables and 3 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Montblanc, a GPU implementation of the Radio interferometer
measurement equation (RIME) in support of the Bayesian inference for radio
observations (BIRO) technique. BIRO uses Bayesian inference to select sky
models that best match the visibilities observed by a radio interferometer. To
accomplish this, BIRO evaluates the RIME multiple times, varying sky model
parameters to produce multiple model visibilities. Chi-squared values computed
from the model and observed visibilities are used as likelihood values to drive
the Bayesian sampling process and select the best sky model.
  As most of the elements of the RIME and chi-squared calculation are
independent of one another, they are highly amenable to parallel computation.
Additionally, Montblanc caters for iterative RIME evaluation to produce
multiple chi-squared values. Modified model parameters are transferred to the
GPU between each iteration.
  We implemented Montblanc as a Python package based upon NVIDIA's CUDA
architecture. As such, it is easy to extend and implement different pipelines.
At present, Montblanc supports point and Gaussian morphologies, but is designed
for easy addition of new source profiles.
  Montblanc's RIME implementation is performant: On an NVIDIA K40, it is
approximately 250 times faster than MeqTrees on a dual hexacore Intel E5-2620v2
CPU. Compared to the OSKAR simulator's GPU-implemented RIME components it is
7.7 and 12 times faster on the same K40 for single and double-precision
floating point respectively. However, OSKAR's RIME implementation is more
general than Montblanc's BIRO-tailored RIME.
  Theoretical analysis of Montblanc's dominant CUDA kernel suggests that it is
memory bound. In practice, profiling shows that is balanced between compute and
memory, as much of the data required by the problem is retained in L1 and L2
cache.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07721</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07721</id><created>2015-01-30</created><authors><author><keyname>Barba</keyname><forenames>L.</forenames></author><author><keyname>Caraballo</keyname><forenames>L. E.</forenames></author><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>J. M.</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>R.</forenames></author><author><keyname>P&#xe9;rez-Castillo</keyname><forenames>E.</forenames></author></authors><title>Asymmetric polygons with maximum area</title><categories>math.MG cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that a polygon inscribed in the circle is asymmetric if it contains no
two antipodal points being the endpoints of a diameter. Given $n$ diameters of
a circle and a positive integer $k&lt;n$, this paper addresses the problem of
computing a maximum area asymmetric $k$-gon having as vertices $k&lt;n$ endpoints
of the given diameters. The study of this type of polygons is motivated by
ethnomusiciological applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07723</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07723</id><created>2015-01-30</created><authors><author><keyname>Kalokidou</keyname><forenames>Vaia</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Piechocki</keyname><forenames>Robert</forenames></author></authors><title>A hybrid TIM-NOMA scheme for the SISO Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, submitted to IEEE ICC'15 - IEEE SCAN Workshop</comments><doi>10.1109/ICCW.2015.7247210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future mobile communication networks will require enhanced network efficiency
and reduced system overhead due to their user density and high data rate
demanding applications of the mobile devices. Research on Blind Interference
Alignment (BIA) and Topological Interference Management (TIM) has shown that
optimal Degrees of Freedom (DoF) can be achieved, in the absence of Channel
State Information (CSI) at the transmitters, reducing the network's overhead.
Moreover, the recently emerged Non-Orthogonal Multiple Access (NOMA) scheme
suggests a different multiple access approach, compared to the current
orthogonal methods employed in 4G networks, resulting in high capacity gains.
Our contribution is a hybrid TIM-NOMA scheme in Single-Input-Single-Output
(SISO) K-user cells, in which users are divided into T groups, and 1/T DoF is
achieved for each user. By superimposing users in the power domain, we
introduce a two-stage decoding process, managing 'inter-group' interference
based on the TIM principles, and 'intra-group' interference based on Successful
Interference Cancellation (SIC), as proposed by NOMA. We show that for high SNR
values the hybrid scheme can improve the sum rate by at least 100% when
compared to Time Division Multiple Access (TDMA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07725</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07725</id><created>2015-01-30</created><updated>2015-06-29</updated><authors><author><keyname>Dyer</keyname><forenames>Martin</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Haiko</forenames></author></authors><title>On the switch Markov chain for perfect matchings</title><categories>cs.DS math.CO</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple Markov chain, the switch chain, on the set of all perfect
matchings in a bipartite graph. This Markov chain was proposed by Diaconis,
Graham and Holmes as a possible approach to a sampling problem arising in
Statistics. We ask: for which classes of graphs is the Markov chain ergodic and
for which is it rapidly mixing? We provide a precise answer to the ergodicity
question and close bounds on the mixing question. We show for the first time
that the mixing time of the switch chain is polynomial in the case of monotone
graphs, a class that includes examples of interest in the statistical setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07738</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07738</id><created>2015-01-30</created><authors><author><keyname>Mor&#xe8;re</keyname><forenames>Olivier</forenames></author><author><keyname>Goh</keyname><forenames>Hanlin</forenames></author><author><keyname>Veillard</keyname><forenames>Antoine</forenames></author><author><keyname>Chandrasekhar</keyname><forenames>Vijay</forenames></author><author><keyname>Lin</keyname><forenames>Jie</forenames></author></authors><title>Co-Regularized Deep Representations for Video Summarization</title><categories>cs.CV</categories><comments>Video summarization, deep convolutional neural networks,
  co-regularized restricted Boltzmann machines</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compact keyframe-based video summaries are a popular way of generating
viewership on video sharing platforms. Yet, creating relevant and compelling
summaries for arbitrarily long videos with a small number of keyframes is a
challenging task. We propose a comprehensive keyframe-based summarization
framework combining deep convolutional neural networks and restricted Boltzmann
machines. An original co-regularization scheme is used to discover meaningful
subject-scene associations. The resulting multimodal representations are then
used to select highly-relevant keyframes. A comprehensive user study is
conducted comparing our proposed method to a variety of schemes, including the
summarization currently in use by one of the most popular video sharing
websites. The results show that our method consistently outperforms the
baseline schemes for any given amount of keyframes both in terms of
attractiveness and informativeness. The lead is even more significant for
smaller summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07740</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07740</id><created>2015-01-30</created><authors><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author><author><keyname>Wang</keyname><forenames>Ping-Chung</forenames></author></authors><title>Adaptive Compute-and-Forward with Lattice Codes Over Algebraic Integers</title><categories>cs.IT math.IT</categories><comments>17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the compute-and-forward relay network with limited feedback. A
novel scheme called adaptive compute-and-forward is proposed to exploit the
channel knowledge by working with the best ring of imaginary quadratic
integers. This is enabled by generalizing Construction A lattices to other
rings of imaginary quadratic integers which may not form principal ideal
domains and by showing such construction can produce good lattices for coding
in the sense of Poltyrev and for MSE quantization. Since there are channel
coefficients (complex numbers) which are closer to elements of rings of
imaginary quadratic integers other than Gaussian and Eisenstein integers, by
always working with the best ring among them, one expects to obtain a better
performance than that provided by working over Gaussian or Eisenstein integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07754</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07754</id><created>2015-01-30</created><updated>2015-02-09</updated><authors><author><keyname>Qin</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Zhong</keyname><forenames>Xiaoxiong</forenames></author></authors><title>NFCU: A New Friendship-based Routing with Buffer Management in
  Opportunistic Networks</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to some crucial
  parameters setting error in experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing in opportunistic networks is a very important and challenging problem
because opportunistic network utilizes the contact opportunities of mobile
nodes to achieve data communication.Social-based routing uses behavior of human
beings which can form a community with the same interests to deliver the
message.In this paper,we analyze the drawbacks of the original friendship-based
algorithm, which defined social pressure metric to determine nodes'friendship
community, but the metric couldn't distinguish the distribution
characterization of the connection length which has an important impact on the
selection of links with better quality. Further, the existing friendship-based
routing doesn't consider the buffer management, which is vital for routing
design in opportunistic networks. We propose a New Friendship-based routing
with buffer management based on Copy Utility, named NFCU. NFCU algorithm, which
not only considers the contact periods in constructing social pressure metric
to solve the drawbacks of the original friendship-based routing scheme
efficiently, but also considers the buffer management, that can efficiently
determine which copy of the message should be deleted timely according to the
copy utility function. Our proposed strategy can reduce the network overhead
significantly, and increase the message delivery ratio. The extensive
simulation results demonstrate that NFCU performs better than the original
friendship-based routing. Moreover, we compare NFCU with other four
classicalrouting schemes in opportunistic networks in terms of message delivery
ratio, average delay, and comprehensive metric- message delivery
ratio*(1/average delay). The simulation results show that our scheme NFCU can
achieve better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07756</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07756</id><created>2015-01-30</created><authors><author><keyname>Maitra</keyname><forenames>Arpita</forenames></author><author><keyname>Paul</keyname><forenames>Goutam</forenames></author></authors><title>A Resilient Quantum Secret Sharing Scheme</title><categories>quant-ph cs.CR</categories><comments>12 pages, 2 figures</comments><journal-ref>In International Journal of Theoretical Physics, pages 398--408,
  vol. 54, issue 2, February 2015</journal-ref><doi>10.1007/s10773-014-2233-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A resilient secret sharing scheme is supposed to generate the secret
correctly even after some shares are damaged. In this paper, we show how
quantum error correcting codes can be exploited to design a resilient quantum
secret sharing scheme, where a quantum state is shared among more than one
parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07758</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07758</id><created>2015-01-30</created><authors><author><keyname>Kellner</keyname><forenames>Elias</forenames></author><author><keyname>Dhital</keyname><forenames>Bibek</forenames></author><author><keyname>Reisert</keyname><forenames>Marco</forenames></author></authors><title>Gibbs-Ringing Artifact Removal Based on Local Subvoxel-shifts</title><categories>physics.med-ph cs.CV</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gibbs-ringing is a well known artifact which manifests itself as spurious
oscillations in the vicinity of sharp image transients, e.g. at tissue
boundaries. The origin can be seen in the truncation of k-space during MRI
data-acquisition. Consequently, correction techniques like Gegenbauer
reconstruction or extrapolation methods aim at recovering these missing data.
Here, we present a simple and robust method which exploits a different view on
the Gibbs-phenomena. The truncation in k-space can be interpreted as a
convolution with a sinc-function in image space. Hence, the severity of the
artifacts depends on how the sinc-function is sampled. We propose to
re-interpolate the image based on local, subvoxel shifts to sample the ringing
pattern at the zero-crossings of the oscillating sinc-function. With this, the
artifact can effectively and robustly be removed with a minimal amount of
smoothing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07773</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07773</id><created>2015-01-30</created><authors><author><keyname>Breuer</keyname><forenames>Felix</forenames></author><author><keyname>Zafeirakopoulos</keyname><forenames>Zafeirakis</forenames></author></authors><title>Polyhedral Omega: A New Algorithm for Solving Linear Diophantine Systems</title><categories>math.CO cs.SC math.NT</categories><comments>49 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polyhedral Omega is a new algorithm for solving linear Diophantine systems
(LDS), i.e., for computing a multivariate rational function representation of
the set of all non-negative integer solutions to a system of linear equations
and inequalities. Polyhedral Omega combines methods from partition analysis
with methods from polyhedral geometry. In particular, we combine MacMahon's
iterative approach based on the Omega operator and explicit formulas for its
evaluation with geometric tools such as Brion decompositions and Barvinok's
short rational function representations. In this way, we connect two recent
branches of research that have so far remained separate, unified by the concept
of symbolic cones which we introduce. The resulting LDS solver Polyhedral Omega
is significantly faster than previous solvers based on partition analysis and
it is competitive with state-of-the-art LDS solvers based on geometric methods.
Most importantly, this synthesis of ideas makes Polyhedral Omega the simplest
algorithm for solving linear Diophantine systems available to date. Moreover,
we provide an illustrated geometric interpretation of partition analysis, with
the aim of making ideas from both areas accessible to readers from a wide range
of backgrounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07774</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07774</id><created>2015-01-30</created><authors><author><keyname>Sharma</keyname><forenames>Vikram</forenames></author><author><keyname>Batra</keyname><forenames>Prashant</forenames></author></authors><title>Near Optimal Subdivision Algorithms for Real Root Isolation</title><categories>cs.NA cs.SC math.NA</categories><comments>19 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a subroutine that improves the running time of any subdivision
algorithm for real root isolation. The subroutine first detects clusters of
roots using a result of Ostrowski, and then uses Newton iteration to converge
to them. Near a cluster, we switch to subdivision, and proceed recursively. The
subroutine has the advantage that it is independent of the predicates used to
terminate the subdivision. This gives us an alternative and simpler approach to
recent developments of Sagraloff (2012) and Sagraloff-Mehlhorn (2013), assuming
exact arithmetic.
  The subdivision tree size of our algorithm using predicates based on
Descartes's rule of signs is bounded by $O(n\log n)$, which is better by
$O(n\log L)$ compared to known results. Our analysis differs in two key
aspects. First, we use the general technique of continuous amortization from
Burr-Krahmer-Yap (2009), and second, we use the geometry of clusters of roots
instead of the Davenport-Mahler bound. The analysis naturally extends to other
predicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07788</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07788</id><created>2015-01-30</created><updated>2015-07-15</updated><authors><author><keyname>Lenormand</keyname><forenames>Maxime</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Tugores</keyname><forenames>Ant&#xf2;nia</forenames></author><author><keyname>Ramasco</keyname><forenames>Jos&#xe9; J.</forenames></author></authors><title>Human diffusion and city influence</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an stat.AP</categories><comments>9 pages, 7 figures + appendix</comments><journal-ref>J. R. Soc. Interface 12, 20150473 (2015)</journal-ref><doi>10.1098/rsif.2015.0473</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cities are characterized by concentrating population, economic activity and
services. However, not all cities are equal and a natural hierarchy at local,
regional or global scales spontaneously emerges. In this work, we introduce a
method to quantify city influence using geolocated tweets to characterize human
mobility. Rome and Paris appear consistently as the cities attracting most
diverse visitors. The ratio between locals and non-local visitors turns out to
be fundamental for a city to truly be global. Focusing only on urban residents'
mobility flows, a city to city network can be constructed. This network allows
us to analyze centrality measures at different scales. New York and London play
a predominant role at the global scale, while urban rankings suffer substantial
changes if the focus is set at a regional level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07800</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07800</id><created>2015-01-30</created><updated>2015-09-18</updated><authors><author><keyname>Rubensson</keyname><forenames>Emanuel H.</forenames></author><author><keyname>Rudberg</keyname><forenames>Elias</forenames></author></authors><title>Locality-aware parallel block-sparse matrix-matrix multiplication using
  the Chunks and Tasks programming model</title><categories>cs.DC</categories><comments>22 pages, 13 figures</comments><msc-class>68N19, 65F50</msc-class><acm-class>D.1.3; F.1.2; G.1.0; G.4; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a library for parallel block-sparse matrix-matrix multiplication
on distributed memory clusters. By using a quadtree matrix representation data
locality is exploited without any prior information about the matrix sparsity
pattern. A distributed quadtree matrix representation is straightforward to
implement due to our recent development of the Chunks and Tasks programming
model [Parallel Comput. 40, 328 (2014)]. The quadtree representation combined
with the Chunks and Tasks model leads to favorable weak and strong scaling of
the communication cost with the number of processes, as shown both
theoretically and in numerical experiments.
  Matrices are represented by sparse quadtrees of chunk objects. The leaves in
the hierarchy are block-sparse submatrices. Sparsity is dynamically detected by
the matrix library and may occur at any level in the hierarchy and/or within
the submatrix leaves. In case Graphics Processing Units (GPUs) are available,
both CPUs and GPUs are used for leaf-level multiplication work, thus making use
of the full computing capacity of each node.
  The performance of our matrix-matrix multiplication implementation is
evaluated for dense and banded matrices and matrices with randomly distributed
submatrix blocks, as well as matrices from linear scaling electronic structure
calculations. We show that, compared to methods that do not exploit data
locality, our locality-aware approach reduces communication significantly,
achieving essentially constant communication per node in weak scaling tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07814</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07814</id><created>2015-01-30</created><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory Z.</forenames></author><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author></authors><title>Valued Workflow Satisfiability Problem</title><categories>cs.DS cs.CC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A workflow is a collection of steps that must be executed in some specific
order to achieve an objective. A computerised workflow management system may
enforce authorisation policies and constraints, thereby restricting which users
can perform particular steps in a workflow. The existence of policies and
constraints may mean that a workflow is unsatisfiable, in the sense that it is
impossible to find an authorised user for each step in the workflow and satisfy
all constraints. In this paper, we consider the problem of finding the &quot;least
bad&quot; assignment of users to workflow steps by assigning a weight to each policy
and constraint violation. To this end, we introduce a framework for associating
costs with the violation of workflow policies and constraints and define the
\emph{valued workflow satisfiability problem} (Valued WSP), whose solution is
an assignment of steps to users of minimum cost. We establish the computational
complexity of Valued WSP with user-independent constraints and show that it is
fixed-parameter tractable. We then describe an algorithm for solving Valued WSP
with user-independent constraints and evaluate its performance, comparing it to
that of an off-the-shelf mixed integer programming package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07844</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07844</id><created>2015-01-30</created><authors><author><keyname>Baxter</keyname><forenames>John S. H.</forenames></author><author><keyname>Rajchl</keyname><forenames>Martin</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author></authors><title>A Proximal Bregman Projection Approach to Continuous Max-Flow Problems
  Using Entropic Distances</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One issue limiting the adaption of large-scale multi-region segmentation is
the sometimes prohibitive memory requirements. This is especially troubling
considering advances in massively parallel computing and commercial graphics
processing units because of their already limited memory compared to the
current random access memory used in more traditional computation. To address
this issue in the field of continuous max-flow segmentation, we have developed
a \textit{pseudo-flow} framework using the theory of Bregman proximal
projections and entropic distances which implicitly represents flow variables
between labels and designated source and sink nodes. This reduces the memory
requirements for max-flow segmentation by approximately 20\% for Potts models
and approximately 30\% for hierarchical max-flow (HMF) and directed acyclic
graph max-flow (DAGMF) models. This represents a great improvement in the
state-of-the-art in max-flow segmentation, allowing for much larger problems to
be addressed and accelerated using commercially available graphics processing
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07847</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07847</id><created>2015-01-30</created><authors><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Olaniyi</keyname><forenames>Mikhail</forenames></author><author><keyname>Emuoyibofarhe</keyname><forenames>Justice</forenames></author><author><keyname>Osobu</keyname><forenames>Funbi</forenames></author></authors><title>Electronic Medication Prescribing Support System for Diagnosing Tropical
  Diseases</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the development of an e-prescription system for
diagnosing tropical diseases.Results after testing the developed system by
medical experts indicated that the e-prescription systems is more efficient and
less susceptible to common errors associated with the conventional handwritten
medical prescription and can also go a long way to help to improve patients
health outcome in the health industry especially in the tropics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07857</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07857</id><created>2015-01-30</created><authors><author><keyname>Mryglod</keyname><forenames>O.</forenames></author><author><keyname>Kenna</keyname><forenames>R.</forenames></author><author><keyname>Holovatch</keyname><forenames>Yu.</forenames></author><author><keyname>Berche</keyname><forenames>B.</forenames></author></authors><title>Predicting Results of the Research Excellence Framework using
  Departmental h-Index -- Revisited</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit our recent study [Predicting results of the Research Excellence
Framework using departmental h-index, Scientometrics, 2014, 1-16;
arXiv:1411.1996] in which we attempted to predict outcomes of the UK's Research
Excellence Framework (REF~2014) using the so-called departmental $h$-index.
Here we report that our predictions failed to anticipate with any accuracy
either overall REF outcomes or movements of individual institutions in the
rankings relative to their positions in the previous Research Assessment
Exercise (RAE~2008).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07860</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07860</id><created>2015-01-30</created><authors><author><keyname>Stoddard</keyname><forenames>Greg</forenames></author></authors><title>Popularity and Quality in Social News Aggregators: A Study of Reddit and
  Hacker News</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we seek to understand the relationship between the online
popularity of an article and its intrinsic quality. Prior experimental work
suggests that the relationship between quality and popularity can be very
distorted due to factors like social influence bias and inequality in
visibility. We conduct a study of popularity on two different social news
aggregators, Reddit and Hacker News. We define quality as the relative number
of votes an article would have received if each article was shown, in a
bias-free way, to an equal number of users. We propose a simple poisson
regression method to estimate this quality metric from time-series voting data.
We validate our methods on data from Reddit and Hacker News, as well the
experimental data from prior work. This method works well even though the
collected data is subject to common social media biases. Using these estimates,
we find that popularity on Reddit and Hacker News is a stronger reflection of
intrinsic quality than expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07862</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07862</id><created>2015-01-30</created><authors><author><keyname>Nandy</keyname><forenames>Mahua</forenames><affiliation>Pal</affiliation></author><author><keyname>Saha</keyname><forenames>Satadal</forenames></author></authors><title>An Analytical Study of different Document Image Binarization Methods</title><categories>cs.CV</categories><comments>National Conference on Computing and Communication Systems
  (COCOSYS-09), UIT, Burdwan, January 02-04, 2009, pp. 71-76</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document image has been the area of research for a couple of decades because
of its potential application in the area of text recognition, line recognition
or any other shape recognition from the image. For most of these purposes
binarization of image becomes mandatory as far as recognition is concerned.
Throughout couple decades standard algorithms have already been developed for
this purpose. Some of these algorithms are applicable to degraded image also.
Our objective behind this work is to study the existing techniques, compare
them in view of advantages and disadvantages and modify some of these
algorithms to optimize time or performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07864</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07864</id><created>2015-01-30</created><authors><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Wijsen</keyname><forenames>Jef</forenames></author></authors><title>A Trichotomy in the Data Complexity of Certain Query Answering for
  Conjunctive Queries</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A relational database is said to be uncertain if primary key constraints can
possibly be violated. A repair (or possible world) of an uncertain database is
obtained by selecting a maximal number of tuples without ever selecting two
distinct tuples with the same primary key value. For any Boolean query q,
CERTAINTY(q) is the problem that takes an uncertain database db on input, and
asks whether q is true in every repair of db. The complexity of this problem
has been particularly studied for q ranging over the class of self-join-free
Boolean conjunctive queries. A research challenge is to determine, given q,
whether CERTAINTY(q) belongs to complexity classes FO, P, or coNP-complete. In
this paper, we combine existing techniques for studying the above complexity
classification task. We show that for any self-join-free Boolean conjunctive
query q, it can be decided whether or not CERTAINTY(q) is in FO. Further, for
any self-join-free Boolean conjunctive query q, CERTAINTY(q) is either in P or
coNP-complete, and the complexity dichotomy is effective. This settles a
research question that has been open for ten years, since [9].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07865</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07865</id><created>2015-01-30</created><authors><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Emuoyibofarhe</keyname><forenames>Justice</forenames></author></authors><title>A Criticism of the Current Security, Privacy and Accountability Issues
  in Electronic Health Records</title><categories>cs.CR cs.CY</categories><comments>published (2014)</comments><journal-ref>International Journal of Applied Information Systems (IJAIS),
  2014, Vol.7, No.8, pp 11-18</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cryptography has been widely accepted for security and partly for privacy
control as discovered from past works. However, many of these works did not
provide a way to manage cryptographic keys effectively especially in EHR
applications, as this is the Achilles heel of cryptographic techniques
currently proposed. The issue of accountability for legitimate users also has
not been so popular and only a few considered it in EHR. Unless a different
approach is used, the reliant on cryptography and password or escrow based
system for key management will impede trust of the system and hence its
acceptability. Also users with right access should also be monitored without
affecting the clinician workflow. This paper presents a detailed review of some
selected recent approaches to ensuring security, privacy and accountability in
EHR and gaps for future research were also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07866</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07866</id><created>2015-01-28</created><authors><author><keyname>Ma</keyname><forenames>Zichen</forenames></author><author><keyname>Fokoue</keyname><forenames>Ernest</forenames></author></authors><title>A Comparison of Classifiers in Performing Speaker Accent Recognition
  Using MFCCs</title><categories>cs.SD stat.AP</categories><comments>9 pages, 7 figures</comments><msc-class>62H25, 62H30</msc-class><journal-ref>Open Journal of Statistics, 2014, 4, 258-266</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm involving Mel-Frequency Cepstral Coefficients (MFCCs) is
provided to perform signal feature extraction for the task of speaker accent
recognition. Then different classifiers are compared based on the MFCC feature.
For each signal, the mean vector of MFCC matrix is used as an input vector for
pattern recognition. A sample of 330 signals, containing 165 US voice and 165
non-US voice, is analyzed. By comparison, k-nearest neighbors yield the highest
average test accuracy, after using a cross-validation of size 500, and least
time being used in the computation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07867</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07867</id><created>2015-01-30</created><authors><author><keyname>Mousavi</keyname><forenames>Hojjat Seyed</forenames></author><author><keyname>Srinivas</keyname><forenames>Umamahesh</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Suo</keyname><forenames>Yuanming</forenames></author><author><keyname>Dao</keyname><forenames>Minh</forenames></author><author><keyname>Tran</keyname><forenames>Trac. D.</forenames></author></authors><title>Multi-task Image Classification via Collaborative, Hierarchical
  Spike-and-Slab Priors</title><categories>cs.CV</categories><comments>Accepted to International Conference in Image Processing (ICIP) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Promising results have been achieved in image classification problems by
exploiting the discriminative power of sparse representations for
classification (SRC). Recently, it has been shown that the use of
\emph{class-specific} spike-and-slab priors in conjunction with the
class-specific dictionaries from SRC is particularly effective in low training
scenarios. As a logical extension, we build on this framework for multitask
scenarios, wherein multiple representations of the same physical phenomena are
available. We experimentally demonstrate the benefits of mining joint
information from different camera views for multi-view face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07870</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07870</id><created>2015-01-30</created><authors><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Fractional Coloring (Orthogonal Access) achieves All-unicast Capacity
  (DoF) Region of Index Coding (TIM) if and only if Network Topology is Chordal</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main result of this work is that fractional coloring (orthogonal access)
achieves the all-unicast capacity (degrees of freedom (DoF)) region of the
index coding (topological interference management (TIM)) problem if and only if
the bipartite network topology graph (with sources on one side and destinations
on the other, and edges identifying presence of nontrivial channels whose
communication capacity is not zero or infinity) is chordal, i.e., every cycle
that can contain a chord, does contain a chord. The all-unicast capacity (DoF)
region includes the capacity (DoF) region for any arbitrary choice of a unicast
message set, so e.g., the results of Maleki and Jafar on the optimality of
orthogonal access for the sum-capacity (DoF) of one-dimensional convex networks
are recovered as a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07873</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07873</id><created>2015-01-30</created><updated>2015-07-21</updated><authors><author><keyname>Yu</keyname><forenames>Qian</forenames></author><author><keyname>Yang</keyname><forenames>Yongxin</forenames></author><author><keyname>Song</keyname><forenames>Yi-Zhe</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author></authors><title>Sketch-a-Net that Beats Humans</title><categories>cs.CV cs.NE</categories><comments>Accepted to BMVC 2015 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multi-scale multi-channel deep neural network framework that,
for the first time, yields sketch recognition performance surpassing that of
humans. Our superior performance is a result of explicitly embedding the unique
characteristics of sketches in our model: (i) a network architecture designed
for sketch rather than natural photo statistics, (ii) a multi-channel
generalisation that encodes sequential ordering in the sketching process, and
(iii) a multi-scale network ensemble with joint Bayesian fusion that accounts
for the different levels of abstraction exhibited in free-hand sketches. We
show that state-of-the-art deep networks specifically engineered for photos of
natural objects fail to perform well on sketch recognition, regardless whether
they are trained using photo or sketch. Our network on the other hand not only
delivers the best performance on the largest human sketch dataset to date, but
also is small in size making efficient training possible using just CPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07884</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07884</id><created>2015-01-30</created><updated>2015-02-10</updated><authors><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Geometry-based Estimation of Stability Region for A Class of Structure
  Preserving Power Grids</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing development of the electric power grid, the largest engineered
system ever, to an even more complicated and larger system requires a new
generation of stability assessment methods that are computationally tractable
and feasible in real-time. In this paper we first extend the recently
introduced Lyapunov Functions Family (LFF) transient stability assessment
approach, that has potential to reduce the computational cost on large scale
power grids, to structure-preserving power grids. Then, we introduce a new
geometry-based method to construct the stability region estimate of power
systems. Our conceptual demonstration shows that this new method can certify
stability of a broader set of initial conditions compared to the
minimization-based LFF method and the energy methods (closest UEP and
controlling UEP methods).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.07887</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1501.07887</id><created>2015-01-30</created><authors><author><keyname>Coniglio</keyname><forenames>Stefano</forenames></author><author><keyname>Grimm</keyname><forenames>Boris</forenames></author><author><keyname>Koster</keyname><forenames>Arie M. C. A.</forenames></author><author><keyname>Tieves</keyname><forenames>Martin</forenames></author><author><keyname>Werner</keyname><forenames>Axel</forenames></author></authors><title>Optimal offline virtual network embedding with rent-at-bulk aspects</title><categories>cs.NI</categories><acm-class>G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network virtualization techniques allow for the coexistence of many virtual
networks (VNs) jointly sharing the resources of an underlying substrate
network. The Virtual Network Embedding problem (VNE) arises when looking for
the most profitable set of VNs to embed onto the substrate. In this paper, we
address the offline version of the problem. We propose a Mixed-Integer Linear
Programming formulation to solve it to optimality which accounts for acceptance
and rejection of virtual network requests, allowing for both splittable and
unsplittable (single path) routing schemes. Our formulation also considers a
Rent-at-Bulk (RaB) model for the rental of substrate capacities where economies
of scale apply. To better emphasize the importance of RaB, we also compare our
method to a baseline one which only takes RaB into account a posteriori, once a
solution to VNE, oblivious to RaB, has been found. Computational experiments
show the viability of our approach, stressing the relevance of addressing RaB
directly with an exact formulation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00030</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00030</id><created>2015-01-30</created><authors><author><keyname>Bondugula</keyname><forenames>Sravanthi</forenames></author><author><keyname>Manjunatha</keyname><forenames>Varun</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author><author><keyname>Doermann</keyname><forenames>David</forenames></author></authors><title>SHOE: Supervised Hashing with Output Embeddings</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a supervised binary encoding scheme for image retrieval that
learns projections by taking into account similarity between classes obtained
from output embeddings. Our motivation is that binary hash codes learned in
this way improve both the visual quality of retrieval results and existing
supervised hashing schemes. We employ a sequential greedy optimization that
learns relationship aware projections by minimizing the difference between
inner products of binary codes and output embedding vectors. We develop a joint
optimization framework to learn projections which improve the accuracy of
supervised hashing over the current state of the art with respect to standard
and sibling evaluation metrics. We further boost performance by applying the
supervised dimensionality reduction technique on kernelized input CNN features.
Experiments are performed on three datasets: CUB-2011, SUN-Attribute and
ImageNet ILSVRC 2010. As a by-product of our method, we show that using a
simple k-nn pooling classifier with our discriminative codes improves over the
complex classification models on fine grained datasets like CUB and offer an
impressive compression ratio of 1024 on CNN features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00033</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00033</id><created>2015-01-30</created><authors><author><keyname>Giovanidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Corrales</keyname><forenames>Luis David Alvarez</forenames></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames></author></authors><title>Analyzing Interference from Static Cellular Cooperation using the
  Nearest Neighbour Model</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures, 12 total subfigures, WIOPT-SPASWIN 2015</comments><doi>10.1109/WIOPT.2015.7151121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of base station cooperation has recently been set within the
framework of Stochastic Geometry. Existing works consider that a user
dynamically chooses the set of stations that cooperate for his/her service.
However, this assumption often does not hold. Cooperation groups could be
predefined and static, with nodes connected by fixed infrastructure. To analyse
such a potential network, in this work we propose a grouping method based on
proximity. It is a variation of the so called Nearest Neighbour Model. We
restrict ourselves to the simplest case where only singles and pairs of base
stations are allowed to be formed. For this, two new point processes are
defined from the dependent thinning of a Poisson Point Process, one for the
singles and one for the pairs. Structural characteristics for the two are
provided, including their density, Voronoi surface, nearest neighbour, empty
space and J-function. We further make use of these results to analyse their
interference fields and give explicit formulas to their expected value and
their Laplace transform. The results constitute a novel toolbox towards the
performance evaluation of networks with static cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00045</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00045</id><created>2015-01-30</created><authors><author><keyname>Beyer</keyname><forenames>Dirk</forenames></author><author><keyname>L&#xf6;we</keyname><forenames>Stefan</forenames></author><author><keyname>Wendler</keyname><forenames>Philipp</forenames></author></authors><title>Domain-Type-Guided Refinement Selection Based on Sliced Path Prefixes</title><categories>cs.SE cs.PL</categories><comments>10 pages, 5 figures, 1 table, 4 algorithms</comments><report-no>MIP-1501</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstraction is a successful technique in software verification, and
interpolation on infeasible error paths is a successful approach to
automatically detect the right level of abstraction in counterexample-guided
abstraction refinement. Because the interpolants have a significant influence
on the quality of the abstraction, and thus, the effectiveness of the
verification, an algorithm for deriving the best possible interpolants is
desirable. We present an analysis-independent technique that makes it possible
to extract several alternative sequences of interpolants from one given
infeasible error path, if there are several reasons for infeasibility in the
error path. We take as input the given infeasible error path and apply a
slicing technique to obtain a set of error paths that are more abstract than
the original error path but still infeasible, each for a different reason. The
(more abstract) constraints of the new paths can be passed to a standard
interpolation engine, in order to obtain a set of interpolant sequences, one
for each new path. The analysis can then choose from this set of interpolant
sequences and select the most appropriate, instead of being bound to the single
interpolant sequence that the interpolation engine would normally return. For
example, we can select based on domain types of variables in the interpolants,
prefer to avoid loop counters, or compare with templates for potential loop
invariants, and thus control what kind of information occurs in the abstraction
of the program. We implemented the new algorithm in the open-source
verification framework CPAchecker and show that our proof-technique-independent
approach yields a significant improvement of the effectiveness and efficiency
of the verification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00046</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00046</id><created>2015-01-30</created><authors><author><keyname>King</keyname><forenames>Davis E.</forenames></author></authors><title>Max-Margin Object Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most object detection methods operate by applying a binary classifier to
sub-windows of an image, followed by a non-maximum suppression step where
detections on overlapping sub-windows are removed. Since the number of possible
sub-windows in even moderately sized image datasets is extremely large, the
classifier is typically learned from only a subset of the windows. This avoids
the computational difficulty of dealing with the entire set of sub-windows,
however, as we will show in this paper, it leads to sub-optimal detector
performance.
  In particular, the main contribution of this paper is the introduction of a
new method, Max-Margin Object Detection (MMOD), for learning to detect objects
in images. This method does not perform any sub-sampling, but instead optimizes
over all sub-windows. MMOD can be used to improve any object detection method
which is linear in the learned parameters, such as HOG or bag-of-visual-word
models. Using this approach we show substantial performance gains on three
publicly available datasets. Strikingly, we show that a single rigid HOG filter
can outperform a state-of-the-art deformable part model on the Face Detection
Data Set and Benchmark when the HOG filter is learned via MMOD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00050</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00050</id><created>2015-01-30</created><authors><author><keyname>Moumen</keyname><forenames>Hamouma</forenames></author></authors><title>Time-Free and Timer-Based Assumptions Can Be Combined to Solve
  Authenticated Byzantine Consensus</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To circumvent the FLP impossibility result in a deterministic way several
protocols have been proposed on top of an asynchronous distributed system
enriched with additional assumptions. In the context of Byzantine failures for
systems where at most t processes may exhibit a Byzantine behavior, two
approaches have been investigated to solve the consensus problem.The first,
relies on the addition of synchrony, called Timer-Based, but the second is
based on the pattern of the messages that are exchanged, called Time-Free. This
paper shows that both types of assumptions are not antagonist and can be
combined to solve authenticated Byzantine consensus. This combined assumption
considers a correct process pi, called 2t-BW, and a set X of 2t processes such
that, eventually, for each query broadcasted by a correct process pj of X, pj
receives a response from pi 2 X among the (n- t) first responses to that query
or both links connecting pi and pj are timely. Based on this combination, a
simple hybrid authenticated Byzantine consensus protocol,benefiting from the
best of both worlds, is proposed. Whereas many hybrid protocols have been
designed for the consensus problem in the crash model, this is, to our
knowledge, the first hybrid deterministic solution to the Byzantine consensus
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00052</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00052</id><created>2015-01-30</created><authors><author><keyname>Wu</keyname><forenames>Qingqing</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Chen</keyname><forenames>Wen</forenames></author></authors><title>Joint Tx/Rx Energy-Efficient Scheduling in Multi-Radio Networks: A
  Divide-and-Conque Approach</title><categories>cs.IT math.IT</categories><comments>ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the existing works on energy-efficient wireless communication systems
only consider the transmitter (Tx) or the receiver (Rx) side power consumption
but not both. Moreover, they often assume the static circuit power consumption.
To be more practical, this paper considers the joint Tx and Rx power
consumption in multiple-access radio networks, where the power model takes both
the transmission power and the dynamic circuit power into account. We formulate
the joint Tx and Rx energy efficiency (EE) maximization problem which is a
combinatorial-type one due to the indicator function for scheduling users and
activating radio links. The link EE and the user EE are then introduced which
have the similar structure as the system EE. Their hierarchical relationships
are exploited to tackle the problem using a divide-and-conquer approach, which
is only of linear complexity. We further reveal that the static receiving power
plays a critical role in the user scheduling. Finally, comprehensive numerical
results are provided to validate the theoretical findings and demonstrate the
effectiveness of the proposed algorithm for improving the system EE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00054</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00054</id><created>2015-01-30</created><authors><author><keyname>Aygun</keyname><forenames>Bengi</forenames></author><author><keyname>Boban</keyname><forenames>Mate</forenames></author><author><keyname>Wyglinski</keyname><forenames>Alexander M.</forenames></author></authors><title>ECPR: Environment- and Context-aware Combined Power and Rate Distributed
  Congestion Control for Vehicular Communications</title><categories>cs.OH</categories><comments>26 Pages, 12 Figures, 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safety and efficiency applications in vehicular networks rely on the exchange
of periodic messages between vehicles. These messages contain position, speed,
heading, and other vital information that makes the vehicles aware of their
surroundings. The drawback of exchanging periodic cooperative messages is that
they generate significant channel load. Decentralized Congestion Control (DCC)
algorithms have been proposed to control the channel load. However, while the
rationale for periodic message exchange is to improve awareness, existing DCC
algorithms do not use awareness as a metric for deciding when, at what power,
and at what rate the periodic messages need to be sent in order to make sure
that the hard-to-reach vehicles are informed. We propose ECPR, an environment
and context-aware DCC algorithm, which combines power and rate control to
improve cooperative awareness by adapting to both specific propagation
environments (such as urban intersections, open highways, suburban roads) as
well as application requirements (e.g., different target cooperative awareness
range). Using the current context that the vehicle operates in (e.g., speed,
direction, and application requirement), ECPR adjusts the transmit power of the
messages to reach the desired awareness ratio at the target distance while at
the same time controlling the channel load using an adaptive rate control
algorithm. By performing extensive simulations, including realistic propagation
and environment modeling and realistic vehicle contexts (varying demand on both
awareness range and rate), we show that ECPR can increase awareness by 20%
while keeping the channel load and interference at almost the same level. When
the awareness requirements allow, ECPR can improve the average message rate by
18% compared to algorithms that perform rate adaptation only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00060</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00060</id><created>2015-01-30</created><updated>2015-09-15</updated><authors><author><keyname>He</keyname><forenames>Xing</forenames></author><author><keyname>Qiu</keyname><forenames>Robert Caiming</forenames></author><author><keyname>Ai</keyname><forenames>Qian</forenames></author><author><keyname>Cao</keyname><forenames>Yinshuang</forenames></author><author><keyname>Gu</keyname><forenames>Jie</forenames></author><author><keyname>Jin</keyname><forenames>Zhijian</forenames></author></authors><title>A Random Matrix Theoretical Approach to Early Event Detection in Smart
  Grid</title><categories>stat.ME cs.LG</categories><comments>12 pages, 11 figures, submitted to IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power systems are developing very fast nowadays, both in size and in
complexity; this situation is a challenge for Early Event Detection (EED). This
paper proposes a data- driven unsupervised learning method to handle this
challenge. Specifically, the random matrix theories (RMTs) are introduced as
the statistical foundations for random matrix models (RMMs); based on the RMMs,
linear eigenvalue statistics (LESs) are defined via the test functions as the
system indicators. By comparing the values of the LES between the experimental
and the theoretical ones, the anomaly detection is conducted. Furthermore, we
develop 3D power-map to visualize the LES; it provides a robust auxiliary
decision-making mechanism to the operators. In this sense, the proposed method
conducts EED with a pure statistical procedure, requiring no knowledge of
system topologies, unit operation/control models, etc. The LES, as a key
ingredient during this procedure, is a high dimensional indictor derived
directly from raw data. As an unsupervised learning indicator, the LES is much
more sensitive than the low dimensional indictors obtained from supervised
learning. With the statistical procedure, the proposed method is universal and
fast; moreover, it is robust against traditional EED challenges (such as error
accumulations, spurious correlations, and even bad data in core area). Case
studies, with both simulated data and real ones, validate the proposed method.
To manage large-scale distributed systems, data fusion is mentioned as another
data processing ingredient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00062</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00062</id><created>2015-01-30</created><authors><author><keyname>Rao</keyname><forenames>Vadrevu Sree Hari</forenames></author><author><keyname>Kumar</keyname><forenames>Mallenahalli Naresh</forenames></author></authors><title>A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue
  Fever</title><categories>stat.ML cs.AI cs.LG</categories><comments>7 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1501.07093</comments><journal-ref>Information Technology in Biomedicine, IEEE Transactions on ,
  vol.16, no.1, pp.112,118, Jan. 2012</journal-ref><doi>10.1109/TITB.2011.2171978</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identification of the influential clinical symptoms and laboratory features
that help in the diagnosis of dengue fever in early phase of the illness would
aid in designing effective public health management and virological
surveillance strategies. Keeping this as our main objective we develop in this
paper, a new computational intelligence based methodology that predicts the
diagnosis in real time, minimizing the number of false positives and false
negatives. Our methodology consists of three major components (i) a novel
missing value imputation procedure that can be applied on any data set
consisting of categorical (nominal) and/or numeric (real or integer) (ii) a
wrapper based features selection method with genetic search for extracting a
subset of most influential symptoms that can diagnose the illness and (iii) an
alternating decision tree method that employs boosting for generating highly
accurate decision rules. The predictive models developed using our methodology
are found to be more accurate than the state-of-the-art methodologies used in
the diagnosis of the dengue fever.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00064</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00064</id><created>2015-01-30</created><authors><author><keyname>Wang</keyname><forenames>Huan</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel</forenames></author></authors><title>A Batchwise Monotone Algorithm for Dictionary Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a batchwise monotone algorithm for dictionary learning. Unlike the
state-of-the-art dictionary learning algorithms which impose sparsity
constraints on a sample-by-sample basis, we instead treat the samples as a
batch, and impose the sparsity constraint on the whole. The benefit of
batchwise optimization is that the non-zeros can be better allocated across the
samples, leading to a better approximation of the whole. To accomplish this, we
propose procedures to switch non-zeros in both rows and columns in the support
of the coefficient matrix to reduce the reconstruction error. We prove in the
proposed support switching procedure the objective of the algorithm, i.e., the
reconstruction error, decreases monotonically and converges. Furthermore, we
introduce a block orthogonal matching pursuit algorithm that also operates on
sample batches to provide a warm start. Experiments on both natural image
patches and UCI data sets show that the proposed algorithm produces a better
approximation with the same sparsity levels compared to the state-of-the-art
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00065</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00065</id><created>2015-01-30</created><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Cheng</keyname><forenames>Shin-Ming</forenames></author></authors><title>Sequential Defense Against Random and Intentional Attacks in Complex
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 14 figures</comments><doi>10.1103/PhysRevE.91.022805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network robustness against attacks is one of the most fundamental researches
in network science as it is closely associated with the reliability and
functionality of various networking paradigms. However, despite the study on
intrinsic topological vulnerabilities to node removals, little is known on the
network robustness when network defense mechanisms are implemented, especially
for networked engineering systems equipped with detection capabilities. In this
paper, a sequential defense mechanism is firstly proposed in complex networks
for attack inference and vulnerability assessment, where the data fusion center
sequentially infers the presence of an attack based on the binary attack status
reported from the nodes in the network. The network robustness is evaluated in
terms of the ability to identify the attack prior to network disruption under
two major attack schemes, i.e., random and intentional attacks. We provide a
parametric plug-in model for performance evaluation on the proposed mechanism
and validate its effectiveness and reliability via canonical complex network
models and real-world large-scale network topology. The results show that the
sequential defense mechanism greatly improves the network robustness and
mitigates the possibility of network disruption by acquiring limited attack
status information from a small subset of nodes in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00067</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00067</id><created>2015-01-30</created><updated>2016-02-11</updated><authors><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author></authors><title>Quantum interpretations of AWPP and APP</title><categories>quant-ph cs.CC</categories><comments>22 pages, 1 figure</comments><journal-ref>Quantum Information and Computation 16, pp.0498-0514 (2016)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AWPP is a complexity class introduced by Fenner, Fortnow, Kurtz, and Li,
which is defined using GapP functions. Although it is an important class as the
best upperbound of BQP, its definition seems to be somehow artificial, and
therefore it would be better if we have some &quot;physical interpretation&quot; of AWPP.
Here we provide a quantum physical interpretation of AWPP: we show that AWPP is
equal to the class of problems efficiently solved by a quantum computer with
the ability of postselecting an event whose probability is close to an FP
function. This result is applied to also obtain a quantum physical
interpretation of APP. In addition, we consider &quot;classical physical analogue&quot;
of these results, and show that a restricted version of ${\rm BPP}_{\rm path}$
contains ${\rm UP}\cap{\rm coUP}$ and is contained in WAPP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00068</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00068</id><created>2015-01-30</created><updated>2015-03-08</updated><authors><author><keyname>Sparks</keyname><forenames>Evan R.</forenames></author><author><keyname>Talwalkar</keyname><forenames>Ameet</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author></authors><title>TuPAQ: An Efficient Planner for Large-scale Predictive Analytic Queries</title><categories>cs.DB cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of massive datasets combined with the development of
sophisticated analytical techniques have enabled a wide variety of novel
applications such as improved product recommendations, automatic image tagging,
and improved speech-driven interfaces. These and many other applications can be
supported by Predictive Analytic Queries (PAQs). A major obstacle to supporting
PAQs is the challenging and expensive process of identifying and training an
appropriate predictive model. Recent efforts aiming to automate this process
have focused on single node implementations and have assumed that model
training itself is a black box, thus limiting the effectiveness of such
approaches on large-scale problems. In this work, we build upon these recent
efforts and propose an integrated PAQ planning architecture that combines
advanced model search techniques, bandit resource allocation via runtime
algorithm introspection, and physical optimization via batching. The result is
TuPAQ, a component of the MLbase system, which solves the PAQ planning problem
with comparable quality to exhaustive strategies but an order of magnitude more
efficiently than the standard baseline approach, and can scale to models
trained on terabytes of data across hundreds of machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00075</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00075</id><created>2015-01-31</created><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Byzantine Broadcast Under a Selective Broadcast Model for Single-hop
  Wireless Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores an old problem, {\em Byzantine fault-tolerant Broadcast}
(BB), under a new model, {\em selective broadcast model}. The new model
&quot;interpolates&quot; between the two traditional models in the literature. In
particular, it allows fault-free nodes to exploit the benefits of a broadcast
channel (a feature from reliable broadcast model) and allows faulty nodes to
send mismatching messages to different neighbors (a feature from point-to-point
model) simultaneously. The {\em selective broadcast} model is motivated by the
potential for {\em directional} transmissions on a wireless channel.
  We provide a collection of results for a single-hop wireless network under
the new model. First, we present an algorithm for {\em Multi-Valued} BB that is
order-optimal in bit complexity. Then, we provide an algorithm that is designed
to achieve BB efficiently in terms of message complexity. Third, we determine
some lower bounds on both bit and message complexities of BB problems in the
{\em selective broadcast model}. Finally, we present a conjecture on an &quot;exact&quot;
lower bound on the bit complexity of BB under the {\em selective broadcast}
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00076</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00076</id><created>2015-01-31</created><authors><author><keyname>Shahabuddin</keyname><forenames>Shahriar</forenames></author><author><keyname>Janhunen</keyname><forenames>Janne</forenames></author><author><keyname>Bayramoglu</keyname><forenames>Muhammet Fatih</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author><author><keyname>Ghazi</keyname><forenames>Amanullah</forenames></author><author><keyname>Silven</keyname><forenames>Olli</forenames></author></authors><title>Design of a Unified Transport Triggered Processor for LDPC/Turbo Decoder</title><categories>cs.IT math.IT</categories><comments>8 pages, 7 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the design of a programmable processor with transport
triggered architecture (TTA) for decoding LDPC and turbo codes. The processor
architecture is designed in such a manner that it can be programmed for LDPC or
turbo decoding for the purpose of internetworking and roaming between different
networks. The standard trellis based maximum a posteriori (MAP) algorithm is
used for turbo decoding. Unlike most other implementations, a supercode based
sum-product algorithm is used for the check node message computation for LDPC
decoding. This approach ensures the highest hardware utilization of the
processor architecture for the two different algorithms. Up to our knowledge,
this is the first attempt to design a TTA processor for the LDPC decoder. The
processor is programmed with a high level language to meet the time-to-market
requirement. The optimization techniques and the usage of the function units
for both algorithms are explained in detail. The processor achieves 22.64 Mbps
throughput for turbo decoding with a single iteration and 10.12 Mbps throughput
for LDPC decoding with five iterations for a clock frequency of 200 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00079</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00079</id><created>2015-01-31</created><authors><author><keyname>Huang</keyname><forenames>Kechao</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>EXIT Chart Analysis of Block Markov Superposition Transmission of Short
  Codes</title><categories>cs.IT math.IT</categories><comments>submitted to ISIT2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a modified extrinsic information transfer (EXIT) chart
analysis that takes into account the relation between mutual information (MI)
and bit-error-rate (BER) is presented to study the convergence behavior of
block Markov superposition transmission (BMST) of short codes (referred to as
basic codes). We show that the threshold curve of BMST codes using an iterative
sliding window decoding algorithm with a fixed decoding delay achieves a lower
bound in the high signal-to-noise ratio (SNR) region, while in the low SNR
region, due to error propagation, the thresholds of BMST codes become slightly
worse as the encoding memory increases. We also demonstrate that the threshold
results are consistent with finite-length performance simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00082</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00082</id><created>2015-01-31</created><authors><author><keyname>Sarvadevabhatla</keyname><forenames>Ravi Kiran</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Category-Epitomes : Discriminatively Minimalist Representations for
  Object Categories</title><categories>cs.CV</categories><comments>Submitted to ICIP-2015, 5 pages, sketch, object category recognition,
  temporal, sparse</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Freehand line sketches are an interesting and unique form of visual
representation. Typically, such sketches are studied and utilized as an end
product of the sketching process. However, we have found it instructive to
study the sketches as sequentially accumulated composition of drawing strokes
added over time. Studying sketches in this manner has enabled us to create
novel sparse yet discriminative sketch-based representations for object
categories which we term category-epitomes. Our procedure for obtaining these
epitomes concurrently provides a natural measure for quantifying the sparseness
underlying the original sketch, which we term epitome-score. We construct and
analyze category-epitomes and epitome-scores for freehand sketches belonging to
various object categories. Our analysis provides a novel viewpoint for studying
the semantic nature of object categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00087</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00087</id><created>2015-01-31</created><authors><author><keyname>Tassi</keyname><forenames>Andrea</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author></authors><title>On Optimization of Network-coded Scalable Multimedia Service
  Multicasting</title><categories>cs.IT cs.MM cs.NI cs.PF math.IT</categories><comments>Proc. of the 6th Systems and Networks Optimization for Wireless
  (SNOW) Workshop 2015, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the near future, the delivery of multimedia multicast services over
next-generation networks is likely to become one of the main pillars of future
cellular networks. In this extended abstract, we address the issue of
efficiently multicasting layered video services by defining a novel
optimization paradigm that is based on an Unequal Error Protection
implementation of Random Linear Network Coding, and aims to ensure target
service coverages by using a limited amount of radio resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00089</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00089</id><created>2015-01-31</created><updated>2015-07-03</updated><authors><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Klasing</keyname><forenames>Ralf</forenames></author><author><keyname>Neggaz</keyname><forenames>Yessin M.</forenames></author><author><keyname>Peters</keyname><forenames>Joseph G.</forenames></author></authors><title>Efficiently Testing T-Interval Connectivity in Dynamic Graphs</title><categories>cs.DS cs.DC</categories><comments>Long version of a CIAC 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many types of dynamic networks are made up of durable entities whose links
evolve over time. When considered from a {\em global} and {\em discrete}
standpoint, these networks are often modelled as evolving graphs, i.e. a
sequence of graphs ${\cal G}=(G_1,G_2,...,G_{\delta})$ such that $G_i=(V,E_i)$
represents the network topology at time step $i$. Such a sequence is said to be
$T$-interval connected if for any $t\in [1, \delta-T+1]$ all graphs in
$\{G_t,G_{t+1},...,G_{t+T-1}\}$ share a common connected spanning subgraph. In
this paper, we consider the problem of deciding whether a given sequence ${\cal
G}$ is $T$-interval connected for a given $T$. We also consider the related
problem of finding the largest $T$ for which a given ${\cal G}$ is $T$-interval
connected. We assume that the changes between two consecutive graphs are
arbitrary, and that two operations, {\em binary intersection} and {\em
connectivity testing}, are available to solve the problems. We show that
$\Omega(\delta)$ such operations are required to solve both problems, and we
present optimal $O(\delta)$ online algorithms for both problems. We extend our
online algorithms to a dynamic setting in which connectivity is based on the
recent evolution of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00093</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00093</id><created>2015-01-31</created><authors><author><keyname>Koyamada</keyname><forenames>Sotetsu</forenames></author><author><keyname>Shikauchi</keyname><forenames>Yumi</forenames></author><author><keyname>Nakae</keyname><forenames>Ken</forenames></author><author><keyname>Koyama</keyname><forenames>Masanori</forenames></author><author><keyname>Ishii</keyname><forenames>Shin</forenames></author></authors><title>Deep learning of fMRI big data: a novel approach to subject-transfer
  decoding</title><categories>stat.ML cs.LG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a technology to read brain states from measurable brain activities, brain
decoding are widely applied in industries and medical sciences. In spite of
high demands in these applications for a universal decoder that can be applied
to all individuals simultaneously, large variation in brain activities across
individuals has limited the scope of many studies to the development of
individual-specific decoders. In this study, we used deep neural network (DNN),
a nonlinear hierarchical model, to construct a subject-transfer decoder. Our
decoder is the first successful DNN-based subject-transfer decoder. When
applied to a large-scale functional magnetic resonance imaging (fMRI) database,
our DNN-based decoder achieved higher decoding accuracy than other baseline
methods, including support vector machine (SVM). In order to analyze the
knowledge acquired by this decoder, we applied principal sensitivity analysis
(PSA) to the decoder and visualized the discriminative features that are common
to all subjects in the dataset. Our PSA successfully visualized the
subject-independent features contributing to the subject-transferability of the
trained decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00094</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00094</id><created>2015-01-31</created><authors><author><keyname>Dovgopol</keyname><forenames>Roman</forenames></author><author><keyname>Nohelty</keyname><forenames>Matt</forenames></author></authors><title>Twitter Hash Tag Recommendation</title><categories>cs.IR cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The rise in popularity of microblogging services like Twitter has led to
increased use of content annotation strategies like the hashtag. Hashtags
provide users with a tagging mechanism to help organize, group, and create
visibility for their posts. This is a simple idea but can be challenging for
the user in practice which leads to infrequent usage. In this paper, we will
investigate various methods of recommending hashtags as new posts are created
to encourage more widespread adoption and usage. Hashtag recommendation comes
with numerous challenges including processing huge volumes of streaming data
and content which is small and noisy. We will investigate preprocessing methods
to reduce noise in the data and determine an effective method of hashtag
recommendation based on the popular classification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00096</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00096</id><created>2015-01-31</created><authors><author><keyname>Beyer</keyname><forenames>Dirk</forenames></author><author><keyname>Dangl</keyname><forenames>Matthias</forenames></author><author><keyname>Wendler</keyname><forenames>Philipp</forenames></author></authors><title>Combining k-Induction with Continuously-Refined Invariants</title><categories>cs.SE cs.PL</categories><comments>12 pages, 5 figures, 2 tables, 2 algorithms</comments><report-no>MIP-1503</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded model checking (BMC) is a well-known and successful technique for
finding bugs in software. k-induction is an approach to extend BMC-based
approaches from falsification to verification. Automatically generated
auxiliary invariants can be used to strengthen the induction hypothesis. We
improve this approach and further increase effectiveness and efficiency in the
following way: we start with light-weight invariants and refine these
invariants continuously during the analysis. We present and evaluate an
implementation of our approach in the open-source verification-framework
CPAchecker. Our experiments show that combining k-induction with
continuously-refined invariants significantly increases effectiveness and
efficiency, and outperforms all existing implementations of k-induction-based
software verification in terms of successful verification results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00101</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00101</id><created>2015-01-31</created><authors><author><keyname>Dovgopol</keyname><forenames>Roman</forenames></author><author><keyname>Rosonke</keyname><forenames>Matthew</forenames></author></authors><title>Hybrid Update/Invalidate Schemes for Cache Coherence Protocols</title><categories>cs.DC</categories><comments>Appearing in MSTUCA Scientific Bulletin - August 2015, sec:CS; 10
  pages, 6 figure</comments><report-no>ISSN 2079-0619</report-no><journal-ref>MSTUCA Scientific Bulletin 218 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general when considering cache coherence, write back schemes are the
default. These schemes invalidate all other copies of a data block during a
write. In this paper we propose several hybrid schemes that will switch between
updating and invalidating on processor writes at runtime, depending on program
conditions. We created our own cache simulator on which we could implement our
schemes, and generated data sets from both commercial benchmarks and through
artificial methods to run on the simulator. We analyze the results of running
the benchmarks with various schemes, and suggest further research that can be
done in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00111</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00111</id><created>2015-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Du</keyname><forenames>Yuxian</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>Nonextensive analysis on the local structure entropy of complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local structure entropy is a new method which is proposed to identify the
influential nodes in the complex networks. In this paper a new form of the
local structure entropy of the complex networks is proposed based on the
Tsallis entropy. The value of the entropic index $q$ will influence the
property of the local structure entropy. When the value of $q$ is equal to 0,
the nonextensive local structure entropy is degenerated to a new form of the
degree centrality. When the value of $q$ is equal to 1, the nonextensive local
structure entropy is degenerated to the existing form of the local structure
entropy. We also have find a nonextensive threshold value in the nonextensive
local structure entropy. When the value of $q$ is bigger than the nonextensive
threshold value, change the value of $q$ will has no influence on the property
of the local structure entropy, and different complex networks have different
nonextensive threshold value.
  The results in this paper show that the new nonextensive local structure
entropy is a generalised of the local structure entropy. It is more reasonable
and useful than the existing one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00112</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00112</id><created>2015-01-31</created><updated>2015-12-28</updated><authors><author><keyname>Krivine</keyname><forenames>Jean-Louis</forenames></author></authors><title>Bar recursion in classical realisability : dependent choice and well
  ordering of R</title><categories>cs.LO math.LO</categories><comments>11 pages</comments><msc-class>03E40</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about the bar recursion operator in the context of classical
realizability. After the pioneering work of Berardi, Bezem &amp; Coquand [1], T.
Streicher has shown [10], by means of their bar recursion operator, that the
realizability models of ZF, obtained from usual models of $\lambda$-calculus
(Scott domains, coherent spaces, . . .), satisfy the axiom of dependent choice.
We give a proof of this result, using the tools of classical realizability.
Moreover, we show that these realizability models satisfy the well ordering of
$\mathbb{R}$ and the continuum hypothesis These formulas are therefore realized
by closed $\lambda_c$-terms. This allows to obtain programs from proofs of
arithmetical formulas using all these axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00115</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00115</id><created>2015-01-31</created><authors><author><keyname>Lu</keyname><forenames>Can-Yi</forenames></author><author><keyname>Huang</keyname><forenames>De-Shuang</forenames></author></authors><title>Optimized Projection for Sparse Representation Based Classification</title><categories>cs.CV</categories><comments>Neurocomputing 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality reduction (DR) methods have been commonly used as a principled
way to understand the high-dimensional data such as facial images. In this
paper, we propose a new supervised DR method called Optimized Projection for
Sparse Representation based Classification (OP-SRC), which is based on the
recent face recognition method, Sparse Representation based Classification
(SRC). SRC seeks a sparse linear combination on all the training data for a
given query image, and make the decision by the minimal reconstruction
residual. OP-SRC is designed on the decision rule of SRC, it aims to reduce the
within-class reconstruction residual and simultaneously increase the
between-class reconstruction residual on the training data. The projections are
optimized and match well with the mechanism of SRC. Therefore, SRC performs
well in the OP-SRC transformed space. The feasibility and effectiveness of the
proposed method is verified on the Yale, ORL and UMIST databases with promising
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00116</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00116</id><created>2015-01-31</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Wang</keyname><forenames>Gongpu</forenames></author></authors><title>Intercept Behavior Analysis of Industrial Wireless Sensor Networks in
  the Presence of Eavesdropping Attack</title><categories>cs.IT math.IT</categories><comments>8 pages, IEEE Transactions on Industrial Informatics, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the intercept behavior of an industrial wireless sensor
network (WSN) consisting of a sink node and multiple sensors in the presence of
an eavesdropping attacker, where the sensors transmit their sensed information
to the sink node through wireless links. Due to the broadcast nature of radio
wave propagation, the wireless transmission from the sensors to the sink can be
readily overheard by the eavesdropper for interception purposes. In an
information-theoretic sense, the secrecy capacity of the wireless transmission
is the difference between the channel capacity of the main link (from sensor to
sink) and that of the wiretap link (from sensor to eavesdropper). If the
secrecy capacity becomes non-positive due to the wireless fading effect, the
sensor's data transmission could be successfully intercepted by the
eavesdropper and an intercept event occurs in this case. However, in industrial
environments, the presence of machinery obstacles, metallic frictions and
engine vibrations makes the wireless fading fluctuate drastically, resulting in
the degradation of the secrecy capacity. As a consequence, an optimal sensor
scheduling scheme is proposed in this paper to protect the legitimate wireless
transmission against the eavesdropping attack, where a sensor with the highest
secrecy capacity is scheduled to transmit its sensed information to the sink.
Closed-form expressions of the probability of occurrence of an intercept event
(called intercept probability) are derived for the conventional round-robin
scheduling and the proposed optimal scheduling schemes. Also, an asymptotic
intercept probability analysis is conducted to provide an insight into the
impact of the sensor scheduling on the wireless security. Numerical results
demonstrate that the proposed sensor scheduling scheme outperforms the
conventional round-robin scheduling in terms of the intercept probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00120</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00120</id><created>2015-01-31</created><authors><author><keyname>Kiyak</keyname><forenames>Cem</forenames></author><author><keyname>de Vries</keyname><forenames>Andreas</forenames></author></authors><title>Electricity markets regarding the operational flexibility of power
  plants</title><categories>cs.CY</categories><comments>6 pages, 2 figures, 3 tables</comments><msc-class>91B24</msc-class><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electricity market mechanisms designed to steer sustainable generation of
electricity play an important role for the energy transition intended to
mitigate climate change. One of the major problems is to complement volatile
renewable energy sources by operationally flexible capacity reserves. In this
paper a proposal is given to determine prices on electricity markets taking
into account the operational flexibility of power plants, such that the costs
of long-term capacity reserves can be paid by short-term electricity spot
markets. For this purpose, a measure of operational flexibility is introduced
enabling to compute an inflexibility fee charging each individual power plant
on a wholesale electricity spot market. The total sum of inflexibility fees
accumulated on the spot markets then can be used to finance a capacity market
keeping the necessary reserves to warrant grid reliability. Here each reserve
power plant then gets a reliability payment depending on its operational
flexibility. The proposal is applied to a small exemplary grid, illustrating
its main idea and also revealing the caveat that too high fees paradoxically
could create incentives to employ highly flexible power plants on the spot
market rather than to run them as backup capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00121</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00121</id><created>2015-01-31</created><updated>2015-02-03</updated><authors><author><keyname>Levenchuk</keyname><forenames>Anatoly</forenames></author></authors><title>Towards a Systems Engineering Essence</title><categories>cs.SE</categories><comments>28 pages. Recommended as an INCOSE Russian chapter product at 99th
  INCOSE Russian chapter meeting, 28-Jan-2015</comments><acm-class>D.2.0; D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SEMAT/OMG Essence provides a powerful Language and a Kernel for describing
software development processes. How can it be tweaked to apply it to systems
engineering methods description? We must harmonize Essence and various systems
engineering standards in order to provide a more formal system approach to
obtaining a Systems Engineering Essence. In this paper, an approach of using
Essence for systems engineering is presented. In this approach we partly
modified a Kernel only within engineering solution area of concerns and
completely preserved Language as an excellent situational method engineering
foundation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00130</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00130</id><created>2015-01-31</created><authors><author><keyname>Corneli</keyname><forenames>Joseph</forenames></author><author><keyname>Maclean</keyname><forenames>Ewen</forenames></author></authors><title>The Search for Computational Intelligence</title><categories>cs.NE cs.AI</categories><comments>8 pages. Submitted to Social Aspects of Cognition and Computing
  symposium at AISB 2015</comments><acm-class>F.1.1; I.6.3; J.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We define and explore in simulation several rules for the local evolution of
generative rules for 1D and 2D cellular automata. Our implementation uses
strategies from conceptual blending. We discuss potential applications to
modelling social dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00133</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00133</id><created>2015-01-31</created><authors><author><keyname>Jamieson</keyname><forenames>Kevin</forenames></author><author><keyname>Katariya</keyname><forenames>Sumeet</forenames></author><author><keyname>Deshpande</keyname><forenames>Atul</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>Sparse Dueling Bandits</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dueling bandit problem is a variation of the classical multi-armed bandit
in which the allowable actions are noisy comparisons between pairs of arms.
This paper focuses on a new approach for finding the &quot;best&quot; arm according to
the Borda criterion using noisy comparisons. We prove that in the absence of
structural assumptions, the sample complexity of this problem is proportional
to the sum of the inverse squared gaps between the Borda scores of each
suboptimal arm and the best arm. We explore this dependence further and
consider structural constraints on the pairwise comparison matrix (a particular
form of sparsity natural to this problem) that can significantly reduce the
sample complexity. This motivates a new algorithm called Successive Elimination
with Comparison Sparsity (SECS) that exploits sparsity to find the Borda winner
using fewer samples than standard algorithms. We also evaluate the new
algorithm experimentally with synthetic and real data. The results show that
the sparsity model and the new algorithm can provide significant improvements
over standard approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00134</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00134</id><created>2015-01-31</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Liu</keyname><forenames>Chi Harold</forenames></author><author><keyname>Jayawardena</keyname><forenames>Srimal</forenames></author></authors><title>The Emerging Internet of Things Marketplace From an Industrial
  Perspective: A Survey</title><categories>cs.CY</categories><comments>IEEE Transactions on Emerging Topics in Computing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is a dynamic global information network
consisting of internet-connected objects, such as Radio-frequency
identification (RFIDs), sensors, actuators, as well as other instruments and
smart appliances that are becoming an integral component of the future
internet. Over the last decade, we have seen a large number of the IoT
solutions developed by start-ups, small and medium enterprises, large
corporations, academic research institutes (such as universities), and private
and public research organisations making their way into the market. In this
paper, we survey over one hundred IoT smart solutions in the marketplace and
examine them closely in order to identify the technologies used,
functionalities, and applications. More importantly, we identify the trends,
opportunities and open challenges in the industry-based the IoT solutions.
Based on the application domain, we classify and discuss these solutions under
five different categories: smart wearable, smart home, smart, city, smart
environment, and smart enterprise. This survey is intended to serve as a
guideline and conceptual framework for future research in the IoT and to
motivate and inspire further developments. It also provides a systematic
exploration of existing research and suggests a number of potentially
significant research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00137</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00137</id><created>2015-01-31</created><updated>2015-10-30</updated><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Hybrid Radio/Free-Space Optical Design for Next Generation Backhaul
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deluge of date rate in today's networks imposes a cost burden on the
backhaul network design. Developing cost efficient backhaul solutions becomes
an exciting, yet challenging, problem. Traditional technologies for backhaul
networks include either radio-frequency backhauls (RF) or optical fibers (OF).
While RF is a cost-effective solution as compared to OF, it supports lower data
rate requirements. Another promising backhaul solution is the free-space optics
(FSO) as it offers both a high data rate and a relatively low cost. FSO,
however, is sensitive to nature conditions, e.g., rain, fog, line-of-sight.
This paper combines both RF and FSO advantages and proposes a hybrid RF/FSO
backhaul solution. It considers the problem of minimizing the cost of the
backhaul network by choosing either OF or hybrid RF/FSO backhaul links between
the base-stations (BS) so as to satisfy data rate, connectivity, and
reliability constraints. It shows that under a specified realistic assumption
about the cost of OF and hybrid RF/FSO links, the problem is equivalent to a
maximum weight clique problem, which can be solved with moderate complexity.
Simulation results show that the proposed solution shows a close-to-optimal
performance, especially for practical prices of the hybrid RF/FSO links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00138</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00138</id><created>2015-01-31</created><authors><author><keyname>Farzan</keyname><forenames>Azadeh</forenames></author><author><keyname>Kincaid</keyname><forenames>Zachary</forenames></author></authors><title>Compositional Invariant Generation via Linear Recurrence Analysis</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method for automatically generating numerical
invariants for imperative programs. Given a program, our procedure computes a
binary input/output relation on program states which over-approximates the
behaviour of the program. It is compositional in the sense that it operates by
decomposing the program into parts, computing an abstract meaning of each part,
and then composing the meanings. Our method for approximating loop behaviour is
based on first approximating the meaning of the loop body, extracting
recurrence relations from that approximation, and then using the closed forms
to approximate the loop. Our experiments demonstrate that on verification
tasks, our method is competitive with leading invariant generation and
verification tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00139</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00139</id><created>2015-01-31</created><authors><author><keyname>Shaghaghi</keyname><forenames>Mahdi</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Subspace Leakage Analysis and Improved DOA Estimation with Small Sample
  Size</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>37 pages, 10 figures, Submitted to the IEEE Transactions on Signal
  Processing in July 2014</comments><journal-ref>IEEE Trans. Signal Processing, vol. 63, no. 12, pp. 3251-3265,
  June 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical methods of DOA estimation such as the MUSIC algorithm are based on
estimating the signal and noise subspaces from the sample covariance matrix.
For a small number of samples, such methods are exposed to performance
breakdown, as the sample covariance matrix can largely deviate from the true
covariance matrix. In this paper, the problem of DOA estimation performance
breakdown is investigated. We consider the structure of the sample covariance
matrix and the dynamics of the root-MUSIC algorithm. The performance breakdown
in the threshold region is associated with the subspace leakage where some
portion of the true signal subspace resides in the estimated noise subspace. In
this paper, the subspace leakage is theoretically derived. We also propose a
two-step method which improves the performance by modifying the sample
covariance matrix such that the amount of the subspace leakage is reduced.
Furthermore, we introduce a phenomenon named as root-swap which occurs in the
root-MUSIC algorithm in the low sample size region and degrades the performance
of the DOA estimation. A new method is then proposed to alleviate this problem.
Numerical examples and simulation results are given for uncorrelated and
correlated sources to illustrate the improvement achieved by the proposed
methods. Moreover, the proposed algorithms are combined with the pseudo-noise
resampling method to further improve the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00141</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00141</id><created>2015-01-31</created><authors><author><keyname>Lagrange</keyname><forenames>Mathieu</forenames></author><author><keyname>Lafay</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Rossignol</keyname><forenames>Mathias</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Roebel</keyname><forenames>Axel</forenames></author></authors><title>An evaluation framework for event detection using a morphological model
  of acoustic scenes</title><categories>stat.ML cs.SD</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a model of environmental acoustic scenes which adopts a
morphological approach by ab-stracting temporal structures of acoustic scenes.
To demonstrate its potential, this model is employed to evaluate the
performance of a large set of acoustic events detection systems. This model
allows us to explicitly control key morphological aspects of the acoustic scene
and isolate their impact on the performance of the system under evaluation.
Thus, more information can be gained on the behavior of evaluated systems,
providing guidance for further improvements. The proposed model is validated
using submitted systems from the IEEE DCASE Challenge; results indicate that
the proposed scheme is able to successfully build datasets useful for
evaluating some aspects the performance of event detection systems, more
particularly their robustness to new listening conditions and the increasing
level of background sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00143</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00143</id><created>2015-01-31</created><authors><author><keyname>Gunes</keyname><forenames>Thouraya Toukabri</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author></authors><title>Hybrid model for LTE Network-Assisted D2D communications</title><categories>cs.NI</categories><doi>10.1007/978-3-319-07425-2_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New Architecture to support D2D communications, where discovery is made
directly between devices while communications occur with the help of the E-Node
B
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00145</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00145</id><created>2015-01-31</created><updated>2015-02-04</updated><authors><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>LACL</affiliation></author></authors><title>An in-between &quot;implicit&quot; and &quot;explicit&quot; complexity: Automata</title><categories>cs.LO cs.CC math.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implicit Computational Complexity makes two aspects implicit, by manipulating
programming languages rather than models of com-putation, and by internalizing
the bounds rather than using external measure. We survey how automata theory
contributed to complexity with a machine-dependant with implicit bounds model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00152</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00152</id><created>2015-01-31</created><updated>2015-06-18</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Leung</keyname><forenames>Samantha</forenames></author></authors><title>Minimizing Regret in Dynamic Decision Problems</title><categories>cs.AI</categories><comments>Full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The menu-dependent nature of regret-minimization creates subtleties when it
is applied to dynamic decision problems. Firstly, it is not clear whether
\emph{forgone opportunities} should be included in the \emph{menu}, with
respect to which regrets are computed, at different points of the decision
problem. If forgone opportunities are included, however, we can characterize
when a form of dynamic consistency is guaranteed. Secondly, more subtleties
arise when sophistication is used to deal with dynamic inconsistency. In the
full version of this paper, we examine, axiomatically and by common examples,
the implications of different menu definitions for sophisticated,
regret-minimizing agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00154</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00154</id><created>2015-01-31</created><updated>2016-02-20</updated><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author></authors><title>Localizability and Distributed Protocols for Bearing-Based Network
  Localization in Arbitrary Dimensions</title><categories>math.OC cs.SY</categories><comments>Accepted by Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of bearing-based network localization, which
aims to localize all the nodes in a static network given the locations of a
subset of nodes termed anchors and inter-node bearings measured in a common
reference frame. The contributions of the paper are twofold. Firstly, we
propose necessary and sufficient conditions for network localizability with
both algebraic and rigidity theoretic interpretations. The analysis of the
localizability heavily relies on the recently developed bearing rigidity theory
and a special matrix termed the bearing Laplacian. Secondly, we propose a
linear distributed protocol for bearing-based network localization. The
protocol can globally localize a network if and only if the network is
localizable. The sensitivity of the protocol to constant measurement errors is
also analyzed. One novelty of this work is that the localizability analysis and
localization protocol are applicable to networks in arbitrary dimensional
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00163</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00163</id><created>2015-01-31</created><updated>2015-06-10</updated><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Spectral Detection in the Censored Block Model</title><categories>cs.SI cond-mat.dis-nn cs.LG math.PR</categories><comments>ISIT 2015</comments><journal-ref>Information Theory (ISIT), 2015 IEEE International Symposium on ,
  vol., no., pp.1184-1188, 14-19 June 2015</journal-ref><doi>10.1109/ISIT.2015.7282642</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of partially recovering hidden binary variables from
the observation of (few) censored edge weights, a problem with applications in
community detection, correlation clustering and synchronization. We describe
two spectral algorithms for this task based on the non-backtracking and the
Bethe Hessian operators. These algorithms are shown to be asymptotically
optimal for the partial recovery problem, in that they detect the hidden
assignment as soon as it is information theoretically possible to do so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00164</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00164</id><created>2015-01-31</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Member</keyname><forenames>Chi Harold Liu</forenames></author><author><keyname>Jayawardena</keyname><forenames>Srimal</forenames></author><author><keyname>Chen</keyname><forenames>Min</forenames></author></authors><title>Context-aware Computing in the Internet of Things: A Survey on Internet
  of Things From Industrial Market Perspective</title><categories>cs.CY cs.NI</categories><comments>IEEE ACCESS 2015</comments><doi>10.1109/ACCESS.2015.2389854</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is a dynamic global information network
consisting of Internet-connected objects, such as RFIDs, sensors, and
actuators, as well as other instruments and smart appliances that are becoming
an integral component of the Internet. Over the last few years, we have seen a
plethora of IoT solutions making their way into the industry marketplace.
Context-aware communication and computing has played a critical role throughout
the last few years of ubiquitous computing and is expected to play a
significant role in the IoT paradigm as well. In this article, we examine a
variety of popular and innovative IoT solutions in terms of context-aware
technology perspectives. More importantly, we evaluate these IoT solutions
using a framework that we built around well-known context-aware computing
theories. This survey is intended to serve as a guideline and a conceptual
framework for contextaware product development and research in the IoT
paradigm. It also provides a systematic exploration of existing IoT products in
the marketplace and highlights a number of potentially significant research
directions and trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00166</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00166</id><created>2015-01-31</created><authors><author><keyname>Thij</keyname><forenames>Marijn ten</forenames></author><author><keyname>Ouboter</keyname><forenames>Tanneke</forenames></author><author><keyname>Worm</keyname><forenames>Daniel</forenames></author><author><keyname>Litvak</keyname><forenames>Nelly</forenames></author><author><keyname>Berg</keyname><forenames>Hans van den</forenames></author><author><keyname>Bhulai</keyname><forenames>Sandjai</forenames></author></authors><title>Modelling of trends in Twitter using retweet graph dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>16 pages, 5 figures, presented at WAW 2014</comments><journal-ref>Algorithms and Models for the Web Graph, 11th International
  Workshop, WAW 2014, Beijing, China, December 17-18, 2014, Proceedings pp
  132-147, Lecture Notes in Computer Science, Springer</journal-ref><doi>10.1007/978-3-319-13123-8_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we model user behaviour in Twitter to capture the emergence of
trending topics. For this purpose, we first extensively analyse tweet datasets
of several different events. In particular, for these datasets, we construct
and investigate the retweet graphs. We find that the retweet graph for a
trending topic has a relatively dense largest connected component (LCC). Next,
based on the insights obtained from the analyses of the datasets, we design a
mathematical model that describes the evolution of a retweet graph by three
main parameters. We then quantify, analytically and by simulation, the
influence of the model parameters on the basic characteristics of the retweet
graph, such as the density of edges and the size and density of the LCC.
Finally, we put the model in practice, estimate its parameters and compare the
resulting behavior of the model to our datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00172</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00172</id><created>2015-01-31</created><updated>2015-03-06</updated><authors><author><keyname>Durnoga</keyname><forenames>Konrad</forenames></author><author><keyname>Kazana</keyname><forenames>Tomasz</forenames></author><author><keyname>Zaj&#x105;c</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Zdanowicz</keyname><forenames>Maciej</forenames></author></authors><title>Leakage-resilient Cryptography with key derived from sensitive data</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of large space consumption for protocols
in the Bounded Retrieval Model (BRM), which require users to store large secret
keys subject to adversarial leakage. We propose a method to derive keys for
such protocols on-the-fly from weakly random private data (like text documents
or photos, users keep on their disks anyway for non-cryptographic purposes) in
such a way that no extra storage is needed. We prove that any leakage-resilient
protocol (belonging to a certain, arguably quite broad class) when run with a
key obtained this way retains a similar level of security as the original
protocol had. Additionally, we guarantee privacy of the data the actual keys
are derived from. That is, an adversary can hardly gain any knowledge about the
private data except that he could otherwise obtain via leakage. Our reduction
works in the Random Oracle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00182</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00182</id><created>2015-01-31</created><updated>2016-02-12</updated><authors><author><keyname>Rahmani</keyname><forenames>Mostafa</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author></authors><title>A Subspace Learning Approach to High-Dimensional Matrix Decomposition
  with Efficient Information Sampling</title><categories>cs.NA cs.DS cs.LG math.NA stat.ML</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the problem of low-rank plus sparse matrix
decomposition for big data. Conventional algorithms for matrix decomposition
use the entire data to extract the low-rank and sparse components, and are
based on optimization problems that scale with the dimension of the data, which
limit their scalability. Furthermore, the existing randomized approaches mostly
rely on uniform random sampling, which can be quite inefficient for many real
world data matrices that exhibit additional structures (e.g. clustering). In
this paper, a scalable subspace-pursuit approach that transforms the
decomposition problem to a subspace learning problem is proposed. The
decomposition is carried out using a small data sketch formed from sampled
columns/rows. Even when the data is sampled uniformly at random, it is shown
that the sufficient number of sampled columns/rows is roughly O(r \mu), where
\mu is the coherency parameter and r the rank of the low-rank component. In
addition, efficient sampling algorithms are proposed to address the problem of
column/row sampling from structured data. The proposed sampling algorithms can
be independently used for feature selection from high-dimensional data. The
proposed approach is amenable to online implementation and an online scheme is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00186</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00186</id><created>2015-01-31</created><updated>2015-05-01</updated><authors><author><keyname>Huang</keyname><forenames>Haiping</forenames></author><author><keyname>Toyoizumi</keyname><forenames>Taro</forenames></author></authors><title>Advanced Mean Field Theory of Restricted Boltzmann Machine</title><categories>cond-mat.stat-mech cs.LG q-bio.NC stat.ML</categories><comments>5 pages, 4 figures, accepted by Phys Rev E (Rapid Communication)</comments><journal-ref>Phys. Rev. E 91, 050101 (2015)</journal-ref><doi>10.1103/PhysRevE.91.050101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning in restricted Boltzmann machine is typically hard due to the
computation of gradients of log-likelihood function. To describe the network
state statistics of the restricted Boltzmann machine, we develop an advanced
mean field theory based on the Bethe approximation. Our theory provides an
efficient message passing based method that evaluates not only the partition
function (free energy) but also its gradients without requiring statistical
sampling. The results are compared with those obtained by the computationally
expensive sampling based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00189</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00189</id><created>2015-01-31</created><authors><author><keyname>Gad</keyname><forenames>Eyal En</forenames></author><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Li</keyname><forenames>Yue</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Rewriting Flash Memories by Message Passing</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constructs WOM codes that combine rewriting and error correction
for mitigating the reliability and the endurance problems in flash memory. We
consider a rewriting model that is of practical interest to flash applications
where only the second write uses WOM codes. Our WOM code construction is based
on binary erasure quantization with LDGM codes, where the rewriting uses
message passing and has potential to share the efficient hardware
implementations with LDPC codes in practice. We show that the coding scheme
achieves the capacity of the rewriting model. Extensive simulations show that
the rewriting performance of our scheme compares favorably with that of polar
WOM code in the rate region where high rewriting success probability is
desired. We further augment our coding schemes with error correction
capability. By drawing a connection to the conjugate code pairs studied in the
context of quantum error correction, we develop a general framework for
constructing error-correction WOM codes. Under this framework, we give an
explicit construction of WOM codes whose codewords are contained in BCH codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00190</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00190</id><created>2015-01-31</created><authors><author><keyname>Wang</keyname><forenames>Chuang</forenames></author><author><keyname>Agaskar</keyname><forenames>Ameya</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author></authors><title>Randomized Kaczmarz Algorithm for Inconsistent Linear Systems: An Exact
  MSE Analysis</title><categories>math.NA cs.IT cs.NA math.IT math.OC</categories><comments>5 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a complete characterization of the randomized Kaczmarz algorithm
(RKA) for inconsistent linear systems. The Kaczmarz algorithm, known in some
fields as the algebraic reconstruction technique, is a classical method for
solving large-scale overdetermined linear systems through a sequence of
projection operators; the randomized Kaczmarz algorithm is a recent proposal by
Strohmer and Vershynin to randomize the sequence of projections in order to
guarantee exponential convergence (in mean square) to the solutions. A flurry
of work followed this development, with renewed interest in the algorithm, its
extensions, and various bounds on their performance. Earlier, we studied the
special case of consistent linear systems and provided an exact formula for the
mean squared error (MSE) in the value reconstructed by RKA, as well as a simple
way to compute the exact decay rate of the error. In this work, we consider the
case of inconsistent linear systems, which is a more relevant scenario for most
applications. First, by using a &quot;lifting trick&quot;, we derive an exact formula for
the MSE given a fixed noise vector added to the measurements. Then we show how
to average over the noise when it is drawn from a distribution with known first
and second-order statistics. Finally, we demonstrate the accuracy of our exact
MSE formulas through numerical simulations, which also illustrate that previous
upper bounds in the literature may be several orders of magnitude too high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00192</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00192</id><created>2015-01-31</created><authors><author><keyname>Zhu</keyname><forenames>Menglong</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Pose and Shape Estimation with Discriminatively Learned Parts</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach for estimating the 3D pose and the 3D shape of an
object from a single image. Given a training set of view exemplars, we learn
and select appearance-based discriminative parts which are mapped onto the 3D
model from the training set through a facil- ity location optimization. The
training set of 3D models is summarized into a sparse set of shapes from which
we can generalize by linear combination. Given a test picture, we detect
hypotheses for each part. The main challenge is to select from these hypotheses
and compute the 3D pose and shape coefficients at the same time. To achieve
this, we optimize a function that minimizes simultaneously the geometric
reprojection error as well as the appearance matching of the parts. We apply
the alternating direction method of multipliers (ADMM) to minimize the
resulting convex function. We evaluate our approach on the Fine Grained 3D Car
dataset with superior performance in shape and pose errors. Our main and novel
contribution is the simultaneous solution for part localization, 3D pose and
shape by maximizing both geometric and appearance compatibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00193</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00193</id><created>2015-01-31</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Evolutionary Artificial Neural Network Based on Chemical Reaction
  Optimization</title><categories>cs.NE</categories><doi>10.1109/CEC.2011.5949872</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms (EAs) are very popular tools to design and evolve
artificial neural networks (ANNs), especially to train them. These methods have
advantages over the conventional backpropagation (BP) method because of their
low computational requirement when searching in a large solution space. In this
paper, we employ Chemical Reaction Optimization (CRO), a newly developed global
optimization method, to replace BP in training neural networks. CRO is a
population-based metaheuristics mimicking the transition of molecules and their
interactions in a chemical reaction. Simulation results show that CRO
outperforms many EA strategies commonly used to train neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00194</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00194</id><created>2015-01-31</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Real-Coded Chemical Reaction Optimization with Different Perturbation
  Functions</title><categories>cs.NE</categories><doi>10.1109/CEC.2012.6252925</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimics
the interactions of molecules in chemical reactions to search for the global
optimum. The perturbation function greatly influences the performance of CRO on
solving different continuous problems. In this paper, we study four different
probability distributions, namely, the Gaussian distribution, the Cauchy
distribution, the exponential distribution, and a modified Rayleigh
distribution, for the perturbation function of CRO. Different distributions
have different impacts on the solutions. The distributions are tested by a set
of well-known benchmark functions and simulation results show that problems
with different characteristics have different preference on the distribution
function. Our study gives guidelines to design CRO for different types of
optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00195</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00195</id><created>2015-01-31</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author></authors><title>Sensor Deployment for Air Pollution Monitoring Using Public
  Transportation System</title><categories>cs.NE</categories><doi>10.1109/CEC.2012.6256495</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Air pollution monitoring is a very popular research topic and many monitoring
systems have been developed. In this paper, we formulate the Bus Sensor
Deployment Problem (BSDP) to select the bus routes on which sensors are
deployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is
a recently proposed metaheuristic designed to solve a wide range of
optimization problems. Using the real world data, namely Hong Kong Island bus
route data, we perform a series of simulations and the results show that CRO is
capable of solving this optimization problem efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00196</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00196</id><created>2015-01-31</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author></authors><title>Optimal V2G Scheduling of Electric Vehicles and Unit Commitment using
  Chemical Reaction Optimization</title><categories>cs.NE</categories><doi>10.1109/CEC.2013.6557596</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An electric vehicle (EV) may be used as energy storage which allows the
bi-directional electricity flow between the vehicle's battery and the electric
power grid. In order to flatten the load profile of the electricity system, EV
scheduling has become a hot research topic in recent years. In this paper, we
propose a new formulation of the joint scheduling of EV and Unit Commitment
(UC), called EVUC. Our formulation considers the characteristics of EVs while
optimizing the system total running cost. We employ Chemical Reaction
Optimization (CRO), a general-purpose optimization algorithm to solve this
problem and the simulation results on a widely used set of instances indicate
that CRO can effectively optimize this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00197</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00197</id><created>2015-01-31</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author></authors><title>An Inter-molecular Adaptive Collision Scheme for Chemical Reaction
  Optimization</title><categories>cs.NE</categories><doi>10.1109/CEC.2014.6900234</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization techniques are frequently applied in science and engineering
research and development. Evolutionary algorithms, as a kind of general-purpose
metaheuristic, have been shown to be very effective in solving a wide range of
optimization problems. A recently proposed chemical-reaction-inspired
metaheuristic, Chemical Reaction Optimization (CRO), has been applied to solve
many global optimization problems. However, the functionality of the
inter-molecular ineffective collision operator in the canonical CRO design
overlaps that of the on-wall ineffective collision operator, which can
potential impair the overall performance. In this paper we propose a new
inter-molecular ineffective collision operator for CRO for global optimization.
To fully utilize our newly proposed operator, we also design a scheme to adapt
the algorithm to optimization problems with different search space
characteristics. We analyze the performance of our proposed algorithm with a
number of widely used benchmark functions. The simulation results indicate that
the new algorithm has superior performance over the canonical CRO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00199</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00199</id><created>2015-01-31</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Chemical Reaction Optimization for the Set Covering Problem</title><categories>cs.NE</categories><doi>10.1109/CEC.2014.6900233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The set covering problem (SCP) is one of the representative combinatorial
optimization problems, having many practical applications. This paper
investigates the development of an algorithm to solve SCP by employing chemical
reaction optimization (CRO), a general-purpose metaheuristic. It is tested on a
wide range of benchmark instances of SCP. The simulation results indicate that
this algorithm gives outstanding performance compared with other heuristics and
metaheuristics in solving SCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00200</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00200</id><created>2015-02-01</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Base Station Switching Problem for Green Cellular Networks with Social
  Spider Algorithm</title><categories>cs.NI</categories><doi>10.1109/CEC.2014.6900235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent explosion in mobile data, the energy consumption and carbon
footprint of the mobile communications industry is rapidly increasing. It is
critical to develop more energy-efficient systems in order to reduce the
potential harmful effects to the environment. One potential strategy is to
switch off some of the under-utilized base stations during off-peak hours. In
this paper, we propose a binary Social Spider Algorithm to give guidelines for
selecting base stations to switch off. In our implementation, we use a penalty
function to formulate the problem and manage to bypass the large number of
constraints in the original optimization problem. We adopt several randomly
generated cellular networks for simulation and the results indicate that our
algorithm can generate superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00202</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00202</id><created>2015-02-01</created><authors><author><keyname>Ji</keyname><forenames>Wen</forenames></author><author><keyname>Chen</keyname><forenames>Bo-Wei</forenames></author><author><keyname>Chen</keyname><forenames>Yiqiang</forenames></author></authors><title>Fountain Uncorrectable Sets and Finite-Length Analysis</title><categories>cs.IT math.IT</categories><comments>4 pages; 2 figures; submitted to ISIT2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decoding performance of Fountain codes for the binary erasure channel (BEC)
depends on two aspects. One is the essential code structure, on which stopping
set analysis operates. The other is the effect from the channel characteristic,
which is difficult to give a precise estimation. To tackle these problems, in
this paper, we propose a solution to analyzing the performance of Fountain
codes based on the uncorrectable set. We give the condition for Fountain
decoding failure over the BEC. Then, we conduct the analysis of uncorrectable
set on Fountain codes. Finally, we combine the stopping set and the
uncorrectable set to provide the integrated analysis on the performance of
Fountain codes for BEC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00206</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00206</id><created>2015-02-01</created><updated>2015-04-29</updated><authors><author><keyname>Alhamazani</keyname><forenames>Khalid</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Jayaraman</keyname><forenames>Prem Prakash</forenames></author><author><keyname>Mitra</keyname><forenames>Karan</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Rabhi</keyname><forenames>Fethi</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Wang</keyname><forenames>Lizhe</forenames></author></authors><title>Cross-Layer Multi-Cloud Real-Time Application QoS Monitoring and
  Benchmarking As-a-Service Framework</title><categories>cs.DC</categories><comments>A revised version of this technical report has been accepted for
  publication by IEEE Transactions on Cloud Computing on April 29, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing provides on-demand access to affordable hardware (multi-core
CPUs, GPUs, disks, and networking equipment) and software (databases,
application servers and data processing frameworks) platforms with features
such as elasticity, pay-per-use, low upfront investment and low time to market.
This has led to the proliferation of business critical applications that
leverage various cloud platforms. Such applications hosted on single or
multiple cloud provider platforms have diverse characteristics requiring
extensive monitoring and benchmarking mechanisms to ensure run-time Quality of
Service (QoS) (e.g., latency and throughput). This paper proposes, develops and
validates CLAMBS:Cross-Layer Multi-Cloud Application Monitoring and
Benchmarking as-a-Service for efficient QoS monitoring and benchmarking of
cloud applications hosted on multi-clouds environments. The major highlight of
CLAMBS is its capability of monitoring and benchmarking individual application
components such as databases and web servers, distributed across cloud layers,
spread among multiple cloud providers. We validate CLAMBS using prototype
implementation and extensive experimentation and show that CLAMBS efficiently
monitors and benchmarks application components on multi-cloud platforms
including Amazon EC2 and Microsoft Azure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00207</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00207</id><created>2015-02-01</created><authors><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Quantum game players can have advantage without discord</title><categories>quant-ph cs.CC cs.GT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last two decades have witnessed a rapid development of quantum
information processing, a new paradigm which studies the power and limit of
&quot;quantum advantages&quot; in various information processing tasks. Problems such as
when quantum advantage exists, and if existing, how much it could be, are at a
central position of these studies. In a broad class of scenarios, there are,
implicitly or explicitly, at least two parties involved, who share a state, and
the correlation in this shared state is the key factor to the efficiency under
concern. In these scenarios, the shared \emph{entanglement} or \emph{discord}
is usually what accounts for quantum advantage. In this paper, we examine a
fundamental problem of this nature from the perspective of game theory, a
branch of applied mathematics studying selfish behaviors of two or more
players. We exhibit a natural zero-sum game, in which the chance for any player
to win the game depends only on the ending correlation. We show that in a
certain classical equilibrium, a situation in which no player can further
increase her payoff by any local classical operation, whoever first uses a
quantum computer has a big advantage over its classical opponent. The
equilibrium is fair to both players and, as a shared correlation, it does not
contain any discord, yet a quantum advantage still exists. This indicates that
at least in game theory, the previous notion of discord as a measure of
non-classical correlation needs to be reexamined, when there are two players
with different objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00210</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00210</id><created>2015-02-01</created><authors><author><keyname>Tian</keyname><forenames>Jing</forenames></author><author><keyname>Cui</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Si-liang</forenames></author></authors><title>A New Parameter Estimation Algorithm Based on Sub-band Dual Frequency
  Conjugate LVT</title><categories>cs.IT math.IT stat.AP</categories><comments>27 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new parameter estimation algorithm, known as Sub-band Dual Frequency
Conjugate LVT (SDFC-LVT), is proposed for the ground moving targets. This
algorithm first constructs two sub-band signals with different central
frequencies. After that, the two signals are shifted by different values in
frequency domain and a new signal is constructed by multiplying one with the
conjugate of the other. Finally, Keystone transform and LVT operation are
performed on the constructed signal to attain the estimates. The cross-term and
the performance of the proposed method are analyzed in detail. Since the
equivalent carrier frequency is reduced greatly, the proposed method is capable
of obtaining the accurate parameter estimates and resolving the problem of
ambiguity which invalidates Keystone transform. It is search-free and can
compensate the range walk of multiple targets simultaneously, thereby reducing
the computational burden. The effectiveness of the proposed method is
demonstrated by both simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00212</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00212</id><created>2015-02-01</created><authors><author><keyname>Gomaa</keyname><forenames>Ahmad</forenames></author><author><keyname>Jalloul</keyname><forenames>Louay M. A.</forenames></author><author><keyname>Gomadam</keyname><forenames>Krishna S.</forenames></author><author><keyname>Tujkovic</keyname><forenames>Djordje</forenames></author><author><keyname>Mansour</keyname><forenames>Mohammad M.</forenames></author></authors><title>Multi-User MIMO Receivers With Partial State Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-user multiple-input multiple-output (MU-MIMO) system that
uses orthogonal frequency division multiplexing (OFDM). Several receivers are
developed for data detection of MU-MIMO transmissions where two users share the
same OFDM time and frequency resources. The receivers have partial state
information about the MU-MIMO transmission with each receiver having knowledge
of the MU-MIMO channel, however the modulation constellation of the
co-scheduled user is unknown. We propose a joint maximum likelihood (ML)
modulation classification of the co-scheduled user and data detection receiver
using the max-log-MAP approximation. It is shown that the decision metric for
the modulation classification is an accumulation over a set of tones of
Euclidean distance computations that are also used by the max-log-MAP detector
for bit log-likelihood ratio (LLR) soft decision generation. An efficient
hardware implementation emerges that exploits this commonality between the
classification and detection steps and results in sharing of the hardware
resources. Comparisons of the link performance of the proposed receiver to
several linear receivers is demonstrated through computer simulations. It is
shown that the proposed receiver offers \unit[1.5]{dB} improvement in
signal-to-noise ratio (SNR) over the nulling projection receiver at $1\%$ block
error rate (BLER) for $64$-QAM with turbo code rate of $1/2$ in the case of
zero transmit and receiver antenna correlations. However, in the case of high
antenna correlation, the linear receiver approaches suffer significant loss
relative to the optimal receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00215</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00215</id><created>2015-02-01</created><authors><author><keyname>Konstantinou</keyname><forenames>Charalambos</forenames></author></authors><title>A Study on the Impact of Wind Generation on the Stability of
  Electromechanical Oscillations</title><categories>cs.SY</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wind is becoming an increasingly significant source of energy in modern power
generation. Amongst existing technologies, Variable Speed Wind Turbines (VSWT)
equipped with Double Fed Induction Generators (DFIG) is widely deployed.
Consequently, power systems are now experiencing newer power flow patterns and
operating conditions. This paper investigates the impact of a DFIG based Wind
Farm (WF) on the stability of electromechanical oscillations. This is achieved
by performing modal analysis to evaluate the stability of a two-area power
network when subjected to different wind penetration levels and different
geographical installed locations. The approach via eigenvalues analysis
involves the design of voltage and Supplementary Damping Controllers (SDCs)
that contribute to network damping. The effect of Power System Stabilizer (PSS)
is also examined for several network conditions. Simulations demonstrate a
damping improvement up to 933% when the control systems are activated and the
system operates with 25% wind integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00229</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00229</id><created>2015-02-01</created><updated>2015-08-15</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>de Nooy</keyname><forenames>Wouter</forenames></author></authors><title>Can &quot;Hot Spots&quot; in the Sciences Be Mapped Using the Dynamics of
  Aggregated Journal-Journal Citation Relations?</title><categories>cs.DL</categories><comments>accepted for publication in the Journal of the Association for
  Information Science and Technology (Aug. 10, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using three years of the Journal Citation Reports (2011, 2012, and 2013),
indicators of transitions in 2012 (between 2011 and 2013) are studied using
methodologies based on entropy statistics. Changes can be indicated at the
level of journals using the margin totals of entropy production along the row
or column vectors, but also at the level of links among journals by importing
the transition matrices into network analysis and visualization programs (and
using community-finding algorithms). Seventy-four journals are flagged in terms
of discontinuous changes in their citations; but 3,114 journals are involved in
&quot;hot&quot; links. Most of these links are embedded in a main component; 78 clusters
(containing 172 journals) are flagged as potential &quot;hot spots&quot; emerging at the
network level. An additional finding is that PLoS ONE introduced a new
communication dynamics into the database. The limitations of the methodology
are elaborated using an example. The results of the study indicate where
developments in the citation dynamics can be considered as significantly
unexpected. This can be used as heuristic information; but what a &quot;hot spot&quot; in
terms of the entropy statistics of aggregated citation relations means
substantively can be expected to vary from case to case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00231</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00231</id><created>2015-02-01</created><authors><author><keyname>Chen</keyname><forenames>Zhijun</forenames></author><author><keyname>Wu</keyname><forenames>Chaozhong</forenames></author><author><keyname>Zhang</keyname><forenames>Yishi</forenames></author><author><keyname>Huang</keyname><forenames>Zhen</forenames></author><author><keyname>Ran</keyname><forenames>Bin</forenames></author><author><keyname>Zhong</keyname><forenames>Ming</forenames></author><author><keyname>Lyu</keyname><forenames>Nengchao</forenames></author></authors><title>Feature Selection with Redundancy-complementariness Dispersion</title><categories>cs.LG stat.ML</categories><comments>28 pages, 13 figures, 7 tables</comments><msc-class>68T10, 94A17, 62B10, 68U35</msc-class><acm-class>I.5.2; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection has attracted significant attention in data mining and
machine learning in the past decades. Many existing feature selection methods
eliminate redundancy by measuring pairwise inter-correlation of features,
whereas the complementariness of features and higher inter-correlation among
more than two features are ignored. In this study, a modification item
concerning the complementariness of features is introduced in the evaluation
criterion of features. Additionally, in order to identify the interference
effect of already-selected False Positives (FPs), the
redundancy-complementariness dispersion is also taken into account to adjust
the measurement of pairwise inter-correlation of features. To illustrate the
effectiveness of proposed method, classification experiments are applied with
four frequently used classifiers on ten datasets. Classification results verify
the superiority of proposed method compared with five representative feature
selection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00237</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00237</id><created>2015-02-01</created><updated>2015-02-28</updated><authors><author><keyname>Konstantinou</keyname><forenames>Charalambos</forenames></author></authors><title>Ensuring a Secure and Resilient Smart Grid: Cyber-Attacks and
  Countermeasures</title><categories>cs.CR cs.NI</categories><comments>This paper has been withdrawn by the author due the need for
  reevaluation of the attacks and countermeasures categories related with smart
  grid security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys the latest on Smart Grid security. It focuses on the deep
understanding of the risk in terms of threats, vulnerabilities and consequences
that arise from cyber-attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00238</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00238</id><created>2015-02-01</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>On instruction sets for Boolean registers in program algebra</title><categories>cs.PL</categories><comments>18 pages, the preliminaries are largely the same as the preliminaries
  in arXiv:1402.4950 [cs.PL] and some earlier papers</comments><acm-class>F.1.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In program algebra, different instruction sets for Boolean registers are
conceivable. In previous work on instruction sequence size complexity, we chose
instruction sets for Boolean registers that contain only a few of the possible
instructions. In the current paper, we study instruction sequence size bounded
functional completeness of instruction sets for Boolean registers. This work is
among other things a requisite for making progress with proving lower bounds of
non-asymptotic instruction sequence size complexity in cases where auxiliary
Boolean registers may be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00245</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00245</id><created>2015-02-01</created><authors><author><keyname>Perone</keyname><forenames>Christian S.</forenames></author></authors><title>Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This study describes the experimental application of Machine Learning
techniques to build prediction models that can assess the injury risk
associated with traffic accidents. This work uses an freely available data set
of traffic accident records that took place in the city of Porto Alegre/RS
(Brazil) during the year of 2013. This study also provides an analysis of the
most important attributes of a traffic accident that could produce an outcome
of injury to the people involved in the accident.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00250</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00250</id><created>2015-02-01</created><authors><author><keyname>Craye</keyname><forenames>C&#xe9;line</forenames></author><author><keyname>Karray</keyname><forenames>Fakhri</forenames></author></authors><title>Driver distraction detection and recognition using RGB-D sensor</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driver inattention assessment has become a very active field in intelligent
transportation systems. Based on active sensor Kinect and computer vision
tools, we have built an efficient module for detecting driver distraction and
recognizing the type of distraction. Based on color and depth map data from the
Kinect, our system is composed of four sub-modules. We call them eye behavior
(detecting gaze and blinking), arm position (is the right arm up, down, right
of forward), head orientation, and facial expressions. Each module produces
relevant information for assessing driver inattention. They are merged together
later on using two different classification strategies: AdaBoost classifier and
Hidden Markov Model. Evaluation is done using a driving simulator and 8 drivers
of different gender, age and nationality for a total of more than 8 hours of
recording. Qualitative and quantitative results show strong and accurate
detection and recognition capacity (85% accuracy for the type of distraction
and 90% for distraction detection). Moreover, each module is obtained
independently and could be used for other types of inference, such as fatigue
detection, and could be implemented for real cars systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00254</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00254</id><created>2015-02-01</created><updated>2015-02-04</updated><authors><author><keyname>Sarvadevabhatla</keyname><forenames>Ravi Kiran</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Freehand Sketch Recognition Using Deep Features</title><categories>cs.CV</categories><comments>Submitted to ICIP-2015, 5 pages. Removed an erroneous claim regarding
  a work cited in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Freehand sketches often contain sparse visual detail. In spite of the
sparsity, they are easily and consistently recognized by humans across
cultures, languages and age groups. Therefore, analyzing such sparse sketches
can aid our understanding of the neuro-cognitive processes involved in visual
representation and recognition. In the recent past, Convolutional Neural
Networks (CNNs) have emerged as a powerful framework for feature representation
and recognition for a variety of image domains. However, the domain of sketch
images has not been explored. This paper introduces a freehand sketch
recognition framework based on &quot;deep&quot; features extracted from CNNs. We use two
popular CNNs for our experiments -- Imagenet CNN and a modified version of
LeNet CNN. We evaluate our recognition framework on a publicly available
benchmark database containing thousands of freehand sketches depicting everyday
objects. Our results are an improvement over the existing state-of-the-art
accuracies by 3% - 11%. The effectiveness and relative compactness of our deep
features also make them an ideal candidate for related problems such as
sketch-based image retrieval. In addition, we provide a preliminary glimpse of
how such features can help identify crucial attributes (e.g. object-parts) of
the sketched objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00256</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00256</id><created>2015-02-01</created><authors><author><keyname>Xu</keyname><forenames>Yuanlu</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Zheng</keyname><forenames>Wei-Shi</forenames></author><author><keyname>Liu</keyname><forenames>Xiaobai</forenames></author></authors><title>Human Re-identification by Matching Compositional Template with Cluster
  Sampling</title><categories>cs.CV</categories><comments>This manuscript has 8 pages with 7 figures, and a preliminary version
  was published in ICCV 2013</comments><msc-class>68U01</msc-class><doi>10.1109/ICCV.2013.391</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at a newly raising task in visual surveillance:
re-identifying people at a distance by matching body information, given several
reference examples. Most of existing works solve this task by matching a
reference template with the target individual, but often suffer from large
human appearance variability (e.g. different poses/views, illumination) and
high false positives in matching caused by conjunctions, occlusions or
surrounding clutters. Addressing these problems, we construct a simple yet
expressive template from a few reference images of a certain individual, which
represents the body as an articulated assembly of compositional and alternative
parts, and propose an effective matching algorithm with cluster sampling. This
algorithm is designed within a candidacy graph whose vertices are matching
candidates (i.e. a pair of source and target body parts), and iterates in two
steps for convergence. (i) It generates possible partial matches based on
compatible and competitive relations among body parts. (ii) It confirms the
partial matches to generate a new matching solution, which is accepted by the
Markov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate
the superior performance of our approach on three public databases compared to
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00258</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00258</id><created>2015-02-01</created><authors><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Cao</keyname><forenames>Liangliang</forenames></author></authors><title>Learning Latent Spatio-Temporal Compositional Model for Human Action
  Recognition</title><categories>cs.CV</categories><comments>This manuscript has 10 pages with 7 figures, and a preliminary
  version was published in ACM MM'13</comments><msc-class>68U01</msc-class><acm-class>I.5; I.4</acm-class><doi>10.1145/2502081.2502089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action recognition is an important problem in multimedia understanding. This
paper addresses this problem by building an expressive compositional action
model. We model one action instance in the video with an ensemble of
spatio-temporal compositions: a number of discrete temporal anchor frames, each
of which is further decomposed to a layout of deformable parts. In this way,
our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the
latent structure of actions e.g. triple jumping, swinging and high jumping. The
STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for
detecting various action parts within video patches; (ii) the or-nodes over
bottom, i.e. switch variables to activate their children leaf-nodes for
structural variability; (iii) the and-nodes within an anchor frame for
verifying spatial composition; and (iv) the root-node at top for aggregating
scores over temporal anchor frames. Moreover, the contextual interactions are
defined between leaf-nodes in both spatial and temporal domains. For model
training, we develop a novel weakly supervised learning algorithm which
iteratively determines the structural configuration (e.g. the production of
leaf-nodes associated with the or-nodes) along with the optimization of
multi-layer parameters. By fully exploiting spatio-temporal compositions and
interactions, our approach handles well large intra-class action variance (e.g.
different views, individual appearances, spatio-temporal structures). The
experimental results on the challenging databases demonstrate superior
performance of our approach over other competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00274</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00274</id><created>2015-02-01</created><authors><author><keyname>Sichani</keyname><forenames>Arash Kh.</forenames></author><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>A Gradient Descent Approach to Optimal Coherent Quantum LQG Controller
  Design</title><categories>quant-ph cs.CE cs.SY math.OC</categories><comments>11 pages, 2 figures. A version of this paper will appear in the
  Proceedings of the 2015 American Control Conference, July 1-3, Chicago,
  Illinois, USA</comments><msc-class>81Q93, 81S22, 93E20, 65K10, 90C30, 49M05, 15A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the Coherent Quantum Linear Quadratic Gaussian
(CQLQG) control problem of finding a stabilizing measurement-free quantum
controller for a quantum plant so as to minimize an infinite-horizon mean
square performance index for the fully quantum closed-loop system. In
comparison with the observation-actuation structure of classical controllers,
the coherent quantum feedback is less invasive to the quantum dynamics and
quantum information. Both the plant and the controller are open quantum systems
whose dynamic variables satisfy the canonical commutation relations (CCRs) of a
quantum harmonic oscillator and are governed by linear quantum stochastic
differential equations (QSDEs). In order to correspond to such oscillators,
these QSDEs must satisfy physical realizability (PR) conditions, which are
organised as quadratic constraints on the controller matrices and reflect the
preservation of CCRs in time. The CQLQG problem is a constrained optimization
problem for the steady-state quantum covariance matrix of the plant-controller
system satisfying an algebraic Lyapunov equation. We propose a gradient descent
algorithm equipped with adaptive stepsize selection for the numerical solution
of the problem. The algorithm finds a local minimum of the LQG cost over the
parameters of the Hamiltonian and coupling operators of a stabilizing PR
quantum controller, thus taking the PR constraints into account. A convergence
analysis of the proposed algorithm is presented. A numerical example of a
locally optimal CQLQG controller design is provided to demonstrate the
algorithm performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00277</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00277</id><created>2015-02-01</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>T&#xe1;vora</keyname><forenames>R. G. F.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Fast Finite Field Hartley Transforms Based on Hadamard Decomposition</title><categories>cs.NA math.NT stat.CO</categories><comments>6 pages, 3 tables, fixed typos, submitted to the Sixth International
  Symposium on Communication Theory and Applications (ISCTA'01), 2001</comments><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new transform over finite fields, the finite field Hartley transform
(FFHT), was recently introduced and a number of promising applications on the
design of efficient multiple access systems and multilevel spread spectrum
sequences were proposed. The FFHT exhibits interesting symmetries, which are
exploited to derive tailored fast transform algorithms. The proposed fast
algorithms are based on successive decompositions of the FFHT by means of
Hadamard-Walsh transforms (HWT). The introduced decompositions meet the lower
bound on the multiplicative complexity for all the cases investigated. The
complexity of the new algorithms is compared with that of traditional
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00281</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00281</id><created>2015-02-01</created><authors><author><keyname>&#x110;&#xe0;o</keyname><forenames>Ngoc-D&#x169;ng</forenames></author><author><keyname>Zhang</keyname><forenames>Hang</forenames></author><author><keyname>Farmanbar</keyname><forenames>Hamid</forenames></author><author><keyname>Li</keyname><forenames>Xu</forenames></author></authors><title>Handling Mobility in Dense Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network densification is one of key technologies in future networks to
significantly increase network capacity. The gain obtained by network
densification for fixed terminals have been studied and proved. However for
mobility users, there are a number of issues, such as more frequent handover,
packet loss due to high mobility, interference management and so on. The
conventional solutions are to handover high speed mobiles to macro base
stations or multicast traffic to multiple base stations. These solutions fail
to exploit the capacity of dense networks and overuse the backhaul capacity. In
this paper we propose a set of solutions to systematically solve the technical
challenges of mobile dense networks. We introduce network architecture together
with data transmission protocols to support mobile users. A software-defined
protocol (SDP) concept is presented so that combinations of transport protocols
and physical layer functions can be optimized and triggered on demand. Our
solutions can significantly boost performance of dense networks and simplify
the packet handling process. Importantly, the gain brought by network
densification to fixed users can also be achieved for mobile users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00284</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00284</id><created>2015-02-01</created><updated>2015-02-23</updated><authors><author><keyname>Jiang</keyname><forenames>Bin</forenames></author><author><keyname>Ma</keyname><forenames>Ding</forenames></author></authors><title>Defining Least Community as a Homogeneous Group in Complex Networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>9 pages, 3 figures, 3 tables; Physica A, 2015, xx(x), xx-xx</comments><journal-ref>Physica A, 2015, 428, 154-160</journal-ref><doi>10.1016/j.physa.2015.02.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new concept of least community that is as homogeneous
as a random graph, and develops a new community detection algorithm from the
perspective of homogeneity or heterogeneity. Based on this concept, we adopt
head/tail breaks - a newly developed classification scheme for data with a
heavy-tailed distribution - and rely on edge betweenness given its heavy-tailed
distribution to iteratively partition a network into many heterogeneous and
homogeneous communities. Surprisingly, the derived communities for any
self-organized and/or self-evolved large networks demonstrate very striking
power laws, implying that there are far more small communities than large ones.
This notion of far more small things than large ones constitutes a new
fundamental way of thinking for community detection. Keywords: head/tail
breaks, ht-index, scaling, k-means, natural breaks, and classification
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00290</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00290</id><created>2015-02-01</created><authors><author><keyname>Tabassum</keyname><forenames>Huma</forenames></author><author><keyname>Javaid</keyname><forenames>Sameena</forenames></author><author><keyname>Farooq</keyname><forenames>Humera</forenames></author></authors><title>Survey on Awareness of Privacy Issues in Ubiquitous Environment</title><categories>cs.CY</categories><comments>5 pages, 3 figures, 1 table. To be published in International Journal
  of Computer Science &amp; Information Security (IJCSIS) Volume:13 Issue:1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study aims to determine privacy awareness among people in ubiquitous
environment through a questionnaire based survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00296</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00296</id><created>2015-02-01</created><authors><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Dimitrov</keyname><forenames>V. S.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Fragile Watermarking Using Finite Field Trigonometrical Transforms</title><categories>cs.MM cs.IT math.IT math.NT</categories><comments>9 pages, 7 figures, 2 tables</comments><journal-ref>Image Communication, Volume 24, Issue 7, August, 2009, pp. 587-597</journal-ref><doi>10.1016/j.image.2009.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fragile digital watermarking has been applied for authentication and
alteration detection in images. Utilizing the cosine and Hartley transforms
over finite fields, a new transform domain fragile watermarking scheme is
introduced. A watermark is embedded into a host image via a blockwise
application of two-dimensional finite field cosine or Hartley transforms.
Additionally, the considered finite field transforms are adjusted to be number
theoretic transforms, appropriate for error-free calculation. The employed
technique can provide invisible fragile watermarking for authentication systems
with tamper location capability. It is shown that the choice of the finite
field characteristic is pivotal to obtain perceptually invisible watermarked
images. It is also shown that the generated watermarked images can be used as
publicly available signature data for authentication purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00303</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00303</id><created>2015-02-01</created><authors><author><keyname>Qi</keyname><forenames>Xianbiao</forenames></author><author><keyname>Li</keyname><forenames>Chun-Guang</forenames></author><author><keyname>Zhao</keyname><forenames>Guoying</forenames></author><author><keyname>Hong</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Pietik&#xe4;inen</keyname><forenames>Matti</forenames></author></authors><title>Dynamic texture and scene classification by transferring deep image
  features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic texture and scene classification are two fundamental problems in
understanding natural video content. Extracting robust and effective features
is a crucial step towards solving these problems. However the existing
approaches suffer from the sensitivity to either varying illumination, or
viewpoint changing, or even camera motion, and/or the lack of spatial
information. Inspired by the success of deep structures in image
classification, we attempt to leverage a deep structure to extract feature for
dynamic texture and scene classification. To tackle with the challenges in
training a deep structure, we propose to transfer some prior knowledge from
image domain to video domain. To be specific, we propose to apply a
well-trained Convolutional Neural Network (ConvNet) as a mid-level feature
extractor to extract features from each frame, and then form a representation
of a video by concatenating the first and the second order statistics over the
mid-level features. We term this two-level feature extraction scheme as a
Transferred ConvNet Feature (TCoF). Moreover we explore two different
implementations of the TCoF scheme, i.e., the \textit{spatial} TCoF and the
\textit{temporal} TCoF, in which the mean-removed frames and the difference
between two adjacent frames are used as the inputs of the ConvNet,
respectively. We evaluate systematically the proposed spatial TCoF and the
temporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN,
and Maryland, and demonstrate that the proposed approach yields superior
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00316</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00316</id><created>2015-02-01</created><authors><author><keyname>Gao</keyname><forenames>Xiaoming</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Qiu</keyname><forenames>Judy</forenames></author></authors><title>Parallel clustering of high-dimensional social media data streams</title><categories>cs.DC cs.DB cs.SI</categories><comments>IEEE/ACM CCGrid 2015: 15th IEEE/ACM International Symposium on
  Cluster, Cloud and Grid Computing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Cloud DIKW as an analysis environment supporting scientific
discovery through integrated parallel batch and streaming processing, and apply
it to one representative domain application: social media data stream
clustering. Recent work demonstrated that high-quality clusters can be
generated by representing the data points using high-dimensional vectors that
reflect textual content and social network information. Due to the high cost of
similarity computation, sequential implementations of even single-pass
algorithms cannot keep up with the speed of real-world streams. This paper
presents our efforts to meet the constraints of real-time social stream
clustering through parallelization. We focus on two system-level issues. Most
stream processing engines like Apache Storm organize distributed workers in the
form of a directed acyclic graph, making it difficult to dynamically
synchronize the state of parallel workers. We tackle this challenge by creating
a separate synchronization channel using a pub-sub messaging system. Due to the
sparsity of the high-dimensional vectors, the size of centroids grows quickly
as new data points are assigned to the clusters. Traditional synchronization
that directly broadcasts cluster centroids becomes too expensive and limits the
scalability of the parallel algorithm. We address this problem by communicating
only dynamic changes of the clusters rather than the whole centroid vectors.
Our algorithm under Cloud DIKW can process the Twitter 10% data stream in
real-time with 96-way parallelism. By natural improvements to Cloud DIKW,
including advanced collective communication techniques developed in our Harp
project, we will be able to process the full Twitter stream in real-time with
1000-way parallelism. Our use of powerful general software subsystems will
enable many other applications that need integration of streaming and batch
data analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00317</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00317</id><created>2015-02-01</created><updated>2015-02-03</updated><authors><author><keyname>Warnock</keyname><forenames>David</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author></authors><title>An Exploration of Cursor tracking Data</title><categories>cs.HC</categories><comments>Mouse, Cursor, Tracking, Engagement, Involvement, Aesthetics,
  Self-report Measure, Experimental Design</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cursor tracking data contains information about website visitors which may
provide new ways to understand visitors and their needs. This paper presents an
Amazon Mechanical Turk study where participants were tracked as they used
modified variants of the Wikipedia and BBC News websites. Participants were
asked to complete reading and information-finding tasks. The results showed
that it was possible to differentiate between users reading content and users
looking for information based on cursor data. The effects of website
aesthetics, user interest and cursor hardware were also analysed which showed
it was possible to identify hardware from cursor data, but no relationship
between cursor data and engagement was found. The implications of these
results, from the impact on web analytics to the design of experiments to
assess user engagement, are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00318</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00318</id><created>2015-02-01</created><authors><author><keyname>Horton</keyname><forenames>Nicholas J.</forenames></author><author><keyname>Baumer</keyname><forenames>Benjamin S.</forenames></author><author><keyname>Wickham</keyname><forenames>Hadley</forenames></author></authors><title>Setting the stage for data science: integration of data management
  skills in introductory and second courses in statistics</title><categories>stat.CO cs.CY stat.OT</categories><msc-class>62-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many have argued that statistics students need additional facility to express
statistical computations. By introducing students to commonplace tools for data
management, visualization, and reproducible analysis in data science and
applying these to real-world scenarios, we prepare them to think statistically.
In an era of increasingly big data, it is imperative that students develop
data-related capacities, beginning with the introductory course. We believe
that the integration of these precursors to data science into our
curricula-early and often-will help statisticians be part of the dialogue
regarding &quot;Big Data&quot; and &quot;Big Questions&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00319</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00319</id><created>2015-02-01</created><updated>2015-10-28</updated><authors><author><keyname>Salarian</keyname><forenames>Mahdi</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author></authors><title>Efficient refinement of GPS-based localization in urban areas using
  visual information and sensor parameter</title><categories>cs.CV cs.IR</categories><comments>this paper has been withdrawn bt the authors due to couple of errors
  in exprimental part</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient method is proposed for refining GPS-acquired location
coordinates in urban areas using camera images, Google Street View (GSV) and
sensor parameters. The main goal is to compensate for GPS location imprecision
in dense area of cities due to proximity to walls and buildings. Avail-able
methods for better localization often use visual information by using query
images acquired with camera-equipped mobile devices and applying image
retrieval techniques to find the closest match in a GPS-referenced image data
set. The search areas required for reliable search are about 1-2 sq. Km and the
accuracy is typically 25-100 meters. Here we describe a method based on image
retrieval where a reliable search can be confined to areas of 0.01 sq. Km and
the accuracy in our experiments is less than 10 meters. To test our procedure
we created a database by acquiring all Google Street View images close to what
is seen by a pedestrian in a large region of downtown Chicago and saved all
coordinates and orientation data to be used for confining our search region.
Prior knowledge from approximate position of query image is leveraged to
address complexity and accuracy issues of our search in a large scale
geo-tagged data set. One key aspect that differentiates our work is that it
utilizes the sensor information of GPS SOS and the camera orientation in
improving localization. Finally we demonstrate retrieval-based technique are
less accurate in sparse open areas compared with purely GPS measurement. The
effectiveness of our approach is discussed in detail and experimental results
show improved performance when compared with regular approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00324</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00324</id><created>2015-02-01</created><authors><author><keyname>Salarian</keyname><forenames>M.</forenames></author><author><keyname>Naimi</keyname><forenames>H. Miar</forenames></author></authors><title>Modified Fast Fractal Image Compression Algorithm in spatial domain</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new fractal image compression algorithm is proposed in which
the time of encoding process is considerably reduced. The algorithm exploits a
domain pool reduction approach, along with using innovative predefined values
for contrast scaling factor, S, instead of searching it across [0,1]. Only the
domain blocks with entropy greater than a threshold are considered as domain
pool. As a novel point, it is assumed that in each step of the encoding
process, the domain block with small enough distance shall be found only for
the range blocks with low activity (equivalently low entropy). This novel point
is used to find reasonable estimations of S, and use them in the encoding
process as predefined values, mentioned above, the remaining range blocks are
split into four new smaller range blocks and the algorithm must be iterated for
them, considered as the other step of encoding process. The algorithm has been
examined for some of the well-known images and the results have been compared
with the state-of-the-art algorithms. The experiments show that our proposed
algorithm has considerably lower encoding time than the other where the encoded
images are approximately the same in quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00326</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00326</id><created>2015-02-01</created><authors><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Adaptive Estimation of Shannon Entropy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider estimating the Shannon entropy of a discrete distribution $P$
from $n$ i.i.d. samples. Recently, Jiao, Venkat, Han, and Weissman, and Wu and
Yang constructed approximation theoretic estimators that achieve the minimax
$L_2$ rates in estimating entropy. Their estimators are consistent given $n \gg
\frac{S}{\ln S}$ samples, where $S$ is the alphabet size, and it is the best
possible sample complexity. In contrast, the Maximum Likelihood Estimator
(MLE), which is the empirical entropy, requires $n\gg S$ samples.
  In the present paper we significantly refine the minimax results of existing
work. To alleviate the pessimism of minimaxity, we adopt the adaptive
estimation framework, and show that the minimax rate-optimal estimator in Jiao,
Venkat, Han, and Weissman achieves the minimax rates simultaneously over a
nested sequence of subsets of distributions $P$, without knowing the alphabet
size $S$ or which subset $P$ lies in. In other words, their estimator is
adaptive with respect to this nested sequence of the parameter space, which is
characterized by the entropy of the distribution. We also characterize the
maximum risk of the MLE over this nested sequence, and show, for every subset
in the sequence, that the performance of the minimax rate-optimal estimator
with $n$ samples is essentially that of the MLE with $n\ln n$ samples, thereby
further substantiating the generality of the phenomenon discovered by Jiao,
Venkat, Han, and Weissman.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00327</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00327</id><created>2015-02-01</created><updated>2015-03-10</updated><authors><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Does Dirichlet Prior Smoothing Solve the Shannon Entropy Estimation
  Problem?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dirichlet prior is widely used in estimating discrete distributions and
functionals of discrete distributions. In terms of Shannon entropy estimation,
one approach is to plug-in the Dirichlet prior smoothed distribution into the
entropy functional, while the other one is to calculate the Bayes estimator for
entropy under the Dirichlet prior for squared error, which is the conditional
expectation. We show that in general they do \emph{not} improve over the
maximum likelihood estimator, which plugs-in the empirical distribution into
the entropy functional. No matter how we tune the parameters in the Dirichlet
prior, this approach cannot achieve the minimax rates in entropy estimation, as
recently characterized by Jiao, Venkat, Han, and Weissman, and Wu and Yang. The
performance of the minimax rate-optimal estimator with $n$ samples is
essentially \emph{at least} as good as that of the Dirichlet smoothed entropy
estimators with $n\ln n$ samples.
  We harness the theory of approximation using positive linear operators for
analyzing the bias of plug-in estimators for general functionals under
arbitrary statistical models, thereby further consolidating the interplay
between these two fields, which was thoroughly developed and exploited by Jiao,
Venkat, Han, and Weissman. We establish new results in approximation theory,
and apply them to analyze the bias of the Dirichlet prior smoothed plug-in
entropy estimator. This interplay between bias analysis and approximation
theory is of relevance and consequence far beyond the specific problem setting
in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00341</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00341</id><created>2015-02-01</created><authors><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Lai</keyname><forenames>Jian-Huang</forenames></author></authors><title>Discriminatively Trained And-Or Graph Models for Object Shape Detection</title><categories>cs.CV</categories><comments>15 pages, 14 figures, TPAMI 2014</comments><msc-class>68U01</msc-class><journal-ref>Pattern Analysis and Machine Intelligence, IEEE Transactions on ,
  vol.PP, no.99, pp.1,1, 2014</journal-ref><doi>10.1109/TPAMI.2014.2359888</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a novel reconfigurable part-based model, namely
And-Or graph model, to recognize object shapes in images. Our proposed model
consists of four layers: leaf-nodes at the bottom are local classifiers for
detecting contour fragments; or-nodes above the leaf-nodes function as the
switches to activate their child leaf-nodes, making the model reconfigurable
during inference; and-nodes in a higher layer capture holistic shape
deformations; one root-node on the top, which is also an or-node, activates one
of its child and-nodes to deal with large global variations (e.g. different
poses and views). We propose a novel structural optimization algorithm to
discriminatively train the And-Or model from weakly annotated data. This
algorithm iteratively determines the model structures (e.g. the nodes and their
layouts) along with the parameter learning. On several challenging datasets,
our model demonstrates the effectiveness to perform robust shape-based object
detection against background clutter and outperforms the other state-of-the-art
approaches. We also release a new shape database with annotations, which
includes more than 1500 challenging shape instances, for recognition and
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00344</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00344</id><created>2015-02-01</created><authors><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Xu</keyname><forenames>Yuanlu</forenames></author><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Lai</keyname><forenames>Jianhuang</forenames></author></authors><title>Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal
  Models</title><categories>cs.CV</categories><comments>12 pages, 7 figures</comments><msc-class>68U01</msc-class><journal-ref>Image Processing, IEEE Transactions on , vol.23, no.7,
  pp.3191,3202, July 2014</journal-ref><doi>10.1109/TIP.2014.2326776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although it has been widely discussed in video surveillance, background
subtraction is still an open problem in the context of complex scenarios, e.g.,
dynamic backgrounds, illumination variations, and indistinct foreground
objects. To address these challenges, we propose an effective background
subtraction method by learning and maintaining an array of dynamic texture
models within the spatio-temporal representations. At any location of the
scene, we extract a sequence of regular video bricks, i.e. video volumes
spanning over both spatial and temporal domain. The background modeling is thus
posed as pursuing subspaces within the video bricks while adapting the scene
variations. For each sequence of video bricks, we pursue the subspace by
employing the ARMA (Auto Regressive Moving Average) Model that jointly
characterizes the appearance consistency and temporal coherence of the
observations. During online processing, we incrementally update the subspaces
to cope with disturbances from foreground objects and scene changes. In the
experiments, we validate the proposed method in several complex scenarios, and
show superior performances over other state-of-the-art approaches of background
subtraction. The empirical studies of parameter setting and component analysis
are presented as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00348</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00348</id><created>2015-02-01</created><authors><author><keyname>Aminikashani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Uysal</keyname><forenames>Murat</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>A Novel Statistical Channel Model for Turbulence-Induced Fading in
  Free-Space Optical Systems</title><categories>math-ph cs.IT math.IT math.MP physics.ao-ph</categories><comments>10 Pages, 8 figures, journal paper</comments><journal-ref>Journal of Lightwave Technology 33.11 (2015): 2303 - 2312</journal-ref><doi>10.1109/JLT.2015.2410695</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new probability distribution function which
accurately describes turbulence-induced fading under a wide range of turbulence
conditions. The proposed model, termed Double Generalized Gamma (Double GG), is
based on a doubly stochastic theory of scintillation and developed via the
product of two Generalized Gamma (GG) distributions. The proposed Double GG
distribution generalizes many existing turbulence channel models and provides
an excellent fit to the published plane and spherical waves simulation data.
Using this new statistical channel model, we derive closed form expressions for
the outage probability and the average bit error as well as corresponding
asymptotic expressions of free-space optical communication systems over
turbulence channels. We demonstrate that our derived expressions cover many
existing results in the literature earlier reported for Gamma-Gamma,
Double-Weibull and K channels as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00353</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00353</id><created>2015-02-01</created><authors><author><keyname>da Cunha</keyname><forenames>Bruno Requi&#xe3;o</forenames></author><author><keyname>Gonz&#xe1;lez-Avella</keyname><forenames>Juan Carlos</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Sebasti&#xe1;n</forenames></author></authors><title>Complex networks vulnerability to module-based attacks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the multidisciplinary field of Network Science, optimization of procedures
for efficiently breaking complex networks is attracting much attention from
practical points of view. In this contribution we present a module-based method
to efficiently break complex networks. The procedure first identifies the
communities in which the network can be represented, then it deletes the nodes
(edges) that connect different modules by its order in the betweenness
centrality ranking list. We illustrate the method by applying it to various
well known examples of social, infrastructure, and biological networks. We show
that the proposed method always outperforms vertex (edge) attacks which are
based on the ranking of node (edge) degree or centrality, with a huge gain in
efficiency for some examples. Remarkably, for the US power grid, the present
method breaks the original network of 4941 nodes to many fragments smaller than
197 nodes (4% of the original size) by removing mere 164 nodes (~3%) identified
by the procedure. By comparison, any degree or centrality based procedure,
deleting the same amount of nodes, removes only 22% of the original network,
i.e. more than 3800 nodes continue to be connected after that
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00354</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00354</id><created>2015-02-01</created><authors><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author></authors><title>A Web-based Interactive Visual Graph Analytics Platform</title><categories>cs.SI cs.HC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a web-based visual graph analytics platform for
interactive graph mining, visualization, and real-time exploration of networks.
GraphVis is fast, intuitive, and flexible, combining interactive visualizations
with analytic techniques to reveal important patterns and insights for sense
making, reasoning, and decision making. Networks can be visualized and explored
within seconds by simply drag-and-dropping a graph file into the web browser.
The structure, properties, and patterns of the network are computed
automatically and can be instantly explored in real-time. At the heart of
GraphVis lies a multi-level interactive network visualization and analytics
engine that allows for real-time graph mining and exploration across multiple
levels of granularity simultaneously. Both the graph analytic and visualization
techniques (at each level of granularity) are dynamic and interactive, with
immediate and continuous visual feedback upon every user interaction (e.g.,
change of a slider for filtering). Furthermore, nodes, edges, and subgraphs are
easily inserted, deleted or exported via a number of novel techniques and tools
that make it extremely easy and flexible for exploring, testing hypothesis, and
understanding networks in real-time over the web. A number of interactive
visual graph analytic techniques are also proposed including interactive role
discovery methods, community detection, as well as a number of novel block
models for generating graphs with community structure. Finally, we also
highlight other key aspects including filtering, querying, ranking,
manipulating, exporting, partitioning, as well as tools for dynamic network
analysis and visualization, interactive graph generators, and a variety of
multi-level network analysis, summarization, and statistical techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00355</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00355</id><created>2015-02-01</created><authors><author><keyname>Zhao</keyname><forenames>Kunyang</forenames></author><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author><author><keyname>Zhang</keyname><forenames>Jiayin</forenames></author></authors><title>On the Accelerating of Two-dimensional Smart Laplacian Smoothing on the
  GPU</title><categories>cs.DC</categories><comments>The author declares that this paper has been submitted to the
  International Conference on Computational Science ICCS 2015. 10 pages, 4
  figures</comments><journal-ref>Journal of Information &amp; Computational Science 12:13 (2015)
  5133-5143</journal-ref><doi>10.12733/jics20106587</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a GPU-accelerated implementation of two-dimensional Smart
Laplacian smoothing. This implementation is developed under the guideline of
our paradigm for accelerating Laplacianbased mesh smoothing [13]. Two types of
commonly used data layouts, Array-of-Structures (AoS) and Structure-of-Arrays
(SoA) are used to represent triangular meshes in our implementation. Two
iteration forms that have different choices of the swapping of intermediate
data are also adopted. Furthermore, the feature CUDA Dynamic Parallelism (CDP)
is employed to realize the nested parallelization in Smart Laplacian smoothing.
Experimental results demonstrate that: (1) our implementation can achieve the
speedups of up to 44x on the GPU GT640; (2) the data layout AoS can always
obtain better efficiency than the SoA layout; (3) the form that needs to swap
intermediate nodal coordinates is always slower than the one that does not swap
data; (4) the version of our implementation with the use of the feature CDP is
slightly faster than the version where the CDP is not adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00357</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00357</id><created>2015-02-01</created><updated>2015-02-04</updated><authors><author><keyname>Lee</keyname><forenames>Chia-Jung</forenames></author><author><keyname>Lokam</keyname><forenames>Satya V.</forenames></author><author><keyname>Tsai</keyname><forenames>Shi-Chun</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Chuan</forenames></author></authors><title>On Restricting No-Junta Boolean Function and Degree Lower Bounds by
  Polynomial Method</title><categories>cs.CC</categories><comments>5 pages, ISIT 2015. Simplified proof</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Let $\mathcal{F}_{n}^*$ be the set of Boolean functions depending on all $n$
variables. We prove that for any $f\in \mathcal{F}_{n}^*$, $f|_{x_i=0}$ or
$f|_{x_i=1}$ depends on the remaining $n-1$ variables, for some variable $x_i$.
This existent result suggests a possible way to deal with general Boolean
functions via its subfunctions of some restrictions.
  As an application, we consider the degree lower bound of representing
polynomials over finite rings. Let $f\in \mathcal{F}_{n}^*$ and denote the
exact representing degree over the ring $\mathbb{Z}_m$ (with the integer $m&gt;2$)
as $d_m(f)$. Let $m=\Pi_{i=1}^{r}p_i^{e_i}$, where $p_i$'s are distinct primes,
and $r$ and $e_i$'s are positive integers. If $f$ is symmetric, then $m\cdot
d_{p_1^{e_1}}(f)... d_{p_r^{e_r}}(f) &gt; n$. If $f$ is non-symmetric, by the
second moment method we prove almost always $m\cdot d_{p_1^{e_1}}(f)...
d_{p_r^{e_r}}(f) &gt; \lg{n}-1$. In particular, as $m=pq$ where $p$ and $q$ are
arbitrary distinct primes, we have $d_p(f)d_q(f)=\Omega(n)$ for symmetric $f$
and $d_p(f)d_q(f)=\Omega(\lg{n}-1)$ almost always for non-symmetric $f$. Hence
any $n$-variate symmetric Boolean function can have exact representing degree
$o(\sqrt{n})$ in at most one finite field, and for non-symmetric functions,
with $o(\sqrt{\lg{n}})$-degree in at most one finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00362</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00362</id><created>2015-02-02</created><authors><author><keyname>Gounaris</keyname><forenames>Chrysanthos E.</forenames></author><author><keyname>Rajendran</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Kevrekidis</keyname><forenames>Ioannis G.</forenames></author><author><keyname>Floudas</keyname><forenames>Christodoulos A.</forenames></author></authors><title>Designing Networks: A Mixed-Integer Linear Optimization Approach</title><categories>math.OC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing networks with specified collective properties is useful in a
variety of application areas, enabling the study of how given properties affect
the behavior of network models, the downscaling of empirical networks to
workable sizes, and the analysis of network evolution. Despite the importance
of the task, there currently exists a gap in our ability to systematically
generate networks that adhere to theoretical guarantees for the given property
specifications. In this paper, we propose the use of Mixed-Integer Linear
Optimization modeling and solution methodologies to address this Network
Generation Problem. We present a number of useful modeling techniques and apply
them to mathematically express and constrain network properties in the context
of an optimization formulation. We then develop complete formulations for the
generation of networks that attain specified levels of connectivity, spread,
assortativity and robustness, and we illustrate these via a number of
computational case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00363</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00363</id><created>2015-02-02</created><authors><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Wang</keyname><forenames>Faqiang</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Huang</keyname><forenames>Yuchi</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>Iterated Support Vector Machines for Distance Metric Learning</title><categories>cs.LG cs.CV</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance metric learning aims to learn from the given training data a valid
distance metric, with which the similarity between data samples can be more
effectively evaluated for classification. Metric learning is often formulated
as a convex or nonconvex optimization problem, while many existing metric
learning algorithms become inefficient for large scale problems. In this paper,
we formulate metric learning as a kernel classification problem, and solve it
by iterated training of support vector machines (SVM). The new formulation is
easy to implement, efficient in training, and tractable for large-scale
problems. Two novel metric learning models, namely Positive-semidefinite
Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained
Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the
global optimality of their solutions. Experimental results on UCI dataset
classification, handwritten digit recognition, face verification and person
re-identification demonstrate that the proposed metric learning methods achieve
higher classification accuracy than state-of-the-art methods and they are
significantly more efficient in training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00364</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00364</id><created>2015-02-02</created><authors><author><keyname>Aminikashani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>On the Performance of Single- and Multi-carrier Modulation Schemes for
  Indoor Visible Light Communication Systems</title><categories>cs.IT math.IT</categories><comments>6 Pages, IEEE Globecom conference 2014</comments><journal-ref>IEEE Global Communications Conference (GLOBECOM), 2084-2089, 2014</journal-ref><doi>10.1109/GLOCOM.2014.7037115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate and compare the performance of single- and
multi-carrier modulation schemes for indoor visible light communication (VLC).
Particularly, the performances of single carrier frequency domain equalization
(SCFDE), orthogonal frequency division multiplexing (OFDM) and on-off keying
(OOK) with minimum mean square error equalization (MMSE) are analyzed in order
to mitigate the effect of multipath distortion of the indoor optical channel
where nonlinearity distortion of light emitting diode (LED) transfer function
is taken into account. Our results indicate that SCFDE system, in contrast to
OFDM system, does not suffer from high peak to average power ratio (PAPR) and
can outperform OFDM and OOK systems. We further investigate the impact of LED
bias point on the performance of OFDM systems and show that biasing LED with
the optimum value can significantly enhance the performance of the system.
Bit-interleaved coded modulation (BICM) is also considered for OFDM and SCFDE
systems to further compensate signal degradation due to inter-symbol
interference (ISI) and LED nonlinearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00365</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00365</id><created>2015-02-02</created><authors><author><keyname>Aminikashani</keyname><forenames>Mohmmadreza</forenames></author><author><keyname>Uysal</keyname><forenames>Murat</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>On the Performance of MIMO FSO Communications over Double Generalized
  Gamma Fading Channels</title><categories>cs.IT math.IT</categories><comments>6 Pages, 4 figure, IEEE ICC conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major performance degrading factor in free space optical communication
(FSO) systems is atmospheric turbulence. Spatial diversity techniques provide a
promising approach to mitigate turbulence-induced fading. In this paper, we
study the error rate performance of FSO links with spatial diversity over
atmospheric turbulence channels described by the Double Generalized Gamma
distribution which is a new generic statistical model covering all turbulence
conditions. We assume intensity modulation/direct detection with on-off keying
and present the BER performance of single-input multiple-output (SIMO),
multiple-input single-output (MISO) and multiple-input multiple-output (MIMO)
FSO systems over this new channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00367</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00367</id><created>2015-02-02</created><authors><author><keyname>Suzuki</keyname><forenames>Toshio</forenames></author></authors><title>A Solution to Yamakami's Problem on Advised Context-free Languages</title><categories>cs.FL</categories><msc-class>03D05, 68Q45</msc-class><acm-class>F.4.3; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Yamakami [2011, Theoret. Comput. Sci.] studies context-free languages with
advice functions. Here, the length of an advice is assumed to be the same as
that of an input. Let CFL and CFL/n denote the class of all context-free
languages and that with advice functions, respectively. We let CFL(2) denote
the class of intersections of two context-free languages. An interesting
direction of a research is asking how complex CFL(2) is, relative to CFL.
Yamakami raised a problem whether there is a CFL-immune set in CFL(2) - CFL/n.
The best known so far is that LSPACE - CFL/n has a CFL-immune set, where LSPACE
denotes the class of languages recognized in logarithmic-space. We present an
affirmative solution to his problem. Two key concepts of our proof are the
nested palindrome and Yamakami's swapping lemma. The swapping lemma is
applicable to the setting where the pumping lemma (Bar-Hillel's lemma) does not
work. Our proof is an example showing how useful the swapping lemma is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00374</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00374</id><created>2015-02-02</created><authors><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Ruimao</forenames></author><author><keyname>Duan</keyname><forenames>Xiaohua</forenames></author></authors><title>Adaptive Scene Category Discovery with Generative Learning and
  Compositional Sampling</title><categories>cs.CV</categories><comments>11 pages, 7 figures</comments><msc-class>68U01</msc-class><journal-ref>Circuits and Systems for Video Technology, IEEE Transactions on ,
  vol.PP, no.99, pp.1,1, 2014</journal-ref><doi>10.1109/TCSVT.2014.2313897</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a general framework to discover categories of
unlabeled scene images according to their appearances (i.e., textures and
structures). We jointly solve the two coupled tasks in an unsupervised manner:
(i) classifying images without pre-determining the number of categories, and
(ii) pursuing generative model for each category. In our method, each image is
represented by two types of image descriptors that are effective to capture
image appearances from different aspects. By treating each image as a graph
vertex, we build up an graph, and pose the image categorization as a graph
partition process. Specifically, a partitioned sub-graph can be regarded as a
category of scenes, and we define the probabilistic model of graph partition by
accumulating the generative models of all separated categories. For efficient
inference with the graph, we employ a stochastic cluster sampling algorithm,
which is designed based on the Metropolis-Hasting mechanism. During the
iterations of inference, the model of each category is analytically updated by
a generative learning algorithm. In the experiments, our approach is validated
on several challenging databases, and it outperforms other popular
state-of-the-art methods. The implementation details and empirical analysis are
presented as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00377</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00377</id><created>2015-02-02</created><authors><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Lu</keyname><forenames>Yongyi</forenames></author><author><keyname>Pan</keyname><forenames>Yan</forenames></author><author><keyname>Chen</keyname><forenames>Xiaowu</forenames></author></authors><title>Integrating Graph Partitioning and Matching for Trajectory Analysis in
  Video Surveillance</title><categories>cs.CV</categories><comments>10 pages, 12 figures</comments><msc-class>68U01</msc-class><journal-ref>Image Processing, IEEE Transactions on , vol.21, no.12,
  pp.4844-4857, 2012</journal-ref><doi>10.1109/TIP.2012.2211373</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to track the moving objects in long range against occlusion,
interruption, and background clutter, this paper proposes a unified approach
for global trajectory analysis. Instead of the traditional frame-by-frame
tracking, our method recovers target trajectories based on a short sequence of
video frames, e.g. $15$ frames. We initially calculate a foreground map at each
frame, as obtained from a state-of-the-art background model. An attribute graph
is then extracted from the foreground map, where the graph vertices are image
primitives represented by the composite features. With this graph
representation, we pose trajectory analysis as a joint task of spatial graph
partitioning and temporal graph matching. The task can be formulated by
maximizing a posteriori under the Bayesian framework, in which we integrate the
spatio-temporal contexts and the appearance models. The probabilistic inference
is achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a
peroid of observed frames, the algorithm simulates a ergodic and aperiodic
Markov Chain, and it visits a sequence of solution states in the joint space of
spatial graph partitioning and temporal graph matching. In the experiments, our
method is tested on several challenging videos from the public datasets of
visual surveillance, and it outperforms the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00378</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00378</id><created>2015-02-02</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>REGAL, UPMC</affiliation></author><author><keyname>Kaaouachi</keyname><forenames>Mohamed-Hamza</forenames><affiliation>REGAL, UPMC</affiliation></author><author><keyname>Petit</keyname><forenames>Franck</forenames><affiliation>REGAL, UPMC</affiliation></author></authors><title>Enabling Minimal Dominating Set in Highly Dynamic Distributed Systems</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1412.6007</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of computing a Minimal Dominating Set in highly
dynamic distributed systems. We assume weak connectivity, i.e., the network may
be disconnected at each time instant and topological changes are unpredictable.
We make only weak assumptions on the communication: every process is infinitely
often able to communicate with other processes (not necessarily directly). Our
contribution is threefold. First, we propose a new definition of minimal
dominating set suitable for the context of time-varying graphs that seems more
relevant than existing ones. Next, we provide a necessary and sufficient
topological condition for the existence of a deterministic algorithm for
minimal dominating set construction in our settings. Finally, we propose a new
measure of time complexity in time-varying graph in order to to allow fair
comparison between algorithms. Indeed, this measure takes account of
communication delays attributable to dynamicity of the graph and not to the
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00389</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00389</id><created>2015-02-02</created><authors><author><keyname>Shi</keyname><forenames>Junjie</forenames></author><author><keyname>Zhang</keyname><forenames>Yuan</forenames></author><author><keyname>Zhong</keyname><forenames>Sheng</forenames></author></authors><title>Privacy-preserving Network Functionality Outsourcing</title><categories>cs.CR cs.NI</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the advent of software defined networks ({SDN}), there have been many
attempts to outsource the complex and costly local network functionality, i.e.
the middlebox, to the cloud in the same way as outsourcing computation and
storage. The privacy issues, however, may thwart the enterprises' willingness
to adopt this innovation since the underlying configurations of these
middleboxes may leak crucial and confidential information which can be utilized
by attackers. To address this new problem, we use firewall as an sample
functionality and propose the first privacy preserving outsourcing framework
and schemes in SDN. The basic technique that we exploit is a ground-breaking
tool in cryptography, the \textit{cryptographic multilinear map}. In contrast
to the infeasibility in efficiency if a naive approach is adopted, we devise
practical schemes that can outsource the middlebox as a blackbox after
\textit{obfuscating} it such that the cloud provider can efficiently perform
the same functionality without knowing its underlying private configurations.
Both theoretical analysis and experiments on real-world firewall rules
demonstrate that our schemes are secure, accurate, and practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00392</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00392</id><created>2015-02-02</created><authors><author><keyname>Kan</keyname><forenames>Jia-Qian</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author></authors><title>Effects of awareness diffusion and self-initiated awareness behavior on
  epidemic spreading - an approach based on multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the interplay between the epidemic spreading and the
diffusion of awareness in multiplex networks. In the model, an infectious
disease can spread in one network representing the paths of epidemic spreading
(contact network), leading to the diffusion of awareness in the other network
(information network), and then the diffusion of awareness will cause
individuals to take social distances, which in turn affects the epidemic
spreading. As for the diffusion of awareness, we assume that, on the one hand,
individuals can be informed by other aware neighbors in information network, on
the other hand, the susceptible individuals can be self-awareness induced by
the infected neighbors in the contact networks (local information) or mass
media (global information). Through Markov chain approach and numerical
computations, we find that the density of infected individuals and the epidemic
threshold can be affected by the structures of the two networks and the
effective transmission rate of the awareness. However, we prove that though the
introduction of the self-awareness can lower the density of infection, which
cannot increase the epidemic threshold no matter of the local information or
global information. Our finding is remarkably different to many previous
results--local information based behavioral response can alter the epidemic
threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00395</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00395</id><created>2015-02-02</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Threshold Functions in Random s-Intersection Graphs</title><categories>physics.soc-ph cs.DM cs.SI math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random $s$-intersection graphs have recently received considerable attention
in a wide range of application areas. In such a graph, each vertex is equipped
with a set of items in some random manner, and any two vertices establish an
undirected edge in between if and only if they have at least $s$ common items.
In particular, in a uniform random $s$-intersection graph, each vertex
independently selects a fixed number of items uniformly at random from a common
item pool, while in a binomial random $s$-intersection graph, each item in some
item pool is independently attached to each vertex with the same probability.
  For binomial/uniform random $s$-intersection graphs, we establish threshold
functions for perfect matching containment, Hamilton cycle containment, and
$k$-robustness, where $k$-robustness is in the sense of Zhang and Sundaram
[IEEE Conf. on Decision &amp; Control '12]. We show that these threshold functions
resemble those of classical Erd\H{o}s-R\'{e}nyi graphs, where each pair of
vertices has an undirected edge independently with the same probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00400</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00400</id><created>2015-02-02</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>k-Connectivity of Random Key Graphs</title><categories>physics.soc-ph cs.DM cs.SI math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random key graphs represent topologies of secure wireless sensor networks
that apply the seminal Eschenauer-Gligor random key predistribution scheme to
secure communication between sensors. These graphs have received much attention
and also been used in diverse application areas beyond secure sensor networks;
e.g., cryptanalysis, social networks, and recommender systems. Formally, a
random key graph with $n$ nodes is constructed by assigning each node $X_n$
keys selected uniformly at random from a pool of $Y_n$ keys and then putting an
undirected edge between any two nodes sharing at least one key. Considerable
progress has been made in the literature to analyze connectivity and
$k$-connectivity of random key graphs, where $k$-connectivity of a graph
ensures connectivity even after the removal of $k$ nodes or $k$ edges. Yet, it
still remains an open question for $k$-connectivity in random key graphs under
$X_n \geq 2$ and $X_n = o(\sqrt{\ln n})$ (the case of $X_n=1$ is trivial). In
this paper, we answer the above problem by providing an exact analysis of
$k$-connectivity in random key graphs under $X_n \geq 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00404</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00404</id><created>2015-02-02</created><updated>2015-02-18</updated><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>A curious gap in one-dimensional geometric random graphs between
  connectivity and the absence of isolated node</title><categories>physics.soc-ph cs.DM cs.SI math.CO math.PR</categories><comments>Similar (yet slightly weaker) results were already proved by Appel
  and Russo in their 1997 paper in Advances in Applied Probability and their
  2002 paper in Statistics and Probability Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-dimensional geometric random graphs are constructed by distributing $n$
nodes uniformly and independently on a unit interval and then assigning an
undirected edge between any two nodes that have a distance at most $r_n$. These
graphs have received much interest and been used in various applications
including wireless networks. A threshold of $r_n$ for connectivity is known as
$r_n^{*} = \frac{\ln n}{n}$ in the literature. In this paper, we prove that a
threshold of $r_n$ for the absence of isolated node is $\frac{\ln n}{2 n}$
(i.e., a half of the threshold $r_n^{*}$). Our result shows there is a curious
gap between thresholds of connectivity and the absence of isolated node in
one-dimensional geometric random graphs; in particular, when $r_n$ equals
$\frac{c\ln n}{ n}$ for a constant $c \in( \frac{1}{2}, 1)$, a one-dimensional
geometric random graph has no isolated node but is not connected. This curious
gap in one-dimensional geometric random graphs is in sharp contrast to the
prevalent phenomenon in many other random graphs such as two-dimensional
geometric random graphs, Erd\H{o}s-R\'enyi graphs, and random intersection
graphs, all of which in the asymptotic sense become connected as soon as there
is no isolated node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00405</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00405</id><created>2015-02-02</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Monotone Increasing Properties and Their Phase Transitions in Uniform
  Random Intersection Graphs</title><categories>physics.soc-ph cs.DM cs.SI math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uniform random intersection graphs have received much interest and been used
in diverse applications. A uniform random intersection graph with $n$ nodes is
constructed as follows: each node selects a set of $K_n$ different items
uniformly at random from the same pool of $P_n$ distinct items, and two nodes
establish an undirected edge in between if and only if they share at least one
item. For such graph denoted by $G(n, K_n, P_n)$, we present the following
results in this paper. First, we provide an exact analysis on the probabilities
of $G(n, K_n, P_n)$ having a perfect matching and having a Hamilton cycle
respectively, under $P_n = \omega\big(n (\ln n)^5\big)$ (all asymptotic
notation are understood with $n \to \infty$). The analysis reveals that just
like ($k$-)connectivity shown in prior work, for both properties of perfect
matching containment and Hamilton cycle containment, $G(n, K_n, P_n)$ also
exhibits phase transitions: for each property above, as $K_n$ increases, the
limit of the probability that $G(n, K_n, P_n)$ has the property increases from
$0$ to $1$. Second, we compute the phase transition widths of $G(n, K_n, P_n)$
for $k$-connectivity (KC), perfect matching containment (PMC), and Hamilton
cycle containment (HCC), respectively. For a graph property $R$ and a positive
constant $a &lt; \frac{1}{2}$, with the phase transition width $d_n(R, a)$ defined
as the difference between the minimal $K_n$ ensuring $G(n, K_n, P_n)$ having
property $R$ with probability at least $1-a$ or $a$, we show for any positive
constants $a&lt;\frac{1}{2}$ and $k$: (i) If $P_n=\Omega(n)$ and $P_n=o(n\ln n)$,
then $d_n(KC, a)$ is either $0$ or $1$ for each $n$ sufficiently large. (ii) If
$P_n=\Theta(n\ln n)$, then $d_n(KC, a)=\Theta(1)$. (iii) If $P_n=\omega(n\ln
n)$, then $d_n(KC, a)=\omega(1)$. (iv) If $P_n=\omega\big(n (\ln n)^5\big)$,
$d_n(PMC, a)$ and $d_n(HCC, a)$ are both $\omega(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00407</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00407</id><created>2015-02-02</created><authors><author><keyname>Nguyen</keyname><forenames>Van Minh</forenames></author><author><keyname>Chen</keyname><forenames>Chung Shue</forenames></author><author><keyname>Thomas</keyname><forenames>Laurent</forenames></author></authors><title>A Unified Stochastic Model of Handover Measurement in Mobile Networks</title><categories>cs.NI cs.IT math.IT math.PR</categories><comments>Published in IEEE/ACM Transaction on Networking, vol.22, no.5,
  pp.1559,1576, Oct. 2014</comments><journal-ref>IEEE/ACM Transactions on Networking, vol.22, no.5, pp.1559,1576,
  Oct. 2014</journal-ref><doi>10.1109/TNET.2013.2283577</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handover measurement is responsible for finding a handover target and
directly decides the performance of mobility management. It is governed by a
complex combination of parameters dealing with multi-cell scenarios and system
dynamics. A network design has to offer an appropriate handover measurement
procedure in such a multi-constraint problem. The present paper proposes a
unified framework for the network analysis and optimization. The exposition
focuses on the stochastic modeling and addresses its key probabilistic events
namely (i) suitable handover target found, (ii) service failure, (iii) handover
measurement triggering, and (iv) handover measurement withdrawal. We derive
their closed-form expressions and provide a generalized setup for the analysis
of handover measurement failure and target cell quality by the best signal
quality and \st{minimum duration outage} \textit{level crossing properties}.
Finally, we show its application and effectiveness in today's 3GPP-LTE cellular
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00413</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00413</id><created>2015-02-02</created><updated>2015-02-03</updated><authors><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Moshkovitz</keyname><forenames>Guy</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author><author><keyname>Shapira</keyname><forenames>Asaf</forenames></author></authors><title>Constructing Near Spanning Trees with Few Local Inspections</title><categories>math.CO cs.DS</categories><comments>References fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing a spanning tree of a graph is one of the most basic tasks in
graph theory. Motivated by several recent studies of local graph algorithms, we
consider the following variant of this problem. Let G be a connected
bounded-degree graph. Given an edge $e$ in $G$ we would like to decide whether
$e$ belongs to a connected subgraph $G'$ consisting of $(1+\epsilon)n$ edges
(for a prespecified constant $\epsilon &gt;0$), where the decision for different
edges should be consistent with the same subgraph $G'$. Can this task be
performed by inspecting only a {\em constant} number of edges in $G$? Our main
results are:
  (1) We show that if every $t$-vertex subgraph of $G$ has expansion $1/(\log
t)^{1+o(1)}$ then one can (deterministically) construct a sparse spanning
subgraph $G'$ of $G$ using few inspections. To this end we analyze a &quot;local&quot;
version of a famous minimum-weight spanning tree algorithm.
  (2) We show that the above expansion requirement is sharp even when allowing
randomization. To this end we construct a family of $3$-regular graphs of high
girth, in which every $t$-vertex subgraph has expansion $1/(\log t)^{1-o(1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00416</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00416</id><created>2015-02-02</created><authors><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>Lu</keyname><forenames>Yongyi</forenames></author><author><keyname>Li</keyname><forenames>Xiying</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Towards a solid solution of real-time fire and flame detection</title><categories>cs.CV</categories><comments>14 pages, 6 figures</comments><msc-class>68U01</msc-class><journal-ref>Multimedia Tools and Applications, pp. 1380-7501, 2014</journal-ref><doi>10.1007/s11042-014-2106-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the object detection and recognition has received growing attention
for decades, a robust fire and flame detection method is rarely explored. This
paper presents an empirical study, towards a general and solid approach to fast
detect fire and flame in videos, with the applications in video surveillance
and event retrieval. Our system consists of three cascaded steps: (1) candidate
regions proposing by a background model, (2) fire region classifying with
color-texture features and a dictionary of visual words, and (3) temporal
verifying. The experimental evaluation and analysis are done for each step. We
believe that it is a useful service to both academic research and real-world
application. In addition, we release the software of the proposed system with
the source code, as well as a public benchmark and data set, including 64 video
clips covered both indoor and outdoor scenes under different conditions. We
achieve an 82% Recall with 93% Precision on the data set, and greatly improve
the performance by state-of-the-arts methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00423</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00423</id><created>2015-02-02</created><authors><author><keyname>Giglio</keyname><forenames>Davide</forenames></author></authors><title>Fundamental lemmas for the determination of optimal control strategies
  for a class of single machine family scheduling problems</title><categories>math.OC cs.SY</categories><comments>Technical Report, 103 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four lemmas, which constitute the theoretical foundation necessary to
determine optimal control strategies for a class of single machine family
scheduling problems, are presented in this technical report. The scheduling
problem is characterized by the presence of sequence-dependent batch setup and
controllable processing times; moreover, the generalized due-date model is
adopted in the problem. The lemmas are employed within a constructive procedure
(proposed by the Author and based on the application of dynamic programming)
that allows determining the decisions which optimally solve the scheduling
problem as functions of the system state. Two complete examples of single
machine family scheduling problem are included in the technical report with the
aim of illustrating the application of the fundamental lemmas in the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00433</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00433</id><created>2015-02-02</created><authors><author><keyname>Tchapgnouo</keyname><forenames>Hortense Boudjou</forenames></author><author><keyname>Ciss</keyname><forenames>Abdoul Aziz</forenames></author></authors><title>Multi-sources Randomness Extraction over Finite Fields and Elliptic
  Curve</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This work is based on the proposal of a deterministic randomness extractor of
a random Diffie-Hellman element defined over two prime order multiplicative
subgroups of a finite fields $\mathbb{F}_{p^n}$, $G_1$ and $G_2$. We show that
the least significant bits of a random element in $G_1*G_2$, are
indistinguishable from a uniform bit-string of the same length.
  One of the main application of this extractor is to replace the use of hash
functions in pairing by the use of a good deterministic randomness extractor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00447</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00447</id><created>2015-02-02</created><authors><author><keyname>Tian</keyname><forenames>Wenhong</forenames></author><author><keyname>Wang</keyname><forenames>Xinyang</forenames></author><author><keyname>Xiong</keyname><forenames>Qin</forenames></author><author><keyname>Chen</keyname><forenames>Yu</forenames></author></authors><title>Obtaining Quality-Proved Near Optimal Results for Traveling Salesman
  Problem</title><categories>cs.DS</categories><comments>14 pages</comments><report-no>Technical Report 20150129</report-no><msc-class>Algorithm</msc-class><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The traveling salesman problem (TSP) is one of the most challenging NP-hard
problems. It has widely applications in various disciplines such as physics,
biology, computer science and so forth. The best known approximation algorithm
for Symmetric TSP (STSP) whose cost matrix satisfies the triangle inequality
(called $\triangle$STSP) is Christofides algorithm which was proposed in 1976
and is a $\frac{3}{2}$-approximation. Since then no proved improvement is made
and improving upon this bound is a fundamental open question in combinatorial
optimization.
  In this paper, for the first time, we propose Truncated Generalized Beta
distribution (TGB) for the probability distribution of optimal tour lengths in
a TSP. We then introduce an iterative TGB approach to obtain quality-proved
near optimal approximation, i.e.,
(1+$\frac{1}{2}(\frac{\alpha+1}{\alpha+2})^{K-1}$)-approximation where $K$ is
the number of iterations in TGB and $\alpha (&gt;&gt;1)$ is the shape parameters of
TGB. The result can approach the true optimum as $K$ increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00451</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00451</id><created>2015-02-02</created><authors><author><keyname>Kisseleff</keyname><forenames>S.</forenames></author><author><keyname>Sackenreuter</keyname><forenames>B.</forenames></author><author><keyname>Akyildiz</keyname><forenames>I. F.</forenames></author><author><keyname>Gerstacker</keyname><forenames>W.</forenames></author></authors><title>On Capacity of Active Relaying in Magnetic Induction based Wireless
  Underground Sensor Networks</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted for presentation at IEEE ICC 2015. It
  has 6 pages, 5 figures (4 colored), and 17 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless underground sensor networks (WUSNs) present a variety of new
research challenges. Magnetic induction (MI) based transmission has been
proposed to overcome the very harsh propagation conditions in underground
communications in recent years. In this approach, induction coils are utilized
as antennas in the sensor nodes. This solution achieves longer transmission
ranges compared to the traditional electromagnetic (EM) waves based approach.
Furthermore, a passive relaying technique has been proposed in the literature
where additional resonant circuits are deployed between the nodes. However,
this solution is shown to provide only a limited performance improvement under
practical system design contraints. In this work, the potential of an active
relay device is investigated which may improve the performance of the system by
combining the benefits of the traditional wireless relaying and the MI based
signal transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00478</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00478</id><created>2015-02-02</created><updated>2015-07-25</updated><authors><author><keyname>Wen</keyname><forenames>Yandong</forenames></author><author><keyname>Liu</keyname><forenames>Weiyang</forenames></author><author><keyname>Yang</keyname><forenames>Meng</forenames></author><author><keyname>Fu</keyname><forenames>Yuli</forenames></author><author><keyname>Xiang</keyname><forenames>Youjun</forenames></author><author><keyname>Hu</keyname><forenames>Rui</forenames></author></authors><title>Structured Occlusion Coding for Robust Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Occlusion in face recognition is a common yet challenging problem. While
sparse representation based classification (SRC) has been shown promising
performance in laboratory conditions (i.e. noiseless or random pixel
corrupted), it performs much worse in practical scenarios. In this paper, we
consider the practical face recognition problem, where the occlusions are
predictable and available for sampling. We propose the structured occlusion
coding (SOC) to address occlusion problems. The structured coding here lies in
two folds. On one hand, we employ a structured dictionary for recognition. On
the other hand, we propose to use the structured sparsity in this formulation.
Specifically, SOC simultaneously separates the occlusion and classifies the
image. In this way, the problem of recognizing an occluded image is turned into
seeking a structured sparse solution on occlusion-appended dictionary. In order
to construct a well-performing occlusion dictionary, we propose an occlusion
mask estimating technique via locality constrained dictionary (LCD), showing
striking improvement in occlusion sample. On a category-specific occlusion
dictionary, we replace norm sparsity with the structured sparsity which is
shown more robust, further enhancing the robustness of our approach. Moreover,
SOC achieves significant improvement in handling large occlusion in real world.
Extensive experiments are conducted on public data sets to validate the
superiority of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00481</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00481</id><created>2015-02-02</created><updated>2015-06-09</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Cococcioni</keyname><forenames>Giorgia</forenames></author></authors><title>Social setting, intuition, and experience in lab experiments interact to
  shape cooperative decision-making</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><comments>Forthcoming in Proceedings of the Royal Society B: Biological
  Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies suggest that cooperative decision-making in one-shot
interactions is a history-dependent dynamic process: promoting intuition versus
deliberation has typically a positive effect on cooperation (dynamism) among
people living in a cooperative setting and with no previous experience in
economic games on cooperation (history-dependence). Here we report on a lab
experiment exploring how these findings transfer to a non-cooperative setting.
We find two major results: (i) promoting intuition versus deliberation has no
effect on cooperative behavior among inexperienced subjects living in a
non-cooperative setting; (ii) experienced subjects cooperate more than
inexperienced subjects, but only under time pressure. These results suggest
that cooperation is a learning process, rather than an instinctive impulse or a
self-controlled choice, and that experience operates primarily via the channel
of intuition. In doing so, our findings shed further light on the cognitive
basis of human cooperative decision-making and provide further support for the
recently proposed Social Heuristics Hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00495</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00495</id><created>2015-01-30</created><authors><author><keyname>Bui</keyname><forenames>H.</forenames></author><author><keyname>Fukagawa</keyname><forenames>R.</forenames></author><author><keyname>Sako</keyname><forenames>K.</forenames></author></authors><title>A Study of the Matter of SPH Application to Saturated Soil Problems</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an application of SPH to saturated soilproblems. Herein, the
standard SPH formulation was improved to model saturated soil. It is shown that
the proposed formulation could yield several advantages such as: it takes into
account the pore-water pressure in an accurate manner, it automatically
satisfies the dynamics boundary conditions between submerged soil and water,
and it reduced the computational cost. Discussions on the use of the standard
and the new SPH formulations are also given through some numerical tests.
Furthermore, some techniques to obtained correct SPH solution are also proposed
and discussed. To the end, this paper suggests that the proposed SPH
formulation should be considered as the basic formulation for further
developments of SPH for soil-water couple problems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00500</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00500</id><created>2015-02-02</created><authors><author><keyname>Heredia</keyname><forenames>Miguel</forenames></author><author><keyname>Endres</keyname><forenames>Felix</forenames></author><author><keyname>Burgard</keyname><forenames>Wolfram</forenames></author><author><keyname>Sanz</keyname><forenames>Rafael</forenames></author></authors><title>Fast and Robust Feature Matching for RGB-D Based Localization</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel approach to global localization using an
RGB-D camera in maps of visual features. For large maps, the performance of
pure image matching techniques decays in terms of robustness and computational
cost. Particularly, repeated occurrences of similar features due to repeating
structure in the world (e.g., doorways, chairs, etc.) or missing associations
between observations pose critical challenges to visual localization. We
address these challenges using a two-step approach. We first estimate a
candidate pose using few correspondences between features of the current camera
frame and the feature map. The initial set of correspondences is established by
proximity in feature space. The initial pose estimate is used in the second
step to guide spatial matching of features in 3D, i.e., searching for
associations where the image features are expected to be found in the map. A
RANSAC algorithm is used to compute a fine estimation of the pose from the
correspondences. Our approach clearly outperforms localization based on feature
matching exclusively in feature space, both in terms of estimation accuracy and
robustness to failure and allows for global localization in real time (30Hz).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00501</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00501</id><created>2015-02-02</created><authors><author><keyname>Liang</keyname><forenames>Zhujin</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Huang</keyname><forenames>Rui</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>An Expressive Deep Model for Human Action Parsing from A Single Image</title><categories>cs.CV</categories><comments>6 pages, 8 figures, ICME 2014</comments><msc-class>68U01</msc-class><doi>10.1109/ICME.2014.6890158</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at one newly raising task in vision and multimedia research:
recognizing human actions from still images. Its main challenges lie in the
large variations in human poses and appearances, as well as the lack of
temporal motion information. Addressing these problems, we propose to develop
an expressive deep model to naturally integrate human layout and surrounding
contexts for higher level action understanding from still images. In
particular, a Deep Belief Net is trained to fuse information from different
noisy sources such as body part detection and object detection. To bridge the
semantic gap, we used manually labeled data to greatly improve the
effectiveness and efficiency of the pre-training and fine-tuning stages of the
DBN training. The resulting framework is shown to be robust to sometimes
unreliable inputs (e.g., imprecise detections of human parts and objects), and
outperforms the state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00507</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00507</id><created>2015-02-02</created><authors><author><keyname>Chandra</keyname><forenames>Kishor</forenames></author><author><keyname>Doff</keyname><forenames>Arjan</forenames></author><author><keyname>Cao</keyname><forenames>Zizheng</forenames></author><author><keyname>Prasad</keyname><forenames>R. Venkatesha</forenames></author><author><keyname>Niemegeers</keyname><forenames>Ignas</forenames></author></authors><title>60 GHz MAC Standardization: Progress and Way Forward</title><categories>cs.NI cs.ET cs.IT math.IT</categories><journal-ref>Published in Consumer Communications and Networking Conference
  (CCNC), 2015, IEEE, Jan. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication at mmWave frequencies has been the focus in the recent years.
In this paper, we discuss standardization efforts in 60 GHz short range
communication and the progress therein. We compare the available standards in
terms of network architecture, medium access control mechanisms, physical layer
techniques and several other features. Comparative analysis indicates that IEEE
802.11ad is likely to lead the short-range indoor communication at 60 GHz. We
bring to the fore resolved and unresolved issues pertaining to robust WLAN
connectivity at 60 GHz. Further, we discuss the role of mmWave bands in 5G
communication scenarios and highlight the further efforts required in terms of
research and standardization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00512</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00512</id><created>2015-02-02</created><authors><author><keyname>Williams</keyname><forenames>Will</forenames></author><author><keyname>Prasad</keyname><forenames>Niranjani</forenames></author><author><keyname>Mrva</keyname><forenames>David</forenames></author><author><keyname>Ash</keyname><forenames>Tom</forenames></author><author><keyname>Robinson</keyname><forenames>Tony</forenames></author></authors><title>Scaling Recurrent Neural Network Language Models</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the scaling properties of Recurrent Neural Network
Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and
address the questions of how RNNLMs scale with respect to model size,
training-set size, computational costs and memory. Our analysis shows that
despite being more costly to train, RNNLMs obtain much lower perplexities on
standard benchmarks than n-gram models. We train the largest known RNNs and
present relative word error rates gains of 18% on an ASR task. We also present
the new lowest perplexities on the recently released billion word language
modelling benchmark, 1 BLEU point gain on machine translation and a 17%
relative hit rate gain in word prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00517</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00517</id><created>2015-02-02</created><authors><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Codes for DNA Sequence Profiles</title><categories>cs.IT math.CO math.IT</categories><comments>27 pages, 5 figures. Journal version of arXiv:1410.8837</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of storing and retrieving information from synthetic
DNA media. The mathematical basis of the problem is the construction and design
of sequences that may be discriminated based on their collection of substrings
observed through a noisy channel. This problem of reconstructing sequences from
traces was first investigated in the noiseless setting under the name of
&quot;Markov type&quot; analysis. Here, we explain the connection between the
reconstruction problem and the problem of DNA synthesis and sequencing, and
introduce the notion of a DNA storage channel. We analyze the number of
sequence equivalence classes under the channel mapping and propose new
asymmetric coding techniques to combat the effects of synthesis and sequencing
noise. In our analysis, we make use of restricted de Bruijn graphs and Ehrhart
theory for rational polytopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00521</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00521</id><created>2015-02-02</created><authors><author><keyname>Horvath</keyname><forenames>I.</forenames></author><author><keyname>Telek</keyname><forenames>M.</forenames></author></authors><title>A constructive proof of the phase-type characterization theorem</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new proof of O'Cinneide's characterization theorem. It
is much simpler than the original one and constructive in the sense that we not
only show the existence of a phase type representation, but present a procedure
which creates a phase type representation. We prove that the procedure succeeds
when the conditions of the characterization theorem hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00523</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00523</id><created>2015-02-02</created><updated>2015-03-10</updated><authors><author><keyname>Ying</keyname><forenames>Qiu Fang</forenames></author><author><keyname>Venkatramanan</keyname><forenames>Srinivasan</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>Modeling and Analysis of Scholar Mobility on Scientific Landscape</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>To appear in BigScholar, WWW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific literature till date can be thought of as a partially revealed
landscape, where scholars continue to unveil hidden knowledge by exploring
novel research topics. How do scholars explore the scientific landscape , i.e.,
choose research topics to work on? We propose an agent-based model of topic
mobility behavior where scholars migrate across research topics on the space of
science following different strategies, seeking different utilities. We use
this model to study whether strategies widely used in current scientific
community can provide a balance between individual scientific success and the
efficiency and diversity of the whole academic society. Through extensive
simulations, we provide insights into the roles of different strategies, such
as choosing topics according to research potential or the popularity. Our model
provides a conceptual framework and a computational approach to analyze
scholars' behavior and its impact on scientific production. We also discuss how
such an agent-based modeling approach can be integrated with big real-world
scholarly data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00524</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00524</id><created>2015-02-02</created><updated>2015-10-23</updated><authors><author><keyname>Marxer</keyname><forenames>Ricard</forenames></author><author><keyname>Purwins</keyname><forenames>Hendrik</forenames></author></authors><title>Unsupervised Incremental Learning and Prediction of Music Signals</title><categories>cs.SD cs.IR cs.LG stat.ML</categories><comments>13 pages, 10 figures</comments><msc-class>68T05</msc-class><acm-class>I.2.6; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system is presented that segments, clusters and predicts musical audio in
an unsupervised manner, adjusting the number of (timbre) clusters
instantaneously to the audio input. A sequence learning algorithm adapts its
structure to a dynamically changing clustering tree. The flow of the system is
as follows: 1) segmentation by onset detection, 2) timbre representation of
each segment by Mel frequency cepstrum coefficients, 3) discretization by
incremental clustering, yielding a tree of different sound classes (e.g.
instruments) that can grow or shrink on the fly driven by the instantaneous
sound events, resulting in a discrete symbol sequence, 4) extraction of
statistical regularities of the symbol sequence, using hierarchical N-grams and
the newly introduced conceptual Boltzmann machine, and 5) prediction of the
next sound event in the sequence. The system's robustness is assessed with
respect to complexity and noisiness of the signal. Clustering in isolation
yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing
voice and drums. Onset detection jointly with clustering achieve an ARI of
81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /
39.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00527</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00527</id><created>2015-02-02</created><authors><author><keyname>Volkovs</keyname><forenames>Maksims</forenames></author></authors><title>Context Models For Web Search Personalization</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our solution to the Yandex Personalized Web Search Challenge. The
aim of this challenge was to use the historical search logs to personalize
top-N document rankings for a set of test users. We used over 100 features
extracted from user- and query-depended contexts to train neural net and
tree-based learning-to-rank and regression models. Our final submission, which
was a blend of several different models, achieved an NDCG@10 of 0.80476 and
placed 4'th amongst the 194 teams winning 3'rd prize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00530</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00530</id><created>2015-02-02</created><authors><author><keyname>Boroojeni</keyname><forenames>Kianoosh G.</forenames></author><author><keyname>Mokhtari</keyname><forenames>Shekoufeh</forenames></author><author><keyname>Amini</keyname><forenames>M. H.</forenames></author><author><keyname>Iyengar</keyname><forenames>S. S.</forenames></author></authors><title>Optimal Two-Tier Forecasting Power Generation Model in Smart Grids</title><categories>cs.SY</categories><comments>10 pages, 3 figures, journal paper in International Journal of
  Information Processing (IJIP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been an increasing trend in the electric power system from a
centralized generation-driven grid to a more reliable, environmental friendly,
and customer-driven grid. One of the most important issues which the designers
of smart grids need to deal with is to forecast the fluctuations of power
demand and generation in order to make the power system facilities more
flexible to the variable nature of renewable power resources and demand-side.
This paper proposes a novel two-tier scheme for forecasting the power demand
and generation in a general residential electrical gird which uses the
distributed renewable resources as the primary energy resource. The proposed
forecasting scheme has two tiers: long-term demand/generation forecaster which
is based on Maximum-Likelihood Estimator (MLE) and real-time demand/generation
forecaster which is based on Auto-Regressive Integrated Moving-Average (ARIMA)
model. The paper also shows that how bulk generation improves the adequacy of
proposed residential system by canceling-out the forecasters estimation errors
which are in the form of Gaussian White noises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00536</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00536</id><created>2015-02-02</created><updated>2015-12-09</updated><authors><author><keyname>Kalev</keyname><forenames>Amir</forenames></author><author><keyname>Kosut</keyname><forenames>Robert L.</forenames></author><author><keyname>Deutsch</keyname><forenames>Ivan H.</forenames></author></authors><title>Quantum Tomography Protocols with Positivity are Compressed Sensing
  Protocols</title><categories>quant-ph cs.IT math.IT</categories><comments>9 + 5 pages, 3 figures; v3 published version</comments><journal-ref>npj Quantum Information (2015) 1, 15018</journal-ref><doi>10.1038/npjqi.2015.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing complex quantum systems is a vital task in quantum information
science. Quantum tomography, the standard tool used for this purpose, uses a
well-designed measurement record to reconstruct quantum states and processes.
It is, however, notoriously inefficient. Recently, the classical signal
reconstruction technique known as &quot;compressed sensing&quot; has been ported to
quantum information science to overcome this challenge: accurate tomography can
be achieved with substantially fewer measurement settings, thereby greatly
enhancing the efficiency of quantum tomography. Here we show that compressed
sensing tomography of quantum systems is essentially guaranteed by a special
property of quantum mechanics itself---that the mathematical objects that
describe the system in quantum mechanics are matrices with nonnegative
eigenvalues. This result has an impact on the way quantum tomography is
understood and implemented. In particular, it implies that the information
obtained about a quantum system through compressed sensing methods exhibits a
new sense of &quot;informational completeness.&quot; This has important consequences on
the efficiency of data taking for quantum tomography, and enables us to
construct informationally complete measurements that are robust to noise and
modeling errors. Moreover, our result shows that one can expand the numerical
tool-box used in quantum tomography and employ highly efficient algorithms
developed to handle large dimensional matrices on a large dimensional Hilbert
space. While we mainly present our results in the context of quantum
tomography, they apply to the general case of positive semidefinite matrix
recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00549</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00549</id><created>2015-01-29</created><authors><author><keyname>Ngamkham</keyname><forenames>W.</forenames></author><author><keyname>van Dongen</keyname><forenames>M. N.</forenames></author><author><keyname>Serdijn</keyname><forenames>W. A.</forenames></author><author><keyname>Bes</keyname><forenames>C. J.</forenames></author><author><keyname>Briaire</keyname><forenames>J. J.</forenames></author><author><keyname>Frijns</keyname><forenames>J. H. M.</forenames></author></authors><title>A 0.042 mm^2 programmable biphasic stimulator for cochlear implants
  suitable for a large number of channels</title><categories>q-bio.NC cs.ET physics.ins-det</categories><comments>13 pages, 12 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a compact programmable biphasic stimulator for cochlear
implants. By employing double-loop negative feedback, the output impedance of
the current generator is increased, while maximizing the voltage compliance of
the output transistor. To make the stimulator circuit compact, the stimulation
current is set by scaling a reference current using a two stage binary-weighted
transistor DAC (comprising a 3 bit high-voltage transistor DAC and a 4 bit
low-voltage transistor DAC). With this structure the power consumption and the
area of the circuit can be minimized. The proposed circuit has been implemented
in AMS 0.18um high-voltage CMOS IC technology, using an active chip area of
about 0.042mm^2. Measurement results show that proper charge balance of the
anodic and cathodic stimulation phases is achieved and a dc blocking capacitor
can be omitted. The resulting reduction in the required area makes the proposed
system suitable for a large number of channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00555</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00555</id><created>2015-01-28</created><authors><author><keyname>Oliveira</keyname><forenames>P. A. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Kulasekera</keyname><forenames>S.</forenames></author><author><keyname>Madanayake</keyname><forenames>A.</forenames></author></authors><title>A Discrete Tchebichef Transform Approximation for Image and Video Coding</title><categories>stat.ME cs.CV cs.MM cs.NA stat.CO</categories><comments>13 pages, 5 figures, 2 tables</comments><journal-ref>IEEE Signal Processing Letters, vol. 22, issue 8, pp. 1137-1141,
  2015</journal-ref><doi>10.1109/LSP.2015.2389899</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a low-complexity approximation for the discrete
Tchebichef transform (DTT). The proposed forward and inverse transforms are
multiplication-free and require a reduced number of additions and bit-shifting
operations. Numerical compression simulations demonstrate the efficiency of the
proposed transform for image and video coding. Furthermore, Xilinx Virtex-6
FPGA based hardware realization shows 44.9% reduction in dynamic power
consumption and 64.7% lower area when compared to the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00558</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00558</id><created>2015-02-02</created><updated>2015-02-09</updated><authors><author><keyname>Cicconet</keyname><forenames>Marcelo</forenames></author><author><keyname>Geiger</keyname><forenames>Davi</forenames></author><author><keyname>Werman</keyname><forenames>Michael</forenames></author></authors><title>Complex-Valued Hough Transforms for Circles</title><categories>cs.CV</categories><comments>The paper has been withdrawn since the authors concluded a more
  comprehensive study on the choice of parameters needs to be performed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates the use of complex variables to represent votes in the
Hough transform for circle detection. Replacing the positive numbers
classically used in the parameter space of the Hough transforms by complex
numbers allows cancellation effects when adding up the votes. Cancellation and
the computation of shape likelihood via a complex number's magnitude square
lead to more robust solutions than the &quot;classic&quot; algorithms, as shown by
computational experiments on synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00561</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00561</id><created>2015-02-02</created><updated>2015-02-09</updated><authors><author><keyname>Cicconet</keyname><forenames>Marcelo</forenames></author><author><keyname>Geiger</keyname><forenames>Davi</forenames></author><author><keyname>Werman</keyname><forenames>Michael</forenames></author></authors><title>Quantum Pairwise Symmetry: Applications in 2D Shape Analysis</title><categories>cs.CV</categories><comments>The paper has been withdrawn since the authors concluded a more
  comprehensive study on the choice of parameters needs to be performed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pair of rooted tangents -- defining a quantum triangle -- with an
associated quantum wave of spin 1/2 is proposed as the primitive to represent
and compute symmetry. Measures of the spin characterize how &quot;isosceles&quot; or how
&quot;degenerate&quot; these triangles are -- which corresponds to their mirror or
parallel symmetry. We also introduce a complex-valued kernel to model
probability errors in the parameter space, which is more robust to noise and
clutter than the classical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00582</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00582</id><created>2015-02-02</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Lermam</keyname><forenames>Kristina</forenames></author></authors><title>VIP: Incorporating Human Cognitive Biases in a Probabilistic Model of
  Retweeting</title><categories>cs.SI</categories><comments>SBP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information spread in social media depends on a number of factors, including
how the site displays information, how users navigate it to find items of
interest, users' tastes, and the `virality' of information, i.e., its
propensity to be adopted, or retweeted, upon exposure. Probabilistic models can
learn users' tastes from the history of their item adoptions and recommend new
items to users. However, current models ignore cognitive biases that are known
to affect behavior. Specifically, people pay more attention to items at the top
of a list than those in lower positions. As a consequence, items near the top
of a user's social media stream have higher visibility, and are more likely to
be seen and adopted, than those appearing below. Another bias is due to the
item's fitness: some items have a high propensity to spread upon exposure
regardless of the interests of adopting users. We propose a probabilistic model
that incorporates human cognitive biases and personal relevance in the
generative model of information spread. We use the model to predict how
messages containing URLs spread on Twitter. Our work shows that models of user
behavior that account for cognitive factors can better describe and predict
user behavior in social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00584</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00584</id><created>2015-02-02</created><authors><author><keyname>Zhirov</keyname><forenames>O. V.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>D. L.</forenames></author></authors><title>Anderson transition for Google matrix eigenstates</title><categories>cond-mat.dis-nn cs.SI physics.soc-ph</categories><comments>9 pages, 12 figs, revtex</comments><journal-ref>Ann. Phys. (Berlin) v.527, N9-10, p.713 (2015)</journal-ref><doi>10.1002/andp.201500110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a number of random matrix models describing the Google matrix G
of directed networks. The properties of their spectra and eigenstates are
analyzed by numerical matrix diagonalization. We show that for certain models
it is possible to have an algebraic decay of PageRank vector with the exponent
similar to real directed networks. At the same time the spectrum has no
spectral gap and a broad distribution of eigenvalues in the complex plain. The
eigenstates of G are characterized by the Anderson transition from localized to
delocalized states and a mobility edge curve in the complex plane of
eigenvalues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00588</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00588</id><created>2015-02-02</created><authors><author><keyname>D'Oro</keyname><forenames>Salvatore</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Palazzo</keyname><forenames>Sergio</forenames></author></authors><title>Cost-Efficient Throughput Maximization in Multi-Carrier Cognitive Radio
  Systems</title><categories>cs.IT cs.GT math.IT</categories><comments>24 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio (CR) systems allow opportunistic, secondary users (SUs) to
access portions of the spectrum that are unused by the network's licensed
primary users (PUs), provided that the induced interference does not compromise
the primary users' performance guarantees. To account for interference
constraints of this type, we consider a flexible spectrum access pricing scheme
that charges secondary users based on the interference that they cause to the
system's primary users (individually, globally, or both), and we examine how
secondary users can maximize their achievable transmission rate in this
setting. We show that the resulting non-cooperative game admits a unique Nash
equilibrium under very mild assumptions on the pricing mechanism employed by
the network operator, and under both static and ergodic (fast-fading) channel
conditions. In addition, we derive a dynamic power allocation policy that
converges to equilibrium within a few iterations (even for large numbers of
users), and which relies only on local signal-to-interference-and-noise
measurements; importantly, the proposed algorithm retains its convergence
properties even in the ergodic channel regime, despite the inherent
stochasticity thereof. Our theoretical analysis is complemented by extensive
numerical simulations which illustrate the performance and scalability
properties of the proposed pricing scheme under realistic network conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00592</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00592</id><created>2015-02-02</created><authors><author><keyname>Tablada</keyname><forenames>C. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author></authors><title>A Class of DCT Approximations Based on the Feig-Winograd Algorithm</title><categories>stat.ME cs.CV cs.MM cs.NA stat.AP</categories><comments>26 pages, 4 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of matrices based on a parametrization of the Feig-Winograd
factorization of 8-point DCT is proposed. Such parametrization induces a matrix
subspace, which unifies a number of existing methods for DCT approximation. By
solving a comprehensive multicriteria optimization problem, we identified
several new DCT approximations. Obtained solutions were sought to possess the
following properties: (i) low multiplierless computational complexity, (ii)
orthogonality or near orthogonality, (iii) low complexity invertibility, and
(iv) close proximity and performance to the exact DCT. Proposed approximations
were submitted to assessment in terms of proximity to the DCT, coding
performance, and suitability for image compression. Considering Pareto
efficiency, particular new proposed approximations could outperform various
existing methods archived in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00598</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00598</id><created>2015-02-02</created><updated>2016-01-12</updated><authors><author><keyname>Kaptein</keyname><forenames>Maurits</forenames></author><author><keyname>Iannuzzi</keyname><forenames>Davide</forenames></author></authors><title>Lock in Feedback in Sequential Experiments</title><categories>cs.LG</categories><comments>20 Pages, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We often encounter situations in which an experimenter wants to find, by
sequential experimentation, $x_{max} = \arg\max_{x} f(x)$, where $f(x)$ is a
(possibly unknown) function of a well controllable variable $x$. Taking
inspiration from physics and engineering, we have designed a new method to
address this problem. In this paper, we first introduce the method in
continuous time, and then present two algorithms for use in sequential
experiments. Through a series of simulation studies, we show that the method is
effective for finding maxima of unknown functions by experimentation, even when
the maximum of the functions drifts or when the signal to noise ratio is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00611</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00611</id><created>2015-02-02</created><updated>2015-07-04</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Kom&#xe1;rkov&#xe1;</keyname><forenames>Zuzana</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Unifying Two Views on Multiple Mean-Payoff Objectives in Markov Decision
  Processes</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Markov decision processes (MDPs) with multiple limit-average (or
mean-payoff) objectives. There exist two different views: (i) the expectation
semantics, where the goal is to optimize the expected mean-payoff objective,
and (ii) the satisfaction semantics, where the goal is to maximize the
probability of runs such that the mean-payoff value stays above a given vector.
We consider optimization with respect to both objectives at once, thus unifying
the existing semantics. Precisely, the goal is to optimize the expectation
while ensuring the satisfaction constraint. Our problem captures the notion of
optimization with respect to strategies that are risk-averse (i.e., ensure
certain probabilistic guarantee). Our main results are as follows: First, we
present algorithms for the decision problems which are always polynomial in the
size of the MDP. We also show that an approximation of the Pareto-curve can be
computed in time polynomial in the size of the MDP, and the approximation
factor, but exponential in the number of dimensions. Second, we present a
complete characterization of the strategy complexity (in terms of memory bounds
and randomization) required to solve our problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00621</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00621</id><created>2015-02-01</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Yuan</keyname><forenames>Kun</forenames></author><author><keyname>Gao</keyname><forenames>Jhih-Rong</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>E-BLOW: E-Beam Lithography Overlapping aware Stencil Planning for MCC
  System</title><categories>cs.OH</categories><comments>arXiv admin note: text overlap with arXiv:1402.2435</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electron beam lithography (EBL) is a promising maskless solution for the
technology beyond 14nm logic node. To overcome its throughput limitation,
industry has proposed character projection (CP) technique, where some complex
shapes (characters) can be printed in one shot. Recently the traditional EBL
system is extended into multi-column cell (MCC) system to further improve the
throughput. In MCC system, several independent CPs are used to further speed-up
the writing process. Because of the area constraint of stencil, MCC system
needs to be packed/planned carefully to take advantage of the characters. In
this paper, we prove that the overlapping aware stencil planning (OSP) problem
is NP-hard. To solve OSP problem in MCC system, we present a tool, E-BLOW, with
several novel speedup techniques, such as successive relaxation, dynamic
programming, and KD-Tree based clustering. Experimental results show that,
compared with previous works, E-BLOW demonstrates better performance for both
conventional EBL system and MCC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00647</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00647</id><created>2015-02-02</created><authors><author><keyname>G&#xfc;l</keyname><forenames>G&#xf6;khan</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author></authors><title>Minimax Robust Hypothesis Testing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimax robust hypothesis testing problem for the case where the nominal
probability distributions are subject to both modeling errors and outliers is
studied in twofold. First, a robust hypothesis testing scheme based on a
relative entropy distance is designed. This approach provides robustness with
respect to modeling errors and is a generalization of a previous work proposed
by Levy. Then, it is shown that this scheme can be combined with Huber's robust
test through a composite uncertainty class, for which the existence of a saddle
value condition is also proven. The composite version of the robust hypothesis
testing scheme as well as the individual robust tests are extended to fixed
sample size and sequential probability ratio tests. The composite model is
shown to extend to robust estimation problems as well. Simulation results are
provided to validate the proposed assertions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00652</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00652</id><created>2015-02-02</created><authors><author><keyname>Ladick&#xfd;</keyname><forenames>&#x13d;ubor</forenames></author><author><keyname>H&#xe4;ne</keyname><forenames>Christian</forenames></author><author><keyname>Pollefeys</keyname><forenames>Marc</forenames></author></authors><title>Learning the Matching Function</title><categories>cs.CV</categories><comments>rejected from ACCV 2014 and probably from CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The matching function for the problem of stereo reconstruction or optical
flow has been traditionally designed as a function of the distance between the
features describing matched pixels. This approach works under assumption, that
the appearance of pixels in two stereo cameras or in two consecutive video
frames does not change dramatically. However, this might not be the case, if we
try to match pixels over a large interval of time.
  In this paper we propose a method, which learns the matching function, that
automatically finds the space of allowed changes in visual appearance, such as
due to the motion blur, chromatic distortions, different colour calibration or
seasonal changes. Furthermore, it automatically learns the importance of
matching scores of contextual features at different relative locations and
scales. Proposed classifier gives reliable estimations of pixel disparities
already without any form of regularization.
  We evaluated our method on two standard problems - stereo matching on KITTI
outdoor dataset, optical flow on Sintel data set, and on newly introduced
TimeLapse change detection dataset. Our algorithm obtained very promising
results comparable to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00656</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00656</id><created>2015-02-02</created><updated>2015-02-08</updated><authors><author><keyname>Zeng</keyname><forenames>Weifei</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Alignment based Network Coding for Two-Unicast-Z Networks</title><categories>cs.IT math.IT</categories><comments>The paper is an extended version of our earlier paper at ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the wireline two-unicast-Z communication network over
directed acyclic graphs. The two-unicast-Z network is a two-unicast network
where the destination intending to decode the second message has apriori side
information of the first message. We make three contributions in this paper:
  1. We describe a new linear network coding algorithm for two-unicast-Z
networks over directed acyclic graphs. Our approach includes the idea of
interference alignment as one of its key ingredients. For graphs of a bounded
degree, our algorithm has linear complexity in terms of the number of vertices,
and polynomial complexity in terms of the number of edges.
  2. We prove that our algorithm achieves the rate-pair (1, 1) whenever it is
feasible in the network. Our proof serves as an alternative, albeit restricted
to two-unicast-Z networks over directed acyclic graphs, to an earlier result of
Wang et al. which studied necessary and sufficient conditions for feasibility
of the rate pair (1, 1) in two-unicast networks.
  3. We provide a new proof of the classical max-flow min-cut theorem for
directed acyclic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00658</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00658</id><created>2015-02-02</created><authors><author><keyname>Mingers</keyname><forenames>John</forenames></author><author><keyname>White</keyname><forenames>Leroy</forenames></author></authors><title>Throwing Out the Baby with the Bathwater: The Undesirable Effects of
  National Research Assessment Exercises on Research</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of the quality of research at a national level has become
increasingly common. The UK has been at the forefront of this trend having
undertaken many assessments since 1986, the latest being the Research
Excellence Framework in 2014. The argument of this paper is that, whatever the
intended results in terms of evaluating and improving research, there have been
many, presumably unintended, results that are highly undesirable for research
and the university community more generally. We situate our analysis using
Bourdieu's theory of cultural reproduction and then focus on the peculiarities
of the 2008 RAE and the 2014 REF the rules of which allowed for, and indeed
encouraged, significant game-playing on the part of striving universities. We
conclude with practical recommendations to maintain the general intention of
research assessment without the undesirable side-effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00679</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00679</id><created>2015-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Competition and Coalition Formation of Renewable Power Producers</title><categories>math.OC cs.GT cs.SY</categories><comments>To appear in IEEE Transaction on Power Systems, Special Section on
  Wind &amp; Solar Energy: Uncovering and Accommodating their Impacts on
  Electricity Markets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate group formations and strategic behaviors of renewable power
producers in electricity markets. These producers currently bid into the
day-ahead market in a conservative fashion because of the real-time risk
associated with not meeting their bid amount. It has been suggested in the
literature that producers would bid less conservatively if they can form large
groups to take advantages of spatial diversity to reduce the uncertainty in
their aggregate output. We show that large groups of renewable producers would
act strategically to lower the aggregate output because of market power. To
maximize renewable power production, we characterize the trade-off between
market power and generation uncertainty as a function of the size of the
groups. We show there is a sweet spot in the sense that there exists groups
that are large enough to achieve the uncertainty reduction of the grand
coalition, but are small enough such that they have no significant market
power.We consider both independent and correlated forecast errors under a fixed
real-time penalty. We also consider a real-time market where both selling and
buying of energy are allowed. We validate our claims using PJM and NREL data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00690</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00690</id><created>2015-02-02</created><authors><author><keyname>Toole</keyname><forenames>Jameson L.</forenames></author><author><keyname>Herrera-Yague</keyname><forenames>Carlos</forenames></author><author><keyname>Schneider</keyname><forenames>Christian M.</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta C.</forenames></author></authors><title>Coupling Human Mobility and Social Ties</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies using massive, passively data collected from communication
technologies have revealed many ubiquitous aspects of social networks, helping
us understand and model social media, information diffusion, and organizational
dynamics. More recently, these data have come tagged with geographic
information, enabling studies of human mobility patterns and the science of
cities. We combine these two pursuits and uncover reproducible mobility
patterns amongst social contacts. First, we introduce measures of mobility
similarity and predictability and measure them for populations of users in
three large urban areas. We find individuals' visitations patterns are far more
similar to and predictable by social contacts than strangers and that these
measures are positively correlated with tie strength. Unsupervised clustering
of hourly variations in mobility similarity identifies three categories of
social ties and suggests geography is an important feature to contextualize
social relationships. We find that the composition of a user's ego network in
terms of the type of contacts they keep is correlated with mobility behavior.
Finally, we extend a popular mobility model to include movement choices based
on social contacts and compare it's ability to reproduce empirical measurements
with two additional models of mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00695</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00695</id><created>2015-02-02</created><authors><author><keyname>M</keyname><forenames>Rudra Kumar</forenames></author><author><keyname>Rao</keyname><forenames>A Ananda</forenames></author></authors><title>Assessing the Fault Proneness Degree (DFP) by Estimating the Impact of
  Change Request Artifacts Correlation</title><categories>cs.SE</categories><comments>Originally published in Eighth International Conference on Data
  Mining and Warehousing (ICDMW-2014), Further the extended version is selected
  to publish in International Journal of Information Processing, volume 8 and
  issue 4, 2014</comments><journal-ref>International Journal of Information Processing, 8(4), 35-44, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploring the impact of change requests applied to a software maintenance
project helps to assess the fault-proneness of the change request to be handled
further, which is perhaps a bug fix or even a new feature demand. In practice,
the major development community stores change requests and related data using
bug tracking systems such as Bugzilla. These data, together with the data
stored in a versioning system, such as Concurrent Versioning Systems, are a
valuable source of information to create descriptions and also can perform
useful analyzes. In our earlier work, we proposed a novel statistical bipartite
weighted graph-based approach to assessing the degree of fault-proneness of the
change request and Change Request artifacts. With the motivation gained from
this model, here we propose a novel strategy that estimates the degree of
fault-proneness of a change request by assessing the impact of a change request
artifact towards fault-proneness that considers the correlation between change
requests artifact as another factor, which is in addition to our earlier
strategy. The proposed model can be titled as Assessing the Fault Proneness
Degree of Change Request Artifacts by estimating the impact of change requests
correlation (DFP-CRC). As stated in our earlier model, the method DFP-CRC also
makes use of information retrieval methods to identify the change request
artifacts of the devised change request. And further evaluates the degree of
fault-proneness of the Change Requests by estimating the correlation between
change requests. The proposed method is evaluated by applying on concurrent
versioning and Change request logs of the production level maintenance project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00702</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00702</id><created>2015-02-02</created><updated>2015-06-05</updated><authors><author><keyname>Zhang</keyname><forenames>Shiliang</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author></authors><title>Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to
  Probe and Learn Neural Networks</title><categories>cs.LG cs.NE</categories><comments>31 pages, 5 Figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel model for high-dimensional data, called the
Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a
linear orthogonal projection and a finite mixture model under a unified
generative modeling framework. The HOPE model itself can be learned
unsupervised from unlabelled data based on the maximum likelihood estimation as
well as discriminatively from labelled data. More interestingly, we have shown
the proposed HOPE models are closely related to neural networks (NNs) in a
sense that each hidden layer can be reformulated as a HOPE model. As a result,
the HOPE framework can be used as a novel tool to probe why and how NNs work,
more importantly, to learn NNs in either supervised or unsupervised ways. In
this work, we have investigated the HOPE framework to learn NNs for several
standard tasks, including image recognition on MNIST and speech recognition on
TIMIT. Experimental results have shown that the HOPE framework yields
significant performance gains over the current state-of-the-art methods in
various types of NN learning problems, including unsupervised feature learning,
supervised or semi-supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00705</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00705</id><created>2015-02-02</created><authors><author><keyname>Ongie</keyname><forenames>Greg</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Recovery of Piecewise Smooth Images from Few Fourier Samples</title><categories>cs.CV</categories><comments>5 pages. Submitted to SampTA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a Prony-like method to recover a continuous domain 2-D piecewise
smooth image from few of its Fourier samples. Assuming the discontinuity set of
the image is localized to the zero level-set of a trigonometric polynomial, we
show the Fourier transform coefficients of partial derivatives of the signal
satisfy an annihilation relation. We present necessary and sufficient
conditions for unique recovery of piecewise constant images using the above
annihilation relation. We pose the recovery of the Fourier coefficients of the
signal from the measurements as a convex matrix completion algorithm, which
relies on the lifting of the Fourier data to a structured low-rank matrix; this
approach jointly estimates the signal and the annihilating filter. Finally, we
demonstrate our algorithm on the recovery of MRI phantoms from few
low-resolution Fourier samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00712</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00712</id><created>2015-02-02</created><authors><author><keyname>Peng</keyname><forenames>Zhanglin</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Ruimao</forenames></author><author><keyname>Xu</keyname><forenames>Jing</forenames></author></authors><title>Deep Boosting: Layered Feature Mining for General Image Classification</title><categories>cs.CV</categories><comments>6 pages, 4 figures, ICME 2014</comments><msc-class>68U01</msc-class><journal-ref>Multimedia and Expo (ICME), 2014 IEEE International Conference on
  , vol., no., pp.1,6, 14-18 July 2014</journal-ref><doi>10.1109/ICME.2014.6890323</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing effective representations is a critical but challenging problem
in multimedia understanding. The traditional handcraft features often rely on
domain knowledge, limiting the performances of exiting methods. This paper
discusses a novel computational architecture for general image feature mining,
which assembles the primitive filters (i.e. Gabor wavelets) into compositional
features in a layer-wise manner. In each layer, we produce a number of base
classifiers (i.e. regression stumps) associated with the generated features,
and discover informative compositions by using the boosting algorithm. The
output compositional features of each layer are treated as the base components
to build up the next layer. Our framework is able to generate expressive image
representations while inducing very discriminate functions for image
classification. The experiments are conducted on several public datasets, and
we demonstrate superior performances over state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00714</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00714</id><created>2015-02-02</created><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Kim</keyname><forenames>Taeyoung</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Seol</keyname><forenames>Ji-yun</forenames></author></authors><title>Exploiting the Preferred Domain of FDD Massive MIMO Systems with Uniform
  Planar Arrays</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, to appear in ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) systems hold the potential to
be an enabling technology for 5G cellular. Uniform planar array (UPA) antenna
structures are a focus of much commercial discussion because of their ability
to enable a large number of antennas in a relatively small area. With UPA
antenna structures, the base station can control the beam direction in both the
horizontal and vertical domains simultaneously. However, channel conditions may
dictate that one dimension requires higher channel state information (CSI)
accuracy than the other. We propose the use of an additional one bit of
feedback information sent from the user to the base station to indicate the
preferred domain on top of the feedback overhead of CSI quantization in
frequency division duplexing (FDD) massive MIMO systems. Combined with
variable-rate CSI quantization schemes, the numerical studies show that the
additional one bit of feedback can increase the quality of CSI significantly
for UPA antenna structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00716</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00716</id><created>2015-02-02</created><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Le</keyname><forenames>Hung</forenames></author></authors><title>Optimal dynamic program for r-domination problems over tree
  decompositions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been recent progress in showing that the exponential dependence on
treewidth in dynamic programming algorithms for solving NP-hard problems are
optimal under the Strong Exponential Time Hypothesis (SETH). We extend this
work to $r$-domination problems. In $r$-dominating set, one wished to find a
minimum subset $S$ of vertices such that every vertex of $G$ is within $r$ hops
of some vertex in $S$. In connected $r$-dominating set, one additionally
requires that the set induces a connected subgraph of $G$. We give a
$O((2r+1)^{\mathrm{tw}} n)$ time algorithm for $r$-dominating set and a
$O((2r+2)^{\mathrm{tw}} n^{O(1)})$ time algorithm for connected $r$-dominating
set in $n$-vertex graphs of treewidth $\mathrm{tw}$. We show that the running
time dependence on $r$ and $\mathrm{tw}$ is the best possible under SETH. This
adds to earlier observations that a &quot;+1&quot; in the denominator is required for
connectivity constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00717</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00717</id><created>2015-02-02</created><authors><author><keyname>Zhu</keyname><forenames>Hongyuan</forenames></author><author><keyname>Meng</keyname><forenames>Fanman</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author><author><keyname>Lu</keyname><forenames>Shijian</forenames></author></authors><title>Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image
  Segmentation and Cosegmentation</title><categories>cs.CV</categories><comments>submitted to Elsevier Journal of Visual Communications and Image
  Representation</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Image segmentation refers to the process to divide an image into
nonoverlapping meaningful regions according to human perception, which has
become a classic topic since the early ages of computer vision. A lot of
research has been conducted and has resulted in many applications. However,
while many segmentation algorithms exist, yet there are only a few sparse and
outdated summarizations available, an overview of the recent achievements and
issues is lacking. We aim to provide a comprehensive review of the recent
progress in this field. Covering 180 publications, we give an overview of broad
areas of segmentation topics including not only the classic bottom-up
approaches, but also the recent development in superpixel, interactive methods,
object proposals, semantic image parsing and image cosegmentation. In addition,
we also review the existing influential datasets and evaluation metrics.
Finally, we suggest some design flavors and research directions for future
research in image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00718</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00718</id><created>2015-02-02</created><updated>2015-04-25</updated><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Shabani</keyname><forenames>Alireza</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author></authors><title>Product Reservoir Computing: Time-Series Computation with Multiplicative
  Neurons</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Echo state networks (ESN), a type of reservoir computing (RC) architecture,
are efficient and accurate artificial neural systems for time series processing
and learning. An ESN consists of a core of recurrent neural networks, called a
reservoir, with a small number of tunable parameters to generate a
high-dimensional representation of an input, and a readout layer which is
easily trained using regression to produce a desired output from the reservoir
states. Certain computational tasks involve real-time calculation of high-order
time correlations, which requires nonlinear transformation either in the
reservoir or the readout layer. Traditional ESN employs a reservoir with
sigmoid or tanh function neurons. In contrast, some types of biological neurons
obey response curves that can be described as a product unit rather than a sum
and threshold. Inspired by this class of neurons, we introduce a RC
architecture with a reservoir of product nodes for time series computation. We
find that the product RC shows many properties of standard ESN such as
short-term memory and nonlinear capacity. On standard benchmarks for chaotic
prediction tasks, the product RC maintains the performance of a standard
nonlinear ESN while being more amenable to mathematical analysis. Our study
provides evidence that such networks are powerful in highly nonlinear tasks
owing to high-order statistics generated by the recurrent product node
reservoir.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00723</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00723</id><created>2015-02-02</created><authors><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Lai</keyname><forenames>Jianhuang</forenames></author></authors><title>Learning Contour-Fragment-based Shape Model with And-Or Tree
  Representation</title><categories>cs.CV</categories><comments>8 pages, 7 figures, CVPR 2012</comments><msc-class>68U01</msc-class><journal-ref>Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
  Conference on , vol., no., pp.135,142, 16-21 June 2012</journal-ref><doi>10.1109/CVPR.2012.6247668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple yet effective method to learn the hierarchical
object shape model consisting of local contour fragments, which represents a
category of shapes in the form of an And-Or tree. This model extends the
traditional hierarchical tree structures by introducing the &quot;switch&quot; variables
(i.e. the or-nodes) that explicitly specify production rules to capture shape
variations. We thus define the model with three layers: the leaf-nodes for
detecting local contour fragments, the or-nodes specifying selection of
leaf-nodes, and the root-node encoding the holistic distortion. In the training
stage, for optimization of the And-Or tree learning, we extend the
concave-convex procedure (CCCP) by embedding the structural clustering during
the iterative learning steps. The inference of shape detection is consistent
with the model optimization, which integrates the local testings via the
leaf-nodes and or-nodes with the global verification via the root-node. The
advantages of our approach are validated on the challenging shape databases
(i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method
is able to accurately localize shape contours against unreliable edge detection
and edge tracing. (2) The And-Or tree model enables us to well capture the
intraclass variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00724</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00724</id><created>2015-02-02</created><authors><author><keyname>Saha</keyname><forenames>Amal</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Review of Considerations for Mobile Device based Secure Access to
  Financial Services and Risk Handling Strategy for CIOs, CISOs and CTOs</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information technology and security stakeholders like CIOs, CISOs and
CTOs in financial services organization are often asked to identify the risks
with mobile computing channel for financial services that they support. They
are also asked to come up with approaches for handling risks, define risk
acceptance level and mitigate them. This requires them to articulate strategy
for supporting a huge variety of mobile devices from various vendors with
different operating systems and hardware platforms and at the same time stay
within the accepted risk level. These articulations should be captured in
information security policy document or other suitable document of financial
services organization like banks, payment service provider, etc. While risks
and mitigation approaches are available from multiple sources, the senior
stakeholders may find it challenging to articulate the issues in a
comprehensive manner for sharing with business owners and other technology
stakeholders. This paper reviews the current research that addresses the issues
mentioned above and articulates a strategy that the senior stakeholders may use
in their organization. It is assumed that this type of comprehensive strategy
guide for senior stakeholders is not readily available and CIOs, CISOs and CTOs
would find this paper to be very useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00725</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00725</id><created>2015-02-02</created><authors><author><keyname>Li</keyname><forenames>Hongwei</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author></authors><title>Cheaper and Better: Selecting Good Workers for Crowdsourcing</title><categories>stat.ML cs.AI cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing provides a popular paradigm for data collection at scale. We
study the problem of selecting subsets of workers from a given worker pool to
maximize the accuracy under a budget constraint. One natural question is
whether we should hire as many workers as the budget allows, or restrict on a
small number of top-quality workers. By theoretically analyzing the error rate
of a typical setting in crowdsourcing, we frame the worker selection problem
into a combinatorial optimization problem and propose an algorithm to solve it
efficiently. Empirical results on both simulated and real-world datasets show
that our algorithm is able to select a small number of high-quality workers,
and performs as good as, sometimes even better than, the much larger crowds as
the budget allows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00731</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00731</id><created>2015-02-02</created><updated>2015-06-15</updated><authors><author><keyname>Shin</keyname><forenames>Jaeho</forenames></author><author><keyname>Wu</keyname><forenames>Sen</forenames></author><author><keyname>Wang</keyname><forenames>Feiran</forenames></author><author><keyname>De Sa</keyname><forenames>Christopher</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Incremental Knowledge Base Construction Using DeepDive</title><categories>cs.DB cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Populating a database with unstructured information is a long-standing
problem in industry and research that encompasses problems of extraction,
cleaning, and integration. Recent names used for this problem include dealing
with dark data and knowledge base construction (KBC). In this work, we describe
DeepDive, a system that combines database and machine learning ideas to help
develop KBC systems, and we present techniques to make the KBC process more
efficient. We observe that the KBC process is iterative, and we develop
techniques to incrementally produce inference results for KBC systems. We
propose two methods for incremental inference, based respectively on sampling
and variational techniques. We also study the tradeoff space of these methods
and develop a simple rule-based optimizer. DeepDive includes all of these
contributions, and we evaluate DeepDive on five KBC systems, showing that it
can speed up KBC inference tasks by up to two orders of magnitude with
negligible impact on quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00734</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00734</id><created>2015-02-02</created><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author></authors><title>A New Cell Association Scheme In Heterogeneous Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted by IEEE ICC 2015 - Next Generation Networking Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell association scheme determines which base station (BS) and mobile user
(MU) should be associated with and also plays a significant role in determining
the average data rate a MU can achieve in heterogeneous networks. However, the
explosion of digital devices and the scarcity of spectra collectively force us
to carefully re-design cell association scheme which was kind of taken for
granted before. To address this, we develop a new cell association scheme in
heterogeneous networks based on joint consideration of the
signal-to-interference-plus-noise ratio (SINR) which a MU experiences and the
traffic load of candidate BSs1. MUs and BSs in each tier are modeled as several
independent Poisson point processes (PPPs) and all channels experience
independently and identically distributed ( i.i.d.) Rayleigh fading. Data rate
ratio and traffic load ratio distributions are derived to obtain the tier
association probability and the average ergodic MU data rate. Through numerical
results, We find that our proposed cell association scheme outperforms cell
range expansion (CRE) association scheme. Moreover, results indicate that
allocating small sized and high-density BSs will improve spectral efficiency if
using our proposed cell association scheme in heterogeneous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00739</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00739</id><created>2015-02-02</created><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Clothing Co-Parsing by Joint Image Segmentation and Labeling</title><categories>cs.CV</categories><comments>8 pages, 5 figures, CVPR 2014</comments><msc-class>68U01</msc-class><journal-ref>Computer Vision and Pattern Recognition (CVPR), 2014 IEEE
  Conference on , vol., no., pp.3182,3189, 23-28 June 2014</journal-ref><doi>10.1109/CVPR.2014.407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at developing an integrated system of clothing co-parsing, in
order to jointly parse a set of clothing images (unsegmented but annotated with
tags) into semantic configurations. We propose a data-driven framework
consisting of two phases of inference. The first phase, referred as &quot;image
co-segmentation&quot;, iterates to extract consistent regions on images and jointly
refines the regions over all images by employing the exemplar-SVM (E-SVM)
technique [23]. In the second phase (i.e. &quot;region co-labeling&quot;), we construct a
multi-image graphical model by taking the segmented regions as vertices, and
incorporate several contexts of clothing configuration (e.g., item location and
mutual interactions). The joint label assignment can be solved using the
efficient Graph Cuts algorithm. In addition to evaluate our framework on the
Fashionista dataset [30], we construct a dataset called CCP consisting of 2098
high-resolution street fashion photos to demonstrate the performance of our
system. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89%
recognition rate on the Fashionista and the CCP datasets, respectively, which
are superior compared with state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00741</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00741</id><created>2015-02-03</created><authors><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</title><categories>cs.CV</categories><comments>9 pages, 4 figures, NIPS 2012</comments><msc-class>68U01</msc-class><journal-ref>Advances in Neural Information Processing Systems (pp. 242-250),
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a novel discriminative part-based model to represent and
recognize object shapes with an &quot;And-Or graph&quot;. We define this model consisting
of three layers: the leaf-nodes with collaborative edges for localizing local
parts, the or-nodes specifying the switch of leaf-nodes, and the root-node
encoding the global verification. A discriminative learning algorithm, extended
from the CCCP [23], is proposed to train the model in a dynamical manner: the
model structure (e.g., the configuration of the leaf-nodes associated with the
or-nodes) is automatically determined with optimizing the multi-layer
parameters during the iteration. The advantages of our method are two-fold. (i)
The And-Or graph model enables us to handle well large intra-class variance and
background clutters for object shape detection from images. (ii) The proposed
learning algorithm is able to obtain the And-Or graph representation without
requiring elaborate supervision and initialization. We validate the proposed
method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and
UIUC-People), and it outperforms the state-of-the-arts approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00743</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00743</id><created>2015-02-03</created><authors><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Zhang</keyname><forenames>Liliang</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Liang</keyname><forenames>Zhujin</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author></authors><title>Deep Joint Task Learning for Generic Object Extraction</title><categories>cs.CV</categories><comments>9 pages, 4 figures, NIPS 2014</comments><msc-class>68U01</msc-class><journal-ref>Advances in Neural Information Processing Systems (pp. 523-531),
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates how to extract objects-of-interest without relying on
hand-craft features and sliding windows approaches, that aims to jointly solve
two sub-tasks: (i) rapidly localizing salient objects from images, and (ii)
accurately segmenting the objects based on the localizations. We present a
general joint task learning framework, in which each task (either object
localization or object segmentation) is tackled via a multi-layer convolutional
neural network, and the two networks work collaboratively to boost performance.
In particular, we propose to incorporate latent variables bridging the two
networks in a joint optimization manner. The first network directly predicts
the positions and scales of salient objects from raw images, and the latent
variables adjust the object localizations to feed the second network that
produces pixelwise object masks. An EM-type method is presented for the
optimization, iterating with two steps: (i) by using the two networks, it
estimates the latent variables by employing an MCMC-based sampling method; (ii)
it optimizes the parameters of the two networks unitedly via back propagation,
with the fixed latent variables. Extensive experiments suggest that our
framework significantly outperforms other state-of-the-art approaches in both
accuracy and efficiency (e.g. 1000 times faster than competing approaches).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00744</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00744</id><created>2015-02-03</created><authors><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Huang</keyname><forenames>Lichao</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Incorporating Structural Alternatives and Sharing into Hierarchy for
  Multiclass Object Recognition and Detection</title><categories>cs.CV</categories><comments>8 pages, 6 figures, CVPR 2013</comments><msc-class>68U01</msc-class><journal-ref>Computer Vision and Pattern Recognition (CVPR), 2013 IEEE
  Conference on , vol., no., pp.3334,3341, 23-28 June 2013</journal-ref><doi>10.1109/CVPR.2013.428</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a reconfigurable model to recognize and detect multiclass
(or multiview) objects with large variation in appearance. Compared with well
acknowledged hierarchical models, we study two advanced capabilities in
hierarchy for object modeling: (i) &quot;switch&quot; variables(i.e. or-nodes) for
specifying alternative compositions, and (ii) making local classifiers (i.e.
leaf-nodes) shared among different classes. These capabilities enable us to
account well for structural variabilities while preserving the model compact.
Our model, in the form of an And-Or Graph, comprises four layers: a batch of
leaf-nodes with collaborative edges in bottom for localizing object parts; the
or-nodes over bottom to activate their children leaf-nodes; the and-nodes to
classify objects as a whole; one root-node on the top for switching multiclass
classification, which is also an or-node. For model training, we present an
EM-type algorithm, namely dynamical structural optimization (DSO), to
iteratively determine the structural configuration, (e.g., leaf-node generation
associated with their parent or-nodes and shared across other classes), along
with optimizing multi-layer parameters. The proposed method is valid on
challenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achieves
state-of-the-arts performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00749</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00749</id><created>2015-02-03</created><authors><author><keyname>Liu</keyname><forenames>Xionghao</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Cai</keyname><forenames>Zhaoquan</forenames></author><author><keyname>Lai</keyname><forenames>Jianhuang</forenames></author></authors><title>Data-Driven Scene Understanding with Adaptively Retrieved Exemplars</title><categories>cs.CV</categories><comments>8 pages, 5 figures</comments><msc-class>68U01</msc-class><journal-ref>MultiMedia, IEEE , vol.PP, no.99, pp.1,1, 2015</journal-ref><doi>10.1109/MMUL.2015.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates a data-driven approach for semantically scene
understanding, without pixelwise annotation and classifier training. Our
framework parses a target image with two steps: (i) retrieving its exemplars
(i.e. references) from an image database, where all images are unsegmented but
annotated with tags; (ii) recovering its pixel labels by propagating semantics
from the references. We present a novel framework making the two steps mutually
conditional and bootstrapped under the probabilistic Expectation-Maximization
(EM) formulation. In the first step, the references are selected by jointly
matching their appearances with the target as well as the semantics (i.e. the
assigned labels of the target and the references). We process the second step
via a combinatorial graphical representation, in which the vertices are
superpixels extracted from the target and its selected references. Then we
derive the potentials of assigning labels to one vertex of the target, which
depend upon the graph edges that connect the vertex to its spatial neighbors of
the target and to its similar vertices of the references. Besides, the proposed
framework can be naturally applied to perform image annotation on new test
images. In the experiments, we validate our approach on two public databases,
and demonstrate superior performances over the state-of-the-art methods in both
semantic segmentation and image annotation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00750</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00750</id><created>2015-02-03</created><authors><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Cao</keyname><forenames>Qingxing</forenames></author><author><keyname>Huang</keyname><forenames>Rui</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with
  Discriminatively Trained Spatio-Temporal Model</title><categories>cs.CV</categories><comments>5 pages, 1 figures</comments><msc-class>68U01</msc-class><journal-ref>Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium
  on , vol., no., pp.1184-1187, April 2014</journal-ref><doi>10.1109/ISBI.2014.6868087</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this study is to provide an automatic computational framework to
assist clinicians in diagnosing Focal Liver Lesions (FLLs) in
Contrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clip
as an ensemble of Region-of-Interests (ROIs), whose locations are modeled as
latent variables in a discriminative model. Different types of FLLs are
characterized by both spatial and temporal enhancement patterns of the ROIs.
The model is learned by iteratively inferring the optimal ROI locations and
optimizing the model parameters. To efficiently search the optimal spatial and
temporal locations of the ROIs, we propose a data-driven inference algorithm by
combining effective spatial and temporal pruning. The experiments show that our
method achieves promising results on the largest dataset in the literature (to
the best of our knowledge), which we have made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00756</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00756</id><created>2015-02-03</created><updated>2015-06-03</updated><authors><author><keyname>Chaudhry</keyname><forenames>Shonal</forenames></author><author><keyname>Chandra</keyname><forenames>Rohitash</forenames></author></authors><title>Design of a Mobile Face Recognition System for Visually Impaired Persons</title><categories>cs.CY cs.CV cs.HC</categories><comments>Added author names in sections 1 and 2. Certain details in sections 3
  and 4 are now clearer. Removed external camera from implementation, results
  unaffected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is estimated that 285 million people globally are visually impaired. A
majority of these people live in developing countries and are among the elderly
population. One of the most difficult tasks faced by the visually impaired is
identification of people. While naturally, voice recognition is a common method
of identification, it is an intuitive and difficult process. The rise of
computation capability of mobile devices gives motivation to develop
applications that can assist visually impaired persons. With the availability
of mobile devices, these people can be assisted by an additional method of
identification through intelligent software based on computer vision
techniques. In this paper, we present the design and implementation of a face
detection and recognition system for the visually impaired through the use of
mobile computing. This mobile system is assisted by a server-based support
system. The system was tested on a custom video database. Experiment results
show high face detection accuracy and promising face recognition accuracy in
suitable conditions. The challenges of the system lie in better recognition
techniques for difficult situations in terms of lighting and weather.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00762</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00762</id><created>2015-02-03</created><authors><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Cai</keyname><forenames>Kai</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Feng</keyname><forenames>Rongquan</forenames></author></authors><title>On the Solvability of 3s/nt Sum-Network---A Region Decomposition and
  Weak Decentralized Code Method</title><categories>cs.IT math.IT</categories><comments>41 pages. arXiv admin note: text overlap with arXiv:1401.3941</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study the network coding problem of sum-networks with 3 sources and n
terminals (3s/nt sum-network), for an arbitrary positive integer n, and derive
a sufficient and necessary condition for the solvability of a family of
so-called terminal-separable sum-network. Both the condition of
terminal-separable and the solvability of a terminal-separable sum-network can
be decided in polynomial time. Consequently, we give another necessary and
sufficient condition, which yields a faster (O(|E|) time) algorithm than that
of Shenvi and Dey ([18], (O(|E|^3) time), to determine the solvability of the
3s/3t sum-network. To obtain the results, we further develop the region
decomposition method in [22], [23] and generalize the decentralized coding
method in [21]. Our methods provide new efficient tools for multiple source
multiple sink network coding problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00765</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00765</id><created>2015-02-03</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Sampled-Data Stabilization of Nonlinear Delay Systems with a Compact
  Absorbing Set</title><categories>math.OC cs.SY</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a methodology for the global sampled-data stabilization of systems
with a compact absorbing set and input/measurement delays. The methodology is
based on the Inter-Sample-Predictor, Observer, Predictor, Delay-Free Controller
(ISP-O-P-DFC) scheme and the stabilization is robust to perturbations of the
sampling schedule. The obtained results are novel even for the delay-free case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00780</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00780</id><created>2015-02-03</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Li</keyname><forenames>Meizhu</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>Measure the similarity of nodes in the complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measure the similarity of the nodes in the complex networks have interested
many researchers to explore it. In this paper, a new method which is based on
the degree centrality and the Relative-entropy is proposed to measure the
similarity of the nodes in the complex networks. The results in this paper show
that, the nodes which have a common structure property always have a high
similarity to others nodes. The nodes which have a high influential to others
always have a small value of similarity to other nodes and the marginal nodes
also have a low similar to other nodes. The results in this paper show that the
proposed method is useful and reasonable to measure the similarity of the nodes
in the complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00781</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00781</id><created>2015-02-03</created><authors><author><keyname>Chen</keyname><forenames>Pengfei</forenames></author><author><keyname>Qi</keyname><forenames>Yong</forenames></author><author><keyname>Hou</keyname><forenames>Di</forenames></author></authors><title>CHAOS: Accurate and Realtime Detection of Aging-Oriented Failure Using
  Entropy</title><categories>cs.OH</categories><comments>15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even well-designed software systems suffer from chronic performance
degradation, also named &quot;software aging&quot;, due to internal (e.g. software bugs)
and external (e.g. resource exhaustion) impairments. These chronic problems
often fly under the radar of software monitoring systems before causing severe
impacts (e.g. system failure). Therefore it's a challenging issue how to timely
detect these problems to prevent system crash. Although a large quantity of
approaches have been proposed to solve this issue, the accuracy and
effectiveness of these approaches are still far from satisfactory due to the
insufficiency of aging indicators adopted by them. In this paper, we present a
novel entropy-based aging indicator, Multidimensional Multi-scale Entropy
(MMSE). MMSE employs the complexity embedded in runtime performance metrics to
indicate software aging and leverages multi-scale and multi-dimension
integration to tolerate system fluctuations. Via theoretical proof and
experimental evaluation, we demonstrate that MMSE satisfies Stability,
Monotonicity and Integration which we conjecture that an ideal aging indicator
should have. Based upon MMSE, we develop three failure detection approaches
encapsulated in a proof-of-concept named CHAOS. The experimental evaluations in
a Video on Demand (VoD) system and in a real-world production system,
AntVision, show that CHAOS can detect the failure-prone state in an
extraordinarily high accuracy and a near 0 Ahead-Time-To-Failure (ATTF).
Compared to previous approaches, CHAOS improves the detection accuracy by about
5 times and reduces the ATTF even by 3 orders of magnitude. In addition, CHAOS
is light-weight enough to satisfy the realtime requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00794</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00794</id><created>2015-02-03</created><authors><author><keyname>Nguyen</keyname><forenames>Van Minh</forenames></author><author><keyname>Baccelli</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Thomas</keyname><forenames>Laurent</forenames></author><author><keyname>Chen</keyname><forenames>Chung Shue</forenames></author></authors><title>Best Signal Quality in Cellular Networks: Asymptotic Properties and
  Applications to Mobility Management in Small Cell Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Published on EURASIP Journal on Wireless Communications and
  Networking, Special Issue on Femtocell Networks, 2010</comments><journal-ref>EURASIP Journal on Wireless Communications and Networking 2010,
  2010:690161</journal-ref><doi>10.1155/2010/690161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quickly increasing data traffic and the user demand for a full coverage
of mobile services anywhere and anytime are leading mobile networking into a
future of small cell networks. However, due to the high-density and randomness
of small cell networks, there are several technical challenges. In this paper,
we investigate two critical issues: \emph{best signal quality} and
\emph{mobility management}. Under the assumptions that base stations are
uniformly distributed in a ring shaped region and that shadowings are
lognormal, independent and identically distributed, we prove that when the
number of sites in the ring tends to infinity, then (i) the maximum signal
strength received at the center of the ring tends in distribution to a Gumbel
distribution when properly renormalized, and (ii) it is asymptotically
independent of the interference. Using these properties, we derive the
distribution of the best signal quality. Furthermore, an optimized random cell
scanning scheme is proposed, based on the evaluation of the optimal number of
sites to be scanned for maximizing the user data throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00797</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00797</id><created>2015-02-03</created><authors><author><keyname>Misra</keyname><forenames>Prasant</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Warrior</keyname><forenames>Jay</forenames></author></authors><title>Towards a Practical Architecture for the Next Generation Internet of
  Things</title><categories>cs.CY</categories><comments>arXiv admin note: text overlap with arXiv:1407.0434</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things, or the IoT is a vision for a ubiquitous society
wherein people and &quot;Things&quot; are connected in an immersively networked computing
environment, with the connected &quot;Things&quot; providing utility to
people/enterprises and their digital shadows, through intelligent social and
commercial services. Translating this idea to a conceivable reality is a work
in progress for more than a decade. Current IoT architectures are predicated on
optimistic assumptions on the evolution and deployment of IoT technologies. We
believe many of these assumptions will not be met, consequently impeding the
practical and sustainable deployment of IoT. In this article, we explore
use-cases across diff?erent applications domains that can potentially benefi?t
from an IoT infrastructure, and analyze them in the context of an alternative
world-view that is more grounded in reality. Despite this more conservative
approach, we argue that adopting certain design paradigms when architecting an
IoT ecosystem can achieve much of the promised benefi?ts in a practical and
sustainable manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00802</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00802</id><created>2015-02-03</created><authors><author><keyname>Semma</keyname><forenames>Amine</forenames></author><author><keyname>Elouafiq</keyname><forenames>Ismail</forenames></author></authors><title>Algorithm for Achieving Consensus Over Conflicting Rumors: Convergence
  Analysis and Applications</title><categories>cs.DC cs.SI math.PR</categories><comments>IEEE Student Paper Contest</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the large expansion in the study of social networks, this paper
deals with the problem of multiple messages spreading over the same network
using gossip algorithms. Given two messages distributed over some nodes of the
graph, we first investigate the final distribution of the messages given an
initial state. Then, an algorithm is presented to achieve consensus over one of
the messages. Finally, a game theoretical application and an analogy with
word-of-mouth marketing are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00803</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00803</id><created>2015-02-03</created><updated>2015-08-04</updated><authors><author><keyname>Zhang</keyname><forenames>Xiaojing</forenames></author><author><keyname>Grammatico</keyname><forenames>Sergio</forenames></author><author><keyname>Schildbach</keyname><forenames>Georg</forenames></author><author><keyname>Goulart</keyname><forenames>Paul</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>On the Sample Size of Random Convex Programs with Structured Dependence
  on the Uncertainty (Extended Version)</title><categories>math.OC cs.SY</categories><comments>Accepted for publication at Automatica</comments><journal-ref>Automatica, volume 60, pages 182-188, 2015</journal-ref><doi>10.1016/j.automatica.2015.07.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;scenario approach&quot; provides an intuitive method to address chance
constrained problems arising in control design for uncertain systems. It
addresses these problems by replacing the chance constraint with a finite
number of sampled constraints (scenarios). The sample size critically depends
on Helly's dimension, a quantity always upper bounded by the number of decision
variables. However, this standard bound can lead to computationally expensive
programs whose solutions are conservative in terms of cost and violation
probability. We derive improved bounds of Helly's dimension for problems where
the chance constraint has certain structural properties. The improved bounds
lower the number of scenarios required for these problems, leading both to
improved objective value and reduced computational complexity. Our results are
generally applicable to Randomized Model Predictive Control of chance
constrained linear systems with additive uncertainty and affine disturbance
feedback. The efficacy of the proposed bound is demonstrated on an inventory
management example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00804</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00804</id><created>2015-02-03</created><updated>2015-03-05</updated><authors><author><keyname>Cummins</keyname><forenames>Ronan</forenames></author><author><keyname>Paik</keyname><forenames>Jiaul Hoque</forenames></author><author><keyname>Lv</keyname><forenames>Yuanhua</forenames></author></authors><title>A Polya Urn Document Language Model for Improved Information Retrieval</title><categories>cs.IR</categories><comments>37 page journal submission (accepted for publication in TOIS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multinomial language model has been one of the most effective models of
retrieval for over a decade. However, the multinomial distribution does not
model one important linguistic phenomenon relating to term-dependency, that is
the tendency of a term to repeat itself within a document (i.e. word
burstiness). In this article, we model document generation as a random process
with reinforcement (a multivariate Polya process) and develop a Dirichlet
compound multinomial language model that captures word burstiness directly.
  We show that the new reinforced language model can be computed as efficiently
as current retrieval models, and with experiments on an extensive set of TREC
collections, we show that it significantly outperforms the state-of-the-art
language model for a number of standard effectiveness metrics. Experiments also
show that the tuning parameter in the proposed model is more robust than in the
multinomial language model. Furthermore, we develop a constraint for the
verbosity hypothesis and show that the proposed model adheres to the
constraint. Finally, we show that the new language model essentially introduces
a measure closely related to idf which gives theoretical justification for
combining the term and document event spaces in tf-idf type schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00814</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00814</id><created>2015-02-03</created><updated>2015-08-10</updated><authors><author><keyname>Kuznets</keyname><forenames>Roman</forenames></author><author><keyname>Lellmann</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Grafting Hypersequents onto Nested Sequents</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new Gentzen-style framework of grafted hypersequents that
combines the formalism of nested sequents with that of hypersequents. To
illustrate the potential of the framework, we present novel calculi for the
modal logics $\mathsf{K5}$ and $\mathsf{KD5}$, as well as for extensions of the
modal logics $\mathsf{K}$ and $\mathsf{KD}$ with the axiom for shift
reflexivity. The latter of these extensions is also known as $\mathsf{SDL}^+$
in the context of deontic logic. All our calculi enjoy syntactic cut
elimination and can be used in backwards proof search procedures of optimal
complexity. The tableaufication of the calculi for $\mathsf{K5}$ and
$\mathsf{KD5}$ yields simplified prefixed tableau calculi for these logic
reminiscent of the simplified tableau system for $\mathsf{S5}$, which might be
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00821</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00821</id><created>2015-02-03</created><authors><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Software that Learns from its Own Failures</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All non-trivial software systems suffer from unanticipated production
failures. However, those systems are passive with respect to failures and do
not take advantage of them in order to improve their future behavior: they
simply wait for them to happen and trigger hard-coded failure recovery
strategies. Instead, I propose a new paradigm in which software systems learn
from their own failures. By using an advanced monitoring system they have a
constant awareness of their own state and health. They are designed in order to
automatically explore alternative recovery strategies inferred from past
successful and failed executions. Their recovery capabilities are assessed by
self-injection of controlled failures; this process produces knowledge in
prevision of future unanticipated failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00823</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00823</id><created>2015-02-03</created><authors><author><keyname>Fhom</keyname><forenames>Hervais Simo</forenames></author></authors><title>Big Data: Opportunities and Privacy Challenges</title><categories>cs.CR</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in data collection and computational statistics coupled with
increases in computer processing power, along with the plunging costs of
storage are making technologies to effectively analyze large sets of
heterogeneous data ubiquitous. Applying such technologies (often referred to as
big data technologies) to an ever growing number and variety of internal and
external data sources, businesses and institutions can discover hidden
correlations between data items, and extract actionable insights needed for
innovation and economic growth. While on one hand big data technologies yield
great promises, on the other hand, they raise critical security, privacy, and
ethical issues, which if left unaddressed may become significant barriers to
the fulfillment of expected opportunities and long-term success of big data. In
this paper, we discuss the benefits of big data to individuals and society at
large, focusing on seven key use cases: Big data for business optimization and
customer analytics, big data and science, big data and health care, big data
and finance, big data and the emerging energy distribution systems, big/open
data as enablers of openness and efficiency in government, and big data
security. In addition to benefits and opportunities, we discuss the security,
privacy, and ethical issues at stake.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00827</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00827</id><created>2015-02-03</created><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>On the Duality of Additivity and Tensorization</title><categories>cs.IT math.IT</categories><comments>42 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function is said to be additive if, similar to mutual information, expands
by a factor of $n$, when evaluated on $n$ i.i.d. repetitions of a source or
channel. On the other hand, a function is said to satisfy the tensorization
property if it remains unchanged when evaluated on i.i.d. repetitions. Additive
rate regions are of fundamental importance in network information theory,
serving as capacity regions or upper bounds thereof. Tensorizing measures of
correlation have also found applications in distributed source and channel
coding problems as well as the distribution simulation problem. Prior to our
work only two measures of correlation, namely the hypercontractivity ribbon and
maximal correlation (and their derivatives), were known to have the
tensorization property. In this paper, we provide a general framework to obtain
a region with the tensorization property from any additive rate region. We
observe that hypercontractivity ribbon indeed comes from the dual of the rate
region of the Gray-Wyner source coding problem, and generalize it to the
multipartite case. Then we define other measures of correlation with similar
properties from other source coding problems. We also present some applications
of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00831</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00831</id><created>2015-02-03</created><updated>2015-02-04</updated><authors><author><keyname>Piedeleu</keyname><forenames>Robin</forenames></author><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>Open System Categorical Quantum Semantics in Natural Language Processing</title><categories>cs.CL cs.LO math.CT math.QA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Originally inspired by categorical quantum mechanics (Abramsky and Coecke,
LiCS'04), the categorical compositional distributional model of natural
language meaning of Coecke, Sadrzadeh and Clark provides a conceptually
motivated procedure to compute the meaning of a sentence, given its grammatical
structure within a Lambek pregroup and a vectorial representation of the
meaning of its parts. The predictions of this first model have outperformed
that of other models in mainstream empirical language processing tasks on large
scale data. Moreover, just like CQM allows for varying the model in which we
interpret quantum axioms, one can also vary the model in which we interpret
word meaning.
  In this paper we show that further developments in categorical quantum
mechanics are relevant to natural language processing too. Firstly, Selinger's
CPM-construction allows for explicitly taking into account lexical ambiguity
and distinguishing between the two inherently different notions of homonymy and
polysemy. In terms of the model in which we interpret word meaning, this means
a passage from the vector space model to density matrices. Despite this change
of model, standard empirical methods for comparing meanings can be easily
adopted, which we demonstrate by a small-scale experiment on real-world data.
This experiment moreover provides preliminary evidence of the validity of our
proposed new model for word meaning.
  Secondly, commutative classical structures as well as their non-commutative
counterparts that arise in the image of the CPM-construction allow for encoding
relative pronouns, verbs and adjectives, and finally, iteration of the
CPM-construction, something that has no counterpart in the quantum realm,
enables one to accommodate both entailment and ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00836</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00836</id><created>2015-02-03</created><authors><author><keyname>Sun</keyname><forenames>Xiaoxia</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Task-Driven Dictionary Learning for Hyperspectral Image Classification
  with Structured Sparsity Constraints</title><categories>cs.CV</categories><doi>10.1109/TGRS.2015.2399978</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation models a signal as a linear combination of a small
number of dictionary atoms. As a generative model, it requires the dictionary
to be highly redundant in order to ensure both a stable high sparsity level and
a low reconstruction error for the signal. However, in practice, this
requirement is usually impaired by the lack of labelled training samples.
Fortunately, previous research has shown that the requirement for a redundant
dictionary can be less rigorous if simultaneous sparse approximation is
employed, which can be carried out by enforcing various structured sparsity
constraints on the sparse codes of the neighboring pixels. In addition,
numerous works have shown that applying a variety of dictionary learning
methods for the sparse representation model can also improve the classification
performance. In this paper, we highlight the task-driven dictionary learning
algorithm, which is a general framework for the supervised dictionary learning
method. We propose to enforce structured sparsity priors on the task-driven
dictionary learning method in order to improve the performance of the
hyperspectral classification. Our approach is able to benefit from both the
advantages of the simultaneous sparse representation and those of the
supervised dictionary learning. We enforce two different structured sparsity
priors, the joint and Laplacian sparsity, on the task-driven dictionary
learning method and provide the details of the corresponding optimization
algorithms. Experiments on numerous popular hyperspectral images demonstrate
that the classification performance of our approach is superior to sparse
representation classifier with structured priors or the task-driven dictionary
learning method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00839</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00839</id><created>2015-02-03</created><authors><author><keyname>Correia</keyname><forenames>Luis</forenames></author><author><keyname>Manso</keyname><forenames>Antonio</forenames></author></authors><title>A multiset model of multi-species evolution to solve big deceptive
  problems</title><categories>cs.NE q-bio.PE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This chapter presents SMuGA, an integration of symbiogenesis with the
Multiset Genetic Algorithm (MuGA). The symbiogenetic approach used here is
based on the host-parasite model with the novelty of varying the length of
parasites along the evolutionary process. Additionally, it models
collaborations between multiple parasites and a single host. To improve
efficiency, we introduced proxy evaluation of parasites, which saves fitness
function calls and exponentially reduces the symbiotic collaborations produced.
Another novel feature consists of breaking the evolutionary cycle into two
phases: a symbiotic phase and a phase of independent evolution of both hosts
and parasites. SMuGA was tested in optimization of a variety of deceptive
functions, with results one order of magnitude better than state of the art
symbiotic algorithms. This allowed to optimize deceptive problems with large
sizes, and showed a linear scaling in the number of iterations to attain the
optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00842</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00842</id><created>2015-02-03</created><updated>2015-07-28</updated><authors><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Erasure codes with symbol locality and group decodability for
  distributed storage</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a new family of erasure codes, called group decodable code
(GDC), for distributed storage system. Given a set of design parameters
{\alpha; \beta; k; t}, where k is the number of information symbols, each
codeword of an (\alpha; \beta; k; t)-group decodable code is a t-tuple of
strings, called buckets, such that each bucket is a string of \beta symbols
that is a codeword of a [\beta; \alpha] MDS code (which is encoded from \alpha
information symbols). Such codes have the following two properties: (P1)
Locally Repairable: Each code symbol has locality (\alpha; \beta-\alpha + 1).
(P2) Group decodable: From each bucket we can decode \alpha information
symbols. We establish an upper bound of the minimum distance of (\alpha; \beta;
k; t)-group decodable code for any given set of {\alpha; \beta; k; t}; We also
prove that the bound is achievable when the coding field F has size |F| &gt; n-1
\choose k-1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00852</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00852</id><created>2015-02-03</created><authors><author><keyname>Sagonas</keyname><forenames>Christos</forenames></author><author><keyname>Panagakis</keyname><forenames>Yannis</forenames></author><author><keyname>Zafeiriou</keyname><forenames>Stefanos</forenames></author><author><keyname>Pantic</keyname><forenames>Maja</forenames></author></authors><title>Face frontalization for Alignment and Recognition</title><categories>cs.CV</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it was shown that excellent results can be achieved in both face
landmark localization and pose-invariant face recognition. These breakthroughs
are attributed to the efforts of the community to manually annotate facial
images in many different poses and to collect 3D faces data. In this paper, we
propose a novel method for joint face landmark localization and frontal face
reconstruction (pose correction) using a small set of frontal images only. By
observing that the frontal facial image is the one with the minimum rank from
all different poses we formulate an appropriate model which is able to jointly
recover the facial landmarks as well as the frontalized version of the face. To
this end, a suitable optimization problem, involving the minimization of the
nuclear norm and the matrix $\ell_1$ norm, is solved. The proposed method is
assessed in frontal face reconstruction (pose correction), face landmark
localization, and pose-invariant face recognition and verification by
conducting experiments on $6$ facial images databases. The experimental results
demonstrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00858</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00858</id><created>2015-02-03</created><updated>2015-03-17</updated><authors><author><keyname>Yatawatta</keyname><forenames>Sarod</forenames></author></authors><title>Distributed Radio Interferometric Calibration</title><categories>astro-ph.IM cs.DC</categories><comments>MNRAS Accepted 2015 March 13. Received 2015 January 28; in original
  form 2014 November 6, low resolution figures</comments><doi>10.1093/mnras/stv596</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing data volumes delivered by a new generation of radio
interferometers require computationally efficient and robust calibration
algorithms. In this paper, we propose distributed calibration as a way of
improving both computational cost as well as robustness in calibration. We
exploit the data parallelism across frequency that is inherent in radio
astronomical observations that are recorded as multiple channels at different
frequencies. Moreover, we also exploit the smoothness of the variation of
calibration parameters across frequency. Data parallelism enables us to
distribute the computing load across a network of compute agents. Smoothness in
frequency enables us reformulate calibration as a consensus optimization
problem. With this formulation, we enable flow of information between compute
agents calibrating data at different frequencies, without actually passing the
data, and thereby improving robustness. We present simulation results to show
the feasibility as well as the advantages of distributed calibration as opposed
to conventional calibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00859</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00859</id><created>2015-02-03</created><authors><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Wiechert</keyname><forenames>Veit</forenames></author></authors><title>An on-line competitive algorithm for coloring bipartite graphs without
  long induced paths</title><categories>cs.DS math.CO</categories><msc-class>05C85 (Primary), 05C15, 68R10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of an on-line competitive algorithm for coloring bipartite
graphs remains a tantalizing open problem. So far there are only partial
positive results for bipartite graphs with certain small forbidden graphs as
induced subgraphs. We propose a new on-line competitive coloring algorithm for
$P_9$-free bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00860</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00860</id><created>2015-02-03</created><authors><author><keyname>Wu</keyname><forenames>Liang</forenames></author><author><keyname>Ding</keyname><forenames>Yiming</forenames></author></authors><title>Wavelet-based Estimator for the Hurst Parameters of Fractional Brownian
  Sheet</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proposed a class of statistical estimators $\hat H =(\hat H_1, \ldots,
\hat H_d)$ for the Hurst parameters $H=(H_1, \ldots, H_d)$ of fractional
Brownian field via multi-dimensional wavelet analysis and least squares, which
are asymptotically normal. These estimators can be used to detect
self-similarity and long-range dependence in multi-dimensional signals, which
is important in texture classification and improvement of diffusion tensor
imaging (DTI) of nuclear magnetic resonance (NMR). Some fractional Brownian
sheets will be simulated and the simulated data are used to validate these
estimators. We find that when $H_i \geq 1/2$, the estimators are efficient, and
when $H_i &lt; 1/2$, there are some bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00868</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00868</id><created>2015-02-03</created><authors><author><keyname>Caviglione</keyname><forenames>Luca</forenames></author><author><keyname>Lalande</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author></authors><title>Analysis of Human Awareness of Security and Privacy Threats in Smart
  Environments</title><categories>cs.CY cs.CR</categories><comments>12 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart environments integrate Information and Communication Technologies (ICT)
into devices, vehicles, buildings and cities to offer an increased quality of
life, energy efficiency and economical sustainability. In this perspective, the
individual has a core role and so has networking, which enables such entities
to cooperate. However, the huge amount of sensitive data, social aspects and
the mixed set of protocols offer many opportunities to inject hazards,
exfiltrate information, mass profiling of citizens, or produce a new wave of
attacks. This work reviews the major risks arising from the usage of
ICT-techniques for smart environments, with emphasis on networking. Its main
contribution is to explain the role of different stakeholders for causing a
lack of security and to envision future threats by considering human aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00870</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00870</id><created>2015-02-03</created><authors><author><keyname>Borgohain</keyname><forenames>Tuhin</forenames></author><author><keyname>Borgohain</keyname><forenames>Amardeep</forenames></author><author><keyname>Kumar</keyname><forenames>Uday</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Authentication Systems in Internet of Things</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses the various authentication systems implemented for
enhanced security and private re-position of an individual's log-in
credentials. The first part of the paper describes the multi-factor
authentication (MFA) systems, which, though not applicable to the field of
Internet of Things, provides great security to a user's credentials. MFA is
followed by a brief description of the working mechanism of interaction of
third party clients with private resources over the OAuth protocol framework
and a study of the delegation based authentication system in IP-based IoT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00873</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00873</id><created>2015-02-03</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Liang</keyname><forenames>Ding</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>DeepID3: Face Recognition with Very Deep Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state-of-the-art of face recognition has been significantly advanced by
the emergence of deep learning. Very deep neural networks recently achieved
great success on general object recognition because of their superb learning
capacity. This motivates us to investigate their effectiveness on face
recognition. This paper proposes two very deep neural network architectures,
referred to as DeepID3, for face recognition. These two architectures are
rebuilt from stacked convolution and inception layers proposed in VGG net and
GoogLeNet to make them suitable to face recognition. Joint face
identification-verification supervisory signals are added to both intermediate
and final feature extraction layers during training. An ensemble of the
proposed two architectures achieves 99.53% LFW face verification accuracy and
96.0% LFW rank-1 face identification accuracy, respectively. A further
discussion of LFW face verification result is given in the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00890</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00890</id><created>2015-02-03</created><authors><author><keyname>Botbol</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Dickenstein</keyname><forenames>Alicia</forenames></author></authors><title>Implicitization of rational hypersurfaces via linear syzygies: a
  practical overview</title><categories>math.AG cs.CG math.AC</categories><comments>22 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We unveil in concrete terms the general machinery of the syzygy-based
algorithms for the implicitization of rational surfaces in terms of the
monomials in the polynomials defining the parametrization, following and
expanding our joint article with M. Dohm. These algebraic techniques, based on
the theory of approximation complexes due to J. Herzog, A, Simis and W.
Vasconcelos, were introduced for the implicitization problem by J.-P.
Jouanolou, L. Bus\'e, and M. Chardin. Their work was inspired by the practical
method of moving curves, proposed by T. Sederberg and F. Chen, translated into
the language of syzygies by D. Cox. Our aim is to express the theoretical
results and resulting algorithms into very concrete terms, avoiding the use of
the advanced homological commutative algebra tools which are needed for their
proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00894</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00894</id><created>2015-02-03</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Extended Unary Coding</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extended variants of the recently introduced spread unary coding are
described. These schemes, in which the length of the code word is fixed, allow
representation of approximately n^2 numbers for n bits, rather than the n
numbers of the standard unary coding. In the first of two proposed schemes the
spread increases, whereas in the second scheme the spread remains constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00910</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00910</id><created>2015-02-03</created><authors><author><keyname>Babar</keyname><forenames>Zunaira</forenames></author><author><keyname>Ng</keyname><forenames>Soon Xin</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>EXIT-Chart Aided Near-Capacity Quantum Turbo Code Design</title><categories>quant-ph cs.IT math.IT</categories><comments>10 pages</comments><journal-ref>IEEE Transactions on Vehicular Technology, vol.PP, no.99, pp.1,1
  (2014)</journal-ref><doi>10.1109/TVT.2014.2328638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of Quantum Turbo Codes (QTCs) typically relies on the analysis of
their distance spectra, followed by Monte-Carlo simulations. By contrast, in
this paper we appropriately adapt the conventional non-binary EXtrinsic
Information Transfer (EXIT) charts for quantum turbo codes by exploiting the
intrinsic quantum-to-classical isomorphism. The EXIT chart analysis not only
allows us to dispense with the time-consuming Monte-Carlo simulations, but also
facilitates the design of near-capacity codes without resorting to the analysis
of their distance spectra. We have demonstrated that our EXIT chart predictions
are in line with the Monte-Carlo simulations results. We have also optimized
the entanglement-assisted QTC using EXIT charts, which outperforms the existing
distance spectra based QTCs. More explicitly, the performance of our optimized
QTC is as close as 0.3 dB to the corresponding hashing bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00911</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00911</id><created>2015-02-03</created><authors><author><keyname>de Verdi&#xe8;re</keyname><forenames>&#xc9;ric Colin</forenames></author></authors><title>Multicuts in Planar and Bounded-Genus Graphs with Bounded Number of
  Terminals</title><categories>cs.DS cs.CG</categories><acm-class>F.2.2; G.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected, edge-weighted graph G together with pairs of vertices,
called pairs of terminals, the minimum multicut problem asks for a
minimum-weight set of edges such that, after deleting these edges, the two
terminals of each pair belong to different connected components of the graph.
Relying on topological techniques, we provide a polynomial-time algorithm for
this problem in the case where G is embedded on a fixed surface of genus g
(e.g., when G is planar) and has a fixed number t of terminals. The running
time is a polynomial of degree O(sqrt{g^2+gt}) in the input size.
  In the planar case, our result corrects an error in an extended abstract by
Bentz [Int. Workshop on Parameterized and Exact Computation, 109-119, 2012].
The minimum multicut problem is also a generalization of the multiway cut
problem, a.k.a. multiterminal cut problem; even for this special case, no
dedicated algorithm was known for graphs embedded on surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00926</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00926</id><created>2015-02-03</created><authors><author><keyname>Siami</keyname><forenames>Milad</forenames></author><author><keyname>Motee</keyname><forenames>Nader</forenames></author></authors><title>Scaling Laws for Disturbance Propagation in Cyclic Dynamical Networks</title><categories>cs.SY math.DS math.OC</categories><comments>14 pages; submitted for possible journal publication. arXiv admin
  note: text overlap with arXiv:1403.1494</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to analyze performance of stable linear dynamical networks
subject to external stochastic disturbances. The square of the $\mathcal
H_2$-norm of the network is used as a performance measure to quantify the
expected steady-state dispersion of the outputs of the network. We show that
this performance measure can be tightly bounded from below and above by some
spectral functions of the state-space matrices of the network. This result is
applied to a class of cyclic linear networks and shown that their performance
measure scale quadratically with the network size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00944</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00944</id><created>2015-02-03</created><updated>2015-02-23</updated><authors><author><keyname>D'Osualdo</keyname><forenames>Emanuele</forenames></author><author><keyname>Ong</keyname><forenames>Luke</forenames></author></authors><title>A Type System for proving Depth Boundedness in the pi-calculus</title><categories>cs.LO cs.PL</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The depth-bounded fragment of the pi-calculus is an expressive class of
systems enjoying decidability of some important verification problems.
Unfortunately membership of the fragment is undecidable. We propose a novel
type system, parameterised over a finite forest, that formalises name usage by
pi-terms in a manner that respects the forest. Type checking is decidable and
type inference is computable; furthermore typable pi-terms are guaranteed to be
depth bounded.
  The second contribution of the paper is a proof of equivalence between the
semantics of typable terms and nested data class memory automata, a class of
automata over data words. We believe this connection can help to establish new
links between the rich theory of infinite-alphabet automata and nominal
calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00946</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00946</id><created>2015-02-03</created><authors><author><keyname>Chepushtanova</keyname><forenames>Sofya</forenames></author><author><keyname>Kirby</keyname><forenames>Michael</forenames></author></authors><title>Classification of Hyperspectral Imagery on Embedded Grassmannians</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach for capturing the signal variability in hyperspectral
imagery using the framework of the Grassmann manifold. Labeled points from each
class are sampled and used to form abstract points on the Grassmannian. The
resulting points on the Grassmannian have representations as orthonormal
matrices and as such do not reside in Euclidean space in the usual sense. There
are a variety of metrics which allow us to determine a distance matrices that
can be used to realize the Grassmannian as an embedding in Euclidean space. We
illustrate that we can achieve an approximately isometric embedding of the
Grassmann manifold using the chordal metric while this is not the case with
geodesic distances. However, non-isometric embeddings generated by using a
pseudometric on the Grassmannian lead to the best classification results. We
observe that as the dimension of the Grassmannian grows, the accuracy of the
classification grows to 100% on two illustrative examples. We also observe a
decrease in classification rates if the dimension of the points on the
Grassmannian is too large for the dimension of the Euclidean space. We use
sparse support vector machines to perform additional model reduction. The
resulting classifier selects a subset of dimensions of the embedding without
loss in classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00950</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00950</id><created>2015-02-03</created><authors><author><keyname>Lira</keyname><forenames>M. M. S.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Carvalho</keyname><forenames>M. A.</forenames><suffix>Jr</suffix></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Compactly Supported Wavelets Derived From Legendre Polynomials:
  Spherical Harmonic Wavelets</title><categories>cs.NA math.NA stat.ME</categories><comments>6 pages, 6 figures, 1 table In: Computational Methods in Circuits and
  Systems Applications, WSEAS press, pp.211-215, 2003. ISBN: 960-8052-88-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new family of wavelets is introduced, which is associated with Legendre
polynomials. These wavelets, termed spherical harmonic or Legendre wavelets,
possess compact support. The method for the wavelet construction is derived
from the association of ordinary second order differential equations with
multiresolution filters. The low-pass filter associated with Legendre
multiresolution analysis is a linear phase finite impulse response filter
(FIR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00956</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00956</id><created>2015-02-03</created><updated>2015-09-18</updated><authors><author><keyname>Mur-Artal</keyname><forenames>Raul</forenames></author><author><keyname>Montiel</keyname><forenames>J. M. M.</forenames></author><author><keyname>Tardos</keyname><forenames>Juan D.</forenames></author></authors><title>ORB-SLAM: a Versatile and Accurate Monocular SLAM System</title><categories>cs.RO cs.CV</categories><comments>17 pages. 13 figures. IEEE Transactions on Robotics, 2015. Project
  webpage (videos, code): http://webdiis.unizar.es/~raulmur/orbslam/</comments><doi>10.1109/TRO.2015.2463671</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents ORB-SLAM, a feature-based monocular SLAM system that
operates in real time, in small and large, indoor and outdoor environments. The
system is robust to severe motion clutter, allows wide baseline loop closing
and relocalization, and includes full automatic initialization. Building on
excellent algorithms of recent years, we designed from scratch a novel system
that uses the same features for all SLAM tasks: tracking, mapping,
relocalization, and loop closing. A survival of the fittest strategy that
selects the points and keyframes of the reconstruction leads to excellent
robustness and generates a compact and trackable map that only grows if the
scene content changes, allowing lifelong operation. We present an exhaustive
evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves
unprecedented performance with respect to other state-of-the-art monocular SLAM
approaches. For the benefit of the community, we make the source code public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00963</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00963</id><created>2015-02-03</created><updated>2015-11-26</updated><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>The Sample Complexity of Revenue Maximization</title><categories>cs.GT</categories><comments>Updated version of previous version which corrects some mistakes;
  this was itself an updated version of the STOC 2014 paper, which extended the
  result to all regular distributions</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the design and analysis of revenue-maximizing auctions, auction
performance is typically measured with respect to a prior distribution over
inputs. The most obvious source for such a distribution is past data. The goal
is to understand how much data is necessary and sufficient to guarantee
near-optimal expected revenue.
  Our basic model is a single-item auction in which bidders' valuations are
drawn independently from unknown and non-identical distributions. The seller is
given $m$ samples from each of these distributions &quot;for free&quot; and chooses an
auction to run on a fresh sample. How large does m need to be, as a function of
the number k of bidders and eps &gt; 0, so that a (1 - eps)-approximation of the
optimal revenue is achievable?
  We prove that, under standard tail conditions on the underlying
distributions, m = poly(k, 1/eps) samples are necessary and sufficient. Our
lower bound stands in contrast to many recent results on simple and
prior-independent auctions and fundamentally involves the interplay between
bidder competition, non-identical distributions, and a very close (but still
constant) approximation of the optimal revenue. It effectively shows that the
only way to achieve a sufficiently good constant approximation of the optimal
revenue is through a detailed understanding of bidders' valuation
distributions. Our upper bound is constructive and applies in particular to a
variant of the empirical Myerson auction, the natural auction that runs the
revenue-maximizing auction with respect to the empirical distributions of the
samples.
  Our sample complexity lower bound depends on the set of allowable
distributions, and to capture this we introduce alpha-strongly regular
distributions, which interpolate between the well-studied classes of regular
(alpha = 0) and MHR (alpha = 1) distributions. We give evidence that this
definition is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00974</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00974</id><created>2015-02-03</created><updated>2015-02-26</updated><authors><author><keyname>Ord&#xf3;&#xf1;ez-Hurtado</keyname><forenames>Rodrigo H.</forenames></author><author><keyname>Crisostomi</keyname><forenames>Emanuele</forenames></author><author><keyname>Griggs</keyname><forenames>Wynita M.</forenames></author><author><keyname>Shorten</keyname><forenames>Robert N.</forenames></author></authors><title>An Assessment on the Use of Stationary Vehicles as a Support to
  Cooperative Positioning</title><categories>math.OC cs.RO</categories><comments>This version of the paper is an updated version of the initial
  submission, where some initial comments of reviewers have been taken into
  account</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the use of stationary vehicles as tools to enhance
the localisation capabilities of moving vehicles in a VANET. We examine the
idea in terms of its potential benefits, technical requirements, algorithmic
design and experimental evaluation. Simulation results are given to illustrate
the efficacy of the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00978</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00978</id><created>2015-02-03</created><authors><author><keyname>Bokov</keyname><forenames>Grigoriy V.</forenames></author></authors><title>Undecidable problems for propositional calculi with implication</title><categories>math.LO cs.LO</categories><comments>18 pages. arXiv admin note: text overlap with arXiv:1407.7010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we deal with propositional calculi over a signature
containing the classical implication $\to$ with the rules of modus ponens and
substitution. For these calculi we consider few recognizing problems such as
recognizing derivations, extensions, completeness, and axiomatizations. The
main result of this paper is to prove that the problem of recognizing
extensions is undecidable for every propositional calculus, and the problems of
recognizing axiomatizations and completeness are undecidable for propositional
calculi containing the formula $x \to ( y \to x )$. As a corollary, the problem
of derivability of a fixed formula $A$ is also undecidable for all $A$.
Moreover, we give a historical survey of related results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00979</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00979</id><created>2015-01-31</created><authors><author><keyname>Sun</keyname><forenames>Fengyou</forenames></author><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author><author><keyname>Li</keyname><forenames>Luqun</forenames></author></authors><title>Further Properties of Wireless Channel Capacity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless communication calls for exploration of more efficient use of
wireless channel capacity to meet the increasing demand on higher data rate and
less latency. However, while ergodic capacity and instantaneous capacity are
fundamental properties of a wireless channel, they are in many cases not
sufficient for use in assessing if data transmission over the channel meets the
quality of service (QoS) requirements. To address this limitation, we focus on
the concept of &quot;cumulative capacity&quot;, which is the cumulated capacity over a
time period, and study its properties. Specifically, the moment generating
function, the stochastic service curve, and the Mellin transform properties of
cumulative capacity are investigated and related results are derived. It is
appealing that with these properties, QoS assessment of data transmission over
the channel can be further performed based on stochastic network calculus, a
newly developed theory for (stochastic) QoS analysis. To demonstrate the
derived properties, a Rayleigh fading channel is considered and numerical
results are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00993</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00993</id><created>2015-02-03</created><updated>2015-09-30</updated><authors><author><keyname>Viard</keyname><forenames>Jordan</forenames><affiliation>LIP6</affiliation></author><author><keyname>Latapy</keyname><forenames>Matthieu</forenames><affiliation>LIP6</affiliation></author><author><keyname>Magnien</keyname><forenames>Cl&#xe9;mence</forenames><affiliation>LIP6</affiliation></author></authors><title>Computing maximal cliques in link streams</title><categories>cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A link stream is a collection of triplets $(t, u, v)$ indicating that an
interaction occurred between u and v at time t. We generalize the classical
notion of cliques in graphs to such link streams: for a given $\Delta$, a
$\Delta$-clique is a set of nodes and a time interval such that all pairs of
nodes in this set interact at least once during each sub-interval of duration
$\Delta$. We propose an algorithm to enumerate all maximal (in terms of nodes
or time interval) cliques of a link stream, and illustrate its practical
relevance on a real-world contact trace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00996</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00996</id><created>2015-02-03</created><updated>2015-02-10</updated><authors><author><keyname>Thomas</keyname><forenames>Brian</forenames></author><author><keyname>Jenness</keyname><forenames>Tim</forenames></author><author><keyname>Economou</keyname><forenames>Frossie</forenames></author><author><keyname>Greenfield</keyname><forenames>Perry</forenames></author><author><keyname>Hirst</keyname><forenames>Paul</forenames></author><author><keyname>Berry</keyname><forenames>David S.</forenames></author><author><keyname>Bray</keyname><forenames>Erik</forenames></author><author><keyname>Gray</keyname><forenames>Norman</forenames></author><author><keyname>Muna</keyname><forenames>Demitri</forenames></author><author><keyname>Turner</keyname><forenames>James</forenames></author><author><keyname>de Val-Borro</keyname><forenames>Miguel</forenames></author><author><keyname>Santander-Vela</keyname><forenames>Juande</forenames></author><author><keyname>Shupe</keyname><forenames>David</forenames></author><author><keyname>Good</keyname><forenames>John</forenames></author><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>Kitaeff</keyname><forenames>Slava</forenames></author><author><keyname>Fay</keyname><forenames>Jonathan</forenames></author><author><keyname>Laurino</keyname><forenames>Omar</forenames></author><author><keyname>Alexov</keyname><forenames>Anastasia</forenames></author><author><keyname>Landry</keyname><forenames>Walter</forenames></author><author><keyname>Masters</keyname><forenames>Joe</forenames></author><author><keyname>Brazier</keyname><forenames>Adam</forenames></author><author><keyname>Schaaf</keyname><forenames>Reinhold</forenames></author><author><keyname>Edwards</keyname><forenames>Kevin</forenames></author><author><keyname>Redman</keyname><forenames>Russell O.</forenames></author><author><keyname>Marsh</keyname><forenames>Thomas R.</forenames></author><author><keyname>Streicher</keyname><forenames>Ole</forenames></author><author><keyname>Norris</keyname><forenames>Pat</forenames></author><author><keyname>Pascual</keyname><forenames>Sergio</forenames></author><author><keyname>Davie</keyname><forenames>Matthew</forenames></author><author><keyname>Droettboom</keyname><forenames>Michael</forenames></author><author><keyname>Robitaille</keyname><forenames>Thomas</forenames></author><author><keyname>Campana</keyname><forenames>Riccardo</forenames></author><author><keyname>Hagen</keyname><forenames>Alex</forenames></author><author><keyname>Hartogh</keyname><forenames>Paul</forenames></author><author><keyname>Klaes</keyname><forenames>Dominik</forenames></author><author><keyname>Craig</keyname><forenames>Matthew W.</forenames></author><author><keyname>Homeier</keyname><forenames>Derek</forenames></author></authors><title>Learning from FITS: Limitations in use in modern astronomical research</title><categories>astro-ph.IM cs.SE</categories><doi>10.1016/j.ascom.2015.01.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Flexible Image Transport System (FITS) standard has been a great boon to
astronomy, allowing observatories, scientists and the public to exchange
astronomical information easily. The FITS standard, however, is showing its
age. Developed in the late 1970s, the FITS authors made a number of
implementation choices that, while common at the time, are now seen to limit
its utility with modern data. The authors of the FITS standard could not
anticipate the challenges which we are facing today in astronomical computing.
Difficulties we now face include, but are not limited to, addressing the need
to handle an expanded range of specialized data product types (data models),
being more conducive to the networked exchange and storage of data, handling
very large datasets, and capturing significantly more complex metadata and data
relationships.
  There are members of the community today who find some or all of these
limitations unworkable, and have decided to move ahead with storing data in
other formats. If this fragmentation continues, we risk abandoning the
advantages of broad interoperability, and ready archivability, that the FITS
format provides for astronomy. In this paper we detail some selected important
problems which exist within the FITS standard today. These problems may provide
insight into deeper underlying issues which reside in the format and we provide
a discussion of some lessons learned. It is not our intention here to prescribe
specific remedies to these issues; rather, it is to call attention of the FITS
and greater astronomical computing communities to these problems in the hope
that it will spur action to address them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.00997</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.00997</id><created>2015-02-03</created><authors><author><keyname>Rakhshan</keyname><forenames>Ali</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author><author><keyname>Nekoui</keyname><forenames>Mohammad</forenames></author></authors><title>Driver-Based Adaptation of Vehicular Ad Hoc Networks for Design of
  Active Safety Systems</title><categories>cs.NI cs.IT cs.SY math.IT</categories><comments>3 pages Summary, CISS 2015, JOHNS HOPKINS University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the need for individualizing vehicular communications in
order to improve collision warning systems for an N-lane highway scenario. By
relating the traffic-based and communications studies, we aim at reducing
highway traffic accidents. To the best of our knowledge, this is the first
paper that shows how to customize vehicular communications to driver's
characteristics and traffic information. We propose to develop VANET protocols
that selectively identify crash relevant information and customize the
communications of that information based on each driver's assigned safety
score. In this paper, first, we derive the packet success probability by
accounting for multi-user interference, path loss, and fading. Then, by Monte
carlo simulations, we demonstrate how appropriate channel access probabilities
that satisfy the delay requirements of the safety application result in
noticeable performance enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01032</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01032</id><created>2015-02-03</created><authors><author><keyname>Vu</keyname><forenames>Tiep H.</forenames></author><author><keyname>Mousavi</keyname><forenames>Hojjat S.</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Rao</keyname><forenames>UK Arvind</forenames></author><author><keyname>Rao</keyname><forenames>Ganesh</forenames></author></authors><title>DFDL: Discriminative Feature-oriented Dictionary Learning for
  Histopathological Image Classification</title><categories>cs.CV</categories><comments>Accepted to IEEE International Symposium on Biomedical Imaging
  (ISBI), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In histopathological image analysis, feature extraction for classification is
a challenging task due to the diversity of histology features suitable for each
problem as well as presence of rich geometrical structure. In this paper, we
propose an automatic feature discovery framework for extracting discriminative
class-specific features and present a low-complexity method for classification
and disease grading in histopathology. Essentially, our Discriminative
Feature-oriented Dictionary Learning (DFDL) method learns class-specific
features which are suitable for representing samples from the same class while
are poorly capable of representing samples from other classes. Experiments on
three challenging real-world image databases: 1) histopathological images of
intraductal breast lesions, 2) mammalian lung images provided by the Animal
Diagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor
images from The Cancer Genome Atlas (TCGA) database, show the significance of
DFDL model in a variety problems over state-of-the-art methods
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01038</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01038</id><created>2015-02-01</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>A Factorization Scheme for Some Discrete Hartley Transform Matrices</title><categories>cs.NA math.NA stat.ME</categories><comments>10 pages, 4 figures, 2 tables, International Conference on System
  Engineering, Communications and Information Technologies, 2001, Punta Arenas.
  ICSECIT 2001 Proceedings. Punta Arenas: Universidad de Magallanes, 2001</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete transforms such as the discrete Fourier transform (DFT) and the
discrete Hartley transform (DHT) are important tools in numerical analysis. The
successful application of transform techniques relies on the existence of
efficient fast transforms. In this paper some fast algorithms are derived. The
theoretical lower bound on the multiplicative complexity for the DFT/DHT are
achieved. The approach is based on the factorization of DHT matrices.
Algorithms for short blocklengths such as $N \in \{3, 5, 6, 12, 24 \}$ are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01053</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01053</id><created>2015-02-03</created><updated>2015-10-28</updated><authors><author><keyname>Zhu</keyname><forenames>Shengyu</forenames></author><author><keyname>Chen</keyname><forenames>Biao</forenames></author></authors><title>Quantized Consensus by the ADMM: Probabilistic versus Deterministic
  Quantizers</title><categories>cs.SY</categories><comments>Revised manuscript for IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops efficient algorithms for distributed average consensus
with quantized communication using the alternating direction method of
multipliers (ADMM). We first study the effects of probabilistic and
deterministic quantizations on a distributed ADMM algorithm. With probabilistic
quantization, this algorithm yields linear convergence to the desired average
in the mean sense with a bounded variance. When deterministic quantization is
employed, the distributed ADMM converges to a consensus within
$3+\lceil\log_{1+\delta} \Omega\rceil$ iterations where $\delta&gt;0$ depends on
the network topology and $\Omega$ is a polynomial of the quantization
resolution, the agents' data, and the network topology. A tight upper bound on
the consensus error is also obtained, which depends only on the quantization
resolution and the average degree of the graph. This bound is much preferred in
large scale networks over existing algorithms whose consensus errors are
increasing in the range of agents' data, the quantization resolution, and the
number of agents. We finally propose our algorithm which combines both
probabilistic and deterministic quantizations. Simulations show that the
consensus error of our algorithm is typically less than one quantization
resolution for all connected networks where agents' data can be of arbitrary
magnitudes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01057</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01057</id><created>2015-02-03</created><authors><author><keyname>Zhou</keyname><forenames>Li</forenames></author></authors><title>Personalized Web Search</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalization is important for search engines to improve user experience.
Most of the existing work do pure feature engineering and extract a lot of
session-style features and then train a ranking model. Here we proposed a novel
way to model both long term and short term user behavior using Multi-armed
bandit algorithm. Our algorithm can generalize session information across users
well, and as an Explore-Exploit style algorithm, it can generalize to new urls
and new users well. Experiments show that our algorithm can improve performance
over the default ranking and outperforms several popular Multi-armed bandit
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01063</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01063</id><created>2015-02-03</created><updated>2015-04-02</updated><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>K&#xfc;nnemann</keyname><forenames>Marvin</forenames></author></authors><title>Quadratic Conditional Lower Bounds for String Problems and Dynamic Time
  Warping</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classic similarity measures of strings are longest common subsequence and
Levenshtein distance (i.e., the classic edit distance). A classic similarity
measure of curves is dynamic time warping. These measures can be computed by
simple $O(n^2)$ dynamic programming algorithms, and despite much effort no
algorithms with significantly better running time are known.
  We prove that, even restricted to binary strings or one-dimensional curves,
respectively, these measures do not have strongly subquadratic time algorithms,
i.e., no algorithms with running time $O(n^{2-\varepsilon})$ for any
$\varepsilon &gt; 0$, unless the Strong Exponential Time Hypothesis fails. We
generalize the result to edit distance for arbitrary fixed costs of the four
operations (deletion in one of the two strings, matching, substitution), by
identifying trivial cases that can be solved in constant time, and proving
quadratic-time hardness on binary strings for all other cost choices. This
improves and generalizes the known hardness result for Levenshtein distance
[Backurs, Indyk STOC'15] by the restriction to binary strings and the
generalization to arbitrary costs, and adds important problems to a recent line
of research showing conditional lower bounds for a growing number of quadratic
time problems.
  As our main technical contribution, we introduce a framework for proving
quadratic-time hardness of similarity measures. To apply the framework it
suffices to construct a single gadget, which encapsulates all the expressive
power necessary to emulate a reduction from satisfiability.
  Finally, we prove quadratic-time hardness for longest palindromic subsequence
and longest tandem subsequence via reductions from longest common subsequence,
showing that conditional lower bounds based on the Strong Exponential Time
Hypothesis also apply to string problems that are not necessarily similarity
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01065</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01065</id><created>2015-02-03</created><authors><author><keyname>Xu</keyname><forenames>S.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Distributed Compressed Estimation for Wireless Sensor Networks Based on
  Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures; IEEE Signal Processing Letters, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a novel distributed compressed estimation scheme for
sparse signals and systems based on compressive sensing techniques. The
proposed scheme consists of compression and decompression modules inspired by
compressive sensing to perform distributed compressed estimation. A design
procedure is also presented and an algorithm is developed to optimize
measurement matrices, which can further improve the performance of the proposed
distributed compressed estimation scheme. Simulations for a wireless sensor
network illustrate the advantages of the proposed scheme and algorithm in terms
of convergence rate and mean square error performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01066</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01066</id><created>2015-02-03</created><authors><author><keyname>Gostar</keyname><forenames>Amirali K.</forenames></author><author><keyname>Hoseinnezhad</keyname><forenames>Reza</forenames></author><author><keyname>Bab-Hadiashar</keyname><forenames>Alireza</forenames></author></authors><title>Information theoretic approach to robust multi-Bernoulli sensor control</title><categories>cs.IT cs.NA cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel sensor control solution is presented, formulated within a
Multi-Bernoulli-based multi-target tracking framework. The proposed method is
especially designed for the general multi-target tracking case, where no prior
knowledge of the clutter distribution or the probability of detection profile
are available. In an information theoretic approach, our method makes use of
R\`{e}nyi divergence as the reward function to be maximized for finding the
optimal sensor control command at each step. We devise a Monte Carlo sampling
method for computation of the reward. Simulation results demonstrate successful
performance of the proposed method in a challenging scenario involving five
targets maneuvering in a relatively uncertain space with unknown
distance-dependent clutter rate and probability of detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01070</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01070</id><created>2015-02-03</created><updated>2015-02-12</updated><authors><author><keyname>Shi</keyname><forenames>Zhan</forenames></author><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author></authors><title>Optimization of distributed EPR entanglement generated between two
  Gaussian fields by the modified steepest descent method</title><categories>quant-ph cs.SY</categories><comments>17 pages, 4 figures. To appear in Proceedings of the 2015 American
  Control Conference (ACC), http://acc2015.a2c2.org. A more recent review paper
  on continuous-variable quantum information added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent theoretical investigations on quantum coherent feedback networks have
found that with the same pump power, the Einstein-Podolski-Rosen (EPR)-like
entanglement generated via a dual nondegenerate optical parametric amplifier
(NOPA) system placed in a certain coherent feedback loop is stronger than the
EPR-like entangled pairs produced by a single NOPA. In this paper, we present a
linear quantum system consisting of two NOPAs and a static linear passive
network of optical devices. The network has six inputs and six outputs, among
which four outputs and four inputs are connected in a coherent feedback loop
with the two NOPAs. This passive network is represented by a $6 \times 6$
complex unitary matrix. A modified steepest descent method is used to find a
passive complex unitary matrix at which the entanglement of this dual-NOPA
network is locally maximized. Here we choose the matrix corresponding to a
dual-NOPA coherent feedback network from our previous work as a starting point
for the modified steepest descent algorithm. By decomposing the unitary matrix
obtained by the algorithm as the product of so-called two-level unitary
matrices, we find an optimized configuration in which the complex matrix is
realized by a static optical network made of beam splitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01074</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01074</id><created>2015-02-03</created><updated>2015-05-28</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Consensus with Linear Objective Maps</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A consensus system is a linear multi-agent system in which agents communicate
to reach a so-called consensus state, defined as the average of the initial
states of the agents. Consider a more generalized situation in which each agent
is given a positive weight and the consensus state is defined as the weighted
average of the initial conditions. We characterize in this paper the weighted
averages that can be evaluated in a decentralized way by agents communicating
over a directed graph. Specifically, we introduce a linear function, called the
objective map, that defines the desired final state as a function of the
initial states of the agents. We then provide a complete answer to the question
of whether there is a decentralized consensus dynamics over a given digraph
which converges to the final state specified by an objective map. In
particular, we characterize not only the set of objective maps that are
feasible for a given digraph, but also the consensus dynamics that implements
the objective map. In addition, we present a decentralized algorithm to design
the consensus dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01075</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01075</id><created>2015-02-03</created><updated>2015-04-23</updated><authors><author><keyname>Dzhafarov</keyname><forenames>Damir D.</forenames></author><author><keyname>Dzhafarov</keyname><forenames>Ehtibar N.</forenames></author></authors><title>Classificatory Sorites, Probabilistic Supervenience, and Rule-Making</title><categories>cs.AI math.LO</categories><msc-class>03A05, 03B42, 03B48</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We view sorites in terms of stimuli acting upon a system and evoking this
system's responses. Supervenience of responses on stimuli implies that they
either lack tolerance (i.e., they change in every vicinity of some of the
stimuli), or stimuli are not always connectable by finite chains of stimuli in
which successive members are `very similar'. If supervenience does not hold,
the properties of tolerance and connectedness cannot be formulated and
therefore soritical sequences cannot be constructed. We hypothesize that
supervenience in empirical systems (such as people answering questions) is
fundamentally probabilistic. The supervenience of probabilities of responses on
stimuli is stable, in the sense that `higher-order' probability distributions
can always be reduced to `ordinary' ones. In making rules about which stimuli
ought to correspond to which responses, the main characterization of choices in
soritical situations is their arbitrariness. We argue that arbitrariness poses
no problems for classical logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01094</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01094</id><created>2015-02-04</created><updated>2015-10-27</updated><authors><author><keyname>Bahrampour</keyname><forenames>Soheil</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Ray</keyname><forenames>Asok</forenames></author><author><keyname>Jenkins</keyname><forenames>W. Kenneth</forenames></author></authors><title>Multimodal Task-Driven Dictionary Learning for Image Classification</title><categories>stat.ML cs.CV cs.LG</categories><comments>To appear at IEEE Transactions on Image Processing</comments><doi>10.1109/TIP.2015.2496275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary learning algorithms have been successfully used for both
reconstructive and discriminative tasks, where an input signal is represented
with a sparse linear combination of dictionary atoms. While these methods are
mostly developed for single-modality scenarios, recent studies have
demonstrated the advantages of feature-level fusion based on the joint sparse
representation of the multimodal inputs. In this paper, we propose a multimodal
task-driven dictionary learning algorithm under the joint sparsity constraint
(prior) to enforce collaborations among multiple homogeneous/heterogeneous
sources of information. In this task-driven formulation, the multimodal
dictionaries are learned simultaneously with their corresponding classifiers.
The resulting multimodal dictionaries can generate discriminative latent
features (sparse codes) from the data that are optimized for a given task such
as binary or multiclass classification. Moreover, we present an extension of
the proposed formulation using a mixed joint and independent sparsity prior
which facilitates more flexible fusion of the modalities at feature level. The
efficacy of the proposed algorithms for multimodal classification is
illustrated on four different applications -- multimodal face recognition,
multi-view face recognition, multi-view action recognition, and multimodal
biometric recognition. It is also shown that, compared to the counterpart
reconstructive-based dictionary learning algorithms, the task-driven
formulations are more computationally efficient in the sense that they can be
equipped with more compact dictionaries and still achieve superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01095</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01095</id><created>2015-02-04</created><authors><author><keyname>Nirmala</keyname><forenames>A. P.</forenames></author><author><keyname>Sridaran</keyname><forenames>Dr. R.</forenames></author></authors><title>A Novel architecture for improving performance under virtualized
  environments</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though virtualization provides a lot of advantages in cloud computing,
it does not provide effective performance isolation between the virtualization
machines. In other words, the performance may get affected due the
interferences caused by co-virtual machines. This can be achieved by the proper
management of resource allocations between the Virtual Machines running
simultaneously. This paper aims at providing a proposed novel architecture that
is based on Fast Genetic K-means++ algorithm and test results show positive
improvements in terms of performance improvements over a similar existing
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01097</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01097</id><created>2015-02-04</created><updated>2015-07-31</updated><authors><author><keyname>Hu</keyname><forenames>Jingwen</forenames></author><author><keyname>Xia</keyname><forenames>Gui-Song</forenames></author><author><keyname>Hu</keyname><forenames>Fan</forenames></author><author><keyname>Zhang</keyname><forenames>Liangpei</forenames></author></authors><title>Dense v.s. Sparse: A Comparative Study of Sampling Analysis in Scene
  Classification of High-Resolution Remote Sensing Imagery</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to the submission
  requirement of a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene classification is a key problem in the interpretation of
high-resolution remote sensing imagery. Many state-of-the-art methods, e.g.
bag-of-visual-words model and its variants, the topic models as well as deep
learning-based approaches, share similar procedures: patch sampling, feature
description/learning and classification. Patch sampling is the first and a key
procedure which has a great influence on the results. In the literature, many
different sampling strategies have been used, {e.g. dense sampling, random
sampling, keypoint-based sampling and saliency-based sampling, etc. However, it
is still not clear which sampling strategy is suitable for the scene
classification of high-resolution remote sensing images. In this paper, we
comparatively study the effects of different sampling strategies under the
scenario of scene classification of high-resolution remote sensing images. We
divide the existing sampling methods into two types: dense sampling and sparse
sampling, the later of which includes random sampling, keypoint-based sampling
and various saliency-based sampling proposed recently. In order to compare
their performances, we rely on a standard bag-of-visual-words model to
construct our testing scheme, owing to their simplicity, robustness and
efficiency. The experimental results on two commonly used datasets show that
dense sampling has the best performance among all the strategies but with high
spatial and computational complexity, random sampling gives better or
comparable results than other sparse sampling methods, like the sophisticated
multi-scale key-point operators and the saliency-based methods which are
intensively studied and commonly used recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01119</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01119</id><created>2015-02-04</created><authors><author><keyname>Hansbo</keyname><forenames>Peter</forenames></author><author><keyname>Salomonsson</keyname><forenames>Kent</forenames></author></authors><title>A discontinuous Galerkin method for cohesive zone modelling</title><categories>cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a discontinuous finite element method for small strain elasticity
allowing for cohesive zone modeling. The method yields a seamless transition
between the discontinuous Galerkin method and classical cohesive zone modeling.
Some relevant numerical examples are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01120</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01120</id><created>2015-02-04</created><authors><author><keyname>Abd-Elrahman</keyname><forenames>Emad</forenames></author><author><keyname>Rekik</keyname><forenames>Tarek</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author></authors><title>Optimization of Quality of Experience through File Duplication in Video
  Sharing Servers</title><categories>cs.NI cs.MM</categories><comments>6 pages</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consumers of short videos on Internet can have a bad Quality of Experience
QoE due to the long distance between the consumers and the servers that hosting
the videos. We propose an optimization of the file allocation in
telecommunication operators content sharing servers to improve the QoE through
files duplication, thus bringing the files closer to the consumers. This
optimization allows the network operator to set the level of QoE and to have
control over the users access cost by setting a number of parameters. Two
optimization methods are given and are followed by a comparison of their
efficiency. Also, the hosting costs versus the gain of optimization are
analytically discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01122</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01122</id><created>2015-02-04</created><authors><author><keyname>Abd-Elrahman</keyname><forenames>Emad</forenames></author><author><keyname>Boutabia</keyname><forenames>Mohamed</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author></authors><title>Hash Chain Links Resynchronization Methods in Video Streaming Security
  Performance Comparison</title><categories>cs.CR cs.MM cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hash chains provide a secure and light way of security to data authentication
including two aspects: Data Integrity and Data Origin Authentication. The real
challenge of using the hash chains is how it could recover the synchronization
state and continue keeping the hash link in case of packet loss? Based on the
packet loss tolerance and some accepted delay of video delivery which are
representing the permitted tolerance for heavy loaded applications, we propose
different mechanisms for such synchronization recovery. Each mechanism is
suitable to use according to the video use case and the low capabilities of end
devices. This paper proposes comparative results between them based on the
status of each one and its overhead. Then, we propose a hybrid technique based
Redundancy Code (RC). This hybrid algorithm is simulated and compared
analytically against the other techniques (SHHC, TSP, MLHC and TSS). Moreover,
a global performance evaluation in terms of delay and overhead is conducted for
all techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01133</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01133</id><created>2015-02-04</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Jang</keyname><forenames>Yunsik</forenames></author></authors><title>Practical and Legal Challenges of Cloud Investigations</title><categories>cs.CY</categories><comments>7 pages</comments><acm-class>K.4.1; K.4.2</acm-class><journal-ref>The Journal of The Institute of Internet, Broadcasting and
  Communication, 14(6), 33-39, 2014</journal-ref><doi>10.7236/JIIBC.2014.14.6.33</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An area presenting new opportunities for both legitimate business, as well as
criminal organizations, is Cloud computing. This work gives a strong background
in current digital forensic science, as well as a basic understanding of the
goal of Law Enforcement when conducting digital forensic investigations. These
concepts are then applied to digital forensic investigation of cloud
environments in both theory and practice, and supplemented with current
literature on the subject. Finally, legal challenges with digital forensic
investigations in cloud environments are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01134</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01134</id><created>2015-02-04</created><authors><author><keyname>Pappas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Jeon</keyname><forenames>Jeongho</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author><author><keyname>Traganitis</keyname><forenames>Apostolos</forenames></author></authors><title>Effect of Energy Harvesting on Stable Throughput in Cooperative Relay
  Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the impact of energy constraints on a two-hop network with a
source, a relay and a destination under random medium access is studied. A
collision channel with erasures is considered, and the source and the relay
nodes have energy harvesting capabilities and an unlimited battery to store the
harvested energy. Additionally, the source and the relay node have external
traffic arrivals and the relay forwards a fraction of the source node's traffic
to the destination; the cooperation is performed at the network level. An inner
and an outer bound of the stability region for a given transmission probability
vector are obtained. Then, the closure of the inner and the outer bound is
obtained separately and they turn out to be identical. This work is not only a
step in connecting information theory and networking, by studying the maximum
stable throughput region metric but also it taps the relatively unexplored and
important domain of energy harvesting and assesses the effect of that on this
important measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01139</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01139</id><created>2015-02-04</created><authors><author><keyname>Fasino</keyname><forenames>Dario</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Generalized modularity matrices</title><categories>math.SP cs.SI math.NA</categories><msc-class>05C50, 15A18, 15B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various modularity matrices appeared in the recent literature on network
analysis and algebraic graph theory. Their purpose is to allow writing as
quadratic forms certain combinatorial functions appearing in the framework of
graph clustering problems. In this paper we put in evidence certain common
traits of various modularity matrices and shed light on their spectral
properties that are at the basis of various theoretical results and practical
spectral-type algorithms for community detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01142</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01142</id><created>2015-02-04</created><authors><author><keyname>Nemoto</keyname><forenames>Keiichi</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Garcia</keyname><forenames>Cristobal J.</forenames></author><author><keyname>Gluesing</keyname><forenames>Julia</forenames></author><author><keyname>Iba</keyname><forenames>Takashi</forenames></author><author><keyname>Lassenius</keyname><forenames>Casper</forenames></author><author><keyname>Miller</keyname><forenames>Christine</forenames></author><author><keyname>Paasivaara</keyname><forenames>Maria</forenames></author><author><keyname>Riopelle</keyname><forenames>Ken</forenames></author></authors><title>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015</title><categories>cs.SI</categories><comments>34 papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5th annual international conference on Collaborative Innovation Networks
Conference (COINS) takes place at Keio University from March 12 to 14, 2015.
COINS15 brings together practitioners, researchers and students of the emerging
science of collaboration to share their work, learn from each other, and get
inspired through creative new ideas.
  Where science, design, business and art meet, COINS15 looks at the emerging
forces behind the phenomena of open-source, creative, entrepreneurial and
social movements. Through interactive workshops, professional presentations,
and fascinating keynotes, COINS15 combines a wide range of interdisciplinary
fields such as social network analysis, group dynamics, design and
visualization, information systems, collective action and the psychology and
sociality of collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01157</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01157</id><created>2015-02-04</created><authors><author><keyname>Wiecek</keyname><forenames>Piotr</forenames></author><author><keyname>Haddad</keyname><forenames>Majed</forenames></author><author><keyname>Habachi</keyname><forenames>Oussama</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author></authors><title>Toward Fully Coordinated Multi-level Multi-carrier Energy Efficient
  Networks</title><categories>cs.NI cs.GT cs.IT math.IT</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enabling coordination between products from different vendors is a key
characteristic of the design philosophy behind future wireless communication
networks. As an example, different devices may have different implementations,
leading to different user experiences. A similar story emerges when devices
running different physical and link layer protocols share frequencies in the
same spectrum in order to maximize the system-wide spectral efficiency. In such
situations, coordinating multiple interfering devices presents a significant
challenge not only from an interworking perspective (as a result of reduced
infrastructure), but also from an implementation point of view. The following
question may then naturally arise: How to accommodate integrating such
heterogeneous wireless devices seamlessly? One approach is to coordinate the
spectrum in a centralized manner. However, the desired autonomous feature of
future wireless systems makes the use of a central authority for spectrum
management less appealing. Alternately, intelligent spectrum coordination have
spurred great interest and excitement in the recent years. This paper presents
a multi-level (hierarchical) power control game where users jointly choose
their channel control and power control selfishly in order to maximize their
individual energy efficiency. By hierarchical, we mean that some users'
decision priority is higher/lower than the others. We propose two simple and
nearly-optimal algorithms that ensure complete spectrum coordination among
users. Interestingly, it turns out that the complexity of the two proposed
algorithms is, in the worst case, quadratic in the number of users, whereas the
complexity of the optimal solution (obtained through exhaustive search) is N!.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01176</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01176</id><created>2015-02-04</created><authors><author><keyname>Fetaya</keyname><forenames>Ethan</forenames></author><author><keyname>Ullman</keyname><forenames>Shimon</forenames></author></authors><title>Learning Local Invariant Mahalanobis Distances</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many tasks and data types, there are natural transformations to which the
data should be invariant or insensitive. For instance, in visual recognition,
natural images should be insensitive to rotation and translation. This
requirement and its implications have been important in many machine learning
applications, and tolerance for image transformations was primarily achieved by
using robust feature vectors. In this paper we propose a novel and
computationally efficient way to learn a local Mahalanobis metric per datum,
and show how we can learn a local invariant metric to any transformation in
order to improve performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01181</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01181</id><created>2015-02-04</created><updated>2015-11-25</updated><authors><author><keyname>Mineraud</keyname><forenames>Julien</forenames></author><author><keyname>Mazhelis</keyname><forenames>Oleksiy</forenames></author><author><keyname>Su</keyname><forenames>Xiang</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author></authors><title>A gap analysis of Internet-of-Things platforms</title><categories>cs.CY</categories><comments>14 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are experiencing an abundance of Internet-of-Things (IoT) middleware
solutions that provide connectivity for sensors and actuators to the Internet.
To gain a widespread adoption, these middleware solutions, referred to as
platforms, have to meet the expectations of different players in the IoT
ecosystem, including device providers, application developers, and end-users,
among others. In this article, we evaluate a representative sample of these
platforms, both proprietary and open-source, on the basis of their ability to
meet the expectations of the IoT. The evaluation is completed by a gap analysis
of the current IoT landscape with respect to (i) the support of heterogeneous
hardware, (ii) the capabilities of the platform for data management, (iii) the
support of application developers, (iv) the extensibility of the different
platforms for the formation of ecosystems, as well as (v) the availability of
dedicated marketplaces to the IoT. The gap analysis aims to highlight the
deficiencies of today's solutions to improve their integration to tomorrow's
ecosystem. In order to strengthen the finding of our analysis, we conducted a
survey amongst the partners of the Finnish Internet of Things program, counting
over 350 experts, to evaluate the most critical issues for the development of
future IoT platforms. Based on the results of our analysis and our survey, we
conclude this article with a list of recommendations for extending these IoT
platforms in order to fill in the gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01187</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01187</id><created>2015-02-04</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Kamalika</forenames></author><author><keyname>Das</keyname><forenames>Sukanta</forenames></author></authors><title>Reversibility of d-State Finite Cellular Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates reversibility properties of 1-dimensional
3-neighborhood d-state finite cellular automata (CAs) under periodic boundary
condition. A tool named reachability tree has been developed from de Bruijn
graph which represents all possible reachable configurations of an n-cell CA.
This tool has been used to test reversibility of CAs. We have identified a
large set of reversible CAs using this tool by following some greedy
strategies. Our conjecture is that the reversible CAs, defined over infinite
lattice, are always reversible when the CAs are finite. However, the reverse
may not be true.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01188</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01188</id><created>2015-02-04</created><updated>2015-07-01</updated><authors><author><keyname>Nielsen</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Madue&#xf1;o</keyname><forenames>Germ&#xe1;n C.</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>S&#xf8;rensen</keyname><forenames>Ren&#xe9; B.</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>&#x10c;edomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>What Can Wireless Cellular Technologies Do about the Upcoming Smart
  Metering Traffic?</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted; change: corrected location of eSM box in Fig. 1; May 22,
  2015: Major revision after review; v4: revised, accepted for publication</comments><doi>10.1109/MCOM.2015.7263371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of smart electricity meters with cellular radio interface
puts an additional load on the wireless cellular networks. Currently, these
meters are designed for low duty cycle billing and occasional system check,
which generates a low-rate sporadic traffic. As the number of distributed
energy resources increases, the household power will become more variable and
thus unpredictable from the viewpoint of the Distribution System Operator
(DSO). It is therefore expected, in the near future, to have an increased
number of Wide Area Measurement System (WAMS) devices with Phasor Measurement
Unit (PMU)-like capabilities in the distribution grid, thus allowing the
utilities to monitor the low voltage grid quality while providing information
required for tighter grid control. From a communication standpoint, the traffic
profile will change drastically towards higher data volumes and higher rates
per device. In this paper, we characterize the current traffic generated by
smart electricity meters and supplement it with the potential traffic
requirements brought by introducing enhanced Smart Meters, i.e., meters with
PMU-like capabilities. Our study shows how GSM/GPRS and LTE cellular system
performance behaves with the current and next generation smart meters traffic,
where it is clearly seen that the PMU data will seriously challenge these
wireless systems. We conclude by highlighting the possible solutions for
upgrading the cellular standards, in order to cope with the upcoming smart
metering traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01199</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01199</id><created>2015-02-04</created><updated>2015-08-26</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>A Multiple-Expert Binarization Framework for Multispectral Images</title><categories>cs.CV</categories><comments>12 pages, 8 figures, 6 tables. Presented at ICDAR'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a multiple-expert binarization framework for multispectral
images is proposed. The framework is based on a constrained subspace selection
limited to the spectral bands combined with state-of-the-art gray-level
binarization methods. The framework uses a binarization wrapper to enhance the
performance of the gray-level binarization. Nonlinear preprocessing of the
individual spectral bands is used to enhance the textual information. An
evolutionary optimizer is considered to obtain the optimal and some suboptimal
3-band subspaces from which an ensemble of experts is then formed. The
framework is applied to a ground truth multispectral dataset with promising
results. In addition, a generalization to the cross-validation approach is
developed that not only evaluates generalizability of the framework, it also
provides a practical instance of the selected experts that could be then
applied to unseen inputs despite the small size of the given ground truth
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01220</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01220</id><created>2015-02-04</created><authors><author><keyname>Lahlou</keyname><forenames>Tarek A.</forenames></author><author><keyname>Oppenheim</keyname><forenames>Alan V.</forenames></author></authors><title>Unveiling The Tree: A Convex Framework for Sparse Problems</title><categories>cs.DS</categories><doi>10.1109/ICASSP.2015.7178687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general framework for generating greedy algorithms for
solving convex constraint satisfaction problems for sparse solutions by mapping
the satisfaction problem into one of graph traversal on a rooted tree of
unknown topology. For every pre-walk of the tree an initial set of generally
dense feasible solutions is processed in such a way that the sparsity of each
solution increases with each generation unveiled. The specific computation
performed at any particular child node is shown to correspond to an embedding
of a polytope into the polytope received from that nodes parent. Several issues
related to pre-walk order selection, computational complexity and tractability,
and the use of heuristic and/or side information is discussed. An example of a
single-path, depth-first algorithm on a tree with randomized vertex reduction
and a run-time path selection algorithm is presented in the context of sparse
lowpass filter design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01222</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01222</id><created>2015-02-04</created><authors><author><keyname>Hu</keyname><forenames>Xueheng</forenames></author><author><keyname>Song</keyname><forenames>Lixing</forenames></author><author><keyname>Van Bruggen</keyname><forenames>Dirk</forenames></author><author><keyname>Striegel</keyname><forenames>Aaron</forenames></author></authors><title>Is There WiFi Yet? How Aggressive WiFi Probe Requests Deteriorate Energy
  and Throughput</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiFi offloading has emerged as a key component of cellular operator strategy
to meet the data needs of rich, mobile devices. As such, mobile devices tend to
aggressively seek out WiFi in order to provide improved user Quality of
Experience (QoE) and cellular capacity relief. For home and work environments,
aggressive WiFi scans can significantly improve the speed on which mobile nodes
join the WiFi network. Unfortunately, the same aggressive behavior that excels
in the home environment incurs considerable side effects across crowded
wireless environments. In this paper, we show through empirical studies at both
large (stadium) and small (laboratory) scales how aggressive WiFi scans can
have significant implications for energy and throughput, both for the mobile
nodes scanning and other nearby mobile nodes. We close with several thoughts on
the disjoint incentives for properly balancing WiFi discovery speed and
ultra-dense network interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01227</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01227</id><created>2015-02-04</created><authors><author><keyname>Meng</keyname><forenames>Xuesong</forenames></author><author><keyname>Sewell</keyname><forenames>Phillip</forenames></author><author><keyname>Phang</keyname><forenames>Sendy</forenames></author><author><keyname>Vukovic</keyname><forenames>Ana</forenames></author><author><keyname>Benson</keyname><forenames>Trevor M.</forenames></author></authors><title>Modeling Curved Carbon Fiber Composite (CFC) Structures in the
  Transmission-Line Modeling (TLM) Method</title><categories>cs.CE</categories><comments>8 pages, 12 figures, accepted for publication in IEEE Transactions on
  EMC</comments><journal-ref>Electromagnetic Compatibility, IEEE Transactions on (Volume:PP ,
  Issue: 99 ), 2015</journal-ref><doi>10.1109/TEMC.2015.2400055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new embedded model for curved thin panels is developed in the Transmission
Line Modeling (TLM) method. In this model, curved panels are first linearized
and then embedded between adjacent 2D TLM nodes allowing for arbitrary
positioning between adjacent node centers. The embedded model eliminates the
necessity for fine discretization thus reducing the run time and memory
requirements for the calculation. The accuracy and convergence of the model are
verified by comparing the resonant frequencies of an elliptical cylinder formed
using carbon fiber composite (CFC) materials with those of the equivalent metal
cylinder. Furthermore, the model is used to analyze the shielding performance
of CFC airfoil NACA2415.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01228</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01228</id><created>2015-02-04</created><updated>2015-12-28</updated><authors><author><keyname>Meshry</keyname><forenames>Moustafa</forenames></author><author><keyname>Hussein</keyname><forenames>Mohamed E.</forenames></author><author><keyname>Torki</keyname><forenames>Marwan</forenames></author></authors><title>Linear-time Online Action Detection From 3D Skeletal Data Using Bags of
  Gesturelets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sliding window is one direct way to extend a successful recognition system to
handle the more challenging detection problem. While action recognition decides
only whether or not an action is present in a pre-segmented video sequence,
action detection identifies the time interval where the action occurred in an
unsegmented video stream. Sliding window approaches for action detection can
however be slow as they maximize a classifier score over all possible
sub-intervals. Even though new schemes utilize dynamic programming to speed up
the search for the optimal sub-interval, they require offline processing on the
whole video sequence. In this paper, we propose a novel approach for online
action detection based on 3D skeleton sequences extracted from depth data. It
identifies the sub-interval with the maximum classifier score in linear time.
Furthermore, it is invariant to temporal scale variations and is suitable for
real-time applications with low latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01233</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01233</id><created>2015-02-04</created><authors><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Adegbola</keyname><forenames>Omotanwa</forenames></author><author><keyname>Adelakin</keyname><forenames>Barakat</forenames></author><author><keyname>Adelakun</keyname><forenames>Adeyemi</forenames></author><author><keyname>Emuoyibofarhe</keyname><forenames>Justice</forenames></author></authors><title>Exploiting Multimodal Biometrics in E-Privacy Scheme for Electronic
  Health Records</title><categories>cs.CR cs.CY</categories><comments>Published</comments><journal-ref>JBAH 2014, Vol , No 18, pp 22 - 33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing approaches to protect the privacy of Electronic Health Records are
either insufficient for existing medical laws or they are too restrictive in
their usage. For example, smart card-based encryption systems require the
patient to be always present to authorize access to medical records.
Questionnaires were administered by 50 medical practitioners to identify and
categorize different Electronic Health Records attributes. The system was
implemented using multi biometrics of patients to access patient record in
pre-hospital care.The software development tools employed were JAVA and MySQL
database. The system provides applicable security when patients records are
shared either with other practitioners, employers, organizations or research
institutes. The result of the system evaluation shows that the average response
time of 6 seconds and 11.1 seconds for fingerprint and iris respectively after
ten different simulations. The system protects privacy and confidentiality by
limiting the amount of data exposed to users.The system also enables emergency
medical technicians to gain easy and reliable access to necessary attributes of
patients Electronic Health Records while still maintaining the privacy and
confidentiality of the data using the patients fingerprint and iris.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01237</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01237</id><created>2015-02-04</created><authors><author><keyname>Strauch</keyname><forenames>Tobias</forenames></author></authors><title>Running Identical Threads in C-Slow Retiming based Designs for
  Functional Failure Detection</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows the usage of C-Slow Retiming (CSR) in safety critical and
low power applications. CSR generates C copies of a design by reusing the given
logic resources in a time sliced fashion. When all C design copies are
stimulated with the same input values, then all C design copies should behave
the same way and will therefore create a redundant system. The paper shows that
this special method of using CSR offers great benefits when used in safety
critical and low power applications. Additional optimization techniques towards
reducing register count are shown and an on-the-fly recovery mechanism is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01240</identifier>
 <datestamp>2015-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01240</id><created>2015-02-04</created><authors><author><keyname>Abraham</keyname><forenames>Subil</forenames></author><author><keyname>Nair</keyname><forenames>Suku</forenames></author></authors><title>A Predictive Framework for Cyber Security Analytics using Attack Graphs</title><categories>cs.CR</categories><comments>17 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1501.01901</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) January 2015. ISSN:0974-9322; 0975-2293</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security metrics serve as a powerful tool for organizations to understand the
effectiveness of protecting computer networks. However majority of these
measurement techniques don't adequately help corporations to make informed risk
management decisions. In this paper we present a stochastic security framework
for obtaining quantitative measures of security by taking into account the
dynamic attributes associated with vulnerabilities that can change over time.
Our model is novel as existing research in attack graph analysis do not
consider the temporal aspects associated with the vulnerabilities, such as the
availability of exploits and patches which can affect the overall network
security based on how the vulnerabilities are interconnected and leveraged to
compromise the system. In order to have a more realistic representation of how
the security state of the network would vary over time, a nonhomogeneous model
is developed which incorporates a time dependent covariate, namely the
vulnerability age. The daily transition-probability matrices are estimated
using Frei's Vulnerability Lifecycle model. We also leverage the trusted CVSS
metric domain to analyze how the total exploitability and impact measures
evolve over a time period for a given network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01241</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01241</id><created>2015-02-04</created><updated>2015-02-05</updated><authors><author><keyname>Farzmahdi</keyname><forenames>Amirhossein</forenames></author><author><keyname>Rajaei</keyname><forenames>Karim</forenames></author><author><keyname>Ghodrati</keyname><forenames>Masoud</forenames></author><author><keyname>Ebrahimpour</keyname><forenames>Reza</forenames></author><author><keyname>Khaligh-Razavi</keyname><forenames>Seyed-Mahdi</forenames></author></authors><title>A specialized face-processing network consistent with the
  representational geometry of monkey face patches</title><categories>q-bio.NC cs.CV</categories><comments>41 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ample evidence suggests that face processing in human and non-human primates
is performed differently compared with other objects. Converging reports, both
physiologically and psychophysically, indicate that faces are processed in
specialized neural networks in the brain -i.e. face patches in monkeys and the
fusiform face area (FFA) in humans. We are all expert face-processing agents,
and able to identify very subtle differences within the category of faces,
despite substantial visual and featural similarities. Identification is
performed rapidly and accurately after viewing a whole face, while
significantly drops if some of the face configurations (e.g. inversion,
misalignment) are manipulated or if partial views of faces are shown due to
occlusion. This refers to a hotly-debated, yet highly-supported concept, known
as holistic face processing. We built a hierarchical computational model of
face-processing based on evidence from recent neuronal and behavioural studies
on faces processing in primates. Representational geometries of the last three
layers of the model have characteristics similar to those observed in monkey
face patches (posterior, middle and anterior patches). Furthermore, several
face-processing-related phenomena reported in the literature automatically
emerge as properties of this model. The representations are evolved through
several computational layers, using biologically plausible learning rules. The
model satisfies face inversion effect, composite face effect, other race
effect, view and identity selectivity, and canonical face views. To our
knowledge, no models have so far been proposed with this performance and
agreement with biological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01245</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01245</id><created>2015-02-04</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Authorship recognition via fluctuation analysis of network topology and
  word intermittency</title><categories>cs.CL</categories><journal-ref>J. Stat. Mech. (2015) P03005</journal-ref><doi>10.1088/1742-5468/2015/03/P03005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical methods have been widely employed in many practical natural
language processing applications. More specifically, complex networks concepts
and methods from dynamical systems theory have been successfully applied to
recognize stylistic patterns in written texts. Despite the large amount of
studies devoted to represent texts with physical models, only a few studies
have assessed the relevance of attributes derived from the analysis of
stylistic fluctuations. Because fluctuations represent a pivotal factor for
characterizing a myriad of real systems, this study focused on the analysis of
the properties of stylistic fluctuations in texts via topological analysis of
complex networks and intermittency measurements. The results showed that
different authors display distinct fluctuation patterns. In particular, it was
found that it is possible to identify the authorship of books using the
intermittency of specific words. Taken together, the results described here
suggest that the patterns found in stylistic fluctuations could be used to
analyze other related complex systems. Furthermore, the discovery of novel
patterns related to textual stylistic fluctuations indicates that these
patterns could be useful to improve the state of the art of many
stylistic-based natural language processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01253</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01253</id><created>2015-02-04</created><authors><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Prices Matter for the Parameterized Complexity of Shift Bribery</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Shift Bribery problem, we are given an election (based on preference
orders), a preferred candidate $p$, and a budget. The goal is to ensure that
$p$ wins by shifting $p$ higher in some voters' preference orders. However,
each such shift request comes at a price (depending on the voter and on the
extent of the shift) and we must not exceed the given budget. We study the
parameterized computational complexity of Shift Bribery with respect to a
number of parameters (pertaining to the nature of the solution sought and the
size of the election) and several classes of price functions. When we
parameterize Shift Bribery by the number of affected voters, then for each of
our voting rules (Borda, Maximin, Copeland) the problem is W[2]-hard. If,
instead, we parameterize by the number of positions by which $p$ is shifted in
total,then the problem is fixed-parameter tractable for Borda and Maximin,and
is W[1]-hard for Copeland. If we parameterize by the budget, then the results
depend on the price function class. We also show that Shift Bribery tends to be
tractable when parameterized by the number of voters, but that the results for
the number of candidates are more enigmatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01255</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01255</id><created>2015-02-04</created><updated>2015-05-04</updated><authors><author><keyname>Arvind</keyname><forenames>V.</forenames></author><author><keyname>K&#xf6;bler</keyname><forenames>Johannes</forenames></author><author><keyname>Rattan</keyname><forenames>Gaurav</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>Graph Isomorphism, Color Refinement, and Compactness</title><categories>cs.CC cs.DM math.CO</categories><comments>30 pages; Lemma 10 is now corrected (see Theorem 9 in the new
  version); P-hardness proofs for the classes Discrete, Amenable, Compact,
  Tinhofer, and Refinable are included; a graph separating the classes Tinhofer
  and Refinable is now included, we had left this open in the previous versions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color refinement is a classical technique used to show that two given graphs
G and H are non-isomorphic; it is very efficient, although it does not succeed
on all graphs. We call a graph G amenable to color refinement if it succeeds in
distinguishing G from any non-isomorphic graph H. Tinhofer (1991) explored a
linear programming approach to Graph Isomorphism and defined compact graphs: A
graph is compact if its fractional automorphisms polytope is integral. Tinhofer
noted that isomorphism testing for compact graphs can be done quite efficiently
by linear programming. However, the problem of characterizing and recognizing
compact graphs in polynomial time remains an open question.
  Our results are summarized below:
  - We show that amenable graphs are recognizable in time O((n + m)logn), where
n and m denote the number of vertices and the number of edges in the input
graph.
  - We show that all amenable graphs are compact.
  - We study related combinatorial and algebraic graph properties introduced by
Tinhofer and Godsil. The corresponding classes of graphs form a hierarchy and
we prove that recognizing each of these graph classes is P-hard. In particular,
this gives a first complexity lower bound for recognizing compact graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01257</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01257</id><created>2015-02-04</created><updated>2015-07-02</updated><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Towards a Complexity-through-Realisability Theory</title><categories>cs.LO cs.CC math.LO math.OA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explain how recent developments in the fields of realisability models for
linear logic -- or geometry of interaction -- and implicit computational
complexity can lead to a new approach of implicit computational complexity.
This semantic-based approach should apply uniformly to various computational
paradigms, and enable the use of new mathematical methods and tools to attack
problem in computational complexity. This paper provides the background,
motivations and perspectives of this complexity-through-realisability theory to
be developed, and illustrates it with recent results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01264</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01264</id><created>2015-02-04</created><authors><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Adegbola</keyname><forenames>Omotanwa</forenames></author><author><keyname>Mikail</keyname><forenames>Olaniyi Olayemi</forenames></author><author><keyname>Emuoyibofarhe</keyname><forenames>Justice</forenames></author></authors><title>A Secure Electronic Prescription System Using Steganography with
  Encryption Key Implementation</title><categories>cs.CR cs.CY cs.MM</categories><comments>Published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years health care has seen major improvement due to the introduction
information and communication technology with electronic medical prescription
being one the areas benefiting from it. Within the overall context of
protection of health care information, privacy of prescription data needs
special treatment. This paper presents an e-prescription system that addresses
some challenges pertaining to the prescription privacy protection in the
process of drug prescription. The developed system uses spread spectrum image
steganography algorithm with Advanced Encryption Standard (AES) key
implementation to provide a secure means of delivering medical prescription to
the parties involved. The architecture for encoding and decoding was
implemented with an electronic health record. The software development tools
used were PHP and MySQL database management system for front end and backend
data management respectively. The designed system demonstration shows that the
synergistic combination of steganography and cryptography technologies in
medical prescription is capable of providing a secure transmission to properly
provide security for patients medical prescription.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01265</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01265</id><created>2015-02-04</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Optimal transport over a linear dynamical system</title><categories>math.OC cs.SY</categories><comments>25 pages, 13 figures</comments><msc-class>93E20, 49L99, 60G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of steering an initial probability density for the
state vector of a linear system to a final one, in finite time, using minimum
energy control. In the case where the dynamics correspond to an integrator
($\dot x(t) = u(t)$) this amounts to a Monge-Kantorovich Optimal Mass Transport
(OMT) problem. In general, we show that the problem can again be reduced to
solving an OMT problem and that it has a unique solution. In parallel, we study
the optimal steering of the state-density of a linear stochastic system with
white noise disturbance; this is known to correspond to a Schr\&quot;odinger bridge.
As the white noise intensity tends to zero, the flow of densities converges to
that of the deterministic dynamics and can serve as a way to compute the
solution of its deterministic counterpart. The solution can be expressed in
closed-form for Gaussian initial and final state densities in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01271</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01271</id><created>2015-02-04</created><updated>2016-01-06</updated><authors><author><keyname>Grefenstette</keyname><forenames>Gregory</forenames><affiliation>TAO</affiliation></author></authors><title>INRIASAC: Simple Hypernym Extraction Methods</title><categories>cs.CL</categories><comments>SemEval 2015, Jun 2015, Denver, United States</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of terms from a given domain, how can we structure them into a
taxonomy without manual intervention? This is the task 17 of SemEval 2015. Here
we present our simple taxonomy structuring techniques which, despite their
simplicity, ranked first in this 2015 benchmark. We use large quantities of
text (English Wikipedia) and simple heuristics such as term overlap and
document and sentence co-occurrence to produce hypernym lists. We describe
these techniques and pre-sent an initial evaluation of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01272</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01272</id><created>2015-02-04</created><updated>2015-11-15</updated><authors><author><keyname>Bagchi</keyname><forenames>Shrobona</forenames></author><author><keyname>Pati</keyname><forenames>Arun Kumar</forenames></author></authors><title>Monogamy, polygamy, and other properties of entanglement of purification</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, 2 figures, Published version</comments><journal-ref>Phys. Rev. A 91, 042323 (2015)</journal-ref><doi>10.1103/PhysRevA.91.042323</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For bipartite pure and mixed quantum states, in addition to the quantum
mutual information, there is another measure of total correlation, namely, the
entanglement of purification. We study the monogamy, polygamy, and additivity
properties of the entanglement of purification for pure and mixed states. In
this paper, we show that, in contrast to the quantum mutual information which
is strictly monogamous for any tripartite pure states, the entanglement of
purification is polygamous for the same. This shows that there can be genuinely
two types of total correlation across any bipartite cross in a pure tripartite
state. Furthermore, we find the lower bound and actual values of the
entanglement of purification for different classes of tripartite and
higher-dimensional bipartite mixed states. Thereafter, we show that if
entanglement of purification is not additive on tensor product states, it is
actually subadditive. Using these results, we identify some states which are
additive on tensor products for entanglement of purification. The implications
of these findings on the quantum advantage of dense coding are briefly
discussed, whereby we show that for tripartite pure states, it is strictly
monogamous and if it is nonadditive, then it is superadditive on tensor product
states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01278</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01278</id><created>2015-02-04</created><updated>2015-02-05</updated><authors><author><keyname>Jakob</keyname><forenames>Robert</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>A Falsification View of Success Typing</title><categories>cs.PL</categories><comments>extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic languages are praised for their flexibility and expressiveness, but
static analysis often yields many false positives and verification is
cumbersome for lack of structure. Hence, unit testing is the prevalent
incomplete method for validating programs in such languages.
  Falsification is an alternative approach that uncovers definite errors in
programs. A falsifier computes a set of inputs that definitely crash a program.
  Success typing is a type-based approach to document programs in dynamic
languages. We demonstrate that success typing is, in fact, an instance of
falsification by mapping success (input) types into suitable logic formulae.
Output types are represented by recursive types. We prove the correctness of
our mapping (which establishes that success typing is falsification) and we
report some experiences with a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01292</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01292</id><created>2015-02-04</created><updated>2015-11-17</updated><authors><author><keyname>Katis</keyname><forenames>Andreas</forenames></author><author><keyname>Gacek</keyname><forenames>Andrew</forenames></author><author><keyname>Whalen</keyname><forenames>Michael W.</forenames></author></authors><title>Machine-Checked Proofs For Realizability Checking Algorithms</title><categories>cs.SE</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual integration techniques focus on building architectural models of
systems that can be analyzed early in the design cycle to try to lower cost,
reduce risk, and improve quality of complex embedded systems. Given appropriate
architectural descriptions, assume/guarantee contracts, and compositional
reasoning rules, these techniques can be used to prove important safety
properties about the architecture prior to system construction. For these
proofs to be meaningful, each leaf-level component contract must be realizable;
i.e., it is possible to construct a component such that for any input allowed
by the contract assumptions, there is some output value that the component can
produce that satisfies the contract guarantees. We have recently proposed (in
[1]) a contract-based realizability checking algorithm for assume/guarantee
contracts over infinite theories supported by SMT solvers such as linear
integer/real arithmetic and uninterpreted functions. In that work, we used an
SMT solver and an algorithm similar to k-induction to establish the
realizability of a contract, and justified our approach via a hand proof. Given
the central importance of realizability to our virtual integration approach, we
wanted additional confidence that our approach was sound. This paper describes
a complete formalization of the approach in the Coq proof and specification
language. During formalization, we found several small mistakes and missing
assumptions in our reasoning. Although these did not compromise the correctness
of the algorithm used in the checking tools, they point to the value of
machine-checked formalization. In addition, we believe this is the first
machine-checked formalization for a realizability algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01312</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01312</id><created>2015-01-13</created><authors><author><keyname>Vieira</keyname><forenames>Vilson</forenames></author><author><keyname>Lunhani</keyname><forenames>Guilherme</forenames></author><author><keyname>Junior</keyname><forenames>Geraldo Magela de Castro Rocha</forenames></author><author><keyname>Luporini</keyname><forenames>Caleb Mascarenhas</forenames></author><author><keyname>Penalva</keyname><forenames>Daniel</forenames></author><author><keyname>Fabbri</keyname><forenames>Ricardo</forenames></author><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author></authors><title>Vivace: A Collaborative Live Coding Language</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the principles and the design of Vivace, a live coding
language and environment built with Web technologies to be executed, ideally,
in any ordinary browser. It starts by reviewing what motivated and inspired the
creation of the language, in the context of actual performances. That leads to
specifications of the language and how it is parsed and then executed using the
recently created real-time Web Audio API. A brief discussion is presented on
why the Web is an environment of interest to collaborative live coding and how
it affects the performances. This work concludes by describing how Vivace has
motivated the creation of &quot;freak coding&quot;, a live coding sub-genre.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01321</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01321</id><created>2015-02-03</created><authors><author><keyname>Nayak</keyname><forenames>Sukanta</forenames></author><author><keyname>Chakraverty</keyname><forenames>Snehashish</forenames></author></authors><title>Numerical Solution of Fuzzy Stochastic Differential Equation</title><categories>cs.NA cs.AI math-ph math.MP math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an alternative approach to solve uncertain Stochastic
Differential Equation (SDE) is proposed. This uncertainty occurs due to the
involved parameters in system and these are considered as Triangular Fuzzy
Numbers (TFN). Here the proposed fuzzy arithmetic in [2] is used as a tool to
handle Fuzzy Stochastic Differential Equation (FSDE). In particular, a system
of Ito stochastic differential equations is analysed with fuzzy parameters.
Further exact and Euler Maruyama approximation methods with fuzzy values are
demonstrated and solved some standard SDE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01322</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01322</id><created>2015-02-02</created><authors><author><keyname>Gostar</keyname><forenames>Amirali K.</forenames></author><author><keyname>Hoseinnezhad</keyname><forenames>Reza</forenames></author><author><keyname>Bab-Hadiashar</keyname><forenames>Alireza</forenames></author></authors><title>Sensor Control for Multi-Object Tracking Using Labeled Multi-Bernoulli
  Filter</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently developed labeled multi-Bernoulli (LMB) filter uses better
approximations in its update step, compared to the unlabeled multi-Bernoulli
filters, and more importantly, it provides us with not only the estimates for
the number of targets and their states, but also with labels for existing
tracks. This paper presents a novel sensor-control method to be used for
optimal multi-target tracking within the LMB filter. The proposed method uses a
task-driven cost function in which both the state estimation errors and
cardinality estimation errors are taken into consideration. Simulation results
demonstrate that the proposed method can successfully guide a mobile sensor in
a challenging multi-target tracking scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01329</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01329</id><created>2015-02-03</created><updated>2015-02-08</updated><authors><author><keyname>Mobley</keyname><forenames>David L.</forenames></author><author><keyname>Zuckerman</keyname><forenames>Daniel M.</forenames></author></authors><title>A proposal for regularly updated review/survey articles: &quot;Perpetual
  Reviews&quot;</title><categories>cs.DL</categories><comments>This is a draft white paper and we seek comments from the community</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We advocate the publication of review/survey articles that will be updated
regularly, both in traditional journals and novel venues. We call these
&quot;perpetual reviews.&quot; This idea naturally builds on the dissemination and
archival capabilities present in the modern internet, and indeed perpetual
reviews exist already in some forms. Perpetual review articles allow authors to
maintain over time the relevance of non-research scholarship that requires a
significant investment of effort. Further, such reviews published in a purely
electronic format without space constraints can also permit more pedagogical
scholarship and clearer treatment of technical issues that remain obscure in a
brief treatment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01335</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01335</id><created>2015-02-04</created><updated>2015-12-23</updated><authors><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author></authors><title>Approximately Counting H-Colourings is #BIS-Hard</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of counting H-colourings from an input graph G to a
target graph H. We show that if H is any fixed graph without trivial
components, then the problem is as hard as the well-known problem #BIS, which
is the problem of (approximately) counting independent sets in a bipartite
graph. #BIS is a complete problem in an important complexity class for
approximate counting, and is believed not to have an FPRAS. If this is so, then
our result shows that for every graph H without trivial components, the
H-colouring counting problem has no FPRAS. This problem was studied a decade
ago by Goldberg, Kelk and Paterson. They were able to show that approximately
sampling H-colourings is #BIS-hard, but it was not known how to get the result
for approximate counting. Our solution builds on non-constructive ideas using
the work of Lovasz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01359</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01359</id><created>2015-02-04</created><updated>2016-01-18</updated><authors><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames><suffix>III</suffix></author></authors><title>The Three-Terminal Interactive Lossy Source Coding Problem</title><categories>cs.IT math.IT</categories><comments>New version with changes suggested by reviewers.Revised and
  resubmitted to IEEE Transactions on Information Theory. 92 pages, 11 figures,
  1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three-node multiterminal lossy source coding problem is investigated. We
derive an inner bound to the general rate-distortion region of this problem
which is a natural extension of the seminal work by Kaspi'85 on the interactive
two-terminal source coding problem. It is shown that this (rather involved)
inner bound contains several rate-distortion regions of some relevant source
coding settings. In this way, besides the non-trivial extension of the
interactive two terminal problem, our results can be seen as a generalization
and hence unification of several previous works in the field. Specializing to
particular cases we obtain novel rate-distortion regions for several lossy
source coding problems. We finish by describing some of the open problems and
challenges. However, the general three-node multiterminal lossy source coding
problem seems to offer a formidable mathematical complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01367</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01367</id><created>2015-02-04</created><updated>2015-02-11</updated><authors><author><keyname>Rohe</keyname><forenames>Klaus</forenames></author></authors><title>Visualizing Marden's theorem with Scilab</title><categories>cs.MS</categories><comments>Scilab, Marden's theorem, 2D implicit plots, geometry of complex
  numbers, Steiner ellipses</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theorem which is named after the American Mathematician Moris Marden states
a very surprising and interesting fact concerning the relationship between the
points of a triangle in the complex plane and the zeros of two complex
polynomials related to this triangle: &quot;Suppose the zeroes z1, z2, and z3 of a
third-degree polynomial p(z) are non-collinear. There is a unique ellipse
inscribed in the triangle with vertices z1, z2, z3 and tangent to the sides at
their midpoints: the Steiner in-ellipse. The foci of that ellipse are the
zeroes of the derivative p'(z).&quot; (Wikipedia contributors, &quot;Marden's theorem&quot;,
http://en.wikipedia.org/wiki/Marden%27s_theorem). This document describes how
Scilab, a popular and powerful open source alternative to MATLAB, can be used
to visualize the above stated theorem for arbitrary complex numbers z1, z2, and
z3 which are not collinear. It is further demonstrated how the equations of the
Steiner ellipses of a triangle in the complex plane can be calculated and
plotted by applying this theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01377</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01377</id><created>2015-02-04</created><authors><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Dimitrov</keyname><forenames>V. S.</forenames></author></authors><title>The Arithmetic Cosine Transform: Exact and Approximate Algorithms</title><categories>cs.NA math.NA stat.AP stat.CO stat.ME</categories><comments>17 pages, 3 figures</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 58, no. 6, pp.
  3076-3085, June 2010</journal-ref><doi>10.1109/TSP.2010.2045781</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new class of transform method --- the
arithmetic cosine transform (ACT). We provide the central mathematical
properties of the ACT, necessary in designing efficient and accurate
implementations of the new transform method. The key mathematical tools used in
the paper come from analytic number theory, in particular the properties of the
Riemann zeta function. Additionally, we demonstrate that an exact signal
interpolation is achievable for any block-length. Approximate calculations were
also considered. The numerical examples provided show the potential of the ACT
for various digital signal processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01380</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01380</id><created>2015-02-04</created><updated>2016-01-27</updated><authors><author><keyname>Mare&#x161;</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Janouchov&#xe1;</keyname><forenames>Eli&#x161;ka</forenames></author><author><keyname>Ku&#x10d;erov&#xe1;</keyname><forenames>Anna</forenames></author></authors><title>Artificial neural networks in calibration of nonlinear mechanical models</title><categories>cs.NE cs.CE</categories><comments>26 pages, 8 figures, 11 tables, accepted for publication in Advances
  in Engineering Software</comments><journal-ref>Advances in Engineering Software, 95:68-81, 2016</journal-ref><doi>10.1016/j.advengsoft.2016.01.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid development in numerical modelling of materials and the complexity of
new models increases quickly together with their computational demands. Despite
the growing performance of modern computers and clusters, calibration of such
models from noisy experimental data remains a nontrivial and often
computationally exhaustive task. The layered neural networks thus represent a
robust and efficient technique to overcome the time-consuming simulations of a
calibrated model. The potential of neural networks consists in simple
implementation and high versatility in approximating nonlinear relationships.
Therefore, there were several approaches proposed to accelerate the calibration
of nonlinear models by neural networks. This contribution reviews and compares
three possible strategies based on approximating (i) model response, (ii)
inverse relationship between the model response and its parameters and (iii)
error function quantifying how well the model fits the data. The advantages and
drawbacks of particular strategies are demonstrated on the calibration of four
parameters of the affinity hydration model from simulated data as well as from
experimental measurements. This model is highly nonlinear, but computationally
cheap thus allowing its calibration without any approximation and better
quantification of results obtained by the examined calibration strategies. The
paper can be thus viewed as a guide intended for the engineers to help them
select an appropriate strategy in their particular calibration problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01385</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01385</id><created>2015-02-04</created><authors><author><keyname>Demanet</keyname><forenames>Laurent</forenames></author><author><keyname>Nguyen</keyname><forenames>Nam</forenames></author></authors><title>The recoverability limit for superresolution via sparsity</title><categories>cs.IT math.IT math.NA</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of robustly recovering a $k$-sparse coefficient
vector from the Fourier series that it generates, restricted to the interval
$[- \Omega, \Omega]$. The difficulty of this problem is linked to the
superresolution factor SRF, equal to the ratio of the Rayleigh length (inverse
of $\Omega$) by the spacing of the grid supporting the sparse vector. In the
presence of additive deterministic noise of norm $\sigma$, we show upper and
lower bounds on the minimax error rate that both scale like $(SRF)^{2k-1}
\sigma$, providing a partial answer to a question posed by Donoho in 1992. The
scaling arises from comparing the noise level to a restricted isometry constant
at sparsity $2k$, or equivalently from comparing $2k$ to the so-called
$\sigma$-spark of the Fourier system. The proof involves new bounds on the
singular values of restricted Fourier matrices, obtained in part from old
techniques in complex analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01400</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01400</id><created>2015-02-04</created><updated>2016-02-02</updated><authors><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steve</forenames></author></authors><title>Fast unsupervised Bayesian image segmentation with adaptive spatial
  regularisation</title><categories>stat.CO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new Bayesian estimation technique for hidden
Potts-Markov random fields with unknown regularisation parameters, with
application to fast unsupervised K-class image segmentation. The technique is
derived by first removing the regularisation parameter from the Bayesian model
by marginalisation, followed by a small-variance-asymptotic (SVA) analysis in
which the spatial regularisation and the integer-constrained terms of the Potts
model are decoupled. The evaluation of this SVA Bayesian estimator is then
relaxed into a problem that can be computed efficiently by iteratively solving
a convex total-variation denoising problem and a least-squares clustering
(K-means) problem, both of which can be solved straightforwardly, even in
high-dimensions, and with parallel computing techniques. This leads to a fast
fully unsupervised Bayesian image segmentation methodology in which the
strength of the spatial regularisation is adapted automatically to the observed
image during the inference procedure, and that can be easily applied in large
2D and 3D scenarios or in applications requiring low computing times.
Experimental results on real images, as well as extensive comparisons with
state-of-the-art algorithms, confirm that the proposed methodology offer
extremely fast convergence and produces accurate segmentation results, with the
important additional advantage of self-adjusting regularisation parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01403</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01403</id><created>2015-02-04</created><updated>2015-02-06</updated><authors><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms
  and Lower Bounds</title><categories>cs.DS cs.CC stat.ML</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following generalized matrix rank estimation problem: given an
$n \times n$ matrix and a constant $c \geq 0$, estimate the number of
eigenvalues that are greater than $c$. In the distributed setting, the matrix
of interest is the sum of $m$ matrices held by separate machines. We show that
any deterministic algorithm solving this problem must communicate $\Omega(n^2)$
bits, which is order-equivalent to transmitting the whole matrix. In contrast,
we propose a randomized algorithm that communicates only $\widetilde O(n)$
bits. The upper bound is matched by an $\Omega(n)$ lower bound on the
randomized communication complexity. We demonstrate the practical effectiveness
of the proposed algorithm with some numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01410</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01410</id><created>2015-02-04</created><authors><author><keyname>Velez</keyname><forenames>Martin</forenames></author><author><keyname>Qiu</keyname><forenames>Dong</forenames></author><author><keyname>Zhou</keyname><forenames>You</forenames></author><author><keyname>Barr</keyname><forenames>Earl T.</forenames></author><author><keyname>Su</keyname><forenames>Zhendong</forenames></author></authors><title>A Study of &quot;Wheat&quot; and &quot;Chaff&quot; in Source Code</title><categories>cs.SE</categories><comments>10 pages, Under Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language is robust against noise. The meaning of many sentences
survives the loss of words, sometimes many of them. Some words in a sentence,
however, cannot be lost without changing the meaning of the sentence. We call
these words &quot;wheat&quot; and the rest &quot;chaff.&quot; The word &quot;not&quot; in the sentence &quot;I do
not like rain&quot; is wheat and &quot;do&quot; is chaff. For human understanding of the
purpose and behavior of source code, we hypothesize that the same holds. To
quantify the extent to which we can separate code into &quot;wheat&quot; and &quot;chaff&quot;, we
study a large (100M LOC), diverse corpus of real-world projects in Java. Since
methods represent natural, likely distinct units of code, we use the,
approximately, 9M Java methods in the corpus to approximate a universe of
&quot;sentences.&quot; We &quot;thresh&quot;, or lex, functions, then &quot;winnow&quot; them to extract
their wheat by computing the minimal distinguishing subset (MINSET). Our
results confirm that programs contain much chaff. On average, MINSETS have 1.56
words (none exceeds 6) and comprise 4% of their methods. Beyond its intrinsic
scientific interest, our work offers the first quantitative evidence for recent
promising work on keyword-based programming and insight into how to develop
powerful, alternative programming systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01414</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01414</id><created>2015-02-04</created><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Goparaju</keyname><forenames>Sreechakra</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Cyclic LRC Codes and their Subfield Subcodes</title><categories>cs.IT math.IT</categories><comments>Submitted for publication</comments><msc-class>94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider linear cyclic codes with the locality property, or locally
recoverable codes (LRC codes). A family of LRC codes that generalizes the
classical construction of Reed-Solomon codes was constructed in a recent paper
by I. Tamo and A. Barg (IEEE Transactions on Information Theory, no. 8, 2014;
arXiv:1311.3284). In this paper we focus on the optimal cyclic codes that arise
from the general construction. We give a characterization of these codes in
terms of their zeros, and observe that there are many equivalent ways of
constructing optimal cyclic LRC codes over a given field. We also study
subfield subcodes of cyclic LRC codes (BCH-like LRC codes) and establish
several results about their locality and minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01418</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01418</id><created>2015-02-04</created><updated>2015-02-07</updated><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>RELEAF: An Algorithm for Learning and Exploiting Relevance</title><categories>cs.LG stat.ML</categories><comments>to appear in IEEE Journal of Selected Topics in Signal Processing,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems, medical diagnosis, network security, etc., require
on-going learning and decision-making in real time. These -- and many others --
represent perfect examples of the opportunities and difficulties presented by
Big Data: the available information often arrives from a variety of sources and
has diverse features so that learning from all the sources may be valuable but
integrating what is learned is subject to the curse of dimensionality. This
paper develops and analyzes algorithms that allow efficient learning and
decision-making while avoiding the curse of dimensionality. We formalize the
information available to the learner/decision-maker at a particular time as a
context vector which the learner should consider when taking actions. In
general the context vector is very high dimensional, but in many settings, the
most relevant information is embedded into only a few relevant dimensions. If
these relevant dimensions were known in advance, the problem would be simple --
but they are not. Moreover, the relevant dimensions may be different for
different actions. Our algorithm learns the relevant dimensions for each
action, and makes decisions based in what it has learned. Formally, we build on
the structure of a contextual multi-armed bandit by adding and exploiting a
relevance relation. We prove a general regret bound for our algorithm whose
time order depends only on the maximum number of relevant dimensions among all
the actions, which in the special case where the relevance relation is
single-valued (a function), reduces to $\tilde{O}(T^{2(\sqrt{2}-1)})$; in the
absence of a relevance relation, the best known contextual bandit algorithms
achieve regret $\tilde{O}(T^{(D+1)/(D+2)})$, where $D$ is the full dimension of
the context vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01423</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01423</id><created>2015-02-04</created><updated>2015-04-09</updated><authors><author><keyname>Fang</keyname><forenames>Chen</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author></authors><title>Collaborative Feature Learning from Social Media</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image feature representation plays an essential role in image recognition and
related tasks. The current state-of-the-art feature learning paradigm is
supervised learning from labeled data. However, this paradigm requires
large-scale category labels, which limits its applicability to domains where
labels are hard to obtain. In this paper, we propose a new data-driven feature
learning paradigm which does not rely on category labels. Instead, we learn
from user behavior data collected on social media. Concretely, we use the image
relationship discovered in the latent space from the user behavior data to
guide the image feature learning. We collect a large-scale image and user
behavior dataset from Behance.net. The dataset consists of 1.9 million images
and over 300 million view records from 1.9 million users. We validate our
feature learning paradigm on this dataset and find that the learned feature
significantly outperforms the state-of-the-art image features in learning
better image similarities. We also show that the learned feature performs
competitively on various recognition benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01424</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01424</id><created>2015-02-04</created><authors><author><keyname>Vermehren</keyname><forenames>V. V.</forenames></author><author><keyname>Wesen</keyname><forenames>J. E.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Close Approximations for Daublets and their Spectra</title><categories>cs.NA math.CA</categories><comments>6 pages, 6 figures, 3 tables. Conference: International
  Telecommunication Symposium, ITS 2010, Manaus, AM , Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper offers a new regard on compactly supported wavelets derived from
FIR filters. Although being continuous wavelets, analytical formulation are
lacking for such wavelets. Close approximations for daublets (Daubechies
wavelets) and their spectra are introduced here. The frequency detection
properties of daublets are investigated through scalograms derived from these
new analytical expressions. These near-daublets have been implemented on the
Matlab wavelet toolbox and a few scalograms presented. This approach can be
valuable for wavelet synthesis from hardware or for application involving
continuous wavelet-based systems, such as wavelet OFDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01435</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01435</id><created>2015-02-05</created><authors><author><keyname>Stout</keyname><forenames>Quentin F.</forenames></author></authors><title>Optimal component labeling algorithms for mesh-connected computers and
  VLSI</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G$ of $n$ weighted edges, stored one edge per
processor in a square mesh of $n$ processors, we show how to determine the
connected components and a minimal spanning forest in $\Theta(\sqrt{n})$ time.
More generally, we show how to solve these problems in $\Theta(n^{1/d})$ time
when the mesh is a $d$-dimensional cube, where the implied constants depend
upon $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01446</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01446</id><created>2015-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Jiajun</forenames></author><author><keyname>Liu</keyname><forenames>Shujie</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author><author><keyname>Zong</keyname><forenames>Chengqing</forenames></author></authors><title>Beyond Word-based Language Model in Statistical Machine Translation</title><categories>cs.CL</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language model is one of the most important modules in statistical machine
translation and currently the word-based language model dominants this
community. However, many translation models (e.g. phrase-based models) generate
the target language sentences by rendering and compositing the phrases rather
than the words. Thus, it is much more reasonable to model dependency between
phrases, but few research work succeed in solving this problem. In this paper,
we tackle this problem by designing a novel phrase-based language model which
attempts to solve three key sub-problems: 1, how to define a phrase in language
model; 2, how to determine the phrase boundary in the large-scale monolingual
data in order to enlarge the training set; 3, how to alleviate the data
sparsity problem due to the huge vocabulary size of phrases. By carefully
handling these issues, the extensive experiments on Chinese-to-English
translation show that our phrase-based language model can significantly improve
the translation quality by up to +1.47 absolute BLEU score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01454</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01454</id><created>2015-02-05</created><authors><author><keyname>AbdelAziz</keyname><forenames>Ali Mohamed</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>The Diversity and Scale Matter: Ubiquitous Transportation Mode Detection
  using Single Cell Tower Information</title><categories>cs.NI cs.CY</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the transportation mode of a user is important for a wide range of
applications. While a number of recent systems addressed the transportation
mode detection problem using the ubiquitous mobile phones, these studies either
leverage GPS, the inertial sensors, and/or multiple cell towers information.
However, these different phone sensors have high energy consumption, limited to
a small subset of phones (e.g. high-end phones or phones that support
neighbouring cell tower information), cannot work in certain areas (e.g. inside
tunnels for GPS), and/or work only from the user side.
  In this paper, we present a transportation mode detection system, MonoSense,
that leverages the phone serving cell information only. The basic idea is that
the phone speed can be correlated with features extracted from both the serving
cell tower ID and the received signal strength from it. To achieve high
detection accuracy with this limited information, MonoSense leverages diversity
along multiple axes to extract novel features. Specifically, MonoSense extracts
features from both the time and frequency domain information available from the
serving cell tower over different sliding widow sizes. More importantly, we
show also that both the logarithmic and linear RSS scales can provide different
information about the movement of a phone, further enriching the feature space
and leading to higher accuracy.
  Evaluation of MonoSense using 135 hours of cellular traces covering 485 km
and collected by four users using different Android phones shows that it can
achieve an average precision and recall of 89.26% and 89.84% respectively in
differentiating between the stationary, walking, and driving modes using only
the serving cell tower information, highlighting MonoSense ability to enable a
wide set of intelligent transportation applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01456</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01456</id><created>2015-02-05</created><updated>2015-02-09</updated><authors><author><keyname>Tang</keyname><forenames>Wanrong</forenames></author><author><keyname>Zhang</keyname><forenames>Ying Jun</forenames></author></authors><title>Online Electric Vehicle Charging Control with Multistage Stochastic
  Programming</title><categories>math.OC cs.PF cs.SY</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing adoption of plug-in electric vehicles (PEVs), it is
critical to develop efficient charging coordination mechanisms that minimize
the cost and impact of PEV integration to the power grid. In this paper, we
consider the optimal PEV charging scheduling problem in a practical scenario
where the non-causal information about future PEV arrivals is not known in
advance, but its statistical distribution can be estimated. This leads to an
&quot;online&quot; charging scheduling problem that is formulated as a multistage
stochastic programming (MSP) problem in this paper. Instead of solving the MSP
using the standard approaches such as sample average approximation (SAA), we
show that solving a deterministic counterpart of the stochastic problem yields
significant complexity reduction with negligible performance loss. Compared
with the SAA approach, the computational complexity of the proposed algorithm
is drastically reduced from $O(\epsilon^T)$ to $O(T^3)$ per time stage, where
$T$ is the total number of the time stages and $\epsilon$ is a constant related
to the accuracy of the solution. More importantly, we show that the proposed
algorithm can be made scalable when the random process describing the arrival
of charging demands is first-order periodic. That is, the complexity of
obtaining the charging schedule at each time stage is $O(1)$ and is independent
of $T$. Extensive simulations show that the proposed online algorithm performs
very closely to the optimal online algorithm. The performance gap is smaller
than $0.4\%$ in most cases. As such, the proposed online algorithm is very
appealing for practical implementation due to its scalable computational
complexity and close to optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01461</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01461</id><created>2015-02-05</created><authors><author><keyname>Bliznets</keyname><forenames>Ivan</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Karpov</keyname><forenames>Nikolay</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Parameterized Complexity of Superstring Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Shortest Superstring problem we are given a set of strings $S=\{s_1,
\ldots, s_n\}$ and integer $\ell$ and the question is to decide whether there
is a superstring $s$ of length at most $\ell$ containing all strings of $S$ as
substrings. We obtain several parameterized algorithms and complexity results
for this problem.
  In particular, we give an algorithm which in time $2^{O(k)}
\operatorname{poly}(n)$ finds a superstring of length at most $\ell$ containing
at least $k$ strings of $S$. We complement this by the lower bound showing that
such a parameterization does not admit a polynomial kernel up to some
complexity assumption. We also obtain several results about &quot;below guaranteed
values&quot; parameterization of the problem. We show that parameterization by
compression admits a polynomial kernel while parameterization &quot;below matching&quot;
is hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01462</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01462</id><created>2015-02-05</created><updated>2015-03-11</updated><authors><author><keyname>Gainutdinova</keyname><forenames>Aida</forenames></author><author><keyname>Yakaryilmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Unary probabilistic and quantum automata on promise problems</title><categories>cs.CC cs.FL</categories><comments>Minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the systematic investigation of probabilistic and quantum finite
automata (PFAs and QFAs) on promise problems by focusing on unary languages. We
show that bounded-error QFAs are more powerful than PFAs. But, in contrary to
the binary problems, the computational powers of Las-Vegas QFAs and
bounded-error PFAs are equivalent to deterministic finite automata (DFAs).
Lastly, we present a new family of unary promise problems with two parameters
such that when fixing one parameter QFAs can be exponentially more succinct
than PFAs and when fixing the other parameter PFAs can be exponentially more
succinct than DFAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01475</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01475</id><created>2015-02-05</created><authors><author><keyname>Han</keyname><forenames>Peng</forenames></author></authors><title>Fast Constraint Propagation for Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel selective constraint propagation method for
constrained image segmentation. In the literature, many pairwise constraint
propagation methods have been developed to exploit pairwise constraints for
cluster analysis. However, since most of these methods have a polynomial time
complexity, they are not much suitable for segmentation of images even with a
moderate size, which is actually equivalent to cluster analysis with a large
data size. Considering the local homogeneousness of a natural image, we choose
to perform pairwise constraint propagation only over a selected subset of
pixels, but not over the whole image. Such a selective constraint propagation
problem is then solved by an efficient graph-based learning algorithm. To
further speed up our selective constraint propagation, we also discard those
less important propagated constraints during graph-based learning. Finally, the
selectively propagated constraints are exploited based on $L_1$-minimization
for normalized cuts over the whole image. The experimental results demonstrate
the promising performance of the proposed method for segmentation with
selectively propagated constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01480</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01480</id><created>2015-02-05</created><authors><author><keyname>Paleo</keyname><forenames>Pierre</forenames></author><author><keyname>Mirone</keyname><forenames>Alessandro</forenames></author></authors><title>Ring artifacts correction in compressed sensing tomographic
  reconstruction</title><categories>physics.comp-ph cs.CV</categories><comments>IUCR template, preprint mode, 35 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to handle ring artifacts correction in compressed
sensing tomographic reconstruction. The correction is part of the
reconstruction process, which differs from classical sinogram pre-processing
and image post-processing techniques. The principle of compressed sensing
tomographic reconstruction is presented. Then, we show that the ring artifacts
correction can be integrated in the reconstruction problem formalism. We
provide numerical results for both simulated and real data. This technique is
included in the PyHST2 code which is used at the European Synchrotron Radiation
Facility for tomographic reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01482</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01482</id><created>2015-02-05</created><updated>2015-10-06</updated><authors><author><keyname>Orsino</keyname><forenames>Antonino</forenames></author><author><keyname>Araniti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molinaro</keyname><forenames>Antonella</forenames></author><author><keyname>Iera</keyname><forenames>Antonio</forenames></author></authors><title>Effective RAT Selection Approach for 5G Dense Wireless Networks</title><categories>cs.NI cs.PF</categories><comments>We just realized that the submitted version is not compliant with the
  final version of the manuscript. In addition, there are also crucial error in
  the formulation of the analytical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense Networks (DenseNet) and Multi-Radio Access Technologies (Multi-RATs)
are considered as key features of the emerging fifth generation (5G) wireless
systems. A Multi-RAT DenseNet is characterized by a very dense deployment of
low-power base stations (BSs) and by a multi-tier architecture consisting of
heterogeneous radio access technologies. Such a network aims to guarantee high
data-rates, low latency and low energy consumption. Although the usage of a
Multi RAT DenseNet solves problems such as coverage holes and low performance
at the cell edge, frequent and unnecessary RAT handovers may occur with a
consequent high signaling load. In this work, we propose an effective RAT
selection algorithm that efficiently manages the RAT handover procedure by
\emph{(i)} choosing the most suitable RAT that guarantees high system and user
performance, and \emph{(ii)} reducing unnecessary handover events. In
particular, the decision to trigger a handover is based on a new system
parameter named Reference Base Station Efficiency (RBSE). This parameter takes
into account metrics related to both the system and the user: the BS
transmitted power, the BS traffic load and the users' spectral efficiency. We
compare, by simulation, the proposed scheme with the standardized 3GPP
policies. Results show that the proposed RAT selection scheme significantly
reduces the number of handovers and the end-to-end delay while maintaining high
system throughput and user spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01485</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01485</id><created>2015-02-05</created><updated>2015-10-06</updated><authors><author><keyname>Araniti</keyname><forenames>Giuseppe</forenames></author><author><keyname>Condoluci</keyname><forenames>Massimo</forenames></author><author><keyname>Orsino</keyname><forenames>Antonino</forenames></author><author><keyname>Iera</keyname><forenames>Antonio</forenames></author><author><keyname>Molinaro</keyname><forenames>Antonella</forenames></author></authors><title>Effective Resource Allocation in 5G-Satellite Networks</title><categories>cs.NI</categories><comments>We just realized that the submitted version is not compliant with the
  final version of the manuscript. In addition, there are also crucial error in
  the formulation of the analytical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the radio resource management of multicast transmissions
in the emerging fifth generation satellite systems (5G-Satellite). A
subgrouping approach is exploited to provide video streaming services to
satellite users by splitting any multicast group into subgroups. This allows an
effective exploitation of multi-user diversity according to the experienced
channel conditions and the achievement of a high throughput level. The main
drawback is the high computational cost usually related to the selection of the
optimal subgroup configuration. In this paper we propose a low-complexity
subgrouping algorithm that achieves performance close to optimum. Our solution
is suitable for implementation in practical systems, such as Satellite-Long
Term Evolution (S-LTE), since the computational cost does not depend on the
multicast group size and the number of available resources. Through simulation
campaigns conducted in different radio propagation and multicast group
environments, the effectiveness of the proposed subgroup formation scheme is
assessed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01493</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01493</id><created>2015-02-05</created><authors><author><keyname>Branders</keyname><forenames>Samuel</forenames></author><author><keyname>D'Ambrosio</keyname><forenames>Roberto</forenames></author><author><keyname>Dupont</keyname><forenames>Pierre</forenames></author></authors><title>A mixture Cox-Logistic model for feature selection from survival and
  classification data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an original approach for jointly fitting survival times
and classifying samples into subgroups. The Coxlogit model is a generalized
linear model with a common set of selected features for both tasks. Survival
times and class labels are here assumed to be conditioned by a common risk
score which depends on those features. Learning is then naturally expressed as
maximizing the joint probability of subgroup labels and the ordering of
survival events, conditioned to a common weight vector. The model is estimated
by minimizing a regularized log-likelihood through a coordinate descent
algorithm.
  Validation on synthetic and breast cancer data shows that the proposed
approach outperforms a standard Cox model or logistic regression when both
predicting the survival times and classifying new samples into subgroups. It is
also better at selecting informative features for both tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01494</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01494</id><created>2015-02-05</created><authors><author><keyname>Tomasi</keyname><forenames>Alessandro</forenames></author><author><keyname>Meneghetti</keyname><forenames>Alessio</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>Code generator matrices as entropy extractors</title><categories>cs.IT math.IT</categories><msc-class>94A17 (Primary) 94A60, 94B05 (Secondary)</msc-class><acm-class>E.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the connection between the Walsh spectrum of the output of a binary
random number generator (RNG) and the bias of individual bits, and use this to
show how previously known bounds on the performance of linear binary codes as
entropy extractors can be derived by considering generator matrices as a
selector of a subset of that spectrum. We explicitly show the connection with
the code's weight distribution, then extend this framework to the case of
non-binary finite fields by the Fourier transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01496</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01496</id><created>2015-02-05</created><authors><author><keyname>Abd-Elrahman</keyname><forenames>Emad</forenames></author><author><keyname>Said</keyname><forenames>Adel Mounir</forenames></author><author><keyname>Toukabri</keyname><forenames>Thouraya</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Marot</keyname><forenames>Michel</forenames></author></authors><title>Assisting V2V failure recovery using Device-to-Device Communications</title><categories>cs.NI</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to propose a new solution for failure recovery (dead-ends) in
Vehicle to Vehicle (V2V) communications through LTE-assisted Device-to-Device
communications (D2D). Based on the enhanced networking capabilities offered by
Intelligent Transportation Systems (ITS) architecture, our solution can
efficiently assist V2V communications in failure recovery situations. We also
derive an analytical model to evaluate generic V2V routing recovery failures.
Moreover, the proposed hybrid model is simulated and compared to the generic
model under different constrains of worst and best cases of D2D discovery and
communication. According to our comparison and simulation results, the hybrid
model decreases the delay for alarm message propagation to the destination
(typically the Traffic Control Center TCC) through the Road Side Unit (RSU)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01497</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01497</id><created>2015-02-05</created><authors><author><keyname>Teijeiro</keyname><forenames>Tom&#xe1;s</forenames></author><author><keyname>F&#xe9;lix</keyname><forenames>Paulo</forenames></author><author><keyname>Presedo</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>Using temporal abduction for biosignal interpretation: A case study on
  QRS detection</title><categories>cs.AI</categories><comments>7 pages, Healthcare Informatics (ICHI), 2014 IEEE International
  Conference on</comments><doi>10.1109/ICHI.2014.52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose an abductive framework for biosignal interpretation,
based on the concept of Temporal Abstraction Patterns. A temporal abstraction
pattern defines an abstraction relation between an observation hypothesis and a
set of observations constituting its evidence support. New observations are
generated abductively from any subset of the evidence of a pattern, building an
abstraction hierarchy of observations in which higher levels contain those
observations with greater interpretative value of the physiological processes
underlying a given signal. Non-monotonic reasoning techniques have been applied
to this model in order to find the best interpretation of a set of initial
observations, permitting even to correct these observations by removing, adding
or modifying them in order to make them consistent with the available domain
knowledge. Some preliminary experiments have been conducted to apply this
framework to a well known and bounded problem: the QRS detection on ECG
signals. The objective is not to provide a new better QRS detector, but to test
the validity of an abductive paradigm. These experiments show that a knowledge
base comprising just a few very simple rhythm abstraction patterns can enhance
the results of a state of the art algorithm by significantly improving its
detection F1-score, besides proving the ability of the abductive framework to
correct both sensitivity and specificity failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01504</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01504</id><created>2015-02-05</created><updated>2015-02-10</updated><authors><author><keyname>Carboni</keyname><forenames>Davide</forenames></author></authors><title>Feedback based Reputation on top of the Bitcoin Blockchain</title><categories>cs.CR</categories><comments>10 pages, 4 figures, 20 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to assess the reputation of a member in a web community is a need
addressed in many different ways according to the many different stages in
which the nature of communities has evolved over time. In the case of
reputation of goods/services suppliers, the solutions available to prevent the
feedback abuse are generally reliable but centralized under the control of few
big Internet companies. In this paper we show how a decentralized and
distributed feedback management system can be built on top of the Bitcoin
blockchain
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01509</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01509</id><created>2015-02-05</created><authors><author><keyname>Coti</keyname><forenames>Camille</forenames></author><author><keyname>Greneche</keyname><forenames>Nicolas</forenames></author></authors><title>OS-level Failure Injection with SystemTap</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Failure injection in distributed systems has been an important issue to
experiment with robust, resilient distributed systems. In order to reproduce
real-life conditions, parts of the application must be killed without letting
the operating system close the existing network communications in a &quot;clean&quot;
way. When a process is simply killed, the OS closes them. SystemTap is a an
infrastructure that probes the Linux kernel's internal calls. If processes are
killed at kernel-level, they can be destroyed without letting the OS do
anything else. In this paper, we present a kernel-level failure injection
system based on SystemTap. We present how it can be used to implement
deterministic and probabilistic failure scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01514</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01514</id><created>2015-02-05</created><authors><author><keyname>McClatchey</keyname><forenames>R.</forenames></author></authors><title>Facilitating Evolution during Design and Implementation</title><categories>cs.SE</categories><comments>6 paqes; 3 figures in Kunstliche Intelligenz AI Market 2014</comments><acm-class>H.2.8</acm-class><doi>10.1007/s13218-014-0324-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The volumes and complexity of data that companies need to handle are
increasing at an accelerating rate. In order to compete effectively and ensure
their commercial sustainability, it is becoming crucial for them to achieve
robust traceability in both their data and the evolving designs of their
systems. This is addressed by the CRISTAL software which was originally
developed at CERN by UWE, Bristol, for one of the particle detectors at the
Large Hadron Collider, and has been subsequently transferred into the
commercial world. Companies have been able to demonstrate increased agility,
generate additional revenue, and improve the efficiency and cost-effectiveness
with which they develop and implement systems in various areas, including
business process management (BPM), healthcare and accounting applications.
CRISTALs ability to manage data and its provenance at the terabyte scale, with
full traceability over extended timescales, together with its
description-driven approach, has provided the flexible adaptability required to
future proof dynamically evolving software for these businesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01523</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01523</id><created>2015-02-05</created><updated>2015-02-16</updated><authors><author><keyname>Lin</keyname><forenames>Min Chih</forenames></author><author><keyname>Mizrahi</keyname><forenames>Michel J.</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>Efficient and Perfect domination on circular-arc graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G = (V,E)$, a \emph{perfect dominating set} is a subset of
vertices $V' \subseteq V(G)$ such that each vertex $v \in V(G)\setminus V'$ is
dominated by exactly one vertex $v' \in V'$. An \emph{efficient dominating set}
is a perfect dominating set $V'$ where $V'$ is also an independent set. These
problems are usually posed in terms of edges instead of vertices. Both
problems, either for the vertex or edge variant, remains NP-Hard, even when
restricted to certain graphs families. We study both variants of the problems
for the circular-arc graphs, and show efficient algorithms for all of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01526</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01526</id><created>2015-02-05</created><authors><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Shen</keyname><forenames>Jie</forenames></author></authors><title>Object Proposal via Partial Re-ranking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object proposals are an ensemble of bounding boxes with high potential to
contain objects. Usually, the ranking models are utilized in order to provide a
manageable number of candidate boxes. To obtain the rank for each candidate,
prior ranking models generally compare each pair of candidates. However, one
may be interested in only the top-$k$ candidates rather than all ones. Thus, in
this paper, we propose a new ranking model for object proposals, which aims to
produce a reliable estimation for only the top-$k$ candidates. To this end, we
compute the IoU for each candidate and split the candidates into two subsets
consisting of the top-$k$ candidates and the others respectively. Partial
ranking constraints are imposed on the two subsets: any candidate from the
first subset is better than that from the second one. In this way, the
constraints are reduced dramatically compared to the full ranking model, which
further facilitates an efficient learning procedure. Moreover, we show that our
partial ranking model can be reduced into the large margin based framework.
Extensive experiments demonstrate that after a re-ranking step of our model,
the top-$k$ detection rate can be significantly improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01538</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01538</id><created>2015-02-05</created><updated>2015-10-23</updated><authors><author><keyname>Johnson</keyname><forenames>Aaron M.</forenames></author><author><keyname>Burden</keyname><forenames>Samuel A.</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>A Hybrid Systems Model for Simple Manipulation and Self-Manipulation
  Systems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rigid bodies, plastic impact, persistent contact, Coulomb friction, and
massless limbs are ubiquitous simplifications introduced to reduce the
complexity of mechanics models despite the obvious physical inaccuracies that
each incurs individually. In concert, it is well known that the interaction of
such idealized approximations can lead to conflicting and even paradoxical
results. As robotics modeling moves from the consideration of isolated
behaviors to the analysis of tasks requiring their composition, a
mathematically tractable framework for building models that combine these
simple approximations yet achieve reliable results is overdue. In this paper we
present a formal hybrid dynamical system model that introduces suitably
restricted compositions of these familiar abstractions with the guarantee of
consistency analogous to global existence and uniqueness in classical dynamical
systems. The hybrid system developed here provides a discontinuous but
self-consistent approximation to the continuous (though possibly very stiff and
fast) dynamics of a physical robot undergoing intermittent impacts. The
modeling choices sacrifice some quantitative numerical efficiencies while
maintaining qualitatively correct and analytically tractable results with
consistency guarantees promoting their use in formal reasoning about mechanism,
feedback control, and behavior design in robots that make and break contact
with their environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01539</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01539</id><created>2015-02-05</created><authors><author><keyname>Hasham</keyname><forenames>Khawar</forenames></author><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Scientific Workflow Repeatability through Cloud-Aware Provenance</title><categories>cs.DB</categories><comments>6 pages; 5 figures; 3 tables in Proceedings of the Recomputability
  2014 workshop of the 7th IEEE/ACM International Conference on Utility and
  Cloud Computing (UCC 2014). London December 2014</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transformations, analyses and interpretations of data in scientific
workflows are vital for the repeatability and reliability of scientific
workflows. This provenance of scientific workflows has been effectively carried
out in Grid based scientific workflow systems. However, recent adoption of
Cloud-based scientific workflows present an opportunity to investigate the
suitability of existing approaches or propose new approaches to collect
provenance information from the Cloud and to utilize it for workflow
repeatability in the Cloud infrastructure. The dynamic nature of the Cloud in
comparison to the Grid makes it difficult because resources are provisioned
on-demand unlike the Grid. This paper presents a novel approach that can assist
in mitigating this challenge. This approach can collect Cloud infrastructure
information along with workflow provenance and can establish a mapping between
them. This mapping is later used to re-provision resources on the Cloud. The
repeatability of the workflow execution is performed by: (a) capturing the
Cloud infrastructure information (virtual machine configuration) along with the
workflow provenance, and (b) re-provisioning the similar resources on the Cloud
and re-executing the workflow on them. The evaluation of an initial prototype
suggests that the proposed approach is feasible and can be investigated
further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01540</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01540</id><created>2015-02-05</created><authors><author><keyname>Xu</keyname><forenames>Xun</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Semantic Embedding Space for Zero-Shot Action Recognition</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of categories for action recognition is growing rapidly. It is
thus becoming increasingly hard to collect sufficient training data to learn
conventional models for each category. This issue may be ameliorated by the
increasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework a
mapping is constructed between visual features and a human interpretable
semantic description of each category, allowing categories to be recognised in
the absence of any training data. Existing ZSL studies focus primarily on image
data, and attribute-based semantic representations. In this paper, we address
zero-shot recognition in contemporary video action recognition tasks, using
semantic word vector space as the common space to embed videos and category
labels. This is more challenging because the mapping between the semantic space
and space-time features of videos containing complex actions is more complex
and harder to learn. We demonstrate that a simple self-training and data
augmentation strategy can significantly improve the efficacy of this mapping.
Experiments on human action datasets including HMDB51 and UCF101 demonstrate
that our approach achieves the state-of-the-art zero-shot action recognition
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01545</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01545</id><created>2015-02-05</created><authors><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>Kovacs</keyname><forenames>Zsolt</forenames></author><author><keyname>Consortium</keyname><forenames>the CRISTAL-ISE</forenames></author></authors><title>Designing Traceability into Big Data Systems</title><categories>cs.DB</categories><comments>10 pages; 6 figures in Proceedings of the 5th Annual International
  Conference on ICT: Big Data, Cloud and Security (ICT-BDCS 2015), Singapore
  July 2015. arXiv admin note: text overlap with arXiv:1402.5764,
  arXiv:1402.5753</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing an appropriate level of accessibility and traceability to data or
process elements (so-called Items) in large volumes of data, often
Cloud-resident, is an essential requirement in the Big Data era.
Enterprise-wide data systems need to be designed from the outset to support
usage of such Items across the spectrum of business use rather than from any
specific application view. The design philosophy advocated in this paper is to
drive the design process using a so-called description-driven approach which
enriches models with meta-data and description and focuses the design process
on Item re-use, thereby promoting traceability. Details are given of the
description-driven design of big data systems at CERN, in health informatics
and in business process management. Evidence is presented that the approach
leads to design simplicity and consequent ease of management thanks to loose
typing and the adoption of a unified approach to Item management and usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01556</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01556</id><created>2015-02-05</created><authors><author><keyname>Arshad</keyname><forenames>Bilal</forenames></author><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Liaquat</keyname><forenames>Saad</forenames></author></authors><title>Position Paper: Provenance Data Visualisation for Neuroimaging Analysis</title><categories>cs.DB</categories><comments>6 pages; 6 figures in Proceedings of the 12th International
  Conference on Frontiers of Information Technology (FIT 2014) Islamabad,
  Pakistan December 2014</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualisation facilitates the understanding of scientific data both through
exploration and explanation of visualised data. Provenance contributes to the
understanding of data by containing the contributing factors behind a result.
With the significant increase in data volumes and algorithm complexity,
clinical researchers are struggling with information tracking, analysis
reproducibility and the verification of scientific output. Data coming from
various heterogeneous sources (multiple sources with varying level of trust) in
a collaborative environment adds to the uncertainty of the scientific output.
Systems are required that offer provenance data capture and visualisation
support for analyses. We present an account for the need to visualise
provenance information in order to aid the process of verification of
scientific outputs, comparison of analyses,progression and evolution of results
for neuroimaging analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01563</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01563</id><created>2015-02-05</created><authors><author><keyname>Frandi</keyname><forenames>Emanuele</forenames></author><author><keyname>Nanculef</keyname><forenames>Ricardo</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>A PARTAN-Accelerated Frank-Wolfe Algorithm for Large-Scale SVM
  Classification</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frank-Wolfe algorithms have recently regained the attention of the Machine
Learning community. Their solid theoretical properties and sparsity guarantees
make them a suitable choice for a wide range of problems in this field. In
addition, several variants of the basic procedure exist that improve its
theoretical properties and practical performance. In this paper, we investigate
the application of some of these techniques to Machine Learning, focusing in
particular on a Parallel Tangent (PARTAN) variant of the FW algorithm that has
not been previously suggested or studied for this type of problems. We provide
experiments both in a standard setting and using a stochastic speed-up
technique, showing that the considered algorithms obtain promising results on
several medium and large-scale benchmark datasets for SVM classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01566</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01566</id><created>2015-02-05</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>de Oliveira</keyname><forenames>R. C.</forenames></author></authors><title>A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths
  N=4 (mod 8)</title><categories>cs.DS cs.DM</categories><comments>6 pages, 2 figures, 2 tables. Conference: XXVII Simposio Brasileiro
  de Telecomunicacoes - SBrT'09, 2009, Blumenau, SC, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General guidelines for a new fast computation of blocklength 8m+4 DFTs are
presented, which is based on a Laurent series involving matrices. Results of
non-trivial real multiplicative complexity are presented for blocklengths N=64,
achieving lower multiplication counts than previously published FFTs. A
detailed description for the cases m=1 and m=2 is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01567</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01567</id><created>2015-02-05</created><updated>2015-04-21</updated><authors><author><keyname>L&#xe1;zaro</keyname><forenames>Francisco</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Bauch</keyname><forenames>Gerhard</forenames></author></authors><title>On The Weight Distribution of Fixed-Rate Raptor Codes</title><categories>cs.IT math.IT</categories><comments>This paper will be presented at the 2015 IEEE International Symposium
  on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper Raptor code ensembles with linear random precodes in a
fixed-rate setting are considered. An expression for the average distance
spectrum is derived and this expression is used to obtain the asymptotic
exponent of the weight distribution. The asymptotic growth rate analysis is
then exploited to develop a necessary and sufficient condition under which the
fixed-rate Raptor code ensemble exhibits a strictly positive typical minimum
distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01570</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01570</id><created>2015-02-05</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Lins</keyname><forenames>R. D.</forenames></author></authors><title>Taylor Series as Wide-sense Biorthogonal Wavelet Decomposition</title><categories>math.CA cs.IT math-ph math.IT math.MP</categories><comments>6 pages, 4 figures. conference: XXII Simposio Brasileiro de
  Telecomunicacoes, SBrT'05, 2005, Campinas, SP, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pointwise-supported generalized wavelets are introduced, based on Dirac,
doublet and further derivatives of delta. A generalized biorthogonal analysis
leads to standard Taylor series and new Dual-Taylor series that may be
interpreted as Laurent Schwartz distributions. A Parseval-like identity is also
derived for Taylor series, showing that Taylor series support an energy
theorem. New representations for signals called derivagrams are introduced,
which are similar to spectrograms. This approach corroborates the impact of
wavelets in modern signal analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01601</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01601</id><created>2015-02-05</created><authors><author><keyname>Feng</keyname><forenames>Ling</forenames></author><author><keyname>Monterola</keyname><forenames>Christopher Pineda</forenames></author><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author></authors><title>A Simplified Self-Consistent Probabilities Framework to Characterize
  Percolation Phenomena on Interdependent Networks : An Overview</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interdependent networks are ubiquitous in our society, ranging from
infrastructure to economics, and the study of their cascading behaviors using
percolation theory has attracted much attention in the recent years. To analyze
the percolation phenomena of these systems, different mathematical frameworks
have been proposed including generating functions, eigenvalues among some
others. These different frameworks approach the phase transition behaviors from
different angles, and have been very successful in shaping the different
quantities of interest including critical threshold, size of the giant
component, order of phase transition and the dynamics of cascading. These
methods also vary in their mathematical complexity in dealing with
interdependent networks that have additional complexity in terms of the
correlation among different layers of networks or links. In this work, we
review a particular approach of simple self-consistent probability equations,
and illustrate that it can greatly simplify the mathematical analysis for
systems ranging from single layer network to various different interdependent
networks. We give an overview on the detailed framework to study the nature of
the critical phase transition, value of the critical threshold and size of the
giant component for these different systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01602</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01602</id><created>2015-02-05</created><updated>2015-05-21</updated><authors><author><keyname>Bel&#xe1;k</keyname><forenames>V&#xe1;clav</forenames></author><author><keyname>Mashhadi</keyname><forenames>Afra</forenames></author><author><keyname>Sala</keyname><forenames>Alessandra</forenames></author><author><keyname>Morrison</keyname><forenames>Donn</forenames></author></authors><title>Phantom cascades: The effect of hidden nodes on information diffusion</title><categories>cs.SI physics.soc-ph</categories><comments>Preprint submitted to Elsevier Computer Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on information diffusion generally assumes complete knowledge of the
underlying network. However, in the presence of factors such as increasing
privacy awareness, restrictions on application programming interfaces (APIs)
and sampling strategies, this assumption rarely holds in the real world which
in turn leads to an underestimation of the size of information cascades. In
this work we study the effect of hidden network structure on information
diffusion processes. We characterise information cascades through activation
paths traversing visible and hidden parts of the network. We quantify diffusion
estimation error while varying the amount of hidden structure in five empirical
and synthetic network datasets and demonstrate the effect of topological
properties on this error. Finally, we suggest practical recommendations for
practitioners and propose a model to predict the cascade size with minimal
information regarding the underlying network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01609</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01609</id><created>2015-02-05</created><updated>2015-02-12</updated><authors><author><keyname>W&#xfc;chner</keyname><forenames>Tobias</forenames></author><author><keyname>Ochoa</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>Pretschner</keyname><forenames>Alexander</forenames></author></authors><title>Robust and Effective Malware Detection through Quantitative Data Flow
  Graph Metrics</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel malware detection approach based on metrics over
quantitative data flow graphs. Quantitative data flow graphs (QDFGs) model
process behavior by interpreting issued system calls as aggregations of
quantifiable data flows.Due to the high abstraction level we consider QDFG
metric based detection more robust against typical behavior obfuscation like
bogus call injection or call reordering than other common behavioral models
that base on raw system calls. We support this claim with experiments on
obfuscated malware logs and demonstrate the superior obfuscation robustness in
comparison to detection using n-grams. Our evaluations on a large and diverse
data set consisting of about 7000 malware and 500 goodware samples show an
average detection rate of 98.01% and a false positive rate of 0.48%. Moreover,
we show that our approach is able to detect new malware (i.e. samples from
malware families not included in the training set) and that the consideration
of quantities in itself significantly improves detection precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01621</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01621</id><created>2015-02-05</created><authors><author><keyname>Mondal</keyname><forenames>Bishwarup</forenames></author><author><keyname>Thomas</keyname><forenames>Timothy A.</forenames></author><author><keyname>Visotsky</keyname><forenames>Eugene</forenames></author><author><keyname>Vook</keyname><forenames>Frederick W.</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author><author><keyname>Nam</keyname><forenames>Young-Han</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Charlie</forenames></author><author><keyname>Zhang</keyname><forenames>Min</forenames></author><author><keyname>Luo</keyname><forenames>Qinglin</forenames></author><author><keyname>Kakishima</keyname><forenames>Yuichi</forenames></author><author><keyname>Kitao</keyname><forenames>Koshiro</forenames></author></authors><title>3D Channel Model in 3GPP</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-antenna techniques capable of exploiting the elevation dimension are
anticipated to be an important air-interface enhancement targeted to handle the
expected growth in mobile traffic. In order to enable the development and
evaluation of such multi-antenna techniques, the 3rd generation partnership
project (3GPP) has recently developed a 3-dimensional (3D) channel model. The
existing 2-dimensional (2D) channel models do not capture the elevation channel
characteristics lending them insufficient for such studies. This article
describes the main components of the newly developed 3D channel model and the
motivations behind introducing them. One key aspect is the ability to model
channels for users located on different floors of a building (at different
heights). This is achieved by capturing a user height dependency in modelling
some channel characteristics including pathloss, line-of-sight (LOS)
probability, etc. In general this 3D channel model follows the framework of
WINNERII/WINNER+ while also extending the applicability and the accuracy of the
model by introducing some height and distance dependent elevation related
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01625</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01625</id><created>2015-02-05</created><updated>2015-02-09</updated><authors><author><keyname>Hokke</keyname><forenames>Olivier</forenames></author><author><keyname>Kolpa</keyname><forenames>Alex</forenames></author><author><keyname>Oever</keyname><forenames>Joris van den</forenames></author><author><keyname>Walterbos</keyname><forenames>Alex</forenames></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames></author></authors><title>A Self-Compiling Android Data Obfuscation Tool</title><categories>cs.CR</categories><comments>Johan Pouwelse was the Course Supervisor</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smartphones are becoming more significant in storing and transferring data.
However, techniques ensuring this data is not compromised after a confiscation
of the device are not readily available. DroidStealth is an open source Android
application which combines data encryption and application obfuscation
techniques to provide users with a way to securely hide content on their
smartphones. This includes hiding the application's default launch methods and
providing methods such as dial-to-launch or invisible launch buttons. A novel
technique provided by DroidStealth is the ability to transform its appearance
to be able to hide in plain sight on devices. To achieve this, it uses
self-compilation, without requiring any special permissions. This Two-Layer
protection aims to protect the user and its data from casual search in various
situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01632</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01632</id><created>2015-02-05</created><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author></authors><title>A Simple Expression for Mill's Ratio of the Student's $t$-Distribution</title><categories>cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I show a simple expression of the Mill's ratio of the Student's
t-Distribution. I use it to prove Conjecture 1 in P. Auer, N. Cesa-Bianchi, and
P. Fischer. Finite-time analysis of the multiarmed bandit problem. Mach.
Learn., 47(2-3):235--256, May 2002.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01633</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01633</id><created>2015-02-05</created><authors><author><keyname>Gramoli</keyname><forenames>Vincent</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author><author><keyname>Shang</keyname><forenames>Di</forenames></author></authors><title>A Concurrency-Optimal List-Based Set</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a highly concurrent data structure is an important challenge that
is not easy to meet. As we show in this paper, even for a data structure as
simple as a linked list used to implement the set type, the most efficient
algorithms known so far may reject correct concurrent schedules.
  We propose a new algorithm based on a versioned try-lock that we show to
achieve optimal concurrency: it only rejects concurrent schedules that violate
correctness of the implemented type. We show empirically that reaching
optimality does not induce a significant overhead. In fact, our implementation
of the optimal algorithm outperforms both the Lazy Linked List and the
Harris-Michael state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01643</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01643</id><created>2015-02-05</created><authors><author><keyname>Mariotti</keyname><forenames>Letizia</forenames></author><author><keyname>Devaney</keyname><forenames>Nicholas</forenames></author></authors><title>Performance Analysis of Cone Detection Algorithms</title><categories>physics.med-ph cs.CV</categories><comments>13 pages, 7 figures, 2 tables</comments><doi>10.1364/JOSAA.32.000497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms have been proposed to help clinicians evaluate cone density
and spacing, as these may be related to the onset of retinal diseases. However,
there has been no rigorous comparison of the performance of these algorithms.
In addition, the performance of such algorithms is typically determined by
comparison with human observers. Here we propose a technique to simulate
realistic images of the cone mosaic. We use the simulated images to test the
performance of two popular cone detection algorithms and we introduce an
algorithm which is used by astronomers to detect stars in astronomical images.
We use Free Response Operating Characteristic (FROC) curves to evaluate and
compare the performance of the three algorithms. This allows us to optimize the
performance of each algorithm. We observe that performance is significantly
enhanced by up-sampling the images. We investigate the effect of noise and
image quality on cone mosaic parameters estimated using the different
algorithms, finding that the estimated regularity is the most sensitive
parameter.
  This paper was published in JOSA A and is made available as an electronic
reprint with the permission of OSA. The paper can be found at the following URL
on the OSA website: http://www.opticsinfobase.org/abstract.cfm?msid=224577.
Systematic or multiple reproduction or distribution to multiple locations via
electronic or other means is prohibited and is subject to penalties under law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01646</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01646</id><created>2015-02-05</created><updated>2015-03-21</updated><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>K.</forenames></author><author><keyname>Malarz</keyname><forenames>K.</forenames></author><author><keyname>Krawczyk</keyname><forenames>M. J.</forenames></author></authors><title>Heavy context dependence---decisions of underground soldiers</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 5 figures, for the European Conference on Modelling and
  Simulation (ECMS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An attempt is made to simulate the disclosure of underground soldiers in
terms of theory of networks. The coupling mechanism between the network nodes
is the possibility that a disclosed soldier is going to disclose also his
acquaintances. We calculate the fraction of disclosed soldiers as dependent on
the fraction of those who, once disclosed, reveal also their colleagues. The
simulation is immersed in the historical context of the Polish Home Army under
the communist rule in 1946-49.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01653</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01653</id><created>2015-02-05</created><authors><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author></authors><title>In an Uncertain World: Distributed Optimization in MIMO Systems with
  Imperfect Information</title><categories>cs.IT math.IT</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a distributed algorithm that optimizes the
Gaussian signal covariance matrices of multi-antenna users transmitting to a
common multi-antenna receiver under imperfect and possibly delayed channel
state information. The algorithm is based on an extension of exponential
learning techniques to a semidefinite setting and it requires the same
information as distributed water-filling methods. Unlike water-filling however,
the proposed matrix exponential learning (MXL) algorithm converges to the
system's optimum signal covariance profile under very mild conditions on the
channel uncertainty statistics; moreover, the algorithm retains its convergence
properties even in the presence of user update asynchronicities, random delays
and/or ergodically changing channel conditions. In particular, by properly
tuning the algorithm's learning rate (or step size), the algorithm converges
within a few iterations, even for large numbers of users and/or antennas per
user. Our theoretical analysis is complemented by numerical simulations which
illustrate the algorithm's robustness and scalability in realistic network
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01657</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01657</id><created>2015-02-05</created><authors><author><keyname>Fleder</keyname><forenames>Michael</forenames></author><author><keyname>Kester</keyname><forenames>Michael S.</forenames></author><author><keyname>Pillai</keyname><forenames>Sudeep</forenames></author></authors><title>Bitcoin Transaction Graph Analysis</title><categories>cs.CR cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoins have recently become an increasingly popular cryptocurrency through
which users trade electronically and more anonymously than via traditional
electronic transfers. Bitcoin's design keeps all transactions in a public
ledger. The sender and receiver for each transaction are identified only by
cryptographic public-key ids. This leads to a common misconception that it
inherently provides anonymous use. While Bitcoin's presumed anonymity offers
new avenues for commerce, several recent studies raise user-privacy concerns.
We explore the level of anonymity in the Bitcoin system. Our approach is
two-fold: (i) We annotate the public transaction graph by linking bitcoin
public keys to &quot;real&quot; people - either definitively or statistically. (ii) We
run the annotated graph through our graph-analysis framework to find and
summarize activity of both known and unknown users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01659</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01659</id><created>2015-02-05</created><authors><author><keyname>Pillai</keyname><forenames>Sudeep</forenames></author><author><keyname>Walter</keyname><forenames>Matthew R.</forenames></author><author><keyname>Teller</keyname><forenames>Seth</forenames></author></authors><title>Learning Articulated Motions From Visual Demonstration</title><categories>cs.RO cs.CV</categories><comments>Published in Robotics: Science and Systems X, Berkeley, CA. ISBN:
  978-0-9923747-0-9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many functional elements of human homes and workplaces consist of rigid
components which are connected through one or more sliding or rotating
linkages. Examples include doors and drawers of cabinets and appliances;
laptops; and swivel office chairs. A robotic mobile manipulator would benefit
from the ability to acquire kinematic models of such objects from observation.
This paper describes a method by which a robot can acquire an object model by
capturing depth imagery of the object as a human moves it through its range of
motion. We envision that in future, a machine newly introduced to an
environment could be shown by its human user the articulated objects particular
to that environment, inferring from these &quot;visual demonstrations&quot; enough
information to actuate each object independently of the user.
  Our method employs sparse (markerless) feature tracking, motion segmentation,
component pose estimation, and articulation learning; it does not require prior
object models. Using the method, a robot can observe an object being exercised,
infer a kinematic model incorporating rigid, prismatic and revolute joints,
then use the model to predict the object's motion from a novel vantage point.
We evaluate the method's performance, and compare it to that of a previously
published technique, for a variety of household objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01664</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01664</id><created>2015-02-05</created><authors><author><keyname>Evans</keyname><forenames>Lewis P. G.</forenames></author><author><keyname>Adams</keyname><forenames>Niall M.</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Christoforos</forenames></author></authors><title>Estimating Optimal Active Learning via Model Retraining Improvement</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.8042</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central question for active learning (AL) is: &quot;what is the optimal
selection?&quot; Defining optimality by classifier loss produces a new
characterisation of optimal AL behaviour, by treating expected loss reduction
as a statistical target for estimation. This target forms the basis of model
retraining improvement (MRI), a novel approach providing a statistical
estimation framework for AL. This framework is constructed to address the
central question of AL optimality, and to motivate the design of estimation
algorithms. MRI allows the exploration of optimal AL behaviour, and the
examination of AL heuristics, showing precisely how they make sub-optimal
selections. The abstract formulation of MRI is used to provide a new guarantee
for AL, that an unbiased MRI estimator should outperform random selection. This
MRI framework reveals intricate estimation issues that in turn motivate the
construction of new statistical AL algorithms. One new algorithm in particular
performs strongly in a large-scale experimental study, compared to standard AL
methods. This competitive performance suggests that practical efforts to
minimise estimation bias may be important for AL applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01682</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01682</id><created>2015-02-05</created><authors><author><keyname>Baker</keyname><forenames>Kathryn</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Dorr</keyname><forenames>Bonnie J.</forenames></author><author><keyname>Callison-Burch</keyname><forenames>Chris</forenames></author><author><keyname>Filardo</keyname><forenames>Nathaniel W.</forenames></author><author><keyname>Piatko</keyname><forenames>Christine</forenames></author><author><keyname>Levin</keyname><forenames>Lori</forenames></author><author><keyname>Miller</keyname><forenames>Scott</forenames></author></authors><title>Use of Modality and Negation in Semantically-Informed Syntactic MT</title><categories>cs.CL cs.LG stat.ML</categories><comments>28 pages, 13 figures, 2 tables; appeared in Computational
  Linguistics, 38(2):411-438, 2012</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>Computational Linguistics, 38(2):411-438, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the resource- and system-building efforts of an
eight-week Johns Hopkins University Human Language Technology Center of
Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on
Semantically-Informed Machine Translation (SIMT). We describe a new
modality/negation (MN) annotation scheme, the creation of a (publicly
available) MN lexicon, and two automated MN taggers that we built using the
annotation scheme and lexicon. Our annotation scheme isolates three components
of modality and negation: a trigger (a word that conveys modality or negation),
a target (an action associated with modality or negation) and a holder (an
experiencer of modality). We describe how our MN lexicon was semi-automatically
produced and we demonstrate that a structure-based MN tagger results in
precision around 86% (depending on genre) for tagging of a standard LDC data
set.
  We apply our MN annotation scheme to statistical machine translation using a
syntactic framework that supports the inclusion of semantic annotations.
Syntactic tags enriched with semantic annotations are assigned to parse trees
in the target-language training texts through a process of tree grafting. While
the focus of our work is modality and negation, the tree grafting procedure is
general and supports other types of semantic information. We exploit this
capability by including named entities, produced by a pre-existing tagger, in
addition to the MN elements produced by the taggers described in this paper.
The resulting system significantly outperformed a linguistically naive baseline
model (Hiero), and reached the highest scores yet reported on the NIST 2009
Urdu-English test set. This finding supports the hypothesis that both syntactic
and semantic information can improve translation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01687</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01687</id><created>2015-02-05</created><authors><author><keyname>Lamm</keyname><forenames>Sebastian</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Graph Partitioning for Independent Sets</title><categories>cs.DS cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing maximum independent sets in graphs is an important problem in
computer science. In this paper, we develop an evolutionary algorithm to tackle
the problem. The core innovations of the algorithm are very natural combine
operations based on graph partitioning and local search algorithms. More
precisely, we employ a state-of-the-art graph partitioner to derive operations
that enable us to quickly exchange whole blocks of given independent sets. To
enhance newly computed offsprings we combine our operators with a local search
algorithm. Our experimental evaluation indicates that we are able to outperform
state-of-the-art algorithms on a variety of instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01694</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01694</id><created>2015-02-05</created><authors><author><keyname>Prelee</keyname><forenames>Matthew A.</forenames></author><author><keyname>Neuhoff</keyname><forenames>David L.</forenames></author></authors><title>Multidimensional Manhattan Sampling and Reconstruction</title><categories>cs.IT math.IT</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Manhattan sampling in two and higher dimensions, and
proves sampling theorems. In two dimensions, Manhattan sampling, which takes
samples densely along a Manhattan grid of lines, can be viewed as sampling on
the union of two rectangular lattices, one dense horizontally, being a multiple
of the fine spacing of the other. The sampling theorem shows that images
bandlimited to the union of the Nyquist regions of the two rectangular lattices
can be recovered from their Manhattan samples, and an efficient procedure for
doing so is given. Such recovery is possible even though there is overlap among
the spectral replicas induced by Manhattan sampling.
  In three and higher dimensions, there are many possible configurations for
Manhattan sampling, each consisting of the union of special rectangular
lattices called bi-step lattices. This paper identifies them, proves a sampling
theorem showing that images bandlimited to the union of the Nyquist regions of
the bi-step rectangular lattices are recoverable from Manhattan samples, and
presents an efficient onion-peeling procedure for doing so. Furthermore, it
develops a special representation for the bi-step lattices and an algebra with
nice properties. It is also shown that the set of reconstructable images is
maximal in the Landau sense.
  While most of the paper deals with continuous-space images, Manhattan
sampling of discrete-space images is also considered, for infinite, as well as
finite, support images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01699</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01699</id><created>2015-02-05</created><authors><author><keyname>Eren</keyname><forenames>Tolga</forenames></author></authors><title>Graph invariants for unique localizability in cooperative localization
  of wireless sensor networks: rigidity index and redundancy index</title><categories>cs.SY</categories><comments>13 pages, 7 figures, to be submitted for possible journal publication</comments><msc-class>05C90, 94C15</msc-class><acm-class>G.2.2; I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rigidity theory enables us to specify the conditions of unique localizability
in the cooperative localization problem of wireless sensor networks. This paper
presents a combinatorial rigidity approach to measure (i) generic rigidity and
(ii) generalized redundant rigidity properties of graph structures through
graph invariants for the localization problem in wireless sensor networks. We
define the rigidity index as a graph invariant based on independent set of
edges in the rigidity matroid. It has a value between 0 and 1, and it indicates
how close we are to rigidity. Redundant rigidity is required for global
rigidity, which is associated with unique realization of graphs. Moreover,
redundant rigidity also provides rigidity robustness in networked systems
against structural changes, such as link losses. Here, we give a broader
definition of redundant edge that we call the &quot;generalized redundant edge.&quot;
This definition of redundancy is valid for both rigid and non-rigid graphs.
Next, we define the redundancy index as a graph invariant based on generalized
redundant edges in the rigidity matroid. It also has a value between 0 and 1,
and it indicates the percentage of redundancy in a graph. These two indices
allow us to explore the transition from non-rigidity to rigidity and the
transition from rigidity to redundant rigidity. Examples on graphs are provided
to demonstrate this approach. From a sensor network point of view, these two
indices enable us to evaluate the effects of sensing radii of sensors on the
rigidity properties of networks, which in turn, allow us to examine the
localizability of sensor networks. We evaluate the required changes in sensing
radii for localizability by means of the rigidity index and the redundancy
index using random geometric graphs in simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01705</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01705</id><created>2015-02-05</created><authors><author><keyname>Zhao</keyname><forenames>Xiaozhao</forenames></author><author><keyname>Hou</keyname><forenames>Yuexian</forenames></author><author><keyname>Song</keyname><forenames>Dawei</forenames></author><author><keyname>Li</keyname><forenames>Wenjie</forenames></author></authors><title>A Confident Information First Principle for Parametric Reduction and
  Model Selection of Boltzmann Machines</title><categories>cs.LG stat.ML</categories><comments>16pages. arXiv admin note: substantial text overlap with
  arXiv:1302.3931</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical dimensionality reduction (DR) methods are often data-oriented,
focusing on directly reducing the number of random variables (features) while
retaining the maximal variations in the high-dimensional data. In unsupervised
situations, one of the main limitations of these methods lies in their
dependency on the scale of data features. This paper aims to address the
problem from a new perspective and considers model-oriented dimensionality
reduction in parameter spaces of binary multivariate distributions.
  Specifically, we propose a general parameter reduction criterion, called
Confident-Information-First (CIF) principle, to maximally preserve confident
parameters and rule out less confident parameters. Formally, the confidence of
each parameter can be assessed by its contribution to the expected Fisher
information distance within the geometric manifold over the neighbourhood of
the underlying real distribution.
  We then revisit Boltzmann machines (BM) from a model selection perspective
and theoretically show that both the fully visible BM (VBM) and the BM with
hidden units can be derived from the general binary multivariate distribution
using the CIF principle. This can help us uncover and formalize the essential
parts of the target density that BM aims to capture and the non-essential parts
that BM should discard. Guided by the theoretical analysis, we develop a
sample-specific CIF for model selection of BM that is adaptive to the observed
samples. The method is studied in a series of density estimation experiments
and has been shown effective in terms of the estimate accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01707</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01707</id><created>2015-02-05</created><authors><author><keyname>Savic</keyname><forenames>Trifun</forenames></author><author><keyname>Albijanic</keyname><forenames>Radoje</forenames></author></authors><title>CS reconstruction of the speech and musical signals</title><categories>cs.SD cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of Compressive sensing approach to the speech and musical
signals is considered in this paper. Compressive sensing (CS) is a new approach
to the signal sampling that allows signal reconstruction from a small set of
randomly acquired samples. This method is developed for the signals that
exhibit the sparsity in a certain domain. Here we have observed two sparsity
domains: discrete Fourier and discrete cosine transform domain. Furthermore,
two different types of audio signals are analyzed in terms of sparsity and CS
performance - musical and speech signals. Comparative analysis of the CS
reconstruction using different number of signal samples is performed in the two
domains of sparsity. It is shown that the CS can be successfully applied to
both, musical and speech signals, but the speech signals are more demanding in
terms of the number of observations. Also, our results show that discrete
cosine transform domain allows better reconstruction using lower number of
observations, compared to the Fourier transform domain, for both types of
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01708</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01708</id><created>2015-02-05</created><authors><author><keyname>Rigazzi</keyname><forenames>Giovanni</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Fantacci</keyname><forenames>Romano</forenames></author></authors><title>Aggregation and Trunking of M2M Traffic via D2D Connections</title><categories>cs.NI</categories><comments>This paper has been accepted for presentation at IEEE ICC 2015 -
  Mobile and Wireless Networking Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-to-Machine (M2M) communications is one of the key enablers of the
Internet of Things (IoT). Billions of devices are expected to be deployed in
the next future for novel M2M applications demanding ubiquitous access and
global connectivity. In order to cope with the massive number of machines,
there is a need for new techniques to coordinate the access and allocate the
resources. Although the majority of the proposed solutions are focused on the
adaptation of the traditional cellular networks to the M2M traffic patterns,
novel approaches based on the direct communication among nearby devices may
represent an effective way to avoid access congestion and cell overload. In
this paper, we propose a new strategy inspired by the classical Trunked Radio
Systems (TRS), exploiting the Device-to-Device (D2D) connectivity between
cellular users and Machine-Type Devices (MTDs). The aggregation of the locally
generated packets is performed by a user device, which aggregates the
machine-type data, supplements it with its own data and transmits all of them
to the Base Station. We observe a fundamental trade-off between latency and the
transmit power needed to deliver the aggregate traffic, in a sense that lower
latency requires increase in the transmit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01710</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01710</id><created>2015-02-05</created><updated>2015-09-08</updated><authors><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Text Understanding from Scratch</title><categories>cs.LG cs.CL</categories><comments>This technical report is superseded by a paper entitled
  &quot;Character-level Convolutional Networks for Text Classification&quot;,
  arXiv:1509.01626. It has considerably more experimental results and a
  rewritten introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article demontrates that we can apply deep learning to text
understanding from character-level inputs all the way up to abstract text
concepts, using temporal convolutional networks (ConvNets). We apply ConvNets
to various large-scale datasets, including ontology classification, sentiment
analysis, and text categorization. We show that temporal ConvNets can achieve
astonishing performance without the knowledge of words, phrases, sentences and
any other syntactic or semantic structures with regards to a human language.
Evidence shows that our models can work for both English and Chinese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01730</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01730</id><created>2015-02-05</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Pach</keyname><forenames>Janos</forenames></author><author><keyname>Suk</keyname><forenames>Andrew</forenames></author></authors><title>A polynomial regularity lemma for semi-algebraic hypergraphs and its
  applications in geometry and property testing</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fox, Gromov, Lafforgue, Naor, and Pach proved a regularity lemma for
semi-algebraic $k$-uniform hypergraphs of bounded complexity, showing that for
each $\epsilon&gt;0$ the vertex set can be equitably partitioned into a bounded
number of parts (in terms of $\epsilon$ and the complexity) so that all but an
$\epsilon$-fraction of the $k$-tuples of parts are homogeneous. We prove that
the number of parts can be taken to be polynomial in $1/\epsilon$. Our improved
regularity lemma can be applied to geometric problems and to the following
general question on property testing: is it possible to decide, with query
complexity polynomial in the reciprocal of the approximation parameter, whether
a hypergraph has a given hereditary property? We give an affirmative answer for
testing typical hereditary properties for semi-algebraic hypergraphs of bounded
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01733</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01733</id><created>2015-02-05</created><authors><author><keyname>Soufan</keyname><forenames>Othman</forenames></author><author><keyname>Arafat</keyname><forenames>Samer</forenames></author></authors><title>Arrhythmia Detection using Mutual Information-Based Integration Method</title><categories>cs.CE cs.LG</categories><comments>6 pages, 1 figure, 7 tables, WConSC 2011 conference
  http://www.ece.ualberta.ca/~reform/WConSC/ (2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to propose an application of mutual
information-based ensemble methods to the analysis and classification of heart
beats associated with different types of Arrhythmia. Models of multilayer
perceptrons, support vector machines, and radial basis function neural networks
were trained and tested using the MIT-BIH arrhythmia database. This research
brings a focus to an ensemble method that, to our knowledge, is a novel
application in the area of ECG Arrhythmia detection. The proposed classifier
ensemble method showed improved performance, relative to either majority voting
classifier integration or to individual classifier performance. The overall
ensemble accuracy was 98.25%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01734</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01734</id><created>2015-01-27</created><authors><author><keyname>Villani</keyname><forenames>Marco</forenames></author><author><keyname>Roli</keyname><forenames>Andrea</forenames></author><author><keyname>Filisetti</keyname><forenames>Alessandro</forenames></author><author><keyname>Fiorucci</keyname><forenames>Marco</forenames></author><author><keyname>Poli</keyname><forenames>Irene</forenames></author><author><keyname>Serra</keyname><forenames>Roberto</forenames></author></authors><title>The search for candidate relevant subsets of variables in complex
  systems</title><categories>q-bio.MN cs.IT math.IT</categories><comments>22 pages</comments><msc-class>37Fxx</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a method to identify &quot;relevant subsets&quot; of
variables, useful to understand the organization of a dynamical system. The
variables belonging to a relevant subset should have a strong integration with
the other variables of the same relevant subset, and a much weaker interaction
with the other system variables. On this basis, extending previous works on
neural networks, an information-theoretic measure is introduced, i.e. the
Dynamical Cluster Index, in order to identify good candidate relevant subsets.
The method does not require any previous knowledge of the relationships among
the system variables, but relies on observations of their values in time. We
show its usefulness in several application domains, including: (i) random
boolean networks, where the whole network is made of different subnetworks with
different topological relationships (independent or interacting subnetworks);
(ii) leader-follower dynamics, subject to noise and fluctuations; (iii)
catalytic reaction networks in a flow reactor; (iv) the MAPK signaling pathway
in eukaryotes. The validity of the method has been tested in cases where the
data are generated by a known dynamical model and the Dynamical Cluster Index
method is applied in order to uncover significant aspects of its organization;
however it is important to stress that it can also be applied to time series
coming from field data without any reference to a model. Given that it is based
on relative frequencies of sets of values, the method could be applied also to
cases where the data are not ordered in time. Several indications to improve
the scope and effectiveness of the Dynamical Cluster Index to analyze the
organization of complex systems are finally given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01753</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01753</id><created>2015-02-05</created><authors><author><keyname>Wittek</keyname><forenames>Peter</forenames></author><author><keyname>Dar&#xe1;nyi</keyname><forenames>S&#xe1;ndor</forenames></author><author><keyname>Kontopoulos</keyname><forenames>Efstratios</forenames></author><author><keyname>Moysiadis</keyname><forenames>Theodoros</forenames></author><author><keyname>Kompatsiaris</keyname><forenames>Ioannis</forenames></author></authors><title>Monitoring Term Drift Based on Semantic Consistency in an Evolving
  Vector Field</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>8 pages, 1 figure. Code used to conduct the experiments is available
  at https://github.com/peterwittek/concept_drifts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the Aristotelian concept of potentiality vs. actuality allowing for
the study of energy and dynamics in language, we propose a field approach to
lexical analysis. Falling back on the distributional hypothesis to
statistically model word meaning, we used evolving fields as a metaphor to
express time-dependent changes in a vector space model by a combination of
random indexing and evolving self-organizing maps (ESOM). To monitor semantic
drifts within the observation period, an experiment was carried out on the term
space of a collection of 12.8 million Amazon book reviews. For evaluation, the
semantic consistency of ESOM term clusters was compared with their respective
neighbourhoods in WordNet, and contrasted with distances among term vectors by
random indexing. We found that at 0.05 level of significance, the terms in the
clusters showed a high level of semantic consistency. Tracking the drift of
distributional patterns in the term space across time periods, we found that
consistency decreased, but not at a statistically significant level. Our method
is highly scalable, with interpretations in philosophy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01761</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01761</id><created>2015-02-05</created><authors><author><keyname>Lee</keyname><forenames>Tom</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Levinshtein</keyname><forenames>Alex</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author><author><keyname>Dickinson</keyname><forenames>Sven</forenames></author></authors><title>A Framework for Symmetric Part Detection in Cluttered Scenes</title><categories>cs.CV</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of symmetry in computer vision has waxed and waned in importance
during the evolution of the field from its earliest days. At first figuring
prominently in support of bottom-up indexing, it fell out of favor as shape
gave way to appearance and recognition gave way to detection. With a strong
prior in the form of a target object, the role of the weaker priors offered by
perceptual grouping was greatly diminished. However, as the field returns to
the problem of recognition from a large database, the bottom-up recovery of the
parts that make up the objects in a cluttered scene is critical for their
recognition. The medial axis community has long exploited the ubiquitous
regularity of symmetry as a basis for the decomposition of a closed contour
into medial parts. However, today's recognition systems are faced with
cluttered scenes, and the assumption that a closed contour exists, i.e. that
figure-ground segmentation has been solved, renders much of the medial axis
community's work inapplicable. In this article, we review a computational
framework, previously reported in Lee et al. (2013), Levinshtein et al. (2009,
2013), that bridges the representation power of the medial axis and the need to
recover and group an object's parts in a cluttered scene. Our framework is
rooted in the idea that a maximally inscribed disc, the building block of a
medial axis, can be modeled as a compact superpixel in the image. We evaluate
the method on images of cluttered scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01763</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01763</id><created>2015-02-05</created><authors><author><keyname>Ro&#x15f;ie</keyname><forenames>R&#x103;zvan</forenames></author></authors><title>Randomness of Spritz via DieHarder testing</title><categories>cs.CR</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RC4 is a stream cipher included in the TLS protocol, and widely used for
encrypting network traffic during the last decades. Spritz is a possible
candidate for replacing RC4. Spritz is based on a sponge construction and
preserves the byte-oriented behaviour existing in RC4, but introduces an
interface that provides encryption, hashing or MAC-generation functionalities.
We present here the results obtained after applying several statistical tests
on the keystreams generated by Spritz when used in the cipher mode. Our
methodology makes use of 1024 keystreams of 2^25 bits. The algorithm was tested
against the DieHarder test suite. None of the tests failed. Few tests produced
weak results that were corrected when the number of samples increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01779</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01779</id><created>2015-02-05</created><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>Cheong</keyname><forenames>Otfried</forenames></author><author><keyname>Dobbins</keyname><forenames>Michael Gene</forenames></author><author><keyname>Goaoc</keyname><forenames>Xavier</forenames></author></authors><title>The Number of Holes in the Union of Translates of a Convex Set in Three
  Dimensions</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the union of $n$ translates of a convex body in $\mathbb{R}^3$
can have $\Theta(n^3)$ holes in the worst case, where a hole in a set $X$ is a
connected component of $\mathbb{R}^3 \setminus X$. This refutes a 20-year-old
conjecture. As a consequence, we also obtain improved lower bounds on the
complexity of motion planning problems and of Voronoi diagrams with convex
distance functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01780</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01780</id><created>2015-02-05</created><authors><author><keyname>Kalogerias</keyname><forenames>Dionysios S.</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author></authors><title>Sequential Channel State Tracking &amp; SpatioTemporal Channel Prediction in
  Mobile Wireless Sensor Networks</title><categories>stat.AP cs.IT math.IT</categories><comments>Original paper submitted to the IEEE Transactions on Signal and
  Information Processing over Networks; 22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a nonlinear filtering framework for approaching the problems of
channel state tracking and spatiotemporal channel gain prediction in mobile
wireless sensor networks, in a Bayesian setting. We assume that the wireless
channel constitutes an observable (by the sensors/network nodes),
spatiotemporal, conditionally Gaussian stochastic process, which is
statistically dependent on a set of hidden channel parameters, called the
channel state. The channel state evolves in time according to a known, non
stationary, nonlinear and/or non Gaussian Markov stochastic kernel. This
formulation results in a partially observable system, with a temporally varying
global state and spatiotemporally varying observations. Recognizing the
intractability of general nonlinear state estimation, we advocate the use of
grid based approximate filters as an effective and robust means for recursive
tracking of the channel state. We also propose a sequential spatiotemporal
predictor for tracking the channel gains at any point in time and space,
providing real time sequential estimates for the respective channel gain map,
for each sensor in the network. Additionally, we show that both estimators
converge towards the true respective MMSE optimal estimators, in a common,
relatively strong sense. Numerical simulations corroborate the practical
effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01782</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01782</id><created>2015-02-05</created><authors><author><keyname>Carvajal</keyname><forenames>Johanna</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>McCool</keyname><forenames>Chris</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Multi-Action Recognition via Stochastic Modelling of Optical Flow and
  Gradients</title><categories>cs.CV</categories><acm-class>I.4.6; I.4.7; I.4.8; I.5.1; I.5.4</acm-class><journal-ref>Workshop on Machine Learning for Sensory Data Analysis (MLSDA),
  pp. 19-24, 2014</journal-ref><doi>10.1145/2689746.2689748</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel approach to multi-action recognition that
performs joint segmentation and classification. This approach models each
action using a Gaussian mixture using robust low-dimensional action features.
Segmentation is achieved by performing classification on overlapping temporal
windows, which are then merged to produce the final result. This approach is
considerably less complicated than previous methods which use dynamic
programming or computationally expensive hidden Markov models (HMMs). Initial
experiments on a stitched version of the KTH dataset show that the proposed
approach achieves an accuracy of 78.3%, outperforming a recent HMM-based
approach which obtained 71.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01783</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01783</id><created>2015-02-05</created><authors><author><keyname>Qian</keyname><forenames>Jing</forenames></author><author><keyname>Root</keyname><forenames>Jonathan</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Learning Efficient Anomaly Detectors from $K$-NN Graphs</title><categories>cs.LG stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1405.0530</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a non-parametric anomaly detection algorithm for high dimensional
data. We score each datapoint by its average $K$-NN distance, and rank them
accordingly. We then train limited complexity models to imitate these scores
based on the max-margin learning-to-rank framework. A test-point is declared as
an anomaly at $\alpha$-false alarm level if the predicted score is in the
$\alpha$-percentile. The resulting anomaly detector is shown to be
asymptotically optimal in that for any false alarm rate $\alpha$, its decision
region converges to the $\alpha$-percentile minimum volume level set of the
unknown underlying density. In addition, we test both the statistical
performance and computational efficiency of our algorithm on a number of
synthetic and real-data experiments. Our results demonstrate the superiority of
our algorithm over existing $K$-NN based anomaly detection algorithms, with
significant computational savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01801</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01801</id><created>2015-02-06</created><authors><author><keyname>Fan</keyname><forenames>Chuchu</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author></authors><title>Bounded Verification with On-the-Fly Discrepancy Computation</title><categories>cs.SY cs.NA math.NA</categories><comments>24 pages</comments><report-no>University of Illinois Urbana Champaign, Tech Report
  UILU-ENG-15-2201</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation-based verification algorithms can provide formal safety guarantees
for nonlinear and hybrid systems. The previous algorithms rely on user provided
model annotations called discrepancy function, which are crucial for computing
reachtubes from simulations. In this paper, we eliminate this requirement by
presenting an algorithm for computing piece-wise exponential discrepancy
functions. The algorithm relies on computing local convergence or divergence
rates of trajectories along a simulation using a coarse over-approximation of
the reach set and bounding the maximal eigenvalue of the Jacobian over this
over-approximation. The resulting discrepancy function preserves the soundness
and the relative completeness of the verification algorithm. We also provide a
coordinate transformation method to improve the local estimates for the
convergence or divergence rates in practical examples. We extend the method to
get the input-to-state discrepancy of nonlinear dynamical systems which can be
used for compositional analysis. Our experiments show that the approach is
effective in terms of running time for several benchmark problems, scales
reasonably to larger dimensional systems, and compares favorably with respect
to available tools for nonlinear models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01802</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01802</id><created>2015-02-06</created><updated>2015-04-14</updated><authors><author><keyname>Chan</keyname><forenames>T-H. Hubert</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Kang</keyname><forenames>Ning</forenames></author></authors><title>Online Convex Covering and Packing Problems</title><categories>cs.DS</categories><comments>Fixed an error in Theorem 3.2 together with its proof, and changed
  Corollary 3.1 accordingly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the online convex covering problem and online convex packing
problem. The (offline) convex covering problem is modeled by the following
convex program: $\min_{x \in R_+^n} f(x) \ \text{s.t}\ A x \ge 1$, where $f :
R_+^n \mapsto R_+$ is a monotone and convex cost function, and $A$ is an $m
\times n$ matrix with non-negative entries. Each row of the constraint matrix
$A$ corresponds to a covering constraint. In the online problem, each row of
$A$ comes online and the algorithm must maintain a feasible assignment $x$ and
may only increase $x$ over time. The (offline) convex packing problem is
modeled by the following convex program: $\max_{y\in R_+^m} \sum_{j = 1}^m y_j
- g(A^T y)$, where $g : R_+^n \mapsto R_+$ is a monotone and convex cost
function. It is the Fenchel dual program of convex covering when $g$ is the
convex conjugate of $f$. In the online problem, each variable $y_j$ arrives
online and the algorithm must decide the value of $y_j$ on its arrival.
  We propose simple online algorithms for both problems using the online primal
dual technique, and obtain nearly optimal competitive ratios for both problems
for the important special case of polynomial cost functions. For any convex
polynomial cost functions with non-negative coefficients and maximum degree
$\tau$, we introduce an $O(\tau \log n)^\tau$-competitive online convex
covering algorithm, and an $O(\tau)$-competitive online convex packing
algorithm, matching the known $\Omega(\tau \log n)^\tau$ and $\Omega(\tau)$
lower bounds respectively.
  There is a large family of online resource allocation problems that can be
modeled under this online convex covering and packing framework, including
online covering and packing problems (with linear objectives), online mixed
covering and packing, and online combinatorial auction. Our framework allows us
to study these problems using a unified approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01803</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01803</id><created>2015-02-06</created><authors><author><keyname>Abahmane</keyname><forenames>Omar</forenames></author><author><keyname>Binkkour</keyname><forenames>Mohamed</forenames></author></authors><title>Strategic and Operational information support of decision making
  processes and systems</title><categories>cs.CY</categories><comments>Proceedings of the Information Systems and Business Intelligence
  Conference, (SIIE-2008) Hammamet, Tunisia, February, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to present the different aspects and characteristics of
strategic and operational information and propose a categorization pattern
allowing to consider an information as strategic or operational. This
categorization is to be used in the two decision making processes to assist its
mining, and usage by the two related decision support systems. This is
conducted trough the results of an investigative study of information used as
basis for strategic decisions inside three different companies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01812</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01812</id><created>2015-02-06</created><authors><author><keyname>Li</keyname><forenames>Teng</forenames></author><author><keyname>Chang</keyname><forenames>Huan</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Ni</keyname><forenames>Bingbing</forenames></author><author><keyname>Hong</keyname><forenames>Richang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Crowded Scene Analysis: A Survey</title><categories>cs.CV</categories><comments>20 pages in IEEE Transactions on Circuits and Systems for Video
  Technology, 2015</comments><msc-class>68-02</msc-class><doi>10.1109/TCSVT.2014.2358029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated scene analysis has been a topic of great interest in computer
vision and cognitive science. Recently, with the growth of crowd phenomena in
the real world, crowded scene analysis has attracted much attention. However,
the visual occlusions and ambiguities in crowded scenes, as well as the complex
behaviors and scene semantics, make the analysis a challenging task. In the
past few years, an increasing number of works on crowded scene analysis have
been reported, covering different aspects including crowd motion pattern
learning, crowd behavior and activity analysis, and anomaly detection in
crowds. This paper surveys the state-of-the-art techniques on this topic. We
first provide the background knowledge and the available features related to
crowded scenes. Then, existing models, popular algorithms, evaluation
protocols, as well as system performance are provided corresponding to
different aspects of crowded scene analysis. We also outline the available
datasets for performance evaluation. Finally, some research problems and
promising future directions are presented with discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01815</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01815</id><created>2015-02-06</created><authors><author><keyname>Luan</keyname><forenames>Tom H.</forenames></author><author><keyname>Gao</keyname><forenames>Longxiang</forenames></author><author><keyname>Li</keyname><forenames>Zhi</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author><author><keyname>Sun</keyname><forenames>Limin</forenames></author></authors><title>Fog Computing: Focusing on Mobile Users at the Edge</title><categories>cs.NI</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With smartphones becoming our everyday companions, high-quality mobile
applications have become an important integral of people's lives. The intensive
and ubiquitous use of mobile applications have also led to the explosive growth
of mobile data traffics. Therefore, to accommodate the surge mobile traffic yet
providing the guaranteed service quality to mobile users represent a key issue
of the next generation mobile networks. This motivates the emergence of Fog
computing as a promising, practical and efficient solution tailored to serving
mobile traffics. Fog computing deploys highly virtualized computing and
communication facilities at the proximity of mobile users. Dedicated to serving
the mobile users, Fog computing explores the predictable service demand
patterns of mobile users and typically provides desirable localized services
accordingly. Stitching above features, Fog computing can provide mobile users
with the demanded services via low-latency and short-distance local
connections. In this article, we outline the main features of Fog computing and
describe its concept, architecture and design goals. Lastly, we discuss on the
potential research issues from the networking's perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01818</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01818</id><created>2015-02-06</created><authors><author><keyname>Heutte</keyname><forenames>Jean</forenames><affiliation>LGMT</affiliation></author><author><keyname>Galaup</keyname><forenames>Michel</forenames><affiliation>LGMT</affiliation></author><author><keyname>Lelardeux</keyname><forenames>Catherine</forenames><affiliation>LGMT</affiliation></author><author><keyname>Lagarrigue</keyname><forenames>Pierre</forenames><affiliation>LGMT</affiliation></author><author><keyname>Fenouillet</keyname><forenames>Fabien</forenames></author></authors><title>Etude des d\'eterminants psychologiques de la persistance dans l'usage
  d'un jeu s\'erieux : \'evaluation de l'environnement optimal d'apprentissage
  avec Mecagenius?</title><categories>cs.OH</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>STICEF, ATIEF, 2014, Evaluation dans les jeux s\'erieux, 21,
  pp.ISSN : 1764-7223, http://sticef.org/</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to show the relevance of motivational key concepts
in evaluating the use of serious game. This research involves 115 students
training with Mecagenius (serious game in mechanical engineering). The results
of the exploratory study also confirm the relevance of the use of flow in
Education scale (EduFlow) to evaluate the optimal learning experienc ewith a
serious game. It also appears that EduFlow is related to specific actions
within the school context such as self-efficacy, motivational climate and
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01823</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01823</id><created>2015-02-06</created><authors><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Unsupervised Fusion Weight Learning in Multiple Classifier Systems</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an unsupervised method to learn the weights with
which the scores of multiple classifiers must be combined in classifier fusion
settings. We also introduce a novel metric for ranking instances based on an
index which depends upon the rank of weighted scores of test points among the
weighted scores of training points. We show that the optimized index can be
used for computing measures such as average precision. Unlike most classifier
fusion methods where a single weight is learned to weigh all examples our
method learns instance-specific weights. The problem is formulated as learning
the weight which maximizes a clarity index; subsequently the index itself and
the learned weights both are used separately to rank all the test points. Our
method gives an unsupervised method of optimizing performance on actual test
data, unlike the well known stacking-based methods where optimization is done
over a labeled training set. Moreover, we show that our method is tolerant to
noisy classifiers and can be used for selecting N-best classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01827</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01827</id><created>2015-02-06</created><authors><author><keyname>Zhou</keyname><forenames>Guang-Tong</forenames></author><author><keyname>Hwang</keyname><forenames>Sung Ju</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author></authors><title>Hierarchical Maximum-Margin Clustering</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hierarchical maximum-margin clustering method for unsupervised
data analysis. Our method extends beyond flat maximum-margin clustering, and
performs clustering recursively in a top-down manner. We propose an effective
greedy splitting criteria for selecting which cluster to split next, and employ
regularizers that enforce feature sharing/competition for capturing data
semantics. Experimental results obtained on four standard datasets show that
our method outperforms flat and hierarchical clustering baselines, while
forming clean and semantically meaningful cluster hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01837</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01837</id><created>2015-02-06</created><authors><author><keyname>Lilienthal</keyname><forenames>Scott</forenames></author></authors><title>Bipartite Synthesis Method applied to the Subset Sum Problem
  demonstrates capability as decision and optimization tool</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a deterministic algorithm for solving an instance of
the Subset Sum Problem based on a new method entitled the Bipartite Synthesis
Method. The algorithm is described and shown to have worst-case limiting
performance over similar to the best deterministic algorithms achieving run
time complexity on the order of O(2^0.5n). This algorithm is representative of
a more expansive capability that might convey significant advantages over
existing deterministic or probabilistic methods, and it is amenable to blending
with existing methods. The method introduced can be applied to a variety of
decision and optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01838</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01838</id><created>2015-02-06</created><updated>2015-04-28</updated><authors><author><keyname>Jegourel</keyname><forenames>Cyrille</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Sedwards</keyname><forenames>Sean</forenames></author><author><keyname>Traonouez</keyname><forenames>Louis-Marie</forenames></author></authors><title>Distributed Verification of Rare Properties using Importance Splitting
  Observers</title><categories>cs.LO cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rare properties remain a challenge for statistical model checking (SMC) due
to the quadratic scaling of variance with rarity. We address this with a
variance reduction framework based on lightweight importance splitting
observers. These expose the model-property automaton to allow the construction
of score functions for high performance algorithms.
  The confidence intervals defined for importance splitting make it appealing
for SMC, but optimising its performance in the standard way makes distribution
inefficient. We show how it is possible to achieve equivalently good results in
less time by distributing simpler algorithms. We first explore the challenges
posed by importance splitting and present an algorithm optimised for
distribution. We then define a specific bounded time logic that is compiled
into memory-efficient observers to monitor executions. Finally, we demonstrate
our framework on a number of challenging case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01852</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01852</id><created>2015-02-06</created><authors><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Ren</keyname><forenames>Shaoqing</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Delving Deep into Rectifiers: Surpassing Human-Level Performance on
  ImageNet Classification</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rectified activation units (rectifiers) are essential for state-of-the-art
neural networks. In this work, we study rectifier neural networks for image
classification from two aspects. First, we propose a Parametric Rectified
Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU
improves model fitting with nearly zero extra computational cost and little
overfitting risk. Second, we derive a robust initialization method that
particularly considers the rectifier nonlinearities. This method enables us to
train extremely deep rectified models directly from scratch and to investigate
deeper or wider network architectures. Based on our PReLU networks
(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012
classification dataset. This is a 26% relative improvement over the ILSVRC 2014
winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass
human-level performance (5.1%, Russakovsky et al.) on this visual recognition
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01853</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01853</id><created>2015-02-06</created><authors><author><keyname>Degraux</keyname><forenames>K.</forenames></author><author><keyname>Cambareri</keyname><forenames>V.</forenames></author><author><keyname>Jacques</keyname><forenames>L.</forenames></author><author><keyname>Geelen</keyname><forenames>B.</forenames></author><author><keyname>Blanch</keyname><forenames>C.</forenames></author><author><keyname>Lafruit</keyname><forenames>G.</forenames></author></authors><title>Generalized Inpainting Method for Hyperspectral Image Acquisition</title><categories>cs.CV</categories><comments>Keywords: Hyperspectral, inpainting, iterative hard thresholding,
  sparse models, CMOS, Fabry-P\'erot</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently designed hyperspectral imaging device enables multiplexed
acquisition of an entire data volume in a single snapshot thanks to
monolithically-integrated spectral filters. Such an agile imaging technique
comes at the cost of a reduced spatial resolution and the need for a
demosaicing procedure on its interleaved data. In this work, we address both
issues and propose an approach inspired by recent developments in compressed
sensing and analysis sparse models. We formulate our superresolution and
demosaicing task as a 3-D generalized inpainting problem. Interestingly, the
target spatial resolution can be adjusted for mitigating the compression level
of our sensing. The reconstruction procedure uses a fast greedy method called
Pseudo-inverse IHT. We also show on simulations that a random arrangement of
the spectral filters on the sensor is preferable to regular mosaic layout as it
improves the quality of the reconstruction. The efficiency of our technique is
demonstrated through numerical experiments on both synthetic and real data as
acquired by the snapshot imager.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01861</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01861</id><created>2015-02-06</created><updated>2015-02-13</updated><authors><author><keyname>Kowalski</keyname><forenames>Tomasz</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author></authors><title>Indexing arbitrary-length $k$-mers in sequencing reads</title><categories>cs.DS q-bio.GN</categories><doi>10.1371/journal.pone.0133198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a lightweight data structure for indexing and querying collections
of NGS reads data in main memory. The data structure supports the interface
proposed in the pioneering work by Philippe et al. for counting and locating
$k$-mers in sequencing reads. Our solution, PgSA (pseudogenome suffix array),
based on finding overlapping reads, is competitive to the existing algorithms
in the space use, query times, or both. The main applications of our index
include variant calling, error correction and analysis of reads from RNA-seq
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01865</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01865</id><created>2015-02-06</created><authors><author><keyname>Jukna</keyname><forenames>Stasys</forenames></author></authors><title>Lower Bounds for Monotone Counting Circuits</title><categories>cs.CC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {+,x}-circuit counts a given multivariate polynomial f, if its values on
0-1 inputs are the same as those of f; on other inputs the circuit may output
arbitrary values. Such a circuit counts the number of monomials of f evaluated
to 1 by a given 0-1 input vector (with multiplicities given by their
coefficients). A circuit decides $f$ if it has the same 0-1 roots as f. We
first show that some multilinear polynomials can be exponentially easier to
count than to compute them, and can be exponentially easier to decide than to
count them. Then we give general lower bounds on the size of counting circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01866</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01866</id><created>2015-02-06</created><authors><author><keyname>R&#xea;go</keyname><forenames>L. C.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author></authors><title>An Extension of the Dirichlet Density for Sets of Gaussian Integers</title><categories>math.NT cs.CG math.CA math.PR math.ST stat.TH</categories><comments>13 pages, 1 figure</comments><msc-class>11B05, 11M99, 11N99</msc-class><journal-ref>Canad. Math. Bull. 56(2013), 161-172</journal-ref><doi>10.4153/CMB-2011-149-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several measures for the density of sets of integers have been proposed, such
as the asymptotic density, the Schnirelmann density, and the Dirichlet density.
There has been some work in the literature on extending some of these concepts
of density to higher dimensional sets of integers. In this work, we propose an
extension of the Dirichlet density for sets of Gaussian integers and
investigate some of its properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01871</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01871</id><created>2015-02-06</created><updated>2015-04-04</updated><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Shannon and Renyi Entropy of Wavelets</title><categories>cs.IT math.CA math.IT</categories><comments>6 pages, 1 figure</comments><journal-ref>International Journal of Mathematics and Computer Science,
  Vol.10(2015), no. 1, 13-26. ISSN 1814-0432</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports a new reading for wavelets, which is based on the
classical 'De Broglie' principle. The wave-particle duality principle is
adapted to wavelets. Every continuous basic wavelet is associated with a proper
probability density, allowing defining the Shannon entropy of a wavelet.
Further entropy definitions are considered, such as Jumarie or Renyi entropy of
wavelets. We proved that any wavelet of the same family has the same Shannon
entropy of its mother wavelet. Finally, the Shannon entropy for a few standard
wavelet families is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01872</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01872</id><created>2015-02-06</created><authors><author><keyname>Malik</keyname><forenames>Yasir</forenames></author></authors><title>Comprehensive Efficient Implementations of ECC on C54xx Family of
  Low-cost Digital Signal Processors</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource constraints in smart devices demand an efficient cryptosystem that
allows for low power and memory consumption. This has led to popularity of
comparatively efficient Elliptic curve cryptog-raphy (ECC). Prior to this
paper, much of ECC is implemented on re-configurable hardware i.e. FPGAs, which
are costly and unfavorable as low-cost solutions. We present comprehensive yet
efficient implementations of ECC on fixed-point TMS54xx series of digital
signal processors (DSP). 160-bit prime field GF(p) ECC is implemented over a
wide range of coordinate choices. This paper also implements windowed recoding
technique to provide better execution times. Stalls in the programming are
mini-mized by utilization of loop unrolling and by avoiding data dependence.
Complete scalar multiplication is achieved within 50 msec in coordinate
implementations, which is further reduced till 25 msec for windowed-recoding
method. These are the best known results for fixed-point low power digital
signal processor to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01877</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01877</id><created>2015-02-06</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Falk</keyname><forenames>T. H.</forenames></author></authors><title>On Wavelet Decomposition over Finite Fields</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure. conference: XIX Simposio Brasileiro de
  Telecomunicacoes, 2001, Fortaleza, CE, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces some foundations of wavelets over Galois fields.
Standard orthogonal finite-field wavelets (FF-Wavelets) including FF-Haar and
FF-Daubechies are derived. Non-orthogonal FF-wavelets such as B-spline over
GF(p) are also considered. A few examples of multiresolution analysis over
Finite fields are presented showing how to perform Laplacian pyramid filtering
of finite block lengths sequences. An application of FF-wavelets to design
spread-spectrum sequences is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01880</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01880</id><created>2015-02-06</created><authors><author><keyname>Melo</keyname><forenames>E. F.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>A Fingerprint-based Access Control using Principal Component Analysis
  and Edge Detection</title><categories>cs.CV cs.CR stat.AP</categories><comments>5 pages, 9 figures. SBrT/IEEE International Telecommunication
  Symposium, ITS 2010, Manaus, AM, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for deciding on the appropriateness or
not of an acquired fingerprint image into a given database. The process begins
with the assembly of a training base in an image space constructed by combining
Principal Component Analysis (PCA) and edge detection. Then, the parameter H, a
new feature that helps in the decision making about the relevance of a
fingerprint image in databases, is derived from a relationship between
Euclidean and Mahalanobian distances. This procedure ends with the lifting of
the curve of the Receiver Operating Characteristic (ROC), where the thresholds
defined on the parameter H are chosen according to the acceptable rates of
false positives and false negatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01885</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01885</id><created>2015-02-06</created><updated>2015-02-16</updated><authors><author><keyname>Yan</keyname><forenames>Haode</forenames></author><author><keyname>Liu</keyname><forenames>Chunlei</forenames></author></authors><title>Linearized Reed-Solomon codes and linearized Wenger graphs</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A codeword is associated to a linearized polynomial. The weight distribution
of the codewords is determined as the linearized polynomial varies in a family
of fixed degree. There is a corresponding result on Wenger graphs from
linearized polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01887</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01887</id><created>2015-02-06</created><authors><author><keyname>Zhang</keyname><forenames>Lan</forenames></author><author><keyname>Gang</keyname><forenames>Feng</forenames></author><author><keyname>Shuang</keyname><forenames>Qin</forenames></author></authors><title>A Comparison Study of Coupled and Decoupled Uplink Heterogeneous
  Cellular Networks</title><categories>cs.NI</categories><comments>7 pages,5 figures,conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of mobile cellular networks has brought great changes of
network architecture. For example, heterogeneous cellular network (HetNet) and
Ultra dense network (UDN) have been proposed as promising techniques for 5G
systems. Dense deployment of base stations (BSs) allows a mobile user to be
able to access multiple BSs. Meanwhile the unbalance between UL and DL in
HetNets, such as different received SINR threshold and traffic load, etc.,
becomes increasingly obvious. All these factors naturally inspire us to
consider decoupling of uplink and downlink in radio access network. An
interesting question is that whether the decoupled uplink (UL) /downlink (DL)
access (DUDA) mode outperforms traditional coupled uplink (UL)/downlink (DL)
access (CUDA) mode or not, and how big is the performance difference in terms
of system rate, spectrum efficiency (SE) and energy efficiency (EE), etc. in
HetNets. In this paper, we aim at thoroughly comparing the performance of the
two modes based on stochastic geometry theory. In our analytical model, we take
into account dynamic transmit power control in UL communication. Specifically,
we employ fractional power control (FPC) to model a location-dependent channel
state. Numerical results reveals that DUDA mode significantly outperforms CUDA
mode in system rate, SE and EE in HetNets. In addition, DUDA mode improves load
balance and potential fairness for both different type BSs and associated UEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01899</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01899</id><created>2015-02-06</created><updated>2015-03-10</updated><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Primiero</keyname><forenames>Giuseppe</forenames></author></authors><title>A framework for trustworthiness assessment based on fidelity in cyber
  and physical domains</title><categories>cs.CR</categories><comments>Draft version of a paper accepted for publication in the Proceedings
  of to ANTIFRAGILE 2015 (2nd International Workshop on Computational
  Antifragility and Antifragile Engineering,
  https://sites.google.com/site/antifragile15/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method for the assessment of trust for n-open systems based on
a measurement of fidelity and present a prototypic implementation of a
complaint architecture. We construct a MAPE loop which monitors the compliance
between corresponding figures of interest in cyber- and physical domains;
derive measures of the system's trustworthiness; and use them to plan and
execute actions aiming at guaranteeing system safety and resilience. We
conclude with a view on our future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01911</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01911</id><created>2015-02-06</created><updated>2015-05-02</updated><authors><author><keyname>Molu</keyname><forenames>Mehdi M.</forenames></author><author><keyname>Burr</keyname><forenames>Alister</forenames></author><author><keyname>Goertz</keyname><forenames>Norbert</forenames></author></authors><title>Statistical Analysis of Multi-Antenna Relay Systems and Power Allocation
  Algorithms in a Relay with Partial Channel State Information</title><categories>cs.IT math.IT</categories><doi>10.1109/TWC.2015.2432768</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a dual-hop MIMO relay network is studied in this paper.
The relay is assumed to have access to the statistical channel state
information of its preceding and following channels and it is assumed that
fading at the antennas of the relay is correlated. The cumulative density
function (cdf) of the received SNR at the destination is first studied and
closed-form expressions are derived for the asymptotic cases of the
fully-correlated and non-correlated scenarios; moreover, the statistical
characteristics of the SNR are further studied and an approximate cdf of the
SNR is derived for arbitrary correlation. The cdf is a multipartite function
which does not easily lend itself to further mathematical calculations, e.g.,
rate optimization. However, we use it to propose a simple power allocation
algorithm which we call &quot;proportional power allocation&quot;. The algorithm is
explained in detail for the case of two antennas and three antennas at the
relay and the extension of the algorithm to a relay with an arbitrary number of
the antennas is discussed. Although the proposed method is not claimed to be
optimal, the result is indistinguishable from the benchmark obtained using
exhaustive search. The simplicity of the algorithm combined with its precision
is indeed attractive from the practical point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01916</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01916</id><created>2015-02-06</created><authors><author><keyname>Zhao</keyname><forenames>Wayne Xin</forenames></author><author><keyname>Zhang</keyname><forenames>Xudong</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author><author><keyname>Shan</keyname><forenames>Dongdong</forenames></author><author><keyname>Nie</keyname><forenames>Jian-Yun</forenames></author><author><keyname>Yan</keyname><forenames>Hongfei</forenames></author><author><keyname>Wen</keyname><forenames>Ji-Rong</forenames></author></authors><title>A General SIMD-based Approach to Accelerating Compression Algorithms</title><categories>cs.IR</categories><acm-class>E.4; H.3.1; C.1.2</acm-class><journal-ref>ACM Trans. Inf. Syst. 33, 3, Article 15 (March 2015)</journal-ref><doi>10.1145/2735629</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Compression algorithms are important for data oriented tasks, especially in
the era of Big Data. Modern processors equipped with powerful SIMD instruction
sets, provide us an opportunity for achieving better compression performance.
Previous research has shown that SIMD-based optimizations can multiply decoding
speeds. Following these pioneering studies, we propose a general approach to
accelerate compression algorithms. By instantiating the approach, we have
developed several novel integer compression algorithms, called Group-Simple,
Group-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding
vectorized versions. We evaluate the proposed algorithms on two public TREC
datasets, a Wikipedia dataset and a Twitter dataset. With competitive
compression ratios and encoding speeds, our SIMD-based algorithms outperform
state-of-the-art non-vectorized algorithms with respect to decoding speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01920</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01920</id><created>2015-02-06</created><authors><author><keyname>Anashin</keyname><forenames>Vladimir</forenames></author></authors><title>Quantization causes waves:Smooth finitely computable functions are
  affine</title><categories>math.DS cs.FL math-ph math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an automaton (a letter-to-letter transducer, a dynamical 1-Lipschitz
system on the space $\mathbb Z_p$ of $p$-adic integers) $\mathfrak A$ whose
input and output alphabets are $\mathbb F_p=\{0,1,\ldots,p-1\}$, one visualizes
word transformations performed by $\mathfrak A$ by a point set $\mathbf
P(\mathfrak A)$ in real plane $\mathbb R^2$.
  For a finite-state automaton $\mathfrak A$, it is shown that once some points
of $\mathbf P(\mathfrak A)$ constitute a smooth (of a class $C^2$) curve in
$\mathbb R^2$, the curve is a segment of a straight line with a rational slope;
and there are only finitely many straight lines whose segments are in
$\mathbf{P}(\mathfrak A)$. Moreover, when identifying $\mathbf P(\mathfrak A)$
with a subset of a 2-dimensional torus $\mathbb T^2\subset\mathbb R^3$ (under a
natural mapping of the real unit square $[0,1]^2$ onto $\mathbb T^2$) the
smooth curves from $\mathbf P(\mathfrak A)$ constitute a collection of torus
windings. In cylindrical coordinates either of the windings can be ascribed to
a complex-valued function $\psi(x)=e^{i(Ax-2\pi B(t))}$ $(x\in\mathbb R)$ for
suitable rational $A,B(t)$. Since $\psi(x)$ is a standard expression for a
matter wave in quantum theory (where $B(t)=tB(t_0)$), and since transducers can
be regarded as a mathematical formalization for causal discrete systems, the
paper might serve as a mathematical reasoning why wave phenomena are inherent
in quantum systems: This is because of causality principle and the discreteness
of matter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01924</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01924</id><created>2015-02-06</created><authors><author><keyname>Zarei</keyname><forenames>Shahram</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Low-Complexity Widely-Linear Precoding for Downlink Large-Scale MU-MISO
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present a widely-linear minimum mean square error
(WL-MMSE) precoding scheme employing real-valued transmit symbols for downlink
large-scale multi-user multiple-input single-output (MU-MISO) systems. In
contrast to the existing WL-MMSE transceivers for single-user multiple-input
multiple-output (SU-MIMO) systems, which use both WL precoders and WL
detectors, the proposed scheme uses WL precoding only and simple conventional
detection at the user terminals (UTs). Moreover, to avoid the computational
complexity associated with inversion of large matrices, we modify the WL-MMSE
precoder using polynomial expansion (PE). Our simulation results show that in
overloaded systems, where the number of UTs is larger than the number of base
station antennas, the proposed PE WL-MMSE precoder with only a few terms in the
matrix polynomial achieves a substantially higher sum rate than systems
employing conventional MMSE precoding. Hence, more UTs sharing the same
time/frequency resources can be served in a cell. We validate our simulation
results with an analytical expression for the asymptotic sum rate which is
obtained by using results from random matrix theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01951</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01951</id><created>2015-02-06</created><authors><author><keyname>Tarrataca</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author></authors><title>Tree Search and Quantum Computation</title><categories>cs.DS quant-ph</categories><journal-ref>Quantum Information Processing, 2011, 10:4, 475-500</journal-ref><doi>10.1007/s11128-010-0212-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional tree search algorithms supply a blueprint for modeling problem
solving behaviour. A diverse spectrum of problems can be formulated in terms of
tree search. Quantum computation, in particular Grover's algorithm, has aroused
a great deal of interest since it allows for a quadratic speedup to be obtained
in search procedures. In this work we consider the impact of incorporating
classical search concepts alongside Grover's algorithm into a hybrid quantum
search system. Some of the crucial points examined include: (1) the
reverberations of contemplating the use of non-constant branching factors; (2)
determining the consequences of incorporating an heuristic perspective into a
quantum tree search model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01953</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01953</id><created>2015-02-06</created><updated>2015-03-01</updated><authors><author><keyname>Ramaswamy</keyname><forenames>Arunselvan</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>A Generalization of the Borkar-Meyn Theorem for Stochastic Recursive
  Inclusions</title><categories>cs.SY math.DS stat.ML</categories><msc-class>62L20, 93E03, 93E35, 34A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the stability theorem of Borkar and Meyn is extended to include
the case when the mean field is a differential inclusion. Two different sets of
sufficient conditions are presented that guarantee the stability and
convergence of stochastic recursive inclusions. Our work builds on the works of
Benaim, Hofbauer and Sorin as well as Borkar and Meyn. As a corollary to one of
the main theorems, a natural generalization of the Borkar and Meyn Theorem
follows. In addition, the original theorem of Borkar and Meyn is shown to hold
under slightly relaxed assumptions. Finally, as an application to one of the
main theorems we discuss a solution to the approximate drift problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01954</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01954</id><created>2015-02-06</created><authors><author><keyname>Jachnik</keyname><forenames>Jan</forenames></author><author><keyname>Goldman</keyname><forenames>Dan B</forenames></author><author><keyname>Luo</keyname><forenames>Linjie</forenames></author><author><keyname>Davison</keyname><forenames>Andrew J.</forenames></author></authors><title>Interactive 3D Face Stylization Using Sculptural Abstraction</title><categories>cs.GR cs.CG</categories><comments>9 pages, 16 figures. For associated video and to download some of the
  results, see the project webpage at
  http://wp.doc.ic.ac.uk/robotvision/project/face-stylization/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sculptors often deviate from geometric accuracy in order to enhance the
appearance of their sculpture. These subtle stylizations may emphasize anatomy,
draw the viewer's focus to characteristic features of the subject, or symbolize
textures that might not be accurately reproduced in a particular sculptural
medium, while still retaining fidelity to the unique proportions of an
individual. In this work we demonstrate an interactive system for enhancing
face geometry using a class of stylizations based on visual decomposition into
abstract semantic regions, which we call sculptural abstraction. We propose an
interactive two-scale optimization framework for stylization based on
sculptural abstraction, allowing real-time adjustment of both global and local
parameters. We demonstrate this system's effectiveness in enhancing physical 3D
prints of scans from various sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01956</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01956</id><created>2015-02-06</created><updated>2015-10-09</updated><authors><author><keyname>Ramaswamy</keyname><forenames>Arunselvan</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Stochastic recursive inclusion in two timescales with an application to
  the Lagrangian dual problem</title><categories>cs.SY math.DS stat.ML</categories><msc-class>62L20, 93E03, 93E35, 34A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a framework to analyze the asymptotic behavior of
two timescale stochastic approximation algorithms including those with
set-valued mean fields. This paper builds on the works of Borkar and Perkins &amp;
Leslie. The framework presented herein is more general as compared to the
synchronous two timescale framework of Perkins \&amp; Leslie, however the
assumptions involved are easily verifiable. As an application, we use this
framework to analyze the two timescale stochastic approximation algorithm
corresponding to the Lagrangian dual problem in optimization theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01959</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01959</id><created>2015-02-06</created><authors><author><keyname>Tarrataca</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author></authors><title>Can Quantum Entanglement Detection Schemes Improve Search?</title><categories>cs.DS quant-ph</categories><journal-ref>Quantum Information Processing, 2012, 11:1, 55-66</journal-ref><doi>10.1007/s11128-011-0231-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computation, in particular Grover's algorithm, has aroused a great
deal of interest since it allows for a quadratic speedup to be obtained in
search procedures. Classical search procedures for an $N$ element database
require at most $O(N)$ time complexity. Grover's algorithm is able to find a
solution with high probability in $O(\sqrt{N})$ time through an amplitude
amplification scheme. In this work we draw elements from both classical and
quantum computation to develop an alternative search proposal based on quantum
entanglement detection schemes. In 2002, Horodecki and Ekert proposed an
efficient method for direct detection of quantum entanglement. Our proposition
to quantum search combines quantum entanglement detection alongside
entanglement inducing operators. Grover's quantum search relies on measuring a
quantum superposition after having applied a unitary evolution. We deviate from
the standard method by focusing on fine-tuning a unitary operator in order to
infer the solution with certainty. Our proposal sacrifices space for speed and
depends on the mathematical properties of linear positive maps $\Lambda$ which
have not been operationally characterized. Whether such a $\Lambda$ can be
easily determined remains an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01963</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01963</id><created>2015-02-06</created><authors><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Editorial for the Proceedings of the Workshop Knowledge Maps and
  Information Retrieval (KMIR2014) at Digital Libraries 2014</title><categories>cs.IR cs.DL</categories><comments>URL workshop proceedings: http://ceur-ws.org/Vol-1311/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge maps are promising tools for visualizing the structure of
large-scale information spaces, but still far away from being applicable for
searching. The first international workshop on &quot;Knowledge Maps and Information
Retrieval (KMIR)&quot;, held as part of the International Conference on Digital
Libraries 2014 in London, aimed at bringing together experts in Information
Retrieval (IR) and knowledge mapping in order to discuss the potential of
interactive knowledge maps for information seeking purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01964</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01964</id><created>2015-02-06</created><authors><author><keyname>Nguyen</keyname><forenames>CamLy</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Doi</keyname><forenames>Yusuke</forenames></author></authors><title>Maximum Likelihood based Multihop Localization in Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>6 pages, 6 figures, ICC'15 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For data sets retrieved from wireless sensors to be insightful, it is often
of paramount importance that the data be accurate and also location stamped.
This paper describes a maximum-likelihood based multihop localization algorithm
called kHopLoc for use in wireless sensor networks that is strong in both
isotropic and anisotropic network deployment regions. During an initial
training phase, a Monte Carlo simulation is utilized to produce multihop
connection density functions. Then, sensor node locations are estimated by
maximizing local likelihood functions of the hop counts to anchor nodes.
Compared to other multihop localization algorithms, the proposed kHopLoc
algorithm achieves higher accuracy in varying network configurations and
connection link-models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01965</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01965</id><created>2015-02-06</created><authors><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author><author><keyname>Moussa</keyname><forenames>Karima Haddou ou</forenames></author></authors><title>How can heat maps of indexing vocabularies be utilized for information
  seeking purposes?</title><categories>cs.IR</categories><comments>URL workshop proceedings: http://ceur-ws.org/Vol-1311/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to browse an information space in a structured way by exploiting
similarities and dissimilarities between information objects is crucial for
knowledge discovery. Knowledge maps use visualizations to gain insights into
the structure of large-scale information spaces, but are still far away from
being applicable for searching. The paper proposes a use case for enhancing
search term recommendations by heat map visualizations of co-word
relation-ships taken from indexing vocabulary. By contrasting areas of
different &quot;heat&quot; the user is enabled to indicate mainstream areas of the field
in question more easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01968</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01968</id><created>2015-02-06</created><authors><author><keyname>Petersen</keyname><forenames>Hauke</forenames></author><author><keyname>Lenders</keyname><forenames>Martine</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Hahm</keyname><forenames>Oliver</forenames></author><author><keyname>Baccelli</keyname><forenames>Emmanuel</forenames></author></authors><title>Old Wine in New Skins? Revisiting the Software Architecture for IP
  Network Stacks on Constrained IoT Devices</title><categories>cs.NI</categories><comments>6 pages, 2 figures and tables</comments><acm-class>C.2.1; C.2.2; C.3; D.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we argue that existing concepts for the design and
implementation of network stacks for constrained devices do not comply with the
requirements of current and upcoming Internet of Things (IoT) use cases. The
IoT requires not only a lightweight but also a modular network stack, based on
standards. We discuss functional and non-functional requirements for the
software architecture of the network stack on constrained IoT devices. Then,
revisiting concepts from the early Internet as well as current implementations,
we propose a future-proof alternative to existing IoT network stack
architectures, and provide an initial evaluation of this proposal based on its
implementation running on top of state-of-the-art IoT operating system and
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01972</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01972</id><created>2015-02-06</created><authors><author><keyname>Saint-Guillain</keyname><forenames>Michael</forenames></author><author><keyname>Deville</keyname><forenames>Yves</forenames></author><author><keyname>Solnon</keyname><forenames>Christine</forenames></author></authors><title>A Multistage Stochastic Programming Approach to the Dynamic and
  Stochastic VRPTW - Extended version</title><categories>cs.AI cs.DM</categories><comments>Extended version of the same-name study submitted for publication in
  conference CPAIOR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic vehicle routing problem with time windows and
stochastic customers (DS-VRPTW), such that customers may request for services
as vehicles have already started their tours. To solve this problem, the goal
is to provide a decision rule for choosing, at each time step, the next action
to perform in light of known requests and probabilistic knowledge on requests
likelihood. We introduce a new decision rule, called Global Stochastic
Assessment (GSA) rule for the DS-VRPTW, and we compare it with existing
decision rules, such as MSA. In particular, we show that GSA fully integrates
nonanticipativity constraints so that it leads to better decisions in our
stochastic context. We describe a new heuristic approach for efficiently
approximating our GSA rule. We introduce a new waiting strategy. Experiments on
dynamic and stochastic benchmarks, which include instances of different degrees
of dynamism, show that not only our approach is competitive with
state-of-the-art methods, but also enables to compute meaningful offline
solutions to fully dynamic problems where absolutely no a priori customer
request is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01975</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01975</id><created>2015-02-06</created><authors><author><keyname>Kamath</keyname><forenames>Govinda M.</forenames></author><author><keyname>&#x15e;a&#x15f;o&#x11f;lu</keyname><forenames>Eren</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Optimal Haplotype Assembly from High-Throughput Mate-Pair Reads</title><categories>cs.IT cs.CE math.IT q-bio.GN stat.AP</categories><comments>10 pages, 4 figures, Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans have $23$ pairs of homologous chromosomes. The homologous pairs are
almost identical pairs of chromosomes. For the most part, differences in
homologous chromosome occur at certain documented positions called single
nucleotide polymorphisms (SNPs). A haplotype of an individual is the pair of
sequences of SNPs on the two homologous chromosomes. In this paper, we study
the problem of inferring haplotypes of individuals from mate-pair reads of
their genome. We give a simple formula for the coverage needed for haplotype
assembly, under a generative model. The analysis here leverages connections of
this problem with decoding convolutional codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01980</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01980</id><created>2015-02-06</created><authors><author><keyname>Orhan</keyname><forenames>Oner</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>Low Power Analog-to-Digital Conversion in Millimeter Wave Systems:
  Impact of Resolution and Bandwidth on Performance</title><categories>cs.IT math.IT</categories><comments>8 pages, 6 figures, in Proc. of IEEE Information Theory and
  Applications Workshop, Feb. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide bandwidth and large number of antennas used in millimeter wave
systems put a heavy burden on the power consumption at the receiver. In this
paper, using an additive quantization noise model, the effect of analog-digital
conversion (ADC) resolution and bandwidth on the achievable rate is
investigated for a multi-antenna system under a receiver power constraint. Two
receiver architectures, analog and digital combining, are compared in terms of
performance. Results demonstrate that: (i) For both analog and digital
combining, there is a maximum bandwidth beyond which the achievable rate
decreases; (ii) Depending on the operating regime of the system, analog
combiner may have higher rate but digital combining uses less bandwidth when
only ADC power consumption is considered, (iii) digital combining may have
higher rate when power consumption of all the components in the receiver
front-end are taken into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01986</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01986</id><created>2015-02-06</created><authors><author><keyname>Seif</keyname><forenames>Mohamed</forenames></author><author><keyname>Karmoose</keyname><forenames>Mohammed</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>Censoring for Improved Sensing Performance in Infrastructure-less
  Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Censoring has been proposed to be utilized in wireless distributed detection
networks with a fusion center to enhance network performance in terms of error
probability in addition to the well-established energy saving gains. In this
paper, we further examine the employment of censoring in infrastructure-less
cognitive radio networks, where nodes employ binary consensus algorithms to
take global decisions regarding a binary hypothesis test without a fusion
center to coordinate such a process. We show analytically - and verify by
simulations - that censoring enhances the performance of such networks in terms
of error probability and convergence times. Our protocol shows performance
gains up to 46.6% in terms of average error probability over its conventional
counterpart, in addition to performance gains of about 48.7% in terms of
average energy expenditure and savings up to 50% in incurred transmission
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01993</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01993</id><created>2015-02-06</created><updated>2015-04-17</updated><authors><author><keyname>Bagnol</keyname><forenames>Marc</forenames></author></authors><title>MALL proof equivalence is Logspace-complete, via binary decision
  diagrams</title><categories>cs.LO</categories><comments>in TLCA 2015</comments><acm-class>F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof equivalence in a logic is the problem of deciding whether two proofs
are equivalent modulo a set of permutation of rules that reflects the
commutative conversions of its cut-elimination procedure. As such, it is
related to the question of proofnets: finding canonical representatives of
equivalence classes of proofs that have good computational properties. It can
also be seen as the word problem for the notion of free category corresponding
to the logic.
  It has been recently shown that proof equivalence in MLL (the multiplicative
with units fragment of linear logic) is PSPACE-complete, which rules out any
low-complexity notion of proofnet for this particular logic.
  Since it is another fragment of linear logic for which attempts to define a
fully satisfactory low-complexity notion of proofnet have not been successful
so far, we study proof equivalence in MALL- (multiplicative-additive without
units fragment of linear logic) and discover a situation that is totally
different from the MLL case. Indeed, we show that proof equivalence in MALL-
corresponds (under AC0 reductions) to equivalence of binary decision diagrams,
a data structure widely used to represent and analyze Boolean functions
efficiently.
  We show these two equivalent problems to be LOGSPACE-complete. If this
technically leaves open the possibility for a complete solution to the question
of proofnets for MALL-, the established relation with binary decision diagrams
actually suggests a negative solution to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.01996</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.01996</id><created>2015-02-06</created><authors><author><keyname>Music</keyname><forenames>Jelena</forenames></author><author><keyname>Knezevic</keyname><forenames>Ivan</forenames></author><author><keyname>Franca</keyname><forenames>Edis</forenames></author></authors><title>Wavelet based Watermarking approach in the Compressive Sensing Scenario</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the wide distribution and usage of digital media, an important issue
is protection of the digital content. There is a number of algorithms and
techniques developed for the digital watermarking.In this paper, the invisible
image watermark procedure is considered. Watermark is created as a pseudo
random sequence, embedded in the certain region of the image, obtained using
Haar wavelet decomposition. Generally, the watermarking procedure should be
robust to the various attacks-filtering, noise etc. Here we assume the
Compressive sensing scenario as a new signal processing technique that may
influence the robustness. The focus of this paper was the possibility of the
watermark detection under Compressive Sensing attack with different number of
available image coefficients. The quality of the reconstructed images has been
evaluated using Peak Signal to Noise Ratio (PSNR).The theory is supported with
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02004</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02004</id><created>2015-02-06</created><updated>2015-11-20</updated><authors><author><keyname>Cornu</keyname><forenames>Benoit</forenames><affiliation>SPIRALS</affiliation></author><author><keyname>Barr</keyname><forenames>Earl T.</forenames><affiliation>SPIRALS</affiliation></author><author><keyname>Seinturier</keyname><forenames>Lionel</forenames><affiliation>SPIRALS</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>SPIRALS</affiliation></author></authors><title>Casper: Debugging Null Dereferences with Dynamic Causality Traces</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fixing a software error requires understanding its root cause. In this paper,
we introduce ''causality traces'', crafted execution traces augmented with the
information needed to reconstruct the causal chain from the root cause of a bug
to an execution error. We propose an approach and a tool, called Casper, for
dynamically constructing causality traces for null dereference errors. The core
idea of Casper is to inject special values, called ''ghosts'', into the
execution stream to construct the causality trace at runtime. We evaluate our
contribution by providing and assessing the causality traces of 14 real null
dereference bugs collected over six large, popular open-source projects. Over
this data set, Casper builds a causality trace in less than 5 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02009</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02009</id><created>2015-02-06</created><updated>2015-05-18</updated><authors><author><keyname>Nishihara</keyname><forenames>Robert</forenames></author><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Packard</keyname><forenames>Andrew</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>A General Analysis of the Convergence of ADMM</title><categories>math.OC cs.NA</categories><comments>10 pages, 6 figures</comments><journal-ref>International Conference on Machine Learning 32, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new proof of the linear convergence of the alternating direction
method of multipliers (ADMM) when one of the objective terms is strongly
convex. Our proof is based on a framework for analyzing optimization algorithms
introduced in Lessard et al. (2014), reducing algorithm convergence to
verifying the stability of a dynamical system. This approach generalizes a
number of existing results and obviates any assumptions about specific choices
of algorithm parameters. On a numerical example, we demonstrate that minimizing
the derived bound on the convergence rate provides a practical approach to
selecting algorithm parameters for particular ADMM instances. We complement our
upper bound by constructing a nearly-matching lower bound on the worst-case
rate of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02029</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02029</id><created>2015-02-06</created><authors><author><keyname>Tarrataca</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author></authors><title>A Quantum Production Model</title><categories>cs.AI quant-ph</categories><journal-ref>Quantum Information Processing, 2012,11:1, 189-209</journal-ref><doi>10.1007/s11128-011-0241-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The production system is a theoretical model of computation relevant to the
artificial intelligence field allowing for problem solving procedures such as
hierarchical tree search. In this work we explore some of the connections
between artificial intelligence and quantum computation by presenting a model
for a quantum production system. Our approach focuses on initially developing a
model for a reversible production system which is a simple mapping of Bennett's
reversible Turing machine. We then expand on this result in order to
accommodate for the requirements of quantum computation. We present the details
of how our proposition can be used alongside Grover's algorithm in order to
yield a speedup comparatively to its classical counterpart. We discuss the
requirements associated with such a speedup and how it compares against a
similar quantum hierarchical search approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02030</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02030</id><created>2015-02-06</created><authors><author><keyname>Tarrataca</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author></authors><title>Quantum Iterative Deepening with an application to the Halting problem</title><categories>quant-ph cs.FL cs.LO</categories><journal-ref>PLOS One, March 2013</journal-ref><doi>10.1371/journal.pone.0057309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical models of computation traditionally resort to halting schemes in
order to enquire about the state of a computation. In such schemes, a
computational process is responsible for signalling an end of a calculation by
setting a halt bit, which needs to be systematically checked by an observer.
The capacity of quantum computational models to operate on a superposition of
states requires an alternative approach. From a quantum perspective, any
measurement of an equivalent halt qubit would have the potential to inherently
interfere with the computation by provoking a random collapse amongst the
states. This issue is exacerbated by undecidable problems such as the
\textit{Entscheidungsproblem} which require universal computational models,
\textit{e.g.} the classical Turing machine, to be able to proceed indefinitely.
In this work we present an alternative view of quantum computation based on
production system theory in conjunction with Grover's amplitude amplification
scheme that allows for (1) a detection of halt states without interfering with
the final result of a computation; (2) the possibility of non-terminating
computation and (3) an inherent speedup to occur during computations
susceptible of parallelization. We discuss how such a strategy can be employed
in order to simulate classical Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02045</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02045</id><created>2015-02-06</created><updated>2015-02-10</updated><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author><author><keyname>Bonchis</keyname><forenames>Cosmin</forenames></author></authors><title>Partition into heapable sequences, heap tableaux and a multiset
  extension of Hammersley's process</title><categories>math.CO cond-mat.stat-mech cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate partitioning of integer sequences into heapable subsequences
(previously defined and established by Mitzenmacher et al). We show that an
extension of patience sorting computes the decomposition into a minimal number
of heapable subsequences (MHS). We connect this parameter to an interactive
particle system, a multiset extension of Hammersley's process, and investigate
its expected value on a random permutation. In contrast with the (well studied)
case of the longest increasing subsequence, we bring experimental evidence that
the correct asymptotic scaling is $\frac{1+\sqrt{5}}{2}\cdot \ln(n)$. Finally
we give a heap-based extension of Young tableaux, prove a hook inequality and
an extension of the Robinson-Schensted correspondence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02051</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02051</id><created>2015-02-06</created><updated>2015-08-13</updated><authors><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Approximating ATSP by Relaxing Connectivity</title><categories>cs.DS</categories><comments>25 pages, 2 figures, fixed some typos in previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard LP relaxation of the asymmetric traveling salesman problem has
been conjectured to have a constant integrality gap in the metric case. We
prove this conjecture when restricted to shortest path metrics of node-weighted
digraphs. Our arguments are constructive and give a constant factor
approximation algorithm for these metrics. We remark that the considered case
is more general than the directed analog of the special case of the symmetric
traveling salesman problem for which there were recent improvements on
Christofides' algorithm.
  The main idea of our approach is to first consider an easier problem obtained
by significantly relaxing the general connectivity requirements into local
connectivity conditions. For this relaxed problem, it is quite easy to give an
algorithm with a guarantee of 3 on node-weighted shortest path metrics. More
surprisingly, we then show that any algorithm (irrespective of the metric) for
the relaxed problem can be turned into an algorithm for the asymmetric
traveling salesman problem by only losing a small constant factor in the
performance guarantee. This leaves open the intriguing task of designing a
&quot;good&quot; algorithm for the relaxed problem on general metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02056</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02056</id><created>2015-01-24</created><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Liu</keyname><forenames>Xingjian</forenames></author><author><keyname>Zhou</keyname><forenames>Jiangping</forenames></author><author><keyname>Chai</keyname><forenames>Yanwei</forenames></author></authors><title>Early Birds, Night Owls,and Tireless/Recurring Itinerants: An
  Exploratory Analysis of Extreme Transit Behaviors in Beijing, China</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 2 figures, 4 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper seeks to understand extreme public transit riders in Beijing using
both traditional household survey and emerging new data sources such as Smart
Card Data (SCD). We focus on four types of extreme transit behaviors: public
transit riders who (1) travel significantly earlier than average riders (the
'early birds'); (2) ride in unusual late hours (the 'night owls'); and (3)
commute in excessively long distance (the 'tireless itinerants'); (4) travel
over frequently in a day (the 'recurring itinerants). SCD are used to identify
the spatiotemporal patterns of these three extreme transit behaviors. In
addition, household survey data are employed to supplement the socioeconomic
background and provide a tentative profiling of extreme travelers. While the
research findings are useful to guide urban governance and planning in Beijing,
the methods developed in this paper can be applied to understand travel
patterns elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02057</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02057</id><created>2015-01-29</created><authors><author><keyname>Stevens</keyname><forenames>Cacey S.</forenames></author><author><keyname>Marder</keyname><forenames>Michael P.</forenames></author><author><keyname>Nagel</keyname><forenames>Sidney R.</forenames></author></authors><title>Patterns in Illinois Educational School Data</title><categories>physics.ed-ph cs.CY physics.soc-ph</categories><comments>9 pages, 6 figures</comments><doi>10.1103/PhysRevSTPER.11.010113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine Illinois educational data from standardized exams and analyze
primary factors affecting the achievement of public school students. We focus
on the simplest possible models: representation of data through visualizations
and regressions on single variables. Exam scores are shown to depend on school
type, location, and poverty concentration. For most schools in Illinois,
student test scores decline linearly with poverty concentration. However
Chicago must be treated separately. Selective schools in Chicago, as well as
some traditional and charter schools, deviate from this pattern based on
poverty. For any poverty level, Chicago schools perform better than those in
the rest of Illinois. Selective programs for gifted students show high
performance at each grade level, most notably at the high school level, when
compared to other Illinois school types. The case of Chicago charter schools is
more complex. In the last six years, their students' scores overtook those of
students in traditional Chicago high schools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02063</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02063</id><created>2015-02-06</created><updated>2015-04-09</updated><authors><author><keyname>Hajimirsadeghi</keyname><forenames>Hossein</forenames></author><author><keyname>Yan</keyname><forenames>Wang</forenames></author><author><keyname>Vahdat</keyname><forenames>Arash</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author></authors><title>Visual Recognition by Counting Instances: A Multi-Instance Cardinality
  Potential Kernel</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many visual recognition problems can be approached by counting instances. To
determine whether an event is present in a long internet video, one could count
how many frames seem to contain the activity. Classifying the activity of a
group of people can be done by counting the actions of individual people.
Encoding these cardinality relationships can reduce sensitivity to clutter, in
the form of irrelevant frames or individuals not involved in a group activity.
Learned parameters can encode how many instances tend to occur in a class of
interest. To this end, this paper develops a powerful and flexible framework to
infer any cardinality relation between latent labels in a multi-instance model.
Hard or soft cardinality relations can be encoded to tackle diverse levels of
ambiguity. Experiments on tasks such as human activity recognition, video event
detection, and video summarization demonstrate the effectiveness of using
cardinality relations for improving recognition results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02065</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02065</id><created>2015-02-06</created><authors><author><keyname>Savage</keyname><forenames>Saiph</forenames></author><author><keyname>Monroy-Hern&#xe1;ndez</keyname><forenames>Andr&#xe9;s</forenames></author></authors><title>Participatory Militias: An Analysis of an Armed Movement's Online
  Audience</title><categories>cs.SI cs.CY</categories><comments>Participatory Militias: An Analysis of an Armed Movement's Online
  Audience. Saiph Savage, Andres Monroy-Hernandez. CSCW: ACM Conference on
  Computer-Supported Cooperative Work 2015</comments><acm-class>H.5.3</acm-class><doi>10.1145/2675133.2675295</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Armed groups of civilians known as &quot;self-defense forces&quot; have ousted the
powerful Knights Templar drug cartel from several towns in Michoacan. This
militia uprising has unfolded on social media, particularly in the &quot;VXM&quot;
(&quot;Valor por Michoacan,&quot; Spanish for &quot;Courage for Michoacan&quot;) Facebook page,
gathering more than 170,000 fans. Previous work on the Drug War has documented
the use of social media for real-time reports of violent clashes. However, VXM
goes one step further by taking on a pro-militia propagandist role, engaging in
two-way communication with its audience. This paper presents a descriptive
analysis of VXM and its audience. We examined nine months of posts, from VXM's
inception until May 2014, totaling 6,000 posts by VXM administrators and more
than 108,000 comments from its audience. We describe the main conversation
themes, post frequency and relationships with offline events and public
figures. We also characterize the behavior of VXM's most active audience
members. Our work illustrates VXM's online mobilization strategies, and how its
audience takes part in defining the narrative of this armed conflict. We
conclude by discussing possible applications of our findings for the design of
future communication technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02072</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02072</id><created>2015-02-06</created><authors><author><keyname>Ramsundar</keyname><forenames>Bharath</forenames></author><author><keyname>Kearnes</keyname><forenames>Steven</forenames></author><author><keyname>Riley</keyname><forenames>Patrick</forenames></author><author><keyname>Webster</keyname><forenames>Dale</forenames></author><author><keyname>Konerding</keyname><forenames>David</forenames></author><author><keyname>Pande</keyname><forenames>Vijay</forenames></author></authors><title>Massively Multitask Networks for Drug Discovery</title><categories>stat.ML cs.LG cs.NE</categories><comments>Preliminary work. Under review by the International Conference on
  Machine Learning (ICML)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massively multitask neural architectures provide a learning framework for
drug discovery that synthesizes information from many distinct biological
sources. To train these architectures at scale, we gather large amounts of data
from public sources to create a dataset of nearly 40 million measurements
across more than 200 biological targets. We investigate several aspects of the
multitask framework by performing a series of empirical studies and obtain some
interesting results: (1) massively multitask networks obtain predictive
accuracies significantly better than single-task methods, (2) the predictive
power of multitask networks improves as additional tasks and data are added,
(3) the total amount of data and the total number of tasks both contribute
significantly to multitask improvement, and (4) multitask networks afford
limited transferability to tasks not in the training set. Our results
underscore the need for greater data sharing and further algorithmic innovation
to accelerate the drug discovery process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02076</identifier>
 <datestamp>2015-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02076</id><created>2015-02-06</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>Can Other People Make You Less Creative?</title><categories>cs.MA</categories><comments>7 pages. arXiv admin note: text overlap with arXiv:1501.06009</comments><journal-ref>Psychology Today (online).
  https://www.psychologytoday.com/blog/mindbloggling/201502/can-other-people-make-you-less-creative</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explains in layperson's terms how an agent-based model was used to
investigate the hypothesis that culture evolves more effectively when
novelty-generating creative processes are tempered by imitation processes that
preserve proven successful ideas. Using EVOC, an agent-based model of cultural
evolution we found that (1) the optimal ratio of inventing to imitating ranged
from 1:1 to 2:1 depending on the fitness function, (2) there was a trade-off
between the proportion of creators to conformers and how creative the creators
were, and (3) when agents in increased or decreased their creativity depending
on the success of their latest creative efforts, they segregated into creators
and conformers, and the mean fitness of ideas across the society was higher. It
is tentatively suggested that through the unconscious use of social cues,
members of a society self-organizes to achieve a balanced mix of creators and
conformers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02077</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02077</id><created>2015-02-06</created><updated>2015-09-22</updated><authors><author><keyname>Hirn</keyname><forenames>Matthew</forenames></author><author><keyname>Poilvert</keyname><forenames>Nicolas</forenames></author><author><keyname>Mallat</keyname><forenames>Stephane</forenames></author></authors><title>Quantum Energy Regression using Scattering Transforms</title><categories>cs.LG cs.CV physics.chem-ph physics.comp-ph quant-ph</categories><comments>9 pages, 2 figures, 1 table. v2: Correction to Section 4.3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to the regression of quantum mechanical energies
based on a scattering transform of an intermediate electron density
representation. A scattering transform is a deep convolution network computed
with a cascade of multiscale wavelet transforms. It possesses appropriate
invariant and stability properties for quantum energy regression. This new
framework removes fundamental limitations of Coulomb matrix based energy
regressions, and numerical experiments give state-of-the-art accuracy over
planar molecules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02081</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02081</id><created>2015-02-06</created><authors><author><keyname>Jegatheesan</keyname><forenames>Sowmyan</forenames></author></authors><title>Taxing the Internet - is that feasible ?</title><categories>cs.CY</categories><comments>8 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Governments across the globe are facing challenging times to generate more
revenue because of the economic slowdown and to balance their budgets. There is
a growing need to find new ways of revenue generation as spending cuts and
austerity measures dont go well with most sections of the society, especially
during difficult economic times. Internet Today is seen more as a necessary
commodity and nobody can deny the fact that the Internet has improved peoples
life in an unprecedented way than any other technology in the past.The Internet
as a commodity is taxed in many countries, but the Internet usage or the
transactions carried out are something thats not taxed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02084</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02084</id><created>2015-02-06</created><authors><author><keyname>Jegatheesan</keyname><forenames>Sowmyan</forenames></author><author><keyname>Ahmed</keyname><forenames>Sabbir</forenames></author><author><keyname>Chamney</keyname><forenames>Austin</forenames></author><author><keyname>El-kadri</keyname><forenames>Nour</forenames></author></authors><title>Is a global virtual currency with universal acceptance feasible ?</title><categories>cs.CY</categories><comments>19 Pages</comments><journal-ref>(2013) International Journal of Community Currency Research 17 (A)
  26-44 ISSN 1325-9547</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As digital goods and services become an integral part of modern day society,
the demand for a standardized and ubiquitous form of digital currency
increases. And it is not just about digital goods; the adoption of electronic
and mobile commerce has not reached its expected level at all parts of the
globe as expected. One of the main reasons behind that is the lack of a
universal digital as well as virtual currency. Many countries in the world have
failed to realize the potential of e-commerce, let alone m-commerce, because of
rigid financial regulations and apparent disorientation &amp; gap between monetary
stakeholders across borders and continents. Digital currency which is
internet-based, non-banks issued and circulated within a certain range of
networks has brought a significant impact on the development of e-commerce. The
research and analysis of this paper would focus on the feasibility of the
operation of a digital currency and its economic implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02092</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02092</id><created>2015-02-06</created><authors><author><keyname>Zhang</keyname><forenames>Hang</forenames></author><author><keyname>Dana</keyname><forenames>Kristin</forenames></author><author><keyname>Nishino</keyname><forenames>Ko</forenames></author></authors><title>Reflectance Hashing for Material Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel method for using reflectance to identify materials.
Reflectance offers a unique signature of the material but is challenging to
measure and use for recognizing materials due to its high-dimensionality. In
this work, one-shot reflectance is captured using a unique optical camera
measuring {\it reflectance disks} where the pixel coordinates correspond to
surface viewing angles. The reflectance has class-specific stucture and angular
gradients computed in this reflectance space reveal the material class.
  These reflectance disks encode discriminative information for efficient and
accurate material recognition. We introduce a framework called reflectance
hashing that models the reflectance disks with dictionary learning and binary
hashing. We demonstrate the effectiveness of reflectance hashing for material
recognition with a number of real-world materials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02098</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02098</id><created>2015-02-07</created><updated>2015-09-03</updated><authors><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>Lanting</keyname><forenames>Trevor</forenames></author><author><keyname>Harris</keyname><forenames>Richard</forenames></author></authors><title>Performance of a quantum annealer on range-limited constraint
  satisfaction problems</title><categories>quant-ph cs.DM</categories><comments>6 pages, 8 pages of supplemental material included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a D-Wave Vesuvius quantum annealer was recently compared
to a suite of classical algorithms on a class of constraint satisfaction
instances based on frustrated loops. However, the construction of these
instances leads the maximum coupling strength to increase with problem size. As
a result, larger instances are subject to amplified analog control error, and
are effectively annealed at higher temperatures in both hardware and software.
We generate similar constraint satisfaction instances with limited range of
coupling strength and perform a similar comparison to classical algorithms. On
these instances the D-Wave Vesuvius processor, run with a fixed 20$\mu$s anneal
time, shows a scaling advantage over the software solvers for the hardest
regime studied. This scaling advantage opens the possibility of quantum speedup
on these problems. Our results support the hypothesis that performance of
D-Wave Vesuvius processors is heavily influenced by analog control error, which
can be reduced and mitigated as the technology matures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02103</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02103</id><created>2015-02-07</created><authors><author><keyname>Majhi</keyname><forenames>Subhajit</forenames></author><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>Secondary Outage Analysis of Amplify-and-Forward Cognitive Relays with
  Direct Link and Primary Interference</title><categories>cs.IT cs.NI math.IT</categories><comments>To be presented in 21st National Conference on Communications
  (NCC'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of cognitive relays is an emerging and promising solution to overcome
the problem of spectrum underutilization while achieving the spatial diversity.
In this paper, we perform an outage analysis of the secondary system with
amplify-and-forward relays in a spectrum sharing scenario, where a secondary
transmitter communicates with a secondary destination over a direct link as
well as the best relay. Specifically, under the peak power constraint, we
derive a closed-form expression of the secondary outage probability provided
that the primary outage probability remains below a predefined value. We also
take into account the effect of primary interference on the secondary outage
performance. Finally, we validate the analysis by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02106</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02106</id><created>2015-02-07</created><updated>2016-01-21</updated><authors><author><keyname>Yu</keyname><forenames>Han</forenames></author></authors><title>Building Robust Crowdsourcing Systems with Reputation-aware Decision
  Support Techniques</title><categories>cs.MA cs.CY cs.SI</categories><comments>Book Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing refers to the arrangement in which contributions are solicited
from a large group of unrelated people. Due to this nature, crowdsourcers (or
task requesters) often face uncertainty about the workers' capabilities which,
in turn, affects the quality and timeliness of the results obtained. Trust is a
mechanism used by people to facilitate interactions in human societies where
risk and uncertain are common. The crucial challenge to building a robust
crowdsourcing system is how to make trust-aware task delegation decisions to
efficiently utilize the capacities of workers (or trustee agents) to achieve
high social welfare?
  This book presents the research addressing this challenge. It goes beyond the
existing trust management research framework by removing a widespread
assumption implicitly adopted by existing research: that a trustee agent can
process an unlimited number of interaction requests per discrete time unit
without compromising its performance as perceived by the task requesters (or
truster agents). Decision support in crowdsourcing is re-formalized as a
multi-agent trust game based on the principles of the Congestion Game, which is
solved by two trust-aware interaction decision-making approaches: 1) the Social
Welfare Optimizing approach for Reputation-aware Decision-making (SWORD)
approach, and 2) the Distributed Request Acceptance approach for Fair
utilization of Trustee agents (DRAFT). SWORD is designed for centralized
systems, while DRAFT is designed for fully distributed systems. Theoretical
analyses have shown that the social welfare produced by these two approaches
can be made closer to optimal by adjusting only one key parameter. With these
two approaches, the framework of research for crowdsourcing systems can be
enriched to handle more realistic scenarios where workers have varied and
limited capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02111</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02111</id><created>2015-02-07</created><authors><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Ali</keyname><forenames>Anwaar</forenames></author><author><keyname>Yau</keyname><forenames>Kok-Lim Alvin</forenames></author><author><keyname>Sathiaseelan</keyname><forenames>Arjuna</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Exploiting the power of multiplicity: a holistic survey of network-layer
  multipath</title><categories>cs.NI</categories><doi>10.1109/COMST.2015.2453941</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet is inherently a multipath network---for an underlying network
with only a single path connecting various nodes would have been debilitatingly
fragile. Unfortunately, traditional Internet technologies have been designed
around the restrictive assumption of a single working path between a source and
a destination. The lack of native multipath support constrains network
performance even as the underlying network is richly connected and has
redundant multiple paths. Computer networks can exploit the power of
multiplicity to unlock the inherent redundancy of the Internet. This opens up a
new vista of opportunities promising increased throughput (through concurrent
usage of multiple paths) and increased reliability and fault-tolerance (through
the use of multiple paths in backup/ redundant arrangements). There are many
emerging trends in networking that signify that the Internet's future will be
unmistakably multipath, including the use of multipath technology in datacenter
computing; multi-interface, multi-channel, and multi-antenna trends in
wireless; ubiquity of mobile devices that are multi-homed with heterogeneous
access networks; and the development and standardization of multipath transport
protocols such as MP-TCP.
  The aim of this paper is to provide a comprehensive survey of the literature
on network-layer multipath solutions. We will present a detailed investigation
of two important design issues, namely the control plane problem of how to
compute and select the routes, and the data plane problem of how to split the
flow on the computed paths. The main contribution of this paper is a systematic
articulation of the main design issues in network-layer multipath routing along
with a broad-ranging survey of the vast literature on network-layer
multipathing. We also highlight open issues and identify directions for future
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02112</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02112</id><created>2015-02-07</created><authors><author><keyname>Yang</keyname><forenames>Li</forenames></author><author><keyname>Wu</keyname><forenames>Chenmiao</forenames></author></authors><title>Mutual authenticated quantum no-key encryption scheme over private
  quantum channel</title><categories>quant-ph cs.CR</categories><comments>11 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We realize shamir's no-key protocol via quantum computation of Boolean
permutation and private quantum channel. The quantum no-key (QNK) protocol
presented here is one with mutual authentications, and proved to be
unconditionally secure. An important property of this protocol is that0 its
authentication key can be reused permanently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02125</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02125</id><created>2015-02-07</created><updated>2015-03-24</updated><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Contextual Online Learning for Multimedia Content Aggregation</title><categories>cs.MM cs.LG cs.MA</categories><comments>To appear in IEEE Transactions on Multimedia, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last decade has witnessed a tremendous growth in the volume as well as
the diversity of multimedia content generated by a multitude of sources (news
agencies, social media, etc.). Faced with a variety of content choices,
consumers are exhibiting diverse preferences for content; their preferences
often depend on the context in which they consume content as well as various
exogenous events. To satisfy the consumers' demand for such diverse content,
multimedia content aggregators (CAs) have emerged which gather content from
numerous multimedia sources. A key challenge for such systems is to accurately
predict what type of content each of its consumers prefers in a certain
context, and adapt these predictions to the evolving consumers' preferences,
contexts and content characteristics. We propose a novel, distributed, online
multimedia content aggregation framework, which gathers content generated by
multiple heterogeneous producers to fulfill its consumers' demand for content.
Since both the multimedia content characteristics and the consumers'
preferences and contexts are unknown, the optimal content aggregation strategy
is unknown a priori. Our proposed content aggregation algorithm is able to
learn online what content to gather and how to match content and users by
exploiting similarities between consumer types. We prove bounds for our
proposed learning algorithms that guarantee both the accuracy of the
predictions as well as the learning speed. Importantly, our algorithms operate
efficiently even when feedback from consumers is missing or content and
preferences evolve over time. Illustrative results highlight the merits of the
proposed content aggregation system in a variety of settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02126</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02126</id><created>2015-02-07</created><authors><author><keyname>Saha</keyname><forenames>Sumanta</forenames></author><author><keyname>Lukyanenko</keyname><forenames>Andrey</forenames></author><author><keyname>Yl&#xe4;-J&#xe4;&#xe4;ski</keyname><forenames>Antti</forenames></author></authors><title>Efficient Cache Availability Management in Information-Centric Networks</title><categories>cs.NI</categories><comments>Submitted and under review in Computer Networks Journal - Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-network caching is one of the fundamental operations of
Information-centric networks (ICN). The default caching strategy taken by most
of the current ICN proposals is caching along--default--path, which makes
popular objects to be cached redundantly across the network, resulting in a low
utilization of available cache space. On the other hand, efficient use of
network-wide cache space requires possible cooperation among caching routers
without the use of excessive signaling burden. While most of the cache
optimization efforts strive to improve the latency and the overall traffic
efficiency, we have taken a different path in this work and improved the
storage efficiency of the cache space so that it is utilized to its most.
  In this work we discuss the ICN caching problem, and propose a novel
distributed architecture to efficiently use the network-wide cache storage
space based on distributed caching. The proposal achieves cache retention
efficiency by means of controlled traffic redirection and selective caching. We
utilize the ICN mechanisms and routing protocol messages for decision making,
thus reducing the overall signaling need. Our proposal achieves almost 9-fold
increase in cache storage efficiency, and around 20% increase in server load
reduction when compared to the classic caching methods used in contemporary ICN
proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02127</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02127</id><created>2015-02-07</created><updated>2015-04-06</updated><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>Hyperparameter Search in Machine Learning</title><categories>cs.LG stat.ML</categories><comments>5 pages, accepted for MIC 2015: The XI Metaheuristics International
  Conference in Agadir, Morocco</comments><acm-class>G.1.6; I.2.6; I.2.8; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the hyperparameter search problem in the field of machine
learning and discuss its main challenges from an optimization perspective.
Machine learning methods attempt to build models that capture some element of
interest based on given data. Most common learning algorithms feature a set of
hyperparameters that must be determined before training commences. The choice
of hyperparameters can significantly affect the resulting model's performance,
but determining good values can be complex; hence a disciplined, theoretically
sound search strategy is essential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02131</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02131</id><created>2015-02-07</created><updated>2015-03-08</updated><authors><author><keyname>Berger</keyname><forenames>Ulrich</forenames><affiliation>Swansea University</affiliation></author><author><keyname>Lawrence</keyname><forenames>Andrew</forenames><affiliation>Swansea University</affiliation></author><author><keyname>Forsberg</keyname><forenames>Fredrik Nordvall</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>Seisenberger</keyname><forenames>Monika</forenames><affiliation>Swansea University</affiliation></author></authors><title>Extracting verified decision procedures: DPLL and Resolution</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 10,
  2015) lmcs:766</journal-ref><doi>10.2168/LMCS-11(1:6)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is concerned with the application of the program extraction
technique to a new class of problems: the synthesis of decision procedures for
the classical satisfiability problem that are correct by construction. To this
end, we formalize a completeness proof for the DPLL proof system and extract a
SAT solver from it. When applied to a propositional formula in conjunctive
normal form the program produces either a satisfying assignment or a DPLL
derivation showing its unsatisfiability. We use non-computational quantifiers
to remove redundant computational content from the extracted program and
translate it into Haskell to improve performance. We also prove the equivalence
between the resolution proof system and the DPLL proof system with a bound on
the size of the resulting resolution proof. This demonstrates that it is
possible to capture quantitative information about the extracted program on the
proof level. The formalization is carried out in the interactive proof
assistant Minlog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02134</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02134</id><created>2015-02-07</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Power Efficient Resource Allocation for Full-Duplex Radio Distributed
  Antenna Networks</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the resource allocation algorithm design for
distributed antenna multiuser networks with full-duplex (FD) radio base
stations (BSs) which enable simultaneous uplink and downlink communications.
The considered resource allocation algorithm design is formulated as an
optimization problem taking into account the antenna circuit power consumption
of the BSs and the quality of service (QoS) requirements of both uplink and
downlink users. We minimize the total network power consumption by jointly
optimizing the downlink beamformer, the uplink transmit power, and the antenna
selection. To overcome the intractability of the resulting problem, we
reformulate it as an optimization problem with decoupled binary selection
variables and non-convex constraints. The reformulated problem facilitates the
design of an iterative resource allocation algorithm which obtains an optimal
solution based on the generalized Bender's decomposition (GBD) and serves as a
benchmark scheme. Furthermore, to strike a balance between computational
complexity and system performance, a suboptimal algorithm with polynomial time
complexity is proposed. Simulation results illustrate that the proposed GBD
based iterative algorithm converges to the global optimal solution and the
suboptimal algorithm achieves a close-to-optimal performance. Our results also
demonstrate the trade-off between power efficiency and the number of active
transmit antennas when the circuit power consumption is taken into account. In
particular, activating an exceedingly large number of antennas may not be a
power efficient solution for reducing the total system power consumption. In
addition, our results reveal that FD systems facilitate significant power
savings compared to traditional half-duplex systems, despite the non-negligible
self-interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02135</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02135</id><created>2015-02-07</created><authors><author><keyname>Chakraborty</keyname><forenames>Diptarka</forenames></author><author><keyname>Tewari</keyname><forenames>Raghunath</forenames></author></authors><title>Simultaneous Time-Space Upper Bounds for Certain Problems in Planar
  Graphs</title><categories>cs.CC cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that given a weighted, directed planar graph $G$, and
any $\epsilon &gt;0$, there exists a polynomial time and
$O(n^{\frac{1}{2}+\epsilon})$ space algorithm that computes the shortest path
between two fixed vertices in $G$.
  We also consider the {\RB} problem, which states that given a graph $G$ whose
edges are colored either red or blue and two fixed vertices $s$ and $t$ in $G$,
is there a path from $s$ to $t$ in $G$ that alternates between red and blue
edges. The {\RB} problem in planar DAGs is {\NL}-complete. We exhibit a
polynomial time and $O(n^{\frac{1}{2}+\epsilon})$ space algorithm (for any
$\epsilon &gt;0$) for the {\RB} problem in planar DAG.
  In the last part of this paper, we consider the problem of deciding and
constructing the perfect matching present in a planar bipartite graph and also
a similar problem which is to find a Hall-obstacle in a planar bipartite graph.
We show the time-space bound of these two problems are same as the bound of
shortest path problem in a directed planar graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02137</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02137</id><created>2015-02-07</created><authors><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>Chunlei</forenames></author></authors><title>A class of cyclic codes whose dual have five zeros</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a family of cyclic codes over $\mathbb{F}_{p}$ whose duals
have five zeros is presented, where $p$ is an odd prime. Furthermore, the
weight distributions of these cyclic codes are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02139</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02139</id><created>2015-02-07</created><authors><author><keyname>Ch&#xe1;vez</keyname><forenames>Gustavo</forenames></author><author><keyname>Rockwood</keyname><forenames>Alyn</forenames></author></authors><title>Marching Surfaces: Isosurface Approximation using G$^1$ Multi-Sided
  Surfaces</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marching surfaces is a method for isosurface extraction and approximation
based on a $G^1$ multi-sided patch interpolation scheme. Given a 3D grid of
scalar values, an underlying curve network is formed using second order
information and cubic Hermite splines. Circular arc fitting defines the tangent
vectors for the Hermite curves at specified isovalues. Once the boundary curve
network is formed, a loop of curves is determined for each grid cell and then
interpolated with multi-sided surface patches, which are $G^1$ continuous at
the joins. The data economy of the method and its continuity preserving
properties provide an effective compression scheme, ideal for indirect volume
rendering on mobile devices, or collaborating on the Internet, while enhancing
visual fidelity. The use of multi-sided patches enables a more natural way to
approximate the isosurfaces than using a fixed number of sides or polygons as
is proposed in the literature. This assertion is supported with comparisons to
the traditional Marching Cubes algorithm and other $G^1$ methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02155</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02155</id><created>2015-02-07</created><authors><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Niazadeh</keyname><forenames>Rad</forenames></author></authors><title>Secretary Problems with Non-Uniform Arrival Order</title><categories>cs.DS cs.CC math.OC</categories><comments>To appear in Proceedings of the 47th Annual ACM Symposium on Theory
  of Computing (STOC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many online problems, it is known that the uniform arrival order enables
the design of algorithms with much better performance guarantees than under
worst-case. The quintessential example is the secretary problem. If the
sequence of elements is presented in uniformly random order there is an
algorithm that picks the maximum value with probability 1/e, whereas no
non-trivial performance guarantee is possible if the elements arrive in
worst-case order. This work initiates an investigation into relaxations of the
random-ordering hypothesis in online algorithms, by focusing on the secretary
problems. We present two sets of properties of distributions over permutations
as sufficient conditions, called the block-independence property and
uniform-induced-ordering property. We show these two are asymptotically
equivalent by borrowing some techniques from the approximation theory.
Moreover, we show they both imply the existence of secretary algorithms with
constant probability of correct selection, approaching the optimal constant 1/e
in the limit. We substantiate our idea by providing several constructions of
distributions that satisfy block-independence. We also show that {\Theta}(log
log n) is the minimum entropy of any permutation distribution that permits
constant probability of correct selection in the secretary problem with n
elements. While our block-independence condition is sufficient for constant
probability of correct selection, it is not necessary; however, we present
complexity-theoretic evidence that no simple necessary and sufficient criterion
exists. Finally, we explore the extent to which the performance guarantees of
other algorithms are preserved when one relaxes the uniform random ordering
assumption, obtaining a positive result for Kleinberg's multiple-choice
secretary algorithm and a negative result for the weighted bipartite matching
algorithm of Korula and Pal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02158</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02158</id><created>2015-02-07</created><authors><author><keyname>Weiss</keyname><forenames>Roi</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author></authors><title>Learning Parametric-Output HMMs with Two Aliased States</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various applications involving hidden Markov models (HMMs), some of the
hidden states are aliased, having identical output distributions. The
minimality, identifiability and learnability of such aliased HMMs have been
long standing problems, with only partial solutions provided thus far. In this
paper we focus on parametric-output HMMs, whose output distributions come from
a parametric family, and that have exactly two aliased states. For this class,
we present a complete characterization of their minimality and identifiability.
Furthermore, for a large family of parametric output distributions, we derive
computationally efficient and statistically consistent algorithms to detect the
presence of aliasing and learn the aliased HMM transition and emission
parameters. We illustrate our theoretical analysis by several simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02160</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02160</id><created>2015-02-07</created><authors><author><keyname>Hassanein</keyname><forenames>Allam Shehata</forenames></author><author><keyname>Mohammad</keyname><forenames>Sherien</forenames></author><author><keyname>Sameer</keyname><forenames>Mohamed</forenames></author><author><keyname>Ragab</keyname><forenames>Mohammad Ehab</forenames></author></authors><title>A Survey on Hough Transform, Theory, Techniques and Applications</title><categories>cs.CV</categories><comments>18 pages, and 11 figures</comments><journal-ref>IJCSI Volume 12, Issue 1, January 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For more than half a century, the Hough transform is ever-expanding for new
frontiers. Thousands of research papers and numerous applications have evolved
over the decades. Carrying out an all-inclusive survey is hardly possible and
enormously space-demanding. What we care about here is emphasizing some of the
most crucial milestones of the transform. We describe its variations
elaborating on the basic ones such as the line and circle Hough transforms. The
high demand for storage and computation time is clarified with different
solution approaches. Since most uses of the transform take place on binary
images, we have been concerned with the work done directly on gray or color
images. The myriad applications of the standard transform and its variations
have been classified highlighting the up-to-date and the unconventional ones.
Due to its merits such as noise-immunity and expandability, the transform has
an excellent history, and a bright future as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02163</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02163</id><created>2015-02-07</created><authors><author><keyname>Shi</keyname><forenames>Gui-Yuan</forenames></author><author><keyname>Kong</keyname><forenames>Yi-Xiu</forenames></author><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Analysis of ground state in random bipartite matching</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In human society, a lot of social phenomena can be concluded into a
mathematical problem called the bipartite matching, one of the most well known
model is the marriage problem proposed by Gale and Shapley. In this article, we
try to find out some intrinsic properties of the ground state of this model and
thus gain more insights and ideas about the matching problem. We apply
Kuhn-Munkres Algorithm to find out the numerical ground state solution of the
system. The simulation result proves the previous theoretical analysis using
replica method. In the result, we also find out the amount of blocking pairs
which can be regarded as a representative of the system stability. Furthermore,
we discover that the connectivity in the bipartite matching problem has a great
impact on the stability of the ground state, and the system will become more
unstable if there were more connections between men and women.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02168</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02168</id><created>2015-02-07</created><updated>2015-08-26</updated><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Multilayer Hadamard Decomposition of Discrete Hartley Transforms</title><categories>cs.DS cs.DM</categories><comments>Fixed several typos. 7 pages, 5 figures, XVIII Simp\'osio Brasileiro
  de Telecomunica\c{c}\~oes, 2000, Gramado, RS, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete transforms such as the discrete Fourier transform (DFT) or the
discrete Hartley transform (DHT) furnish an indispensable tool in signal
processing. The successful application of transform techniques relies on the
existence of the so-called fast transforms. In this paper some fast algorithms
are derived which meet the lower bound on the multiplicative complexity of the
DFT/DHT. The approach is based on a decomposition of the DHT into layers of
Walsh-Hadamard transforms. In particular, fast algorithms for short block
lengths such as $N \in \{4, 8, 12, 24\}$ are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02171</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02171</id><created>2015-02-07</created><authors><author><keyname>Zheng</keyname><forenames>Liang</forenames></author><author><keyname>Shen</keyname><forenames>Liyue</forenames></author><author><keyname>Tian</keyname><forenames>Lu</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author><author><keyname>Bu</keyname><forenames>Jiahao</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Person Re-identification Meets Image Search</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For long time, person re-identification and image search are two separately
studied tasks. However, for person re-identification, the effectiveness of
local features and the &quot;query-search&quot; mode make it well posed for image search
techniques.
  In the light of recent advances in image search, this paper proposes to treat
person re-identification as an image search problem. Specifically, this paper
claims two major contributions. 1) By designing an unsupervised Bag-of-Words
representation, we are devoted to bridging the gap between the two tasks by
integrating techniques from image search in person re-identification. We show
that our system sets up an effective yet efficient baseline that is amenable to
further supervised/unsupervised improvements. 2) We contribute a new high
quality dataset which uses DPM detector and includes a number of distractor
images. Our dataset reaches closer to realistic settings, and new perspectives
are provided.
  Compared with approaches that rely on feature-feature match, our method is
faster by over two orders of magnitude. Moreover, on three datasets, we report
competitive results compared with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02174</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02174</id><created>2015-02-07</created><updated>2015-11-03</updated><authors><author><keyname>Kimmel</keyname><forenames>Shelby</forenames></author><author><keyname>Lin</keyname><forenames>Cedric Yen-Yu</forenames></author><author><keyname>Lin</keyname><forenames>Han-Hsuan</forenames></author></authors><title>Oracles with Costs</title><categories>quant-ph cs.CC</categories><comments>In this version: typos fixed and motivating examples added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While powerful tools have been developed to analyze quantum query complexity,
there are still many natural problems that do not fit neatly into the black box
model of oracles. We create a new model that allows multiple oracles with
differing costs. This model captures more of the difficulty of certain natural
problems. We test this model on a simple problem, Search with Two Oracles, for
which we create a quantum algorithm that we prove is asymptotically optimal. We
further give some evidence, using a geometric picture of Grover's algorithm,
that our algorithm is exactly optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02178</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02178</id><created>2015-02-07</created><authors><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author><author><keyname>Mor</keyname><forenames>Ami</forenames></author></authors><title>On the Greedy Algorithm for Combinatorial Auctions with a Random Order</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we study the greedy algorithm for combinatorial auctions with
submodular bidders. It is well known that this algorithm provides an
approximation ratio of $2$ for every order of the items. We show that if the
valuations are vertex cover functions and the order is random then the expected
approximation ratio imrpoves to $\frac 7 4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02179</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02179</id><created>2015-02-07</created><updated>2015-05-25</updated><authors><author><keyname>Chynonova</keyname><forenames>Maryna</forenames></author><author><keyname>Morsi</keyname><forenames>Rania</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Optimal Multiuser Scheduling Schemes for Simultaneous Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at the European Signal Processing
  Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the downlink multiuser scheduling problem for systems
with simultaneous wireless information and power transfer (SWIPT). We design
optimal scheduling algorithms that maximize the long-term average system
throughput under different fairness requirements, such as proportional fairness
and equal throughput fairness. In particular, the algorithm designs are
formulated as non-convex optimization problems which take into account the
minimum required average sum harvested energy in the system. The problems are
solved by using convex optimization techniques and the proposed optimization
framework reveals the tradeoff between the long-term average system throughput
and the sum harvested energy in multiuser systems with fairness constraints.
Simulation results demonstrate that substantial performance gains can be
achieved by the proposed optimization framework compared to existing suboptimal
scheduling algorithms from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02182</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02182</id><created>2015-02-07</created><updated>2015-03-03</updated><authors><author><keyname>Badnjar</keyname><forenames>Jelena</forenames></author></authors><title>Comparison of Algorithms for Compressed Sensing of Magnetic Resonance
  Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonance imaging (MRI) is an essential medical tool with inherently
slow data acquisition process. Slow acquisition process requires patient to be
long time exposed to scanning apparatus. In recent years significant efforts
are made towards the applying Compressive Sensing technique to the acquisition
process of MRI and biomedical images. Compressive Sensing is an emerging theory
in signal processing. It aims to reduce the amount of acquired data required
for successful signal reconstruction. Reducing the amount of acquired image
coefficients leads to lower acquisition time, i.e. time of exposition to the
MRI apparatus. Using optimization algorithms, satisfactory image quality can be
obtained from the small set of acquired samples. A number of optimization
algorithms for the reconstruction of the biomedical images is proposed in the
literature. In this paper, three commonly used optimization algorithms are
compared and results are presented on the several MRI images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02188</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02188</id><created>2015-02-07</created><updated>2015-03-09</updated><authors><author><keyname>Suhov</keyname><forenames>Yuri</forenames></author><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author><author><keyname>Kelbert</keyname><forenames>Mark</forenames></author></authors><title>Entropy-power inequality for weighted entropy</title><categories>cs.IT math.IT math.PR</categories><comments>17 pages, 2 figures</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse an analog of the entropy-power inequality for the weighted
entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02191</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02191</id><created>2015-02-07</created><authors><author><keyname>Georgiou</keyname><forenames>Harris V.</forenames></author></authors><title>Collective decision efficiency and optimal voting mechanisms: A
  comprehensive overview for multi-classifier models</title><categories>cs.GT</categories><report-no>HG/GT.0214.01v1</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A new game-theoretic approach for combining multiple classifiers is proposed.
A short introduction in Game Theory and coalitions illustrate the way any
collective decision scheme can be viewed as a competitive game of coalitions
that are formed naturally when players state their preferences. The winning
conditions and the voting power of each player are studied under the scope of
voting power indices, as well and the collective competence of the group.
Coalitions and power indices are presented in relation to the Condorcet
criterion of optimality in voting systems, and weighted Borda count models are
asserted as a way to implement them in practice. A special case of coalition
games, the weighted majority games (WMG) are presented as a restricted
realization in dichotomy choice situations. As a result, the weighted majority
rules (WMR), an extended version of the simple majority rules, are asserted as
the theoretically optimal and complete solution to this type of coalition
gaming. Subsequently, a generalized version of WMRs is suggested as the means
to design a voting system that is optimal in the sense of both the correct
classification criterion and the Condorcet efficiency criterion. In the scope
of Pattern Recognition, a generalized risk-based approach is proposed as the
framework upon which any classifier combination scheme can be applied. A new
fully adaptive version of WMRs is proposed as a statistically invariant way of
adjusting the design process of the optimal WMR to the arbitrary
non-symmetrical properties of the underlying feature space. SVM theory is
associated with properties and conclusions that emerge from the game-theoretic
approach of the classification in general, while the theoretical and practical
implications of employing SVM experts in WMR combination schemes are briefly
discussed. Finally, a summary of the most important issues for further research
is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02193</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02193</id><created>2015-02-07</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>The Silver Lining Around Fearful Living</title><categories>cs.AI</categories><comments>4 pages, Psychology Today (online).
  https://www.psychologytoday.com/blog/mindbloggling/201502/the-silver-lining-around-fearful-living-0
  (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses in layperson's terms human and computational studies of
the impact of threat and fear on exploration and creativity. A first study
showed that both killifish from a lake with predators and from a lake without
predators explore a new environment to the same degree and plotting number of
new spaces covered over time generates a hump-shaped curve. However, for the
fish from the lake with predators the curve is shifted to the right; they take
longer. This pattern was replicated by a computer model of exploratory behavior
varying only one parameter, the fear parameter. A second study showed that
stories inspired by threatening photographs were rated as more creative than
stories inspired by non-threatening photographs. Various explanations for the
findings are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02206</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02206</id><created>2015-02-07</created><updated>2015-05-20</updated><authors><author><keyname>Chang</keyname><forenames>Kai-Wei</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Daum&#xe9;</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Learning to Search Better Than Your Teacher</title><categories>cs.LG stat.ML</categories><comments>In ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods for learning to search for structured prediction typically imitate a
reference policy, with existing theoretical guarantees demonstrating low regret
compared to that reference. This is unsatisfactory in many applications where
the reference policy is suboptimal and the goal of learning is to improve upon
it. Can learning to search work even when the reference is poor?
  We provide a new learning to search algorithm, LOLS, which does well relative
to the reference policy, but additionally guarantees low regret compared to
deviations from the learned policy: a local-optimality guarantee. Consequently,
LOLS can improve upon the reference policy, unlike previous algorithms. This
enables us to develop structured contextual bandits, a partial information
structured prediction setting with many potential applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02215</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02215</id><created>2015-02-08</created><authors><author><keyname>Wilson</keyname><forenames>Jobin</forenames></author><author><keyname>Kachappilly</keyname><forenames>Chitharanj</forenames></author><author><keyname>Mohan</keyname><forenames>Rakesh</forenames></author><author><keyname>Kapadia</keyname><forenames>Prateek</forenames></author><author><keyname>Soman</keyname><forenames>Arun</forenames></author><author><keyname>Chaudhury</keyname><forenames>Santanu</forenames></author></authors><title>Real World Applications of Machine Learning Techniques over Large Mobile
  Subscriber Datasets</title><categories>cs.LG cs.CY cs.SE</categories><comments>SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)
  https://sites.google.com/site/software4ml/accepted-papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication Service Providers (CSPs) are in a unique position to utilize
their vast transactional data assets generated from interactions of subscribers
with network elements as well as with other subscribers. CSPs could leverage
its data assets for a gamut of applications such as service personalization,
predictive offer management, loyalty management, revenue forecasting, network
capacity planning, product bundle optimization and churn management to gain
significant competitive advantage. However, due to the sheer data volume,
variety, velocity and veracity of mobile subscriber datasets, sophisticated
data analytics techniques and frameworks are necessary to derive actionable
insights in a useable timeframe. In this paper, we describe our journey from a
relational database management system (RDBMS) based campaign management
solution which allowed data scientists and marketers to use hand-written rules
for service personalization and targeted promotions to a distributed Big Data
Analytics platform, capable of performing large scale machine learning and data
mining to deliver real time service personalization, predictive modelling and
product optimization. Our work involves a careful blend of technology,
processes and best practices, which facilitate man-machine collaboration and
continuous experimentation to derive measurable economic value from data. Our
platform has a reach of more than 500 million mobile subscribers worldwide,
delivering over 1 billion personalized recommendations annually, processing a
total data volume of 64 Petabytes, corresponding to 8.5 trillion events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02218</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02218</id><created>2015-02-08</created><updated>2016-03-08</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Universal channel coding for general output alphabet</title><categories>cs.IT math.IT</categories><comments>The assumption for exponential family is removed. Several errors are
  fixed and some new results are added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two types of universal codes that are suited to two asymptotic
regimes when the output alphabet is possibly continuous. The first class has
the property that the error probability decays exponentially fast and we
identify an explicit lower bound on the error exponent. The other class attains
the epsilon-capacity the channel and we also identify the second-order term in
the asymptotic expansion. The proposed encoder is essentially based on the
packing lemma of the method of types. For the decoder, we first derive a
R\'enyi-divergence version of Clarke and Barron's formula the distance between
the true distribution and the Bayesian mixture. This result is of independent
interest and was not required in the author's previous paper. The universal
decoder is stated in terms of this formula and quantities used in the
information spectrum method. The methods contained herein allow us to analyze
universal codes for channels with continuous and discrete output alphabets in a
unified manner, and to analyze their performances in terms of the exponential
decay of the error probability and the second-order coding rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02223</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02223</id><created>2015-02-08</created><updated>2015-08-08</updated><authors><author><keyname>Lee</keyname><forenames>Sungmin</forenames></author><author><keyname>Min</keyname><forenames>Hyeyoung</forenames></author><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author></authors><title>Will solid-state drives accelerate your bioinformatics? In-depth
  profiling, performance analysis, and beyond</title><categories>q-bio.GN cs.CE q-bio.QM</categories><comments>Availability: http://best.snu.ac.kr/pub/biossd; to be published in
  Briefings in Bioinformatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of large-scale data has been produced in bioinformatics. In
response, the need for efficient handling of biomedical big data has been
partly met by parallel computing. However, the time demand of many
bioinformatics programs still remains high for large-scale practical uses due
to factors that hinder acceleration by parallelization. Recently, new
generations of storage devices have emerged, such as NAND flash-based
solid-state drives (SSDs), and with the renewed interest in near-data
processing, they are increasingly becoming acceleration methods that can
accompany parallel processing. In certain cases, a simple drop-in replacement
of hard disk drives (HDDs) by SSDs results in dramatic speedup. Despite the
various advantages and continuous cost reduction of SSDs, there has been little
review of SSD-based profiling and performance exploration of important but
time-consuming bioinformatics programs. For an informative review, we perform
in-depth profiling and analysis of 23 key bioinformatics programs using
multiple types of devices. Based on the insight we obtain from this research,
we further discuss issues related to design and optimize bioinformatics
algorithms and pipelines to fully exploit SSDs. The programs we profile cover
traditional and emerging areas of importance, such as alignment, assembly,
mapping, expression analysis, variant calling, and metagenomics. We explain how
acceleration by parallelization can be combined with SSDs for improved
performance and also how using SSDs can expedite important bioinformatics
pipelines, such as variant calling by the Genome Analysis Toolkit (GATK) and
transcriptome analysis using RNA sequencing (RNA-seq). We hope that this review
can provide useful directions and tips to accompany future bioinformatics
algorithm design procedures that properly consider new generations of powerful
storage devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02226</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02226</id><created>2015-02-08</created><authors><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Baochun</forenames></author><author><keyname>Sun</keyname><forenames>Lifeng</forenames></author><author><keyname>Zhu</keyname><forenames>Wenwu</forenames></author><author><keyname>Yang</keyname><forenames>Shiqiang</forenames></author></authors><title>Dispersing Instant Social Video Service Across Multiple Clouds</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Instant social video sharing which combines the online social network and
user-generated short video streaming services, has become popular in today's
Internet. Cloud-based hosting of such instant social video contents has become
a norm to serve the increasing users with user-generated contents. A
fundamental problem of cloud-based social video sharing service is that users
are located globally, who cannot be served with good service quality with a
single cloud provider. In this paper, we investigate the feasibility of
dispersing instant social video contents to multiple cloud providers. The
challenge is that inter-cloud social \emph{propagation} is indispensable with
such multi-cloud social video hosting, yet such inter-cloud traffic incurs
substantial operational cost. We analyze and formulate the multi-cloud hosting
of an instant social video system as an optimization problem. We conduct
large-scale measurement studies to show the characteristics of instant social
video deployment, and demonstrate the trade-off between satisfying users with
their ideal cloud providers, and reducing the inter-cloud data propagation. Our
measurement insights of the social propagation allow us to propose a heuristic
algorithm with acceptable complexity to solve the optimization problem, by
partitioning a propagation-weighted social graph in two phases: a
preference-aware initial cloud provider selection and a propagation-aware
re-hosting. Our simulation experiments driven by real-world social network
traces show the superiority of our design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02233</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02233</id><created>2015-02-08</created><authors><author><keyname>Beykikhoshk</keyname><forenames>Adham</forenames></author><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Hierarchical Dirichlet process for tracking complex topical structure
  evolution and its application to autism research literature</title><categories>cs.IR cs.CL</categories><comments>In Proc. Pacific-Asia Conference on Knowledge Discovery and Data
  Mining (PAKDD), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a novel framework for the discovery of the topical
content of a data corpus, and the tracking of its complex structural changes
across the temporal dimension. In contrast to previous work our model does not
impose a prior on the rate at which documents are added to the corpus nor does
it adopt the Markovian assumption which overly restricts the type of changes
that the model can capture. Our key technical contribution is a framework based
on (i) discretization of time into epochs, (ii) epoch-wise topic discovery
using a hierarchical Dirichlet process-based model, and (iii) a temporal
similarity graph which allows for the modelling of complex topic changes:
emergence and disappearance, evolution, and splitting and merging. The power of
the proposed framework is demonstrated on the medical literature corpus
concerned with the autism spectrum disorder (ASD) - an increasingly important
research subject of significant social and healthcare importance. In addition
to the collected ASD literature corpus which we will make freely available, our
contributions also include two free online tools we built as aids to ASD
researchers. These can be used for semantically meaningful navigation and
searching, as well as knowledge discovery from this large and rapidly growing
corpus of literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02234</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02234</id><created>2015-02-08</created><authors><author><keyname>Ambrosin</keyname><forenames>Moreno</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>De Gaspari</keyname><forenames>Fabio</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author></authors><title>LineSwitch: Efficiently Managing Switch Flow in Software-Defined
  Networking while Effectively Tackling DoS Attacks</title><categories>cs.NI</categories><comments>In Proceedings of the 10th ACM Symposium on Information, Computer and
  Communications Security (ASIACCS 2015). To appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN) is a new networking architecture which aims
to provide better decoupling between network control (control plane) and data
forwarding functionalities (data plane). This separation introduces several
benefits, such as a directly programmable and (virtually) centralized network
control. However, researchers showed that the required communication channel
between the control and data plane of SDN creates a potential bottleneck in the
system, introducing new vulnerabilities. Indeed, this behavior could be
exploited to mount powerful attacks, such as the control plane saturation
attack, that can severely hinder the performance of the whole network.
  In this paper we present LineSwitch, an efficient and effective solution
against control plane saturation attack. LineSwitch combines SYN proxy
techniques and probabilistic blacklisting of network traffic. We implemented
LineSwitch as an extension of OpenFlow, the current reference implementation of
SDN, and evaluate our solution considering different traffic scenarios (with
and without attack). The results of our preliminary experiments confirm that,
compared to the state-of-the-art, LineSwitch reduces the time overhead up to
30%, while ensuring the same level of protection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02236</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02236</id><created>2015-02-08</created><authors><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author><author><keyname>Oh</keyname><forenames>Nahmsuk</forenames></author><author><keyname>Tehrani</keyname><forenames>Peivand</forenames></author><author><keyname>Chung</keyname><forenames>Eui-Young</forenames></author><author><keyname>De Micheli</keyname><forenames>Giovanni</forenames></author></authors><title>FRAME: Fast and Realistic Attacker Modeling and Evaluation for Temporal
  Logical Correlation in Static Noise</title><categories>cs.OH</categories><acm-class>B.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method called Fast and Realistic Attacker Modeling and
Evaluation (FRAME) that can reduce pessimism in static noise analysis by
exploiting temporal logical correlation of attackers and using novel techniques
termed envelopes and $\sigma$ functions. Unlike conventional pruning-based
approaches, FRAME efficiently considers all relevant attackers, thereby
producing more realistic results. FRAME was tested with complex industrial
design and successfully reduced the pessimism of conventional techniques by
30.4% on average, with little computational overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02239</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02239</id><created>2015-02-08</created><authors><author><keyname>Chung</keyname><forenames>Eui-Young</forenames></author><author><keyname>Son</keyname><forenames>Chang-Il</forenames></author><author><keyname>Bang</keyname><forenames>Kwanhu</forenames></author><author><keyname>Kim</keyname><forenames>Dong</forenames></author><author><keyname>Shin</keyname><forenames>Soong-Mann</forenames></author><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author></authors><title>A High-Performance Solid-State Disk with Double-Data-Rate NAND Flash
  Memory</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel solid-state disk (SSD) architecture that utilizes a
double-data-rate synchronous NAND flash interface for improving read and write
performance. Unlike the conventional design, the data transfer rate in the
proposed design is doubled in harmony with synchronous signaling. The new
architecture does not require any extra pins with respect to the conventional
architecture, thereby guaranteeing backward compatibility. For performance
evaluation, we simulated various SSD designs that adopt the proposed
architecture and measured their performance in terms of read/write bandwidths
and energy consumption. Both NAND flash cell types, namely single-level cells
(SLCs) and multi-level cells (MLCs), were considered. In the experiments using
SLC-type NAND flash chips, the read and write speeds of the proposed
architecture were 1.65-2.76 times and 1.09-2.45 times faster than those of the
conventional architecture, respectively. Similar improvements were observed for
the MLC-based architectures tested. It was particularly effective to combine
the proposed architecture with the way-interleaving technique that multiplexes
the data channel between the controller and each flash chip. For a reasonably
high degree of way interleaving, the read/write performance and the energy
consumption of our approach were notably better than those of the conventional
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02242</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02242</id><created>2015-02-08</created><updated>2016-01-31</updated><authors><author><keyname>Hellings</keyname><forenames>Jelle</forenames></author></authors><title>Querying for Paths in Graphs using Context-Free Path Queries</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigational queries for graph-structured data, such as the regular path
queries and the context-free path queries, are usually evaluated to a relation
of node-pairs $(m, n)$ such that there is a path from $m$ to $n$ satisfying the
conditions of the query. Although this relational query semantics has practical
value, we believe that the relational query semantics can only provide limited
insight in the structure of the graph data. To address the limits of the
relational query semantics, we introduce the all-path query semantics and the
single-path query semantics. Under these path-based query semantics, a query is
evaluated to all paths satisfying the conditions of the query, or,
respectively, to a single such path.
  While focusing on context-free path queries, we provide a formal framework
for evaluating queries on graphs using both path-based query semantics. For the
all-path query semantics, we show that the result of a query can be represented
by a finite context-free grammar annotated with node-information relevant for
deriving each path in the query result. For the single-path query semantics, we
propose to search for a path of minimum length. We reduce the problem of
finding such a path of minimum length to finding a string of minimum length in
a context-free language, and for deriving such a string we propose a novel
algorithm.
  Our initial results show that the path-based query semantics have added
practical value and that query evaluation for both path-based query semantics
is feasible, even when query results grow very large. For the single-path query
semantics, determining strict worst-case upper bounds on the size of the query
result remains the focus of future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02245</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02245</id><created>2015-02-08</created><authors><author><keyname>Shen</keyname><forenames>Xinyue</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Restricted Isometry Property of Subspace Projection Matrix Under Random
  Compression</title><categories>cs.IT math.IT</categories><comments>11 pages, 1 figure, journal paper</comments><doi>10.1109/LSP.2015.2402206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structures play a significant role in the field of signal processing. As a
representative of structural data, low rank matrix along with its restricted
isometry property (RIP) has been an important research topic in compressive
signal processing. Subspace projection matrix is a kind of low rank matrix with
additional structure, which allows for further reduction of its intrinsic
dimension. This leaves room for improving its own RIP, which could work as the
foundation of compressed subspace projection matrix recovery. In this work, we
study the RIP of subspace projection matrix under random orthonormal
compression. Considering the fact that subspace projection matrices of $s$
dimensional subspaces in $\mathbb{R}^N$ form an $s(N-s)$ dimensional
submanifold in $\mathbb{R}^{N\times N}$, our main concern is transformed to the
stable embedding of such submanifold into $\mathbb{R}^{N\times N}$. The result
is that by $O(s(N-s)\log N)$ number of random measurements the RIP of subspace
projection matrix is guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02251</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02251</id><created>2015-02-08</created><updated>2015-06-18</updated><authors><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Niklas</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>From Pixels to Torques: Policy Learning with Deep Dynamical Models</title><categories>stat.ML cs.LG cs.RO cs.SY</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-efficient learning in continuous state-action spaces using very
high-dimensional observations remains a key challenge in developing fully
autonomous systems. In this paper, we consider one instance of this challenge,
the pixels to torques problem, where an agent must learn a closed-loop control
policy from pixel information only. We introduce a data-efficient, model-based
reinforcement learning algorithm that learns such a closed-loop policy directly
from pixel information. The key ingredient is a deep dynamical model that uses
deep auto-encoders to learn a low-dimensional embedding of images jointly with
a predictive model in this low-dimensional feature space. Joint learning
ensures that not only static but also dynamic properties of the data are
accounted for. This is crucial for long-term predictions, which lie at the core
of the adaptive model predictive control strategy that we use for closed-loop
control. Compared to state-of-the-art reinforcement learning methods for
continuous states and actions, our approach learns quickly, scales to
high-dimensional state spaces and is an important step toward fully autonomous
learning from pixels to torques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02253</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02253</id><created>2015-02-08</created><authors><author><keyname>Pezeshkpour</keyname><forenames>Pouya</forenames></author><author><keyname>Tabandeh</keyname><forenames>Mahmoud</forenames></author></authors><title>Data Bits in Karnaugh Map and Increasing Map Capability in Error
  Correcting</title><categories>cs.IT math.IT</categories><comments>8 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To provide reliable communication in data transmission, ability of correcting
errors is of prime importance. This paper intends to suggest an easy algorithm
to detect and correct errors in transmission codes using the well-known
Karnaugh map. Referring to past research done and proving new theorems and also
using a suggested simple technique taking advantage of the easy concept of
Karnaugh map, we offer an algorithm to reduce the number of occupied squares in
the map and therefore, reduce substantially the execution time for placing data
bits in Karnaugh map. Based on earlier papers, we first propose an algorithm
for correction of two simultaneous errors in a code. Then, defining
specifications for empty squares of the map, we limit the choices for selection
of new squares. In addition, burst errors in sending codes is discussed, and
systematically code words for correcting them will be made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02259</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02259</id><created>2015-02-08</created><authors><author><keyname>Hallak</keyname><forenames>Assaf</forenames></author><author><keyname>Di Castro</keyname><forenames>Dotan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Contextual Markov Decision Processes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a planning problem where the dynamics and rewards of the
environment depend on a hidden static parameter referred to as the context. The
objective is to learn a strategy that maximizes the accumulated reward across
all contexts. The new model, called Contextual Markov Decision Process (CMDP),
can model a customer's behavior when interacting with a website (the learner).
The customer's behavior depends on gender, age, location, device, etc. Based on
that behavior, the website objective is to determine customer characteristics,
and to optimize the interaction between them. Our work focuses on one basic
scenario--finite horizon with a small known number of possible contexts. We
suggest a family of algorithms with provable guarantees that learn the
underlying models and the latent contexts, and optimize the CMDPs. Bounds are
obtained for specific naive implementations, and extensions of the framework
are discussed, laying the ground for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02265</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02265</id><created>2015-02-08</created><updated>2015-03-02</updated><authors><author><keyname>Karavelas</keyname><forenames>Menelaos I.</forenames></author><author><keyname>Tzanaki</keyname><forenames>Eleni</forenames></author></authors><title>A geometric approach for the upper bound theorem for Minkowski sums of
  convex polytopes</title><categories>cs.CG math.CO</categories><comments>43 pages; minor changes (mostly typos)</comments><msc-class>52B05, 52B11, 52C45, 68U05</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive tight expressions for the maximum number of $k$-faces,
$0\le{}k\le{}d-1$, of the Minkowski sum, $P_1+...+P_r$, of $r$ convex
$d$-polytopes $P_1,...,P_r$ in $\mathbb{R}^d$, where $d\ge{}2$ and $r&lt;d$, as a
(recursively defined) function on the number of vertices of the polytopes.
  Our results coincide with those recently proved by Adiprasito and Sanyal [2].
In contrast to Adiprasito and Sanyal's approach, which uses tools from
Combinatorial Commutative Algebra, our approach is purely geometric and uses
basic notions such as $f$- and $h$-vector calculus and shellings, and
generalizes the methodology used in [15] and [14] for proving upper bounds on
the $f$-vector of the Minkowski sum of two and three convex polytopes,
respectively.
  The key idea behind our approach is to express the Minkowski sum
$P_1+...+P_r$ as a section of the Cayley polytope $\mathcal{C}$ of the
summands; bounding the $k$-faces of $P_1+...+P_r$ reduces to bounding the
subset of the $(k+r-1)$-faces of $\mathcal{C}$ that contain vertices from each
of the $r$ polytopes.
  We end our paper with a sketch of an explicit construction that establishes
the tightness of the upper bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02268</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02268</id><created>2015-02-08</created><authors><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author></authors><title>SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for minimizing regularized empirical loss:
Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each
iteration we update a random subset of the dual variables. However, unlike
existing methods such as stochastic dual coordinate ascent, SDNA is capable of
utilizing all curvature information contained in the examples, which leads to
striking improvements in both theory and practice - sometimes by orders of
magnitude. In the special case when an L2-regularizer is used in the primal,
the dual problem is a concave quadratic maximization problem plus a separable
term. In this regime, SDNA in each step solves a proximal subproblem involving
a random principal submatrix of the Hessian of the quadratic function; whence
the name of the method. If, in addition, the loss functions are quadratic, our
method can be interpreted as a novel variant of the recently introduced
Iterative Hessian Sketch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02272</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02272</id><created>2015-02-08</created><authors><author><keyname>Wehr</keyname><forenames>Dustin</forenames></author></authors><title>Rigorous Deductive Argumentation for Socially Relevant Issues</title><categories>cs.LO math.LO</categories><comments>PhD Thesis. 130 pages</comments><msc-class>03Bxx, 03B52, 03B10, 03Axx</msc-class><acm-class>I.2.3; F.4.m; I.2.4; F.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important problems for society are describable only in vague terms,
dependent on subjective positions, and missing highly relevant data. This
thesis is intended to revive and further develop the view that giving
non-trivial, rigorous deductive arguments concerning such problems -without
eliminating the complications of vagueness, subjectivity, and uncertainty- is,
though very difficult, not problematic in principle, does not require the
invention of new logics -classical first-order logic will do- and is something
that more mathematically-inclined people should be pursuing. The framework of
interpreted formal proofs is presented for formalizing and criticizing rigorous
deductive arguments about vague, subjective, and uncertain issues, and its
adequacy is supported largely by a number of major examples. This thesis also
documents progress towards a web system for collaboratively authoring and
criticizing such arguments, which is the ultimate goal of this project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02277</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02277</id><created>2015-02-08</created><authors><author><keyname>Na</keyname><forenames>Seung-Hoon</forenames></author><author><keyname>Kang</keyname><forenames>In-Su</forenames></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames></author></authors><title>Improving Term Frequency Normalization for Multi-topical Documents, and
  Application to Language Modeling Approaches</title><categories>cs.IR cs.CL</categories><comments>8 pages, conference paper, published in ECIR '08</comments><acm-class>H.3.3</acm-class><journal-ref>Advances in Information Retrieval Lecture Notes in Computer
  Science Volume 4956, 2008, pp 382-393</journal-ref><doi>10.1007/978-3-540-78646-7_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Term frequency normalization is a serious issue since lengths of documents
are various. Generally, documents become long due to two different reasons -
verbosity and multi-topicality. First, verbosity means that the same topic is
repeatedly mentioned by terms related to the topic, so that term frequency is
more increased than the well-summarized one. Second, multi-topicality indicates
that a document has a broad discussion of multi-topics, rather than single
topic. Although these document characteristics should be differently handled,
all previous methods of term frequency normalization have ignored these
differences and have used a simplified length-driven approach which decreases
the term frequency by only the length of a document, causing an unreasonable
penalization. To attack this problem, we propose a novel TF normalization
method which is a type of partially-axiomatic approach. We first formulate two
formal constraints that the retrieval model should satisfy for documents having
verbose and multi-topicality characteristic, respectively. Then, we modify
language modeling approaches to better satisfy these two constraints, and
derive novel smoothing methods. Experimental results show that the proposed
method increases significantly the precision for keyword queries, and
substantially improves MAP (Mean Average Precision) for verbose queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02280</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02280</id><created>2015-02-08</created><authors><author><keyname>Louka</keyname><forenames>M. A.</forenames></author><author><keyname>Missirlis</keyname><forenames>N. M.</forenames></author></authors><title>A comparison of the Extrapolated Successive Overrelaxation and the
  Preconditioned Simultaneous Displacement methods for augmented linear systems</title><categories>cs.NA math.NA</categories><comments>42 pages, 3 figures, 8 tables; Numerische Mathematik, 2015</comments><msc-class>65F10 - 65N20 - CR:5.13</msc-class><doi>10.1007/s00211-015-0697-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the impact of two types of preconditioning on the
numerical solution of large sparse augmented linear systems. The first
preconditioning matrix is the lower triangular part whereas the second is the
product of the lower triangular part with the upper triangular part of the
augmented system's coefficient matrix. For the first preconditioning matrix we
form the Generalized Modified Extrapolated Successive Overrelaxation (GMESOR)
method, whereas the second preconditioning matrix yields the Generalized
Modified Preconditioned Simultaneous Displacement (GMPSD) method, which is an
extrapolated form of the Symmetric Successive Overrelaxation method. We find
sufficient conditions for each aforementioned iterative method to converge. In
addition, we develop a geometric approach, for determining the optimum values
of their parameters and corresponding spectral radii. It is shown that both
iterative methods studied (GMESOR and GMPSD) attain the same rate of
convergence. Numerical results confirm our theoretical expectations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02281</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02281</id><created>2015-02-08</created><updated>2015-06-24</updated><authors><author><keyname>Johnstone</keyname><forenames>Patrick R.</forenames></author><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author></authors><title>Local and Global Convergence of an Inertial Version of Forward-Backward
  Splitting</title><categories>math.OC cs.NA math.NA</categories><comments>28 pages. The title has changed, several typos have been fixed and
  the abstract and introduction have been modified. All theoretical results are
  the same as earlier versions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of great interest in optimization is to minimize a sum of two
closed, proper, and convex functions where one is smooth and the other has a
computationally inexpensive proximal operator. In this paper we analyze a
family of Inertial Forward-Backward Splitting (I-FBS) algorithms for solving
this problem. We first apply a global Lyapunov analysis to I-FBS and prove weak
convergence of the iterates to a minimizer in a real Hilbert space. We then
show that the algorithms achieve local linear convergence for &quot;sparse
optimization&quot;, which is the important special case where the nonsmooth term is
the $\ell_1$-norm. This result holds under either a restricted strong convexity
or a strict complimentary condition and we do not require the objective to be
strictly convex. For certain parameter choices we determine an upper bound on
the number of iterations until the iterates are confined on a manifold
containing the solution set and linear convergence holds.
  The local linear convergence result for sparse optimization holds for the
Fast Iterative Shrinkage and Soft Thresholding Algorithm (FISTA) due to Beck
and Teboulle which is a particular parameter choice for I-FBS. In spite of its
optimal global objective function convergence rate, we show that FISTA is not
optimal for sparse optimization with respect to the local convergence rate. We
determine the locally optimal parameter choice for the I-FBS family. Finally we
propose a method which inherits the excellent global rate of FISTA but also has
excellent local rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02287</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02287</id><created>2015-02-08</created><authors><author><keyname>Yun</keyname><forenames>Heechul</forenames></author><author><keyname>Gondi</keyname><forenames>Santosh</forenames></author><author><keyname>Biswas</keyname><forenames>Siddhartha</forenames></author></authors><title>Protecting Memory-Performance Critical Sections in Soft Real-Time
  Applications</title><categories>cs.OS</categories><comments>technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft real-time applications such as multimedia applications often show bursty
memory access patterns---regularly requiring a high memory bandwidth for a
short duration of time. Such a period is often critical for timely data
processing. Hence, we call it a memory-performance critical section.
Unfortunately, in multicore architecture, non-real-time applications on
different cores may also demand high memory bandwidth at the same time, which
can substantially increase the time spent on the memory performance critical
sections.
  In this paper, we present BWLOCK, user-level APIs and a memory bandwidth
control mechanism that can protect such memory performance critical sections of
soft real-time applications. BWLOCK provides simple lock like APIs to declare
memory-performance critical sections. If an application enters a
memory-performance critical section, the memory bandwidth control system then
dynamically limit other cores' memory access rates to protect memory
performance of the application until the critical section finishes.
  From case studies with real-world soft real-time applications, we found (1)
such memory-performance critical sections do exist and are often easy to
identify; and (2) applying BWLOCK for memory critical sections significantly
improve performance of the soft real-time applications at a small or no cost in
throughput of non real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02290</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02290</id><created>2015-02-08</created><authors><author><keyname>Dutta</keyname><forenames>Chinmoy</forenames></author><author><keyname>Kanoria</keyname><forenames>Yashodhan</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author></authors><title>How Hard is Computing Parity with Noisy Communications?</title><categories>cs.DC cs.CC</categories><comments>17 pages</comments><acm-class>C.2.1; C.2.2; C.2.4; D.4.4; F.1.1; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a tight lower bound of $\Omega(N \log\log N)$ on the number of
transmissions required to compute the parity of $N$ input bits with constant
error in a noisy communication network of $N$ randomly placed sensors, each
having one input bit and communicating with others using local transmissions
with power near the connectivity threshold. This result settles the lower bound
question left open by Ying, Srikant and Dullerud (WiOpt 06), who showed how the
sum of all the $N$ bits can be computed using $O(N \log\log N)$ transmissions.
The same lower bound has been shown to hold for a host of other functions
including majority by Dutta and Radhakrishnan (FOCS 2008).
  Most works on lower bounds for communication networks considered mostly the
full broadcast model without using the fact that the communication in real
networks is local, determined by the power of the transmitters. In fact, in
full broadcast networks computing parity needs $\theta(N)$ transmissions. To
obtain our lower bound we employ techniques developed by Goyal, Kindler and
Saks (FOCS 05), who showed lower bounds in the full broadcast model by reducing
the problem to a model of noisy decision trees. However, in order to capture
the limited range of transmissions in real sensor networks, we adapt their
definition of noisy decision trees and allow each node of the tree access to
only a limited part of the input. Our lower bound is obtained by exploiting
special properties of parity computations in such noisy decision trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02298</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02298</id><created>2015-02-08</created><authors><author><keyname>Aiguier</keyname><forenames>Marc</forenames></author><author><keyname>Atif</keyname><forenames>Jamal</forenames></author><author><keyname>Bloch</keyname><forenames>Isabelle</forenames></author><author><keyname>Hudelot</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Belief revision in Institutions : A relaxation based approach</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief revision of knowledge bases represented by a set of sentences in a
given logic has been extensively studied but for specific logics, mainly
propositional, but also recently Horn and description logics. Here, we propose
to generalize this operation from a model-theoretic point of view, by defining
revision in a categorical abstract model theory known under the name of theory
of institutions. In this framework, we generalize to any institution the
characterization of the well known AGM postulates given by Katsuno and
Mendelzon for propositional logic in terms of minimal change with respect to an
ordering among interpretations. Moreover, we study how to define revision,
satisfying the AGM postulates, from relaxation notions that have been first
introduced in description logics to define dissimilarity measures between
concepts, and the consequence of which is to relax the set of models of the old
belief until it becomes consistent with the new pieces of knowledge. The
proposed general framework can be instantiated in different logics such as
propositional, description and Horn logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02304</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02304</id><created>2015-02-08</created><authors><author><keyname>Szalkai</keyname><forenames>Istvan</forenames></author><author><keyname>Dosa</keyname><forenames>Gyorgy</forenames></author></authors><title>Online Algorithms for a Generalized Parallel Machine Scheduling Problem</title><categories>cs.DC</categories><comments>International Conference on Recent Achievements in Mechatronics,
  Automation, Computer Sciences and Robotics at SAPIENTIA University, Tirgu
  Mures, Romania on 6-7th March, 2015. http://www.macro.ms.sapientia.ro/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider different online algorithms for a generalized scheduling problem
for parallel machines, described in details in the first section. This problem
is the generalization of the classical parallel machine scheduling problem,
when the make-span is minimized; in that case each job contains only one task.
On the other hand, the problem in consideration is still a special version of
the workflow scheduling problem. We present several heuristic algorithms and
compare them by computer tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02310</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02310</id><created>2015-02-08</created><authors><author><keyname>Devyatov</keyname><forenames>Rostislav</forenames></author></authors><title>On Subword Complexity of Morphic Sequences</title><categories>math.CO cs.FL</categories><comments>61 pages, 5 figures</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study structure of pure morphic and morphic sequences and prove the
following result: the subword complexity of arbitrary morphic sequence is
either $\Theta(n^{1+1/k})$ for some $k\in\mathbb N$, or is $O(n \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02322</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02322</id><created>2015-02-08</created><updated>2015-04-01</updated><authors><author><keyname>Nock</keyname><forenames>Richard</forenames></author><author><keyname>Patrini</keyname><forenames>Giorgio</forenames></author><author><keyname>Friedman</keyname><forenames>Arik</forenames></author></authors><title>Rademacher Observations, Private Data, and Boosting</title><categories>cs.LG</categories><msc-class>68Q32</msc-class><acm-class>E.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimization of the logistic loss is a popular approach to batch
supervised learning. Our paper starts from the surprising observation that,
when fitting linear (or kernelized) classifiers, the minimization of the
logistic loss is \textit{equivalent} to the minimization of an exponential
\textit{rado}-loss computed (i) over transformed data that we call Rademacher
observations (rados), and (ii) over the \textit{same} classifier as the one of
the logistic loss. Thus, a classifier learnt from rados can be
\textit{directly} used to classify \textit{observations}. We provide a learning
algorithm over rados with boosting-compliant convergence rates on the
\textit{logistic loss} (computed over examples). Experiments on domains with up
to millions of examples, backed up by theoretical arguments, display that
learning over a small set of random rados can challenge the state of the art
that learns over the \textit{complete} set of examples. We show that rados
comply with various privacy requirements that make them good candidates for
machine learning in a privacy framework. We give several algebraic, geometric
and computational hardness results on reconstructing examples from rados. We
also show how it is possible to craft, and efficiently learn from, rados in a
differential privacy framework. Tests reveal that learning from differentially
private rados can compete with learning from random rados, and hence with batch
learning from examples, achieving non-trivial privacy vs accuracy tradeoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02327</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02327</id><created>2015-02-08</created><authors><author><keyname>Rocha</keyname><forenames>Herbert</forenames></author><author><keyname>Ismail</keyname><forenames>Hussama</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas</forenames></author><author><keyname>Barreto</keyname><forenames>Raimundo</forenames></author></authors><title>Model Checking C Programs with Loops via k-Induction and Invariants</title><categories>cs.LO cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel proof by induction algorithm, which combines k-induction
with invariants to model check C programs with bounded and unbounded loops. The
k-induction algorithm consists of three cases: in the base case, we aim to find
a counterexample with up to k loop unwindings; in the forward condition, we
check whether loops have been fully unrolled and that the safety property P
holds in all states reachable within k unwindings; and in the inductive step,
we check that whenever P holds for k unwindings, it also holds after the next
unwinding of the system. For each step of the k-induction algorithm, we infer
invariants using affine constraints (i.e., polyhedral) to specify pre- and
post-conditions. The algorithm was implemented in two different ways, with and
without invariants using polyhedral, and the results were compared.
Experimental results show that both forms can handle a wide variety of safety
properties; however, the k-induction algorithm adopting polyhedral solves more
verification tasks, which demonstrate an improvement of the induction algorithm
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02328</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02328</id><created>2015-02-08</created><authors><author><keyname>Graehl</keyname><forenames>Jonathan</forenames></author></authors><title>Context-free Algorithms</title><categories>cs.FL cs.DS</categories><acm-class>G.2.2; F.2.2; F.4.1; D.3.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Algorithms on grammars/transducers with context-free derivations: hypergraph
reachability, shortest path, and inside-outside pruning of 'relatively useless'
arcs that are unused by any near-shortest paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02330</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02330</id><created>2015-02-08</created><authors><author><keyname>Luo</keyname><forenames>Yong</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Wen</keyname><forenames>Yonggang</forenames></author><author><keyname>Ramamohanarao</keyname><forenames>Kotagiri</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction</title><categories>stat.ML cs.CV cs.LG</categories><comments>20 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical correlation analysis (CCA) has proven an effective tool for
two-view dimension reduction due to its profound theoretical foundation and
success in practical applications. In respect of multi-view learning, however,
it is limited by its capability of only handling data represented by two-view
features, while in many real-world applications, the number of views is
frequently many more. Although the ad hoc way of simultaneously exploring all
possible pairs of features can numerically deal with multi-view data, it
ignores the high order statistics (correlation information) which can only be
discovered by simultaneously exploring all features.
  Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly
yet naturally generalizes CCA to handle the data of an arbitrary number of
views by analyzing the covariance tensor of the different views. TCCA aims to
directly maximize the canonical correlation of multiple (more than two) views.
Crucially, we prove that the multi-view canonical correlation maximization
problem is equivalent to finding the best rank-1 approximation of the data
covariance tensor, which can be solved efficiently using the well-known
alternating least squares (ALS) algorithm. As a consequence, the high order
correlation information contained in the different views is explored and thus a
more reliable common subspace shared by all features can be obtained. In
addition, a non-linear extension of TCCA is presented. Experiments on various
challenge tasks, including large scale biometric structure prediction, internet
advertisement classification and web image annotation, demonstrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02341</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02341</id><created>2015-02-08</created><updated>2015-06-02</updated><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>First Women, Second Sex: Gender Bias in Wikipedia</title><categories>cs.SI</categories><comments>10 pages, ACM style. Author's version of a paper to be presented at
  ACM Hypertext 2015</comments><acm-class>H.3.4</acm-class><doi>10.1145/2700171.2791036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contributing to history has never been as easy as it is today. Anyone with
access to the Web is able to play a part on Wikipedia, an open and free
encyclopedia. Wikipedia, available in many languages, is one of the most
visited websites in the world and arguably one of the primary sources of
knowledge on the Web. However, not everyone is contributing to Wikipedia from a
diversity point of view; several groups are severely underrepresented. One of
those groups is women, who make up approximately 16% of the current contributor
community, meaning that most of the content is written by men. In addition,
although there are specific guidelines of verifiability, notability, and
neutral point of view that must be adhered by Wikipedia content, these
guidelines are supervised and enforced by men.
  In this paper, we propose that gender bias is not about participation and
representation only, but also about characterization of women. We approach the
analysis of gender bias by defining a methodology for comparing the
characterizations of men and women in biographies in three aspects: meta-data,
language, and network structure. Our results show that, indeed, there are
differences in characterization and structure. Some of these differences are
reflected from the off-line world documented by Wikipedia, but other
differences can be attributed to gender bias in Wikipedia content. We
contextualize these differences in feminist theory and discuss their
implications for Wikipedia policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02348</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02348</id><created>2015-02-08</created><authors><author><keyname>Azamov</keyname><forenames>Nurulla</forenames></author></authors><title>MATLAB based language for generating randomized multiple choice
  questions</title><categories>cs.CY</categories><comments>73 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we describe a simple MATLAB based language which allows to
create randomized multiple choice questions with minimal effort. This language
has been successfully tested at Flinders University by the author in a number
of mathematics topics including Numerical Analysis, Abstract Algebra and
Partial Differential Equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02358</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02358</id><created>2015-02-08</created><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Using Heavy Clique Base Coarsening to Enhance Virtual Network Embedding</title><categories>cs.DC</categories><comments>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 6, No. 1, 2015</comments><doi>10.14569/IJACSA.2015.060118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network virtualization allows cloud infrastructure providers to accommodate
multiple virtual networks on a single physical network. However, mapping
multiple virtual network resources to physical network components, called
virtual network embedding (VNE), is known to be non-deterministic
polynomial-time hard (NP-hard). Effective virtual network embedding increases
the revenue by increasing the number of accepted virtual networks. In this
paper, we propose virtual network embedding algorithm, which improves virtual
network embedding by coarsening virtual networks. Heavy Clique matching
technique is used to coarsen virtual networks. Then, the coarsened virtual
networks are enhanced by using a refined Kernighan-Lin algorithm. The
performance of the proposed algorithm is evaluated and compared with existing
algorithms using extensive simulations, which show that the proposed algorithm
improves virtual network embedding by increasing the acceptance ratio and the
revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02362</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02362</id><created>2015-02-09</created><updated>2015-05-20</updated><authors><author><keyname>Swaminathan</keyname><forenames>Adith</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Counterfactual Risk Minimization: Learning from Logged Bandit Feedback</title><categories>cs.LG stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a learning principle and an efficient algorithm for batch learning
from logged bandit feedback. This learning setting is ubiquitous in online
systems (e.g., ad placement, web search, recommendation), where an algorithm
makes a prediction (e.g., ad ranking) for a given input (e.g., query) and
observes bandit feedback (e.g., user clicks on presented ads). We first address
the counterfactual nature of the learning problem through propensity scoring.
Next, we prove generalization error bounds that account for the variance of the
propensity-weighted empirical risk estimator. These constructive bounds give
rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM
can be used to derive a new learning method -- called Policy Optimizer for
Exponential Models (POEM) -- for learning stochastic linear rules for
structured output prediction. We present a decomposition of the POEM objective
that enables efficient stochastic gradient optimization. POEM is evaluated on
several multi-label classification problems showing substantially improved
robustness and generalization performance compared to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02367</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02367</id><created>2015-02-09</created><updated>2015-06-17</updated><authors><author><keyname>Chung</keyname><forenames>Junyoung</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Gated Feedback Recurrent Neural Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>9 pages, removed appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel recurrent neural network (RNN) architecture.
The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of
stacking multiple recurrent layers by allowing and controlling signals flowing
from upper recurrent layers to lower layers using a global gating unit for each
pair of layers. The recurrent signals exchanged between layers are gated
adaptively based on the previous hidden states and the current input. We
evaluated the proposed GF-RNN with different types of recurrent units, such as
tanh, long short-term memory and gated recurrent units, on the tasks of
character-level language modeling and Python program evaluation. Our empirical
evaluation of different RNN units, revealed that in both tasks, the GF-RNN
outperforms the conventional approaches to build deep stacked RNNs. We suggest
that the improvement arises because the GF-RNN can adaptively assign different
layers to different timescales and layer-to-layer interactions (including the
top-down ones which are not usually present in a stacked RNN) by learning to
gate these interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02370</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02370</id><created>2015-02-09</created><authors><author><keyname>Borjigin</keyname><forenames>Ailiya</forenames></author></authors><title>Teachable Agent</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teachable Agent (TA) is a special type of pedagogical agent which
instantiates the educational theory of Learning by Teaching. Soon after its
emergence, research of TA becomes an active field, as it can solve the over
scaffolded problem in traditional pedagogical systems, and encourage students
to take the responsibility of learning. Apart from the benefits, existing TA
design also has limitations. One is the lack of enough proactive interactions
with students during the learning process, and the other is the lack of
believability to arouse students empathy so as to offer students an immersive
learning experience. To solve these two problems, we propose a new type of TA,
Affective Teachable Agent, and use a goal oriented approach to design and
implement the agent system allowing agents to proactively interact with
students with affective expressions. The ATA model begins with the analysis of
pedagogical requirements and teaching goals, using Learning by Teaching theory
to design interventions which can authentically promote the learning behaviors
of students. Two crucial capabilities of ATA are highlighted Teachability, to
learn new knowledge and apply the knowledge to certain tasks, and
Affectivability, to establish good relationship with students and encourage
them to teach well. Through executing a hierarchy of goals, the proposed TA can
interact with students by pursuing its own agenda. When a student teaches the
agent, the agent is performed as a naive learning companion, and when an
educator teaches the agent during the design and maintenance time, the agent
can perform as an authoring tool. To facilitate the involvement of educators
into the game design, we develop an authoring tool for proposed ATA system,
which can encapsulate the technical details and provide educational experts a
natural way to convey domain knowledge to agent knowledge base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02376</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02376</id><created>2015-02-09</created><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Ouyang</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>Optimal Relay Selection with Non-negligible Probing Time</title><categories>cs.IT math.IT</categories><comments>8 pages. ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an optimal relay selection algorithm with non-negligible
probing time is proposed and analyzed for cooperative wireless networks. Relay
selection has been introduced to solve the degraded bandwidth efficiency
problem in cooperative communication. Yet complete information of relay
channels often remain unavailable for complex networks which renders the
optimal selection strategies impossible for transmission source without probing
the relay channels. Particularly when the number of relay candidate is large,
even though probing all relay channels guarantees the finding of the best
relays at any time instant, the degradation of bandwidth efficiency due to
non-negligible probing times, which was often neglected in past literature, is
also significant. In this work, a stopping rule based relay selection strategy
is determined for the source node to decide when to stop the probing process
and choose one of the probed relays to cooperate with under wireless channels'
stochastic uncertainties. This relay selection strategy is further shown to
have a simple threshold structure. At the meantime, full diversity order and
high bandwidth efficiency can be achieved simultaneously. Both analytical and
simulation results are provided to verify the claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02377</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02377</id><created>2015-02-09</created><authors><author><keyname>Liu</keyname><forenames>Xuejie</forenames></author><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author></authors><title>Sparse coding with earth mover's distance</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding (Sc) has been studied very well as a powerful data
representation method. It attempts to represent the feature vector of a data
sample by reconstructing it as the sparse linear combination of some basic
elements, and a $l_2$ norm distance function is usually used as the loss
function for the reconstruction error. In this paper, we investigate using Sc
as the representation method within multi-instance learning framework, where a
sample is given as a bag of instances, and further represented as a histogram
of the quantized instances. We argue that for the data type of histogram, using
$l_2$ norm distance is not suitable, and propose to use the earth mover's
distance (EMD) instead of $l_2$ norm distance as a measure of the
reconstruction error. By minimizing the EMD between the histogram of a sample
and the its reconstruction from some basic histograms, a novel sparse coding
method is developed, which is refereed as Sc-EMD. We evaluate its performances
as a histogram representation method in tow multi-instance learning problems
--- abnormal image detection in wireless capsule endoscopy videos, and protein
binding site retrieval. The encouraging results demonstrate the advantages of
the new method over the traditional method using $l_2$ norm distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02385</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02385</id><created>2015-02-09</created><authors><author><keyname>Moghadam</keyname><forenames>Mohammad R. Vedady</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Multiuser Charging Control in Wireless Power Transfer via Magnetic
  Resonant Coupling</title><categories>cs.SY</categories><comments>To appear in ICASSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonant coupling (MRC) is a practically appealing method for
realizing the near-field wireless power transfer (WPT). The MRC-WPT system with
a single pair of transmitter and receiver has been extensively studied in the
literature, while there is limited work on the general setup with multiple
transmitters and/or receivers. In this paper, we consider a point-to-multipoint
MRC-WPT system with one transmitter sending power wirelessly to a set of
distributed receivers simultaneously. We derive the power delivered to the load
of each receiver in closed-form expression, and reveal a &quot;near-far&quot; fairness
issue in multiuser power transmission due to users' distance-dependent mutual
inductances with the transmitter. We also show that by designing the receivers'
load resistances, the near-far issue can be optimally solved. Specifically, we
propose a centralized algorithm to jointly optimize the load resistances to
minimize the power drawn from the energy source at the transmitter under given
power requirements for the loads. We also devise a distributed algorithm for
the receivers to adjust their load resistances iteratively, for ease of
practical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02388</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02388</id><created>2015-02-09</created><updated>2015-03-12</updated><authors><author><keyname>Normann</keyname><forenames>Dag</forenames><affiliation>University of Oslo</affiliation></author></authors><title>The extensional realizability model of continuous functionals and three
  weakly non-constructive classical theorems</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 16,
  2015) lmcs:1174</journal-ref><doi>10.2168/LMCS-11(1:8)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate wether three statements in analysis, that can be proved
classically, are realizable in the realizability model of extensional
continuous functionals induced by Kleene's second model $K_2$. We prove that a
formulation of the Riemann Permutation Theorem as well as the statement that
all partially Cauchy sequences are Cauchy cannot be realized in this model,
while the statement that the product of two anti-Specker spaces is anti-Specker
can be realized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02389</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02389</id><created>2015-02-09</created><authors><author><keyname>Steuwer</keyname><forenames>Michel</forenames></author><author><keyname>Fensch</keyname><forenames>Christian</forenames></author><author><keyname>Dubach</keyname><forenames>Christophe</forenames></author></authors><title>Patterns and Rewrite Rules for Systematic Code Generation (From
  High-Level Functional Patterns to High-Performance OpenCL Code)</title><categories>cs.DC cs.PF cs.PL</categories><comments>Technical Report</comments><acm-class>D.3.3; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing systems have become increasingly complex with the emergence of
heterogeneous hardware combining multicore CPUs and GPUs. These parallel
systems exhibit tremendous computational power at the cost of increased
programming effort. This results in a tension between achieving performance and
code portability. Code is either tuned using device-specific optimizations to
achieve maximum performance or is written in a high-level language to achieve
portability at the expense of performance.
  We propose a novel approach that offers high-level programming, code
portability and high-performance. It is based on algorithmic pattern
composition coupled with a powerful, yet simple, set of rewrite rules. This
enables systematic transformation and optimization of a high-level program into
a low-level hardware specific representation which leads to high performance
code.
  We test our design in practice by describing a subset of the OpenCL
programming model with low-level patterns and by implementing a compiler which
generates high performance OpenCL code. Our experiments show that we can
systematically derive high-performance device-specific implementations from
simple high-level algorithmic expressions. The performance of the generated
OpenCL code is on par with highly tuned implementations for multicore CPUs and
GPUs written by experts
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02401</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02401</id><created>2015-02-09</created><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Random Preferential Attachment Hypergraphs</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random graph model has recently been extended to a random preferential
attachment graph model, in order to enable the study of general asymptotic
properties in network types that are better represented by the preferential
attachment evolution model than by the ordinary (uniform) evolution lodel.
Analogously, this paper extends the random {\em hypergraph} model to a random
{\em preferential attachment hypergraph} model. We then analyze the degree
distribution of random preferential attachment hypergraphs and show that they
possess heavy tail degree distribution properties similar to those of random
preferential attachment graphs. However, our results show that the exponent of
the degree distribution is sensitive to whether one considers the structure as
a hypergraph or as a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02402</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02402</id><created>2015-02-09</created><authors><author><keyname>Boutier</keyname><forenames>Matthieu</forenames><affiliation>PPS</affiliation></author><author><keyname>Chroboczek</keyname><forenames>Juliusz</forenames><affiliation>PPS</affiliation></author></authors><title>User-space Multipath UDP in Mosh</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many network topologies, hosts have multiple IP addresses, and may choose
among multiple network paths by selecting the source and destination addresses
of the packets that they send. This can happen with multihomed hosts (hosts
connected to multiple networks), or in multihomed networks using
source-specific routing. A number of efforts have been made to dynamically
choose between multiple addresses in order to improve the reliability or the
performance of network applications, at the network layer, as in Shim6, or at
the transport layer, as in MPTCP. In this paper, we describe our experience of
implementing dynamic address selection at the application layer within the
Mobile Shell. While our work is specific to Mosh, we hope that it is generic
enough to serve as a basis for designing UDP-based multipath applications or
even more general APIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02403</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02403</id><created>2015-02-09</created><authors><author><keyname>McPhillips</keyname><forenames>Timothy</forenames></author><author><keyname>Song</keyname><forenames>Tianhong</forenames></author><author><keyname>Kolisnik</keyname><forenames>Tyler</forenames></author><author><keyname>Aulenbach</keyname><forenames>Steve</forenames></author><author><keyname>Belhajjame</keyname><forenames>Khalid</forenames></author><author><keyname>Bocinsky</keyname><forenames>Kyle</forenames></author><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Chirigati</keyname><forenames>Fernando</forenames></author><author><keyname>Dey</keyname><forenames>Saumen</forenames></author><author><keyname>Freire</keyname><forenames>Juliana</forenames></author><author><keyname>Huntzinger</keyname><forenames>Deborah</forenames></author><author><keyname>Jones</keyname><forenames>Christopher</forenames></author><author><keyname>Koop</keyname><forenames>David</forenames></author><author><keyname>Missier</keyname><forenames>Paolo</forenames></author><author><keyname>Schildhauer</keyname><forenames>Mark</forenames></author><author><keyname>Schwalm</keyname><forenames>Christopher</forenames></author><author><keyname>Wei</keyname><forenames>Yaxing</forenames></author><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Bieda</keyname><forenames>Mark</forenames></author><author><keyname>Ludaescher</keyname><forenames>Bertram</forenames></author></authors><title>YesWorkflow: A User-Oriented, Language-Independent Tool for Recovering
  Workflow Information from Scripts</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific workflow management systems offer features for composing complex
computational pipelines from modular building blocks, for executing the
resulting automated workflows, and for recording the provenance of data
products resulting from workflow runs. Despite the advantages such features
provide, many automated workflows continue to be implemented and executed
outside of scientific workflow systems due to the convenience and familiarity
of scripting languages (such as Perl, Python, R, and MATLAB), and to the high
productivity many scientists experience when using these languages. YesWorkflow
is a set of software tools that aim to provide such users of scripting
languages with many of the benefits of scientific workflow systems. YesWorkflow
requires neither the use of a workflow engine nor the overhead of adapting code
to run effectively in such a system. Instead, YesWorkflow enables scientists to
annotate existing scripts with special comments that reveal the computational
modules and dataflows otherwise implicit in these scripts. YesWorkflow tools
extract and analyze these comments, represent the scripts in terms of entities
based on the typical scientific workflow model, and provide graphical
renderings of this workflow-like view of the scripts. Future versions of
YesWorkflow also will allow the prospective provenance of the data products of
these scripts to be queried in ways similar to those available to users of
scientific workflow systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02404</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02404</id><created>2015-02-09</created><updated>2015-04-20</updated><authors><author><keyname>de Carvalho</keyname><forenames>Daniel</forenames></author></authors><title>The relational model is injective for Multiplicative Exponential Linear
  Logic</title><categories>cs.LO</categories><comments>50 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the relational semantics is injective for Multiplicative
Exponential Linear Logic proof-nets, i.e. the equality between MELL proof-nets
in the relational model is exactly axiomatized by the cut-elimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02407</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02407</id><created>2015-02-09</created><authors><author><keyname>Yu</keyname><forenames>James J. Q.</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>A Social Spider Algorithm for Global Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing complexity of real-world problems has motivated computer
scientists to search for efficient problem-solving methods. Metaheuristics
based on evolutionary computation and swarm intelligence are outstanding
examples of nature-inspired solution techniques. Inspired by the social
spiders, we propose a novel Social Spider Algorithm to solve global
optimization problems. This algorithm is mainly based on the foraging strategy
of social spiders, utilizing the vibrations on the spider web to determine the
positions of preys. Different from the previously proposed swarm intelligence
algorithms, we introduce a new social animal foraging strategy model to solve
optimization problems. In addition, we perform preliminary parameter
sensitivity analysis for our proposed algorithm, developing guidelines for
choosing the parameter values. The Social Spider Algorithm is evaluated by a
series of widely-used benchmark functions, and our proposed algorithm has
superior performance compared with other state-of-the-art metaheuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02410</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02410</id><created>2015-02-09</created><authors><author><keyname>Vural</keyname><forenames>Elif</forenames></author><author><keyname>Guillemot</keyname><forenames>Christine</forenames></author></authors><title>Out-of-sample generalizations for supervised manifold learning for
  classification</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised manifold learning methods for data classification map data samples
residing in a high-dimensional ambient space to a lower-dimensional domain in a
structure-preserving way, while enhancing the separation between different
classes in the learned embedding. Most nonlinear supervised manifold learning
methods compute the embedding of the manifolds only at the initially available
training points, while the generalization of the embedding to novel points,
known as the out-of-sample extension problem in manifold learning, becomes
especially important in classification applications. In this work, we propose a
semi-supervised method for building an interpolation function that provides an
out-of-sample extension for general supervised manifold learning algorithms
studied in the context of classification. The proposed algorithm computes a
radial basis function (RBF) interpolator that minimizes an objective function
consisting of the total embedding error of unlabeled test samples, defined as
their distance to the embeddings of the manifolds of their own class, as well
as a regularization term that controls the smoothness of the interpolation
function in a direction-dependent way. The class labels of test data and the
interpolation function parameters are estimated jointly with a progressive
procedure. Experimental results on face and object images demonstrate the
potential of the proposed out-of-sample extension algorithm for the
classification of manifold-modeled data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02414</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02414</id><created>2015-02-09</created><authors><author><keyname>Allouche</keyname><forenames>David</forenames></author><author><keyname>Bessiere</keyname><forenames>Christian</forenames></author><author><keyname>Boizumault</keyname><forenames>Patrice</forenames></author><author><keyname>de Givry</keyname><forenames>Simon</forenames></author><author><keyname>Gutierrez</keyname><forenames>Patricia</forenames></author><author><keyname>Lee</keyname><forenames>Jimmy H. M.</forenames></author><author><keyname>Leung</keyname><forenames>Kam Lun</forenames></author><author><keyname>Loudni</keyname><forenames>Samir</forenames></author><author><keyname>M&#xe9;tivier</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author><author><keyname>Wu</keyname><forenames>Yi</forenames></author></authors><title>Tractability and Decompositions of Global Cost Functions</title><categories>cs.AI</categories><comments>32 pages for the main paper, extra Appendix with examples of
  DAG-decomposed global cost functions</comments><msc-class>68T20 Problem solving (heuristics, search strategies, etc.)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enforcing local consistencies in cost function networks is performed by
applying so-called Equivalent Preserving Transformations (EPTs) to the cost
functions. As EPTs transform the cost functions, they may break the property
that was making local consistency enforcement tractable on a global cost
function. A global cost function is called tractable projection-safe when
applying an EPT to it is tractable and does not break the tractability
property. In this paper, we prove that depending on the size r of the smallest
scopes used for performing EPTs, the tractability of global cost functions can
be preserved (r = 0) or destroyed (r &gt; 1). When r = 1, the answer is
indefinite. We show that on a large family of cost functions, EPTs can be
computed via dynamic programming-based algorithms, leading to tractable
projection-safety. We also show that when a global cost function can be
decomposed into a Berge acyclic network of bounded arity cost functions, soft
local consistencies such as soft Directed or Virtual Arc Consistency can
directly emulate dynamic programming. These different approaches to
decomposable cost functions are then embedded in a solver for extensive
experiments that confirm the feasibility and efficiency of our proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02417</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02417</id><created>2015-02-09</created><authors><author><keyname>Camporeale</keyname><forenames>Cecilia</forenames></author><author><keyname>De Nicola</keyname><forenames>Antonio</forenames></author><author><keyname>Villani</keyname><forenames>Maria Luisa</forenames></author></authors><title>Semantics-based services for a low carbon society: An application on
  emissions trading system data and scenarios management</title><categories>cs.AI</categories><acm-class>I.2.4; J.1; D.2.12</acm-class><journal-ref>Environmental Modelling &amp; Software, Vol 64, Feb 2015, Pages
  124-142</journal-ref><doi>10.1016/j.envsoft.2014.11.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A low carbon society aims at fighting global warming by stimulating synergic
efforts from governments, industry and scientific communities. Decision support
systems should be adopted to provide policy makers with possible scenarios,
options for prompt countermeasures in case of side effects on environment,
economy and society due to low carbon society policies, and also options for
information management. A necessary precondition to fulfill this agenda is to
face the complexity of this multi-disciplinary domain and to reach a common
understanding on it as a formal specification. Ontologies are widely accepted
means to share knowledge. Together with semantic rules, they enable advanced
semantic services to manage knowledge in a smarter way. Here we address the
European Emissions Trading System (EU-ETS) and we present a knowledge base
consisting of the EREON ontology and a catalogue of rules. Then we describe two
innovative semantic services to manage ETS data and information on ETS
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02422</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02422</id><created>2015-02-09</created><authors><author><keyname>Kawahara</keyname><forenames>Jun</forenames></author><author><keyname>Kobayashi</keyname><forenames>Koji M.</forenames></author></authors><title>An improved lower bound for one-dimensional online unit clustering</title><categories>cs.DS cs.CG</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online unit clustering problem was proposed by Chan and Zarrabi-Zadeh
(WAOA2007 and Theory of Computing Systems 45(3), 2009), which is defined as
follows: &quot;Points&quot; are given online in the $d$-dimensional Euclidean space one
by one. An algorithm creates a &quot;cluster,&quot; which is a $d$-dimensional rectangle.
The initial length of each edge of a cluster is 0. An algorithm can extend an
edge until it reaches unit length independently of other dimensions. The task
of an algorithm is to cover a new given point either by creating a new cluster
and assigning it to the point, or by extending edges of an existing cluster
created in past times. The goal is to minimize the total number of created
clusters. Chan and Zarrabi-Zadeh proposed some method to obtain a competitive
algorithm for the $d$-dimensional case using an algorithm for the
one-dimensional case, and thus the one-dimensional case has been extensively
studied including some variants of the unit clustering problem. In this paper,
we show a lower bound of $13/8 = 1.625$ on the competitive ratio of any
deterministic online algorithm for the one-dimensional unit clustering,
improving the previous lower bound $8/5 (=1.6)$ presented by Epstein and van
Stee (WAOA2007 and ACM Transactions on Algorithms 7(1), 2010). Note that Ehmsen
and Larsen (SWAT2010 and Theoretical Computer Science, 500, 2013) showed the
current best upper bound of $5/3$, and conjectured that the exact competitive
ratio in the one-dimensional case may be $13/8$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02426</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02426</id><created>2015-02-09</created><authors><author><keyname>Fuchs</keyname><forenames>Fabian</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author></authors><title>Simple Distributed Delta + 1 Coloring in the SINR Model</title><categories>cs.DS cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless ad hoc or sensor networks, distributed node coloring is a
fundamental problem closely related to establishing efficient communication
through TDMA schedules. For networks with maximum degree Delta, a Delta + 1
coloring is the ultimate goal in the distributed setting as this is always
possible. In this work we propose Delta + 1 coloring algorithms for the
synchronous and asynchronous setting. All algorithms have a runtime of O(Delta
log n) time slots. This improves on the previous algorithms for the SINR model
either in terms of the number of required colors or the runtime and matches the
runtime of local broadcasting in the SINR model (which can be seen as a lower
bound).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02427</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02427</id><created>2015-02-09</created><updated>2015-09-09</updated><authors><author><keyname>De Marco</keyname><forenames>Gianluca</forenames></author><author><keyname>Leoncini</keyname><forenames>Mauro</forenames></author><author><keyname>Montangero</keyname><forenames>Manuela</forenames></author></authors><title>A Distributed Message-Optimal Assignment on Rings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of items and a set of $m$ colors, where each item is
associated to one color. Consider also $n$ computational agents connected by a
ring. Each agent holds a subset of the items and items of the same color can be
held by different agents. We analyze the problem of distributively assigning
colors to agents in such a way that (a) each color is assigned to one agent
only and (b) the number of different colors assigned to each agent is minimum.
Since any color assignment requires the items be distributed according to it
(e.g. all items of the same color are to be held by only one agent), we define
the cost of a color assignment as the amount of items that need to be moved,
given an initial allocation. We first show that any distributed algorithm for
this problem requires a message complexity of $\Omega(n\cdot m)$ and then we
exhibit an optimal message complexity algorithm for synchronous rings that in
polynomial time determines a color assignment with cost at most three times the
optimal. We also discuss solutions for the asynchronous setting. Finally, we
show how to get a better cost solution at the expenses of either the message or
the time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02436</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02436</id><created>2015-02-09</created><authors><author><keyname>De Castro</keyname><forenames>Y</forenames><affiliation>LM-Orsay</affiliation></author><author><keyname>Gamboa</keyname><forenames>F</forenames><affiliation>IMT</affiliation></author><author><keyname>Henrion</keyname><forenames>D</forenames><affiliation>LAAS</affiliation></author><author><keyname>Lasserre</keyname><forenames>J. -B</forenames><affiliation>LAAS</affiliation></author></authors><title>Exact solutions to Super Resolution on semi-algebraic domains in higher
  dimensions</title><categories>cs.IT math.IT math.OC stat.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the multi-dimensional Super Resolution problem on closed
semi-algebraic domains for various sampling schemes such as Fourier or moments.
We present a new semidefinite programming (SDP) formulation of the 1
-minimization in the space of Radon measures in the multi-dimensional frame on
semi-algebraic sets. While standard approaches have focused on SDP relaxations
of the dual program (a popular approach is based on Gram matrix
representations), this paper introduces an exact formulation of the primal 1
-minimization exact recovery problem of Super Resolution that unleashes
standard techniques (such as moment-sum-of-squares hier-archies) to overcome
intrinsic limitations of previous works in the literature. Notably, we show
that one can exactly solve the Super Resolution problem in dimension greater
than 2 and for a large family of domains described by semi-algebraic sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02440</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02440</id><created>2015-02-09</created><authors><author><keyname>Kundu</keyname><forenames>Atreyee</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author><author><keyname>Liberzon</keyname><forenames>Daniel</forenames></author></authors><title>Generalized switching signals for input-to-state stability of switched
  systems</title><categories>cs.SY math.OC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with input-to-state stability (ISS) of continuous-time
switched nonlinear systems. Given a family of systems with exogenous inputs
such that not all systems in the family are ISS, we characterize a new and
general class of switching signals under which the resulting switched system is
ISS. Our stabilizing switching signals allow the number of switches to grow
faster than an affine function of the length of a time interval, unlike in the
case of average dwell time switching. We also recast a subclass of average
dwell time switching signals in our setting and establish analogs of two
representative prior results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02441</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02441</id><created>2015-02-09</created><updated>2015-06-10</updated><authors><author><keyname>Ollila</keyname><forenames>Esa</forenames></author></authors><title>Nonparametric Simultaneous Sparse Recovery: an Application to Source
  Localization</title><categories>cs.IT math.IT</categories><comments>Paper appears in Proc. European Signal Processing Conference
  (EUSIPCO'15), Nice, France, Aug 31 -- Sep 4, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multichannel sparse recovery problem where the objective is to
find good recovery of jointly sparse unknown signal vectors from the given
multiple measurement vectors which are different linear combinations of the
same known elementary vectors. Many popular greedy or convex algorithms perform
poorly under non-Gaussian heavy-tailed noise conditions or in the face of
outliers. In this paper, we propose the usage of mixed $\ell_{p,q}$ norms on
data fidelity (residual matrix) term and the conventional $\ell_{0,2}$-norm
constraint on the signal matrix to promote row-sparsity. We devise a greedy
pursuit algorithm based on simultaneous normalized iterative hard thresholding
(SNIHT) algorithm. Simulation studies highlight the effectiveness of the
proposed approaches to cope with different noise environments (i.i.d., row
i.i.d, etc) and outliers. Usefulness of the methods are illustrated in source
localization application with sensor arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02444</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02444</id><created>2015-02-09</created><authors><author><keyname>Garimella</keyname><forenames>Rama</forenames></author><author><keyname>Kicanaoglu</keyname><forenames>Berkay</forenames></author><author><keyname>Gabbouj</keyname><forenames>Moncef</forenames></author></authors><title>On the Dynamics of a Recurrent Hopfield Network</title><categories>cs.NE</categories><comments>6 pages, 6 figures, 1 table, submitted to IJCNN-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper novel real/complex valued recurrent Hopfield Neural
Network (RHNN) is proposed. The method of synthesizing the energy landscape of
such a network and the experimental investigation of dynamics of Recurrent
Hopfield Network is discussed. Parallel modes of operation (other than fully
parallel mode) in layered RHNN is proposed. Also, certain potential
applications are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02445</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02445</id><created>2015-02-09</created><updated>2015-06-25</updated><authors><author><keyname>de Brebisson</keyname><forenames>Alexandre</forenames></author><author><keyname>Montana</keyname><forenames>Giovanni</forenames></author></authors><title>Deep Neural Networks for Anatomical Brain Segmentation</title><categories>cs.CV cs.LG stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to automatically segment magnetic resonance (MR)
images of the human brain into anatomical regions. Our methodology is based on
a deep artificial neural network that assigns each voxel in an MR image of the
brain to its corresponding anatomical region. The inputs of the network capture
information at different scales around the voxel of interest: 3D and orthogonal
2D intensity patches capture the local spatial context while large, compressed
2D orthogonal patches and distances to the regional centroids enforce global
spatial consistency. Contrary to commonly used segmentation methods, our
technique does not require any non-linear registration of the MR images. To
benchmark our model, we used the dataset provided for the MICCAI 2012 challenge
on multi-atlas labelling, which consists of 35 manually segmented MR images of
the brain. We obtained competitive results (mean dice coefficient 0.725, error
rate 0.163) showing the potential of our approach. To our knowledge, our
technique is the first to tackle the anatomical segmentation of the whole brain
using deep neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02448</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02448</id><created>2015-02-09</created><authors><author><keyname>Crick</keyname><forenames>Tom</forenames></author><author><keyname>Hall</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Ishtiaq</keyname><forenames>Samin</forenames></author></authors><title>Dear CAV, We Need to Talk About Reproducibility</title><categories>cs.LO cs.SE</categories><comments>Submitted to the 27th International Conference on Computer Aided
  Verification (CAV 2015); 9 pages, LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How many times have you tried to re-implement a past CAV tool paper, and
failed?
  Reliably reproducing published scientific discoveries has been acknowledged
as a barrier to scientific progress for some time but there remains only a
small subset of software available to support the specific needs of the
research community (i.e. beyond generic tools such as source code
repositories). In this paper we propose an infrastructure for enabling
reproducibility in our community, by automating the build, unit testing and
benchmarking of research software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02454</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02454</id><created>2015-02-09</created><updated>2015-07-10</updated><authors><author><keyname>Le</keyname><forenames>Thuc Duy</forenames></author><author><keyname>Hoang</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Jiuyong</forenames></author><author><keyname>Liu</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Huawen</forenames></author></authors><title>A fast PC algorithm for high dimensional causal discovery with
  multi-core PCs</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering causal relationships from observational data is a crucial problem
and has applications in many research areas. PC algorithm is the
state-of-the-art method in the constraint based approach. However, the PC
algorithm is worst-case exponential to the number of nodes (variables), and
thus it is inefficient when applying to high dimensional data, e.g. gene
expression datasets where the causal relationships between thousands of nodes
(genes) are explored. In this paper, we propose a fast and memory efficient PC
algorithm using the parallel computing technique. We apply our method on a
range of synthetic and real-world high dimensional datasets. Experimental
results on a dataset from DREAM 5 challenge show that the PC algorithm could
not produce any results after running more than 24 hours; meanwhile, our
parallel-PC algorithm with a 4-core CPU computer managed to finish within
around 12.5 hours, and less than 6 hours with a 8-core CPU computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02465</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02465</id><created>2015-02-09</created><authors><author><keyname>Palmieri</keyname><forenames>Luigi</forenames></author></authors><title>A behavioural approach to obstacle avoidance for mobile manipulators
  based on distributed sensing</title><categories>cs.RO</categories><comments>Master's Thesis</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A reactive obstacle avoidance method for mobile manipulators is presented.
The objectives of the developed algorithm are twofold. The first one is to find
a trajectory in the configuration space of a mobile manipulator so as to follow
a given trajectory in the task space. The second objective consists in locally
adjusting the trajectory in the configuration space in order to avoid
collisions with potentially moving obstacles and self-collisions in
unstructured and dynamic environments. The perception is exclusively based on a
set of proximity sensors distributed on the robot mechanical structure and
visual information are not required. Thanks to the adoption of this kind of
proximity distributed perception, the approach does not require a 3D model of
the robot and allows the real-time collision avoidance without the need of a
sensorized environment. To achieve the features cited above, a behaviour-based
technique known as Null-Space-Based (NSB) approach has been adopted with some
modifications.On one hand, the concept of a total pseudo-energy based on the
information from the distributed sensors has been introduced. On the other
hand, a method to combine different tasks has been proposed to guarantee the
smoothness of the realtime trajectory adjustments. Another significant feature
of the method is the strict coordination between the base and the arm
exploiting the redundant degrees of freedom, that is a relevant topic in mobile
manipulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02467</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02467</id><created>2015-02-09</created><authors><author><keyname>Thorstensen</keyname><forenames>Evgenij</forenames></author></authors><title>Structural Decompositions for Problems with Global Constraints</title><categories>cs.AI</categories><comments>The final publication is available at Springer via
  http://dx.doi.org/10.1007/s10601-015-9181-2</comments><doi>10.1007/s10601-015-9181-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of problems can be modelled as constraint satisfaction problems
(CSPs), that is, a set of constraints that must be satisfied simultaneously.
Constraints can either be represented extensionally, by explicitly listing
allowed combinations of values, or implicitly, by special-purpose algorithms
provided by a solver.
  Such implicitly represented constraints, known as global constraints, are
widely used; indeed, they are one of the key reasons for the success of
constraint programming in solving real-world problems. In recent years, a
variety of restrictions on the structure of CSP instances have been shown to
yield tractable classes of CSPs. However, most such restrictions fail to
guarantee tractability for CSPs with global constraints. We therefore study the
applicability of structural restrictions to instances with such constraints.
  We show that when the number of solutions to a CSP instance is bounded in key
parts of the problem, structural restrictions can be used to derive new
tractable classes. Furthermore, we show that this result extends to
combinations of instances drawn from known tractable classes, as well as to CSP
instances where constraints assign costs to satisfying assignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02468</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02468</id><created>2015-02-09</created><authors><author><keyname>Faulwasser</keyname><forenames>Timm</forenames></author><author><keyname>Findeisen</keyname><forenames>Rolf</forenames></author></authors><title>Nonlinear Model Predictive Control for Constrained Output Path Following</title><categories>cs.SY math.OC</categories><comments>12 pages, 4 figures</comments><msc-class>93C83, 34H05, 70Q05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the tracking of geometric paths in output spaces of nonlinear
systems subject to input and state constraints without pre-specified timing
requirements. Such problems are commonly referred to as constrained output
path-following problems. Specifically, we propose a predictive control approach
to constrained path-following problems with and without velocity assignments
and provide sufficient convergence conditions based on terminal regions and end
penalties. Furthermore, we analyze the geometric nature of constrained output
path-following problems and thereby provide insight into the computation of
suitable terminal control laws and terminal regions. We draw upon an example
from robotics to illustrate our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02472</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02472</id><created>2015-02-09</created><authors><author><keyname>Bhattacharya</keyname><forenames>Srimanta</forenames></author></authors><title>Derandomized Construction of Combinatorial Batch Codes</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial Batch Codes (CBCs), replication-based variant of Batch Codes
introduced by Ishai et al. in STOC 2004, abstracts the following data
distribution problem: $n$ data items are to be replicated among $m$ servers in
such a way that any $k$ of the $n$ data items can be retrieved by reading at
most one item from each server with the total amount of storage over $m$
servers restricted to $N$. Given parameters $m, c,$ and $k$, where $c$ and $k$
are constants, one of the challenging problems is to construct $c$-uniform CBCs
(CBCs where each data item is replicated among exactly $c$ servers) which
maximizes the value of $n$. In this work, we present explicit construction of
$c$-uniform CBCs with $\Omega(m^{c-1+{1 \over k}})$ data items. The
construction has the property that the servers are almost regular, i.e., number
of data items stored in each server is in the range $[{nc \over
m}-\sqrt{{n\over 2}\ln (4m)}, {nc \over m}+\sqrt{{n \over 2}\ln (4m)}]$. The
construction is obtained through better analysis and derandomization of the
randomized construction presented by Ishai et al. Analysis reveals almost
regularity of the servers, an aspect that so far has not been addressed in the
literature. The derandomization leads to explicit construction for a wide range
of values of $c$ (for given $m$ and $k$) where no other explicit construction
with similar parameters, i.e., with $n = \Omega(m^{c-1+{1 \over k}})$, is
known. Finally, we discuss possibility of parallel derandomization of the
construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02473</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02473</id><created>2015-02-09</created><authors><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS</affiliation></author><author><keyname>Naldi</keyname><forenames>Simone</forenames><affiliation>LAAS</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>Syst&#xe8;mes Polynomiaux, LIP6</affiliation></author></authors><title>Real root finding for rank defects in linear Hankel matrices</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $H\_0, ..., H\_n$ be $m \times m$ matrices with entries in $\QQ$ and
Hankel structure, i.e. constant skew diagonals. We consider the linear Hankel
matrix $H(\vecx)=H\_0+\X\_1H\_1+...+\X\_nH\_n$ and the problem of computing
sample points in each connected component of the real algebraic set defined by
the rank constraint ${\sf rank}(H(\vecx))\leq r$, for a given integer $r \leq
m-1$. Computing sample points in real algebraic sets defined by rank defects in
linear matrices is a general problem that finds applications in many areas such
as control theory, computational geometry, optimization, etc. Moreover, Hankel
matrices appear in many areas of engineering sciences. Also, since Hankel
matrices are symmetric, any algorithmic development for this problem can be
seen as a first step towards a dedicated exact algorithm for solving
semi-definite programming problems, i.e. linear matrix inequalities. Under some
genericity assumptions on the input (such as smoothness of an incidence
variety), we design a probabilistic algorithm for tackling this problem. It is
an adaptation of the so-called critical point method that takes advantage of
the special structure of the problem. Its complexity reflects this: it is
essentially quadratic in specific degree bounds on an incidence variety. We
report on practical experiments and analyze how the algorithm takes advantage
of this special structure. A first implementation outperforms existing
implementations for computing sample points in general real algebraic sets: it
tackles examples that are out of reach of the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02474</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02474</id><created>2015-02-09</created><authors><author><keyname>Lauri</keyname><forenames>Mikko</forenames></author><author><keyname>Ritala</keyname><forenames>Risto</forenames></author></authors><title>Planning for robotic exploration based on forward simulation</title><categories>cs.RO cs.SY</categories><msc-class>90C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robotic agent is tasked to explore an a priori unknown environment. The
objective is to maximize the amount of information about the partially
observable state. The problem is formulated as a partially observable Markov
decision process (POMDP) with an information-theoretic objective function,
further approximated to a form suitable for robotic exploration. An open-loop
approximation is applied with receding horizon control to solve the problem.
Algorithms based on evaluating the utilities of sequences of actions by forward
simulation are presented for both finite and uncountable action spaces. The
advantages of the receding horizon approach to myopic planning are demonstrated
in simulated and real-world exploration experiments. The proposed method is
applicable to a wide range of domains, both in dynamic and static environments,
by modifying the underlying state transition and observation models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02476</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02476</id><created>2015-02-09</created><updated>2015-06-11</updated><authors><author><keyname>C&#xf4;t&#xe9;</keyname><forenames>Marc-Alexandre</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>An Infinite Restricted Boltzmann Machine</title><categories>cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mathematical construction for the restricted Boltzmann machine
(RBM) that doesn't require specifying the number of hidden units. In fact, the
hidden layer size is adaptive and can grow during training. This is obtained by
first extending the RBM to be sensitive to the ordering of its hidden units.
Then, thanks to a carefully chosen definition of the energy function, we show
that the limit of infinitely many hidden units is well defined. As with RBM,
approximate maximum likelihood training can be performed, resulting in an
algorithm that naturally and adaptively adds trained hidden units during
learning. We empirically study the behaviour of this infinite RBM, showing that
its performance is competitive to that of the RBM, while not requiring the
tuning of a hidden layer size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02478</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02478</id><created>2015-02-09</created><authors><author><keyname>Graham</keyname><forenames>Ben</forenames></author><author><keyname>Reizenstein</keyname><forenames>Jeremy</forenames></author><author><keyname>Robinson</keyname><forenames>Leigh</forenames></author></authors><title>Efficient batchwise dropout training using submatrices</title><categories>cs.NE cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout is a popular technique for regularizing artificial neural networks.
Dropout networks are generally trained by minibatch gradient descent with a
dropout mask turning off some of the units---a different pattern of dropout is
applied to every sample in the minibatch. We explore a very simple alternative
to the dropout mask. Instead of masking dropped out units by setting them to
zero, we perform matrix multiplication using a submatrix of the weight
matrix---unneeded hidden units are never calculated. Performing dropout
batchwise, so that one pattern of dropout is used for each sample in a
minibatch, we can substantially reduce training times. Batchwise dropout can be
used with fully-connected and convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02481</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02481</id><created>2015-02-09</created><updated>2015-12-28</updated><authors><author><keyname>Baswana</keyname><forenames>Surender</forenames></author><author><keyname>Chaudhury</keyname><forenames>Shreejit Ray</forenames></author><author><keyname>Choudhary</keyname><forenames>Keerti</forenames></author><author><keyname>Khan</keyname><forenames>Shahbaz</forenames></author></authors><title>Dynamic DFS Tree in Undirected Graphs: breaking the $O(m)$ barrier</title><categories>cs.DS</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth first search (DFS) tree is a fundamental data structure for solving
various problems in graphs. It is well known that it takes $O(m+n)$ time to
build a DFS tree for a given undirected graph $G=(V,E)$ on $n$ vertices and $m$
edges. We address the problem of maintaining a DFS tree when the graph is
undergoing {\em updates} (insertion and deletion of vertices or edges). We
present the following results for this problem.
  (a) Fault tolerant DFS tree: There exists a data structure of size ${O}(m
~polylog~ n)$ such that given any set ${\cal F}$ of failed vertices or edges, a
DFS tree of the graph $G\setminus {\cal F}$ can be reported in ${O}(n|{\cal F}|
~polylog~ n)$ time.
  (b) Fully dynamic DFS tree: There exists a fully dynamic algorithm for
maintaining a DFS tree that takes worst case ${O}(\sqrt{mn} ~polylog~ n)$ time
per update for any arbitrary online sequence of updates.
  (c) Incremental DFS tree: Given any arbitrary online sequence of edge
insertions, we can maintain a DFS tree in ${O}(n ~polylog~ n)$ worst case time
per edge insertion.
  These are the first $o(m)$ worst case time results for maintaining a DFS tree
in a dynamic environment. Moreover, our fully dynamic algorithm provides, in a
seamless manner, the first deterministic algorithm with $O(1)$ query time and
$o(m)$ worst case update time for the dynamic subgraph connectivity,
biconnectivity, and 2-edge connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02484</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02484</id><created>2015-02-09</created><updated>2015-07-23</updated><authors><author><keyname>Lonsing</keyname><forenames>Florian</forenames></author><author><keyname>Egly</keyname><forenames>Uwe</forenames></author></authors><title>Incrementally Computing Minimal Unsatisfiable Cores of QBFs via a Clause
  Group Solver API</title><categories>cs.LO</categories><comments>(fixed typo), camera-ready version, 6-page tool paper, to appear in
  proceedings of SAT 2015, LNCS, Springer</comments><doi>10.1007/978-3-319-24318-4_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the incremental computation of minimal unsatisfiable cores (MUCs)
of QBFs. To this end, we equipped our incremental QBF solver DepQBF with a
novel API to allow for incremental solving based on clause groups. A clause
group is a set of clauses which is incrementally added to or removed from a
previously solved QBF. Our implementation of the novel API is related to
incremental SAT solving based on selector variables and assumptions. However,
the API entirely hides selector variables and assumptions from the user, which
facilitates the integration of DepQBF in other tools. We present implementation
details and, for the first time, report on experiments related to the
computation of MUCs of QBFs using DepQBF's novel clause group API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02489</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02489</id><created>2015-02-09</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Barros</keyname><forenames>C. M. F.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Fourier Codes and Hartley Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 tables, 1 appedix. conference: XXV Simposio Brasileiro de
  Telecomunicacoes, SBrT'07, Recife, PE, Brazil, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-valued block codes are introduced, which are derived from Discrete
Fourier Transforms (DFT) and Discrete Hartley Transforms (DHT). These algebraic
structures are built from the eigensequences of the transforms. Generator and
parity check matrices were computed for codes up to block length N=24. They can
be viewed as lattices codes so the main parameters (dimension, minimal norm,
area of the Voronoi region, density, and centre density) are computed.
Particularly, Hamming-Hartley and Golay-Hartley block codes are presented.
These codes may possibly help an efficient computation of a DHT/DFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02493</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02493</id><created>2015-02-09</created><updated>2015-07-06</updated><authors><author><keyname>Criado</keyname><forenames>Natalia</forenames></author><author><keyname>Such</keyname><forenames>Jose M.</forenames></author></authors><title>Implicit Contextual Integrity in Online Social Networks</title><categories>cs.SI cs.AI cs.CR cs.CY</categories><comments>Authors Version of the paper accepted for publication in the
  Information Sciences journal
  (http://www.journals.elsevier.com/information-sciences/)</comments><journal-ref>Information Sciences, Vol. 325 pp. 48-69 (2015)</journal-ref><doi>10.1016/j.ins.2015.07.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real incidents demonstrate that users of Online Social Networks need
mechanisms that help them manage their interactions by increasing the awareness
of the different contexts that coexist in Online Social Networks and preventing
them from exchanging inappropriate information in those contexts or
disseminating sensitive information from some contexts to others. Contextual
integrity is a privacy theory that conceptualises the appropriateness of
information sharing based on the contexts in which this information is to be
shared. Computational models of Contextual Integrity assume the existence of
well-defined contexts, in which individuals enact pre-defined roles and
information sharing is governed by an explicit set of norms. However, contexts
in Online Social Networks are known to be implicit, unknown a priori and ever
changing; users relationships are constantly evolving; and the information
sharing norms are implicit. This makes current Contextual Integrity models not
suitable for Online Social Networks.
  In this paper, we propose the first computational model of Implicit
Contextual Integrity, presenting an information model and an Information
Assistant Agent that uses the information model to learn implicit contexts,
relationships and the information sharing norms to help users avoid
inappropriate information exchanges and undesired information disseminations.
Through an experimental evaluation, we validate the properties of Information
Assistant Agents, which are shown to: infer the information sharing norms even
if a small proportion of the users follow the norms and in presence of
malicious users; help reduce the exchange of inappropriate information and the
dissemination of sensitive information with only a partial view of the system
and the information received and sent by their users; and minimise the burden
to the users in terms of raising unnecessary alerts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02506</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02506</id><created>2015-02-09</created><authors><author><keyname>Payan</keyname><forenames>Adrien</forenames></author><author><keyname>Montana</keyname><forenames>Giovanni</forenames></author></authors><title>Predicting Alzheimer's disease: a neuroimaging study with 3D
  convolutional neural networks</title><categories>cs.CV cs.LG stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern recognition methods using neuroimaging data for the diagnosis of
Alzheimer's disease have been the subject of extensive research in recent
years. In this paper, we use deep learning methods, and in particular sparse
autoencoders and 3D convolutional neural networks, to build an algorithm that
can predict the disease status of a patient, based on an MRI scan of the brain.
We report on experiments using the ADNI data set involving 2,265 historical
scans. We demonstrate that 3D convolutional neural networks outperform several
other classifiers reported in the literature and produce state-of-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02511</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02511</id><created>2015-02-09</created><authors><author><keyname>Ghanbarian</keyname><forenames>Behzad</forenames></author><author><keyname>Taslimitehrani</keyname><forenames>Vahid</forenames></author><author><keyname>Dong</keyname><forenames>Guozhu</forenames></author><author><keyname>Pachepsky</keyname><forenames>Yakov A.</forenames></author></authors><title>Measurement Scale Effect on Prediction of Soil Water Retention Curve and
  Saturated Hydraulic Conductivity</title><categories>cs.CE cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soil water retention curve (SWRC) and saturated hydraulic conductivity (SHC)
are key hydraulic properties for unsaturated zone hydrology and groundwater. In
particular, SWRC provides useful information on entry pore-size distribution,
and SHC is required for flow and transport modeling in the hydrologic cycle.
Not only the SWRC and SHC measurements are time-consuming, but also scale
dependent. This means as soil column volume increases, variability of the SWRC
and SHC decreases. Although prediction of the SWRC and SHC from available
parameters, such as textural data, organic matter, and bulk density have been
under investigation for decades, up to now no research has focused on the
effect of measurement scale on the soil hydraulic properties pedotransfer
functions development. In the literature, several data mining approaches have
been applied, such as multiple linear regression, artificial neural networks,
group method of data handling. However, in this study we develop pedotransfer
functions using a novel approach called contrast pattern aided regression
(CPXR) and compare it with the multiple linear regression method. For this
purpose, two databases including 210 and 213 soil samples are collected to
develop and evaluate pedotransfer functions for the SWRC and SHC, respectively,
from the UNSODA database. The 10-fold cross-validation method is applied to
evaluate the accuracy and reliability of the proposed regression-based models.
Our results show that including measurement scale parameters, such as sample
internal diameter and length could substantially improve the accuracy of the
SWRC and SHC pedotransfer functions developed using the CPXR method, while this
is not the case when MLR is used. Moreover, the CPXR method yields remarkably
more accurate soil water retention curve and saturated hydraulic conductivity
predictions than the MLR approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02512</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02512</id><created>2015-02-09</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster
  Technique</title><categories>stat.ME cs.LG stat.AP</categories><comments>4 pages, 2 figures, 2 tables. Congresso Brasileiro de Automatica CBA,
  Natal, RN, Brazil, 2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a variant of the classical hierarchical cluster analysis is
reported. This agglomerative (bottom-up) cluster technique is referred to as
the Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkage
algorithm where the value of the threshold is conveniently up-dated at each
interaction. The superiority of the adaptive clustering with respect to the
average-linkage algorithm follows because it achieves a good compromise on
threshold values: Thresholds based on the cut-off distance are sufficiently
small to assure the homogeneity and also large enough to guarantee at least a
pair of merging sets. This approach is applied to a set of possible
substituents in a chemical series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02519</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02519</id><created>2015-02-09</created><updated>2015-02-10</updated><authors><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Kickstarting Choreographic Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an overview of some recent efforts aimed at the development of
Choreographic Programming, a programming paradigm for the production of
concurrent software that is guaranteed to be correct by construction from
global descriptions of communication behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02535</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02535</id><created>2015-02-09</created><updated>2015-07-31</updated><authors><author><keyname>Bonacina</keyname><forenames>Maria Paola</forenames></author><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Sofronie-Stokkermans</keyname><forenames>Viorica</forenames></author></authors><title>On First-Order Model-Based Reasoning</title><categories>cs.AI</categories><comments>In Narciso Marti-Oliet, Peter Olveczky, and Carolyn Talcott (Eds.),
  &quot;Logic, Rewriting, and Concurrency: Essays in Honor of Jose Meseguer&quot;
  Springer, Lecture Notes in Computer Science 9200, September 2015, 24 pages</comments><doi>10.1007/978-3-319-23165-5_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning semantically in first-order logic is notoriously a challenge. This
paper surveys a selection of semantically-guided or model-based methods that
aim at meeting aspects of this challenge. For first-order logic we touch upon
resolution-based methods, tableaux-based methods, DPLL-inspired methods, and we
give a preview of a new method called SGGS, for Semantically-Guided
Goal-Sensitive reasoning. For first-order theories we highlight hierarchical
and locality-based methods, concluding with the recent Model-Constructing
satisfiability calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02538</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02538</id><created>2015-02-09</created><updated>2015-02-18</updated><authors><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>Sastry</keyname><forenames>Srikanth</forenames></author></authors><title>Consensus using Asynchronous Failure Detectors</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FLP result shows that crash-tolerant consensus is impossible to solve in
asynchronous systems, and several solutions have been proposed for
crash-tolerant consensus under alternative (stronger) models. One popular
approach is to augment the asynchronous system with appropriate failure
detectors, which provide (potentially unreliable) information about process
crashes in the system, to circumvent the FLP impossibility.
  In this paper, we demonstrate the exact mechanism by which (sufficiently
powerful) asynchronous failure detectors enable solving crash-tolerant
consensus. Our approach, which borrows arguments from the FLP impossibility
proof and the famous result from CHT, which shows that $\Omega$ is a weakest
failure detector to solve consensus, also yields a natural proof to $\Omega$ as
a weakest asynchronous failure detector to solve consensus. The use of I/O
automata theory in our approach enables us to model execution in a more
detailed fashion than CHT and also addresses the latent assumptions and
assertions in the original result in CHT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02539</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02539</id><created>2015-02-09</created><updated>2015-02-28</updated><authors><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Gravel</keyname><forenames>Claude</forenames></author></authors><title>Sampling with arbitrary precision</title><categories>cs.IT math.IT</categories><comments>3 figures, one appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of the generation of a continuous random variable when a
source of independent fair coins is available. We first motivate the choice of
a natural criterion for measuring accuracy, the Wasserstein $L_\infty$ metric,
and then show a universal lower bound for the expected number of required fair
coins as a function of the accuracy. In the case of an absolutely continuous
random variable with finite differential entropy, several algorithms are
presented that match the lower bound up to a constant, which can be eliminated
by generating random variables in batches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02551</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02551</id><created>2015-02-09</created><authors><author><keyname>Gupta</keyname><forenames>Suyog</forenames></author><author><keyname>Agrawal</keyname><forenames>Ankur</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Kailash</forenames></author><author><keyname>Narayanan</keyname><forenames>Pritish</forenames></author></authors><title>Deep Learning with Limited Numerical Precision</title><categories>cs.LG cs.NE stat.ML</categories><comments>10 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training of large-scale deep neural networks is often constrained by the
available computational resources. We study the effect of limited precision
data representation and computation on neural network training. Within the
context of low-precision fixed-point computations, we observe the rounding
scheme to play a crucial role in determining the network's behavior during
training. Our results show that deep networks can be trained using only 16-bit
wide fixed-point number representation when using stochastic rounding, and
incur little to no degradation in the classification accuracy. We also
demonstrate an energy-efficient hardware accelerator that implements
low-precision fixed-point arithmetic with stochastic rounding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02557</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02557</id><created>2015-02-09</created><authors><author><keyname>Derka</keyname><forenames>Martin</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Maftuleac</keyname><forenames>Daniela</forenames></author></authors><title>List Colouring Big Graphs On-Line</title><categories>cs.DS cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of graph list colouring in the
on-line setting. We provide several results on paintability of graphs in the
model introduced by Schauz [13] and Zhu [20]. We prove that the on-line version
of Ohba's conjecture is true in the class of planar graphs. We also consider
several alternate on-line list colouring models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02558</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02558</id><created>2015-02-09</created><updated>2015-12-26</updated><authors><author><keyname>Park</keyname><forenames>Mijung</forenames></author><author><keyname>Jitkrittum</keyname><forenames>Wittawat</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author></authors><title>K2-ABC: Approximate Bayesian Computation with Kernel Embeddings</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complicated generative models often result in a situation where computing the
likelihood of observed data is intractable, while simulating from the
conditional density given a parameter value is relatively easy. Approximate
Bayesian Computation (ABC) is a paradigm that enables simulation-based
posterior inference in such cases by measuring the similarity between simulated
and observed data in terms of a chosen set of summary statistics. However,
there is no general rule to construct sufficient summary statistics for complex
models. Insufficient summary statistics will &quot;leak&quot; information, which leads to
ABC algorithms yielding samples from an incorrect (partial) posterior. In this
paper, we propose a fully nonparametric ABC paradigm which circumvents the need
for manually selecting summary statistics. Our approach, K2-ABC, uses maximum
mean discrepancy (MMD) as a dissimilarity measure between the distributions
over observed and simulated data. MMD is easily estimated as the squared
difference between their empirical kernel embeddings. Experiments on a
simulated scenario and a real-world biological problem illustrate the
effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02563</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02563</id><created>2015-02-09</created><updated>2015-12-02</updated><authors><author><keyname>Hajdu&#x161;ek</keyname><forenames>Michal</forenames></author><author><keyname>P&#xe9;rez-Delgado</keyname><forenames>Carlos A.</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author></authors><title>Device-Independent Verifiable Blind Quantum Computation</title><categories>quant-ph cs.CC cs.CR</categories><comments>Shortly before submission of this preprint, the authors became aware
  of parallel and independent research by Gheorghiu, Kashefi and Wallden, which
  also addresses device-independent verifiable blind quantum computation, and
  appears simultaneously</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As progress on experimental quantum processors continues to advance, the
problem of verifying the correct operation of such devices is becoming a
pressing concern. The recent discovery of protocols for verifying computation
performed by entangled but non-communicating quantum processors holds the
promise of certifying the correctness of arbitrary quantum computations in a
fully device-independent manner. Unfortunately, all known schemes have
prohibitive overhead, with resources scaling as extremely high degree
polynomials in the number of gates constituting the computation. Here we
present a novel approach based on a combination of verified blind quantum
computation and Bell state self-testing. This approach has dramatically reduced
overhead, with resources scaling as only $O(m^4\ln m)$ in the number of gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02567</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02567</id><created>2015-02-09</created><updated>2015-06-06</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Opinion formation driven by PageRank node influence on directed networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 6 figures. Published in Physica A 436, 707-715 (2015)</comments><journal-ref>Physica A 436, 707-715 (2015)</journal-ref><doi>10.1016/j.physa.2015.05.095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a two states opinion formation model driven by PageRank node
influence and report an extensive numerical study on how PageRank affects
collective opinion formations in large-scale empirical directed networks. In
our model the opinion of a node can be updated by the sum of its neighbor
nodes' opinions weighted by the node influence of the neighbor nodes at each
step. We consider PageRank probability and its sublinear power as node
influence measures and investigate evolution of opinion under various
conditions. First, we observe that all networks reach steady state opinion
after a certain relaxation time. This time scale is decreasing with the
heterogeneity of node influence in the networks. Second, we find that our model
shows consensus and non-consensus behavior in steady state depending on types
of networks: Web graph, citation network of physics articles, and LiveJournal
social network show non-consensus behavior while Wikipedia article network
shows consensus behavior. Third, we find that a more heterogeneous influence
distribution leads to a more uniform opinion state in the cases of Web graph,
Wikipedia, and Livejournal. However, the opposite behavior is observed in the
citation network. Finally we identify that a small number of influential nodes
can impose their own opinion on significant fraction of other nodes in all
considered networks. Our study shows that the effects of heterogeneity of node
influence on opinion formation can be significant and suggests further
investigations on the interplay between node influence and collective opinion
in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02585</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02585</id><created>2015-02-09</created><updated>2015-02-10</updated><authors><author><keyname>Kouzapas</keyname><forenames>Dimitrios</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames></author></authors><title>Core Higher-Order Session Processes: Tractable Equivalences and Relative
  Expressiveness</title><categories>cs.LO</categories><comments>90 pages</comments><acm-class>D.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes tractable bisimulations for the higher-order pi-calculus
with session primitives (HOpi) and offers a complete study of the expressivity
of its most significant subcalculi. First we develop three typed bisimulations,
which are shown to coincide with contextual equivalence. These
characterisations demonstrate that observing as inputs only a specific finite
set of higher-order values (which inhabit session types) suffices to reason
about HOp} processes. Next, we identify HO, a minimal, second-order subcalculus
of HOpi in which higher-order applications/abstractions, name-passing, and
recursion are absent. We show that HO can encode HOpi extended with
higher-order applications and abstractions and that a first-order session
pi-calculus can encode HOpi. Both encodings are fully abstract. We also prove
that the session pi-calculus with passing of shared names cannot be encoded
into HOpi without shared names. We show that HOpi, HO, and pi are equally
expressive; the expressivity of HO enables effective reasoning about typed
equivalences for higher-order processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02589</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02589</id><created>2015-02-09</created><authors><author><keyname>Nair</keyname><forenames>Chandra</forenames></author><author><keyname>Xia</keyname><forenames>Lingxiao</forenames></author><author><keyname>Yazdanpanah</keyname><forenames>Mehdi</forenames></author></authors><title>Sub-optimality of the Han--Kobayashi Achievable Region for Interference
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Han-Kobayashi achievable region is the best known inner bound for a general
discrete memoryless interference channel. We show that the capacity region can
be strictly larger than the Han-Kobayashi region for some channel realizations,
and hence the strict sub-optimality of Han-Kobayashi achievable region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02590</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02590</id><created>2015-02-09</created><updated>2016-01-07</updated><authors><author><keyname>Fawzi</keyname><forenames>Alhussein</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Analysis of classifiers' robustness to adversarial perturbations</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to analyze an intriguing phenomenon recently
discovered in deep networks, namely their instability to adversarial
perturbations (Szegedy et. al., 2014). We provide a theoretical framework for
analyzing the robustness of classifiers to adversarial perturbations, and show
fundamental upper bounds on the robustness of classifiers. Specifically, we
establish a general upper bound on the robustness of classifiers to adversarial
perturbations, and then illustrate the obtained upper bound on the families of
linear and quadratic classifiers. In both cases, our upper bound depends on a
distinguishability measure that captures the notion of difficulty of the
classification task. Our results for both classes imply that in tasks involving
small distinguishability, no classifier in the considered set will be robust to
adversarial perturbations, even if a good accuracy is achieved. Our theoretical
framework moreover suggests that the phenomenon of adversarial instability is
due to the low flexibility of classifiers, compared to the difficulty of the
classification task (captured by the distinguishability). Moreover, we show the
existence of a clear distinction between the robustness of a classifier to
random noise and its robustness to adversarial perturbations. Specifically, the
former is shown to be larger than the latter by a factor that is proportional
to \sqrt{d} (with d being the signal dimension) for linear classifiers. This
result gives a theoretical explanation for the discrepancy between the two
robustness properties in high dimensional problems, which was empirically
observed in the context of neural networks. To the best of our knowledge, our
results provide the first theoretical work that addresses the phenomenon of
adversarial instability recently observed for deep networks. Our analysis is
complemented by experimental results on controlled and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02599</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02599</id><created>2015-02-09</created><authors><author><keyname>Elshrif</keyname><forenames>Mohamed</forenames></author><author><keyname>Fokoue</keyname><forenames>Ernest</forenames></author></authors><title>Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel adaptive random subspace learning algorithm (RSSL) for
prediction purpose. This new framework is flexible where it can be adapted with
any learning technique. In this paper, we tested the algorithm for regression
and classification problems. In addition, we provide a variety of weighting
schemes to increase the robustness of the developed algorithm. These different
wighting flavors were evaluated on simulated as well as on real-world data sets
considering the cases where the ratio between features (attributes) and
instances (samples) is large and vice versa. The framework of the new algorithm
consists of many stages: first, calculate the weights of all features on the
data set using the correlation coefficient and F-statistic statistical
measurements. Second, randomly draw n samples with replacement from the data
set. Third, perform regular bootstrap sampling (bagging). Fourth, draw without
replacement the indices of the chosen variables. The decision was taken based
on the heuristic subspacing scheme. Fifth, call base learners and build the
model. Sixth, use the model for prediction purpose on test set of the data. The
results show the advancement of the adaptive RSSL algorithm in most of the
cases compared with the synonym (conventional) machine learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02605</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02605</id><created>2015-02-09</created><authors><author><keyname>Brat</keyname><forenames>Guillaume</forenames></author><author><keyname>Bushnell</keyname><forenames>David</forenames></author><author><keyname>Davies</keyname><forenames>Misty</forenames></author><author><keyname>Giannakopoulou</keyname><forenames>Dimitra</forenames></author><author><keyname>Howar</keyname><forenames>Falk</forenames></author><author><keyname>Kahsai</keyname><forenames>Temesghen</forenames></author></authors><title>Verifying the Safety of a Flight-Critical System</title><categories>cs.SE</categories><comments>17 pages, 5 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper describes our work on demonstrating verification technologies on a
flight-critical system of realistic functionality, size, and complexity. Our
work targeted a commercial aircraft control system named Transport Class Model
(TCM), and involved several stages: formalizing and disambiguating requirements
in collaboration with do- main experts; processing models for their use by
formal verification tools; applying compositional techniques at the
architectural and component level to scale verification. Performed in the
context of a major NASA milestone, this study of formal verification in
practice is one of the most challenging that our group has performed, and it
took several person months to complete it. This paper describes the methodology
that we followed and the lessons that we learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02606</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02606</id><created>2015-02-09</created><updated>2015-04-22</updated><authors><author><keyname>Barbosa</keyname><forenames>Rafael da Ponte</forenames></author><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>The Power of Randomization: Distributed Submodular Maximization on
  Massive Datasets</title><categories>cs.LG cs.AI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of problems in machine learning, including exemplar
clustering, document summarization, and sensor placement, can be cast as
constrained submodular maximization problems. Unfortunately, the resulting
submodular optimization problems are often too large to be solved on a single
machine. We develop a simple distributed algorithm that is embarrassingly
parallel and it achieves provable, constant factor, worst-case approximation
guarantees. In our experiments, we demonstrate its efficiency in large problems
with different kinds of constraints with objective values always close to what
is achievable in the centralized setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02609</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02609</id><created>2015-02-09</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Rosenfeld</keyname><forenames>Joel A.</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Efficient model-based reinforcement learning for approximate online
  optimal</title><categories>cs.SY cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the infinite horizon optimal regulation problem is solved
online for a deterministic control-affine nonlinear dynamical system using the
state following (StaF) kernel method to approximate the value function. Unlike
traditional methods that aim to approximate a function over a large compact
set, the StaF kernel method aims to approximate a function in a small
neighborhood of a state that travels within a compact set. Simulation results
demonstrate that stability and approximate optimality of the control system can
be achieved with significantly fewer basis functions than may be required for
global approximation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02635</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02635</id><created>2015-02-09</created><authors><author><keyname>Ferrer</keyname><forenames>Marita</forenames></author><author><keyname>Gary</keyname><forenames>Margarita</forenames></author><author><keyname>Hernandez</keyname><forenames>Salvador</forenames></author></authors><title>Weight-preserving isomorphisms between spaces of continuous functions:
  The scalar case</title><categories>math.FA cs.IT math.GN math.IT</categories><msc-class>Primary 46E10. Secondary 54C35, 47B38, 93B05, 94B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb F$ be a finite field and let $\mathcal A$ and $\mathcal B$ be
vector spaces of $\mathbb F$-valued continuous functions defined on locally
compact spaces $X$ and $Y$, respectively. We look at the representation of
linear bijections $H:\mathcal A\longrightarrow \mathcal B$ by continuous
functions $h:Y\longrightarrow X$ as weighted composition operators. In order to
do it, we extend the notion of Hamming metric to infinite spaces. Our main
result establishes that under some mild conditions, every Hamming isometry can
be represented as a weighted composition operator. Connections to coding theory
are also highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02642</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02642</id><created>2015-02-09</created><updated>2015-03-12</updated><authors><author><keyname>Oulad-Naoui</keyname><forenames>Slimane</forenames></author><author><keyname>Djoudi</keyname><forenames>Mahieddine</forenames></author></authors><title>S2WC2: a User Centric Web Usage Mining Framework</title><categories>cs.DB cs.CC cs.FL</categories><comments>14 pages, 6 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike many works in Web Usage Mining, which took as object of study logs
recorded by Web servers; we develop in this paper a different approach,
qualified as usercentric, judged more accurate and effective. To do so, we
develop first a client side tool, in order to collect the user navigation
traces. Based on Browser Helper Object, this tool has several advantages as the
lightness, the simple exploitation, and almost absence of changes in user
environment. Secondly, we elaborate a pre-processing application to prepare the
raw client logs. This application includes numerous algorithms and modules,
which are originals and appropriates to the log defined format, in order to
clean it, reconstruct user sessions and to do some final formatting tasks. The
last stage in our work is devoted to the task of client web session clustering
using Kohonen Self organizing Maps, with the use of a free knowledge discovery
tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02643</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02643</id><created>2015-02-09</created><authors><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>Random Coordinate Descent Methods for Minimizing Decomposable Submodular
  Functions</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular function minimization is a fundamental optimization problem that
arises in several applications in machine learning and computer vision. The
problem is known to be solvable in polynomial time, but general purpose
algorithms have high running times and are unsuitable for large-scale problems.
Recent work have used convex optimization techniques to obtain very practical
algorithms for minimizing functions that are sums of ``simple&quot; functions. In
this paper, we use random coordinate descent methods to obtain algorithms with
faster linear convergence rates and cheaper iteration costs. Compared to
alternating projection methods, our algorithms do not rely on full-dimensional
vector operations and they converge in significantly fewer iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02647</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02647</id><created>2015-02-09</created><authors><author><keyname>Mukherjee</keyname><forenames>Pritam</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Secure Degrees of Freedom Region of the Two-User MISO Broadcast Channel
  with Alternating CSIT</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two user multiple-input single-output (MISO) broadcast channel with
confidential messages (BCCM) is studied in which the nature of channel state
information at the transmitter (CSIT) from each user can be of the form
$I_{i}$, $i=1,2$ where $I_{1}, I_{2}\in \{\mathsf{P}, \mathsf{D},
\mathsf{N}\}$, and the forms $\mathsf{P}$, $\mathsf{D}$ and $\mathsf{N}$
correspond to perfect and instantaneous, completely delayed, and no CSIT,
respectively. Thus, the overall CSIT can alternate between $9$ possible states
corresponding to all possible values of $I_{1}I_{2}$, with each state occurring
for $\lambda_{I_{1}I_{2}}$ fraction of the total duration. The main
contribution of this paper is to establish the secure degrees of freedom
(s.d.o.f.) region of the MISO BCCM with alternating CSIT with the symmetry
assumption, where $\lambda_{I_{1} I_{2}}=\lambda_{I_{2}I_{1}}$.
  The main technical contributions include developing a) novel achievable
schemes for MISO BCCM with alternating CSIT with security constraints which
also highlight the synergistic benefits of inter-state coding for secrecy, b)
new converse proofs via local statistical equivalence and channel enhancement;
and c) showing the interplay between various aspects of channel knowledge and
their impact on s.d.o.f.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02651</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02651</id><created>2015-02-09</created><authors><author><keyname>Beygelzimer</keyname><forenames>Alina</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author></authors><title>Optimal and Adaptive Algorithms for Online Boosting</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online boosting, the task of converting any weak online learner into
a strong online learner. Based on a novel and natural definition of weak online
learnability, we develop two online boosting algorithms. The first algorithm is
an online version of boost-by-majority. By proving a matching lower bound, we
show that this algorithm is essentially optimal in terms of the number of weak
learners and the sample complexity needed to achieve a specified accuracy. This
optimal algorithm is not adaptive however. Using tools from online loss
minimization, we derive an adaptive online boosting algorithm that is also
parameter-free, but not optimal. Both algorithms work with base learners that
can handle example importance weights directly, as well as by rejection
sampling examples with probability defined by the booster. Results are
complemented with an extensive experimental study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02655</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02655</id><created>2015-02-07</created><authors><author><keyname>&#x160;uster</keyname><forenames>Simon</forenames></author></authors><title>An investigation into language complexity of World-of-Warcraft
  game-external texts</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a language complexity analysis of World of Warcraft (WoW)
community texts, which we compare to texts from a general corpus of web
English. Results from several complexity types are presented, including lexical
diversity, density, readability and syntactic complexity. The language of WoW
texts is found to be comparable to the general corpus on some complexity
measures, yet more specialized on other measures. Our findings can be used by
educators willing to include game-related activities into school curricula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02704</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02704</id><created>2015-02-09</created><authors><author><keyname>Beygelzimer</keyname><forenames>Alina</forenames></author><author><keyname>Daum&#xe9;</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author></authors><title>Learning Reductions that Really Work</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a summary of the mathematical and computational techniques that
have enabled learning reductions to effectively address a wide class of
problems, and show that this approach to solving machine learning problems can
be broadly useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02710</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02710</id><created>2015-02-09</created><updated>2015-04-20</updated><authors><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author></authors><title>Scalable Multilabel Prediction via Randomized Methods</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling the dependence between outputs is a fundamental challenge in
multilabel classification. In this work we show that a generic regularized
nonlinearity mapping independent predictions to joint predictions is sufficient
to achieve state-of-the-art performance on a variety of benchmark problems.
Crucially, we compute the joint predictions without ever obtaining any
independent predictions, while incorporating low-rank and smoothness
regularization. We achieve this by leveraging randomized algorithms for matrix
decomposition and kernel approximation. Furthermore, our techniques are
applicable to the multiclass setting. We apply our method to a variety of
multiclass and multilabel data sets, obtaining state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02711</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02711</id><created>2015-01-16</created><authors><author><keyname>de la Cruz</keyname><forenames>Javier</forenames></author><author><keyname>Kiermaier</keyname><forenames>Michael</forenames></author><author><keyname>Wassermann</keyname><forenames>Alfred</forenames></author><author><keyname>Willems</keyname><forenames>Wolfgang</forenames></author></authors><title>Algebraic structures of MRD Codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on results in finite geometry we prove the existence of MRD codes in
(F_q)_(n,n) with minimum distance $n$ which are essentially different from
Gabidulin codes. The construction results from algebraic structures which are
closely related to those of finite fields. Furthermore we show that an analogue
of MacWilliams' extension theorem does not exist for linear rank metric codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02725</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02725</id><created>2015-02-09</created><updated>2016-03-04</updated><authors><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>Why Transactional Memory Should Not Be Obstruction-Free</title><categories>cs.DC</categories><comments>Model of Transactional Memory identical with arXiv:1407.6876</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory (TM) is an inherently optimistic abstraction: it allows
concurrent processes to execute sequences of shared-data accesses
(transactions) speculatively, with an option of aborting them in the future.
Early TM designs avoided using locks and relied on non-blocking synchronization
to ensure obstruction-freedom: a transaction that encounters no step contention
is not allowed to abort. However, it was later observed that obstruction-free
TMs perform poorly and, as a result, state-of-the-art TM implementations are
nowadays blocking, allowing aborts because of data conflicts rather than step
contention.
  In this paper, we explain this shift in the TM practice theoretically, via
complexity bounds. We prove a few important lower bounds on obstruction-free
TMs. Then we present a lock-based TM implementation that beats all of these
lower bounds. In sum, our results exhibit a considerable complexity gap between
non-blocking and blocking TM implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02727</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02727</id><created>2015-02-09</created><updated>2015-11-19</updated><authors><author><keyname>Le</keyname><forenames>Tuan A.</forenames></author><author><keyname>Nguyen</keyname><forenames>Hieu D.</forenames></author></authors><title>New Multiple Insertion-Deletion Correcting Codes for Non-Binary
  Alphabets</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><msc-class>94</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Helberg's number-theoretic construction of multiple
insertion-deletion correcting binary codes to non-binary alphabets and describe
a linear decoding algorithm for correcting multiple deletions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02733</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02733</id><created>2015-02-09</created><updated>2015-04-22</updated><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Schulte</keyname><forenames>Patrick</forenames></author><author><keyname>Steiner</keyname><forenames>Fabian</forenames></author></authors><title>Bandwidth Efficient and Rate-Matched Low-Density Parity-Check Coded
  Modulation</title><categories>cs.IT math.IT</categories><comments>13 pages, 11 figures, 10 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new coded modulation scheme is proposed. At the transmitter, the
concatenation of a distribution matcher and a systematic binary encoder
performs probabilistic signal shaping and channel coding. At the receiver, the
output of a bitwise demapper is fed to a binary decoder. No iterative demapping
is performed. Rate adaption is achieved by adjusting the input distribution and
the transmission power. The scheme is applied to bipolar amplitude shift keying
(ASK) constellations with equidistant signal points and it is directly
applicable to two-dimensional quadrature amplitude modulation (QAM). The scheme
is implemented by using the DVB-S2 low-density parity-check (LDPC) codes. At a
frame error rate of 1e-3, the new scheme operates within less than 1 dB of the
AWGN capacity 0.5log2(1+SNR) at any spectral efficiency between 1 and 5
bits/s/Hz by using only 5 modes, i.e., 4-ASK with code rate 2/3, 8-ASK with
3/4, 16-ASK and 32-ASK with 5/6 and 64-ASK with 9/10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02734</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02734</id><created>2015-02-09</created><updated>2015-10-05</updated><authors><author><keyname>Papandreou</keyname><forenames>George</forenames></author><author><keyname>Chen</keyname><forenames>Liang-Chieh</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image
  Segmentation</title><categories>cs.CV</categories><comments>Accepted to ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (DCNNs) trained on a large number of
images with strong pixel-level annotations have recently significantly pushed
the state-of-art in semantic image segmentation. We study the more challenging
problem of learning DCNNs for semantic image segmentation from either (1)
weakly annotated training data such as bounding boxes or image-level labels or
(2) a combination of few strongly labeled and many weakly labeled images,
sourced from one or multiple datasets. We develop Expectation-Maximization (EM)
methods for semantic image segmentation model training under these weakly
supervised and semi-supervised settings. Extensive experimental evaluation
shows that the proposed techniques can learn models delivering competitive
results on the challenging PASCAL VOC 2012 image segmentation benchmark, while
requiring significantly less annotation effort. We share source code
implementing the proposed system at
https://bitbucket.org/deeplab/deeplab-public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02741</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02741</id><created>2015-02-09</created><updated>2015-08-30</updated><authors><author><keyname>Tang</keyname><forenames>Sui</forenames></author></authors><title>System Identification in Dynamical Sampling</title><categories>cs.IT math.IT</categories><comments>25 pages, 7 figures</comments><msc-class>94A20, 94A12, 42C15, 15A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of spatiotemporal sampling in a discrete infinite
dimensional spatially invariant evolutionary process $x^{(n)}=A^nx$ to recover
an unknown convolution operator $A$ given by a filter $a \in
\ell^1(\mathbb{Z})$ and an unknown initial state $x$ modeled as avector in
$\ell^2(\mathbb{Z})$. Traditionally, under appropriate hypotheses, any $x$ can
be recovered from its samples on $\mathbb{Z}$ and $A$ can be recovered by the
classical techniques of deconvolution. In this paper, we will exploit the
spatiotemporal correlation and propose a new spatiotemporal sampling scheme to
recover $A$ and $x$ that allows to sample the evolving states $x,Ax, \cdots,
A^{N-1}x$ on a sub-lattice of $\mathbb{Z}$, and thus achieve the spatiotemporal
trade off. The spatiotemporal trade off is motivated by several industrial
applications \cite{Lv09}. Specifically, we show that $\{x(m\mathbb{Z}),
Ax(m\mathbb{Z}), \cdots, A^{N-1}x(m\mathbb{Z}): N \geq 2m\}$ contains enough
information to recover a typical &quot;low pass filter&quot; $a$ and $x$ almost surely,
in which we generalize the idea of the finite dimensional case in \cite{AK14}.
In particular, we provide an algorithm based on a generalized Prony method for
the case when both $a$ and $x$ are of finite impulse response and an upper
bound of their support is known. We also perform the perturbation analysis
based on the spectral properties of the operator $A$ and initial state $x$, and
verify them by several numerical experiments. Finally, we provide several other
numerical methods to stabilize the method and numerical example shows the
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02753</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02753</id><created>2015-02-09</created><updated>2015-02-19</updated><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author></authors><title>Optimum-width upward drawings of trees I: Rooted pathwidth</title><categories>cs.CG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An upward drawing of a rooted tree is a drawing such that no parents are
below their children. It is ordered if the edges to children appear in
prescribed order around each vertex. It is well-known that any tree has an
upward (unordered) drawing with width $\log (n+1)$. For ordered drawings, the
best-known bounds for the width for binary trees is $O(\log n)$, while for
arbitrary trees it is $O(2^{O(\sqrt{\log n})})$. We present algorithms that
compute upward drawings with instance-optimal width, i.e., the width is the
minimum-possible for the input tree. In this first paper, we mostly study
unordered drawings, where the algorithm is very simple and the drawings
obtained are straight-line. We also give 2-approximation algorithms for the
width of upward ordered drawings, and $O(\Delta)$-approximation where
additionally the height is small. In particular any tree has an upward
straight-line ordered drawing of area $O(\Delta n\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02761</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02761</id><created>2015-02-09</created><authors><author><keyname>Li</keyname><forenames>Yujia</forenames></author><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author></authors><title>Generative Moment Matching Networks</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning deep generative models from data. We
formulate a method that generates an independent sample via a single
feedforward pass through a multilayer perceptron, as in the recently proposed
generative adversarial networks (Goodfellow et al., 2014). Training a
generative adversarial network, however, requires careful optimization of a
difficult minimax program. Instead, we utilize a technique from statistical
hypothesis testing known as maximum mean discrepancy (MMD), which leads to a
simple objective that can be interpreted as matching all orders of statistics
between a dataset and samples from the model, and can be trained by
backpropagation. We further boost the performance of this approach by combining
our generative network with an auto-encoder network, using MMD to learn to
generate codes that can then be decoded to produce samples. We show that the
combination of these techniques yields excellent generative models compared to
baseline approaches as measured on MNIST and the Toronto Face Database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02763</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02763</id><created>2015-02-09</created><updated>2015-05-18</updated><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author></authors><title>Cascading Bandits: Learning to Rank in the Cascade Model</title><categories>cs.LG stat.ML</categories><comments>Proceedings of the 32nd International Conference on Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A search engine usually outputs a list of $K$ web pages. The user examines
this list, from the first web page to the last, and chooses the first
attractive page. This model of user behavior is known as the cascade model. In
this paper, we propose cascading bandits, a learning variant of the cascade
model where the objective is to identify $K$ most attractive items. We
formulate our problem as a stochastic combinatorial partial monitoring problem.
We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We
also prove gap-dependent upper bounds on the regret of these algorithms and
derive a lower bound on the regret in cascading bandits. The lower bound
matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We
experiment with our algorithms on several problems. The algorithms perform
surprisingly well even when our modeling assumptions are violated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02764</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02764</id><created>2015-02-09</created><authors><author><keyname>Lin</keyname><forenames>Yu-Ting</forenames></author></authors><title>The Modeling and Quantification of Rhythmic to Non-rhythmic Phenomenon
  in Electrocardiography during Anesthesia</title><categories>q-bio.NC cs.CE</categories><comments>Doctoral Dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variations of instantaneous heart rate appears regularly oscillatory in
deeper levels of anesthesia and less regular in lighter levels of anesthesia.
It is impossible to observe this &quot;rhythmic-to-non-rhythmic&quot; phenomenon from raw
electrocardiography waveform in current standard anesthesia monitors. To
explore the possible clinical value, I proposed the adaptive harmonic model,
which fits the descriptive property in physiology, and provides adequate
mathematical conditions for the quantification. Based on the adaptive harmonic
model, multitaper Synchrosqueezing transform was used to provide time-varying
power spectrum, which facilitates to compute the quantitative index:
&quot;Non-rhythmic-to-Rhythmic Ratio&quot; index (NRR index). I then used a clinical
database to analyze the behavior of NRR index and compare it with other
standard indices of anesthetic depth. The positive statistical results suggest
that NRR index provides addition clinical information regarding motor reaction,
which aligns with current standard tools. Furthermore, the ability to indicates
the noxious stimulation is an additional finding. Lastly, I have proposed an
real-time interpolation scheme to contribute my study further as a clinical
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02766</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02766</id><created>2015-02-09</created><updated>2015-04-20</updated><authors><author><keyname>Farfade</keyname><forenames>Sachin Sudhakar</forenames></author><author><keyname>Saberian</keyname><forenames>Mohammad</forenames></author><author><keyname>Li</keyname><forenames>Li-Jia</forenames></author></authors><title>Multi-view Face Detection Using Deep Convolutional Neural Networks</title><categories>cs.CV</categories><comments>in International Conference on Multimedia Retrieval 2015 (ICMR)</comments><acm-class>I.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we consider the problem of multi-view face detection. While
there has been significant research on this problem, current state-of-the-art
approaches for this task require annotation of facial landmarks, e.g. TSM [25],
or annotation of face poses [28, 22]. They also require training dozens of
models to fully capture faces in all orientations, e.g. 22 models in HeadHunter
method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method
that does not require pose/landmark annotation and is able to detect faces in a
wide range of orientations using a single model based on deep convolutional
neural networks. The proposed method has minimal complexity; unlike other
recent deep learning object detection methods [9], it does not require
additional components such as segmentation, bounding-box regression, or SVM
classifiers. Furthermore, we analyzed scores of the proposed face detector for
faces in different orientations and found that 1) the proposed method is able
to detect faces from different angles and can handle occlusion to some extent,
2) there seems to be a correlation between dis- tribution of positive examples
in the training set and scores of the proposed face detector. The latter
suggests that the proposed methods performance can be further improved by using
better sampling strategies and more sophisticated data augmentation techniques.
Evaluations on popular face detection benchmark datasets show that our
single-model face detector algorithm has similar or better performance compared
to the previous methods, which are more complex and require annotations of
either different poses or facial landmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02768</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02768</id><created>2015-02-09</created><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Virtual Network Embedding Algorithms Based on Best-Fit Subgraph
  Detection</title><categories>cs.DC cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1502.02358</comments><journal-ref>Computer and Information Science; Vol. 8, No. 1; 2015</journal-ref><doi>10.5539/cis.v8n1p62</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main objectives of cloud computing providers is increasing the
revenue of their cloud datacenters by accommodating virtual network requests as
many as possible. However, arrival and departure of virtual network requests
fragment physical network's resources and reduce the possibility of accepting
more virtual network requests. To increase the number of virtual network
requests accommodated by fragmented physical networks, we propose two virtual
network embedding algorithms, which coarsen virtual networks using Heavy Edge
Matching (HEM) technique and embed coarsened virtual networks on best-fit
sub-substrate networks. The performance of the proposed algorithms are
evaluated and compared with existing algorithms using extensive simulations,
which show that the proposed algorithms increase the acceptance ratio and the
revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02772</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02772</id><created>2015-02-09</created><updated>2015-02-10</updated><authors><author><keyname>Lau</keyname><forenames>Kean Hong</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author><author><keyname>Lo</keyname><forenames>Fook Loong</forenames></author></authors><title>A HMAX with LLC for visual recognition</title><categories>cs.CV</categories><comments>10 pages, 3 figures, 2 tables, 23 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's high performance deep artificial neural networks (ANNs) rely heavily
on parameter optimization, which is sequential in nature and even with a
powerful GPU, would have taken weeks to train them up for solving challenging
tasks [22]. HMAX [17] has demonstrated that a simple high performing network
could be obtained without heavy optimization. In this paper, we had improved on
the existing best HMAX neural network [12] in terms of structural simplicity
and performance. Our design replaces the L1 minimization sparse coding (SC)
with a locality-constrained linear coding (LLC) [20] which has a lower
computational demand. We also put the simple orientation filter bank back into
the front layer of the network replacing PCA. Our system's performance has
improved over the existing architecture and reached 79.0% on the challenging
Caltech-101 [7] dataset, which is state-of-the-art for ANNs (without transfer
learning). From our empirical data, the main contributors to our system's
performance include an introduction of partial signal whitening, a spot
detector, and a spatial pyramid matching (SPM) [14] layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02777</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02777</id><created>2015-02-09</created><updated>2015-03-17</updated><authors><author><keyname>Lorince</keyname><forenames>Jared</forenames></author><author><keyname>Zorowitz</keyname><forenames>Sam</forenames></author><author><keyname>Murdock</keyname><forenames>Jaimie</forenames></author><author><keyname>Todd</keyname><forenames>Peter M.</forenames></author></authors><title>The Wisdom of the Few? &quot;Supertaggers&quot; in Collaborative Tagging Systems</title><categories>cs.SI cs.IR</categories><comments>14 pages, 11 figures</comments><doi>10.1561/106.00000002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A folksonomy is ostensibly an information structure built up by the &quot;wisdom
of the crowd&quot;, but is the &quot;crowd&quot; really doing the work? Tagging is in fact a
sharply skewed process in which a small minority of &quot;supertagger&quot; users
generate an overwhelming majority of the annotations. Using data from three
large-scale social tagging platforms, we explore (a) how to best quantify the
imbalance in tagging behavior and formally define a supertagger, (b) how
supertaggers differ from other users in their tagging patterns, and (c) if
effects of motivation and expertise inform our understanding of what makes a
supertagger. Our results indicate that such prolific users not only tag more
than their counterparts, but in quantifiably different ways. Specifically, we
find that supertaggers are more likely to label content in the long tail of
less popular items, that they show differences in patterns of content tagged
and terms utilized, and are measurably different with respect to tagging
expertise and motivation. These findings suggest we should question the extent
to which folksonomies achieve crowdsourced classification via the &quot;wisdom of
the crowd&quot;, especially for broad folksonomies like Last.fm as opposed to narrow
folksonomies like Flickr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02791</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02791</id><created>2015-02-10</created><updated>2015-05-27</updated><authors><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author><author><keyname>Cao</keyname><forenames>Yue</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Learning Transferable Features with Deep Adaptation Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies reveal that a deep neural network can learn transferable
features which generalize well to novel tasks for domain adaptation. However,
as deep features eventually transition from general to specific along the
network, the feature transferability drops significantly in higher layers with
increasing domain discrepancy. Hence, it is important to formally reduce the
dataset bias and enhance the transferability in task-specific layers. In this
paper, we propose a new Deep Adaptation Network (DAN) architecture, which
generalizes deep convolutional neural network to the domain adaptation
scenario. In DAN, hidden representations of all task-specific layers are
embedded in a reproducing kernel Hilbert space where the mean embeddings of
different domain distributions can be explicitly matched. The domain
discrepancy is further reduced using an optimal multi-kernel selection method
for mean embedding matching. DAN can learn transferable features with
statistical guarantees, and can scale linearly by unbiased estimate of kernel
embedding. Extensive empirical evidence shows that the proposed architecture
yields state-of-the-art image classification error rates on standard domain
adaptation benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02793</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02793</id><created>2015-02-10</created><authors><author><keyname>Friedrich</keyname><forenames>Tobias</forenames></author><author><keyname>K&#xf6;tzing</keyname><forenames>Timo</forenames></author><author><keyname>Krejca</keyname><forenames>Martin</forenames></author><author><keyname>Sutton</keyname><forenames>Andrew M.</forenames></author></authors><title>The Benefit of Sex in Noisy Evolutionary Search</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The benefit of sexual recombination is one of the most fundamental questions
both in population genetics and evolutionary computation. It is widely believed
that recombination helps solving difficult optimization problems. We present
the first result, which rigorously proves that it is beneficial to use sexual
recombination in an uncertain environment with a noisy fitness function. For
this, we model sexual recombination with a simple estimation of distribution
algorithm called the Compact Genetic Algorithm (cGA), which we compare with the
classical $\mu+1$ EA. For a simple noisy fitness function with additive
Gaussian posterior noise $\mathcal{N}(0,\sigma^2)$, we prove that the
mutation-only $\mu+1$ EA typically cannot handle noise in polynomial time for
$\sigma^2$ large enough while the cGA runs in polynomial time as long as the
population size is not too small. This shows that in this uncertain environment
sexual recombination is provably beneficial. We observe the same behavior in a
small empirical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02796</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02796</id><created>2015-02-10</created><updated>2015-09-06</updated><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Hume</keyname><forenames>Margee</forenames></author><author><keyname>Reilly</keyname><forenames>John</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Soar</keyname><forenames>Jeffrey</forenames></author></authors><title>Opportunistic and Context-aware Affect Sensing on Smartphones: The
  Concept, Challenges and Opportunities</title><categories>cs.HC cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic affect sensing offers unprecedented potential for capturing
spontaneous affect ubiquitously, obviating biases inherent in the laboratory
setting. Facial expression and voice are two major affective displays, however
most affect sensing systems on smartphone avoid them due to extensive power
requirement. Encouragingly, due to the recent advent of low-power DSP (Digital
Signal Processing) co-processor and GPU (Graphics Processing Unit) technology,
audio and video sensing are becoming more feasible. To properly evaluate
opportunistically captured facial expression and voice, contextual information
about the dynamic audio-visual stimuli needs to be inferred. This paper
discusses recent advances of affect sensing on the smartphone and identifies
the key barriers and potential solutions of implementing opportunistic and
context-aware affect sensing on smartphone platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02799</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02799</id><created>2015-02-10</created><authors><author><keyname>Wang</keyname><forenames>Yisong</forenames></author></authors><title>On Forgetting in Tractable Propositional Fragments</title><categories>cs.AI</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distilling from a knowledge base only the part that is relevant to a subset
of alphabet, which is recognized as forgetting, has attracted extensive
interests in AI community. In standard propositional logic, a general algorithm
of forgetting and its computation-oriented investigation in various fragments
whose satisfiability are tractable are still lacking. The paper aims at filling
the gap. After exploring some basic properties of forgetting in propositional
logic, we present a resolution-based algorithm of forgetting for CNF fragment,
and some complexity results about forgetting in Horn, renamable Horn, q-Horn,
Krom, DNF and CNF fragments of propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02800</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02800</id><created>2015-02-10</created><updated>2016-01-28</updated><authors><author><keyname>Covanov</keyname><forenames>Svyatoslav</forenames><affiliation>CARAMEL</affiliation></author><author><keyname>Thom&#xe9;</keyname><forenames>Emmanuel</forenames><affiliation>CARAMEL</affiliation></author></authors><title>Fast integer multiplication using generalized Fermat primes</title><categories>cs.SC cs.CC cs.DM cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For almost 35 years, Sch{\&quot;o}nhage-Strassen's algorithm has been thefastest
algorithm known for multiplying integers, with a time complexity$O(n \cdot \log
n \cdot \log \relax \log n)$ for multiplying $n$-bit inputs. In 2007,F{\&quot;u}rer
proved that there exists $K\textgreater{}1$ and an algorithm performing
thisoperation in $O(n \cdot \log n \cdot K^{\log^* n})$. Recent work by Harvey,
van der Hoeven, and Lecerf showed that this complexity estimate can be improved
in order to get $K=8$, andconjecturally $K=4$. We obtain here the same result
$K=4$ using simplemodular arithmetic as a building block, and a careful
complexityanalysis. We obtain a similar result $K=4$ using an alternative
somewhat simpler algorithm,which relies on arithmetic modulo generalized Fermat
primes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02803</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02803</id><created>2015-02-10</created><updated>2015-11-09</updated><authors><author><keyname>Shi</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author></authors><title>A TDOA technique with Super-Resolution based on the Volume
  Cross-Correlation Function</title><categories>cs.IT math.IT</categories><comments>13 pages, submitted and revised to IEEE Trans on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time Difference of Arrival (TDOA) is widely used in wireless localization
systems. Among the enormous approaches of TDOA, high resolution TDOA algorithms
have drawn much attention for its ability to resolve closely spaced signal
delays in multipath environment. However, the state-of-art high resolution TDOA
algorithms still have performance weakness on resolving time delays in a
wireless channel with dense multipath effect, as well as difficulties in
implementation for their high computation complexity. In this paper, we propose
a novel TDOA algorithm with super resolution based on a multi-dimensional
cross-correlation function: the Volume Cross-Correlation Function (VCC). The
proposed TDOA algorithm has excellent time resolution capability in multipath
environment, and it also has a much lower computational complexity. Because our
algorithm does not require priori knowledge about the waveform or power
spectrum of transmitted signals, it has great potential of usage in various
passive wireless localization systems. Numerical simulations is also provided
to demonstrate the validity of our conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02809</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02809</id><created>2015-02-10</created><updated>2015-02-10</updated><authors><author><keyname>Kang</keyname><forenames>Qingbo</forenames></author><author><keyname>Li</keyname><forenames>Ke</forenames></author><author><keyname>Chen</keyname><forenames>Hu</forenames></author></authors><title>An SVD-based Fragile Watermarking Scheme With Grouped Blocks</title><categories>cs.CR cs.MM</categories><comments>8 pages, 10 figures, already accept by 2014 2nd International
  Conference on Information Technology and Electronic Commerce (ICITEC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel fragile watermarking scheme for digital image
authentication which is based on Singular Value Decomposition(SVD) and grouped
blocks. The watermark bits which include two types of bits are inserted into
the least significant bit(LSB) plane of the host image using the adaptive
chaotic map to determine the positions. The groped blocks break the block-wise
independence and therefore can withstand the Vector Quantization attack(VQ
attack). The inserting positions are related to the statistical information of
image block data, in order to increase the security and provide an auxiliary
way to authenticate the image data. The effectiveness of the proposed scheme is
checked by a variety of attacks, and the experimental results prove that it has
a remarkable tamper detection ability and also has a precise locating ability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02817</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02817</id><created>2015-02-10</created><updated>2015-02-16</updated><authors><author><keyname>Conforti</keyname><forenames>Michele</forenames></author><author><keyname>Kaibel</keyname><forenames>Volker</forenames></author><author><keyname>Walter</keyname><forenames>Matthias</forenames></author><author><keyname>Weltge</keyname><forenames>Stefan</forenames></author></authors><title>Subgraph Polytopes and Independence Polytopes of Count Matroids</title><categories>cs.DM math.CO</categories><comments>8 pages, update to fix error</comments><msc-class>52B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph, the non-empty subgraph polytope is the convex hull
of the characteristic vectors of pairs (F, S) where S is a non-empty subset of
nodes and F is a subset of the edges with both endnodes in S. We obtain a
strong relationship between the non-empty subgraph polytope and the spanning
forest polytope. We further show that these polytopes provide polynomial size
extended formulations for independence polytopes of count matroids, which
generalizes recent results obtained by Iwata et al. referring to sparsity
matroids. As a byproduct, we obtain new lower bounds on the extension
complexity of the spanning forest polytope in terms of extension complexities
of independence polytopes of these matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02821</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02821</id><created>2015-02-10</created><authors><author><keyname>Doyle</keyname><forenames>Paul</forenames></author></authors><title>Building a scalable global data processing pipeline for large
  astronomical photometric datasets</title><categories>astro-ph.IM cs.DC</categories><comments>PhD Thesis, Dublin Institute of Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomical photometry is the science of measuring the flux of a celestial
object. Since its introduction, the CCD has been the principle method of
measuring flux to calculate the apparent magnitude of an object. Each CCD image
taken must go through a process of cleaning and calibration prior to its use.
As the number of research telescopes increases the overall computing resources
required for image processing also increases. Existing processing techniques
are primarily sequential in nature, requiring increasingly powerful servers,
faster disks and faster networks to process data. Existing High Performance
Computing solutions involving high capacity data centres are complex in design
and expensive to maintain, while providing resources primarily to high profile
science projects. This research describes three distributed pipeline
architectures, a virtualised cloud based IRAF, the Astronomical Compute Node
(ACN), a private cloud based pipeline, and NIMBUS, a globally distributed
system. The ACN pipeline processed data at a rate of 4 Terabytes per day
demonstrating data compression and upload to a central cloud storage service at
a rate faster than data generation. The primary contribution of this research
is NIMBUS, which is rapidly scalable, resilient to failure and capable of
processing CCD image data at a rate of hundreds of Terabytes per day. This
pipeline is implemented using a decentralised web queue to control the
compression of data, uploading of data to distributed web servers, and creating
web messages to identify the location of the data. Using distributed web queue
messages, images are downloaded by computing resources distributed around the
globe. Rigorous experimental evidence is presented verifying the horizontal
scalability of the system which has demonstrated a processing rate of 192
Terabytes per day with clear indications that higher processing rates are
possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02824</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02824</id><created>2015-02-10</created><authors><author><keyname>Nayak</keyname><forenames>Sukanta</forenames></author><author><keyname>Chakraverty</keyname><forenames>Snehashish</forenames></author></authors><title>Fuzzy finite element solution of uncertain neutron diffusion equation
  for imprecisely defined homogeneous triangular bare reactor</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scattering of neutron collision inside a reactor depends upon geometry of the
reactor, diffusion coefficient and absorption coefficient etc. In general these
parameters are not crisp and hence we may get uncertain neutron diffusion
equation. In this paper we have investigated the above problem for a bare
triangular homogeneous reactor. Here the uncertain governing differential
equation is modelled by a modified fuzzy finite element method using newly
proposed interval arithmetic. Obtained eigenvalues by the proposed method are
studied in detail. Further the eigenvalues are compared with the classical
finite element method in special cases and various uncertain results have been
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02834</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02834</id><created>2015-02-10</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames></author><author><keyname>Fellner</keyname><forenames>Andreas</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Counterexample Explanation by Learning Small Strategies in Markov
  Decision Processes</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While for deterministic systems, a counterexample to a property can simply be
an error trace, counterexamples in probabilistic systems are necessarily more
complex. For instance, a set of erroneous traces with a sufficient cumulative
probability mass can be used. Since these are too large objects to understand
and manipulate, compact representations such as subchains have been considered.
In the case of probabilistic systems with non-determinism, the situation is
even more complex. While a subchain for a given strategy (or scheduler,
resolving non-determinism) is a straightforward choice, we take a different
approach. Instead, we focus on the strategy - which can be a counterexample to
violation of or a witness of satisfaction of a property - itself, and extract
the most important decisions it makes, and present its succinct representation.
The key tools we employ to achieve this are (1) introducing a concept of
importance of a state w.r.t. the strategy, and (2) learning using decision
trees. There are three main consequent advantages of our approach. Firstly, it
exploits the quantitative information on states, stressing the more important
decisions. Secondly, it leads to a greater variability and degree of freedom in
representing the strategies. Thirdly, the representation uses a
self-explanatory data structure. In summary, our approach produces more
succinct and more explainable strategies, as opposed to e.g. binary decision
diagrams. Finally, our experimental results show that we can extract several
rules describing the strategy even for very large systems that do not fit in
memory, and based on the rules explain the erroneous behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02839</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02839</id><created>2015-02-10</created><updated>2015-10-02</updated><authors><author><keyname>Li</keyname><forenames>Lvzhou</forenames></author><author><keyname>Qiu</keyname><forenames>Daowen</forenames></author></authors><title>Lower bounds on the size of semi-quantum finite automata</title><categories>cs.FL quant-ph</categories><comments>arXiv admin note: text overlap with arXiv:1206.2131. To appear in
  Theoretical Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature, there exist several interesting hybrid models of finite
automata which have both quantum and classical states. We call them
semi-quantum automata. In this paper, we compare the descriptional power of
these models with that of DFA. Specifically, we present a uniform method that
gives a lower bound on the size of the three existing main models of
semi-quantum automata, and this bound shows that semi-quantum automata can be
at most exponentially more concise than DFA. Compared with a recent work
(Bianchi, Mereghetti, Palano, Theoret. Comput. Sci., 551(2014), 102-115), our
method shows the following two advantages: (i) our method is much more concise;
and (ii) our method is universal, since it is applicable to the three existing
main models of semi-quantum automata, instead of only a specific model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02840</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02840</id><created>2015-02-10</created><authors><author><keyname>Rodriguez-Mier</keyname><forenames>Pablo</forenames></author><author><keyname>Pedrinaci</keyname><forenames>Carlos</forenames></author><author><keyname>Lama</keyname><forenames>Manuel</forenames></author><author><keyname>Mucientes</keyname><forenames>Manuel</forenames></author></authors><title>An Integrated Semantic Web Service Discovery and Composition Framework</title><categories>cs.AI cs.SE</categories><comments>Accepted to appear in IEEE Transactions on Services Computing 2015</comments><doi>10.1109/TSC.2015.2402679</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a theoretical analysis of graph-based service
composition in terms of its dependency with service discovery. Driven by this
analysis we define a composition framework by means of integration with
fine-grained I/O service discovery that enables the generation of a graph-based
composition which contains the set of services that are semantically relevant
for an input-output request. The proposed framework also includes an optimal
composition search algorithm to extract the best composition from the graph
minimising the length and the number of services, and different graph
optimisations to improve the scalability of the system. A practical
implementation used for the empirical analysis is also provided. This analysis
proves the scalability and flexibility of our proposal and provides insights on
how integrated composition systems can be designed in order to achieve good
performance in real scenarios for the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02844</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02844</id><created>2015-02-10</created><authors><author><keyname>Matteini</keyname><forenames>Michele</forenames></author></authors><title>An overview on automatic design of robot controllers for complex tasks</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we will explore different available methodologies to
automatically design controllers for tasks that spans many level of
abstraction, where the gap between primitive behaviours and the task definition
is high. A good understanding of your evolutionary setup is needed to choose
the correct strategy with which to tackle complex tasks thus we'll first review
the most used types of each element composing an evolutionary setup
(controllers, objective functions, ect.) then we'll move the focus on the
bootstrapping problem and on the different strategies used to overcome it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02846</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02846</id><created>2015-02-10</created><updated>2016-01-18</updated><authors><author><keyname>Mahsereci</keyname><forenames>Maren</forenames></author><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author></authors><title>Probabilistic Line Searches for Stochastic Optimization</title><categories>cs.LG math.OC stat.ML</categories><comments>12 pages, including supplements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In deterministic optimization, line searches are a standard tool ensuring
stability and efficiency. Where only stochastic gradients are available, no
direct equivalent has so far been formulated, because uncertain gradients do
not allow for a strict sequence of decisions collapsing the search space. We
construct a probabilistic line search by combining the structure of existing
deterministic methods with notions from Bayesian optimization. Our method
retains a Gaussian process surrogate of the univariate optimization objective,
and uses a probabilistic belief over the Wolfe conditions to monitor the
descent. The algorithm has very low computational cost, and no user-controlled
parameters. Experiments show that it effectively removes the need to define a
learning rate for stochastic gradient descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02849</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02849</id><created>2015-02-10</created><updated>2015-02-12</updated><authors><author><keyname>Jakobsen</keyname><forenames>Sune K.</forenames></author></authors><title>A numbers-on-foreheads game</title><categories>cs.GT math.PR</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is there a joint distribution of $n$ random variables over the natural
numbers, such that they always form an increasing sequence and whenever you
take two subsets of the set of random variables of the same cardinality, their
distribution is almost the same? We show that the answer is yes, but that the
random variables will have to take values as large as $2^{2^{\dots
^{2^{\Theta\left(\frac{1}{\epsilon}\right)}}}}$, where $\epsilon\leq
\epsilon_n$ measures how different the two distributions can be, the tower
contains $n-2$ $2$'s and the constants in the $\Theta$ notation are allowed to
depend on $n$. This result has an important consequence in game theory: It
shows that even though you can define extensive form games that cannot be
implemented on players who can tell the time, you can have implementations that
approximate the game arbitrarily well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02850</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02850</id><created>2015-02-10</created><authors><author><keyname>Madueno</keyname><forenames>German Corrales</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Massive M2M Access with Reliability Guarantees in LTE Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation in ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-to-Machine (M2M) communications are one of the major drivers of the
cellular network evolution towards 5G systems. One of the key challenges is on
how to provide reliability guarantees to each accessing device in a situation
in which there is a massive number of almost-simultaneous arrivals from a large
set of M2M devices. The existing solutions take a reactive approach in dealing
with massive arrivals, such as non-selective barring when a massive arrival
event occurs, which implies that the devices cannot get individual reliability
guarantees. In this paper we propose a proactive approach, based on a standard
operation of the cellular access. The access procedure is divided into two
phases, an estimation phase and a serving phase. In the estimation phase the
number of arrivals is estimated and this information is used to tune the amount
of resources allocated in the serving phase. Our results show that the
proactive approach is instrumental in delivering high access reliability to the
M2M devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02851</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02851</id><created>2015-02-10</created><authors><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Andrieu</keyname><forenames>Vincent</forenames></author><author><keyname>Prieur</keyname><forenames>Christophe</forenames></author></authors><title>A Region-Dependent Gain Condition for Asymptotic Stability</title><categories>math.DS cs.SY math.OC</categories><journal-ref>Automatica, vol. 52, pp. 309-316, 2015</journal-ref><doi>10.1016/j.automatica.2014.12.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sufficient condition for the stability of a system resulting from the
interconnection of dynamical systems is given by the small gain theorem.
Roughly speaking, to apply this theorem, it is required that the gains
composition is continuous, increasing and upper bounded by the identity
function. In this work, an alternative sufficient condition is presented for
the case in which this criterion fails due to either lack of continuity or the
bound of the composed gain is larger than the identity function. More
precisely, the local (resp. non-local) asymptotic stability of the origin
(resp. global attractivity of a compact set) is ensured by a region-dependent
small gain condition. Under an additional condition that implies convergence of
solutions for almost all initial conditions in a suitable domain, the almost
global asymptotic stability of the origin is ensured. Two examples illustrate
and motivate this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02852</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02852</id><created>2015-02-10</created><authors><author><keyname>Gregorek</keyname><forenames>Daniel</forenames></author><author><keyname>Schmidt</keyname><forenames>Robert</forenames></author><author><keyname>Garcia-Ortiz</keyname><forenames>Alberto</forenames></author></authors><title>Transaction Level Analysis for a Clustered and Hardware-Enhanced Task
  Manager on Homogeneous Many-Core Systems</title><categories>cs.DC</categories><comments>Presented at HIP3ES, 2015 (arXiv: 1501.03064)</comments><report-no>HIP3ES/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing parallelism of many-core systems demands for efficient
strategies for the run-time system management. Due to the large number of cores
the management overhead has a rising impact to the overall system performance.
This work analyzes a clustered infrastructure of dedicated hardware nodes to
manage a homogeneous many-core system. The hardware nodes implement a message
passing protocol and perform the task mapping and synchronization at run-time.
To make meaningful mapping decisions, the global management nodes employ a
workload status communication mechanism.
  This paper discusses the design-space of the dedicated infrastructure by
means of task mapping use-cases and a parallel benchmark including
application-interference. We evaluate the architecture in terms of application
speedup and analyze the mechanism for the status communication. A comparison
versus centralized and fully-distributed configurations demonstrates the
reduction of the computation and communication management overhead for our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02860</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02860</id><created>2015-02-10</created><authors><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author><author><keyname>Fox</keyname><forenames>Dieter</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl Edward</forenames></author></authors><title>Gaussian Processes for Data-Efficient Learning in Robotics and Control</title><categories>stat.ML cs.LG cs.RO cs.SY</categories><comments>20 pages, 29 figures</comments><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 37, issue no 2, pages 408-423, February 2015</journal-ref><doi>10.1109/TPAMI.2013.218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous learning has been a promising direction in control and robotics
for more than a decade since data-driven learning allows to reduce the amount
of engineering knowledge, which is otherwise required. However, autonomous
reinforcement learning (RL) approaches typically require many interactions with
the system to learn controllers, which is a practical limitation in real
systems, such as robots, where many interactions can be impractical and time
consuming. To address this problem, current learning approaches typically
require task-specific knowledge in form of expert demonstrations, realistic
simulators, pre-shaped policies, or specific knowledge about the underlying
dynamics. In this article, we follow a different approach and speed up learning
by extracting more information from data. In particular, we learn a
probabilistic, non-parametric Gaussian process transition model of the system.
By explicitly incorporating model uncertainty into long-term planning and
controller learning our approach reduces the effects of model errors, a key
problem in model-based learning. Compared to state-of-the art RL our
model-based policy search method achieves an unprecedented speed of learning.
We demonstrate its applicability to autonomous learning in real robot and
control tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02868</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02868</id><created>2015-02-10</created><authors><author><keyname>Zohdy</keyname><forenames>Maha</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Nafie</keyname><forenames>Mohamed</forenames></author></authors><title>Maximum Throughput Opportunistic Network Coding in Two-Way Relay
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study Two-way relaying networks well-known for its
throughput merits. In particular, we study the fundamental throughput delay
trade-off in two-way relaying networks using opportunistic network coding. We
characterize the optimal opportunistic network coding policy that maximizes the
aggregate network throughput subject to an average packet delay constraint.
Towards this objective, first, we consider a pair of nodes communicating
through a common relay and develop a two dimensional Markov chain model
capturing the buffers' length states at the two nodes. Second, we formulate an
optimization problem for the delay constrained optimal throughput. Exploiting
the structure of the problem, it can be cast as a linear programming problem.
Third, we compare the optimal policy to two baseline schemes and show its
merits with respect to throughput, average delay and power consumption. First,
the optimal policy significantly outperforms the first baseline with respect to
throughput, delay and power consumption. Moreover, it outperforms the second
baseline with respect to the average delay and power consumption for
asymmetrical traffic arrival rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02871</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02871</id><created>2015-02-10</created><authors><author><keyname>Aboufadel</keyname><forenames>Edward</forenames></author><author><keyname>Krawczyk</keyname><forenames>Sylvanna V.</forenames></author><author><keyname>Sherman-Bennett</keyname><forenames>Melissa</forenames></author></authors><title>Talk to the Hand: Generating a 3D Print from Photographs</title><categories>math.HO cs.CV</categories><comments>12 pages, 13 figures, final report from the 2013 REU program at Grand
  Valley State University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript presents a linear algebra-based technique that only requires
two unique photographs from a digital camera to mathematically construct a 3D
surface representation which can then be 3D printed. Basic computer vision
theory and manufacturing principles are also briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02874</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02874</id><created>2015-02-10</created><authors><author><keyname>Yi</keyname><forenames>Jiawang</forenames></author><author><keyname>Tan</keyname><forenames>Guanzheng</forenames></author></authors><title>Invariance of the spark, NSP order and RIP order under elementary
  transformations of matrices</title><categories>cs.IT math.IT</categories><comments>Submitted for publication. 17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of compressed sensing tells us that recovering all k-sparse
signals requires a sensing matrix to satisfy that its spark is greater than 2k,
or its order of the null space property (NSP) or the restricted isometry
property (RIP) is 2k or above. If we perform elementary row or column
operations on the sensing matrix, what are the changes of its spark, NSP order
and RIP order? In this paper, we study this problem and discover that these
three quantitative indexes of sensing matrices all possess invariance under all
matrix elementary transformations except column-addition ones. Putting this
result in form of matrix products, we get the types of matrices which multiply
a sensing matrix and make the products still have the same properties of sparse
recovery as the sensing matrix. According to these types of matrices, we made
an interesting discovery that sensing matrices with deterministic constructions
do not possess the property universality which belongs to sensing matrices with
random constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02887</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02887</id><created>2015-02-10</created><authors><author><keyname>Rey</keyname><forenames>Valentine</forenames></author><author><keyname>Gosselet</keyname><forenames>Pierre</forenames></author><author><keyname>Rey</keyname><forenames>Christian</forenames></author></authors><title>Strict bounding of quantities of interest in computations based on
  domain decomposition</title><categories>physics.comp-ph cs.NA math.NA</categories><comments>Computer Methods in Applied Mechanics and Engineering, Elsevier,
  2015, online preview</comments><proxy>ccsd</proxy><doi>10.1016/j.cma.2015.01.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with bounding the error on the estimation of quantities of
interest obtained by finite element and domain decomposition methods. The
proposed bounds are written in order to separate the two errors involved in the
resolution of reference and adjoint problems : on the one hand the
discretization error due to the finite element method and on the other hand the
algebraic error due to the use of the iterative solver. Beside practical
considerations on the parallel computation of the bounds, it is shown that the
interface conformity can be slightly relaxed so that local enrichment or
refinement are possible in the subdomains bearing singularities or quantities
of interest which simplifies the improvement of the estimation. Academic
assessments are given on 2D static linear mechanic problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02893</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02893</id><created>2015-02-10</created><updated>2015-10-26</updated><authors><author><keyname>Shifrin</keyname><forenames>Mark</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>Gurewitz</keyname><forenames>Omer</forenames></author><author><keyname>Weisman</keyname><forenames>Olga</forenames></author></authors><title>Coded Retransmission in Wireless Networks Via Abstract MDPs: Theory and
  Algorithms</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a transmission scheme with a single transmitter and multiple
receivers over a faulty broadcast channel. For each receiver, the transmitter
has a unique infinite stream of packets, and its goal is to deliver them at the
highest throughput possible. While such multiple-unicast models are unsolved in
general, several network coding based schemes were suggested. In such schemes,
the transmitter can either send an uncoded packet, or a coded packet which is a
function of a few packets. The packets sent can be received by the designated
receiver (with some probability) or heard and stored by other receivers. Two
functional modes are considered; the first presumes that the storage time is
unlimited, while in the second it is limited by a given Time to Expire (TTE)
parameter. We model the transmission process as an infinite-horizon Markov
Decision Process (MDP). Since the large state space renders exact solutions
computationally impractical, we introduce policy restricted and induced MDPs
with significantly reduced state space, and prove that with proper reward
function they have equal optimal value function (hence equal optimal
throughput). We then derive a reinforcement learning algorithm, which learns
the optimal policy for the induced MDP. This optimal strategy of the induced
MDP, once applied to the policy restricted one, significantly improves over
uncoded schemes. Next, we enhance the algorithm by means of analysis of the
structural properties of the resulting reward functional. We demonstrate that
our method scales well in the number of users, and automatically adapts to the
packet loss rates, unknown in advance. In addition, the performance is compared
to the recent bound by Wang, which assumes much stronger coding (e.g.,
intra-session and buffering of coded packets), yet is shown to be comparable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02899</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02899</id><created>2015-02-10</created><authors><author><keyname>Brand&#xe3;o</keyname><forenames>Filipe</forenames></author><author><keyname>Pedroso</keyname><forenames>Jo&#xe3;o Pedro</forenames></author></authors><title>Cutting Stock with Binary Patterns: Arc-flow Formulation with Graph
  Compression</title><categories>math.OC cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1310.6887</comments><report-no>DCC-2013-09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cutting stock problem with binary patterns (0-1 CSP) is a variant of CSP
that usually appears as a relaxation of 2D and 3D packing problems. We present
an exact method, based on an arc-flow formulation with side constraints, for
solving 0-1 CSP by simply representing all the patterns in a very compact
graph.
  Gilmore-Gomory's column generation approach is usually used to compute strong
lower bounds for 0-1 CSP. We report a computational comparison between the
arc-flow approach and the Gilmore-Gomory's approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02905</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02905</id><created>2015-02-10</created><authors><author><keyname>Supe</keyname><forenames>Chaitannya</forenames></author></authors><title>Real Time Implementation of Spatial Filtering On FPGA</title><categories>cs.CV</categories><comments>8 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Field Programmable Gate Array (FPGA) technology has gained vital importance
mainly because of its parallel processing hardware which makes it ideal for
image and video processing. In this paper, a step by step approach to apply a
linear spatial filter on real time video frame sent by Omnivision OV7670 camera
using Zynq Evaluation and Development board based on Xilinx XC7Z020 has been
discussed. Face detection application was chosen to explain above procedure.
This procedure is applicable to most of the complex image processing algorithms
which needs to be implemented using FPGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02908</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02908</id><created>2015-02-10</created><updated>2016-01-27</updated><authors><author><keyname>Bauer</keyname><forenames>Pavol</forenames></author><author><keyname>Engblom</keyname><forenames>Stefan</forenames></author><author><keyname>Widgren</keyname><forenames>Stefan</forenames></author></authors><title>Fast event-based epidemiological simulations on national scales</title><categories>q-bio.PE cs.DC</categories><comments>27 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a computational modeling framework for data-driven simulations and
analysis of infectious disease spread in large populations. For the purpose of
efficient simulations, we devise a parallel solution algorithm targeting
multi-socket shared memory architectures. The model integrates infectious
dynamics as continuous-time Markov chains and available data such as animal
movements or aging are incorporated as externally defined events. To bring out
parallelism and accelerate the computations, we decompose the spatial domain
and optimize cross-boundary communication using dependency-aware task
scheduling. Using registered livestock data at a high spatio-temporal
resolution, we demonstrate that our approach not only is resilient to varying
model configurations, but also scales on all physical cores at realistic work
loads. Finally, we show that these very features enable the solution of inverse
problems on national scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02910</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02910</id><created>2015-02-10</created><authors><author><keyname>Caltais</keyname><forenames>Georgiana</forenames></author></authors><title>Coalgebraic Tools for Bisimilarity and Decorated Trace Semantics</title><categories>cs.LO cs.FL</categories><comments>thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modelling, specification and study of the semantics of concurrent
reactive systems have been interesting research topics for many years now. The
aim of this thesis is to exploit the strengths of the (co)algebraic framework
in modelling reactive systems and reasoning on several types of associated
semantics, in a uniform fashion. In particular, we are interested in handling
notions of behavioural equivalence/preorder ranging from bisimilarity for
systems that can be represented as non-deterministic coalgebras, to decorated
trace semantics for labelled transition systems and probabilistic systems, and
testing semantics for labelled transition systems with internal behaviour.
Moreover, we aim at deriving a suite of corresponding verification algorithms
suitable for implementation in automated tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02921</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02921</id><created>2015-02-10</created><updated>2015-06-11</updated><authors><author><keyname>Saa-Garriga</keyname><forenames>Albert</forenames></author><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Carrabina</keyname><forenames>Jordi</forenames></author></authors><title>OMP2MPI: Automatic MPI code generation from OpenMP programs</title><categories>cs.DC cs.PF cs.PL</categories><comments>Presented at HIP3ES, 2015 (arXiv: 1501.03064)</comments><report-no>HIP3ES/2015/06</report-no><acm-class>D.3.2; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present OMP2MPI a tool that generates automatically MPI
source code from OpenMP. With this transformation the original program can be
adapted to be able to exploit a larger number of processors by surpassing the
limits of the node level on large HPC clusters. The transformation can also be
useful to adapt the source code to execute in distributed memory many-cores
with message passing support. In addition, the resulting MPI code can be used
as an starting point that still can be further optimized by software engineers.
The transformation process is focused on detecting OpenMP parallel loops and
distributing them in a master/worker pattern. A set of micro-benchmarks have
been used to verify the correctness of the the transformation and to measure
the resulting performance. Surprisingly not only the automatically generated
code is correct by construction, but also it often performs faster even when
executed with MPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02925</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02925</id><created>2015-02-10</created><authors><author><keyname>Goldin</keyname><forenames>Dina</forenames></author><author><keyname>Burshtein</keyname><forenames>David</forenames></author></authors><title>On the Finite Length Scaling of Ternary Polar Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polarization process of polar codes over a ternary alphabet is studied.
Recently it has been shown that the scaling of the blocklength of polar codes
with prime alphabet size scales polynomially with respect to the inverse of the
gap between code rate and channel capacity. However, except for the binary
case, the degree of the polynomial in the bound is extremely large. In this
work, it is shown that a much lower degree polynomial can be computed
numerically for the ternary case. Similar results are conjectured for the
general case of prime alphabet size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02927</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02927</id><created>2015-02-10</created><updated>2015-12-24</updated><authors><author><keyname>Caruso</keyname><forenames>Fabrizio</forenames></author><author><keyname>Orsini</keyname><forenames>Emmanuela</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author><author><keyname>Tinnirello</keyname><forenames>Claudia</forenames></author></authors><title>On the shape of the general error locator polynomial for cyclic codes</title><categories>cs.IT math.IT</categories><comments>33 pages, 12 tables, Submitted to IEEE Transactions on Information
  Theory in Feb. 2015, Revised version submitted in Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general result on the explicit form of the general error locator polynomial
for all cyclic codes is given, along with several results for infinite classes
of cyclic codes with $t=2$ and $t=3$. From these, a theoretically justification
of the sparsity of the general error locator polynomial is obtained for all
cyclic codes with $t\leq 3$ and $n&lt;63$, except for three cases where the
sparsity is proved by a computer check. Moreover, we discuss some consequences
of our results to the understanding of the complexity of bounded-distance
decoding of cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02940</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02940</id><created>2015-02-10</created><authors><author><keyname>Bayramoglu</keyname><forenames>Muhammet Fatih</forenames></author></authors><title>The Hilbert Space of Probability Mass Functions and Applications on
  Probabilistic Inference</title><categories>cs.IT math.IT math.PR</categories><comments>PhD Dissertation, 123 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hilbert space of probability mass functions (pmf) is introduced in this
thesis. A factorization method for multivariate pmfs is proposed by using the
tools provided by the Hilbert space of pmfs. The resulting factorization is
special for two reasons. First, it reveals the algebraic relations between the
involved random variables. Second, it determines the conditional independence
relations between the random variables. Due to the first property of the
resulting factorization, it can be shown that channel decoders can be employed
in the solution of probabilistic inference problems other than decoding. This
approach might lead to new probabilistic inference algorithms and new hardware
options for the implementation of these algorithms. An example of new inference
algorithms inspired by the idea of using channel decoder for other inference
tasks is a multiple-input multiple-output (MIMO) detection algorithm which has
a complexity of the square-root of the optimum MIMO detection algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02942</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02942</id><created>2015-02-10</created><authors><author><keyname>Jain</keyname><forenames>Mitesh</forenames></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author></authors><title>Skipping Refinement</title><categories>cs.LO cs.PL</categories><comments>Submitted to CAV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce skipping refinement, a new notion of correctness for reasoning
about optimized reactive systems. Reasoning about reactive systems using
refinement involves defining an abstract, high-level specification system and a
concrete, low-level implementation system. One then shows that every behavior
allowed by the implementation is also allowed by the specification. Due to the
difference in abstraction levels, it is often the case that the implementation
requires many steps to match one step of the specification, hence, it is quite
useful for refinement to directly account for stuttering. Some optimized
implementations, however, can actually take multiple specification steps at
once. For example, a memory controller can buffer the commands to the memory
and at a later time simultaneously update multiple memory locations, thereby
skipping several observable states of the abstract specification, which only
updates one memory location at a time. We introduce skipping simulation
refinement and provide a sound and complete characterization consisting of
&quot;local&quot; proof rules that are amenable to mechanization and automated
verification. We present case studies that highlight the applicability of
skipping refinement: a JVM-inspired stack machine, a simple memory controller
and a scalar to vector compiler transformation. Our experimental results
demonstrate that current model-checking and automated theorem proving tools
have difficultly automatically analyzing these systems using existing notions
of correctness, but they can analyze the systems if we use skipping refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02943</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02943</id><created>2015-02-10</created><authors><author><keyname>Miller</keyname><forenames>Konstantin</forenames></author><author><keyname>Bethanabhotla</keyname><forenames>Dilip</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Wolisz</keyname><forenames>Adam</forenames></author></authors><title>A Control-Theoretic Approach to Adaptive Video Streaming in Dense
  Wireless Networks</title><categories>cs.NI cs.MM</categories><comments>Submitted</comments><acm-class>C.2.1; C.2.4; I.2.8</acm-class><journal-ref>IEEE Transactions on Multimedia, 2015, vol. 17, no. 8, pp. 1309 -
  1322</journal-ref><doi>10.1109/TMM.2015.2441002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the way people consume video content has been undergoing a dramatic
change. Plain TV sets, that have been the center of home entertainment for a
long time, are losing grounds to Hybrid TV's, PC's, game consoles, and, more
recently, mobile devices such as tablets and smartphones. The new predominant
paradigm is: watch what I want, when I want, and where I want.
  The challenges of this shift are manifold. On the one hand, broadcast
technologies such as DVB-T/C/S need to be extended or replaced by mechanisms
supporting asynchronous viewing, such as IPTV and video streaming over
best-effort networks, while remaining scalable to millions of users. On the
other hand, the dramatic increase of wireless data traffic begins to stretch
the capabilities of the existing wireless infrastructure to its limits.
Finally, there is a challenge to video streaming technologies to cope with a
high heterogeneity of end-user devices and dynamically changing network
conditions, in particular in wireless and mobile networks.
  In the present work, our goal is to design an efficient system that supports
a high number of unicast streaming sessions in a dense wireless access network.
We address this goal by jointly considering the two problems of wireless
transmission scheduling and video quality adaptation, using techniques inspired
by the robustness and simplicity of Proportional-Integral-Derivative (PID)
controllers. We show that the control-theoretic approach allows to efficiently
utilize available wireless resources, providing high Quality of Experience
(QoE) to a large number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02960</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02960</id><created>2015-02-10</created><authors><author><keyname>Colledanchise</keyname><forenames>Michele</forenames></author><author><keyname>Marzinotto</keyname><forenames>Alejandro</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>&#xd6;gren</keyname><forenames>Petter</forenames></author></authors><title>Adaptive Fault Tolerant Execution of Multi-Robot Missions using Behavior
  Trees</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-robot teams offer possibilities of improved performance and fault
tolerance, compared to single robot solutions. In this paper, we show how to
realize those possibilities when starting from a single robot system controlled
by a Behavior Tree (BT). By extending the single robot BT to a multi-robot BT,
we are able to combine the fault tolerant properties of the BT, in terms of
built-in fallbacks, with the fault tolerance inherent in multi-robot
approaches, in terms of a faulty robot being replaced by another one.
Furthermore, we improve performance by identifying and taking advantage of the
opportunities of parallel task execution, that are present in the single robot
BT. Analyzing the proposed approach, we present results regarding how mission
performance is affected by minor faults (a robot losing one capability) as well
as major faults (a robot losing all its capabilities). Finally, a detailed
example is provided to illustrate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02961</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02961</id><created>2015-02-10</created><authors><author><keyname>Kennaway</keyname><forenames>Richard</forenames></author></authors><title>Avatar-independent scripting for real-time gesture animation</title><categories>cs.GR</categories><comments>23 pages, 12 figures. Last revised November 2006</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When animation of a humanoid figure is to be generated at run-time, instead
of by replaying pre-composed motion clips, some method is required of
specifying the avatar's movements in a form from which the required motion data
can be automatically generated. This form must be of a more abstract nature
than raw motion data: ideally, it should be independent of the particular
avatar's proportions, and both writable by hand and suitable for automatic
generation from higher-level descriptions of the required actions.
  We describe here the development and implementation of such a scripting
language for the particular area of sign languages of the deaf, called SiGML
(Signing Gesture Markup Language), based on the existing HamNoSys notation for
sign languages.
  We conclude by suggesting how this work may be extended to more general
animation for interactive virtual reality applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02965</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02965</id><created>2015-02-10</created><authors><author><keyname>Han</keyname><forenames>Zhi</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Video Primal Sketch: A Unified Middle-Level Representation for Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a middle-level video representation named Video Primal
Sketch (VPS), which integrates two regimes of models: i) sparse coding model
using static or moving primitives to explicitly represent moving corners,
lines, feature points, etc., ii) FRAME /MRF model reproducing feature
statistics extracted from input video to implicitly represent textured motion,
such as water and fire. The feature statistics include histograms of
spatio-temporal filters and velocity distributions. This paper makes three
contributions to the literature: i) Learning a dictionary of video primitives
using parametric generative models; ii) Proposing the Spatio-Temporal FRAME
(ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling and
synthesizing textured motion; and iii) Developing a parsimonious hybrid model
for generic video representation. Given an input video, VPS selects the proper
models automatically for different motion patterns and is compatible with
high-level action representations. In the experiments, we synthesize a number
of textured motion; reconstruct real videos using the VPS; report a series of
human perception experiments to verify the quality of reconstructed videos;
demonstrate how the VPS changes over the scale transition in videos; and
present the close connection between VPS and high-level action models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02969</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02969</id><created>2015-02-10</created><authors><author><keyname>Ji</keyname><forenames>Ke</forenames></author><author><keyname>Lin</keyname><forenames>Jianbiao</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Ao</forenames></author><author><keyname>Tang</keyname><forenames>Tianjing</forenames></author></authors><title>A DCT And SVD based Watermarking Technique To Identify Tag</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of the multimedia,the secure of the multimedia is
get more concerned. as far as we know , Digital watermarking is an effective
way to protect copyright. The watermark must be generally hidden does not
affect the quality of the original image. In this paper,a novel way based on
discrete cosine transform(DCT) and singular value decomposition(SVD) .In the
proposed way,we decomposition the image into 8*8 blocks, next we use the DCT to
get the transformed block,then we choose the diagonal to embed the information,
after we do this, we recover the image and then we decomposition the image to
8*8 blocks,we use the SVD way to get the diagonal matrix and embed the
information in the matrix. next we extract the information use both inverse of
DCT and SVD, as we all know,after we embed the information seconded time , the
information we first information we embed must be changed, we choose a measure
way called Peak Signal to Noise Ratio(PSNR) to estimate the similarity of the
two image, and set a threshold to ensure whether the information is same or
not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02971</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02971</id><created>2015-02-10</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Schneider</keyname><forenames>Jon</forenames></author></authors><title>Information complexity is computable</title><categories>cs.IT math.IT</categories><comments>30 pages</comments><msc-class>94A17, 68Q05</msc-class><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information complexity of a function $f$ is the minimum amount of
information Alice and Bob need to exchange to compute the function $f$. In this
paper we provide an algorithm for approximating the information complexity of
an arbitrary function $f$ to within any additive error $\alpha &gt; 0$, thus
resolving an open question as to whether information complexity is computable.
  In the process, we give the first explicit upper bound on the rate of
convergence of the information complexity of $f$ when restricted to $b$-bit
protocols to the (unrestricted) information complexity of $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02973</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02973</id><created>2015-02-10</created><authors><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Wang</keyname><forenames>Mengdi</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>A Distributed Tracking Algorithm for Reconstruction of Graph Signals</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures, 2 tables, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development of signal processing on graphs provides a new
perspective for processing large-scale data associated with irregular domains.
In many practical applications, it is necessary to handle massive data sets
through complex networks, in which most nodes have limited computing power.
Designing efficient distributed algorithms is critical for this task. This
paper focuses on the distributed reconstruction of a time-varying bandlimited
graph signal based on observations sampled at a subset of selected nodes. A
distributed least square reconstruction (DLSR) algorithm is proposed to recover
the unknown signal iteratively, by allowing neighboring nodes to communicate
with one another and make fast updates. DLSR uses a decay scheme to annihilate
the out-of-band energy occurring in the reconstruction process, which is
inevitably caused by the transmission delay in distributed systems. Proof of
convergence and error bounds for DLSR are provided in this paper, suggesting
that the algorithm is able to track time-varying graph signals and perfectly
reconstruct time-invariant signals. The DLSR algorithm is numerically
experimented with synthetic data and real-world sensor network data, which
verifies its ability in tracking slowly time-varying graph signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02987</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02987</id><created>2015-02-10</created><updated>2016-01-15</updated><authors><author><keyname>Duan</keyname><forenames>Runyao</forenames></author><author><keyname>Severini</keyname><forenames>Simone</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>On zero-error communication via quantum channels in the presence of
  noiseless feedback</title><categories>quant-ph cs.IT math.CO math.IT</categories><comments>34 pages, 1 figure; v2 has improved presentation, numerous typos
  corrected and many more references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of zero-error communication via quantum channels when
the receiver and sender have at their disposal a noiseless feedback channel of
unlimited quantum capacity, generalizing Shannon's zero-error communication
theory with instantaneous feedback.
  We first show that this capacity is a function only of the linear span of
Choi-Kraus operators of the channel, which generalizes the bipartite
equivocation graph of a classical channel, and which we dub &quot;non-commutative
bipartite graph&quot;. Then we go on to show that the feedback-assisted capacity is
non-zero (with constant activating noiseless communication) if and only if the
non-commutative bipartite graph is non-trivial, and give a number of equivalent
characterizations. This result involves a far-reaching extension of the
&quot;conclusive exclusion&quot; of quantum states [Pusey/Barrett/Rudolph, Nature Phys.
8:475-478].
  We then present an upper bound on the feedback-assisted zero-error capacity,
motivated by a conjecture originally made by Shannon and proved later by
Ahlswede. We demonstrate this bound to have many good properties, including
being additive and given by a minimax formula. We also prove that this quantity
is the entanglement-assisted capacity against an adversarially chosen channel
from the set of all channels with the same Choi-Kraus span, which can also be
interpreted as the feedback-assisted unambiguous capacity. The proof relies on
a generalization of the &quot;Postselection Lemma&quot; [Christandl/Koenig/Renner, PRL
102:020504] that allows to reflect additional constraints, and which we believe
to be of independent interest.
  We illustrate our ideas with a number of examples, including
classical-quantum channels and Weyl diagonal channels, and close with an
extensive discussion of open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.02991</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.02991</id><created>2015-02-10</created><authors><author><keyname>Amram</keyname><forenames>Gal</forenames></author><author><keyname>Mizrahi</keyname><forenames>Lior</forenames></author><author><keyname>Weiss</keyname><forenames>Gera</forenames></author></authors><title>Simple Executions of Snapshot Implementations</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well known snapshot primitive in concurrent programming allows for
n-asynchronous processes to write values to an array of single-writer registers
and, for each process, to take a snapshot of these registers. In this paper we
provide a formulation of the well known linearizability condition for snapshot
algorithms in terms of the existence of certain mathematical functions. In
addition, we identify a simplifying property of snapshot implementations we
call &quot;schedule-based algorithms&quot;. This property is natural to assume in the
sense that as far as we know, every published snapshot algorithm is
schedule-based. Based on this, we prove that when dealing with schedule-based
algorithms, it suffices to consider only a small class of very simple
executions to prove or disprove correctness in terms of linearizability. We
believe that the ideas developed in this paper may help to design automatic
verification of snapshot algorithms. Since verifying linearizability was
recently proved to be EXPSPACE-complete, focusing on unique objects (snapshot
in our case) can potentially lead to designing restricted, but feasible
verification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03005</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03005</id><created>2015-02-10</created><authors><author><keyname>Gacek</keyname><forenames>Andrew</forenames></author><author><keyname>Katis</keyname><forenames>Andreas</forenames></author><author><keyname>Whalen</keyname><forenames>Michael W.</forenames></author><author><keyname>Backes</keyname><forenames>John</forenames></author><author><keyname>Cofer</keyname><forenames>Darren</forenames></author></authors><title>Towards Realizability Checking of Contracts using Theories</title><categories>cs.SE</categories><comments>15 pages, to appear in NASA Formal Methods (NFM) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual integration techniques focus on building architectural models of
systems that can be analyzed early in the design cycle to try to lower cost,
reduce risk, and improve quality of complex embedded systems. Given appropriate
architectural descriptions and compositional reasoning rules, these techniques
can be used to prove important safety properties about the architecture prior
to system construction. Such proofs build from &quot;leaf-level&quot; assume/guarantee
component contracts through architectural layers towards top-level safety
properties. The proofs are built upon the premise that each leaf-level
component contract is realizable; i.e., it is possible to construct a component
such that for any input allowed by the contract assumptions, there is some
output value that the component can produce that satisfies the contract
guarantees. Without engineering support it is all too easy to write leaf-level
components that can't be realized. Realizability checking for propositional
contracts has been well-studied for many years, both for component synthesis
and checking correctness of temporal logic requirements. However, checking
realizability for contracts involving infinite theories is still an open
problem. In this paper, we describe a new approach for checking realizability
of contracts involving theories and demonstrate its usefulness on several
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03032</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03032</id><created>2015-02-10</created><updated>2015-07-27</updated><authors><author><keyname>Yang</keyname><forenames>Jiyan</forenames></author><author><keyname>Meng</keyname><forenames>Xiangrui</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Implementing Randomized Matrix Algorithms in Parallel and Distributed
  Environments</title><categories>cs.DC cs.DS math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this era of large-scale data, distributed systems built on top of clusters
of commodity hardware provide cheap and reliable storage and scalable
processing of massive data. Here, we review recent work on developing and
implementing randomized matrix algorithms in large-scale parallel and
distributed environments. Randomized algorithms for matrix problems have
received a great deal of attention in recent years, thus far typically either
in theory or in machine learning applications or with implementations on a
single machine. Our main focus is on the underlying theory and practical
implementation of random projection and random sampling algorithms for very
large very overdetermined (i.e., overconstrained) $\ell_1$ and $\ell_2$
regression problems. Randomization can be used in one of two related ways:
either to construct sub-sampled problems that can be solved, exactly or
approximately, with traditional numerical methods; or to construct
preconditioned versions of the original full problem that are easier to solve
with traditional iterative algorithms. Theoretical results demonstrate that in
near input-sparsity time and with only a few passes through the data one can
obtain very strong relative-error approximate solutions, with high probability.
Empirical results highlight the importance of various trade-offs (e.g., between
the time to construct an embedding and the conditioning quality of the
embedding, between the relative importance of computation versus communication,
etc.) and demonstrate that $\ell_1$ and $\ell_2$ regression problems can be
solved to low, medium, or high precision in existing distributed systems on up
to terabyte-sized data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03038</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03038</id><created>2015-02-10</created><authors><author><keyname>Aly</keyname><forenames>Heba</forenames></author><author><keyname>Basalamah</keyname><forenames>Anas</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>LaneQuest: An Accurate and Energy-Efficient Lane Detection System</title><categories>cs.CY</categories><comments>Accepted for publication in the 13th IEEE International Conference on
  Pervasive Computing and Communications (IEEE PerCom 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current outdoor localization techniques fail to provide the required accuracy
for estimating the car's lane. In this paper, we present LaneQuest: a system
that leverages the ubiquitous and low-energy inertial sensors available in
commodity smart-phones to provide an accurate estimate of the car's current
lane. LaneQuest leverages hints from the phone sensors about the surrounding
environment to detect the car's lane. For example, a car making a right turn
most probably will be in the right-most lane, a car passing by a pothole will
be in a specific lane, and the car's angular velocity when driving through a
curve reflects its lane. Our investigation shows that there are amble
opportunities in the environment, i.e. lane &quot;anchors&quot;, that provide cues about
the car's lane. To handle the ambiguous location, sensors noise, and fuzzy lane
anchors; LaneQuest employs a novel probabilistic lane estimation algorithm.
Furthermore, it uses an unsupervised crowd-sourcing approach to learn the
position and lane-span distribution of the different lane-level anchors.
  Our evaluation results from implementation on different android devices and
260Km driving traces by 13 drivers in different cities shows that LaneQuest can
detect the different lane-level anchors with an average precision and recall of
more than 90%. This leads to an accurate detection of the exact car's lane
position 80% of the time, increasing to 89% of the time to within one lane.
This comes with a low-energy footprint, allowing LaneQuest to be implemented on
the energy-constrained mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03044</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03044</id><created>2015-02-10</created><updated>2015-02-10</updated><authors><author><keyname>Xu</keyname><forenames>Kelvin</forenames></author><author><keyname>Ba</keyname><forenames>Jimmy</forenames></author><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Show, Attend and Tell: Neural Image Caption Generation with Visual
  Attention</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by recent work in machine translation and object detection, we
introduce an attention based model that automatically learns to describe the
content of images. We describe how we can train this model in a deterministic
manner using standard backpropagation techniques and stochastically by
maximizing a variational lower bound. We also show through visualization how
the model is able to automatically learn to fix its gaze on salient objects
while generating the corresponding words in the output sequence. We validate
the use of attention with state-of-the-art performance on three benchmark
datasets: Flickr8k, Flickr30k and MS COCO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03049</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03049</id><created>2015-02-10</created><updated>2015-04-23</updated><authors><author><keyname>Le</keyname><forenames>Can M.</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>Sparse random graphs: regularization and concentration of the Laplacian</title><categories>math.ST cs.SI math.PR stat.TH</categories><comments>Added references</comments><msc-class>05C80, 05C85, 60B20, 62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study random graphs with possibly different edge probabilities in the
challenging sparse regime of bounded expected degrees. Unlike in the dense
case, neither the graph adjacency matrix nor its Laplacian concentrate around
their expectations due to the highly irregular distribution of node degrees. It
has been empirically observed that simply adding a constant of order $1/n$ to
each entry of the adjacency matrix substantially improves the behavior of
Laplacian. Here we prove that this regularization indeed forces Laplacian to
concentrate even in sparse graphs. As an immediate consequence in network
analysis, we establish the validity of one of the simplest and fastest
approaches to community detection -- regularized spectral clustering, under the
stochastic block model. Our proof of concentration of regularized Laplacian is
based on Grothendieck's inequality and factorization, combined with paving
arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03057</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03057</id><created>2015-02-04</created><authors><author><keyname>Darvishi</keyname><forenames>Mostafa</forenames></author><author><keyname>Audet</keyname><forenames>Yves</forenames></author><author><keyname>Blaqui&#xe8;re</keyname><forenames>Yves</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author></authors><title>Circuit Level Modeling of Extra Combinational Delays in SRAM FPGAs Due
  to Transient Ionizing Radiation</title><categories>physics.space-ph cs.AR</categories><comments>2014 IEEE Nuclear and Space Radiation Effects Conference (NSREC),
  July 14-18, Paris, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel circuit level model that explains and confirms
the extra combinational delays in a SRAM-FPGA (Virtex-5) due to radiation,
which matches the experimental results by proton irradiation at TRIUMF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03068</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03068</id><created>2015-02-10</created><authors><author><keyname>Weerakkody</keyname><forenames>Sean</forenames></author><author><keyname>Mo</keyname><forenames>Yilin</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author><author><keyname>Han</keyname><forenames>Duo</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Multi-Sensor Scheduling for State Estimation with Event-Based,
  Stochastic Triggers</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In networked systems, state estimation is hampered by communication limits.
Past approaches, which consider scheduling sensors through deterministic
event-triggers, reduce communication and maintain estimation quality. However,
these approaches destroy the Gaussian property of the state, making it
computationally intractable to obtain an exact minimum mean squared error
estimate. We propose a stochastic event-triggered sensor schedule for state
estimation which preserves the Gaussianity of the system, extending previous
results from the single-sensor to the multi-sensor case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03086</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03086</id><created>2015-02-10</created><authors><author><keyname>Klein</keyname><forenames>Maximilian</forenames></author><author><keyname>Konieczny</keyname><forenames>Piotr</forenames></author></authors><title>Gender Gap Through Time and Space: A Journey Through Wikipedia
  Biographies and the &quot;WIGI&quot; Index</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this study we investigate how quantification of Wikipedia biographies can
shed light on worldwide longitudinal gender inequality trends. We present an
academic index allowing comparative study of gender inequality through space
and time, the Wikipedia Gender Index (WIGI), based on metadata available
through the Wikidata database. Our research confirms that gender inequality is
a phenomenon with a long history, but whose patterns can be analyzed and
quantified on a larger scale than previously thought possible. Through the use
of Inglehart- Welzel cultural clusters, we show that gender inequality can be
analyzed with regards to world's cultures. In the dimension studied (coverage
of females and other genders in reference works) we show a steadily improving
trend, through one with aspects that deserve careful follow up analysis (such
as the surprisingly high ranking of the Confucian and South Asian clusters).
  Keywords: data mining, Wikidata, Wikipedia, gender gap, demographics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03097</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03097</id><created>2015-02-10</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Barbosa</keyname><forenames>Rui Soares</forenames></author><author><keyname>Kishida</keyname><forenames>Kohei</forenames></author><author><keyname>Lal</keyname><forenames>Raymond</forenames></author><author><keyname>Mansfield</keyname><forenames>Shane</forenames></author></authors><title>Contextuality, Cohomology and Paradox</title><categories>quant-ph cs.LO math.AT</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextuality is a key feature of quantum mechanics that provides an
important non-classical resource for quantum information and computation.
Abramsky and Brandenburger used sheaf theory to give a general treatment of
contextuality in quantum theory [New Journal of Physics 13 (2011) 113036].
However, contextual phenomena are found in other fields as well, for example
database theory. In this paper, we shall develop this unified view of
contextuality. We provide two main contributions: firstly, we expose a
remarkable connection between contexuality and logical paradoxes; secondly, we
show that an important class of contextuality arguments has a topological
origin. More specifically, we show that &quot;All-vs-Nothing&quot; proofs of
contextuality are witnessed by cohomological obstructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03103</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03103</id><created>2015-02-10</created><authors><author><keyname>Shulman</keyname><forenames>Michael</forenames></author><author><keyname>Warner</keyname><forenames>Marc</forenames></author></authors><title>Of Matters Condensed</title><categories>physics.soc-ph cs.DL</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The American Physical Society (APS) March Meeting of condensed matter physics
has grown to nearly 10,000 participants, comprises 23 individual APS groups,
and even warrants its own hashtag (#apsmarch). Here we analyze the text and
data from March Meeting abstracts of the past nine years and discuss trends in
condensed matter physics over this time period. We find that in comparison to
atomic, molecular, and optical physics, condensed matter changes rapidly, and
that condensed matter appears to be moving increasingly toward subject matter
that is traditionally in materials science and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03121</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03121</id><created>2015-02-10</created><authors><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation</title><categories>cs.CV</categories><doi>10.1109/TIP.2015.2458572</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a fast multi-band image fusion algorithm, which combines
a high-spatial low-spectral resolution image and a low-spatial high-spectral
resolution image. The well admitted forward model is explored to form the
likelihoods of the observations. Maximizing the likelihoods leads to solving a
Sylvester equation. By exploiting the properties of the circulant and
downsampling matrices associated with the fusion problem, a closed-form
solution for the corresponding Sylvester equation is obtained explicitly,
getting rid of any iterative update step. Coupled with the alternating
direction method of multipliers and the block coordinate descent method, the
proposed algorithm can be easily generalized to incorporate prior information
for the fusion problem, allowing a Bayesian estimator. Simulation results show
that the proposed algorithm achieves the same performance as existing
algorithms with the advantage of significantly decreasing the computational
complexity of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03124</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03124</id><created>2015-02-10</created><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Tulino</keyname><forenames>Antonia M.</forenames></author><author><keyname>Llorca</keyname><forenames>Jaime</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Order-Optimal Rate of Caching and Coded Multicasting with Random Demands</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the canonical {\em shared link network} formed by a source node,
hosting a library of $m$ information messages (files), connected via a
noiseless common link to $n$ destination nodes (users), each with a cache of
size M files. Users request files at random and independently, according to a
given a-priori demand distribution $\qv$. A coding scheme for this network
consists of a caching placement (i.e., a mapping of the library files into the
user caches) and delivery scheme (i.e., a mapping for the library files and
user demands into a common multicast codeword) such that, after the codeword
transmission, all users can retrieve their requested file. The rate of the
scheme is defined as the {\em average} codeword length normalized with respect
to the length of one file, where expectation is taken over the random user
demands. For the same shared link network, in the case of deterministic
demands, the optimal min-max rate has been characterized within a uniform
bound, independent of the network parameters. In particular, fractional caching
(i.e., storing file segments) and using linear network coding has been shown to
provide a min-max rate reduction proportional to 1/M with respect to standard
schemes such as unicasting or &quot;naive&quot; uncoded multicasting. The case of random
demands was previously considered by applying the same order-optimal min-max
scheme separately within groups of files requested with similar probability.
However, no order-optimal guarantee was provided for random demands under the
average rate performance criterion. In this paper, we consider the random
demand setting and provide general achievability and converse results. In
particular, we consider a family of schemes that combine random fractional
caching according to a probability distribution $\pv$ that depends on the
demand distribution $\qv$, with a linear coded delivery scheme based on ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03126</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03126</id><created>2015-02-10</created><authors><author><keyname>Bahrampour</keyname><forenames>Soheil</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Ray</keyname><forenames>Asok</forenames></author><author><keyname>Jenkins</keyname><forenames>Kenneth W.</forenames></author></authors><title>Kernel Task-Driven Dictionary Learning for Hyperspectral Image
  Classification</title><categories>stat.ML cs.CV cs.LG</categories><comments>5 pages, IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary learning algorithms have been successfully used in both
reconstructive and discriminative tasks, where the input signal is represented
by a linear combination of a few dictionary atoms. While these methods are
usually developed under $\ell_1$ sparsity constrain (prior) in the input
domain, recent studies have demonstrated the advantages of sparse
representation using structured sparsity priors in the kernel domain. In this
paper, we propose a supervised dictionary learning algorithm in the kernel
domain for hyperspectral image classification. In the proposed formulation, the
dictionary and classifier are obtained jointly for optimal classification
performance. The supervised formulation is task-driven and provides learned
features from the hyperspectral data that are well suited for the
classification task. Moreover, the proposed algorithm uses a joint
($\ell_{12}$) sparsity prior to enforce collaboration among the neighboring
pixels. The simulation results illustrate the efficiency of the proposed
dictionary learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03143</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03143</id><created>2015-02-10</created><authors><author><keyname>Calli</keyname><forenames>Berk</forenames></author><author><keyname>Walsman</keyname><forenames>Aaron</forenames></author><author><keyname>Singh</keyname><forenames>Arjun</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Dollar</keyname><forenames>Aaron M.</forenames></author></authors><title>Benchmarking in Manipulation Research: The YCB Object and Model Set and
  Benchmarking Protocols</title><categories>cs.RO</categories><comments>Submitted to Robotics and Automation Magazine (RAM) Special Issue on
  Replicable and Measurable Robotics Research. 35 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the Yale-CMU-Berkeley (YCB) Object and Model set,
intended to be used to facilitate benchmarking in robotic manipulation,
prosthetic design and rehabilitation research. The objects in the set are
designed to cover a wide range of aspects of the manipulation problem; it
includes objects of daily life with different shapes, sizes, textures, weight
and rigidity, as well as some widely used manipulation tests. The associated
database provides high-resolution RGBD scans, physical properties, and
geometric models of the objects for easy incorporation into manipulation and
planning software platforms. In addition to describing the objects and models
in the set along with how they were chosen and derived, we provide a framework
and a number of example task protocols, laying out how the set can be used to
quantitatively evaluate a range of manipulation approaches including planning,
learning, mechanical design, control, and many others. A comprehensive
literature survey on existing benchmarks and object datasets is also presented
and their scope and limitations are discussed. The set will be freely
distributed to research groups worldwide at a series of tutorials at robotics
conferences, and will be otherwise available at a reasonable purchase cost. It
is our hope that the ready availability of this set along with the ground laid
in terms of protocol templates will enable the community of manipulation
researchers to more easily compare approaches as well as continually evolve
benchmarking tests as the field matures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03147</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03147</id><created>2015-02-10</created><authors><author><keyname>Naderializadeh</keyname><forenames>Navid</forenames></author><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Topological Interference Management with just Retransmission: What are
  the &quot;Best&quot; Topologies?</title><categories>cs.IT math.IT</categories><comments>To appear at the IEEE International Conference on Communications (ICC
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of interference management in fast fading wireless
networks, in which the transmitters are only aware of network topology. We
consider a class of retransmission-based schemes, where transmitters in the
network are only allowed to resend their symbols in order to assist with the
neutralization of interference at the receivers. We introduce a necessary and
sufficient condition on the network topology, under which half symmetric
degrees-of-freedom (DoF) is achievable through the considered
retransmission-based schemes. This corresponds to the &quot;best&quot; topologies since
half symmetric DoF is the highest possible value for the symmetric DoF in the
presence of interference. We show that when the condition is satisfied, there
always exists a set of carefully chosen transmitters in the network, such that
by retransmission of their symbols at an appropriate time slot, we can
neutralize all the interfering signals at the receivers. Quite surprisingly, we
also show that for any given network topology, if we cannot achieve half
symmetric DoF by retransmission-based schemes, then there does not exist any
linear scheme that can do so. We also consider a practical network scenario
that models cell edge users in a heterogeneous network, and show that the
characterized condition on the network topology occurs frequently. Furthermore,
we numerically evaluate the achievable rates of the DoF-optimal
retransmission-based scheme in such network scenario, and show that its
throughput gain is not restricted to the asymptotic DoF analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03157</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03157</id><created>2015-02-10</created><authors><author><keyname>C&#xe1;mara</keyname><forenames>Javier</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Proen&#xe7;a</keyname><forenames>Jos&#xe9;</forenames><affiliation>KU Leuven</affiliation></author></authors><title>Proceedings 13th International Workshop on Foundations of Coordination
  Languages and Self-Adaptive Systems</title><categories>cs.DC cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 175, 2015</journal-ref><doi>10.4204/EPTCS.175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of FOCLASA 2014, the 13th International
Workshop on the Foundations of Coordination Languages and Self-Adaptive
Systems. FOCLASA 2014 was held in Rome, Italy, on September 9, 2014 as a
satellite event of CONCUR 2014, the 25th International Conference on
Concurrency Theory.
  Modern software systems are distributed, concurrent, mobile, and often
involve composition of heterogeneous components and stand-alone services.
Service coordination and self-adaptation constitute the core characteristics of
distributed and service-oriented systems. Coordination languages and formal
approaches to modelling and reasoning about self-adaptive behaviour help to
simplify the development of complex distributed service-based systems, enable
functional correctness proofs and improve reusability and maintainability of
such systems. The goal of the FOCLASA workshop is to put together researchers
and practitioners of the aforementioned fields, to share and identify common
problems, and to devise general solutions in the context of coordination
languages and self-adaptive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03158</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03158</id><created>2015-02-10</created><authors><author><keyname>Tutunov</keyname><forenames>Rasul</forenames></author><author><keyname>Ammar</keyname><forenames>Haitham Bou</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>A Fast Distributed Solver for Symmetric Diagonally Dominant Linear
  Equations</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a fast distributed solver for linear equations
given by symmetric diagonally dominant M-Matrices. Our approach is based on a
distributed implementation of the parallel solver of Spielman and Peng by
considering a specific approximated inverse chain which can be computed
efficiently in a distributed fashion. Representing the system of equations by a
graph $\mathbb{G}$, the proposed distributed algorithm is capable of attaining
$\epsilon$-close solutions (for arbitrary $\epsilon$) in time proportional to
$n^{3}$ (number of nodes in $\mathbb{G}$), ${\alpha}$ (upper bound on the size
of the R-Hop neighborhood), and $\frac{{W}_{max}}{{W}_{min}}$ (maximum and
minimum weight of edges in $\mathbb{G}$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03162</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03162</id><created>2015-02-10</created><authors><author><keyname>Luo</keyname><forenames>Yuancheng</forenames></author><author><keyname>Zotkin</keyname><forenames>Dmitry N.</forenames></author><author><keyname>Duraiswami</keyname><forenames>Ramani</forenames></author></authors><title>Sparse Head-Related Impulse Response for Efficient Direct Convolution</title><categories>cs.SD</categories><comments>9 Pages</comments><msc-class>15B05, 65F50, 42A85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Head-related impulse responses (HRIRs) are subject-dependent and
direction-dependent filters used in spatial audio synthesis. They describe the
scattering response of the head, torso, and pinnae of the subject. We propose a
structural factorization of the HRIRs into a product of non-negative and
Toeplitz matrices; the factorization is based on a novel extension of a
non-negative matrix factorization algorithm. As a result, the HRIR becomes
expressible as a convolution between a direction-independent \emph{resonance}
filter and a direction-dependent \emph{reflection} filter. Further, the
reflection filter can be made \emph{sparse} with minimal HRIR distortion. The
described factorization is shown to be applicable to the arbitrary source
signal case and allows one to employ time-domain convolution at a computational
cost lower than using convolution in the frequency domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03163</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03163</id><created>2015-02-10</created><authors><author><keyname>Luo</keyname><forenames>Yuancheng</forenames></author><author><keyname>Zotkin</keyname><forenames>Dmitry N.</forenames></author><author><keyname>Duraiswami</keyname><forenames>Ramani</forenames></author></authors><title>Gaussian Process Models for HRTF based Sound-Source Localization and
  Active-Learning</title><categories>cs.SD cs.LG stat.ML</categories><comments>11 pages</comments><msc-class>60G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From a machine learning perspective, the human ability localize sounds can be
modeled as a non-parametric and non-linear regression problem between binaural
spectral features of sound received at the ears (input) and their sound-source
directions (output). The input features can be summarized in terms of the
individual's head-related transfer functions (HRTFs) which measure the spectral
response between the listener's eardrum and an external point in $3$D. Based on
these viewpoints, two related problems are considered: how can one achieve an
optimal sampling of measurements for training sound-source localization (SSL)
models, and how can SSL models be used to infer the subject's HRTFs in
listening tests. First, we develop a class of binaural SSL models based on
Gaussian process regression and solve a \emph{forward selection} problem that
finds a subset of input-output samples that best generalize to all SSL
directions. Second, we use an \emph{active-learning} approach that updates an
online SSL model for inferring the subject's SSL errors via headphones and a
graphical user interface. Experiments show that only a small fraction of HRTFs
are required for $5^{\circ}$ localization accuracy and that the learned HRTFs
are localized closer to their intended directions than non-individualized
HRTFs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03167</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03167</id><created>2015-02-10</created><updated>2015-03-02</updated><authors><author><keyname>Ioffe</keyname><forenames>Sergey</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author></authors><title>Batch Normalization: Accelerating Deep Network Training by Reducing
  Internal Covariate Shift</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the parameters
of the previous layers change. This slows down the training by requiring lower
learning rates and careful parameter initialization, and makes it notoriously
hard to train models with saturating nonlinearities. We refer to this
phenomenon as internal covariate shift, and address the problem by normalizing
layer inputs. Our method draws its strength from making normalization a part of
the model architecture and performing the normalization for each training
mini-batch. Batch Normalization allows us to use much higher learning rates and
be less careful about initialization. It also acts as a regularizer, in some
cases eliminating the need for Dropout. Applied to a state-of-the-art image
classification model, Batch Normalization achieves the same accuracy with 14
times fewer training steps, and beats the original model by a significant
margin. Using an ensemble of batch-normalized networks, we improve upon the
best published result on ImageNet classification: reaching 4.9% top-5
validation error (and 4.8% test error), exceeding the accuracy of human raters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03169</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03169</id><created>2015-02-10</created><authors><author><keyname>Leek</keyname><forenames>Jeffrey T.</forenames></author><author><keyname>Peng</keyname><forenames>Roger D.</forenames></author></authors><title>Reproducible Research Can Still Be Wrong: Adopting a Prevention Approach</title><categories>stat.AP cs.CY</categories><comments>3 pages, 1 figure</comments><journal-ref>PNAS 112 (6) 1645-1645, 2015</journal-ref><doi>10.1073/pnas.1421412111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reproducibility, the ability to recompute results, and replicability, the
chances other experimenters will achieve a consistent result, are two
foundational characteristics of successful scientific research. Consistent
findings from independent investigators are the primary means by which
scientific evidence accumulates for or against an hypothesis. And yet, of late
there has been a crisis of confidence among researchers worried about the rate
at which studies are either reproducible or replicable. In order to maintain
the integrity of science research and maintain the public's trust in science,
the scientific community must ensure reproducibility and replicability by
engaging in a more preventative approach that greatly expands data analysis
education and routinely employs software tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03175</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03175</id><created>2015-02-10</created><updated>2015-05-30</updated><authors><author><keyname>Polson</keyname><forenames>Nicholas G.</forenames></author><author><keyname>Scott</keyname><forenames>James G.</forenames></author><author><keyname>Willard</keyname><forenames>Brandon T.</forenames></author></authors><title>Proximal Algorithms in Statistics and Machine Learning</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop proximal methods for statistical learning. Proximal
point algorithms are useful in statistics and machine learning for obtaining
optimization solutions for composite functions. Our approach exploits
closed-form solutions of proximal operators and envelope representations based
on the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes.
Envelope representations lead to novel proximal algorithms for statistical
optimisation of composite objective functions which include both non-smooth and
non-convex objectives. We illustrate our methodology with regularized Logistic
and Poisson regression and non-convex bridge penalties with a fused lasso norm.
We provide a discussion of convergence of non-descent algorithms with
acceleration and for non-convex functions. Finally, we provide directions for
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03181</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03181</id><created>2015-02-10</created><authors><author><keyname>Henriksson</keyname><forenames>Erik</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Peters</keyname><forenames>Edwin G. W.</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Multiple Loop Self-Triggered Model Predictive Control for Network
  Scheduling and Control</title><categories>cs.SY math.OC</categories><comments>Accepted for publication in IEEE Transactions on Control Systems
  Technology</comments><doi>10.1109/TCST.2015.2404308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for controlling and scheduling multiple linear
time-invariant processes on a shared bandwidth limited communication network
using adaptive sampling intervals. The controller is centralized and computes
at every sampling instant not only the new control command for a process, but
also decides the time interval to wait until taking the next sample. The
approach relies on model predictive control ideas, where the cost function
penalizes the state and control effort as well as the time interval until the
next sample is taken. The latter is introduced in order to generate an adaptive
sampling scheme for the overall system such that the sampling time increases as
the norm of the system state goes to zero. The paper presents a method for
synthesizing such a predictive controller and gives explicit sufficient
conditions for when it is stabilizing. Further explicit conditions are given
which guarantee conflict free transmissions on the network. It is shown that
the optimization problem may be solved off-line and that the controller can be
implemented as a lookup table of state feedback gains. Simulation studies which
compare the proposed algorithm to periodic sampling illustrate potential
performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03182</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03182</id><created>2015-02-10</created><updated>2015-08-17</updated><authors><author><keyname>Michalevsky</keyname><forenames>Yan</forenames></author><author><keyname>Nakibly</keyname><forenames>Gabi</forenames></author><author><keyname>Schulman</keyname><forenames>Aaron</forenames></author><author><keyname>Veerapandian</keyname><forenames>Gunaa Arumugam</forenames></author><author><keyname>Boneh</keyname><forenames>Dan</forenames></author></authors><title>PowerSpy: Location Tracking using Mobile Device Power Analysis</title><categories>cs.CR</categories><comments>Usenix Security 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern mobile platforms like Android enable applications to read aggregate
power usage on the phone. This information is considered harmless and reading
it requires no user permission or notification. We show that by simply reading
the phone's aggregate power consumption over a period of a few minutes an
application can learn information about the user's location. Aggregate phone
power consumption data is extremely noisy due to the multitude of components
and applications that simultaneously consume power. Nevertheless, by using
machine learning algorithms we are able to successfully infer the phone's
location. We discuss several ways in which this privacy leak can be remedied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03190</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03190</id><created>2015-02-10</created><authors><author><keyname>Lin</keyname><forenames>Xiahong</forenames></author><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Sun</keyname><forenames>Lifeng</forenames></author></authors><title>MAP: Microblogging Assisted Profiling of TV Shows</title><categories>cs.IR cs.MM cs.SI</categories><doi>10.1007/978-3-319-14445-0_38</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Online microblogging services that have been increasingly used by people to
share and exchange information, have emerged as a promising way to profiling
multimedia contents, in a sense to provide users a socialized abstraction and
understanding of these contents. In this paper, we propose a microblogging
profiling framework, to provide a social demonstration of TV shows. Challenges
for this study lie in two folds: First, TV shows are generally offline, i.e.,
most of them are not originally from the Internet, and we need to create a
connection between these TV shows with online microblogging services; Second,
contents in a microblogging service are extremely noisy for video profiling,
and we need to strategically retrieve the most related information for the TV
show profiling.To address these challenges, we propose a MAP, a
microblogging-assisted profiling framework, with contributions as follows: i)
We propose a joint user and content retrieval scheme, which uses information
about both actors and topics of a TV show to retrieve related microblogs; ii)
We propose a social-aware profiling strategy, which profiles a video according
to not only its content, but also the social relationship of its microblogging
users and its propagation in the social network; iii) We present some
interesting analysis, based on our framework to profile real-world TV shows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03191</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03191</id><created>2015-02-10</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Huffman</keyname><forenames>David A.</forenames></author><author><keyname>Koschitz</keyname><forenames>Duks</forenames></author><author><keyname>Tachi</keyname><forenames>Tomohiro</forenames></author></authors><title>Characterization of Curved Creases and Rulings: Design and Analysis of
  Lens Tessellations</title><categories>cs.CG math.DG</categories><comments>23 pages, 10 figures. To appear in Origami^6: Proceedings of the 6th
  International Meeting on Origami in Science, Mathematics and Education</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a general family of curved-crease folding tessellations
consisting of a repeating &quot;lens&quot; motif formed by two convex curved arcs. The
third author invented the first such design in 1992, when he made both a sketch
of the crease pattern and a vinyl model (pictured below). Curve fitting
suggests that this initial design used circular arcs. We show that in fact the
curve can be chosen to be any smooth convex curve without inflection point. We
identify the ruling configuration through qualitative properties that a curved
folding satisfies, and prove that the folded form exists with no additional
creases, through the use of differential geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03201</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03201</id><created>2015-02-11</created><updated>2015-06-19</updated><authors><author><keyname>Hale&#x161;</keyname><forenames>Jozef</forenames><affiliation>UBC-Computer Science</affiliation></author><author><keyname>Ma&#x148;uch</keyname><forenames>J&#xe1;n</forenames><affiliation>UBC-Computer Science</affiliation></author><author><keyname>Ponty</keyname><forenames>Yann</forenames><affiliation>LIX, AMIB</affiliation></author><author><keyname>Stacho</keyname><forenames>Ladislav</forenames></author></authors><title>Combinatorial RNA Design: Designability and Structure-Approximating
  Algorithm</title><categories>q-bio.QM cs.DS</categories><comments>CPM - 26th Annual Symposium on Combinatorial Pattern Matching, Jun
  2015, Ischia Island, Italy. LNCS, 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the Combinatorial RNA Design problem, a minimal
instance of the RNA design problem which aims at finding a sequence that admits
a given target as its unique base pair maximizing structure. We provide
complete characterizations for the structures that can be designed using
restricted alphabets. Under a classic four-letter alphabet, we provide a
complete characterization of designable structures without unpaired bases. When
unpaired bases are allowed, we provide partial characterizations for classes of
designable/undesignable structures, and show that the class of designable
structures is closed under the stutter operation. Membership of a given
structure to any of the classes can be tested in linear time and, for positive
instances, a solution can be found in linear time. Finally, we consider a
structure-approximating version of the problem that allows to extend bands
(helices) and, assuming that the input structure avoids two motifs, we provide
a linear-time algorithm that produces a designable structure with at most twice
more base pairs than the input structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03203</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03203</id><created>2015-02-11</created><authors><author><keyname>Walker</keyname><forenames>Josiah</forenames></author><author><keyname>Houliston</keyname><forenames>Trent</forenames></author><author><keyname>Annable</keyname><forenames>Brendan</forenames></author><author><keyname>Biddulph</keyname><forenames>Alex</forenames></author><author><keyname>Fountain</keyname><forenames>Jake</forenames></author><author><keyname>Metcalfe</keyname><forenames>Mitchell</forenames></author><author><keyname>Sugo</keyname><forenames>Anita</forenames></author><author><keyname>Olejniczak</keyname><forenames>Monica</forenames></author><author><keyname>Chalup</keyname><forenames>Stephan K.</forenames></author><author><keyname>King</keyname><forenames>Robert A. R.</forenames></author><author><keyname>Mendes</keyname><forenames>Alexandre</forenames></author><author><keyname>Turner</keyname><forenames>Peter</forenames></author></authors><title>The NUbots Team Description Paper 2015</title><categories>cs.RO</categories><comments>RoboCup 2015 team description paper. arXiv admin note: substantial
  text overlap with arXiv:1403.6946</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NUbots are an interdisciplinary RoboCup team from The University of
Newcastle, Australia. The team has a history of strong contributions in the
areas of machine learning and computer vision. The NUbots have participated in
RoboCup leagues since 2002, placing first several times in the past. In 2014
the NUbots also partnered with the University of Newcastle Mechatronics
Laboratory to participate in the RobotX Marine Robotics Challenge, which
resulted in several new ideas and improvements to the NUbots vision system for
RoboCup. This paper summarizes the history of the NUbots team, describes the
roles and research of the team members, gives an overview of the NUbots'
robots, their software system, and several associated research projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03204</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03204</id><created>2015-02-11</created><updated>2015-10-20</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>A Proof of the Strong Converse Theorem for Gaussian Multiple Access
  Channels</title><categories>cs.IT math.IT</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that $N$-user Gaussian multiple access channels (MACs) admit the
strong converse, i.e., every sequence of codes with asymptotic average error
probabilities smaller than one has rate tuples that lie in the capacity
(pentagonal) region. Our proof consists of four key ingredients: First, we
perform an expurgation step to convert the code defined in terms of the average
probability of error to one defined in terms of the maximum error without too
much loss in rate. Second, we use a scalar quantizer of increasing precision
with the blocklength to discretize the input spaces so that the wringing
procedure to be performed can yield a useful bound on the correlation among
users' codewords. Third, we use a wringing technique to further expurgate
appropriate codewords so that the resultant quantized code distribution can be
approximated by a product distribution over users' inputs.
  Finally, we obtain upper bounds on achievable sum-rates in terms of the
type-II error of a binary hypothesis test through a judicious choice of output
distributions. Our strong converse result carries over to the two sender
two-receiver Gaussian interference channel under strong interference as long as
the sum of the asymptotic average error probabilities is smaller than one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03212</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03212</id><created>2015-02-11</created><updated>2015-02-12</updated><authors><author><keyname>Xie</keyname><forenames>Hong</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Mathematical Modeling of Insurance Mechanisms for E-commerce Systems</title><categories>cs.MA</categories><comments>17 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic commerce (a.k.a. E-commerce) systems such as eBay and Taobao of
Alibaba are becoming increasingly popular. Having an effective reputation
system is critical to this type of internet service because it can assist
buyers to evaluate the trustworthiness of sellers, and it can also improve the
revenue for reputable sellers and E-commerce operators. We formulate a
stochastic model to analyze an eBay-like reputation system and propose four
measures to quantify its effectiveness: (1) new seller ramp up time, (2) new
seller drop out probability, (3) long term profit gains for sellers, and (4)
average per seller transaction gains for the E-commerce operator. Through our
analysis, we identify key factors which influence these four measures. We
propose a new insurance mechanism which consists of an insurance protocol and a
transaction mechanism to improve the above four measures. We show that our
insurance mechanism can reduce the ramp up time by around 87.2%, and guarantee
new sellers ramp up before the deadline $T_w$ with a high probability (close to
1.0). It also increases the long term profit gains and average per seller
transaction gains by at least 95.3%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03215</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03215</id><created>2015-02-11</created><authors><author><keyname>Bose</keyname><forenames>Smarajit</forenames></author><author><keyname>Pal</keyname><forenames>Amita</forenames></author><author><keyname>Mallick</keyname><forenames>Jhimli</forenames></author><author><keyname>Kumar</keyname><forenames>Sunil</forenames></author><author><keyname>Rudra</keyname><forenames>Pratyaydipta</forenames></author></authors><title>A Hybrid Approach for Improved Content-based Image Retrieval using
  Segmentation</title><categories>cs.IR cs.CV stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of Content-Based Image Retrieval (CBIR) methods is essentially
to extract, from large (image) databases, a specified number of images similar
in visual and semantic content to a so-called query image. To bridge the
semantic gap that exists between the representation of an image by low-level
features (namely, colour, shape, texture) and its high-level semantic content
as perceived by humans, CBIR systems typically make use of the relevance
feedback (RF) mechanism. RF iteratively incorporates user-given inputs
regarding the relevance of retrieved images, to improve retrieval efficiency.
One approach is to vary the weights of the features dynamically via feature
reweighting. In this work, an attempt has been made to improve retrieval
accuracy by enhancing a CBIR system based on color features alone, through
implicit incorporation of shape information obtained through prior segmentation
of the images. Novel schemes for feature reweighting as well as for
initialization of the relevant set for improved relevance feedback, have also
been proposed for boosting performance of RF- based CBIR. At the same time, new
measures for evaluation of retrieval accuracy have been suggested, to overcome
the limitations of existing measures in the RF context. Results of extensive
experiments have been presented to illustrate the effectiveness of the proposed
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03216</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03216</id><created>2015-02-11</created><updated>2015-03-12</updated><authors><author><keyname>Schmidt-Schau&#xdf;</keyname><forenames>Manfred</forenames><affiliation>Dept. Informatik und Mathematik, Inst. Informatik, J.W. Goethe-University, Frank</affiliation></author><author><keyname>Sabel</keyname><forenames>David</forenames><affiliation>Dept. Informatik und Mathematik, Inst. Informatik, J.W. Goethe-University, Frank</affiliation></author><author><keyname>Machkasova</keyname><forenames>Elena</forenames><affiliation>Division of Science and Mathematics, University of Minnesota, Morris, MN, U.S.A</affiliation></author></authors><title>Simulation in the Call-by-Need Lambda-Calculus with Letrec, Case,
  Constructors, and Seq</title><categories>cs.LO</categories><comments>50 pages, 11 figures</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 16,
  2015) lmcs:930</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows equivalence of several versions of applicative similarity
and contextual approximation, and hence also of applicative bisimilarity and
contextual equivalence, in LR, the deterministic call-by-need lambda calculus
with letrec extended by data constructors, case-expressions and Haskell's
seq-operator. LR models an untyped version of the core language of Haskell. The
use of bisimilarities simplifies equivalence proofs in calculi and opens a way
for more convenient correctness proofs for program transformations. The proof
is by a fully abstract and surjective transfer into a call-by-name calculus,
which is an extension of Abramsky's lazy lambda calculus. In the latter
calculus equivalence of our similarities and contextual approximation can be
shown by Howe's method. Similarity is transferred back to LR on the basis of an
inductively defined similarity. The translation from the call-by-need letrec
calculus into the extended call-by-name lambda calculus is the composition of
two translations. The first translation replaces the call-by-need strategy by a
call-by-name strategy and its correctness is shown by exploiting infinite trees
which emerge by unfolding the letrec expressions. The second translation
encodes letrec-expressions by using multi-fixpoint combinators and its
correctness is shown syntactically by comparing reductions of both calculi. A
further result of this paper is an isomorphism between the mentioned calculi,
which is also an identity on letrec-free expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03224</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03224</id><created>2015-02-11</created><authors><author><keyname>Mollgaard</keyname><forenames>Anders</forenames></author><author><keyname>Mathiesen</keyname><forenames>Joachim</forenames></author></authors><title>Emergent user behavior on Twitter modelled by a stochastic differential
  equation</title><categories>physics.soc-ph cs.SI</categories><doi>10.1371/journal.pone.0123876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data from the social-media site, Twitter, is used to study the fluctuations
in tweet rates of brand names. The tweet rates are the result of a strongly
correlated user behavior, which leads to bursty collective dynamics with a
characteristic 1/f noise. Here we use the aggregated &quot;user interest&quot; in a brand
name to model collective human dynamics by a stochastic differential equation
with multiplicative noise. The model is supported by a detailed analysis of the
tweet rate fluctuations and it reproduces both the exact bursty dynamics found
in the data and the 1/f noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03234</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03234</id><created>2015-02-11</created><authors><author><keyname>Springer</keyname><forenames>Paul</forenames></author></authors><title>A Scalable, Linear-Time Dynamic Cutoff Algorithm for Molecular
  Simulations of Interfacial Systems</title><categories>cs.DC cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This master thesis introduces the idea of dynamic cutoffs in molecular
dynamics simulations, based on the distance between particles and the
interface, and presents a solution for detecting interfaces in real-time. Our
dynamic cutoff method (DCM) exhibits a linear-time complexity as well as nearly
ideal weak and strong scaling. The DCM is tailored for massively parallel
architectures and for large interfacial systems with millions of particles. We
implemented the DCM as part of the LAMMPS open-source molecular dynamics
package and demonstrate the nearly ideal weak- and strong-scaling behavior of
this method on an IBM BlueGene/Q supercomputer. Our results for a liquid/vapor
system consisting of Lennard-Jones particles show that the accuracy of DCM is
comparable to that of the traditional particle-particle particle- mesh (PPPM)
algorithm. The performance comparison indicates that DCM is preferable for
large systems due to the limited scaling of FFTs within the PPPM algorithm.
Moreover, the DCM requires the interface to be identified every other MD
timestep. As a consequence, this thesis also presents an interface detection
method which is (1) applicable in real time; (2) parallelizable; and (3) scales
linearly with respect to the number of particles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03240</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03240</id><created>2015-02-11</created><updated>2015-04-30</updated><authors><author><keyname>Zheng</keyname><forenames>Shuai</forenames></author><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Romera-Paredes</keyname><forenames>Bernardino</forenames></author><author><keyname>Vineet</keyname><forenames>Vibhav</forenames></author><author><keyname>Su</keyname><forenames>Zhizhong</forenames></author><author><keyname>Du</keyname><forenames>Dalong</forenames></author><author><keyname>Huang</keyname><forenames>Chang</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>Conditional Random Fields as Recurrent Neural Networks</title><categories>cs.CV</categories><comments>16 pages, 10 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pixel-level labelling tasks, such as semantic segmentation, play a central
role in image understanding. Recent approaches have attempted to harness the
capabilities of deep learning techniques for image recognition to tackle
pixel-level labelling tasks. One central issue in this methodology is the
limited capacity of deep learning techniques to delineate visual objects. To
solve this problem, we introduce a new form of convolutional neural network
that combines the strengths of Convolutional Neural Networks (CNNs) and
Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To
this end, we formulate Conditional Random Fields as Recurrent Neural Networks.
This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a
deep network that has desirable properties of both CNNs and CRFs. Importantly,
our system fully integrates CRF modelling with CNNs, making it possible to
train the whole deep network end-to-end with the usual back-propagation
algorithm, avoiding offline post-processing methods for object delineation. We
apply the proposed method to the problem of semantic image segmentation,
obtaining top results on the challenging Pascal VOC 2012 segmentation
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03241</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03241</id><created>2015-02-11</created><authors><author><keyname>Bang-Jensen</keyname><forenames>J&#xf8;rgen</forenames></author><author><keyname>Larsen</keyname><forenames>Tilde My</forenames></author></authors><title>DAG-width and circumference of digraphs</title><categories>cs.DM math.CO</categories><comments>12 pages</comments><msc-class>05C20, 05C38, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every digraph of circumference $l$ has DAG-width at most $l$
and this is best possible. As a consequence of our result we deduce that the
$k$-linkage problem is polynomially solvable for every fixed $k$ in the class
of digraphs with bounded circumference. This answers a question posed in
\cite{bangTCS562}. We also prove that the weak $k$-linkage problem (where we
ask for arc-disjoint paths) is polynomially solvable for every fixed $k$ in the
class of digraphs with circumference 2 as well as for digraphs with a bounded
number of disjoint cycles each of length at least 3. The case of bounded
circumference digraphs is open. Finally we prove that the minimum spanning
strong subdigraph problem is NP-hard on digraphs of DAG-width at most 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03245</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03245</id><created>2015-02-11</created><updated>2015-02-13</updated><authors><author><keyname>Banescu</keyname><forenames>Sebastian</forenames></author><author><keyname>W&#xfc;chner</keyname><forenames>Tobias</forenames></author><author><keyname>Guggenmos</keyname><forenames>Marius</forenames></author><author><keyname>Ochoa</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>Pretschner</keyname><forenames>Alexander</forenames></author></authors><title>FEEBO: An Empirical Evaluation Framework for Malware Behavior
  Obfuscation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program obfuscation is increasingly popular among malware creators.
Objectively comparing different malware detection approaches with respect to
their resilience against obfuscation is challenging. To the best of our
knowledge, there is no common empirical framework for evaluating the resilience
of malware detection approaches w.r.t. behavior obfuscation. We propose and
implement such a framework that obfuscates the observable behavior of malware
binaries. To assess the framework's utility, we use it to obfuscate known
malware binaries and then investigate the impact on detection effectiveness of
different $n$-gram based detection approaches. We find that the obfuscation
transformations employed by our framework significantly affect the precision of
such detection approaches. Several $n$-gram-based approaches can hence be
concluded not to be resilient against this simple kind of obfuscation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03248</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03248</id><created>2015-02-11</created><updated>2015-03-23</updated><authors><author><keyname>Harutyunyan</keyname><forenames>Anna</forenames></author><author><keyname>Brys</keyname><forenames>Tim</forenames></author><author><keyname>Vrancx</keyname><forenames>Peter</forenames></author><author><keyname>Nowe</keyname><forenames>Ann</forenames></author></authors><title>Off-Policy Reward Shaping with Ensembles</title><categories>cs.AI</categories><comments>To be presented at ALA-15. Short version to appear at AAMAS-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Potential-based reward shaping (PBRS) is an effective and popular technique
to speed up reinforcement learning by leveraging domain knowledge. While PBRS
is proven to always preserve optimal policies, its effect on learning speed is
determined by the quality of its potential function, which, in turn, depends on
both the underlying heuristic and the scale. Knowing which heuristic will prove
effective requires testing the options beforehand, and determining the
appropriate scale requires tuning, both of which introduce additional sample
complexity. We formulate a PBRS framework that reduces learning speed, but does
not incur extra sample complexity. For this, we propose to simultaneously learn
an ensemble of policies, shaped w.r.t. many heuristics and on a range of
scales. The target policy is then obtained by voting. The ensemble needs to be
able to efficiently and reliably learn off-policy: requirements fulfilled by
the recent Horde architecture, which we take as our basis. We demonstrate
empirically that (1) our ensemble policy outperforms both the base policy, and
its single-heuristic components, and (2) an ensemble over a general range of
scales performs at least as well as one with optimally tuned components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03255</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03255</id><created>2015-02-11</created><authors><author><keyname>Hallak</keyname><forenames>Assaf</forenames></author><author><keyname>Schnitzler</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Mann</keyname><forenames>Timothy</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Off-policy evaluation for MDPs with unknown structure</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Off-policy learning in dynamic decision problems is essential for providing
strong evidence that a new policy is better than the one in use. But how can we
prove superiority without testing the new policy? To answer this question, we
introduce the G-SCOPE algorithm that evaluates a new policy based on data
generated by the existing policy. Our algorithm is both computationally and
sample efficient because it greedily learns to exploit factored structure in
the dynamics of the environment. We present a finite sample analysis of our
approach and show through experiments that the algorithm scales well on
high-dimensional problems with few samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03258</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03258</id><created>2015-02-11</created><authors><author><keyname>Fletcher</keyname><forenames>George H. L.</forenames></author><author><keyname>Gyssens</keyname><forenames>Marc</forenames></author><author><keyname>Paredaens</keyname><forenames>Jan</forenames></author><author><keyname>Van Gucht</keyname><forenames>Dirk</forenames></author><author><keyname>Wu</keyname><forenames>Yuqing</forenames></author></authors><title>Structural characterizations of the navigational expressiveness of
  relation algebras on a tree</title><categories>cs.DB cs.LO</categories><comments>58 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a document D in the form of an unordered node-labeled tree, we study
the expressiveness on D of various basic fragments of XPath, the core
navigational language on XML documents. Working from the perspective of these
languages as fragments of Tarski's relation algebra, we give characterizations,
in terms of the structure of D, for when a binary relation on its nodes is
definable by an expression in these algebras. Since each pair of nodes in such
a relation represents a unique path in D, our results therefore capture the
sets of paths in D definable in each of the fragments. We refer to this
perspective on language semantics as the &quot;global view.&quot; In contrast with this
global view, there is also a &quot;local view&quot; where one is interested in the nodes
to which one can navigate starting from a particular node in the document. In
this view, we characterize when a set of nodes in D can be defined as the
result of applying an expression to a given node of D. All these definability
results, both in the global and the local view, are obtained by using a robust
two-step methodology, which consists of first characterizing when two nodes
cannot be distinguished by an expression in the respective fragments of XPath,
and then bootstrapping these characterizations to the desired results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03273</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03273</id><created>2015-02-11</created><updated>2016-03-01</updated><authors><author><keyname>Chen</keyname><forenames>Dai-Qiang</forenames></author></authors><title>Image denoising based on improved data-driven sparse representation</title><categories>cs.CV</categories><comments>19 pages, 5 figures</comments><msc-class>68U10, 90C90, 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation of images under certain transform domain has been
playing a fundamental role in image restoration tasks. One such representative
method is the widely used wavelet tight frame systems. Instead of adopting
fixed filters for constructing a tight frame to sparsely model any input image,
a data-driven tight frame was proposed for the sparse representation of images,
and shown to be very efficient for image denoising very recently. However, in
this method the number of framelet filters used for constructing a tight frame
is the same as the length of filters. In fact, through further investigation it
is found that part of these filters are unnecessary and even harmful to the
recovery effect due to the influence of noise. Therefore, an improved
data-driven sparse representation systems constructed with much less number of
filters are proposed. Numerical results on denoising experiments demonstrate
that the proposed algorithm overall outperforms the original data-driven tight
frame construction scheme on both the recovery quality and computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03276</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03276</id><created>2015-02-11</created><updated>2015-04-21</updated><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund M.</forenames></author><author><keyname>Liu</keyname><forenames>Ran</forenames></author></authors><title>Enhancing the Delay Performance of Dynamic Backpressure Algorithms</title><categories>cs.IT cs.NI math.IT</categories><comments>6 figures, 2 tables; Transactions on Networking, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For general multi-hop queueing networks, delay optimal network control has
unfortunately been an outstanding problem. The dynamic backpressure (BP)
algorithm elegantly achieves throughput optimality, but does not yield good
delay performance in general. In this paper, we obtain an asymptotically delay
optimal control policy, which resembles the BP algorithm in basing resource
allocation and routing on a backpressure calculation, but differs from the BP
algorithm in the form of the backpressure calculation employed. The difference
suggests a possible reason for the unsatisfactory delay performance of the BP
algorithm, i.e., the myopic nature of the BP control. Motivated by this new
connection, we introduce a new class of enhanced backpressure-based algorithms
which incorporate a general queue-dependent bias function into the backpressure
term of the traditional BP algorithm to improve delay performance. These
enhanced algorithms exploit queue state information beyond one hop. We prove
the throughput optimality and characterize the utility-delay tradeoff of the
enhanced algorithms. We further focus on two specific distributed algorithms
within this class, which have demonstrably improved delay performance as well
as acceptable implementation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03288</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03288</id><created>2015-02-11</created><updated>2015-05-14</updated><authors><author><keyname>Prezza</keyname><forenames>Nicola</forenames></author></authors><title>A Compressed-Gap Data-Aware Measure</title><categories>cs.DS</categories><comments>11 pages, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of efficiently representing a set $S$
of $n$ items out of a universe $U=\{0,...,u-1\}$ while supporting a number of
operations on it. Let $G=g_1...g_n$ be the gap stream associated with $S$,
$gap$ its bit-size when encoded with \emph{gap-encoding}, and $H_0(G)$ its
empirical zero-order entropy. We prove that (1) $nH_0(G)\in o(gap)$ if $G$ is
highly compressible, and (2) $nH_0(G) \leq n\log(u/n) + n \leq uH_0(S)$. Let
$d$ be the number of \emph{distinct} gap lengths between elements in $S$. We
firstly propose a new space-efficient zero-order compressed representation of
$S$ taking $n(H_0(G)+1)+\mathcal O(d\log u)$ bits of space. Then, we describe a
fully-indexable dictionary that supports \emph{rank} and \emph{select} queries
in $\mathcal O(\log(u/n)+\log\log u)$ time while requiring asymptotically the
same space as the proposed compressed representation of $S$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03296</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03296</id><created>2015-02-11</created><authors><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author><author><keyname>Gerlach</keyname><forenames>Martin</forenames></author></authors><title>Statistical laws in linguistics</title><categories>physics.soc-ph cs.LG physics.data-an</categories><comments>Proceedings of the Flow Machines Workshop: Creativity and
  Universality in Language, Paris, June 18 to 20, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zipf's law is just one out of many universal laws proposed to describe
statistical regularities in language. Here we review and critically discuss how
these laws can be statistically interpreted, fitted, and tested (falsified).
The modern availability of large databases of written text allows for tests
with an unprecedent statistical accuracy and also a characterization of the
fluctuations around the typical behavior. We find that fluctuations are usually
much larger than expected based on simplifying statistical assumptions (e.g.,
independence and lack of correlations between observations).These
simplifications appear also in usual statistical tests so that the large
fluctuations can be erroneously interpreted as a falsification of the law.
Instead, here we argue that linguistic laws are only meaningful (falsifiable)
if accompanied by a model for which the fluctuations can be computed (e.g., a
generative model of the text). The large fluctuations we report show that the
constraints imposed by linguistic laws on the creativity process of text
generation are not as tight as one could expect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03302</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03302</id><created>2015-02-11</created><updated>2015-03-23</updated><authors><author><keyname>Kuhad</keyname><forenames>Pallavi</forenames></author><author><keyname>Yassine</keyname><forenames>Abdulsalam</forenames></author><author><keyname>Shirmohammadi</keyname><forenames>Shervin</forenames></author></authors><title>Using Distance Estimation and Deep Learning to Simplify Calibration in
  Food Calorie Measurement</title><categories>cs.CY cs.HC cs.LG</categories><comments>This paper has been withdrawn due to errors in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High calorie intake in the human body on the one hand, has proved harmful in
numerous occasions leading to several diseases and on the other hand, a
standard amount of calorie intake has been deemed essential by dieticians to
maintain the right balance of calorie content in human body. As such,
researchers have proposed a variety of automatic tools and systems to assist
users measure their calorie in-take. In this paper, we consider the category of
those tools that use image processing to recognize the food, and we propose a
method for fully automatic and user-friendly calibration of the dimension of
the food portion sizes, which is needed in order to measure food portion weight
and its ensuing amount of calories. Experimental results show that our method,
which uses deep learning, mobile cloud computing, distance estimation and size
calibration inside a mobile device, leads to an accuracy improvement to 95% on
average compared to previous work
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03316</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03316</id><created>2015-02-11</created><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Charikar</keyname><forenames>Moses</forenames></author><author><keyname>Krishnaswamy</keyname><forenames>Ravishankar</forenames></author><author><keyname>Sinop</keyname><forenames>Ali Kemal</forenames></author></authors><title>The Hardness of Approximation of Euclidean k-means</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Euclidean $k$-means problem is a classical problem that has been
extensively studied in the theoretical computer science, machine learning and
the computational geometry communities. In this problem, we are given a set of
$n$ points in Euclidean space $R^d$, and the goal is to choose $k$ centers in
$R^d$ so that the sum of squared distances of each point to its nearest center
is minimized. The best approximation algorithms for this problem include a
polynomial time constant factor approximation for general $k$ and a
$(1+\epsilon)$-approximation which runs in time $poly(n) 2^{O(k/\epsilon)}$. At
the other extreme, the only known computational complexity result for this
problem is NP-hardness [ADHP'09]. The main difficulty in obtaining hardness
results stems from the Euclidean nature of the problem, and the fact that any
point in $R^d$ can be a potential center. This gap in understanding left open
the intriguing possibility that the problem might admit a PTAS for all $k,d$.
  In this paper we provide the first hardness of approximation for the
Euclidean $k$-means problem. Concretely, we show that there exists a constant
$\epsilon &gt; 0$ such that it is NP-hard to approximate the $k$-means objective
to within a factor of $(1+\epsilon)$. We show this via an efficient reduction
from the vertex cover problem on triangle-free graphs: given a triangle-free
graph, the goal is to choose the fewest number of vertices which are incident
on all the edges. Additionally, we give a proof that the current best hardness
results for vertex cover can be carried over to triangle-free graphs. To show
this we transform $G$, a known hard vertex cover instance, by taking a graph
product with a suitably chosen graph $H$, and showing that the size of the
(normalized) maximum independent set is almost exactly preserved in the product
graph using a spectral analysis, which might be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03320</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03320</id><created>2015-02-11</created><authors><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Kutten</keyname><forenames>Shay</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Construction and impromptu repair of an MST in a distributed network
  with o(m) communication</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the CONGEST model, a communications network is an undirected graph whose
$n$ nodes are processors and whose $m$ edges are the communications links
between processors. At any given time step, a message of size $O(\log n)$ may
be sent by each node to each of its neighbors. We show for the synchronous
model: If all nodes start in the same round, and each node knows its ID and the
ID's of its neighbors, or in the case of MST, the distinct weights of its
incident edges and knows $n$, then there are Monte Carlo algorithms which
succeed w.h.p. to determine a minimum spanning forest (MST) and a spanning
forest (ST) using $O(n \log^2 n/\log\log n)$ messages for MST and $O(n \log n
)$ messages for ST, resp. These results contradict the &quot;folk theorem&quot; noted in
Awerbuch, et.al., JACM 1990 that the distributed construction of a broadcast
tree requires $\Omega(m)$ messages. This lower bound has been shown there and
in other papers for some CONGEST models; our protocol demonstrates the limits
of these models.
  A dynamic distributed network is one which undergoes online edge insertions
or deletions. We also show how to repair an MST or ST in a dynamic network with
asynchronous communication. An edge deletion can be processed in $O(n\log n
/\log \log n)$ expected messages in the MST, and $O(n)$ expected messages for
the ST problem, while an edge insertion uses $O(n)$ messages in the worst case.
We call this &quot;impromptu&quot; updating as we assume that between processing of edge
updates there is no preprocessing or storage of additional information.
Previous algorithms for this problem that use an amortized $o(m)$ messages per
update require substantial preprocessing and additional local storage between
updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03322</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03322</id><created>2015-02-11</created><authors><author><keyname>Zhang</keyname><forenames>Yongfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Min</forenames></author><author><keyname>Liu</keyname><forenames>Yiqun</forenames></author><author><keyname>Ma</keyname><forenames>Shaoping</forenames></author></authors><title>Boost Phrase-level Polarity Labelling with Review-level Sentiment
  Classification</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis on user reviews helps to keep track of user reactions
towards products, and make advices to users about what to buy. State-of-the-art
review-level sentiment classification techniques could give pretty good
precisions of above 90%. However, current phrase-level sentiment analysis
approaches might only give sentiment polarity labelling precisions of around
70%~80%, which is far from satisfaction and restricts its application in many
practical tasks. In this paper, we focus on the problem of phrase-level
sentiment polarity labelling and attempt to bridge the gap between phrase-level
and review-level sentiment analysis. We investigate the inconsistency between
the numerical star ratings and the sentiment orientation of textual user
reviews. Although they have long been treated as identical, which serves as a
basic assumption in previous work, we find that this assumption is not
necessarily true. We further propose to leverage the results of review-level
sentiment classification to boost the performance of phrase-level polarity
labelling using a novel constrained convex optimization framework. Besides, the
framework is capable of integrating various kinds of information sources and
heuristics, while giving the global optimal solution due to its convexity.
Experimental results on both English and Chinese reviews show that our
framework achieves high labelling precisions of up to 89%, which is a
significant improvement from current approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03332</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03332</id><created>2015-02-11</created><updated>2015-09-21</updated><authors><author><keyname>Chazelle</keyname><forenames>Bernard</forenames></author><author><keyname>Wang</keyname><forenames>Chu</forenames></author></authors><title>Inertial Hegselmann-Krause Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main contribution of this paper is to prove the convergence of
Hegselmann-Krause systems with closed-minded agents, thus settling a conjecture
of long standing. Convergence follows from an energy bound that we establish
for inertial HK systems, which we define as a variant of the original HK model
in which the agents can change their weights arbitrarily at each step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03337</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03337</id><created>2015-02-11</created><authors><author><keyname>Christoforou</keyname><forenames>Evgenia</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author><author><keyname>Santos</keyname><forenames>Agust&#xed;n</forenames></author></authors><title>A Mechanism for Fair Distribution of Resources with Application to
  Sponsored Search</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a rapid shift of advertisement from the traditional media to the
Internet. A large portion of the traffic created by publicity is due to search
engines through sponsored search. Advertisers pay the search engine to show
their content, usually in order to get traffic to their own websites (where
they may even feature more publicity). Search engines provide limited space for
publicity (i.e. slots) for each keyword search. Since the demand is high they
are conducting auctions to determine the advertisers for each keyword search.
Designing an auction mechanism one must specify how to assign bidders to goods
(or vice versa) and how to set the price for each good won in the auction. This
work proposes a new auction mechanism were the slots are distributed among the
advertisers depending on how much each advertiser values appearing in a keyword
search slot at a specific time. The proposed approach makes payment to the
search engine independent of these values. In particular, we propose that
payments take the form of a flat fee. We show that this auction mechanism
fairly distributes resources (or goods, e.g., slots) in an online fashion,
based on the users' declared preferences, while being socially efficient. While
the main motivation for this work was sponsored search, the proposed mechanism
can be used in general for the fair distribution of resources in an online
fashion among a set of users. Hence, we refer to this mechanism as Fair and
Efficient Distribution of Resources (FEDoR). FEDoR can be used even when the
auction is done in a distributed fashion (i.e., without central authority), and
it provides fairness, social efficiency and incentive compatibility.
Essentially, letting the search engine adjust its revenue through the flat fee
paid by the advertisers, FEDoR allows it to achieve the maximum gain while
being competitive in the search engines' market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03338</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03338</id><created>2015-02-09</created><authors><author><keyname>Gorski</keyname><forenames>Piotr J.</forenames></author><author><keyname>Czaplicka</keyname><forenames>Agnieszka</forenames></author><author><keyname>Holyst</keyname><forenames>Janusz A.</forenames></author></authors><title>Coevolution of Information Processing and Topology in Hierarchical
  Adaptive Random Boolean Networks</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.MN</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Boolean networks (RBNs) are frequently employed for modelling complex
systems driven by information processing, e.g. for gene regulatory networks
(GRNs). Here we propose a hierarchical adaptive RBN (HARBN) as a system
consisting of distinct adaptive RBNs - subnetworks - connected by a set of
permanent interlinks. Information measures and internal subnetworks topology of
HARBN coevolve and reach steady-states that are specific for a given network
structure. We investigate mean node information, mean edge information as well
as a mean node degree as functions of model parameters and demonstrate HARBN's
ability to describe complex hierarchical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03343</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03343</id><created>2015-02-11</created><authors><author><keyname>Backes</keyname><forenames>John</forenames></author><author><keyname>Cofer</keyname><forenames>Darren</forenames></author><author><keyname>Miller</keyname><forenames>Steven</forenames></author><author><keyname>Whalen</keyname><forenames>Mike</forenames></author></authors><title>Requirements Analysis of a Quad-Redundant Flight Control System</title><categories>cs.SE</categories><comments>Accepted to NASA Formal Methods 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we detail our effort to formalize and prove requirements for
the Quad-redundant Flight Control System (QFCS) within NASA's Transport Class
Model (TCM). We use a compositional approach with assume-guarantee contracts
that correspond to the requirements for software components embedded in an AADL
system architecture model. This approach is designed to exploit the
verification effort and artifacts that are already part of typical software
verification processes in the avionics domain. Our approach is supported by an
AADL annex that allows specification of contracts along with a tool, called
AGREE, for performing compositional verification. The goal of this paper is to
show the benefits of a compositional verification approach applied to a
realistic avionics system and to demonstrate the effectiveness of the AGREE
tool in performing this analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03346</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03346</id><created>2015-02-11</created><updated>2015-07-21</updated><authors><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Berrang</keyname><forenames>Pascal</forenames></author><author><keyname>Manoharan</keyname><forenames>Praveen</forenames></author></authors><title>How well do you blend into the crowd? - d-convergence: Assessing
  identifiability and linkability in large-scale, open web settings</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy in open settings such as the Internet is a very different problem
than privacy in closed settings such as statistical databases: there is no
a-priori classification of data into key- and sensitive information, user's
disseminate information over many different data sources, and we can not
globally sanitize disseminated information to achieve privacy. In this paper,
we take a first step in developing a user-centric privacy model in large-scale,
open web environments that exhibit rapid dissemination of unstructured,
heterogeneous data: we propose the d-convergence model, which allows for the
assessment of identity disclosure risks by measuring the similarity of
identities within and across different online platforms.
  We demonstrate the applicability of the d-convergence model on a collection
of 40 million comments collected from the Online Social Network Reddit, which
we stripped down to 15 million comments for our evaluation on two Dell
PowerEdge R820 with 64 virtual cores each. Our evaluation confirms hypotheses
about the identifiability of entities in our dataset and provides deeper
insights into its structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03358</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03358</id><created>2015-02-11</created><authors><author><keyname>Naghibi</keyname><forenames>Farshad</forenames></author><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>The CEO Problem with Secrecy Constraints</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information
  Forensics and Security, 17 pages, 4 figures</comments><doi>10.1109/TIFS.2015.2404134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a lossy source coding problem with secrecy constraints in which a
remote information source should be transmitted to a single destination via
multiple agents in the presence of a passive eavesdropper. The agents observe
noisy versions of the source and independently encode and transmit their
observations to the destination via noiseless rate-limited links. The
destination should estimate the remote source based on the information received
from the agents within a certain mean distortion threshold. The eavesdropper,
with access to side information correlated to the source, is able to listen in
on one of the links from the agents to the destination in order to obtain as
much information as possible about the source. This problem can be viewed as
the so-called CEO problem with additional secrecy constraints. We establish
inner and outer bounds on the rate-distortion-equivocation region of this
problem. We also obtain the region in special cases where the bounds are tight.
Furthermore, we study the quadratic Gaussian case and provide the optimal
rate-distortion-equivocation region when the eavesdropper has no side
information and an achievable region for a more general setup with side
information at the eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03371</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03371</id><created>2015-02-11</created><authors><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Silva</keyname><forenames>D.</forenames></author></authors><title>The Z Transform over Finite Fields</title><categories>math.NT cs.NA</categories><comments>6 pages, 5 figures, Proc. IEEE/SBrT Int. Telecomm. Symp., 2002.
  pp.362-367</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite field transforms have many applications and, in many cases, can be
implemented with a low computational complexity. In this paper, the Z Transform
over a finite field is introduced and some of its properties are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03372</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03372</id><created>2015-02-11</created><updated>2016-02-15</updated><authors><author><keyname>Marasevic</keyname><forenames>Jelena</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>A Fast Distributed Stateless Algorithm for $\alpha$-Fair Packing
  Problems</title><categories>cs.DC cs.DS</categories><comments>Added structural results for asymptotic cases of \alpha-fairness
  (\alpha approaching 0, 1, or infinity), improved presentation, and revised
  throughout</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades, fair resource allocation problems have received
considerable attention in a variety of application areas. However, little
progress has been made in the design of distributed algorithms with convergence
guarantees for general and commonly used $\alpha$-fair allocations. In this
paper, we study weighted $\alpha$-fair packing problems, that is, the problems
of maximizing the objective functions (i) $\sum_j w_j
x_j^{1-\alpha}/(1-\alpha)$ when $\alpha &gt; 0$, $\alpha \neq 1$ and (ii) $\sum_j
w_j \ln x_j$ when $\alpha = 1$, over linear constraints $Ax \leq b$, $x\geq 0$,
where $w_j$ are positive weights and $A$ and $b$ are non-negative. We consider
the distributed computation model that was used for packing linear programs and
network utility maximization problems. Under this model, we provide a
distributed algorithm for general $\alpha$ that converges to an
$\varepsilon-$approximate solution in time (number of distributed iterations)
that has an inverse polynomial dependence on the approximation parameter
$\varepsilon$ and poly-logarithmic dependence on the problem size. This is the
first distributed algorithm for weighted $\alpha-$fair packing with
poly-logarithmic convergence in the input size. The algorithm uses simple local
update rules and is stateless (namely, it allows asynchronous updates, is
self-stabilizing, and allows incremental and local adjustments). We also obtain
a number of structural results that characterize $\alpha-$fair allocations as
the value of $\alpha$ is varied. These results deepen our understanding of
fairness guarantees in $\alpha-$fair packing allocations, and also provide
insight into the behavior of $\alpha-$fair allocations in the asymptotic cases
$\alpha\rightarrow 0$, $\alpha \rightarrow 1$, and $\alpha \rightarrow \infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03379</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03379</id><created>2015-02-11</created><authors><author><keyname>Gambette</keyname><forenames>Philippe</forenames></author><author><keyname>Gunawan</keyname><forenames>Andreas D. M.</forenames></author><author><keyname>Labarre</keyname><forenames>Anthony</forenames></author><author><keyname>Vialette</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Zhang</keyname><forenames>Louxin</forenames></author></authors><title>Locating a Tree in a Phylogenetic Network in Quadratic Time</title><categories>cs.DS cs.CE q-bio.PE</categories><comments>Accepted to RECOMB 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in the study of phylogenetic networks is to determine
whether or not a given phylogenetic network contains a given phylogenetic tree.
We develop a quadratic-time algorithm for this problem for binary nearly-stable
phylogenetic networks. We also show that the number of reticulations in a
reticulation visible or nearly stable phylogenetic network is bounded from
above by a function linear in the number of taxa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03387</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03387</id><created>2015-02-11</created><authors><author><keyname>Filho</keyname><forenames>R. F. B. Sotero</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation
  Recording</title><categories>cs.SD</categories><comments>7 pages, 3 figures, 3 tables, XXXV Cong. Nac. de Matematica Aplicada
  e Computacional, Natal, RN, Brazil 2014</comments><doi>10.5540/03.2015.003.01.0468</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03406</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03406</id><created>2015-02-11</created><authors><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author><author><keyname>Decuyper</keyname><forenames>Adeline</forenames></author><author><keyname>Krings</keyname><forenames>Gautier</forenames></author></authors><title>A survey of results on mobile phone datasets analysis</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we review some advances made recently in the study of mobile
phone datasets. This area of research has emerged a decade ago, with the
increasing availability of large-scale anonymized datasets, and has grown into
a stand-alone topic. We will survey the contributions made so far on the social
networks that can be constructed with such data, the study of personal
mobility, geographical partitioning, urban planning, and help towards
development as well as security and privacy issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03407</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03407</id><created>2015-02-11</created><authors><author><keyname>Saldamli</keyname><forenames>Gokay</forenames></author><author><keyname>Chow</keyname><forenames>Richard</forenames></author><author><keyname>Jin</keyname><forenames>Hongxia</forenames></author></authors><title>Albatross: a Privacy-Preserving Location Sharing System</title><categories>cs.CR</categories><comments>12 Pages, Extended version of ASIACCS 2015 paper</comments><acm-class>C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networking services are increasingly accessed through mobile devices.
This trend has prompted services such as Facebook and Google+ to incorporate
location as a de facto feature of user interaction. At the same time, services
based on location such as Foursquare and Shopkick are also growing as
smartphone market penetration increases. In fact, this growth is happening
despite concerns (growing at a similar pace) about security and third-party use
of private location information (e.g., for advertising). Nevertheless, service
providers have been unwilling to build truly private systems in which they do
not have access to location information. In this paper, we describe an
architecture and a trial implementation of a privacy-preserving location
sharing system called Albatross. The system protects location information from
the service provider and yet enables fine-grained location-sharing. One main
feature of the system is to protect an individual's social network structure.
The pattern of location sharing preferences towards contacts can reveal this
structure without any knowledge of the locations themselves. Albatross protects
locations sharing preferences through protocol unification and masking.
Albatross has been implemented as a standalone solution, but the technology can
also be integrated into location-based services to enhance privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03409</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03409</id><created>2015-02-11</created><authors><author><keyname>Ni</keyname><forenames>Karl</forenames></author><author><keyname>Pearce</keyname><forenames>Roger</forenames></author><author><keyname>Boakye</keyname><forenames>Kofi</forenames></author><author><keyname>Van Essen</keyname><forenames>Brian</forenames></author><author><keyname>Borth</keyname><forenames>Damian</forenames></author><author><keyname>Chen</keyname><forenames>Barry</forenames></author><author><keyname>Wang</keyname><forenames>Eric</forenames></author></authors><title>Large-Scale Deep Learning on the YFCC100M Dataset</title><categories>cs.LG cs.CV</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present a work-in-progress snapshot of learning with a 15 billion
parameter deep learning network on HPC architectures applied to the largest
publicly available natural image and video dataset released to-date. Recent
advancements in unsupervised deep neural networks suggest that scaling up such
networks in both model and training dataset size can yield significant
improvements in the learning of concepts at the highest layers. We train our
three-layer deep neural network on the Yahoo! Flickr Creative Commons 100M
dataset. The dataset comprises approximately 99.2 million images and 800,000
user-created videos from Yahoo's Flickr image and video sharing platform.
Training of our network takes eight days on 98 GPU nodes at the High
Performance Computing Center at Lawrence Livermore National Laboratory.
Encouraging preliminary results and future research directions are presented
and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03415</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03415</id><created>2015-02-11</created><authors><author><keyname>Benachour</keyname><forenames>Sofiane</forenames></author><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Andrieu</keyname><forenames>Vincent</forenames></author></authors><title>Locally optimal controllers and globally inverse optimal controllers</title><categories>math.OC cs.SY</categories><journal-ref>Automatica, vol 50, 11, pp. 2918-2923, 2014</journal-ref><doi>10.1016/j.automatica.2014.10.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of global asymptotic stabilization with
prescribed local behavior. We show that this problem can be formulated in terms
of control Lyapunov functions. Moreover, we show that if the local control law
has been synthesized employing a LQ approach, then the associated Lyapunov
function can be seen as the value function of an optimal problem with some
specific local properties. We illustrate these results on two specific classes
of systems: backstepping and feedforward systems. Finally, we show how this
framework can be employed when considering the orbital transfer problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03426</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03426</id><created>2015-02-11</created><updated>2015-08-08</updated><authors><author><keyname>Ciobanu</keyname><forenames>Laura</forenames></author><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Elder</keyname><forenames>Murray</forenames></author></authors><title>Solution sets for equations over free groups are EDT0L languages --
  ICALP 2015 version</title><categories>cs.LO cs.DM cs.FL math.GR</categories><comments>37 pages, 2 figures</comments><acm-class>F.4; F.2; F.2.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, given a word equation over a finitely generated free group, the
set of all solutions in reduced words forms an EDT0L language. In particular,
it is an indexed language in the sense of Aho. The question of whether a
description of solution sets in reduced words as an indexed language is
possible has been been open for some years, apparently without much hope that a
positive answer could hold. Nevertheless, our answer goes far beyond: they are
EDT0L, which is a proper subclass of indexed languages. We can additionally
handle the existential theory of equations with rational constraints in free
products $\star_{1 \leq i \leq s}F_i$, where each $F_i$ is either a free or
finite group, or a free monoid with involution. In all cases the result is the
same: the set of all solutions in reduced words is EDT0L. This was known only
for quadratic word equations by Fert\'e, Marin and S\'enizergues (ToCS 2014),
which is a very restricted case. Our general result became possible due to the
recent recompression technique of Je\.z. In this paper we use a new method to
integrate solutions of linear Diophantine equations into the process and obtain
more general results than in the related paper (arXiv 1405.5133). For example,
we improve the complexity from quadratic nondeterministic space in (arXiv
1405.5133) to quasi-linear nondeterministic space here. This implies an
improved complexity for deciding the existential theory of non-abelian free
groups: NSPACE($n\log n$). The conjectured complexity is NP, however, we
believe that our results are optimal with respect to space complexity,
independent of the conjectured NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03430</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03430</id><created>2015-02-11</created><authors><author><keyname>Jakobsen</keyname><forenames>Sune K.</forenames></author><author><keyname>S&#xf8;rensen</keyname><forenames>Troels B.</forenames></author><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author></authors><title>Timeability of Extensive-Form Games</title><categories>cs.GT</categories><comments>28 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensive-form games constitute the standard representation scheme for games
with a temporal component. But do all extensive-form games correspond to
protocols that we can implement in the real world? We often rule out games with
imperfect recall, which prescribe that an agent forget something that she knew
before. In this paper, we show that even some games with perfect recall can be
problematic to implement. Specifically, we show that if the agents have a sense
of time passing (say, access to a clock), then some extensive-form games can no
longer be implemented; no matter how we attempt to time the game, some
information will leak to the agents that they are not supposed to have. We say
such a game is not exactly timeable. We provide easy-to-check necessary and
sufficient conditions for a game to be exactly timeable. Most of the technical
depth of the paper concerns how to approximately time games, which we show can
always be done, though it may require large amounts of time. Specifically, we
show that for some games the time required to approximately implement the game
grows as a power tower of height proportional to the number of players and with
a parameter that measures the precision of the approximation at the top of the
power tower. In practice, that makes the games untimeable. Besides the
conceptual contribution to game theory, we believe our methodology can have
applications to preventing information leakage in security protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03431</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03431</id><created>2015-02-11</created><authors><author><keyname>Coti</keyname><forenames>Camille</forenames></author><author><keyname>Evangelista</keyname><forenames>Sami</forenames></author><author><keyname>Klai</keyname><forenames>Kais</forenames></author></authors><title>Time Petri Net Models for a New Queuless and Uncentralized Resource
  Discovery System</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we detail the model using Petri Nets of a new fully
distributed resource reservation system. The basic idea of the considered
distributed system is to let a user reserve a set of resources on a local
network and to use them, without any specific, central administration component
such as a front-end node. Resources can be, for instance, computing resources
(cores, nodes, GPUs...) or some memory on a server. In order to verify some
qualitative and quantitative properties provided by this system, we need to
model it. We detail the algorithms used by this system and the Petri Net models
we made of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03436</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03436</id><created>2015-02-11</created><updated>2015-10-27</updated><authors><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Yu</keyname><forenames>Felix X.</forenames></author><author><keyname>Feris</keyname><forenames>Rogerio S.</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Choudhary</keyname><forenames>Alok</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>An exploration of parameter redundancy in deep networks with circulant
  projections</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the redundancy of parameters in deep neural networks by replacing
the conventional linear projection in fully-connected layers with the circulant
projection. The circulant structure substantially reduces memory footprint and
enables the use of the Fast Fourier Transform to speed up the computation.
Considering a fully-connected neural network layer with d input nodes, and d
output nodes, this method improves the time complexity from O(d^2) to O(dlogd)
and space complexity from O(d^2) to O(d). The space savings are particularly
important for modern deep convolutional neural network architectures, where
fully-connected layers typically contain more than 90% of the network
parameters. We further show that the gradient computation and optimization of
the circulant projections can be performed very efficiently. Our experiments on
three standard datasets show that the proposed approach achieves this
significant gain in storage and efficiency with minimal increase in error rate
compared to neural networks with unstructured projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03439</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03439</id><created>2015-02-11</created><authors><author><keyname>Kalinin</keyname><forenames>Sergei V.</forenames></author><author><keyname>Maksov</keyname><forenames>Artem</forenames></author></authors><title>What makes us a community: structure, correlations, and success in
  scientific world</title><categories>physics.soc-ph cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the statistical structure of scientific community based on
multivariate analysis of publication (or other identifiable metrics)
distribution in the author space. Here, we define community based on keywords,
i.e. projecting semantic content of the documents on predefined meanings;
however, more complex approaches based on semantic clustering of publications
are possible. Remarkably, this simple statistical analysis of publication
metadata allows understanding of internal interactions with community in
general agreement with experience acquired over decades of social interaction
within it. We further discuss potential applications of this approach for
ranking within the community, reviewer selection, and optimization of community
output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03451</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03451</id><created>2015-02-11</created><authors><author><keyname>Walnut</keyname><forenames>David</forenames></author><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author><author><keyname>Kailath</keyname><forenames>Thomas</forenames></author></authors><title>Cornerstones of Sampling of Operator Theory</title><categories>cs.IT math.FA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews some results on the identifiability of classes of
operators whose Kohn-Nirenberg symbols are band-limited (called band-limited
operators), which we refer to as sampling of operators. We trace the motivation
and history of the subject back to the original work of the third-named author
in the late 1950s and early 1960s, and to the innovations in spread-spectrum
communications that preceded that work. We give a brief overview of the NOMAC
(Noise Modulation and Correlation) and Rake receivers, which were early
implementations of spread-spectrum multi-path wireless communication systems.
We examine in detail the original proof of the third-named author
characterizing identifiability of channels in terms of the maximum time and
Doppler spread of the channel, and do the same for the subsequent
generalization of that work by Bello.
  The mathematical limitations inherent in the proofs of Bello and the third
author are removed by using mathematical tools unavailable at the time. We
survey more recent advances in sampling of operators and discuss the
implications of the use of periodically-weighted delta-trains as identifiers
for operator classes that satisfy Bello's criterion for identifiability,
leading to new insights into the theory of finite-dimensional Gabor systems. We
present novel results on operator sampling in higher dimensions, and review
implications and generalizations of the results to stochastic operators, MIMO
systems, and operators with unknown spreading domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03455</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03455</id><created>2015-02-11</created><updated>2015-04-27</updated><authors><author><keyname>Asghari</keyname><forenames>Vahid</forenames></author><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Dynamic Bandwidth-Efficient BCube Topologies for Virtualized Data Center
  Networks</title><categories>cs.NI cs.PF</categories><comments>16 pages, 4 figures, and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network virtualization enables computing networks and data center (DC)
providers to manage their networking resources in a flexible manner using
software running on physical computers. In this paper, we address the existing
issues with the classic DC network topologies in virtualized environment, and
investigate a set of DC network topologies with the capability of providing
dynamic structures according to the service-level required by the active
traffic in a virtual DC network. In particular, we propose three main
approaches to modify the structure of a classic BCube topology as a topology
benchmark, and investigate their associated structural features and maximum
achievable interconnected bandwidth for different routing scenarios. Finally,
we run an extensive simulation program to check the performance of the proposed
modified topologies in a simulation environment which considers failure
analysis and also traffic congestion. Our simulation experiments, which are
consistent to our design goals, show the efficiency of the proposed modified
topologies comparing to the classic BCube in terms of bandwidth availability
and failure resiliency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03461</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03461</id><created>2015-02-11</created><authors><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Andrieu</keyname><forenames>Vincent</forenames></author><author><keyname>Prieur</keyname><forenames>Christophe</forenames></author></authors><title>Relaxed and hybridized backstepping</title><categories>math.OC cs.SY</categories><journal-ref>IEEE Trans. Aut. Control, vol. 58, 12, pp. 3236-3241, 2013</journal-ref><doi>10.1109/TAC.2013.2263613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present work, we consider nonlinear control systems for which there
exist structural obstacles to the design of classical continuous backstepping
feedback laws. We conceive feedback laws such that the origin of the
closed-loop system is not globally asymptotically stable but a suitable
attractor (strictly containing the origin) is practically asymptotically
stable. A design method is suggested to build a hybrid feedback law combining a
backstepping controller with a locally stabilizing controller. A constructive
approach is also suggested employing a differential inclusion representation of
the nonlinear dynamics. The results are illustrated for a nonlinear system
which, due to its structure, does not have {\it a priori} any globally
stabilizing backstepping controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03469</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03469</id><created>2015-02-11</created><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Shi</keyname><forenames>Shuyu</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Ji</keyname><forenames>Yusheng</forenames></author></authors><title>Optimizing Average-Maximum TTR Trade-off for Cognitive Radio Rendezvous</title><categories>cs.NI</categories><comments>Accepted by IEEE International Conference on Communications (ICC
  2015, http://icc2015.ieee-icc.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio (CR) networks, &quot;TTR&quot;, a.k.a. time-to-rendezvous, is one of
the most important metrics for evaluating the performance of a channel hopping
(CH) rendezvous protocol, and it characterizes the rendezvous delay when two
CRs perform channel hopping. There exists a trade-off of optimizing the average
or maximum TTR in the CH rendezvous protocol design. On one hand, the random CH
protocol leads to the best &quot;average&quot; TTR without ensuring a finite &quot;maximum&quot;
TTR (two CRs may never rendezvous in the worst case), or a high rendezvous
diversity (multiple rendezvous channels). On the other hand, many
sequence-based CH protocols ensure a finite maximum TTR (upper bound of TTR)
and a high rendezvous diversity, while they inevitably yield a larger average
TTR. In this paper, we strike a balance in the average-maximum TTR trade-off
for CR rendezvous by leveraging the advantages of both random and
sequence-based CH protocols. Inspired by the neighbor discovery problem, we
establish a design framework of creating a wake-up schedule whereby every CR
follows the sequence-based (or random) CH protocol in the awake (or asleep)
mode. Analytical and simulation results show that the hybrid CH protocols under
this framework are able to achieve a greatly improved average TTR as well as a
low upper-bound of TTR, without sacrificing the rendezvous diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03471</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03471</id><created>2015-02-11</created><updated>2015-09-10</updated><authors><author><keyname>Fay</keyname><forenames>Stephane</forenames></author><author><keyname>Gautrias</keyname><forenames>Sebastien</forenames></author></authors><title>A scientometric study of General Relativity and Quantum Cosmology from
  2000 to 2012</title><categories>physics.hist-ph cs.DL gr-qc</categories><comments>18 pages, 14 figures, to be published in Scientometrics</comments><journal-ref>Scientometrics, Volume 105, Issue 1 (2015), Page 471-484</journal-ref><doi>10.1007/s11192-015-1674-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  2015 is the centennial of Einstein General Relativity. On this occasion, we
examine the General Relativity and Quantum Cosmology (GRQC) field of research
by analysing 38291 papers uploaded on the electronic archives arXiv.org from
2000 to 2012. We establish a map of the countries contributing to GRQC in 2012.
We determine the main journals publishing GRQC papers and which countries
publish in which journals. We find that more and more papers are written by
groups (instead of single) of authors with more and more international
collaborations. There are huge differences between countries. Hence Russia is
the country where most of papers are written by single authors whereas Canada
is one of the countries where the most of papers imply international
collaborations. We also study authors mobility, determining how some groups of
authors spread worldwide with time in different countries. The largest
mobilities are between USA-UK and USA-Germany. Countries attracting the most of
GRQC authors are Netherlands and Canada whereas those undergoing a brain drain
are Italy and India. There are few mobility between Europe and Asia contrarily
to mobility between USA and Asia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03473</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03473</id><created>2015-02-11</created><updated>2015-12-24</updated><authors><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Karatzoglou</keyname><forenames>Alexandros</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>Online Context-Dependent Clustering in Recommendations based on
  Exploration-Exploitation Algorithms</title><categories>cs.LG cs.AI stat.ML</categories><comments>This paper is duplicate with my other work arXiv:1510.03164</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two context-dependent clustering techniques for content
recommendation based on exploration-exploitation strategies in contextual
multiarmed bandit settings. Our algorithms dynamically group users based on the
items under consideration and, possibly, group items based on the similarity of
the clusterings induced over the users. The resulting algorithm thus takes
advantage of preference patterns in the data in a way akin to collaborative
filtering methods. We provide an empirical analysis on extensive real-world
datasets, showing scalability and increased prediction performance over
state-of-the-art methods for clustering bandits. For one of the two algorithms
we also give a regret analysis within a standard linear stochastic noise
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03475</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03475</id><created>2015-02-11</created><updated>2015-11-05</updated><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Talebi</keyname><forenames>M. Sadegh</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author></authors><title>Combinatorial Bandits Revisited</title><categories>cs.LG math.OC stat.ML</categories><comments>30 pages, Advances in Neural Information Processing Systems 28 (NIPS
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates stochastic and adversarial combinatorial multi-armed
bandit problems. In the stochastic setting under semi-bandit feedback, we
derive a problem-specific regret lower bound, and discuss its scaling with the
dimension of the decision space. We propose ESCB, an algorithm that efficiently
exploits the structure of the problem and provide a finite-time analysis of its
regret. ESCB has better performance guarantees than existing algorithms, and
significantly outperforms these algorithms in practice. In the adversarial
setting under bandit feedback, we propose \textsc{CombEXP}, an algorithm with
the same regret scaling as state-of-the-art algorithms, but with lower
computational complexity for some combinatorial problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03482</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03482</id><created>2015-02-11</created><updated>2015-10-06</updated><authors><author><keyname>Thapper</keyname><forenames>Johan</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>Necessary conditions for tractability of valued CSPs</title><categories>cs.CC</categories><comments>To appear in SIAM Journal on Discrete Mathematics (SIDMA)</comments><acm-class>F.2.0</acm-class><journal-ref>SIAM Journal on Discrete Mathematics 29(4) (2015) 2361-2384</journal-ref><doi>10.1137/140990346</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connection between constraint languages and clone theory has been a
fruitful line of research on the complexity of constraint satisfaction
problems. In a recent result, Cohen et al. [SICOMP'13] have characterised a
Galois connection between valued constraint languages and so-called weighted
clones. In this paper, we study the structure of weighted clones. We extend the
results of Creed and Zivny from [CP'11/SICOMP'13] on types of weightings
necessarily contained in every nontrivial weighted clone. This result has
immediate computational complexity consequences as it provides necessary
conditions for tractability of weighted clones and thus valued constraint
languages. We demonstrate that some of the necessary conditions are also
sufficient for tractability, while others are provably not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03487</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03487</id><created>2015-02-11</created><authors><author><keyname>Dowsley</keyname><forenames>Rafael</forenames></author><author><keyname>M&#xfc;ller-Quade</keyname><forenames>J&#xf6;rn</forenames></author><author><keyname>Nilges</keyname><forenames>Tobias</forenames></author></authors><title>Weakening the Isolation Assumption of Tamper-proof Hardware Tokens</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results have shown the usefulness of tamper-proof hardware tokens as a
setup assumption for building UC-secure two-party computation protocols, thus
providing broad security guarantees and allowing the use of such protocols as
buildings blocks in the modular design of complex cryptography protocols. All
these works have in common that they assume the tokens to be completely
isolated from their creator, but this is a strong assumption. In this work we
investigate the feasibility of cryptographic protocols in the setting where the
isolation of the hardware token is weakened.
  We consider two cases: (1) the token can relay messages to its creator, or
(2) the creator can send messages to the token after it is sent to the
receiver. We provide a detailed characterization for both settings, presenting
both impossibilities and information-theoretically secure solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03491</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03491</id><created>2015-02-11</created><authors><author><keyname>Chakraborty</keyname><forenames>Mithun</forenames></author><author><keyname>Das</keyname><forenames>Sanmay</forenames></author><author><keyname>Lavoie</keyname><forenames>Allen</forenames></author></authors><title>How to show a probabilistic model is better</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple theoretical framework, and corresponding practical
procedures, for comparing probabilistic models on real data in a traditional
machine learning setting. This framework is based on the theory of proper
scoring rules, but requires only basic algebra and probability theory to
understand and verify. The theoretical concepts presented are well-studied,
primarily in the statistics literature. The goal of this paper is to advocate
their wider adoption for performance evaluation in empirical machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03492</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03492</id><created>2015-02-11</created><updated>2015-04-02</updated><authors><author><keyname>Maclaurin</keyname><forenames>Dougal</forenames></author><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Gradient-based Hyperparameter Optimization through Reversible Learning</title><categories>stat.ML cs.LG</categories><comments>10 figures. Submitted to ICML</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tuning hyperparameters of learning algorithms is hard because gradients are
usually unavailable. We compute exact gradients of cross-validation performance
with respect to all hyperparameters by chaining derivatives backwards through
the entire training procedure. These gradients allow us to optimize thousands
of hyperparameters, including step-size and momentum schedules, weight
initialization distributions, richly parameterized regularization schemes, and
neural network architectures. We compute hyperparameter gradients by exactly
reversing the dynamics of stochastic gradient descent with momentum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03493</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03493</id><created>2015-02-11</created><authors><author><keyname>Lin</keyname><forenames>Jiun-Ren</forenames></author><author><keyname>Talty</keyname><forenames>Timothy</forenames></author><author><keyname>Tonguz</keyname><forenames>Ozan K.</forenames></author></authors><title>On the Potential of Bluetooth Low Energy Technology for Vehicular
  Applications</title><categories>cs.NI</categories><comments>8 pages, 5 figures</comments><journal-ref>IEEE Communications Magazine, vol. 53, no. 1, January 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing number of sensors in modern vehicles, using an
Intra-Vehicular Wireless Sensor Network (IVWSN) is a possible solution for the
automotive industry to address the potential issues that arise from additional
wiring harness. Such a solution could help car manufacturers develop vehicles
that have better fuel economy and performance, in addition to supporting new
applications. However, which wireless technology for IVWSNs should be used for
maximizing the aforementioned benefits is still an open issue. In this paper,
we propose to use a new wireless technology known as Bluetooth Low Energy (BLE)
and highlight a new architecture for IVWSN. Based on a comprehensive study
which encompasses an example application, it is shown that BLE is an excellent
option that can be used in IVWSNs for certain applications mainly due to its
good performance and low-power, low-complexity, and low-cost attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03496</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03496</id><created>2015-02-11</created><authors><author><keyname>Cheng</keyname><forenames>Dehua</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Spectral Sparsification of Random-Walk Matrix Polynomials</title><categories>cs.DS cs.DM cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fundamental algorithmic question in spectral graph theory:
Compute a spectral sparsifier of random-walk matrix-polynomial
$$L_\alpha(G)=D-\sum_{r=1}^d\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacency
matrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighted
degrees, and $\alpha=(\alpha_1...\alpha_d)$ are nonnegative coefficients with
$\sum_{r=1}^d\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix of
random walks on the graph. The sparsification of $L_\alpha(G)$ appears to be
algorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by all
paths of length $r$, whose precise calculation would be prohibitively
expensive.
  In this paper, we develop the first nearly linear time algorithm for this
sparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$
coefficients $\alpha$, and $\epsilon &gt; 0$, our algorithm runs in time
$O(d^2m\log^2n/\epsilon^{2})$ to construct a Laplacian matrix
$\tilde{L}=D-\tilde{A}$ with $O(n\log n/\epsilon^{2})$ non-zeros such that
$\tilde{L}\approx_{\epsilon}L_\alpha(G)$.
  Matrix polynomials arise in mathematical analysis of matrix functions as well
as numerical solutions of matrix equations. Our work is particularly motivated
by the algorithmic problems for speeding up the classic Newton's method in
applications such as computing the inverse square-root of the precision matrix
of a Gaussian random field, as well as computing the $q$th-root transition (for
$q\geq1$) in a time-reversible Markov model. The key algorithmic step for both
applications is the construction of a spectral sparsifier of a constant degree
random-walk matrix-polynomials introduced by Newton's method. Our algorithm can
also be used to build efficient data structures for effective resistances for
multi-step time-reversible Markov models, and we anticipate that it could be
useful for other tasks in network analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03504</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03504</id><created>2015-02-11</created><authors><author><keyname>Rasmussen</keyname><forenames>Craig</forenames></author><author><keyname>Sottile</keyname><forenames>Matthew</forenames></author><author><keyname>Nagle</keyname><forenames>Daniel</forenames></author><author><keyname>Rasmussen</keyname><forenames>Soren</forenames></author></authors><title>Locally-Oriented Programming: A Simple Programming Model for
  Stencil-Based Computations on Multi-Level Distributed Memory Architectures</title><categories>cs.PL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging hybrid accelerator architectures for high performance computing are
often suited for the use of a data-parallel programming model. Unfortunately,
programmers of these architectures face a steep learning curve that frequently
requires learning a new language (e.g., OpenCL). Furthermore, the distributed
(and frequently multi-level) nature of the memory organization of clusters of
these machines provides an additional level of complexity. This paper presents
preliminary work examining how programming with a local orientation can be
employed to provide simpler access to accelerator architectures. A
locally-oriented programming model is especially useful for the solution of
algorithms requiring the application of a stencil or convolution kernel. In
this programming model, a programmer codes the algorithm by modifying only a
single array element (called the local element), but has read-only access to a
small sub-array surrounding the local element. We demonstrate how a
locally-oriented programming model can be adopted as a language extension using
source-to-source program transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03505</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03505</id><created>2015-02-11</created><authors><author><keyname>Yger</keyname><forenames>Florian</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Supervised LogEuclidean Metric Learning for Symmetric Positive Definite
  Matrices</title><categories>cs.LG</categories><comments>19 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric learning has been shown to be highly effective to improve the
performance of nearest neighbor classification. In this paper, we address the
problem of metric learning for Symmetric Positive Definite (SPD) matrices such
as covariance matrices, which arise in many real-world applications. Naively
using standard Mahalanobis metric learning methods under the Euclidean geometry
for SPD matrices is not appropriate, because the difference of SPD matrices can
be a non-SPD matrix and thus the obtained solution can be uninterpretable. To
cope with this problem, we propose to use a properly parameterized LogEuclidean
distance and optimize the metric with respect to kernel-target alignment, which
is a supervised criterion for kernel learning. Then the resulting non-trivial
optimization problem is solved by utilizing the Riemannian geometry. Finally,
we experimentally demonstrate the usefulness of our LogEuclidean metric
learning algorithm on real-world classification tasks for EEG signals and
texture patches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03508</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03508</id><created>2015-02-11</created><updated>2015-07-03</updated><authors><author><keyname>Ma</keyname><forenames>Chenxin</forenames></author><author><keyname>Smith</keyname><forenames>Virginia</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Adding vs. Averaging in Distributed Primal-Dual Optimization</title><categories>cs.LG</categories><comments>ICML 2015: JMLR W&amp;CP volume37, Proceedings of The 32nd International
  Conference on Machine Learning, pp. 1973-1982</comments><msc-class>90C25, 68W15</msc-class><acm-class>G.1.6; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed optimization methods for large-scale machine learning suffer from
a communication bottleneck. It is difficult to reduce this bottleneck while
still efficiently and accurately aggregating partial work from different
machines. In this paper, we present a novel generalization of the recent
communication-efficient primal-dual framework (CoCoA) for distributed
optimization. Our framework, CoCoA+, allows for additive combination of local
updates to the global parameters at each iteration, whereas previous schemes
with convergence guarantees only allow conservative averaging. We give stronger
(primal-dual) convergence rate guarantees for both CoCoA as well as our new
variants, and generalize the theory for both methods to cover non-smooth convex
loss functions. We provide an extensive experimental comparison that shows the
markedly improved performance of CoCoA+ on several real-world distributed
datasets, especially when scaling up the number of machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03509</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03509</id><created>2015-02-11</created><updated>2015-06-05</updated><authors><author><keyname>Germain</keyname><forenames>Mathieu</forenames></author><author><keyname>Gregor</keyname><forenames>Karol</forenames></author><author><keyname>Murray</keyname><forenames>Iain</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>MADE: Masked Autoencoder for Distribution Estimation</title><categories>cs.LG cs.NE stat.ML</categories><comments>9 pages and 1 page of supplementary material. Updated to match
  published version</comments><journal-ref>Proceedings of the 32nd International Conference on Machine
  Learning, JMLR W&amp;CP 37:881-889, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a lot of recent interest in designing neural network models to
estimate a distribution from a set of examples. We introduce a simple
modification for autoencoder neural networks that yields powerful generative
models. Our method masks the autoencoder's parameters to respect autoregressive
constraints: each input is reconstructed only from previous inputs in a given
ordering. Constrained this way, the autoencoder outputs can be interpreted as a
set of conditional probabilities, and their product, the full joint
probability. We can also train a single network that can decompose the joint
probability in multiple different orderings. Our simple framework can be
applied to multiple architectures, including deep ones. Vectorized
implementations, such as on GPUs, are simple and fast. Experiments demonstrate
that this approach is competitive with state-of-the-art tractable distribution
estimators. At test time, the method is significantly faster and scales better
than other autoregressive estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03512</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03512</id><created>2015-02-11</created><authors><author><keyname>Autili</keyname><forenames>Marco</forenames><affiliation>University of L'Aqula</affiliation></author><author><keyname>Tivoli</keyname><forenames>Massimo</forenames><affiliation>University of L'Aquila</affiliation></author></authors><title>Distributed Enforcement of Service Choreographies</title><categories>cs.SE</categories><comments>In Proceedings FOCLASA 2014, arXiv:1502.03157</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 175, 2015, pp. 18-35</journal-ref><doi>10.4204/EPTCS.175.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern service-oriented systems are often built by reusing, and composing
together, existing services distributed over the Internet. Service choreography
is a possible form of service composition whose goal is to specify the
interactions among participant services from a global perspective. In this
paper, we formalize a method for the distributed and automated enforcement of
service choreographies, and prove its correctness with respect to the
realization of the specified choreography. The formalized method is implemented
as part of a model-based tool chain released to support the development of
choreography-based systems within the EU CHOReOS project. We illustrate our
method at work on a distributed social proximity network scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03513</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03513</id><created>2015-02-11</created><authors><author><keyname>Darquennes</keyname><forenames>Denis</forenames></author><author><keyname>Jacquet</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Linden</keyname><forenames>Isabelle</forenames></author></authors><title>On Distributed Density in Tuple-based Coordination Languages</title><categories>cs.PL cs.DC cs.MA</categories><comments>In Proceedings FOCLASA 2014, arXiv:1502.03157</comments><proxy>EPTCS</proxy><acm-class>D.1.3; D.3.2; F.1.2</acm-class><journal-ref>EPTCS 175, 2015, pp. 36-53</journal-ref><doi>10.4204/EPTCS.175.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the chemical metaphor, this paper proposes an extension of
Linda-like languages in the aim of modeling the coordination of complex
distributed systems. The new language manipulates finite sets of tuples and
distributes a density among them. This new concept adds to the non-determinism
inherent in the selection of matched tuples a non-determinism to the tell, ask
and get primitives on the consideration of different tuples. Furthermore,
thanks to de Boer and Palamidessi's notion of modular embedding, we establish
that this new language strictly increases the expressiveness of the Dense Bach
language introduced earlier and, consequently, Linda-like languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03514</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03514</id><created>2015-02-11</created><authors><author><keyname>Cassar</keyname><forenames>Ian</forenames></author><author><keyname>Francalanza</keyname><forenames>Adrian</forenames></author></authors><title>On Synchronous and Asynchronous Monitor Instrumentation for Actor-based
  systems</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FOCLASA 2014, arXiv:1502.03157</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 175, 2015, pp. 54-68</journal-ref><doi>10.4204/EPTCS.175.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the impact of synchronous and asynchronous monitoring
instrumentation on runtime overheads in the context of a runtime verification
framework for actor-based systems. We show that, in such a context,
asynchronous monitoring incurs substantially lower overhead costs. We also show
how, for certain properties that require synchronous monitoring, a hybrid
approach can be used that ensures timely violation detections for the important
events while, at the same time, incurring lower overhead costs that are closer
to those of an asynchronous instrumentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03515</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03515</id><created>2015-02-11</created><authors><author><keyname>Henrio</keyname><forenames>Ludovic</forenames><affiliation>Univ. of Nice Sophia Antipolis, CNRS, France</affiliation></author><author><keyname>Kulankhina</keyname><forenames>Oleksandra</forenames><affiliation>NRIA Sophia Antipolis Mediterannee, Univ. of Nice Sophia Antipolis, CNRS, France</affiliation></author><author><keyname>Liu</keyname><forenames>Dongqian</forenames><affiliation>MoE Engineering Research Center for Software and Hardware Co-design Technology and Application, ECNU, China</affiliation></author><author><keyname>Madelaine</keyname><forenames>Eric</forenames><affiliation>NRIA Sophia Antipolis Mediterannee, Univ. of Nice Sophia Antipolis, CNRS, France</affiliation></author></authors><title>Verifying the correct composition of distributed components:
  Formalisation and Tool</title><categories>cs.DC cs.SE</categories><comments>In Proceedings FOCLASA 2014, arXiv:1502.03157</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 175, 2015, pp. 69-85</journal-ref><doi>10.4204/EPTCS.175.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides formal definitions characterizing well-formed
composition of components in order to guarantee their safe deployment and
execution. Our work focuses on the structural aspects of component composition;
it puts together most of the concepts common to many component models, but
never formalized as a whole. Our formalization characterizes correct component
architectures made of functional and non-functional aspects, both structured as
component assemblies. Interceptor chains can be used for a safe and controlled
interaction between the two aspects. Our well-formed components guarantee a set
of properties ensuring that the deployed component system has a correct
architecture and can run safely. Finally, those definitions constitute the
formal basis for our Eclipse-based environment for the development and
specification of component-based applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03517</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03517</id><created>2015-02-11</created><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Fairest Constant Sum-rate Transmission for Cooperative Data Exchange: An
  M-convex Minimization Approach</title><categories>cs.IT cs.DM math.IT</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fairness in cooperative data exchange (CDE) problem among a
set of wireless clients. In this system, each client initially obtains a subset
of the packets. They exchange packets in order to reconstruct the entire packet
set. We study the problem of how to find a transmission strategy that
distributes the communication load most evenly in all strategies that have the
same sum-rate (the total number of transmissions) and achieve universal
recovery (the situation when all clients recover the packet set). We formulate
this problem by a discrete minimization problem and prove its $M$-convexity. We
show that our results can also be proved by the submodularity of the feasible
region shown in previous works and are closely related to the resource
allocation problems under submodular constraints. To solve this problem, we
propose to use a steepest descent algorithm (SDA) based on $M$-convexity. By
varying the number of clients and packets, we compare SDA with a deterministic
algorithm (DA) based on submodularity in terms of convergence performance and
complexity. The results show that for the problem of finding the fairest and
minimum sum-rate strategy for the CDE problem SDA is more efficient than DA
when the number of clients is up to five.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03518</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03518</id><created>2015-02-11</created><updated>2015-07-20</updated><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Estimating Minimum Sum-rate for Cooperative Data Exchange</title><categories>cs.IT cs.DM math.IT</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers how to accurately estimate the minimum sum-rate so as to
reduce the complexity of solving cooperative data exchange (CDE) problems. The
CDE system contains a number of geographically close clients who send packets
to help the others recover an entire packet set. The minimum sum-rate is the
minimum value of total number of transmissions that achieves universal recovery
(the situation when all the clients recover the whole packet set). Based on a
necessary and sufficient condition for a supermodular base polyhedron to be
nonempty, we show that the minimum sum-rate for a CDE system can be determined
by a maximization over all possible partitions of the client set. Due to the
high complexity of solving this maximization problem, we propose a
deterministic algorithm to approximate a lower bound on the minimum sum-rate.
We show by experiments that this lower bound is much tighter than those lower
bounds derived in the existing literature. We also show that the deterministic
algorithm prevents from repetitively running the existing algorithms for
solving CDE problems so that the overall complexity can be reduced accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03519</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03519</id><created>2015-02-11</created><authors><author><keyname>Dong</keyname><forenames>Xin Luna</forenames></author><author><keyname>Gabrilovich</keyname><forenames>Evgeniy</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Dang</keyname><forenames>Van</forenames></author><author><keyname>Horn</keyname><forenames>Wilko</forenames></author><author><keyname>Lugaresi</keyname><forenames>Camillo</forenames></author><author><keyname>Sun</keyname><forenames>Shaohua</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources</title><categories>cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of web sources has been traditionally evaluated using exogenous
signals such as the hyperlink structure of the graph. We propose a new approach
that relies on endogenous signals, namely, the correctness of factual
information provided by the source. A source that has few false facts is
considered to be trustworthy. The facts are automatically extracted from each
source by information extraction methods commonly used to construct knowledge
bases. We propose a way to distinguish errors made in the extraction process
from factual errors in the web source per se, by using joint inference in a
novel multi-layer probabilistic model. We call the trustworthiness score we
computed Knowledge-Based Trust (KBT). On synthetic data, we show that our
method can reliably compute the true trustworthiness levels of the sources. We
then apply it to a database of 2.8B facts extracted from the web, and thereby
estimate the trustworthiness of 119M webpages. Manual evaluation of a subset of
the results confirms the effectiveness of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03520</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03520</id><created>2015-02-11</created><updated>2016-02-23</updated><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Yuanzhi</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>RAND-WALK: A Latent Variable Model Approach to Word Embeddings</title><categories>cs.LG cs.CL stat.ML</categories><comments>refinement of the theory with tighter bound on the errors and fix of
  a bug in the proof of earlier version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic word embeddings represent the meaning of a word via a vector, and
are created by diverse methods such as Latent Semantic Analysis (LSA),
generative text models such as topic models, matrix factorization, neural nets,
and energy-based models. Many methods use nonlinear operations ---such as
Pairwise Mutual Information or PMI--- on co-occurrence statistics, and have
hand-tuned hyperparameters and reweightings.
  Often a {\em generative model} can help provide theoretical insight into such
modeling choices, but there appears to be no such model to &quot;explain&quot; the above
nonlinear models. For example, we know of no generative model for which the
correct solution is the usual (dimension-restricted) PMI model.
  This paper gives a new generative model, a dynamic version of the loglinear
topic model of \citet{mnih2007three}. The methodological novelty is to use the
prior to compute {\em closed form} expressions for word statistics. These
provide an explanation for nonlinear models like PMI, {\bf word2vec}, and
GloVe, as well as some hyperparameter choices.
  Experimental support is provided for the generative model assumptions, the
most important of which is that latent word vectors are fairly uniformly
dispersed (&quot;isotropic&quot;) in space.
  The model also helps explain why low-dimensional semantic embeddings contain
linear algebraic structure that allows solution of word analogies, as shown
by~\citet{mikolov2013efficient} and many subsequent papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03526</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03526</id><created>2015-02-11</created><authors><author><keyname>Kannangara</keyname><forenames>S. H.</forenames></author><author><keyname>Wijayanayake</keyname><forenames>W. M. J. I.</forenames></author></authors><title>An Empirical Evaluation of Impact of Refactoring On Internal and
  External Measures of Code Quality</title><categories>cs.SE</categories><journal-ref>IJSEA Journal, Vol.6, No.1, January 2015, PP. 51 - 67</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Refactoring is the process of improving the design of existing code by
changing its internal structure without affecting its external behaviour, with
the main aims of improving the quality of software product. Therefore, there is
a belief that refactoring improves quality factors such as understandability,
flexibility, and reusability. However, there is limited empirical evidence to
support such assumptions. The objective of this study is to validate/invalidate
the claims that refactoring improves software quality. The impact of selected
refactoring techniques was assessed using both external and internal measures.
Ten refactoring techniques were evaluated through experiments to assess
external measures: Resource Utilization, Time Behaviour, Changeability and
Analysability which are ISO external quality factors and five internal
measures: Maintainability Index, Cyclomatic Complexity, Depth of Inheritance,
Class Coupling and Lines of Code. The result of external measures did not show
any improvements in code quality after the refactoring treatment. However, from
internal measures, maintainability index indicated an improvement in code
quality of refactored code than non-refactored code and other internal measures
did not indicate any positive effect on refactored code
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03529</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03529</id><created>2015-02-11</created><updated>2015-07-20</updated><authors><author><keyname>Zhao</keyname><forenames>Shen-Yi</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Scalable Stochastic Alternating Direction Method of Multipliers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic alternating direction method of multipliers (ADMM), which visits
only one sample or a mini-batch of samples each time, has recently been proved
to achieve better performance than batch ADMM. However, most stochastic methods
can only achieve a convergence rate $O(1/\sqrt T)$ on general convex
problems,where T is the number of iterations. Hence, these methods are not
scalable with respect to convergence rate (computation cost). There exists only
one stochastic method, called SA-ADMM, which can achieve convergence rate
$O(1/T)$ on general convex problems. However, an extra memory is needed for
SA-ADMM to store the historic gradients on all samples, and thus it is not
scalable with respect to storage cost. In this paper, we propose a novel
method, called scalable stochastic ADMM(SCAS-ADMM), for large-scale
optimization and learning problems. Without the need to store the historic
gradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the best
stochastic method SA-ADMM and batch ADMM on general convex problems.
Experiments on graph-guided fused lasso show that SCAS-ADMM can achieve
state-of-the-art performance in real applications
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03530</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03530</id><created>2015-02-11</created><authors><author><keyname>Joseph</keyname><forenames>Renien John</forenames></author></authors><title>Single Page Application and Canvas Drawing</title><categories>cs.HC cs.SE</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, with the impact of AJAX a new way of web development techniques
have been emerged. Hence, with the help of this model, single-page web
application was introduced which can be updated/replaced independently. Today
we have a new challenge of building a powerful single-page application using
the currently emerged technologies. Gaining an understanding of navigational
model and user interface structure of the source application is the first step
to successfully build a single- page application. In this paper, it explores
not only building powerful single-page application but also Two Dimensional
(2D) drawings on images and videos. Moreover, in this research it clearly
express the findings on 2D multi-points polygon drawing concepts on client
side; real-time data binding in between drawing module on image, video and view
pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03531</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03531</id><created>2015-02-11</created><authors><author><keyname>Lu</keyname><forenames>Jingyang</forenames></author><author><keyname>Niu</keyname><forenames>Ruixin</forenames></author></authors><title>A State Estimation and Malicious Attack Game in Multi-Sensor Dynamic
  Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of false information injection attack and defense
on state estimation in dynamic multi-sensor systems is investigated from a game
theoretic perspective. The relationship between the Kalman filter and the
adversary can be regarded as a two-person zero-sum game. Under which condition
both sides of the game will reach the Nash equilibrium is investigated in the
paper. The multi-sensor Kalman filter system and the adversary are supposed to
be rational players. The Kalman filter and the adversary have to choose their
respective subsets of sensors to perform system state estimation and false
information injection. It is shown how both sides pick their strategies in
order to gain more and lose less. The optimal solutions are achieved by solving
the minimax problem. Numerical results are also provided in order to illustrate
the effectiveness of the derived optimal strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03532</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03532</id><created>2015-02-11</created><authors><author><keyname>Cao</keyname><forenames>Lijun</forenames></author><author><keyname>Chen</keyname><forenames>Weihua</forenames></author><author><keyname>Chen</keyname><forenames>Xiaotang</forenames></author><author><keyname>Zheng</keyname><forenames>Shuai</forenames></author><author><keyname>Huang</keyname><forenames>Kaiqi</forenames></author></authors><title>An equalised global graphical model-based approach for multi-camera
  object tracking</title><categories>cs.CV</categories><comments>13 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-camera non-overlapping visual object tracking system typically consists
of two tasks: single camera object tracking and inter-camera object tracking.
Since the state-of-the-art approaches are yet not perform perfectly in real
scenes, the errors in single camera object tracking module would propagate into
the module of inter-camera object tracking, resulting much lower overall
performance. In order to address this problem, we develop an approach that
jointly optimise the single camera object tracking and inter-camera object
tracking in an equalised global graphical model. Such an approach has the
advantage of guaranteeing a good overall tracking performance even when there
are limited amount of false tracking in single camera object tracking. Besides,
the similarity metrics used in our approach improve the compatibility of the
metrics used in the two different tasks. Results show that our approach achieve
the state-of-the-art results in multi-camera non-overlapping tracking datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03536</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03536</id><created>2015-02-11</created><authors><author><keyname>Hinrichs</keyname><forenames>Chris</forenames></author><author><keyname>Ithapu</keyname><forenames>Vamsi K</forenames></author><author><keyname>Sun</keyname><forenames>Qinyuan</forenames></author><author><keyname>Johnson</keyname><forenames>Sterling C</forenames></author><author><keyname>Singh</keyname><forenames>Vikas</forenames></author></authors><title>Speeding up Permutation Testing in Neuroimaging</title><categories>stat.CO cs.AI stat.ML</categories><comments>NIPS 13</comments><journal-ref>Advances in neural information processing systems (2013), pp.
  890-898</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple hypothesis testing is a significant problem in nearly all
neuroimaging studies. In order to correct for this phenomena, we require a
reliable estimate of the Family-Wise Error Rate (FWER). The well known
Bonferroni correction method, while simple to implement, is quite conservative,
and can substantially under-power a study because it ignores dependencies
between test statistics. Permutation testing, on the other hand, is an exact,
non-parametric method of estimating the FWER for a given $\alpha$-threshold,
but for acceptably low thresholds the computational burden can be prohibitive.
In this paper, we show that permutation testing in fact amounts to populating
the columns of a very large matrix ${\bf P}$. By analyzing the spectrum of this
matrix, under certain conditions, we see that ${\bf P}$ has a low-rank plus a
low-variance residual decomposition which makes it suitable for highly
sub--sampled --- on the order of $0.5\%$ --- matrix completion methods. Based
on this observation, we propose a novel permutation testing methodology which
offers a large speedup, without sacrificing the fidelity of the estimated FWER.
Our evaluations on four different neuroimaging datasets show that a
computational speedup factor of roughly $50\times$ can be achieved while
recovering the FWER distribution up to very high accuracy. Further, we show
that the estimated $\alpha$-threshold is also recovered faithfully, and is
stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03537</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03537</id><created>2015-02-11</created><authors><author><keyname>Ithapu</keyname><forenames>Vamsi K</forenames></author><author><keyname>Ravi</keyname><forenames>Sathya</forenames></author><author><keyname>Singh</keyname><forenames>Vikas</forenames></author></authors><title>Convergence of gradient based pre-training in Denoising autoencoders</title><categories>cs.LG cs.CV math.OC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of deep architectures is at least in part attributed to the
layer-by-layer unsupervised pre-training that initializes the network. Various
papers have reported extensive empirical analysis focusing on the design and
implementation of good pre-training procedures. However, an understanding
pertaining to the consistency of parameter estimates, the convergence of
learning procedures and the sample size estimates is still unavailable in the
literature. In this work, we study pre-training in classical and distributed
denoising autoencoders with these goals in mind. We show that the gradient
converges at the rate of $\frac{1}{\sqrt{N}}$ and has a sub-linear dependence
on the size of the autoencoder network. In a distributed setting where disjoint
sections of the whole network are pre-trained synchronously, we show that the
convergence improves by at least $\tau^{3/4}$, where $\tau$ corresponds to the
size of the sections. We provide a broad set of experiments to empirically
evaluate the suggested behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03540</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03540</id><created>2015-02-11</created><authors><author><keyname>K&#xf6;nig</keyname><forenames>Daniel</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author></authors><title>Evaluating Matrix Circuits</title><categories>cs.CC math.GR</categories><msc-class>20F10, 68Q17, 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The circuit evaluation problem (also known as the compressed word problem)
for finitely generated linear groups is studied. The best upper bound for this
problem is $\mathsf{coRP}$, which is shown by a reduction to polynomial
identity testing. Conversely, the compressed word problem for the linear group
$\mathsf{SL}_3(\mathbb{Z})$ is equivalent to polynomial identity testing. In
the paper, it is shown that the compressed word problem for every finitely
generated nilpotent group is in $\mathsf{DET} \subseteq \mathsf{NC}^2$. Within
the larger class of polycyclic groups we find examples where the compressed
word problem is at least as hard as polynomial identity testing for skew
arithmetic circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03543</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03543</id><created>2015-02-12</created><authors><author><keyname>Divakar</keyname><forenames>Nithish</forenames></author></authors><title>Primal Dual Affine Scaling on GPUs</title><categories>cs.NA cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we present an implementation of Primal-Dual Affine scaling method to
solve linear optimization problem on GPU based systems. Strategies to convert
the system generated by complementary slackness theorem into a symmetric system
are given. A new CUDA friendly technique to solve the resulting symmetric
positive definite subsystem is also developed. Various strategies to reduce the
memory transfer and storage requirements were also explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03544</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03544</id><created>2015-02-12</created><updated>2015-02-13</updated><authors><author><keyname>Al-Wattar</keyname><forenames>Auday H.</forenames></author><author><keyname>Mahmod</keyname><forenames>Ramlan</forenames></author><author><keyname>Zukarnain</keyname><forenames>Zuriati Ahmad</forenames></author><author><keyname>Udzir</keyname><forenames>Nur Izura</forenames></author></authors><title>A New DNA-Based Approach of Generating Key-dependent ShiftRows
  Transformation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of key-dependent shiftRows can be considered as one of the applied
methods for altering the quality of a cryptographic algorithm. This article
describes one approach for changing the ShiftRows transformation employed in
the algorithm AES. The approach employs methods inspired from DNA processes and
structure which depended on the key while the parameters of the created new
ShiftRows have characteristics identical to those of the original algorithm AES
in addition to increase its resistance against attacks. The proposed new
ShiftRows were tested for coefficient correlation for dynamic and static
independence between the input and output. The NIST Test Suite tests were used
to test the randomness for the block cipher that used the new transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03552</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03552</id><created>2015-02-12</created><authors><author><keyname>Dilek</keyname><forenames>Selma</forenames></author><author><keyname>&#xc7;ak&#x131;r</keyname><forenames>H&#xfc;seyin</forenames></author><author><keyname>Ayd&#x131;n</keyname><forenames>Mustafa</forenames></author></authors><title>Applications of Artificial Intelligence Techniques to Combating Cyber
  Crimes: A Review</title><categories>cs.AI cs.CR cs.CY</categories><comments>19 pages, a survey, in International Journal of Artificial
  Intelligence &amp; Applications (IJAIA), Vol. 6, No. 1, January 2015</comments><msc-class>68-02</msc-class><acm-class>A.1</acm-class><doi>10.5121/ijaia.2015.6102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advances in information technology (IT) criminals are using
cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly
vulnerable to intrusions and other threats. Physical devices and human
intervention are not sufficient for monitoring and protection of these
infrastructures; hence, there is a need for more sophisticated cyber defense
systems that need to be flexible, adaptable and robust, and able to detect a
wide variety of threats and make intelligent real-time decisions. Numerous
bio-inspired computing methods of Artificial Intelligence have been
increasingly playing an important role in cyber crime detection and prevention.
The purpose of this study is to present advances made so far in the field of
applying AI techniques for combating cyber crimes, to demonstrate how these
techniques can be an effective tool for detection and prevention of cyber
attacks, as well as to give the scope for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03556</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03556</id><created>2015-02-12</created><authors><author><keyname>Seddiqui</keyname><forenames>Md. Hanif</forenames></author><author><keyname>Nath</keyname><forenames>Rudra Pratap Deb</forenames></author><author><keyname>Aono</keyname><forenames>Masaki</forenames></author></authors><title>An Efficient Metric of Automatic Weight Generation for Properties in
  Instance Matching Technique</title><categories>cs.AI</categories><comments>17 pages, 5 figures, 3 tables, pp. 1-17, publication year 2015,
  journal publication, vol. 6 number 1</comments><msc-class>68T30</msc-class><acm-class>H.2.8; H.3.2; I.2.4; I.2.6</acm-class><journal-ref>Journal of Web and Semantic Technology (IJWeST), vol.6 no.1, pp.
  1-17 (2015)</journal-ref><doi>10.5121/ijwest.2015.6101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of heterogeneous data sources of semantic knowledge base
intensifies the need of an automatic instance matching technique. However, the
efficiency of instance matching is often influenced by the weight of a property
associated to instances. Automatic weight generation is a non-trivial, however
an important task in instance matching technique. Therefore, identifying an
appropriate metric for generating weight for a property automatically is
nevertheless a formidable task. In this paper, we investigate an approach of
generating weights automatically by considering hypotheses: (1) the weight of a
property is directly proportional to the ratio of the number of its distinct
values to the number of instances contain the property, and (2) the weight is
also proportional to the ratio of the number of distinct values of a property
to the number of instances in a training dataset. The basic intuition behind
the use of our approach is the classical theory of information content that
infrequent words are more informative than frequent ones. Our mathematical
model derives a metric for generating property weights automatically, which is
applied in instance matching system to produce re-conciliated instances
efficiently. Our experiments and evaluations show the effectiveness of our
proposed metric of automatic weight generation for properties in an instance
matching technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03561</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03561</id><created>2015-02-12</created><updated>2015-05-13</updated><authors><author><keyname>Ari</keyname><forenames>Ado Adamou Abba</forenames><affiliation>PRiSM, University of Versailles St-Quentin-en-Yvelines France</affiliation><affiliation>University of Maroua Cameroon</affiliation></author><author><keyname>Gueroui</keyname><forenames>Abdelhak</forenames><affiliation>PRiSM, University of Versailles St-Quentin-en-Yvelines France</affiliation></author><author><keyname>Labraoui</keyname><forenames>Nabila</forenames><affiliation>STIC University of Tlemcen Algeria</affiliation></author><author><keyname>Yenke</keyname><forenames>Blaise Omer</forenames><affiliation>LASE University of Ngaoundere Cameroon</affiliation></author></authors><title>Concepts and evolution of research in the field of wireless sensor
  networks</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Networks &amp; Communications. 7.1
  (2015) 81-98</journal-ref><doi>10.5121/ijcnc.2015.7106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of Wireless Sensor Networks (WSNs) is experiencing a resurgence of
interest and a continuous evolution in the scientific and industrial community.
The use of this particular type of ad hoc network is becoming increasingly
important in many contexts, regardless of geographical position and so,
according to a set of possible application. WSNs offer interesting low cost and
easily deployable solutions to perform a remote real time monitoring, target
tracking and recognition of physical phenomenon. The uses of these sensors
organized into a network continue to reveal a set of research questions
according to particularities target applications. Despite difficulties
introduced by sensor resources constraints, research contributions in this
field are growing day by day. In this paper, we present a comprehensive review
of most recent literature of WSNs and outline open research issues in this
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03573</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03573</id><created>2015-02-12</created><authors><author><keyname>Sakarovitch</keyname><forenames>Jacques</forenames><affiliation>LTCI</affiliation></author></authors><title>Automata and rational expressions</title><categories>cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This text is an extended version of the chapter 'Automata and rational
expressions' in the AutoMathA Handbook that will appear soon, published by the
European Science Foundation and edited by JeanEricPin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03578</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03578</id><created>2015-02-12</created><updated>2015-03-31</updated><authors><author><keyname>Abdi</keyname><forenames>Ali Z.</forenames></author><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author></authors><title>Lower Bounds for Cover-Free Families</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let ${\cal F}$ be a set of blocks of a $t$-set $X$. $(X,{\cal F})$ is called
$(w,r)$-cover-free family ($(w,r)-$CFF) provided that, the intersection of any
$w$ blocks in ${\cal F}$ is not contained in the union of any other $r$ blocks
in ${\cal F}$. We give new asymptotic lower bounds for the number of minimum
points $t$ in a $(w,r)$-CFF when $w\le r=|{\cal F}|^\epsilon$ for some constant
$\epsilon\ge 1/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03581</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03581</id><created>2015-02-12</created><authors><author><keyname>Chandra</keyname><forenames>Ashish</forenames></author><author><keyname>Suaib</keyname><forenames>Mohammad</forenames></author><author><keyname>Beg</keyname><forenames>Dr. Rizwan</forenames></author></authors><title>Web spam classification using supervised artificial neural network
  algorithms</title><categories>cs.NE cs.LG</categories><comments>10 Pages in Advanced Computational Intelligence: An International
  Journal (ACII), Vol.2, No.1, January 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the rapid growth in technology employed by the spammers, there is a
need of classifiers that are more efficient, generic and highly adaptive.
Neural Network based technologies have high ability of adaption as well as
generalization. As per our knowledge, very little work has been done in this
field using neural network. We present this paper to fill this gap. This paper
evaluates performance of three supervised learning algorithms of artificial
neural network by creating classifiers for the complex problem of latest web
spam pattern classification. These algorithms are Conjugate Gradient algorithm,
Resilient Backpropagation learning, and Levenberg-Marquardt algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03595</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03595</id><created>2015-02-12</created><authors><author><keyname>Tomanek</keyname><forenames>Martin</forenames></author><author><keyname>Juricek</keyname><forenames>Jan</forenames></author></authors><title>Project Risk Management Model Based on PRINCE2 and Scrum Frameworks</title><categories>cs.SE</categories><journal-ref>The International Journal of Software Engineering &amp; Applications
  (IJSEA), January 2015, Volume 6, Number 1, ISSN: 0975-9018</journal-ref><doi>10.5121/ijsea.2015.6107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a lack of formal risk management techniques in agile software
development methods Scrum. The need to manage risks in agile project management
is also identified by various authors. Authors of this paper conducted a survey
to find out the current practices in agile project management. Furthermore
authors discuss the new integrated framework of Scrum and PRINCE2 with focus on
risk management. Enrichment of Scrum with selected practices from the
heavy-weight project management framework PRINCE2 promises better results in
delivering software products especially in global development projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03596</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03596</id><created>2015-02-12</created><authors><author><keyname>Martin-del-Campo</keyname><forenames>Sergio</forenames></author><author><keyname>Sandin</keyname><forenames>Fredrik</forenames></author></authors><title>Towards zero-configuration condition monitoring based on dictionary
  learning</title><categories>cs.CV</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Condition-based predictive maintenance can significantly improve overall
equipment effectiveness provided that appropriate monitoring methods are used.
Online condition monitoring systems are customized to each type of machine and
need to be reconfigured when conditions change, which is costly and requires
expert knowledge. Basic feature extraction methods limited to signal
distribution functions and spectra are commonly used, making it difficult to
automatically analyze and compare machine conditions. In this paper, we
investigate the possibility to automate the condition monitoring process by
continuously learning a dictionary of optimized shift-invariant feature vectors
using a well-known sparse approximation method. We study how the feature
vectors learned from a vibration signal evolve over time when a fault develops
within a ball bearing of a rotating machine. We quantify the adaptation rate of
learned features and find that this quantity changes significantly in the
transitions between normal and faulty states of operation of the ball bearing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03601</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03601</id><created>2015-02-12</created><authors><author><keyname>Nagaraj</keyname><forenames>Kalyan</forenames></author><author><keyname>Sridhar</keyname><forenames>Amulyashree</forenames></author></authors><title>A Predictive System for detection of Bankruptcy using Machine Learning
  techniques</title><categories>cs.LG</categories><comments>11 pages, 7 figures</comments><journal-ref>Kalyan Nagaraj, Amulyashree Sridhar (2015). A Predictive System
  for detection of Bankruptcy using Machine learning techniques. IJDKP. 5(1):
  29-40</journal-ref><doi>10.5121/ijdkp.2015.5103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bankruptcy is a legal procedure that claims a person or organization as a
debtor. It is essential to ascertain the risk of bankruptcy at initial stages
to prevent financial losses. In this perspective, different soft computing
techniques can be employed to ascertain bankruptcy. This study proposes a
bankruptcy prediction system to categorize the companies based on extent of
risk. The prediction system acts as a decision support tool for detection of
bankruptcy
  Keywords: Bankruptcy, soft computing, decision support tool
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03612</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03612</id><created>2015-02-12</created><updated>2015-02-13</updated><authors><author><keyname>Yamauchi</keyname><forenames>D.</forenames></author><author><keyname>Ito</keyname><forenames>Y.</forenames></author></authors><title>A Method of Evaluating Effect of QoS Degradation on Multidimensional QoE
  of Web Service with ISO - based Usability</title><categories>cs.NI</categories><comments>15pages, International Journal of Computer Networks &amp;
  Communications(IJCNC) Vol.7, No.1, January 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a method of investigating effect of IP performance (QoS)
degradation on quality of experience (QoE) for a Web service; it considers the
usability based on the ISO 9241-11 as multidimensional QoE of a Web service
(QoE-Web) and the QoS parameters standardized by the IETF. Moreover, the paper
tackles clarification of the relationship between ISO-based QoE-Web and
IETF-based QoS by the multiple regression analysis. The experiment is intended
for the two actual Japanese online shopping services and utilizes 35 subjects.
From the results, the paper quantitatively discusses how the QoE-Web
deteriorates owing to the QoS degradation and shows that it is appropriate to
evaluate the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03620</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03620</id><created>2015-02-12</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Bouton</keyname><forenames>E. A.</forenames></author></authors><title>Multiresolution Division Multiplex (MRDM): A New Wavelet-based Multiplex
  System</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, ITS International Telecomm. Symposium, Brazil,
  2006</comments><doi>10.1109/ITS.2006.4433262</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An original multiplex scheme is introduced, which is based on Mallat's
multiresolution formulation of wavelet systems. This system is adaptable and
its implementation is well matched to digital signal processors and computers.
The approach termed multiresolution division multiplex (MRDM) is intensive in
signal processing (SP) tools, extremely flexible and can combine a variety of
tributaries at different bit rates. A broad variety of orthogonal wavelet
systems can endow with MRDM and the channel waveforms, and consequently the
spectral shape and system performance depend upon the selected wavelets.
Demultiplex can be done efficiently, since the number of floating
multiplications and additions increase only linearly with the length of
signals. A Haar-based MRDM scheme is presented to illustrate the versatility of
this new multiplex approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03628</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03628</id><created>2015-02-12</created><authors><author><keyname>Vulaj</keyname><forenames>Zoja</forenames></author><author><keyname>Kardovic</keyname><forenames>Faris</forenames></author></authors><title>Separation of sinusoidal and chirp components using Compressive sensing
  approach</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with the linear frequency modulated signals and radar
signals that are affected by disturbance which is the inevitable phenomenon in
everyday communications. The considered cases represent the cases when the
signals of interest overlap with other signals or with noise. In order to
successfully separate these signals we propose the compressive sensing method,
which states that the useful signal part can be separated successfully from a
small amount of measurements as long as the acquired signal can be presented as
sparse in a certain transformation domain. The effectiveness of our approach is
proven experimentally through examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03629</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03629</id><created>2015-02-12</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author></authors><title>On the greatest solution of equations in $\text{CLL}_R$</title><categories>cs.LO</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1411.0756</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that, for any equation $X=_{RS} t_X$ in the LLTS-oriented process
calculus $\text{CLL}_R$, if $X$ is strongly guarded in $t_X$, then the
recursive term $\langle X|X=t_X \rangle$ is the greatest solution of this
equation w.r.t L\&quot;{u}ttgen and Vogler's ready simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03630</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03630</id><created>2015-02-12</created><authors><author><keyname>Yang</keyname><forenames>Min</forenames></author><author><keyname>Cui</keyname><forenames>Tianyi</forenames></author><author><keyname>Tu</keyname><forenames>Wenting</forenames></author></authors><title>Ordering-sensitive and Semantic-aware Topic Modeling</title><categories>cs.LG cs.CL cs.IR</categories><comments>To appear in proceedings of AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modeling of textual corpora is an important and challenging problem. In
most previous work, the &quot;bag-of-words&quot; assumption is usually made which ignores
the ordering of words. This assumption simplifies the computation, but it
unrealistically loses the ordering information and the semantic of words in the
context. In this paper, we present a Gaussian Mixture Neural Topic Model
(GMNTM) which incorporates both the ordering of words and the semantic meaning
of sentences into topic modeling. Specifically, we represent each topic as a
cluster of multi-dimensional vectors and embed the corpus into a collection of
vectors generated by the Gaussian mixture model. Each word is affected not only
by its topic, but also by the embedding vector of its surrounding words and the
context. The Gaussian mixture components and the topic of documents, sentences
and words can be learnt jointly. Extensive experiments show that our model can
learn better topics and more accurate word distributions for each topic.
Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM
obtains significantly better performance in terms of perplexity, retrieval
accuracy and classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03634</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03634</id><created>2015-02-12</created><authors><author><keyname>Kim</keyname><forenames>Youngsung</forenames></author><author><keyname>Pereira</keyname><forenames>Francisco C.</forenames></author><author><keyname>Zhao</keyname><forenames>Fang</forenames></author><author><keyname>Ghorpade</keyname><forenames>Ajinkya</forenames></author><author><keyname>Zegras</keyname><forenames>P. Christopher</forenames></author><author><keyname>Ben-Akiva</keyname><forenames>Moshe</forenames></author></authors><title>Activity recognition for a smartphone and web based travel survey</title><categories>cs.CY</categories><acm-class>D.2.8; I.5.2; I.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In transport modeling and prediction, trip purposes play an important role
since mobility choices (e.g. modes, routes, departure times) are made in order
to carry out specific activities. Activity based models, which have been
gaining popularity in recent years, are built from a large number of observed
trips and their purposes. However, data acquired through traditional
interview-based travel surveys lack the accuracy and quantity required by such
models. Smartphones and interactive web interfaces have emerged as an
attractive alternative to conventional travel surveys. A smartphone-based
travel survey, Future Mobility Survey (FMS), was developed and field-tested in
Singapore and collected travel data from more than 1000 participants for
multiple days. To provide a more intelligent interface, inferring the
activities of a user at a certain location is a crucial challenge. This paper
presents a learning model that infers the most likely activity associated to a
certain visited place. The data collected in FMS contain errors or noise due to
various reasons, so a robust approach via ensemble learning is used to improve
generalization performance. Our model takes advantage of cross-user historical
data as well as user-specific information, including socio-demographics. Our
empirical results using FMS data demonstrate that the proposed method
contributes significantly to our travel survey application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03636</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03636</id><created>2015-02-12</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author></authors><title>Axiomatizing L\&quot;{u}ttgen and Vogler's ready simulation for finite
  processes in $\text{CLL}_R$</title><categories>cs.LO</categories><comments>25 pages</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of logic labelled transition system, a variant of weak ready
simulation has been presented by L\&quot;{u}ttgen and Vogler. It has been shown that
such behavioural preorder is the largest precongruence w.r.t parallel and
conjunction composition satisfying desired properties. This paper offers a
ground-complete axiomatization for this precongruence over processes containing
no recursion in the calculus $\text{CLL}_R$. Compared with usual inference
system for process calculus, in addition to axioms about process operators,
such system contains a number of axioms to characterize the interaction between
process operators and logical operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03641</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03641</id><created>2015-02-12</created><authors><author><keyname>Yassine</keyname><forenames>Abdulsalam</forenames></author></authors><title>A Wavelength Broker for Markets of Competing Optical Transport Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current trend in optical networks is to open the entire wholesale market
to competition. As a result, we will see, instead of a single big market
player, optical transport networks competing with each other to attract
customer demand. This paper presents a wavelength broker who acts on behalf of
enterprises, web host companies, financial firm etc. to buy certain number of
wavelengths from such market. We present the system model, the interaction
protocol and provide analysis of the competition. The simulation results of a
business scenario are also recorded in the paper
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03645</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03645</id><created>2015-02-12</created><updated>2015-07-27</updated><authors><author><keyname>Kreienbuehl</keyname><forenames>Andreas</forenames></author><author><keyname>Naegel</keyname><forenames>Arne</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Speck</keyname><forenames>Robert</forenames></author><author><keyname>Wittum</keyname><forenames>Gabriel</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>Numerical simulation of skin transport using Parareal</title><categories>cs.CE cs.DC cs.NA cs.PF math.NA</categories><comments>11 pages, 8 figures</comments><journal-ref>Computing and Visualization in Science 17(2), pp. 99-108, 2015</journal-ref><doi>10.1007/s00791-015-0246-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-silico investigation of skin permeation is an important but also
computationally demanding problem. To resolve all scales involved in full
detail will not only require exascale computing capacities but also suitable
parallel algorithms. This article investigates the applicability of the
time-parallel Parareal algorithm to a brick and mortar setup, a precursory
problem to skin permeation. The C++ library Lib4PrM implementing Parareal is
combined with the UG4 simulation framework, which provides the spatial
discretization and parallelization. The combination's performance is studied
with respect to convergence and speedup. It is confirmed that anisotropies in
the domain and jumps in diffusion coefficients only have a minor impact on
Parareal's convergence. The influence of load imbalances in time due to
differences in number of iterations required by the spatial solver as well as
spatio-temporal weak scaling is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03648</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03648</id><created>2015-02-12</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Over-Sampling in a Deep Neural Network</title><categories>cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNN) are the state of the art on many engineering
problems such as computer vision and audition. A key factor in the success of
the DNN is scalability - bigger networks work better. However, the reason for
this scalability is not yet well understood. Here, we interpret the DNN as a
discrete system, of linear filters followed by nonlinear activations, that is
subject to the laws of sampling theory. In this context, we demonstrate that
over-sampled networks are more selective, learn faster and learn more robustly.
Our findings may ultimately generalize to the human brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03654</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03654</id><created>2015-02-12</created><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Salim</keyname><forenames>Umer</forenames></author></authors><title>Regularized ZF in Cooperative Broadcast Channels under Distributed CSIT:
  A Large System Analysis</title><categories>cs.IT math.IT</categories><comments>Extended version of an ISIT 2015 submission. Addition of the proofs
  omitted due to space constraint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining accurate Channel State Information (CSI) at the transmitters (TX)
is critical to many cooperation schemes such as Network MIMO, Interference
Alignment etc. Practical CSI feedback and limited backhaul-based sharing
inevitably creates degradations of CSI which are specific to each TX, giving
rise to a distributed form of CSI. In the Distributed CSI (D-CSI) broadcast
channel setting, the various TXs design elements of the precoder based on their
individual estimates of the global multiuser channel matrix, which intuitively
degrades performance when compared with the commonly used centralized CSI
assumption. This paper tackles this challenging scenario and presents a first
analysis of the rate performance for the distributed CSI multi-TX broadcast
channel setting, in the large number of antenna regime. Using Random Matrix
Theory (RMT) tools, we derive deterministic equivalents of the Signal to
Interference plus Noise Ratio (SINR) for the popular regularized Zero-Forcing
(ZF) precoder, allowing to unveil the price of distributedness for such
cooperation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03666</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03666</id><created>2015-02-12</created><authors><author><keyname>Jin</keyname><forenames>Hu</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Fundamental Limits of CDF-Based Scheduling: Throughput, Fairness, and
  Feedback Overhead</title><categories>cs.IT cs.NI math.IT</categories><comments>14 pages, 11 figures, to appear in IEEE/ACM Transactions on
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate fundamental performance limits of cumulative
distribution function (CDF)-based scheduling (CS) in downlink cellular
networks. CS is known as an efficient scheduling method that can assign
different time fractions for users or, equivalently, satisfy different channel
access ratio (CAR) requirements of users while exploiting multi-user diversity.
We first mathematically analyze the throughput characteristics of CS in
arbitrary fading statistics and data rate functions. It is shown that the
throughput gain of CS increases as the CAR of a user decreases or the number of
users in a cell increases. For Nakagami-m fading channels, we obtain the
average throughput in closed-form and investigate the effects of the average
signal-to-noise ratio, the shape parameter m, and the CAR on the throughput
performance. In addition, we propose a threshold-based opportunistic feedback
technique in order to reduce feedback overhead while satisfying the CAR
requirements of users. We prove that the average feedback overhead of the
proposed technique is upper bounded by -ln(p), where p is the probability that
no user satisfies the threshold condition in a cell. Finally, we adopt a novel
fairness criterion, called qualitative fairness, which considers not only the
quantity of the allocated resources to users but also the quality of the
resources. It is observed that CS provides a better qualitative fairness than
other scheduling algorithms designed for controlling CARs of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03671</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03671</id><created>2015-02-12</created><updated>2015-04-09</updated><authors><author><keyname>Lebret</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Pinheiro</keyname><forenames>Pedro O.</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>Phrase-based Image Captioning</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating a novel textual description of an image is an interesting problem
that connects computer vision and natural language processing. In this paper,
we present a simple model that is able to generate descriptive sentences given
a sample image. This model has a strong focus on the syntax of the
descriptions. We train a purely bilinear model that learns a metric between an
image representation (generated from a previously trained Convolutional Neural
Network) and phrases that are used to described them. The system is then able
to infer phrases from a given image sample. Based on caption syntax statistics,
we propose a simple language model that can produce relevant descriptions for a
given test image using the phrases inferred. Our approach, which is
considerably simpler than state-of-the-art models, achieves comparable results
in two popular datasets for the task: Flickr30k and the recently proposed
Microsoft COCO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03678</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03678</id><created>2015-02-12</created><authors><author><keyname>AlTarawneh</keyname><forenames>Ragaad</forenames></author><author><keyname>Bauer</keyname><forenames>Jens</forenames></author><author><keyname>Menck</keyname><forenames>Nicole</forenames></author><author><keyname>Humayoun</keyname><forenames>Shah Rukh</forenames></author><author><keyname>Ebert</keyname><forenames>Achim</forenames></author></authors><title>assistME: A Platform for Assisting Engineers in Maintaining the Factory
  Pipeline</title><categories>cs.HC</categories><comments>7 pages, 3 figures, MOBILEng/2014/02</comments><report-no>MOBILEng/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper, we present our approach of utilizing mobile devices
(i.e., mobile phones and tablets) for assisting engineers and experts in
understanding and maintaining the factory pipelines. For this, we present a
platform, called assistME, that is composed of three main components: the
assistME Server, the assistME mobile infrastructure, and the co-assistME
collaborative environment. In order to get full utilization of the assistME
platform, we assume that an initial setup is made in the factory in such a way
that it is equipped with different sensors to collect data about specific
events in the factory pipeline together with the corresponding locations of
these events. The assistME Server works as a central control unit in the
platform and collects data from the installed sensors in the factory pipeline.
In the case of any unexpected behavior or any critical situation in the factory
pipeline, notification and other details are sent to the related group of
engineers and experts through the assistME mobile app. Further, the co-assistME
collaborative environment, equipped with a large shared screen and multiple
mobile devices, helps the engineers and experts to collaborate with to
understand and analyze the current situation in the factory pipeline in order
to maintain it accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03682</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03682</id><created>2015-02-12</created><authors><author><keyname>Mi&#xf1;arro-Gim&#xe9;nez</keyname><forenames>Jose Antonio</forenames></author><author><keyname>Mar&#xed;n-Alonso</keyname><forenames>Oscar</forenames></author><author><keyname>Samwald</keyname><forenames>Matthias</forenames></author></authors><title>Applying deep learning techniques on medical corpora from the World Wide
  Web: a prototypical system and evaluation</title><categories>cs.CL cs.IR cs.LG cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  BACKGROUND: The amount of biomedical literature is rapidly growing and it is
becoming increasingly difficult to keep manually curated knowledge bases and
ontologies up-to-date. In this study we applied the word2vec deep learning
toolkit to medical corpora to test its potential for identifying relationships
from unstructured text. We evaluated the efficiency of word2vec in identifying
properties of pharmaceuticals based on mid-sized, unstructured medical text
corpora available on the web. Properties included relationships to diseases
('may treat') or physiological processes ('has physiological effect'). We
compared the relationships identified by word2vec with manually curated
information from the National Drug File - Reference Terminology (NDF-RT)
ontology as a gold standard. RESULTS: Our results revealed a maximum accuracy
of 49.28% which suggests a limited ability of word2vec to capture linguistic
regularities on the collected medical corpora compared with other published
results. We were able to document the influence of different parameter settings
on result accuracy and found and unexpected trade-off between ranking quality
and accuracy. Pre-processing corpora to reduce syntactic variability proved to
be a good strategy for increasing the utility of the trained vector models.
CONCLUSIONS: Word2vec is a very efficient implementation for computing vector
representations and for its ability to identify relationships in textual data
without any prior domain knowledge. We found that the ranking and retrieved
results generated by word2vec were not of sufficient quality for automatic
population of knowledge bases and ontologies, but could serve as a starting
point for further manual curation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03683</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03683</id><created>2015-02-12</created><updated>2015-12-16</updated><authors><author><keyname>Turrini</keyname><forenames>Paolo</forenames></author></authors><title>Computing rational decisions in extensive games with limited foresight</title><categories>cs.AI cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a class of extensive form games where players might not be able
to foresee the possible consequences of their decisions and form a model of
their opponents which they exploit to achieve a more profitable outcome. We
improve upon existing models of games with limited foresight, endowing players
with the ability of higher-order reasoning and proposing a novel solution
concept to address intuitions coming from real game play. We analyse the
resulting equilibria, devising an effective procedure to compute them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03690</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03690</id><created>2015-02-12</created><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author></authors><title>Semi-dynamic connectivity in the plane</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a path planning problem we consider the following procedure.
Assume that we have two points $s$ and $t$ in the plane and take
$\mathcal{K}=\emptyset$. At each step we add to $\mathcal{K}$ a compact convex
set that does not contain $s$ nor $t$. The procedure terminates when the sets
in $\mathcal{K}$ separate $s$ and $t$. We show how to add one set to
$\mathcal{K}$ in $O(1+k\alpha(n))$ amortized time plus the time needed to find
all sets of $\mathcal{K}$ intersecting the newly added set, where $n$ is the
cardinality of $\mathcal{K}$, $k$ is the number of sets in $\mathcal{K}$
intersecting the newly added set, and $\alpha(\cdot)$ is the inverse of the
Ackermann function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03697</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03697</id><created>2015-02-12</created><updated>2015-09-16</updated><authors><author><keyname>Svensson</keyname><forenames>Andreas</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Kok</keyname><forenames>Manon</forenames></author></authors><title>Nonlinear state space smoothing using the conditional particle filter</title><categories>stat.CO cs.SY math.OC</categories><comments>Accepted for the 17th IFAC Symposium on System Identification
  (SYSID), Beijing, China, October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To estimate the smoothing distribution in a nonlinear state space model, we
apply the conditional particle filter with ancestor sampling. This gives an
iterative algorithm in a Markov chain Monte Carlo fashion, with asymptotic
convergence results. The computational complexity is analyzed, and our proposed
algorithm is successfully applied to the challenging problem of sensor fusion
between ultra-wideband and accelerometer/gyroscope measurements for indoor
positioning. It appears to be a competitive alternative to existing nonlinear
smoothing algorithms, in particular the forward filtering-backward simulation
smoother.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03698</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03698</id><created>2015-02-12</created><authors><author><keyname>Miranda</keyname><forenames>J. P. C. L.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>On Galois-Division Multiple Access Systems: Figures of Merit and
  Performance Evaluation</title><categories>cs.IT cs.DM math.IT</categories><comments>6 pages, 4 figures. In: XIX Simposio Brasileiro de Telecomunicacoes,
  2001, Fortaleza, CE, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach to multiple access based on finite field transforms is
investigated. These schemes, termed Galois-Division Multiple Access (GDMA),
offer compact bandwidth requirements. A new digital transform, the Finite Field
Hartley Transform (FFHT) requires to deal with fields of characteristic p, p
\neq 2. A binary-to-p-ary (p \neq 2) mapping based on the opportunistic
secondary channel is introduced. This allows the use of GDMA in conjunction
with available digital systems. The performance of GDMA is also evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03699</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03699</id><created>2015-02-12</created><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Zhou</keyname><forenames>Yuren</forenames></author></authors><title>Analysis of Solution Quality of a Multiobjective Optimization-based
  Evolutionary Algorithm for Knapsack Problem</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-objective optimisation is regarded as one of the most promising ways
for dealing with constrained optimisation problems in evolutionary
optimisation. This paper presents a theoretical investigation of a
multi-objective optimisation evolutionary algorithm for solving the 0-1
knapsack problem. Two initialisation methods are considered in the algorithm:
local search initialisation and greedy search initialisation. Then the solution
quality of the algorithm is analysed in terms of the approximation ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03708</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03708</id><created>2015-02-12</created><updated>2015-08-08</updated><authors><author><keyname>Elias</keyname><forenames>Yara</forenames></author><author><keyname>Lauter</keyname><forenames>Kristin E.</forenames></author><author><keyname>Ozman</keyname><forenames>Ekin</forenames></author><author><keyname>Stange</keyname><forenames>Katherine E.</forenames></author></authors><title>Provably weak instances of Ring-LWE</title><categories>cs.CR math.NT</categories><comments>24 pages including computer code, minor modifications and typos
  corrected</comments><msc-class>11T71, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ring and polynomial learning with errors problems (Ring-LWE and Poly-LWE)
have been proposed as hard problems to form the basis for cryptosystems, and
various security reductions to hard lattice problems have been presented. So
far these problems have been stated for general (number) rings but have only
been closely examined for cyclotomic number rings. In this paper, we state and
examine the Ring-LWE problem for general number rings and demonstrate provably
weak instances of Ring-LWE. We construct an explicit family of number fields
for which we have an efficient attack. We demonstrate the attack in both theory
and practice, providing code and running times for the attack. The attack runs
in time linear in q, where q is the modulus.
  Our attack is based on the attack on Poly-LWE which was presented in
[Eisentr\&quot;ager-Hallgren-Lauter]. We extend the EHL-attack to apply to a larger
class of number fields, and show how it applies to attack Ring-LWE for a
heuristically large class of fields. Certain Ring-LWE instances can be
transformed into Poly-LWE instances without distorting the error too much, and
thus provide the first weak instances of the Ring-LWE problem. We also provide
additional examples of fields which are vulnerable to our attacks on Poly-LWE,
including power-of-$2$ cyclotomic fields, presented using the minimal
polynomial of $\zeta_{2^n} \pm 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03715</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03715</id><created>2015-02-12</created><updated>2016-01-05</updated><authors><author><keyname>Vygen</keyname><forenames>Jens</forenames></author></authors><title>Reassembling trees for the traveling salesman</title><categories>cs.DM cs.DS math.CO</categories><comments>minor revision, final version, to appear in SIAM Journal of Discrete
  Mathematics, please use color printer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many recent approximation algorithms for different variants of the traveling
salesman problem (asymmetric TSP, graph TSP, s-t-path TSP) exploit the
well-known fact that a solution of the natural linear programming relaxation
can be written as convex combination of spanning trees. The main argument then
is that randomly sampling a tree from such a distribution and then completing
the tree to a tour at minimum cost yields a better approximation guarantee than
simply taking a minimum cost spanning tree (as in Christofides' algorithm). We
argue that an additional step can help: reassembling the spanning trees before
sampling. Exchanging two edges in a pair of spanning trees can improve their
properties under certain conditions. We demonstrate the usefulness for the
metric s-t-path TSP by devising a deterministic polynomial-time algorithm that
improves on Seb\H{o}'s previously best approximation ratio of 8/5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03722</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03722</id><created>2015-02-12</created><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author></authors><title>Optimal sequential fingerprinting: Wald vs. Tardos</title><categories>cs.CR math.ST stat.TH</categories><comments>12 pages, 10 figures</comments><journal-ref>ACM Workshop on Information Hiding and Multimedia Security
  (IH&amp;MMSec), pp. 97-107, 2015</journal-ref><doi>10.1145/2756601.2756603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sequential collusion-resistant fingerprinting, where the
fingerprinting code is generated in advance but accusations may be made between
rounds, and show that in this setting both the dynamic Tardos scheme and
schemes building upon Wald's sequential probability ratio test (SPRT) are
asymptotically optimal. We further compare these two approaches to sequential
fingerprinting, highlighting differences between the two schemes. Based on
these differences, we argue that Wald's scheme should in general be preferred
over the dynamic Tardos scheme, even though both schemes have their merits. As
a side result, we derive an optimal sequential group testing method for the
classical model, which can easily be generalized to different group testing
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03723</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03723</id><created>2015-02-12</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Ranhel</keyname><forenames>J.</forenames></author><author><keyname>Alves</keyname><forenames>R. B. A.</forenames></author></authors><title>Simulation of Color Blindness and a Proposal for Using Google Glass as
  Color-correcting Tool</title><categories>cs.HC cs.CV</categories><comments>4 pages, 6 figures, XXIV Congresso Brasileiro de Engenharia
  Biomedica, Uberlandia, MG, Brazil, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human visual color response is driven by specialized cells called cones,
which exist in three types, viz. R, G, and B. Software is developed to simulate
how color images are displayed for different types of color blindness.
Specified the default color deficiency associated with a user, it generates a
preview of the rainbow (in the visible range, from red to violet) and shows up,
side by side with a colorful image provided as input, the display correspondent
colorblind. The idea is to provide an image processing after image acquisition
to enable a better perception ofcolors by the color blind. Examples of
pseudo-correction are shown for the case of Protanopia (red blindness). The
system is adapted into a screen of an i-pad or a cellphone in which the
colorblind observe the camera, the image processed with color detail previously
imperceptible by his naked eye. As prospecting, wearable computer glasses could
be manufactured to provide a corrected image playback. The approach can also
provide augmented reality for human vision by adding the UV or IR responses as
a new feature of Google Glass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03726</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03726</id><created>2015-02-12</created><authors><author><keyname>Zhang</keyname><forenames>Ruixia</forenames></author><author><keyname>Zhang</keyname><forenames>Liping</forenames></author><author><keyname>Wang</keyname><forenames>Huan</forenames></author><author><keyname>Chen</keyname><forenames>Zhuo</forenames></author></authors><title>A Novel Approach for Clone Group Mapping by using Topic Modeling</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clone group mapping has a very important significance in the evolution of
code clone. The topic modeling techniques were applied into code clone firstly
and a new clone group mapping method was proposed. The method is very effective
for not only Type-1 and Type-2 clone but also Type-3 clone .By making full use
of the source text and structure information, topic modeling techniques
transform the mapping problem of high-dimensional code space into a
low-dimensional topic space, the goal of clone group mapping was indirectly
reached by mapping clone group topics. Experiments on four open source software
show that the recall and precision are up to 0.99, thus the method can
effectively and accurately reach the goal of clone group mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03729</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03729</id><created>2015-02-12</created><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Coherent-Classical Estimation for Linear Quantum Systems</title><categories>math.OC cs.SY quant-ph</categories><comments>14 pages, 13 figures, journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coherent-classical estimation scheme for a class of linear quantum systems
is considered. Here, the estimator is a mixed quantum-classical system that may
or may not involve coherent feedback and yields a classical estimate of a
variable for the quantum plant. We demonstrate that with no coherent feedback,
such coherent-classical estimation provides no improvement over
purely-classical estimation, when the quantum plant or the quantum part of the
estimator (called the coherent controller) is a passive annihilation operator
system. However, when both the quantum plant and the coherent controller are
active systems (that cannot be described by annihilation operators only),
coherent-classical estimation without coherent feedback can provide improvement
over purely-classical estimation in certain cases. Furthermore, we show that
with coherent feedback, it is possible to get better estimation accuracies with
coherent-classical estimation, as compared to classical-only estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03752</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03752</id><created>2015-02-12</created><authors><author><keyname>Alkahtani</keyname><forenames>Saad</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Teahan</keyname><forenames>William J.</forenames></author></authors><title>A new hybrid metric for verifying parallel corpora of Arabic-English</title><categories>cs.CL</categories><comments>in CCSEA-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a new metric that has been applied to verify the quality
in translation between sentence pairs in parallel corpora of Arabic-English.
This metric combines two techniques, one based on sentence length and the other
based on compression code length. Experiments on sample test parallel
Arabic-English corpora indicate the combination of these two techniques
improves accuracy of the identification of satisfactory and unsatisfactory
sentence pairs compared to sentence length and compression code length alone.
The new method proposed in this research is effective at filtering noise and
reducing mis-translations resulting in greatly improved quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03762</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03762</id><created>2015-02-12</created><updated>2016-02-23</updated><authors><author><keyname>Shafieepoorfard</keyname><forenames>Ehsan</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Meyn</keyname><forenames>Sean P.</forenames></author></authors><title>Rationally inattentive control of Markov processes</title><categories>math.OC cs.IT math.IT</categories><comments>30 pages, 2 figures; accepted to SIAM Journal on Control and
  Optimization</comments><msc-class>94A34, 90C40, 90C47</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article poses a general model for optimal control subject to information
constraints, motivated in part by recent work of Sims and others on
information-constrained decision-making by economic agents. In the average-cost
optimal control framework, the general model introduced in this paper reduces
to a variant of the linear-programming representation of the average-cost
optimal control problem, subject to an additional mutual information constraint
on the randomized stationary policy. The resulting optimization problem is
convex and admits a decomposition based on the Bellman error, which is the
object of study in approximate dynamic programming. The theory is illustrated
through the example of information-constrained linear-quadratic-Gaussian (LQG)
control problem. Some results on the infinite-horizon discounted-cost criterion
are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03772</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03772</id><created>2015-02-12</created><authors><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author></authors><title>A Story of Suo Motos, Judicial Activism, and Article 184 (3)</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synergy between Big Data and Open Data has the potential to revolutionize
information access in the developing world. Following this mantra, we present
the analysis of more than a decade worth of open judgements and orders from the
Supreme Court of Pakistan. Our overarching goal is to discern the presence of
judicial activism in the country in the wake of the Lawyers' Movement. Using
Apache Spark as the processing engine we analyze hundreds of unstructured PDF
documents to sketch the evolution and various organs of judicial activism in
Pakistan since 2009. Our results show that the judiciary has indeed been
pursuing matters of public interest, especially those that pertain to the
fundamental rights of the citizens. Furthermore, we show how the size of the
presiding bench in a case and citations of Articles from the Constitution and
prior judgements can aid in classifying legal judgements. Throughout the
analysis we also highlight the challenges that anyone who aims to apply Big
Data techniques to Open Data will face. We hope that this work will be one in a
series of community efforts to use Open Data as a lens to analyze real-world
events and phenomena in the developing world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03774</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03774</id><created>2015-02-12</created><authors><author><keyname>Iyer</keyname><forenames>Aiswarya</forenames></author><author><keyname>Jeyalatha</keyname><forenames>S.</forenames></author><author><keyname>Sumbaly</keyname><forenames>Ronak</forenames></author></authors><title>Diagnosis of diabetes using classification mining techniques</title><categories>cs.CE</categories><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP), Vol.5, No.1, January 2015, pp. 1-14</journal-ref><doi>10.5121/ijdkp.2015.5101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diabetes has affected over 246 million people worldwide with a majority of
them being women. According to the WHO report, by 2025 this number is expected
to rise to over 380 million. The disease has been named the fifth deadliest
disease in the United States with no imminent cure in sight. With the rise of
information technology and its continued advent into the medical and healthcare
sector, the cases of diabetes as well as their symptoms are well documented.
This paper aims at finding solutions to diagnose the disease by analyzing the
patterns found in the data through classification analysis by employing
Decision Tree and Na\&quot;ive Bayes algorithms. The research hopes to propose a
quicker and more efficient technique of diagnosing the disease, leading to
timely treatment of the patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03779</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03779</id><created>2015-02-12</created><authors><author><keyname>Sasaki</keyname><forenames>Tatsuya</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>&#xc5;ke</forenames></author><author><keyname>Okada</keyname><forenames>Isamu</forenames></author><author><keyname>Unemi</keyname><forenames>Tatsuo</forenames></author></authors><title>Unchecked strategy diversification and collapse in continuous voluntary
  public good games</title><categories>physics.soc-ph cs.GT math.DS nlin.AO q-bio.PE</categories><comments>30 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation or defection and participation or withdrawal are well-known
options of behavior in game-like activities in free societies, yet the
co-evolutionary dynamics of these behavioral traits in the individual level are
not well understood. Here we investigate the continuous voluntary public good
game, in which individuals have two types of continuous-valued options: a
probability of joining the public good game and a level of cooperative
investment in the game. Our numerical results reveal hitherto unreported
phenomena: (i) The evolutionary dynamics are initially characterized by
oscillations in individual cooperation and participation levels, in contrast to
the population-level oscillations that have previously been reported. (ii)
Eventually, the population's average cooperation and participation levels
converge to and stabilize at a center. (iii) Then, a most peculiar phenomenon
unfolds: The strategies present in the population diversify and give rise to a
&quot;cloud&quot; of tinkering individuals who each tries out a different strategy, and
this process continues unchecked as long as the population's cooperation and
participation levels remain balanced. Over time, however, imbalances build up
as a consequence of random drift and there is a sudden and abrupt collapse of
the strategy-diversity cloud. The process then repeats again in a cyclic
manner. To understand the three aforementioned phenomena, we investigate the
system analytically using adaptive-dynamics techniques. Our analysis casts
light on the mechanisms which underpin the unexpected and surprising
evolutionary dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03780</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03780</id><created>2015-02-12</created><authors><author><keyname>Wolfe</keyname><forenames>Nikolas</forenames></author></authors><title>An Open-Source Monitoring System for Remote Solar Power Applications</title><categories>cs.CY</categories><comments>19</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Renewable energy systems are an increasingly popular way to generate
electricity. As with any new technological paradigm, new challenges have
emerged which are unique to the utilization of renewable energy systems. One of
these challenges in particular is the development of effective monitoring
technologies to compensate for the decentralized nature of remote power
generation. This project details the development of an open-source monitoring
system for remote solar power systems. The problem space that this project is
specifically concerned with deals with the reduction of cost and the use of
open platforms to make solar monitoring viable in developing countries where
both the resources and general knowledge required to undertake such efforts are
particularly scarce.
  Currently, solar monitoring technologies are expensive, limited in their
application, and for the most part proprietary. It is arguable that such
systems can be developed using non-customized hardware and open-source software
that can be obtained and run anywhere in the world. This project is one such
argument. This proof of concept is sufficient to show that solar remote
monitoring is neither expensive nor particularly cumbersome to implement and
thus warrants further investigation and development by the open source
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03784</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03784</id><created>2015-02-12</created><updated>2015-02-13</updated><authors><author><keyname>Schwarz</keyname><forenames>Andreas</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Coherent-to-Diffuse Power Ratio Estimation for Dereverberation</title><categories>cs.SD</categories><comments>submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing, 2015</comments><doi>10.1109/TASLP.2015.2418571</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of the time- and frequency-dependent coherent-to-diffuse power
ratio (CDR) from the measured spatial coherence between two omnidirectional
microphones is investigated. Known CDR estimators are formulated in a common
framework, illustrated using a geometric interpretation in the complex plane,
and investigated with respect to bias and robustness towards model errors.
Several novel unbiased CDR estimators are proposed, and it is shown that
knowledge of either the direction of arrival (DOA) of the target source or the
coherence of the noise field is sufficient for unbiased CDR estimation. The
validity of the model for the application of CDR estimates to dereverberation
is investigated using measured and simulated impulse responses. A CDR-based
dereverberation system is presented and evaluated using signal-based quality
measures as well as automatic speech recognition accuracy. The results show
that the proposed unbiased estimators have a practical advantage over existing
estimators, and that the proposed DOA-independent estimator can be used for
effective blind dereverberation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03790</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03790</id><created>2015-02-12</created><updated>2015-05-27</updated><authors><author><keyname>Kim</keyname><forenames>Su Min</forenames></author><author><keyname>Do</keyname><forenames>Tan Tai</forenames></author><author><keyname>Oechtering</keyname><forenames>Tobias J.</forenames></author><author><keyname>Peters</keyname><forenames>Gunnar</forenames></author></authors><title>On the Entropy Computation of Large Complex Gaussian Mixture
  Distributions</title><categories>cs.IT math.IT</categories><comments>14 pages, Accepted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2441046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy computation of Gaussian mixture distributions with a large number
of components has a prohibitive computational complexity. In this paper, we
propose a novel approach exploiting the sphere decoding concept to bound and
approximate such entropy terms with reduced complexity and good accuracy.
Moreover, we propose an SNR region based enhancement of the approximation
method to reduce the complexity even further. Using Monte-Carlo simulations,
the proposed methods are numerically demonstrated for the computation of the
mutual information including the entropy term of various channels with finite
constellation modulations such as binary and quadratic amplitude modulation
(QAM) inputs for communication applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03794</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03794</id><created>2015-02-12</created><authors><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Sum Rate Maximization for MU-MISO with Partial CSIT using Joint
  Multicasting and Broadcasting</title><categories>cs.IT math.IT</categories><comments>in IEEE International Conference on Communications (ICC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a MU-MISO system where users have highly accurate
Channel State Information (CSI), while the Base Station (BS) has partial CSI
consisting of an imperfect channel estimate and statistical knowledge of the
CSI error. With the objective of maximizing the Average Sum Rate (ASR) subject
to a power constraint, a special transmission scheme is considered where the BS
transmits a common symbol in a multicast fashion, in addition to the
conventional private symbols. This scheme is termed Joint Multicasting and
Broadcasting (JMB). The ASR problem is transformed into an augmented Average
Weighted Sum Mean Square Error (AWSMSE) problem which is solved using
Alternating Optimization (AO). The enhanced rate performance accompanied with
the incorporation of the multicast part is demonstrated through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03796</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03796</id><created>2015-02-12</created><authors><author><keyname>Cohen</keyname><forenames>David A.</forenames></author><author><keyname>Cooper</keyname><forenames>Martin C.</forenames></author><author><keyname>Escamocher</keyname><forenames>Guillaume</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>Variable and value elimination in binary constraint satisfaction via
  forbidden patterns</title><categories>cs.CC cs.AI cs.DM</categories><comments>A full version of an IJCAI'13 paper to appear in Journal of Computer
  and System Sciences (JCSS)</comments><acm-class>F.2.0</acm-class><journal-ref>Journal of Computer and System Sciences 81(7) 1127-1143 (2015)</journal-ref><doi>10.1016/j.jcss.2015.02.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable or value elimination in a constraint satisfaction problem (CSP) can
be used in preprocessing or during search to reduce search space size. A
variable elimination rule (value elimination rule) allows the polynomial-time
identification of certain variables (domain elements) whose elimination,
without the introduction of extra compensatory constraints, does not affect the
satisfiability of an instance. We show that there are essentially just four
variable elimination rules and three value elimination rules defined by
forbidding generic sub-instances, known as irreducible existential patterns, in
arc-consistent CSP instances. One of the variable elimination rules is the
already-known Broken Triangle Property, whereas the other three are novel. The
three value elimination rules can all be seen as strict generalisations of
neighbourhood substitution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03802</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03802</id><created>2015-02-12</created><authors><author><keyname>Xue</keyname><forenames>Yuanyi</forenames></author><author><keyname>Zhou</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>A two-stage video coding framework with both self-adaptive redundant
  dictionary and adaptively orthonormalized DCT basis</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a two-stage video coding framework, as an extension
of our previous one-stage framework in [1]. The two-stage frameworks consists
two different dictionaries. Specifically, the first stage directly finds the
sparse representation of a block with a self-adaptive dictionary consisting of
all possible inter-prediction candidates by solving an L0-norm minimization
problem using an improved orthogonal matching pursuit with embedded
orthonormalization (eOMP) algorithm, and the second stage codes the residual
using DCT dictionary adaptively orthonormalized to the subspace spanned by the
first stage atoms. The transition of the first stage and the second stage is
determined based on both stages' quantization stepsizes and a threshold. We
further propose a complete context adaptive entropy coder to efficiently code
the locations and the coefficients of chosen first stage atoms. Simulation
results show that the proposed coder significantly improves the RD performance
over our previous one-stage coder. More importantly, the two-stage coder, using
a fixed block size and inter-prediction only, outperforms the H.264 coder
(x264) and is competitive with the HEVC reference coder (HM) over a large rate
range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03805</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03805</id><created>2015-02-12</created><authors><author><keyname>Xue</keyname><forenames>Yuanyi</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>eOMP: Finding Sparser Representation by Recursively Orthonormalizing the
  Remaining Atoms</title><categories>cs.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy algorithms for minimizing L0-norm of sparse decomposition have
profound application impact on many signal processing problems. In the sparse
coding setup, given the observations $\mathrm{y}$ and the redundant dictionary
$\mathbf{\Phi}$, one would seek the most sparse coefficient (signal)
$\mathrm{x}$ with a constraint on approximation fidelity. In this work, we
propose a greedy algorithm based on the classic orthogonal matching pursuit
(OMP) with improved sparsity on $\mathrm{x}$ and better recovery rate, which we
name as eOMP. The key ingredient of the eOMP is recursively performing one-step
orthonormalization on the remaining atoms, and evaluating correlations between
residual and orthonormalized atoms. We show a proof that the proposed eOMP
guarantees to maximize the residual reduction at each iteration. Through
extensive simulations, we show the proposed algorithm has better exact recovery
rate on i.i.d. Gaussian ensembles with Gaussian signals, and more importantly
yields smaller L0-norm under the same approximation fidelity compared to the
original OMP, for both synthetic and practical scenarios. The complexity
analysis and real running time result also show a manageable complexity
increase over the original OMP. We claim that the proposed algorithm has better
practical perspective for finding more sparse representations than existing
greedy algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03817</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03817</id><created>2015-02-12</created><authors><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Achieving Max-Min Fairness for MU-MISO with Partial CSIT: A Multicast
  Assisted Transmission</title><categories>cs.IT math.IT</categories><comments>in IEEE International Conference on Communications (ICC) 2015. arXiv
  admin note: substantial text overlap with arXiv:1502.03794</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the max-min fairness design problem for a MU-MISO system with
partial Channel State Information (CSI) at the Base Station (BS), consisting of
an imperfect channel estimate and statistical knowledge of the estimation
error, and perfect CSI at the receivers. The objective is to maximize the
minimum Average Rate (AR) among users subject to a transmit power constraint.
An unconventional transmission scheme is adopted where the Base Station (BS)
transmits a common message in addition to the conventional private messages. In
situations where the CSIT is not accurate enough to perform interference
nulling, individual rates are assisted by allocating parts of the common
message to different users according to their needs. The AR problem is
transformed into an augmented Average Weighted Mean Square Error (AWMSE)
problem, solved using Alternating Optimization (AO). The benefits of
incorporating the common message are demonstrated through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03845</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03845</id><created>2015-02-12</created><authors><author><keyname>Bonasera</keyname><forenames>Biagio</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Pagano</keyname><forenames>Francesco</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Adaptive Search over Sorted Sets</title><categories>cs.DS</categories><comments>9 pages</comments><acm-class>F.2.2</acm-class><journal-ref>Journal of Discrete Algorithms, Volume 30, 2015, pp. 128--133</journal-ref><doi>10.1016/j.jda.2014.12.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classical algorithms for searching over sorted sets to
introduce an algorithm refinement, called Adaptive Search, that combines the
good features of Interpolation search and those of Binary search. W.r.t.
Interpolation search, only a constant number of extra comparisons is
introduced. Yet, under diverse input data distributions our algorithm shows
costs comparable to that of Interpolation search, i.e., O(log log n) while the
worst-case cost is always in O(log n), as with Binary search. On benchmarks
drawn from large datasets, both synthetic and real-life, Adaptive search scores
better times and lesser memory accesses even than Santoro and Sidney's
Interpolation-Binary search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03847</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03847</id><created>2015-02-12</created><updated>2015-02-23</updated><authors><author><keyname>Bandyapadhyay</keyname><forenames>Sayan</forenames></author><author><keyname>Bhowmick</keyname><forenames>Santanu</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author></authors><title>A Constant Factor Approximation for Orthogonal Order Preserving Layout
  Adjustment</title><categories>cs.CG cs.CC cs.DM</categories><comments>Edited Section 5, re-arranged content</comments><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an initial placement of a set of rectangles in the plane, we consider
the problem of finding a disjoint placement of the rectangles that minimizes
the area of the bounding box and preserves the orthogonal order i.e.\ maintains
the sorted ordering of the rectangle centers along both $x$-axis and $y$-axis
with respect to the initial placement. This problem is known as Layout
Adjustment for Disjoint Rectangles(LADR). It was known that LADR is
$\mathbb{NP}$-hard, but only heuristics were known for it. We show that a
certain decision version of LADR is $\mathbb{APX}$-hard, and give a constant
factor approximation for LADR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03849</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03849</id><created>2015-02-12</created><updated>2016-02-27</updated><authors><author><keyname>Christodoulou</keyname><forenames>George</forenames></author><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Frederiksen</keyname><forenames>Soren Kristoffer Stiil</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Jinshan</forenames></author></authors><title>Social Welfare in One-Sided Matching Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Price of Anarchy of mechanisms for the well-known problem of
one-sided matching, or house allocation, with respect to the social welfare
objective. We consider both ordinal mechanisms, where agents submit preference
lists over the items, and cardinal mechanisms, where agents may submit
numerical values for the items being allocated. We present a general lower
bound of $\Omega(\sqrt{n})$ on the Price of Anarchy, which applies to all
mechanisms. We show that two well-known mechanisms, Probabilistic Serial, and
Random Priority, achieve a matching upper bound. We extend our lower bound to
the Price of Stability of a large class of mechanisms that satisfy a common
proportionality property, and show stronger bounds on the Price of Anarchy of
all deterministic mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03851</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03851</id><created>2015-02-12</created><authors><author><keyname>Khodabandeh</keyname><forenames>Mehran</forenames></author><author><keyname>Vahdat</keyname><forenames>Arash</forenames></author><author><keyname>Zhou</keyname><forenames>Guang-Tong</forenames></author><author><keyname>Hajimirsadeghi</keyname><forenames>Hossein</forenames></author><author><keyname>Roshtkhari</keyname><forenames>Mehrsan Javan</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author><author><keyname>Se</keyname><forenames>Stephen</forenames></author></authors><title>Discovering Human Interactions in Videos with Limited Data Labeling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach for discovering human interactions in videos.
Activity understanding techniques usually require a large number of labeled
examples, which are not available in many practical cases. Here, we focus on
recovering semantically meaningful clusters of human-human and human-object
interaction in an unsupervised fashion. A new iterative solution is introduced
based on Maximum Margin Clustering (MMC), which also accepts user feedback to
refine clusters. This is achieved by formulating the whole process as a unified
constrained latent max-margin clustering problem. Extensive experiments have
been carried out over three challenging datasets, Collective Activity, VIRAT,
and UT-interaction. Empirical results demonstrate that the proposed algorithm
can efficiently discover perfect semantic clusters of human interactions with
only a small amount of labeling effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03872</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03872</id><created>2015-02-12</created><authors><author><keyname>Jung</keyname><forenames>JongHun</forenames></author><author><keyname>Kim</keyname><forenames>Hwan-Kuk</forenames></author><author><keyname>Yoon</keyname><forenames>Soojin</forenames></author></authors><title>Malicious web script-based cyber attack protection technology</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent web-based cyber attacks are evolving into a new form of attacks such
as private information theft and DDoS attack exploiting JavaScript within a web
page. These attacks can be made just by accessing a web site without
distribution of malicious codes and infection. Script-based cyber attacks are
hard to detect with traditional security equipments such as Firewall and IPS
because they inject malicious scripts in a response message for a normal web
request. Furthermore, they are hard to trace because attacks such as DDoS can
be made just by visiting a web page. Due to these reasons, it is expected that
they could result in direct damages and great ripple effects. To cope with
these issues, in this article, a proposal is made for techniques that are used
to detect malicious scripts through real-time web content analysis and to
automatically generate detection signatures for malicious JavaScript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03874</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03874</id><created>2015-02-12</created><authors><author><keyname>Zha</keyname><forenames>Zhengbang</forenames></author><author><keyname>Hu</keyname><forenames>Lei</forenames></author><author><keyname>Sun</keyname><forenames>Siwei</forenames></author><author><keyname>Shan</keyname><forenames>Jinyong</forenames></author></authors><title>Further results on differentially 4-uniform permutations over
  $\F_{2^{2m}}$</title><categories>cs.IT math.IT</categories><comments>15 pages. This paper has been accepted for publication in SCIENCE
  CHINA Mathematics</comments><msc-class>94A60, 11T71</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present several new constructions of differentially
4-uniform permutations over $\F_{2^{2m}}$ by modifying the values of the
inverse function on some subsets of $\F_{2^{2m}}$. The resulted differentially
4-uniform permutations have high nonlinearities and algebraic degrees, which
provide more choices for the design of cryptographic substitution boxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03879</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03879</id><created>2015-02-12</created><authors><author><keyname>Ren</keyname><forenames>Weiya</forenames></author></authors><title>Semi-supervised Data Representation via Affinity Graph Learning</title><categories>cs.LG cs.CV</categories><comments>10 pages,2 Tables. Written in Aug,2013</comments><msc-class>68T10</msc-class><acm-class>I.4.2; I.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the general problem of utilizing both labeled and unlabeled data
to improve data representation performance. A new semi-supervised learning
framework is proposed by combing manifold regularization and data
representation methods such as Non negative matrix factorization and sparse
coding. We adopt unsupervised data representation methods as the learning
machines because they do not depend on the labeled data, which can improve
machine's generation ability as much as possible. The proposed framework forms
the Laplacian regularizer through learning the affinity graph. We incorporate
the new Laplacian regularizer into the unsupervised data representation to
smooth the low dimensional representation of data and make use of label
information. Experimental results on several real benchmark datasets indicate
that our semi-supervised learning framework achieves encouraging results
compared with state-of-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03890</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03890</id><created>2015-02-13</created><authors><author><keyname>Kim</keyname><forenames>Song-Ju</forenames></author><author><keyname>Aono</keyname><forenames>Masashi</forenames></author></authors><title>Decision Maker using Coupled Incompressible-Fluid Cylinders</title><categories>cs.AI</categories><comments>5 pages, 5 figures, Waseda AICS Symposium and the 14th Slovenia-Japan
  Seminar, Waseda University, Tokyo, 24-26 October 2014. in Special Issue of
  ASTE: Advances in Science, Technology and Environmentology (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-armed bandit problem (MBP) is the problem of finding, as accurately
and quickly as possible, the most profitable option from a set of options that
gives stochastic rewards by referring to past experiences. Inspired by
fluctuated movements of a rigid body in a tug-of-war game, we formulated a
unique search algorithm that we call the `tug-of-war (TOW) dynamics' for
solving the MBP efficiently. The cognitive medium access, which refers to
multi-user channel allocations in cognitive radio, can be interpreted as the
competitive multi-armed bandit problem (CMBP); the problem is to determine the
optimal strategy for allocating channels to users which yields maximum total
rewards gained by all users. Here we show that it is possible to construct a
physical device for solving the CMBP, which we call the `TOW Bombe', by
exploiting the TOW dynamics existed in coupled incompressible-fluid cylinders.
This analog computing device achieves the `socially-maximum' resource
allocation that maximizes the total rewards in cognitive medium access without
paying a huge computational cost that grows exponentially as a function of the
problem size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03903</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03903</id><created>2015-02-13</created><authors><author><keyname>Yang</keyname><forenames>Shenghao</forenames></author><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>You</keyname><forenames>Lizhao</forenames></author></authors><title>Coding for Network-Coded Slotted ALOHA</title><categories>cs.IT math.IT</categories><comments>Full version of a conference paper accepted by ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slotted ALOHA can benefit from physical-layer network coding (PNC) by
decoding one or multiple linear combinations of the packets simultaneously
transmitted in a timeslot, forming a system of linear equations. Different
systems of linear equations are recovered in different timeslots. A message
decoder then recovers the original packets of all the users by jointly solving
multiple systems of linear equations obtained over different timeslots. We
propose the batched BP decoding algorithm that combines belief propagation (BP)
and local Gaussian elimination. Compared with pure Gaussian elimination
decoding, our algorithm reduces the decoding complexity from cubic to linear
function of the number of users. Compared with the ordinary BP decoding
algorithm for low-density generator-matrix codes, our algorithm has better
performance and the same order of computational complexity. We analyze the
performance of the batched BP decoding algorithm by generalizing the tree-based
approach and provide an approach to optimize the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03908</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03908</id><created>2015-02-13</created><authors><author><keyname>Hassan</keyname><forenames>Naveed Ul</forenames></author><author><keyname>Khalid</keyname><forenames>Yawar Ismail</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author></authors><title>Customer Engagement Plans for Peak Load Reduction in Residential Smart
  Grids</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and study the effectiveness of customer engagement
plans that clearly specify the amount of intervention in customer's load
settings by the grid operator for peak load reduction. We suggest two different
types of plans, including Constant Deviation Plans (CDPs) and Proportional
Deviation Plans (PDPs). We define an adjustable reference temperature for both
CDPs and PDPs to limit the output temperature of each thermostat load and to
control the number of devices eligible to participate in Demand Response
Program (DRP). We model thermostat loads as power throttling devices and design
algorithms to evaluate the impact of power throttling states and plan
parameters on peak load reduction. Based on the simulation results, we
recommend PDPs to the customers of a residential community with variable
thermostat set point preferences, while CDPs are suitable for customers with
similar thermostat set point preferences. If thermostat loads have multiple
power throttling states, customer engagement plans with less temperature
deviations from thermostat set points are recommended. Contrary to classical
ON/OFF control, higher temperature deviations are required to achieve similar
amount of peak load reduction. Several other interesting tradeoffs and useful
guidelines for designing mutually beneficial incentives for both the grid
operator and customers can also be identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03909</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03909</id><created>2015-02-13</created><authors><author><keyname>Lee</keyname><forenames>Kyu-Min</forenames></author><author><keyname>Min</keyname><forenames>Byungjoon</forenames></author><author><keyname>Goh</keyname><forenames>Kwang-Il</forenames></author></authors><title>Towards real-world complexity: an introduction to multiplex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>20 pages, 10 figures</comments><journal-ref>Eur. Phys. J. B 88, 48 (2015)</journal-ref><doi>10.1140/epjb/e2015-50742-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world complex systems are best modeled by multiplex networks of
interacting network layers. The multiplex network study is one of the newest
and hottest themes in the statistical physics of complex networks. Pioneering
studies have proven that the multiplexity has broad impact on the system's
structure and function. In this Colloquium paper, we present an organized
review of the growing body of current literature on multiplex networks by
categorizing existing studies broadly according to the type of layer coupling
in the problem. Major recent advances in the ?field are surveyed and some
outstanding open challenges and future perspectives will be proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03913</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03913</id><created>2015-02-13</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>L</keyname><forenames>Smitha M.</forenames></author></authors><title>Skeleton Matching based approach for Text Localization in Scene Images</title><categories>cs.CV</categories><comments>10 pages, 8 figures, Eighth International Conference on Image and
  Signal Processing,Elsevier Publications,pp: 145-153, held at UVCE, Bangalore
  in July 2014. ISBN: 9789351072522</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a skeleton matching based approach which aids in
text localization in scene images. The input image is preprocessed and
segmented into blocks using connected component analysis. We obtain the
skeleton of the segmented block using morphology based approach. The
skeletonized images are compared with the trained templates in the database to
categorize into text and non-text blocks. Further, the newly designed
geometrical rules and morphological operations are employed on the detected
text blocks for scene text localization. The experimental results obtained on
publicly available standard datasets illustrate that the proposed method can
detect and localize the texts of various sizes, fonts and colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03918</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03918</id><created>2015-02-13</created><authors><author><keyname>Shekar</keyname><forenames>B. H.</forenames></author><author><keyname>L</keyname><forenames>Smitha M.</forenames></author></authors><title>Gradient Difference based approach for Text Localization in Compressed
  domain</title><categories>cs.CV</categories><comments>11 pages, Second International Conference on Emerging Research in
  Computing, Information, Communications and Applications, Elsevier
  Publications, ISBN: 9789351072638, vol. III, pp: 299-308, held at NMIT,
  Bangalore August 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a gradient difference based approach to text
localization in videos and scene images. The input video frame/ image is first
compressed using multilevel 2-D wavelet transform. The edge information of the
reconstructed image is found which is further used for finding the maximum
gradient difference between the pixels and then the boundaries of the detected
text blocks are computed using zero crossing technique. We perform logical AND
operation of the text blocks obtained by gradient difference and the zero
crossing technique followed by connected component analysis to eliminate the
false positives. Finally, the morphological dilation operation is employed on
the detected text blocks for scene text localization. The experimental results
obtained on publicly available standard datasets illustrate that the proposed
method can detect and localize the texts of various sizes, fonts and colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03919</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03919</id><created>2015-02-13</created><updated>2015-06-08</updated><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Chow</keyname><forenames>Yinlam</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Policy Gradient for Coherent Risk Measures</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several authors have recently developed risk-sensitive policy gradient
methods that augment the standard expected cost minimization problem with a
measure of variability in cost. These studies have focused on specific
risk-measures, such as the variance or conditional value at risk (CVaR). In
this work, we extend the policy gradient method to the whole class of coherent
risk measures, which is widely accepted in finance and operations research,
among other fields. We consider both static and time-consistent dynamic risk
measures. For static risk measures, our approach is in the spirit of policy
gradient algorithms and combines a standard sampling approach with convex
programming. For dynamic risk measures, our approach is actor-critic style and
involves explicit approximation of value function. Most importantly, our
contribution presents a unified approach to risk-sensitive reinforcement
learning that generalizes and extends previous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03942</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03942</id><created>2015-02-13</created><updated>2015-10-19</updated><authors><author><keyname>H&#xfc;bschle-Schneider</keyname><forenames>Lorenz</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Ingo</forenames></author></authors><title>Communication Efficient Algorithms for Top-k Selection Problems</title><categories>cs.DS cs.DC</categories><acm-class>D.1.3; F.2.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present scalable parallel algorithms with sublinear per-processor
communication volume and low latency for several fundamental problems related
to finding the most relevant elements in a set, for various notions of
relevance: We begin with the classical selection problem with unsorted input.
We present generalizations with locally sorted inputs, dynamic content
(bulk-parallel priority queues), and multiple criteria. Then we move on to
finding frequent objects and top-k sum aggregation. Since it is unavoidable
that the output of these algorithms might be unevenly distributed over the
processors, we also explain how to redistribute this data with minimal
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03943</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03943</id><created>2015-02-13</created><authors><author><keyname>Allen</keyname><forenames>Robert B.</forenames></author></authors><title>Improving Access to Digitized Historical Newspapers with Text Mining,
  Coordinated Models, and Formative User Interface Design</title><categories>cs.DL</categories><comments>IFLA Newspaper Section IFLA Newspaper Meeting, New Delhi, February
  2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most tools for accessing digitized historical newspapers emphasize relatively
simple search; but, as increasing numbers of digitized historical newspapers
and other historical resources become available we can consider much richer
modes of interaction with these collections. For instance, users might use
exploratory search for looking at larger issues and events such as elections
and campaigns or to get a sense of &quot;the texture of the city&quot; or &quot;what the city
was thinking&quot;. To take full advantage of rich interface tools, the content of
the newspapers needs to be described systematically and accurately. Moreover,
collections of multiple newspapers need to be richly cross-indexed across
titles and even with historical resources beyond the newspapers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03945</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03945</id><created>2015-02-13</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Xenophon</forenames></author><author><keyname>Kanellopoulos</keyname><forenames>Panagiotis</forenames></author><author><keyname>Krimpas</keyname><forenames>George A.</forenames></author><author><keyname>Protopapas</keyname><forenames>Nikos</forenames></author><author><keyname>Voudouris</keyname><forenames>Alexandros A.</forenames></author></authors><title>Efficiency and complexity of price competition among single-product
  vendors</title><categories>cs.GT cs.AI cs.CC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent progress on pricing in the AI literature, we study
marketplaces that contain multiple vendors offering identical or similar
products and unit-demand buyers with different valuations on these vendors. The
objective of each vendor is to set the price of its product to a fixed value so
that its profit is maximized. The profit depends on the vendor's price itself
and the total volume of buyers that find the particular price more attractive
than the price of the vendor's competitors. We model the behaviour of buyers
and vendors as a two-stage full-information game and study a series of
questions related to the existence, efficiency (price of anarchy) and
computational complexity of equilibria in this game. To overcome situations
where equilibria do not exist or exist but are highly inefficient, we consider
the scenario where some of the vendors are subsidized in order to keep prices
low and buyers highly satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03946</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03946</id><created>2015-02-13</created><updated>2015-07-08</updated><authors><author><keyname>Angelopoulos</keyname><forenames>Spyros</forenames></author><author><keyname>Lucarelli</keyname><forenames>Giorgio</forenames></author><author><keyname>Thang</keyname><forenames>Nguyen Kim</forenames></author></authors><title>Primal-dual and dual-fitting analysis of online scheduling algorithms
  for generalized flow-time problems</title><categories>cs.DS</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online scheduling problems on a single processor that can be viewed
as extensions of the well-studied problem of minimizing total weighted flow
time. In particular, we provide a framework of analysis that is derived by
duality properties, does not rely on potential functions and is applicable to a
variety of scheduling problems. A key ingredient in our approach is bypassing
the need for &quot;black-box&quot; rounding of fractional solutions, which yields
improved competitive ratios.
  We begin with an interpretation of Highest-Density-First (HDF) as a
primal-dual algorithm, and a corresponding proof that HDF is optimal for total
fractional weighted flow time (and thus scalable for the integral objective).
Building upon the salient ideas of the proof, we show how to apply and extend
this analysis to the more general problem of minimizing $\sum_j w_j g(F_j)$,
where $w_j$ is the job weight, $F_j$ is the flow time and $g$ is a
non-decreasing cost function. Among other results, we present improved
competitive ratios for the setting in which $g$ is a concave function, and the
setting of same-density jobs but general cost functions. We further apply our
framework of analysis to online weighted completion time with general cost
functions as well as scheduling under polyhedral constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03951</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03951</id><created>2015-02-13</created><updated>2015-07-02</updated><authors><author><keyname>Straubing</keyname><forenames>Howard</forenames></author><author><keyname>Weil</keyname><forenames>Pascal</forenames></author></authors><title>Varieties</title><categories>cs.FL</categories><msc-class>68Q70, 20M07</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This text is devoted to the theory of varieties, which provides an important
tool, based in universal algebra, for the classification of regular languages.
In the introductory section, we present a number of examples that illustrate
and motivate the fundamental concepts. We do this for the most part without
proofs, and often without precise definitions, leaving these to the formal
development of the theory that begins in Section 2. Our presentation of the
theory draws heavily on the work of Gehrke, Grigorieff and Pin (2008) on the
equational theory of lattices of regular languages. In the subsequent sections
we consider in more detail aspects of varieties that were only briefly evoked
in the introduction: Decidability, operations on languages, and
characterizations in formal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03965</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03965</id><created>2015-02-13</created><authors><author><keyname>Giannopoulou</keyname><forenames>Archontia C.</forenames></author><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Uniform Kernelization Complexity of Hitting Forbidden Minors</title><categories>cs.DS cs.CC</categories><comments>34 pages, 3 figures</comments><msc-class>05C85, 68Q25</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The F-Minor-Free Deletion problem asks, for a fixed set F and an input
consisting of a graph G and integer k, whether k vertices can be removed from G
such that the resulting graph does not contain any member of F as a minor. This
paper analyzes to what extent provably effective and efficient preprocessing is
possible for F-Minor-Free Deletion. Fomin et al. (FOCS 2012) showed that the
special case Planar F-Deletion (when F contains at least one planar graph) has
a kernel of size f(F) * k^{g(F)} for some functions f and g. The degree g of
the polynomial grows very quickly; it is not even known to be computable. Fomin
et al. left open whether Planar F-Deletion has kernels whose size is uniformly
polynomial, i.e., of the form f(F) * k^c for some universal constant c that
does not depend on F. Our results in this paper are twofold. (1) We prove that
some Planar F-Deletion problems do not have uniformly polynomial kernels
(unless NP is in coNP/poly). In particular, we prove that Treewidth-Eta
Deletion does not have a kernel with O(k^{eta/4} - eps) vertices for any eps &gt;
0, unless NP is in coNP/poly. In fact, we even prove the kernelization lower
bound for the larger parameter vertex cover number. This resolves an open
problem of Cygan et al. (IPEC 2011). It is a natural question whether further
restrictions on F lead to uniformly polynomial kernels. However, we prove that
even when F contains a path, the degree of the polynomial must, in general,
depend on the set F. (2) A canonical F-Minor-Free Deletion problem when F
contains a path is Treedepth-eta Deletion: can k vertices be removed to obtain
a graph of treedepth at most eta? We prove that Treedepth-eta Deletion admits
uniformly polynomial kernels with O(k^6) vertices for every fixed eta. In order
to develop the kernelization we prove several new results about the structure
of optimal treedepth-decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03967</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03967</id><created>2015-02-13</created><authors><author><keyname>Liang</keyname><forenames>Ye</forenames></author></authors><title>Extractions: Computable and Visible Analogues of Localizations for
  Polynomial Ideals</title><categories>cs.SC math.AC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When studying local properties of a polynomial ideal, one usually needs a
theoretic technique called localization. For most cases, in spite of its
importance, the computation in a localized ring cannot be algorithmically
preformed. On the other hand, the standard basis method is very effective for
the computation in a special kind of localized rings, but for a general
semigroup order the geometry of the localization of a positive-dimensional
ideal is difficult to interpret.
  In this paper, we introduce a new ideal operation called extraction. For an
ideal $I$ in a polynomial ring $K[x_1,\ldots,x_n]$ over a field $K$, we use
another ideal $J$ to control the primary components of $I$ and the result
$\beta(I,J)$ is called the extraction of $I$ by $J$. It is still a polynomial
ideal and has a concrete geometric meaning in $\bar{K}^n$, i.e., we keep the
branches of $\textbf{V}(I) \subset \bar{K}^n$ that intersect with
$\textbf{V}(J) \subset \bar{K}^n$ and delete others, where $\bar{K}$ is the
algebraic closure of $K$. This is what we mean by visible. On the other hand,
we can use the standard basis method to compute a localized ideal corresponding
to $\beta(I,J)$ without a complete primary decomposition, and can do further
computation in the localized ring such as determining the membership problem of
$\beta(I,J)$. Moreover, we prove that extractions are as powerful as
localizations in the sense that for any multiplicatively closed subset $S$ of
$K[x_1,\ldots,x_n]$ and any polynomial ideal $I$, there always exists a
polynomial ideal $J$ such that $\beta(I,J)=(S^{-1}I)^c$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03971</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03971</id><created>2015-02-13</created><authors><author><keyname>Petersen</keyname><forenames>Casper</forenames></author><author><keyname>Rotbart</keyname><forenames>Noy</forenames></author><author><keyname>Simonsen</keyname><forenames>Jakob Grue</forenames></author><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Near-optimal adjacency labeling scheme for power-law graphs</title><categories>cs.DC cs.DS</categories><acm-class>E.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adjacency labeling scheme is a method that assigns labels to the vertices
of a graph such that adjacency between vertices can be inferred directly from
the assigned label, without using a centralized data structure. We devise
adjacency labeling schemes for the family of power-law graphs. This family that
has been used to model many types of networks, e.g. the Internet AS-level
graph. Furthermore, we prove an almost matching lower bound for this family. We
also provide an asymptotically near- optimal labeling scheme for sparse graphs.
Finally, we validate the efficiency of our labeling scheme by an experimental
evaluation using both synthetic data and real-world networks of up to hundreds
of thousands of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03974</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03974</id><created>2015-02-13</created><authors><author><keyname>Atserias</keyname><forenames>Albert</forenames></author></authors><title>A Note on Semi-Algebraic Proofs and Gaussian Elimination over Prime
  Fields</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we show that unsatisfiable systems of linear equations with a
constant number of variables per equation over prime finite fields have
polynomial-size constant-degree semi-algebraic proofs of unsatisfiability.
These are proofs that manipulate polynomial inequalities over the reals with
variables ranging in $\{0,1\}$. This upper bound is to be put in contrast with
the known fact that, for certain explicit systems of linear equations over the
two-element field, such refutations require linear degree and exponential size
if they are restricted to so-called static semi-algebraic proofs, and even
tree-like semi-algebraic and sums-of-squares proofs. Our upper bound is a more
or less direct translation of an argument due to Grigoriev, Hirsch and
Pasechnik (Moscow Mathematical Journal, 2002) who did it for a family of linear
systems of interest in propositional proof complexity. We point out that their
method is more general and can be thought of as simulating Gaussian
elimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03985</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03985</id><created>2015-02-13</created><authors><author><keyname>Klein</keyname><forenames>Rolf</forenames></author><author><keyname>Kriesel</keyname><forenames>David</forenames></author><author><keyname>Langetepe</keyname><forenames>Elmar</forenames></author></authors><title>A local strategy for cleaning expanding cellular domains by simple
  robots</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strategy SEP for finite state machines tasked with cleaning a
cellular environment in which a contamination spreads. Initially, the
contaminated area is of height $h$ and width $w$. It may be bounded by four
monotonic chains, and contain rectangular holes. The robot does not know the
initial contamination, sensing only the eight cells in its neighborhood. It
moves from cell to cell, $d$ times faster than the contamination spreads, and
is able to clean its current cell. A speed of $d&lt;\sqrt{2}(h+w)$ is in general
not sufficient to contain the contamination. Our strategy SEP succeeds if $d
\geq 3(h+w)$ holds. It ensures that the contaminated cells stay connected.
Greedy strategies violating this principle need speed at least $d \geq 4(h+w)$;
all bounds are up to small additive constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03986</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03986</id><created>2015-02-13</created><updated>2015-04-30</updated><authors><author><keyname>Amadini</keyname><forenames>Roberto</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Mauro</keyname><forenames>Jacopo</forenames></author></authors><title>A Multicore Tool for Constraint Solving</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  *** To appear in IJCAI 2015 proceedings *** In Constraint Programming (CP), a
portfolio solver uses a variety of different solvers for solving a given
Constraint Satisfaction / Optimization Problem. In this paper we introduce
sunny-cp2: the first parallel CP portfolio solver that enables a dynamic,
cooperative, and simultaneous execution of its solvers in a multicore setting.
It incorporates state-of-the-art solvers, providing also a usable and
configurable framework. Empirical results are very promising. sunny-cp2 can
even outperform the performance of the oracle solver which always selects the
best solver of the portfolio for a given problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03989</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03989</id><created>2015-02-13</created><updated>2015-04-21</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Karpov</keyname><forenames>Nikolay</forenames></author><author><keyname>Kulikov</keyname><forenames>Alexander S.</forenames></author></authors><title>Parameterized Complexity of Secluded Connectivity Problems</title><categories>cs.DS</categories><comments>Minor corrections are done</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Secluded Path problem models a situation where a sensitive information
has to be transmitted between a pair of nodes along a path in a network. The
measure of the quality of a selected path is its exposure, which is the total
weight of vertices in its closed neighborhood. In order to minimize the risk of
intercepting the information, we are interested in selecting a secluded path,
i.e. a path with a small exposure. Similarly, the Secluded Steiner Tree problem
is to find a tree in a graph connecting a given set of terminals such that the
exposure of the tree is minimized. The problems were introduced by Chechik et
al. in [ESA 2013]. Among other results, Chechik et al. have shown that Secluded
Path is fixed-parameter tractable (FPT) on unweighted graphs being
parameterized by the maximum vertex degree of the graph and that Secluded
Steiner Tree is FPT parameterized by the treewidth of the graph. In this work,
we obtain the following results about parameterized complexity of secluded
connectivity problems.
  We give FPT-algorithms deciding if a graph G with a given cost function
contains a secluded path and a secluded Steiner tree of exposure at most k with
the cost at most C.
  We initiate the study of &quot;above guarantee&quot; parameterizations for secluded
problems, where the lower bound is given by the size of a Steiner tree.
  We investigate Secluded Steiner Tree from kernelization perspective and
provide several lower and upper bounds when parameters are the treewidth, the
size of a vertex cover, maximum vertex degree and the solution size. Finally,
we refine the algorithmic result of Chechik et al. by improving the exponential
dependence from the treewidth of the input graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.03990</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.03990</id><created>2015-02-13</created><authors><author><keyname>Zamani</keyname><forenames>M.</forenames></author><author><keyname>Trumpf</keyname><forenames>J.</forenames></author><author><keyname>Mahony</keyname><forenames>R.</forenames></author></authors><title>Nonlinear Attitude Filtering: A Comparison Study</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains a concise comparison of a number of nonlinear attitude
filtering methods that have attracted attention in the robotics and aviation
literature. With the help of previously published surveys and comparison
studies, the vast literature on the subject is narrowed down to a small pool of
competitive attitude filters. Amongst these filters is a second-order optimal
minimum-energy filter recently proposed by the authors. Easily comparable
discretized unit quaternion implementations of the selected filters are
provided. We conduct a simulation study and compare the transient behaviour and
asymptotic convergence of these filters in two scenarios with different
initialization and measurement errors inspired by applications in unmanned
aerial robotics and space flight. The second-order optimal minimum-energy
filter is shown to have the best performance of all filters, including the
industry standard multiplicative extended Kalman filter (MEKF).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04010</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04010</id><created>2015-02-13</created><updated>2015-05-23</updated><authors><author><keyname>Zenteno</keyname><forenames>Efrain</forenames></author><author><keyname>Khan</keyname><forenames>Zain Ahmed</forenames></author><author><keyname>Isaksson</keyname><forenames>Magnus</forenames></author><author><keyname>Handel</keyname><forenames>Peter</forenames></author></authors><title>Finding Structural Information of RF Power Amplifiers using an
  Orthogonal Non-Parametric Kernel Smoothing Estimator</title><categories>cs.IT math.IT</categories><comments>Matlab sample code (15 MB):
  https://dl.dropboxusercontent.com/u/106958743/SampleMatlabKernel.zip</comments><doi>10.1109/TVT.2015.2434497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-parametric technique for modeling the behavior of power amplifiers is
presented. The proposed technique relies on the principles of density
estimation using the kernel method and is suited for use in power amplifier
modeling. The proposed methodology transforms the input domain into an
orthogonal memory domain. In this domain, non-parametric static functions are
discovered using the kernel estimator. These orthogonal, non-parametric
functions can be fitted with any desired mathematical structure, thus
facilitating its implementation. Furthermore, due to the orthogonality, the
non-parametric functions can be analyzed and discarded individually, which
simplifies pruning basis functions and provides a tradeoff between complexity
and performance. The results show that the methodology can be employed to model
power amplifiers, therein yielding error performance similar to
state-of-the-art parametric models. Furthermore, a parameter-efficient model
structure with 6 coefficients was derived for a Doherty power amplifier,
therein significantly reducing the deployment's computational complexity.
Finally, the methodology can also be well exploited in digital linearization
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04013</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04013</id><created>2015-02-13</created><authors><author><keyname>Selyunin</keyname><forenames>Konstantin</forenames></author><author><keyname>Ratasich</keyname><forenames>Denise</forenames></author><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author></authors><title>Deep Neural Programs for Adaptive Control in Cyber-Physical Systems</title><categories>cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Deep Neural Programs (DNP), a novel programming paradigm for
writing adaptive controllers for cy-ber-physical systems (CPS). DNP replace if
and while statements, whose discontinuity is responsible for undecidability in
CPS analysis, intractability in CPS design, and frailness in CPS
implementation, with their smooth, neural nif and nwhile counterparts. This not
only makes CPS analysis decidable and CPS design tractable, but also allows to
write robust and adaptive CPS code. In DNP the connection between the sigmoidal
guards of the nif and nwhile statements has to be given as a Gaussian Bayesian
network, which reflects the partial knowledge, the CPS program has about its
environment. To the best of our knowledge, DNP are the first approach linking
neural networks to programs, in a way that makes explicit the meaning of the
network. In order to prove and validate the usefulness of DNP, we use them to
write and learn an adaptive CPS controller for the parallel parking of the
Pioneer rovers available in our CPS lab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04014</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04014</id><created>2015-02-13</created><updated>2015-02-27</updated><authors><author><keyname>Franzago</keyname><forenames>Mirco</forenames></author><author><keyname>Malavolta</keyname><forenames>Ivano</forenames></author><author><keyname>Muccini</keyname><forenames>Henry</forenames></author></authors><title>Stakeholders, Viewpoints and Languages of a Modelling Framework for the
  Design and Development of Data-Intensive Mobile Apps</title><categories>cs.SE</categories><comments>Workshop MOBILEng 2014</comments><report-no>MOBILEng/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today millions of mobile apps are downloaded and used all over the world.
Guidelines and best practices on how to design and develop mobile apps are
being periodically released, mainly by mobile platform vendors and researchers.
They cover different concerns, and refer to different technical and
non-technical stakeholders. Still, mobile applications are developed with
ad-hoc development processes, and on-paper best practices. In this paper we
discuss a multi-view modelling framework supporting the collaborative design
and development of mobile apps. The proposed framework embraces the
Model-Driven Engineering methodology. This paper provides an overall view of
the modelling framework in terms of its main stakeholders, viewpoints, and
modelling languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04015</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04015</id><created>2015-02-13</created><authors><author><keyname>Gipp</keyname><forenames>Bela</forenames></author><author><keyname>Meuschke</keyname><forenames>Norman</forenames></author><author><keyname>Gernandt</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Decentralized Trusted Timestamping using the Crypto Currency Bitcoin</title><categories>cs.CR</categories><comments>to appear at the iConference 2015, Newport Beach, CA, USA, March
  24-27, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trusted timestamping is a process for proving that certain information
existed at a given point in time. This paper presents a trusted timestamping
concept and its implementation in form of a web-based service that uses the
decentralized Bitcoin block chain to store anonymous, tamper-proof timestamps
for digital content. The service allows users to hash files, such as text,
photos or videos, and store the created hashes in the Bitcoin block chain.
Users can then retrieve and verify the timestamps that have been committed to
the block chain. The non-commercial service enables anyone, e.g., researchers,
authors, journalists, students, or artists, to prove that they were in
possession of certain information at a given point in time. Common use cases
include proving that a contract has been signed, a photo taken, a video
recorded, or a task completed prior to a certain date. All procedures maintain
complete privacy of the user's data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04019</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04019</id><created>2015-02-13</created><updated>2015-05-08</updated><authors><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Inducing Approximately Optimal Flow Using Truthful Mediators</title><categories>cs.GT</categories><comments>Version with latencies not normalized</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a classic coordination problem from the perspective of mechanism
design: how can we coordinate a social welfare maximizing flow in a network
congestion game with selfish players? The classical approach, which computes
tolls as a function of known demands, fails when the demands are unknown to the
mechanism designer, and naively eliciting them does not necessarily yield a
truthful mechanism. Instead, we introduce a weak mediator that can provide
suggested routes to players and set tolls as a function of reported demands.
However, players can choose to ignore or misreport their type to this mediator.
Using techniques from differential privacy, we show how to design a weak
mediator such that it is an asymptotic ex-post Nash equilibrium for all players
to truthfully report their types to the mediator and faithfully follow its
suggestion, and that when they do, they end up playing a nearly optimal flow.
Notably, our solution works in settings of incomplete information even in the
absence of a prior distribution on player types. Along the way, we develop new
techniques for privately solving convex programs which may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04022</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04022</id><created>2015-02-13</created><authors><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author><author><keyname>Yodpinyanee</keyname><forenames>Anak</forenames></author></authors><title>Local Computation Algorithms for Graphs of Non-Constant Degrees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the model of \emph{local computation algorithms} (LCAs), we aim to compute
the queried part of the output by examining only a small (sublinear) portion of
the input. Many recently developed LCAs on graph problems achieve time and
space complexities with very low dependence on $n$, the number of vertices.
Nonetheless, these complexities are generally at least exponential in $d$, the
upper bound on the degree of the input graph. Instead, we consider the case
where parameter $d$ can be moderately dependent on $n$, and aim for
complexities with subexponential dependence on $d$, while maintaining
polylogarithmic dependence on $n$. We present: a randomized LCA for computing
maximal independent sets whose time and space complexities are quasi-polynomial
in $d$ and polylogarithmic in $n$; for constant $\epsilon &gt; 0$, a randomized
LCA that provides a $(1-\epsilon)$-approximation to maximum matching whose time
and space complexities are polynomial in $d$ and polylogarithmic in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04023</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04023</id><created>2015-02-13</created><updated>2015-06-11</updated><authors><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author><author><keyname>Banescu</keyname><forenames>Sebastian</forenames></author><author><keyname>Ochoa</keyname><forenames>Mart&#xed;n</forenames></author></authors><title>The Meaning of Attack-Resistant Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a formal notion of partial compliance, called
Attack-resistance, of a computer program running together with a defense
mechanism w.r.t a non-exploitability specification. In our setting, a program
may contain exploitable vulnerabilities, such as buffer overflows, but
appropriate defense mechanisms built into the program or the operating system
render such vulnerabilities hard to exploit by certain attackers, usually
relying on the strength of the randomness of a probabilistic transformation of
the environment or the program and some knowledge on the attacker's goals and
attack strategy. We are motivated by the reality that most large-scale programs
have vulnerabilities despite our best efforts to get rid of them. Security
researchers have responded to this state of affairs by coming up with ingenious
defense mechanisms such as address space layout randomization (ASLR) or
instruction set randomization (ISR) that provide some protection against
exploitation. However, implementations of such mechanism have been often shown
to be insecure, even against the attacks they were designed to prevent. By
formalizing this notion of attack-resistance we pave the way towards addressing
the questions: &quot;How do we formally analyze defense mechanisms? Is there a
mathematical way of distinguishing effective defense mechanisms from
ineffective ones? Can we quantify and show that these defense mechanisms
provide formal security guarantees, albeit partial, even in the presence of
exploitable vulnerabilities?&quot;. To illustrate our approach we discuss under
which circumstances ISR implementations comply with the Attack-resistance
definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04025</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04025</id><created>2015-02-13</created><authors><author><keyname>Arts</keyname><forenames>Paul</forenames></author><author><keyname>Bloch</keyname><forenames>Jacques</forenames></author><author><keyname>Georg</keyname><forenames>Peter</forenames></author><author><keyname>Glaessle</keyname><forenames>Benjamin</forenames></author><author><keyname>Heybrock</keyname><forenames>Simon</forenames></author><author><keyname>Komatsubara</keyname><forenames>Yu</forenames></author><author><keyname>Lohmayer</keyname><forenames>Robert</forenames></author><author><keyname>Mages</keyname><forenames>Simon</forenames></author><author><keyname>Mendl</keyname><forenames>Bernhard</forenames></author><author><keyname>Meyer</keyname><forenames>Nils</forenames></author><author><keyname>Parcianello</keyname><forenames>Alessio</forenames></author><author><keyname>Pleiter</keyname><forenames>Dirk</forenames></author><author><keyname>Rappl</keyname><forenames>Florian</forenames></author><author><keyname>Rossi</keyname><forenames>Mauro</forenames></author><author><keyname>Solbrig</keyname><forenames>Stefan</forenames></author><author><keyname>Tecchiolli</keyname><forenames>Giampietro</forenames></author><author><keyname>Wettig</keyname><forenames>Tilo</forenames></author><author><keyname>Zanier</keyname><forenames>Gianpaolo</forenames></author></authors><title>QPACE 2 and Domain Decomposition on the Intel Xeon Phi</title><categories>cs.DC hep-lat physics.comp-ph</categories><comments>plenary talk at Lattice 2014, to appear in the conference proceedings
  PoS(LATTICE2014), 15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an overview of QPACE 2, which is a custom-designed supercomputer
based on Intel Xeon Phi processors, developed in a collaboration of Regensburg
University and Eurotech. We give some general recommendations for how to write
high-performance code for the Xeon Phi and then discuss our implementation of a
domain-decomposition-based solver and present a number of benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04032</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04032</id><created>2015-02-12</created><authors><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author></authors><title>On Projection Based Operators in Lp space for Exact Similarity Search</title><categories>cs.IR</categories><journal-ref>Fundamenta Informaticae: Annales Societatis Mathematicae Polonae,
  136: 1-14, 2015</journal-ref><doi>10.3233/FI-2015-1166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate exact indexing for high dimensional Lp norms based on the
1-Lipschitz property and projection operators. The orthogonal projection that
satisfies the 1-Lipschitz property for the Lp norm is described. The adaptive
projection defined by the first principal component is introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04033</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04033</id><created>2015-02-13</created><updated>2015-02-16</updated><authors><author><keyname>Reitmaier</keyname><forenames>Tobias</forenames></author><author><keyname>Sick</keyname><forenames>Bernhard</forenames></author></authors><title>The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised
  Training of Support Vector Machines for Classification</title><categories>cs.LG stat.ML</categories><doi>10.1016/j.ins.2015.06.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel functions in support vector machines (SVM) are needed to assess the
similarity of input samples in order to classify these samples, for instance.
Besides standard kernels such as Gaussian (i.e., radial basis function, RBF) or
polynomial kernels, there are also specific kernels tailored to consider
structure in the data for similarity assessment. In this article, we will
capture structure in data by means of probabilistic mixture density models, for
example Gaussian mixtures in the case of real-valued input spaces. From the
distance measures that are inherently contained in these models, e.g.,
Mahalanobis distances in the case of Gaussian mixtures, we derive a new kernel,
the responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernel
emphasizes the influence of model components from which any two samples that
are compared are assumed to originate (that is, the &quot;responsible&quot; model
components). We will see that this kernel outperforms the RBF kernel and other
kernels capturing structure in data (such as the LAP kernel in Laplacian SVM)
in many applications where partially labeled data are available, i.e., for
semi-supervised training of SVM. Other key advantages are that the RWM kernel
can easily be used with standard SVM implementations and training algorithms
such as sequential minimal optimization, and heuristics known for the
parametrization of RBF kernels in a C-SVM can easily be transferred to this new
kernel. Properties of the RWM kernel are demonstrated with 20 benchmark data
sets and an increasing percentage of labeled samples in the training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04042</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04042</id><created>2015-02-13</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Abstract Learning via Demodulation in a Deep Neural Network</title><categories>cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the brain, deep neural networks (DNN) are thought to learn
abstract representations through their hierarchical architecture. However, at
present, how this happens is not well understood. Here, we demonstrate that DNN
learn abstract representations by a process of demodulation. We introduce a
biased sigmoid activation function and use it to show that DNN learn and
perform better when optimized for demodulation. Our findings constitute the
first unambiguous evidence that DNN perform abstract learning in practical use.
Our findings may also explain abstract learning in the human brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04044</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04044</id><created>2015-02-13</created><authors><author><keyname>Hamid</keyname><forenames>Mohamed</forenames></author><author><keyname>Slimane</keyname><forenames>Slimane Ben</forenames></author><author><keyname>Bj&#xf6;rsell</keyname><forenames>Niclas</forenames></author></authors><title>Downlink Throughput Driven Channel Access Framework for Cognitive LTE
  Femto-Cells</title><categories>cs.IT math.IT</categories><comments>30 pages, 11 figures. Submitted to IEEE Transactions on Wireless
  Communications for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an optimized sensing based channel access framework for
the LTE cognitive femto-cells, with an objective of maximizing the femto-cells
downlink throughput. Cognitive femto-cells opportunistically transmit on the
macro-cell channels when they are free of use. Those free channels are located
by means of spectrum sensing using energy detection. Moreover, periodic sensing
is adopted to detect any changes of the sensing outcomes. The maximum
attainable femto-cell downlink throughput varies with the macro-cell channel
occupancy statistics. Therefore, the LTE macro-cell occupancy is empirically
modeled using exponential distributions mixture. The LTE cognitive femto-cell
downlink throughput is maximized by compromising the transmission efficiency,
the explored spectrum opportunities and the interference from the macro-cell.
An analytical solution for the optimal periodic sensing interval that maximizes
the throughput is found and verified by simulations. The obtained results show
that there is indeed a single periodic sensing interval value that maximizes
the LTE cognitive femto-cell downlink throughput. At the peak of the macro-cell
traffic, our framework increases the femto-cell throughput by around 15%
compared to the senseless case. The impact of the available number of channels
for opportunistic access is studied and no significant impact is found for more
than three channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04048</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04048</id><created>2015-02-13</created><updated>2015-04-10</updated><authors><author><keyname>Reitzig</keyname><forenames>Raphael</forenames></author><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author></authors><title>Efficient Algorithms for Envy-Free Stick Division With Fewest Cuts</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segal-Halevi, Hassidim, and Aumann (AAMAS, 2015) propose the problem of
cutting sticks so that at least k sticks have equal length and no other stick
is longer. This allows for an envy-free allocation of sticks to k players, one
each. The resulting number of sticks should also be minimal.
  We analyze the structure of this problem and devise a linear-time algorithm
for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04049</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04049</id><created>2015-02-13</created><authors><author><keyname>Raghavan</keyname><forenames>Preethi</forenames></author><author><keyname>Chen</keyname><forenames>James L.</forenames></author><author><keyname>Fosler-Lussier</keyname><forenames>Eric</forenames></author><author><keyname>Lai</keyname><forenames>Albert M.</forenames></author></authors><title>How essential are unstructured clinical narratives and information
  fusion to clinical trial recruitment?</title><categories>cs.CY cs.AI cs.CL</categories><comments>AMIA TBI 2014, 6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Electronic health records capture patient information using structured
controlled vocabularies and unstructured narrative text. While structured data
typically encodes lab values, encounters and medication lists, unstructured
data captures the physician's interpretation of the patient's condition,
prognosis, and response to therapeutic intervention. In this paper, we
demonstrate that information extraction from unstructured clinical narratives
is essential to most clinical applications. We perform an empirical study to
validate the argument and show that structured data alone is insufficient in
resolving eligibility criteria for recruiting patients onto clinical trials for
chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is
essential to solving 59% of the CLL trial criteria and 77% of the prostate
cancer trial criteria. More specifically, for resolving eligibility criteria
with temporal constraints, we show the need for temporal reasoning and
information integration with medical events within and across unstructured
clinical narratives and structured data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04052</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04052</id><created>2015-02-13</created><updated>2016-02-25</updated><authors><author><keyname>Barthe</keyname><forenames>Gilles</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Arias</keyname><forenames>Emilio Jes&#xfa;s Gallego</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Computer-aided verification in mechanism design</title><categories>cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mechanism design, the gold standard solution concepts are dominant
strategy incentive compatibility, and Bayesian incentive compatibility. While
these incentive properties are simple to state, their proofs are specific to
the mechanism and can be quite complex.
  This raises two concerns. From a practical perspective, checking a complex
proof can be a tedious process, but requires experts knowledgeable in mechanism
design. From a modeling perspective, if unsophisticated agents are not
convinced that incentive properties hold for complex mechanisms, they can
potentially strategize in unpredictable ways---defeating the point of incentive
properties.
  To address both concerns, we propose to use techniques from computer-aided
verification to construct formal proofs of incentive properties. Because formal
proofs can be automatically checked, agents do not need to manually check the
paper proofs. Moreover, formal proofs are an automatically checkable electronic
certificate that the claimed incentive properties hold---the agent does not
need to understand the proof.
  To demonstrate our approach, we present the verification of one sophisticated
mechanism: the generic reduction from Bayesian incentive compatible mechanism
design to algorithm design given by Hartline, Kleinberg, and Malekian. This
mechanism presents new challenges for formal verification, including essential
use of randomness from both the execution of the mechanism and from prior type
distributions. Accordingly, our result formalizes Bayesian incentive
compatibility for the entire family of mechanisms derived via this reduction.
As an intermediate step in our formalization, we also provide the first formal
verification of incentive compatibility for the celebrated
Vickrey-Clarke-Groves mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04054</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04054</id><created>2015-02-13</created><updated>2015-03-13</updated><authors><author><keyname>Doff</keyname><forenames>A. W.</forenames></author><author><keyname>Chandra</keyname><forenames>Kishor</forenames></author><author><keyname>Prasad</keyname><forenames>R. Venkatesha</forenames></author></authors><title>Sensor Assisted Movement Identification and Prediction for Beamformed 60
  GHz Links: A Report</title><categories>cs.NI</categories><comments>Internal report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large available bandwidth in 60\,GHz band promises very high data rates -- in
the order of Gb/s. However, high free-space path loss makes it necessary to
employ beamforming capable directional antennas. When beamforming is used, the
links are sensitive to misalignment in antenna directionality because of
movement of devices. To identify and circumvent the misalignments, we propose
to use the motion sensors (i.e., accelerometer and gyroscope) which are already
present in most of the modern mobile devices. By finding the extent of
misaligned beams, corrective actions are carried out to reconfigure the
antennas. Motion sensors on mobile devices provide means to estimate the extent
of misalignments. We collected real data from motion sensors and steer the
beams appropriately. The results from our study show that the sensors are
capable of detecting the cause of errors as translational or rotational
movements. Furthermore it is also shown that the sensor data can be used to
predict the next location of the user. This can be used to reconfigure the
directional antenna to switch the antenna beam directions and hence avoid
frequent link disruptions. This decreases the number of beam searches thus
lowering the MAC overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04068</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04068</id><created>2015-02-13</created><updated>2015-08-27</updated><authors><author><keyname>Duch&#xea;ne</keyname><forenames>Eric</forenames></author><author><keyname>Dufour</keyname><forenames>Matthieu</forenames></author><author><keyname>Heubach</keyname><forenames>Silvia</forenames></author><author><keyname>Larsson</keyname><forenames>Urban</forenames></author></authors><title>Building Nim</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game of nim, with its simple rules, its elegant solution and its
historical importance is the quintessence of a combinatorial game, which is why
it led to so many generalizations and modifications. We present a modification
with a new spin: building nim. With given finite numbers of tokens and stacks,
this two-player game is played in two stages (thus belonging to the same family
of games as e.g. nine-men's morris): first building, where players alternate to
put one token on one of the, initially empty, stacks until all tokens have been
used. Then, the players play nim. Of course, because the solution for the game
of nim is known, the goal of the player who starts nim play is a placement of
the tokens so that the Nim-sum of the stack heights at the end of building is
different from 0. This game is trivial if the total number of tokens is odd as
the Nim-sum could never be 0, or if both the number of tokens and the number of
stacks are even, since a simple mimicking strategy results in a Nim-sum of 0
after each of the second player's moves. We present the solution for this game
for some non-trivial cases and state a general conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04069</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04069</id><created>2015-02-13</created><authors><author><keyname>Chandra</keyname><forenames>Reza</forenames></author><author><keyname>Nugroho</keyname><forenames>Arif Purwo</forenames></author><author><keyname>Saleh</keyname><forenames>Fikri</forenames></author></authors><title>Evaluating Open Access Paper Repository In Higher Education For Asean
  Region</title><categories>cs.DL</categories><comments>International Conference on Internet Studies
  (http://www.internet-studies.org) August 16-17, 2014, Singapore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper repository at higher education is a collection of scientific articles
created by the academic society. This study took as many as 80 universities in
the Webometrics ranking of repositories in the Southeast Asia region. The tools
used in this research is Google for number of web page and Google Scholar for
number of document paper repository and Ahrefs for referring page, backlink and
reffering domain. The result of this study, Eprints is the most widely used
tools in higher education, as many as 37 higher educations (46,25%). Institut
Teknologi Sepuluh November got the highest score in number of web page in
Google (2.010.000), Bogor Agricultural University Scientific Repository got the
highest score for number of document paper (44.300). University of Sumatera
Utara Repository got the highest score for reffering page (82588) and backlink
(86421). Universiti Teknologi Malaysia Institutional Repository got the highest
score for reffering domain (532).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04071</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04071</id><created>2015-02-13</created><updated>2015-11-16</updated><authors><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>The generalized Lasso with non-linear observations</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages</comments><msc-class>94A12 (primary), 60D05, 90C25 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of signal estimation from non-linear observations when
the signal belongs to a low-dimensional set buried in a high-dimensional space.
A rough heuristic often used in practice postulates that non-linear
observations may be treated as noisy linear observations, and thus the signal
may be estimated using the generalized Lasso. This is appealing because of the
abundance of efficient, specialized solvers for this program. Just as noise may
be diminished by projecting onto the lower dimensional space, the error from
modeling non-linear observations with linear observations will be greatly
reduced when using the signal structure in the reconstruction. We allow general
signal structure, only assuming that the signal belongs to some set K in R^n.
We consider the single-index model of non-linearity. Our theory allows the
non-linearity to be discontinuous, not one-to-one and even unknown. We assume a
random Gaussian model for the measurement matrix, but allow the rows to have an
unknown covariance matrix. As special cases of our results, we recover
near-optimal theory for noisy linear observations, and also give the first
theoretical accuracy guarantee for 1-bit compressed sensing with unknown
covariance matrix of the measurement vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04081</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04081</id><created>2015-02-13</created><updated>2015-05-31</updated><authors><author><keyname>Belanger</keyname><forenames>David</forenames></author><author><keyname>Kakade</keyname><forenames>Sham</forenames></author></authors><title>A Linear Dynamical System Model for Text</title><categories>stat.ML cs.CL cs.LG</categories><comments>Accepted at International Conference of Machine Learning 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low dimensional representations of words allow accurate NLP models to be
trained on limited annotated data. While most representations ignore words'
local context, a natural way to induce context-dependent representations is to
perform inference in a probabilistic latent-variable sequence model. Given the
recent success of continuous vector space word representations, we provide such
an inference procedure for continuous states, where words' representations are
given by the posterior mean of a linear dynamical system. Here, efficient
inference can be performed using Kalman filtering. Our learning algorithm is
extremely scalable, operating on simple cooccurrence counts for both parameter
initialization using the method of moments and subsequent iterations of EM. In
our experiments, we employ our inferred word embeddings as features in standard
tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman
filter updates can be seen as a linear recurrent neural network. We demonstrate
that using the parameters of our model to initialize a non-linear recurrent
neural network language model reduces its training time by a day and yields
lower perplexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04095</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04095</id><created>2015-02-13</created><authors><author><keyname>Geneson</keyname><forenames>Jesse</forenames></author><author><keyname>Tian</keyname><forenames>Peter</forenames></author></authors><title>Sequences of formation width $4$ and alternation length $5$</title><categories>cs.DM math.CO</categories><comments>20 pages</comments><msc-class>05D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence pattern avoidance is a central topic in combinatorics. A sequence
$s$ contains a sequence $u$ if some subsequence of $s$ can be changed into $u$
by a one-to-one renaming of its letters. If $s$ does not contain $u$, then $s$
avoids $u$. A widely studied extremal function related to pattern avoidance is
$Ex(u, n)$, the maximum length of an $n$-letter sequence that avoids $u$ and
has every $r$ consecutive letters pairwise distinct, where $r$ is the number of
distinct letters in $u$.
  We bound $Ex(u, n)$ using the formation width function, $fw(u)$, which is the
minimum $s$ for which there exists $r$ such that any concatenation of $s$
permutations, each on the same $r$ letters, contains $u$. In particular, we
identify every sequence $u$ such that $fw(u)=4$ and $u$ contains $ababa$. The
significance of this result lies in its implication that, for every such
sequence $u$, we have $Ex(u, n) = \Theta(n \alpha(n))$, where $\alpha(n)$
denotes the incredibly slow-growing inverse Ackermann function. We have thus
identified the extremal function of many infinite classes of previously
unidentified sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04099</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04099</id><created>2015-02-13</created><authors><author><keyname>Zhang</keyname><forenames>Yanyan</forenames></author><author><keyname>Han</keyname><forenames>Weijia</forenames></author><author><keyname>Li</keyname><forenames>Di</forenames></author><author><keyname>Zhang</keyname><forenames>Ping</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Power vs. Spectrum 2-D Sensing in Energy Harvesting Cognitive Radio
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvester based cognitive radio is a promising solution to address the
shortage of both spectrum and energy. Since the spectrum access and power
consumption patterns are interdependent, and the power value harvested from
certain environmental sources are spatially correlated, the new power dimension
could provide additional information to enhance the spectrum sensing accuracy.
In this paper, the Markovian behavior of the primary users is considered, based
on which we adopt a hidden input Markov model to specify the primary vs.
secondary dynamics in the system. Accordingly, we propose a 2-D spectrum and
power (harvested) sensing scheme to improve the primary user detection
performance, which is also capable of estimating the primary transmit power
level. Theoretical and simulated results demonstrate the effectiveness of the
proposed scheme, in term of the performance gain achieved by considering the
new power dimension. To the best of our knowledge, this is the first work to
jointly consider the spectrum and power dimensions for the cognitive primary
user detection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04100</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04100</id><created>2014-10-31</created><authors><author><keyname>Giuberti</keyname><forenames>Matteo</forenames></author><author><keyname>Ferrari</keyname><forenames>Gianluigi</forenames></author><author><keyname>Contin</keyname><forenames>Laura</forenames></author><author><keyname>Cimolin</keyname><forenames>Veronica</forenames></author><author><keyname>Azzaro</keyname><forenames>Corrado</forenames></author><author><keyname>Albani</keyname><forenames>Giovanni</forenames></author><author><keyname>Mauro</keyname><forenames>Alessandro</forenames></author></authors><title>Assigning UPDRS Scores in the Leg Agility Task of Parkinsonians: Can It
  Be Done through BSN-based Kinematic Variables?</title><categories>cs.CY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, by characterizing the Leg Agility (LA) task, which contributes
to the evaluation of the degree of severity of the Parkinson's Disease (PD),
through kinematic variables (including the angular amplitude and speed of
thighs' motion), we investigate the link between these variables and Unified
Parkinson's Disease Rating Scale (UPDRS) scores. Our investigation relies on
the use of a few body-worn wireless inertial nodes and represents a first step
in the design of a portable system, amenable to be integrated in Internet of
Things (IoT) scenarios, for automatic detection of the degree of severity (in
terms of UPDRS score) of PD. The experimental investigation is carried out
considering 24 PD patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04108</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04108</id><created>2015-02-13</created><authors><author><keyname>Chu</keyname><forenames>Yoonmi</forenames></author><author><keyname>Allen</keyname><forenames>Robert B.</forenames></author></authors><title>Structured Descriptions of Roles, Activities,and Procedures in the Roman
  Constitution</title><categories>cs.DL</categories><comments>6 pages, 2 figures, Presented at the Italian Research Conference on
  Digital Libraries (IRCDL 2015), Bozen-Bolzano, Italy, 29-30 January, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A highly structured description of entities and events in histories can
support flexible exploration of those histories by users and, ultimately,
support richly-linked full-text digital libraries. Here, we apply the Basic
Formal Ontology (BFO) to structure a passage about the Roman Constitution from
Gibbon's Decline and Fall of the Roman Empire. Specifically, we consider the
specification of Roles such as Consuls, Activities associated with those Roles,
and Procedures for accomplishing those Activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04110</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04110</id><created>2015-02-13</created><updated>2015-04-08</updated><authors><author><keyname>Fua</keyname><forenames>Pascal</forenames></author><author><keyname>Knott</keyname><forenames>Graham</forenames></author></authors><title>Modeling Brain Circuitry over a Wide Range of Scales</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If we are ever to unravel the mysteries of brain function at its most
fundamental level, we will need a precise understanding of how its component
neurons connect to each other. Electron Microscopes (EM) can now provide the
nanometer resolution that is needed to image synapses, and therefore
connections, while Light Microscopes (LM) see at the micrometer resolution
required to model the 3D structure of the dendritic network. Since both the
topology and the connection strength are integral parts of the brain's wiring
diagram, being able to combine these two modalities is critically important.
  In fact, these microscopes now routinely produce high-resolution imagery in
such large quantities that the bottleneck becomes automated processing and
interpretation, which is needed for such data to be exploited to its full
potential. In this paper, we briefly review the Computer Vision techniques we
have developed at EPFL to address this need. They include delineating dendritic
arbors from LM imagery, segmenting organelles from EM, and combining the two
into a consistent representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04117</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04117</id><created>2015-02-13</created><authors><author><keyname>Shahriar</keyname><forenames>Chowdhury</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Overlapped-MIMO Radar Waveform Design for Coexistence With Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>accepted at IEEE WCNC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores an overlapped-multiple-input multiple-output (MIMO)
antenna architecture and a spectrum sharing algorithm via null space projection
(NSP) for radar-communications coexistence. In the overlapped-MIMO
architecture, the transmit array of a collocated MIMO radar is partitioned into
a number of subarrays that are allowed to overlap. Each of the antenna elements
in these subarrays have signals orthogonal to each other and to the elements of
the other subarrays. The proposed architecture not only improves sidelobe
suppression to reduce interference to communications system, but also enjoys
the advantages of MIMO radar without sacrificing the main desirable
characteristics. The radar-centric spectrum sharing algorithm then projects the
radar signal onto the null space of the communications system's interference
channel, which helps to avoid interference from the radar. Numerical results
are presented which show the performance of the proposed waveform design
algorithm in terms of overall beampattern and sidelobe levels of the radar
waveform and finally shows a comparison of the proposed system with existing
collocated MIMO radar architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04120</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04120</id><created>2015-02-16</created><authors><author><keyname>Asor</keyname><forenames>Ohad</forenames></author></authors><title>About Tau-Chain</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tau-chain is a decentralized peer-to-peer network having three unified faces:
Rules, Proofs, and Computer Programs, allowing a generalization of virtually
any centralized or decentralized P2P network, together with many new abilities,
as we present on this note.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04132</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04132</id><created>2015-02-13</created><authors><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Li</keyname><forenames>Xuanchong</forenames></author><author><keyname>Lin</keyname><forenames>Ming</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander G.</forenames></author></authors><title>Long-short Term Motion Feature for Action Classification and Retrieval</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1411.6660</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for representing motion information for video
classification and retrieval. We improve upon local descriptor based methods
that have been among the most popular and successful models for representing
videos. The desired local descriptors need to satisfy two requirements: 1) to
be representative, 2) to be discriminative. Therefore, they need to occur
frequently enough in the videos and to be be able to tell the difference among
different types of motions. To generate such local descriptors, the video
blocks they are based on must contain just the right amount of motion
information. However, current state-of-the-art local descriptor methods use
video blocks with a single fixed size, which is insufficient for covering
actions with varying speeds. In this paper, we introduce a long-short term
motion feature that generates descriptors from video blocks with multiple
lengths, thus covering motions with large speed variance. Experimental results
show that, albeit simple, our model achieves state-of-the-arts results on
several benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04137</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04137</id><created>2015-02-13</created><authors><author><keyname>Abasi</keyname><forenames>Hasan</forenames></author><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author><author><keyname>Mazzawi</keyname><forenames>Hanna</forenames></author></authors><title>Non-Adaptive Learning a Hidden Hipergraph</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new deterministic algorithm that non-adaptively learns a hidden
hypergraph from edge-detecting queries. All previous non-adaptive algorithms
either run in exponential time or have non-optimal query complexity. We give
the first polynomial time non-adaptive learning algorithm for learning
hypergraph that asks almost optimal number of queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04147</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04147</id><created>2015-02-13</created><updated>2015-10-25</updated><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Bayesian Incentive-Compatible Bandit Exploration</title><categories>cs.GT</categories><comments>An extended abstract of this paper has been published in ACM EC 2015.
  This version contains complete proofs, revamped introductory sections (incl.
  a discussion of potential applications to medical trials), and thoroughly
  revised and streamlined presentation of the technical material. Two major
  extensions are fleshed out, whereas they were only informally described in
  the conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individual decision-makers consume information revealed by the previous
decision makers, and produce information that may help in future decisions.
This phenomenon is common in a wide range of scenarios in the Internet economy,
as well as in other domains such as medical decisions. Each decision-maker
would individually prefer to &quot;exploit&quot;: select an action with the highest
expected reward given her current information. At the same time, each
decision-maker would prefer previous decision-makers to &quot;explore&quot;, producing
information about the rewards of various actions. A social planner, by means of
carefully designed information disclosure, can incentivize the agents to
balance the exploration and exploitation so as to maximize social welfare.
  We formulate this problem as a multi-armed bandit problem (and various
generalizations thereof) under incentive-compatibility constraints induced by
the agents' Bayesian priors. We design an incentive-compatible bandit algorithm
for the social planner whose regret is asymptotically optimal among all bandit
algorithms (incentive-compatible or not). Further, we provide a black-box
reduction from an arbitrary multi-arm bandit algorithm to an
incentive-compatible one, with only a constant multiplicative increase in
regret. This reduction works for very general bandit setting that incorporate
contexts and arbitrary auxiliary feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04148</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04148</id><created>2015-02-13</created><updated>2015-10-01</updated><authors><author><keyname>Voss</keyname><forenames>James</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author></authors><title>A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA</title><categories>cs.LG stat.ML</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent Component Analysis (ICA) is a popular model for blind signal
separation. The ICA model assumes that a number of independent source signals
are linearly mixed to form the observed signals. We propose a new algorithm,
PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for
ICA with Gaussian noise. The main technical innovation of the algorithm is to
use a fixed point iteration in a pseudo-Euclidean (indefinite &quot;inner product&quot;)
space. The use of this indefinite &quot;inner product&quot; resolves technical issues
common to several existing algorithms for noisy ICA. This leads to an algorithm
which is conceptually simple, efficient and accurate in testing.
  Our second contribution is combining PEGI with the analysis of objectives for
optimal recovery in the noisy ICA model. It has been observed that the direct
approach of demixing with the inverse of the mixing matrix is suboptimal for
signal recovery in terms of the natural Signal to Interference plus Noise Ratio
(SINR) criterion. There have been several partial solutions proposed in the ICA
literature. It turns out that any solution to the mixing matrix reconstruction
problem can be used to construct an SINR-optimal ICA demixing, despite the fact
that SINR itself cannot be computed from data. That allows us to obtain a
practical and provably SINR-optimal recovery method for ICA with arbitrary
Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04149</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04149</id><created>2015-02-13</created><updated>2015-09-30</updated><authors><author><keyname>Huang</keyname><forenames>Po-Sen</forenames></author><author><keyname>Kim</keyname><forenames>Minje</forenames></author><author><keyname>Hasegawa-Johnson</keyname><forenames>Mark</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>Joint Optimization of Masks and Deep Recurrent Neural Networks for
  Monaural Source Separation</title><categories>cs.SD cs.AI cs.LG cs.MM</categories><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  vol.23, no.12, pp.2136-2147, Dec. 2015</journal-ref><doi>10.1109/TASLP.2015.2468583</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monaural source separation is important for many real world applications. It
is challenging because, with only a single channel of information available,
without any constraints, an infinite number of solutions are possible. In this
paper, we explore joint optimization of masking functions and deep recurrent
neural networks for monaural source separation tasks, including monaural speech
separation, monaural singing voice separation, and speech denoising. The joint
optimization of the deep recurrent neural networks with an extra masking layer
enforces a reconstruction constraint. Moreover, we explore a discriminative
criterion for training neural networks to further enhance the separation
performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT
datasets for speech separation, singing voice separation, and speech denoising
tasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to
NMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and
4.32--5.42 dB GSIR gain compared to existing models in the singing voice
separation task, and outperform NMF and DNN baselines in the speech denoising
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04154</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04154</id><created>2015-02-13</created><authors><author><keyname>Goldberg</keyname><forenames>Mark K.</forenames></author><author><keyname>Hayvanovych</keyname><forenames>Mykola</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author><author><keyname>Wallace</keyname><forenames>William A.</forenames></author></authors><title>Extracting Hidden Groups and their Structure from Streaming Interaction
  Data</title><categories>cs.SI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When actors in a social network interact, it usually means they have some
general goal towards which they are collaborating. This could be a research
collaboration in a company or a foursome planning a golf game. We call such
groups \emph{planning groups}. In many social contexts, it might be possible to
observe the \emph{dyadic interactions} between actors, even if the actors do
not explicitly declare what groups they belong too. When groups are not
explicitly declared, we call them \emph{hidden groups}. Our particular focus is
hidden planning groups. By virtue of their need to further their goal, the
actors within such groups must interact in a manner which differentiates their
communications from random background communications. In such a case, one can
infer (from these interactions) the composition and structure of the hidden
planning groups. We formulate the problem of hidden group discovery from
streaming interaction data, and we propose efficient algorithms for identifying
the hidden group structures by isolating the hidden group's non-random,
planning-related, communications from the random background communications. We
validate our algorithms on real data (the Enron email corpus and Blog
communication data). Analysis of the results reveals that our algorithms
extract meaningful hidden group structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04156</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04156</id><created>2015-02-13</created><updated>2015-11-24</updated><authors><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Lee</keyname><forenames>Dong-Hyun</forenames></author><author><keyname>Bornschein</keyname><forenames>Jorg</forenames></author><author><keyname>Lin</keyname><forenames>Zhouhan</forenames></author></authors><title>Towards Biologically Plausible Deep Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuroscientists have long criticised deep learning algorithms as incompatible
with current knowledge of neurobiology. We explore more biologically plausible
versions of deep representation learning, focusing here mostly on unsupervised
learning but developing a learning mechanism that could account for supervised,
unsupervised and reinforcement learning. The starting point is that the basic
learning rule believed to govern synaptic weight updates
(Spike-Timing-Dependent Plasticity) can be interpreted as gradient descent on
some objective function so long as the neuronal dynamics push firing rates
towards better values of the objective function (be it supervised,
unsupervised, or reward-driven). The second main idea is that this corresponds
to a form of the variational EM algorithm, i.e., with approximate rather than
exact posteriors, implemented by neural dynamics. Another contribution of this
paper is that the gradients required for updating the hidden states in the
above variational interpretation can be estimated using an approximation that
only requires propagating activations forward and backward, with pairs of
layers learning to form a denoising auto-encoder. Finally, we extend the theory
about the probabilistic interpretation of auto-encoders to justify improved
sampling schemes based on the generative interpretation of denoising
auto-encoders, and we validate all these ideas on generative learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04163</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04163</id><created>2015-02-13</created><authors><author><keyname>Junlin</keyname><forenames>Zhang</forenames></author><author><keyname>Heng</keyname><forenames>Cai</forenames></author><author><keyname>Tongwen</keyname><forenames>Huang</forenames></author><author><keyname>Huiping</keyname><forenames>Xue</forenames></author></authors><title>A Distributional Representation Model For Collaborative Filtering</title><categories>cs.IR cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a very concise deep learning approach for
collaborative filtering that jointly models distributional representation for
users and items. The proposed framework obtains better performance when
compared against current state-of-art algorithms and that made the
distributional representation model a promising direction for further research
in the collaborative filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04165</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04165</id><created>2015-02-13</created><updated>2015-05-05</updated><authors><author><keyname>Zhang</keyname><forenames>Haijun</forenames></author><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author></authors><title>Coexistence of Wi-Fi and Heterogeneous Small Cell Networks Sharing
  Unlicensed Spectrum</title><categories>cs.IT math.IT</categories><comments>Haijun Zhang; Xiaoli Chu; Weisi Guo; Siyi Wang, &quot;Coexistence of Wi-Fi
  and heterogeneous small cell networks sharing unlicensed spectrum,&quot; IEEE
  Communications Magazine, vol.53, no.3, pp.158,164, March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As two major players in terrestrial wireless communications, Wi-Fi systems
and cellular networks have different origins and have largely evolved
separately. Motivated by the exponentially increasing wireless data demand,
cellular networks are evolving towards a heterogeneous and small cell network
architecture, wherein small cells are expected to provide very high capacity.
However, due to the limited licensed spectrum for cellular networks, any effort
to achieve capacity growth through network densification will face the
challenge of severe inter-cell interference. In view of this, recent
standardization developments have started to consider the opportunities for
cellular networks to use the unlicensed spectrum bands, including the 2.4 GHz
and 5 GHz bands that are currently used by Wi-Fi, Zigbee and some other
communication systems. In this article, we look into the coexistence of Wi-Fi
and 4G cellular networks sharing the unlicensed spectrum. We introduce a
network architecture where small cells use the same unlicensed spectrum that
Wi-Fi systems operate in without affecting the performance of Wi-Fi systems. We
present an almost blank subframe (ABS) scheme without priority to mitigate the
co-channel interference from small cells to Wi-Fi systems, and propose an
interference avoidance scheme based on small cells estimating the density of
nearby Wi-Fi access points to facilitate their coexistence while sharing the
same unlicensed spectrum. Simulation results show that the proposed network
architecture and interference avoidance schemes can significantly increase the
capacity of 4G heterogeneous cellular networks while maintaining the service
quality of Wi-Fi systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04168</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04168</id><created>2015-02-14</created><updated>2015-09-10</updated><authors><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author></authors><title>Nonparametric regression using needlet kernels for spherical data</title><categories>cs.LG stat.ML</categories><comments>21 pages</comments><msc-class>68T05, 62J02</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Needlets have been recognized as state-of-the-art tools to tackle spherical
data, due to their excellent localization properties in both spacial and
frequency domains.
  This paper considers developing kernel methods associated with the needlet
kernel for nonparametric regression problems whose predictor variables are
defined on a sphere. Due to the localization property in the frequency domain,
we prove that the regularization parameter of the kernel ridge regression
associated with the needlet kernel can decrease arbitrarily fast. A natural
consequence is that the regularization term for the kernel ridge regression is
not necessary in the sense of rate optimality. Based on the excellent
localization property in the spacial domain further, we also prove that all the
$l^{q}$ $(01\leq q &lt; \infty)$ kernel regularization estimates associated with
the needlet kernel, including the kernel lasso estimate and the kernel bridge
estimate, possess almost the same generalization capability for a large range
of regularization parameters in the sense of rate optimality.
  This finding tentatively reveals that, if the needlet kernel is utilized,
then the choice of $q$ might not have a strong impact in terms of the
generalization capability in some modeling contexts. From this perspective, $q$
can be arbitrarily specified, or specified merely by other no generalization
criteria like smoothness, computational complexity, sparsity, etc..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04169</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04169</id><created>2015-02-14</created><updated>2016-02-28</updated><authors><author><keyname>Sharma</keyname><forenames>Abhay</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author></authors><title>Computationally Tractable Algorithms for Finding a Subset of
  Non-defective Items from a Large Population</title><categories>cs.IT math.IT</categories><comments>In this revision: Unified some proofs and reorganized the paper,
  corrected a small mistake in one of the proofs, added more references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical non-adaptive group testing setup, pools of items are tested
together, and the main goal of a recovery algorithm is to identify the
&quot;complete defective set&quot; given the outcomes of different group tests. In
contrast, the main goal of a &quot;non-defective subset recovery&quot; algorithm is to
identify a &quot;subset&quot; of non-defective items given the test outcomes. In this
paper, we present a suite of computationally efficient and analytically
tractable non-defective subset recovery algorithms. By analyzing the
probability of error of the algorithms, we obtain bounds on the number of tests
required for non-defective subset recovery with arbitrarily small probability
of error. Our analysis accounts for the impact of both the additive noise
(false positives) and dilution noise (false negatives). By comparing with the
information theoretic lower bounds, we show that the upper bounds on the number
of tests are order-wise tight up to a $\log^2K$ factor, where $K$ is the number
of defective items. We also provide simulation results that compare the
relative performance of the different algorithms and provide further insights
into their practical utility. The proposed algorithms significantly outperform
the straightforward approaches of testing items one-by-one, and of first
identifying the defective set and then choosing the non-defective items from
the complement set, in terms of the number of measurements required to ensure a
given success rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04170</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04170</id><created>2015-02-14</created><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author></authors><title>Human Factors in Agile Software Development</title><categories>cs.SE</categories><comments>Book Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through our four years experiments on students' Scrum based agile software
development (ASD) process, we have gained deep understanding into the human
factors of agile methodology. We designed an agile project management tool -
the HASE collaboration development platform to support more than 400 students
self-organized into 80 teams to practice ASD. In this thesis, Based on our
experiments, simulations and analysis, we contributed a series of solutions and
insights in this researches, including 1) a Goal Net based method to enhance
goal and requirement management for ASD process, 2) a novel Simple Multi-Agent
Real-Time (SMART) approach to enhance intelligent task allocation for ASD
process, 3) a Fuzzy Cognitive Maps (FCMs) based method to enhance emotion and
morale management for ASD process, 4) the first large scale in-depth empirical
insights on human factors in ASD process which have not yet been well studied
by existing research, and 5) the first to identify ASD process as a
human-computation system that exploit human efforts to perform tasks that
computers are not good at solving. On the other hand, computers can assist
human decision making in the ASD process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04174</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04174</id><created>2015-02-14</created><authors><author><keyname>Ma</keyname><forenames>Xuezhe</forenames></author><author><keyname>Zhao</keyname><forenames>Hai</forenames></author></authors><title>Probabilistic Models for High-Order Projective Dependency Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents generalized probabilistic models for high-order
projective dependency parsing and an algorithmic framework for learning these
statistical models involving dependency trees. Partition functions and
marginals for high-order dependency trees can be computed efficiently, by
adapting our algorithms which extend the inside-outside algorithm to
higher-order cases. To show the effectiveness of our algorithms, we perform
experiments on three languages---English, Chinese and Czech, using maximum
conditional likelihood estimation for model training and L-BFGS for parameter
estimation. Our methods achieve competitive performance for English, and
outperform all previously reported dependency parsers for Chinese and Czech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04184</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04184</id><created>2015-02-14</created><updated>2015-03-08</updated><authors><author><keyname>Yuan</keyname><forenames>Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Qian-Ming</forenames></author><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Zhang</keyname><forenames>Linyan</forenames></author><author><keyname>Wan</keyname><forenames>Xue-Song</forenames></author><author><keyname>Yu</keyname><forenames>Xiao-Jun</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Promotion and resignation in employee networks</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 3 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprises have put more and more emphasis on data analysis so as to obtain
effective management advices. Managers and researchers are trying to dig out
the major factors that lead to employees' promotion and resignation. Most
previous analyses were based on questionnaire survey, which usually consists of
a small fraction of samples and contains biases caused by psychological
defense. In this paper, we successfully collect a data set consisting of all
the employees' work-related interactions (action network, AN for short) and
online social connections (social network, SN for short) of a company, which
inspires us to reveal the correlations between structural features and
employees' career development, namely promotion and resignation. Through
statistical analysis and prediction, we show that the structural features of
both AN and SN are correlated and predictive to employees' promotion and
resignation, and the AN has higher correlation and predictability. More
specifically, the in-degree in AN is the most relevant indicator for promotion;
while the k-shell index in AN and in-degree in SN are both very predictive to
resignation. Our results provide a novel and actionable understanding of
enterprise management and suggest that to enhance the interplays among
employees, no matter work-related or social interplays, can largely improve the
loyalty of employees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04186</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04186</id><created>2015-02-14</created><authors><author><keyname>Cho</keyname><forenames>Byungjin</forenames></author><author><keyname>Koufos</keyname><forenames>Konstantinos</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Li</keyname><forenames>Zexian</forenames></author><author><keyname>Uusitalo</keyname><forenames>Mikko A.</forenames></author></authors><title>Spectrum Allocation for Multi-Operator Device-to-Device Communication</title><categories>cs.NI cs.GT</categories><comments>To appear in IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to harvest the business potential of device-to-device (D2D)
communication, direct communication between devices subscribed to different
mobile operators should be supported. This would also support meeting
requirements resulting from D2D relevant scenarios, like vehicle-to-vehicle
communication. In this paper, we propose to allocate the multi-operator D2D
communication over dedicated cellular spectral resources contributed from both
operators. Ideally, the operators should negotiate about the amount of spectrum
to contribute, without revealing proprietary information to each other and/or
to other parties. One possible way to do that is to use the sequence of
operators' best responses, i.e., the operators make offers about the amount of
spectrum to contribute using a sequential updating procedure until reaching
consensus. Besides spectrum allocation, we need a mode selection scheme for the
multi-operator D2D users. We use a stochastic geometry framework to capture the
impact of mode selection on the distribution of D2D users and assess the
performance of the best response iteration algorithm. With the performance
metrics considered in the paper, we show that the best response iteration has a
unique Nash equilibrium that can be reached from any initial strategy. In
general, asymmetric operators would contribute unequal amounts of spectrum for
multi-operator D2D communication. Provided that the multi-operator D2D density
is not negligible, we show that both operators may experience significant
performance gains as compared to the scheme without spectrum sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04187</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04187</id><created>2015-02-14</created><updated>2015-07-08</updated><authors><author><keyname>Keshmiri</keyname><forenames>Soheil</forenames></author><author><keyname>Zheng</keyname><forenames>Xin</forenames></author><author><keyname>Chew</keyname><forenames>Chee Meng</forenames></author><author><keyname>Pang</keyname><forenames>Chee Khiang</forenames></author></authors><title>Application of Deep Neural Network in Estimation of the Weld Bead
  Parameters</title><categories>cs.LG</categories><comments>Disapproval of funding organization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deep learning approach to estimation of the bead parameters in
welding tasks. Our model is based on a four-hidden-layer neural network
architecture. More specifically, the first three hidden layers of this
architecture utilize Sigmoid function to produce their respective intermediate
outputs. On the other hand, the last hidden layer uses a linear transformation
to generate the final output of this architecture. This transforms our deep
network architecture from a classifier to a non-linear regression model. We
compare the performance of our deep network with a selected number of results
in the literature to show a considerable improvement in reducing the errors in
estimation of these values. Furthermore, we show its scalability on estimating
the weld bead parameters with same level of accuracy on combination of datasets
that pertain to different welding techniques. This is a nontrivial result that
is counter-intuitive to the general belief in this field of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04189</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04189</id><created>2015-02-14</created><authors><author><keyname>Chiani</keyname><forenames>Marco</forenames></author></authors><title>On the probability that all eigenvalues of Gaussian and Wishart random
  matrices lie within an interval</title><categories>math.ST cs.IT math-ph math.IT math.MP stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the probability $\psi(a,b)=\Pr(a\leq \lambda_{\min}({\bf M}),
\lambda_{\max}({\bf M})\leq b)$ that all eigenvalues of a random matrix $\bf M$
lie within an arbitrary interval $[a,b]$, when $\bf M$ is a real or complex
finite dimensional Wishart, double Wishart, or Gaussian symmetric/hermitian
matrix. We give efficient recursive formulas allowing, for instance, the exact
evaluation of $\psi(a,b)$ for Wishart matrices with number of variates $500$
and degrees of freedom $1000$. We also prove that the probability that all
eigenvalues are within the limiting spectral support (given by the
Marchenko-Pastur or the semicircle laws) tends for large dimensions to the
universal values $0.6921$ and $0.9397$ for the real and complex cases,
respectively. Applications include improved bounds for the probability that a
Gaussian measurement matrix has a given restricted isometry constant in
compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04190</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04190</id><created>2015-02-14</created><updated>2015-07-08</updated><authors><author><keyname>Keshmiri</keyname><forenames>Soheil</forenames></author><author><keyname>Ahmed</keyname><forenames>Syeda Mariam</forenames></author><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Chew</keyname><forenames>Chee Meng</forenames></author><author><keyname>Pang</keyname><forenames>Chee Khiang</forenames></author></authors><title>An Adaptive Sampling Approach to 3D Reconstruction of Weld Joint</title><categories>cs.RO</categories><comments>Disapproval of funding organization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an adaptive sampling approach to 3D reconstruction of the welding
joint using the point cloud that is generated by a laser sensor. We start with
a randomized strategy to approximate the surface of the volume of interest
through selection of a number of pivotal candidates. Furthermore, we introduce
three proposal distributions over the neighborhood of each of these pivots to
adaptively sample from their neighbors to refine the original randomized
approximation to incrementally reconstruct this welding space. We prevent our
algorithm from being trapped in a neighborhood via permanently labeling the
visited samples. In addition, we accumulate the accepted candidates along with
their selected neighbors in a queue structure to allow every selected sample to
contribute to the evolution of the reconstructed welding space as the algorithm
progresses. We analyze the performance of our adaptive sampling algorithm in
contrast to the random sampling, with and without replacement, to show a
significant improvement in total number of samples that are drawn to identify
the region of interest, thereby expanding upon neighboring samples to extract
the entire region in a fewer iterations and a shorter computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04204</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04204</id><created>2015-02-14</created><updated>2015-02-19</updated><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Gray-Level Image Transitions Driven by Tsallis Entropic Index</title><categories>cs.CV</categories><comments>Tsallis Entropy, Image Processing, Image Segmentation, Image
  Thresholding, Texture Transitions, Medical Image Processing, Typos emended</comments><journal-ref>International Journal of Sciences 4(2), 16-25, 2015</journal-ref><doi>10.18483/ijSci.621</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum entropy principle is largely used in thresholding and
segmentation of images. Among the several formulations of this principle, the
most effectively applied is that based on Tsallis non-extensive entropy. Here,
we discuss the role of its entropic index in determining the thresholds. When
this index is spanning the interval (0,1), for some images, the values of
thresholds can have large leaps. In this manner, we observe abrupt transitions
in the appearance of corresponding bi-level or multi-level images. These
gray-level image transitions are analogous to order or texture transitions
observed in physical systems, transitions which are driven by the temperature
or by other physical quantities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04205</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04205</id><created>2015-02-14</created><authors><author><keyname>Cheng</keyname><forenames>Yi</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Leader-follower Tracking Control with Guaranteed Consensus Performance
  for Interconnected Systems with Linear Dynamic Uncertain Coupling</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the leader-follower tracking control problem for linear
interconnected systems with undirected topology and linear dynamic coupling.
Interactions between the systems are treated as linear dynamic uncertainty and
are described in terms of integral quadratic constraints (IQCs). A
consensus-type tracking control protocol is proposed for each system based on
its state relative its neighbors. In addition a selected set of subsystems uses
for control their relative states with respect to the leader. Two methods are
proposed for the design of this control protocol. One method uses a coordinate
transformation to recast the protocol design problem as a decentralized robust
control problem for an auxiliary interconnected large scale system. Another
method is direct, it does not employ coordinate transformation; it also allows
for more general linear uncertain interactions. Using these methods, sufficient
conditions are obtained which guarantee that the system tracks the leader.
These conditions guarantee a suboptimal bound on the system consensus and
tracking performance. The proposed methods are compared using a simulation
example, and their effectiveness is discussed. Also, algorithms are proposed
for computing suboptimal controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04210</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04210</id><created>2015-02-14</created><authors><author><keyname>Hernandez</keyname><forenames>Bryan</forenames></author><author><keyname>Sison</keyname><forenames>Virgilio</forenames></author></authors><title>Grassmannian Codes as Lifts of Matrix Codes Derived as Images of Linear
  Block Codes over Finite Fields</title><categories>cs.IT math.IT</categories><msc-class>94B05, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be a prime such that $p \equiv 2$ or $3$ mod $5$. Linear block codes
over the non-commutative matrix ring of $2 \times 2$ matrices over the prime
field $GF(p)$ endowed with the Bachoc weight are derived as isometric images of
linear block codes over the Galois field $GF(p^2)$ endowed with the Hamming
metric. When seen as rank metric codes, this family of matrix codes satisfies
the Singleton bound and thus are maximum rank distance codes, which are then
lifted to form a special class of subspace codes, the Grassmannian codes, that
meet the anticode bound. These so-called anticode-optimal Grassmannian codes
are associated in some way with complete graphs. New examples of these maximum
rank distance codes and anticode-optimal Grassmannian codes are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04220</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04220</id><created>2015-02-14</created><authors><author><keyname>Lu</keyname><forenames>Can</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author><author><keyname>Li</keyname><forenames>Rong-Hua</forenames></author><author><keyname>Wei</keyname><forenames>Hao</forenames></author></authors><title>Exploring Hierarchies in Online Social Networks</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social hierarchy (i.e., pyramid structure of societies) is a fundamental
concept in sociology and social network analysis. The importance of social
hierarchy in a social network is that the topological structure of the social
hierarchy is essential in both shaping the nature of social interactions
between individuals and unfolding the structure of the social networks. The
social hierarchy found in a social network can be utilized to improve the
accuracy of link prediction, provide better query results, rank web pages, and
study information flow and spread in complex networks. In this paper, we model
a social network as a directed graph G, and consider the social hierarchy as
DAG (directed acyclic graph) of G, denoted as GD. By DAG, all the vertices in G
can be partitioned into different levels, the vertices at the same level
represent a disjoint group in the social hierarchy, and all the edges in DAG
follow one direction. The main issue we study in this paper is how to find DAG
GD in G. The approach we take is to find GD by removing all possible cycles
from G such that G = U(G) + GD where U(G) is a maximum Eulerian subgraph which
contains all possible cycles. We give the reasons for doing so, investigate the
properties of GD found, and discuss the applications. In addition, we develop a
novel two-phase algorithm, called Greedy-&amp;-Refine, which greedily computes an
Eulerian subgraph and then refines this greedy solution to find the maximum
Eulerian subgraph. We give a bound between the greedy solution and the optimal.
The quality of our greedy approach is high. We conduct comprehensive
experimental studies over 14 real-world datasets. The results show that our
algorithms are at least two orders of magnitude faster than the baseline
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04221</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04221</id><created>2015-02-14</created><authors><author><keyname>Madanayake</keyname><forenames>A.</forenames></author><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Onen</keyname><forenames>D.</forenames></author><author><keyname>Dimitrov</keyname><forenames>V. S.</forenames></author><author><keyname>Rajapaksha</keyname><forenames>N. T.</forenames></author><author><keyname>Bruton</keyname><forenames>L. T.</forenames></author><author><keyname>Edirisuriya</keyname><forenames>A.</forenames></author></authors><title>A Row-parallel 8$\times$8 2-D DCT Architecture Using Algebraic Integer
  Based Exact Computation</title><categories>cs.AR cs.DM math.NT stat.CO stat.ME</categories><comments>28 pages, 9 figures, 7 tables, corrected typos</comments><journal-ref>IEEE Transactions on Circuits and Systems for Video Technology,
  vol. 22, no. 6, pp. 915--929, 2012</journal-ref><doi>10.1109/TCSVT.2011.2181232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algebraic integer (AI) based time-multiplexed row-parallel architecture
and two final-reconstruction step (FRS) algorithms are proposed for the
implementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The
architecture directly realizes an error-free 2-D DCT without using FRSs between
row-column transforms, leading to an 8$\times$8 2-D DCT which is entirely free
of quantization errors in AI basis. As a result, the user-selectable accuracy
for each of the coefficients in the FRS facilitates each of the 64 coefficients
to have its precision set independently of others, avoiding the leakage of
quantization noise between channels as is the case for published DCT designs.
The proposed FRS uses two approaches based on (i) optimized Dempster-Macleod
multipliers and (ii) expansion factor scaling. This architecture enables
low-noise high-dynamic range applications in digital video processing that
requires full control of the finite-precision computation of the 2-D DCT. The
proposed architectures and FRS techniques are experimentally verified and
validated using hardware implementations that are physically realized and
verified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using
the two proposed FRS schemes, have been designed, simulated, physically
implemented and measured. The maximum clock rate and block-rate achieved among
8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a
pixel rate of 8$\times$307.787$\approx$2.462 GHz if eventually embedded in a
real-time video-processing system. The equivalent frame rate is about 1187.35
Hz for the image size of 1920$\times$1080. All implementations are functional
on a Xilinx Virtex-6 XC6VLX240T FPGA device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04226</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04226</id><created>2015-02-14</created><updated>2015-03-07</updated><authors><author><keyname>Khadiev</keyname><forenames>Kamil</forenames></author></authors><title>Width Hierarchy for k-OBDD of Small Width</title><categories>cs.CC</categories><comments>8 pages</comments><journal-ref>Lobachevskii Journal of Mathematics, V. 36, I. 2, pp 178-183 ,
  2015</journal-ref><doi>10.1134/S1995080215020092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper was explored well known model k-OBDD. There are proven width
based hierarchy of classes of boolean functions which computed by k-OBDD. The
proof of hierarchy is based on sufficient condition of Boolean function's non
representation as k-OBDD and complexity properties of Boolean function SAF.
This function is modification of known Pointer Jumping (PJ) and Indirect
Storage Access (ISA) functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04232</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04232</id><created>2015-02-14</created><authors><author><keyname>Zou</keyname><forenames>Changqing</forenames></author><author><keyname>Huang</keyname><forenames>Zhe</forenames></author><author><keyname>Lau</keyname><forenames>Rynson W. H.</forenames></author><author><keyname>Liu</keyname><forenames>Jianzhuang</forenames></author><author><keyname>Fu</keyname><forenames>Hongbo</forenames></author></authors><title>Sketch-based Shape Retrieval using Pyramid-of-Parts</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a multi-scale approach to sketch-based shape retrieval. It is
based on a novel multi-scale shape descriptor called Pyramidof- Parts, which
encodes the features and spatial relationship of the semantic parts of query
sketches. The same descriptor can also be used to represent 2D projected views
of 3D shapes, allowing effective matching of query sketches with 3D shapes
across multiple scales. Experimental results show that the proposed method
outperforms the state-of-the-art method, whether the sketch segmentation
information is obtained manually or automatically by considering each stroke as
a semantic part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04236</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04236</id><created>2015-02-14</created><updated>2015-11-23</updated><authors><author><keyname>Liu</keyname><forenames>Xuan</forenames></author><author><keyname>Li</keyname><forenames>Zhiyi</forenames></author><author><keyname>Li</keyname><forenames>Zuyi</forenames></author></authors><title>Impacts of Bad Data on the PMU based Line Outage Detection</title><categories>cs.CR cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power systems become more prone to cyber-attacks due to the high integration
of information technologies. In this paper, we demonstrate that the outages of
some lines can be masked by injecting false data into a set of measurements.
The success of the topology attack can be guaranteed by making that: 1)the
injected false data obeys KCL and KVL to avoid being detected by the bad data
detection program in the state estimation; 2)the residual is increased such
that the line outage cannot be detected by PMU data. A quadratic programming
problem is set up to determine the optimal attack vector that can maximize the
residual of the outaged line. The IEEE 39-bus system is used to demonstrate the
masking scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04240</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04240</id><created>2015-02-14</created><updated>2015-06-15</updated><authors><author><keyname>Furma&#x144;czyk</keyname><forenames>H.</forenames></author><author><keyname>Kubale</keyname><forenames>M.</forenames></author></authors><title>Scheduling of unit-length jobs with cubic incompatibility graphs on
  three uniform machines</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we consider the problem of scheduling $n$ identical jobs on 3
uniform machines with speeds $s_1, s_2,$ and $s_3$ to minimize the schedule
length. We assume that jobs are subjected to some kind of mutual exclusion
constraints, modeled by a cubic incompatibility graph. We show that if the
graph is 2-chromatic then the problem can be solved in $O(n^2)$ time. If the
graph is 3-chromatic, the problem becomes NP-hard even if $s_1&gt;s_2=s_3$.
However, in this case there exists a $4/3$-approximation algorithm running in
$O(n^3)$ time. Moreover, this algorithm solves the problem almost surely to
optimality if $3s_1/4 \leq s_2 = s_3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04244</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04244</id><created>2015-02-14</created><authors><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author><author><keyname>Li</keyname><forenames>Nian</forenames></author></authors><title>Optimal cyclic codes with generalized Niho type zeroes and the weight
  distribution</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the works \cite{gegeng2,XLZD} further in two
directions and compute the weight distribution of these cyclic codes under more
relaxed conditions. It is interesting to note that many cyclic codes in the
family are optimal and have only a few non-zero weights. Besides using similar
ideas from \cite{gegeng2,XLZD}, we carry out some subtle manipulation of
certain exponential sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04245</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04245</id><created>2015-02-14</created><authors><author><keyname>Williams</keyname><forenames>Joseph Jay</forenames></author><author><keyname>Ostrow</keyname><forenames>Korinn</forenames></author><author><keyname>Xiong</keyname><forenames>Xiaolu</forenames></author><author><keyname>Glassman</keyname><forenames>Elena</forenames></author><author><keyname>Kim</keyname><forenames>Juho</forenames></author><author><keyname>Maldonado</keyname><forenames>Samuel G.</forenames></author><author><keyname>Li</keyname><forenames>Na</forenames></author><author><keyname>Reich</keyname><forenames>Justin</forenames></author><author><keyname>Hefferman</keyname><forenames>Neil</forenames></author></authors><title>Using and Designing Platforms for In Vivo Education Experiments</title><categories>cs.HC cs.CY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to typical laboratory experiments, the everyday use of online
educational resources by large populations and the prevalence of software
infrastructure for A/B testing leads us to consider how platforms can embed in
vivo experiments that do not merely support research, but ensure practical
improvements to their educational components. Examples are presented of
randomized experimental comparisons conducted by subsets of the authors in
three widely used online educational platforms Khan Academy, edX, and
ASSISTments. We suggest design principles for platform technology to support
randomized experiments that lead to practical improvements enabling Iterative
Improvement and Collaborative Work and explain the benefit of their
implementation by WPI co-authors in the ASSISTments platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04246</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04246</id><created>2015-02-14</created><updated>2015-08-18</updated><authors><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Soloveichik</keyname><forenames>David</forenames></author></authors><title>Stable Leader Election in Population Protocols Requires Linear Time</title><categories>cs.DC cs.CC q-bio.MN</categories><comments>added several examples and intuition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A population protocol *stably elects a leader* if, for all $n$, starting from
an initial configuration with $n$ agents each in an identical state, with
probability 1 it reaches a configuration $\mathbf{y}$ that is correct (exactly
one agent is in a special leader state $\ell$) and stable (every configuration
reachable from $\mathbf{y}$ also has a single agent in state $\ell$). We show
that any population protocol that stably elects a leader requires $\Omega(n)$
expected &quot;parallel time&quot; --- $\Omega(n^2)$ expected total pairwise interactions
--- to reach such a stable configuration. Our result also informs the
understanding of the time complexity of chemical self-organization by showing
an essential difficulty in generating exact quantities of molecular species
quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04247</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04247</id><created>2015-02-14</created><authors><author><keyname>Williams</keyname><forenames>Joseph Jay</forenames></author><author><keyname>Kim</keyname><forenames>Juho</forenames></author><author><keyname>Keegan</keyname><forenames>Brian C.</forenames></author></authors><title>Supporting Instructors in Collaborating with Researchers using MOOClets</title><categories>cs.CY cs.HC</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most education and workplace learning takes place in classroom contexts far
removed from laboratories or field sites with special arrangements for
scientific research. But digital online resources provide a novel opportunity
for large scale efforts to bridge the real world and laboratory settings which
support data collection and randomized A/B experiments comparing different
versions of content or interactions [2]. However, there are substantial
technological and practical barriers in aligning instructors and researchers to
use learning technologies like blended lessons/exercises &amp; MOOCs as both a
service for students and a realistic context to conduct research. This paper
explains how the concept of a MOOClet can facilitate research-practitioner
collaborations. MOOClets [3] are defined as modular components of a digital
resource that can be implemented in technology to: (1) allow modification to
create multiple versions, (2) allow experimental comparison and personalization
of different versions, (3) reliably specify what data are collected. We suggest
a framework in which instructors specify what kinds of changes to lessons,
exercises, and emails they would be willing to adopt, and what data they will
collect and make available. Researchers can then: (1) specify or design
experiments that compare the effects of different versions on quantifiable
outcomes. (2) Explore algorithms for maximizing particular outcomes by choosing
alternative versions of a MOOClet based on the input variables available. We
present a prototype survey tool for instructors intended to facilitate
practitioner researcher matches and successful collaborations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04248</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04248</id><created>2015-02-14</created><authors><author><keyname>Anis</keyname><forenames>Aamir</forenames></author><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Asymptotic Justification of Bandlimited Interpolation of Graph signals
  for Semi-Supervised Learning</title><categories>cs.LG cs.IT math.IT</categories><comments>To appear in ICASSP 2015, 5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based methods play an important role in unsupervised and
semi-supervised learning tasks by taking into account the underlying geometry
of the data set. In this paper, we consider a statistical setting for
semi-supervised learning and provide a formal justification of the recently
introduced framework of bandlimited interpolation of graph signals. Our
analysis leads to the interpretation that, given enough labeled data, this
method is very closely related to a constrained low density separation problem
as the number of data points tends to infinity. We demonstrate the practical
utility of our results through simple experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04250</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04250</id><created>2015-02-14</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Hyperlinks embedded in Twitter as a proxy for total external inlinks to
  international university websites</title><categories>cs.DL</categories><comments>38 pages, 5 figures and 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article analyzes Twitter as a potential alternative source of external
links for use in webometric analysis because of its capacity to embed
hyperlinks in different tweets. Given the limitations on searching Twitter's
public API, we decided to use the Topsy search engine as a source for compiling
tweets. To this end, we took a global sample of 200 universities and compiled
all the tweets with hyperlinks to any of these institutions. Further link data
was obtained from alternative sources (MajesticSEO and OpenSiteExplorer) in
order to compare the results. Thereafter, various statistical tests were
performed to determine the correlation between the indicators and the ability
to predict external links from the collected tweets. The results indicate a
high volume of tweets, although they are skewed by the presence and performance
of specific universities and countries. The data provided by Topsy correlated
significantly with all link indicators, particularly with OpenSiteExplorer
(r=0.769). Finally, prediction models do not provide optimum results because of
high error rates, which fall slightly in nonlinear models applied to specific
environments. We conclude that the use of Twitter (via Topsy) as a source of
hyperlinks to universities produces promising results due to its high
correlation with link indicators, though limited by policies and culture
regarding use and presence in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04252</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04252</id><created>2015-02-14</created><authors><author><keyname>Shewaye</keyname><forenames>Tizita Nesibu</forenames></author></authors><title>Cardiac MR Image Segmentation Techniques: an overview</title><categories>cs.CV</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadly speaking, the objective in cardiac image segmentation is to delineate
the outer and inner walls of the heart to segment out either the entire or
parts of the organ boundaries. This paper will focus on MR images as they are
the most widely used in cardiac segmentation -- as a result of the accurate
morphological information and better soft tissue contrast they provide. This
cardiac segmentation information is very useful as it eases physical
measurements that provides useful metrics for cardiac diagnosis such as
infracted volumes, ventricular volumes, ejection fraction, myocardial mass,
cardiac movement, and the like. But, this task is difficult due to the
intensity and texture similarities amongst the different cardiac and background
structures on top of some noisy artifacts present in MR images. Thus far,
various researchers have proposed different techniques to solve some of the
pressing issues. This seminar paper presents an overview of representative
medical image segmentation techniques. The paper also highlights preferred
approaches for segmentation of the four cardiac chambers: the left ventricle
(LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on short
axis image planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04254</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04254</id><created>2015-02-14</created><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Esnaola</keyname><forenames>Inaki</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Sparse Attack Construction and State Estimation in the Smart Grid:
  Centralized and Distributed Models</title><categories>cs.IT cs.SY math.IT</categories><comments>11 pages, 5 figures</comments><journal-ref>IEEE JSAC, vol. 31, no. 7, pp. 1306-1318, Jul. 2013</journal-ref><doi>10.1109/JSAC.2013.130713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New methods that exploit sparse structures arising in smart grid networks are
proposed for the state estimation problem when data injection attacks are
present. First, construction strategies for unobservable sparse data injection
attacks on power grids are proposed for an attacker with access to all network
information and nodes. Specifically, novel formulations for the optimization
problem that provide a flexible design of the trade-off between performance and
false alarm are proposed. In addition, the centralized case is extended to a
distributed framework for both the estimation and attack problems. Different
distributed scenarios are proposed depending on assumptions that lead to the
spreading of the resources, network nodes and players. Consequently, for each
of the presented frameworks a corresponding optimization problem is introduced
jointly with an algorithm to solve it. The validity of the presented procedures
in real settings is studied through extensive simulations in the IEEE test
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04262</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04262</id><created>2015-02-14</created><authors><author><keyname>Izquierdo</keyname><forenames>Eduardo J.</forenames></author><author><keyname>Williams</keyname><forenames>Paul L.</forenames></author><author><keyname>Beer</keyname><forenames>Randall D.</forenames></author></authors><title>Information flow through a model of the C. elegans klinotaxis circuit</title><categories>q-bio.NC cs.IT math.IT</categories><journal-ref>PLoS ONE 10(10): e0140397. (2015)</journal-ref><doi>10.1371/journal.pone.0140397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how information about external stimuli is transformed into
behavior is one of the central goals of neuroscience. Here we characterize the
information flow through a complete sensorimotor circuit: from stimulus, to
sensory neurons, to interneurons, to motor neurons, to muscles, to motion.
Specifically, we apply a recently developed framework for quantifying
information flow to a previously published ensemble of models of salt
klinotaxis in the nematode worm C. elegans. The models are grounded in the
neuroanatomy and currently known neurophysiology of the worm. The unknown model
parameters were optimized to reproduce the worm's behavior. Information flow
analysis reveals several key principles underlying how the models operate: (1)
Interneuron class AIY is responsible for integrating information about positive
and negative changes in concentration, and exhibits a strong left/right
information asymmetry. (2) Gap junctions play a crucial role in the transfer of
information responsible for the information symmetry observed in interneuron
class AIZ. (3) Neck motor neuron class SMB implements an information gating
mechanism that underlies the circuit's state-dependent response. (4) The neck
carries non-uniform distribution about changes in concentration. Thus, not all
directions of movement are equally informative. Each of these findings
corresponds to an experimental prediction that could be tested in the worm to
greatly refine our understanding of the neural circuit underlying klinotaxis.
Information flow analysis also allows us to explore how information flow
relates to underlying electrophysiology. Despite large variations in the neural
parameters of individual circuits, the overall information flow architecture
circuit is remarkably consistent across the ensemble, suggesting that
information flow analysis captures general principles of operation for the
klinotaxis circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04264</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04264</id><created>2015-02-14</created><authors><author><keyname>Fagnani</keyname><forenames>Fabio</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author></authors><title>The robustness of democratic consensus</title><categories>cs.SY cs.MA math.OC math.PR</categories><comments>13 pages, 2 figs</comments><journal-ref>The robustness of democratic consensus, Automatica, Volume 52,
  February 2015, Pages 232-241</journal-ref><doi>10.1016/j.automatica.2014.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In linear models of consensus dynamics, the state of the various agents
converges to a value which is a convex combination of the agents' initial
states. We call it democratic if in the large scale limit (number of agents
going to infinity) the vector of convex weights converges to 0 uniformly.
  Democracy is a relevant property which naturally shows up when we deal with
opinion dynamic models and cooperative algorithms such as consensus over a
network: it says that each agent's measure/opinion is going to play a
negligeable role in the asymptotic behavior of the global system. It can be
seen as a relaxation of average consensus, where all agents have exactly the
same weight in the final value, which becomes negligible for a large number of
agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04265</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04265</id><created>2015-02-14</created><updated>2015-05-28</updated><authors><author><keyname>Kappmeier</keyname><forenames>Jan-Philipp W.</forenames></author><author><keyname>Schmidt</keyname><forenames>Daniel R.</forenames></author><author><keyname>Schmidt</keyname><forenames>Melanie</forenames></author></authors><title>Solving $k$-means on High-dimensional Big Data</title><categories>cs.DS</categories><comments>23 pages, 9 figures, published at the 14th International Symposium on
  Experimental Algorithms - SEA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there have been major efforts to develop data stream
algorithms that process inputs in one pass over the data with little memory
requirement. For the $k$-means problem, this has led to the development of
several $(1+\varepsilon)$-approximations (under the assumption that $k$ is a
constant), but also to the design of algorithms that are extremely fast in
practice and compute solutions of high accuracy. However, when not only the
length of the stream is high but also the dimensionality of the input points,
then current methods reach their limits.
  We propose two algorithms, piecy and piecy-mr that are based on the recently
developed data stream algorithm BICO that can process high dimensional data in
one pass and output a solution of high quality. While piecy is suited for high
dimensional data with a medium number of points, piecy-mr is meant for high
dimensional data that comes in a very long stream. We provide an extensive
experimental study to evaluate piecy and piecy-mr that shows the strength of
the new algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04266</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04266</id><created>2015-02-14</created><authors><author><keyname>Abbaszadeh</keyname><forenames>Masoud</forenames></author><author><keyname>Solgi</keyname><forenames>Reza</forenames></author></authors><title>Constrained Nonlinear Model Predictive Control of an MMA Polymerization
  Process via Evolutionary Optimization</title><categories>cs.SY cs.AI math.OC</categories><comments>12 pages, 9 figures, 28 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a nonlinear model predictive controller is developed for a
batch polymerization process. The physical model of the process is
parameterized along a desired trajectory resulting in a trajectory linearized
piecewise model (a multiple linear model bank) and the parameters are
identified for an experimental polymerization reactor. Then, a multiple model
adaptive predictive controller is designed for thermal trajectory tracking of
the MMA polymerization. The input control signal to the process is constrained
by the maximum thermal power provided by the heaters. The constrained
optimization in the model predictive controller is solved via genetic
algorithms to minimize a DMC cost function in each sampling interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04268</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04268</id><created>2015-02-14</created><authors><author><keyname>Huypens</keyname><forenames>Valere</forenames></author></authors><title>Relative Squared Distances to a Conic Berserkless 8-Connected Midpoint
  Algorithm</title><categories>cs.GR</categories><comments>20 pages, 3 figures, International Journal of Computer Graphics &amp;
  Animation (IJCGA) Vol. 5, No. 1, January 2015</comments><msc-class>68U05, 65D18</msc-class><acm-class>I.3.3; I.3.5; I.3.7; B.7.1</acm-class><journal-ref>International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol. 5, No. 1, January 2015</journal-ref><doi>10.5121/ijcga.2015.5102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The midpoint method or technique is a measurement and as each measurement it
has a tolerance, but worst of all it can be invalid, called Out-of-Control or
OoC. The core of all midpoint methods is the accurate measurement of the
difference of the squared distances of two points to the polar of their
midpoint with respect to the conic. When this measurement is valid, it also
measures the difference of the squared distances of these points to the conic,
although it may be inaccurate, called Out-of-Accuracy or OoA. The primary
condition is the necessary and sufficient condition that a measurement is
valid. It is comletely new and it can be checked ultra fast and before the
actual measurement starts. Modeling an incremental algorithm, shows that the
curve must be subdivided into piecewise monotonic sections, the start point
must be optimal, and it explains that the 2D-incremental method can find,
locally, the global Least Square Distance. Locally means that there are at most
three candidate points for a given monotonic direction; therefore the
2D-midpoint method has, locally, at most three measurements. When all the
possible measurements are invalid, the midpoint method cannot be applied, and
in that case the ultra fast OoC-rule selects the candidate point. This
guarantees, for the first time, a 100% stable, ultra-fast, berserkless midpoint
algorithm, which can be easily transformed to hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04269</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04269</id><created>2015-02-14</created><updated>2016-01-26</updated><authors><author><keyname>Ustun</keyname><forenames>Berk</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Supersparse Linear Integer Models for Optimized Medical Scoring Systems</title><categories>stat.ML cs.DM cs.LG stat.AP stat.ME</categories><comments>This version reflects our findings on SLIM as of January 2016
  (arXiv:1306.5860 and arXiv:1405.4047 are out-of-date). The final published
  version of this articled is available at http://www.springerlink.com</comments><doi>10.1007/s10994-015-5528-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scoring systems are linear classification models that only require users to
add, subtract and multiply a few small numbers in order to make a prediction.
These models are in widespread use by the medical community, but are difficult
to learn from data because they need to be accurate and sparse, have coprime
integer coefficients, and satisfy multiple operational constraints. We present
a new method for creating data-driven scoring systems called a Supersparse
Linear Integer Model (SLIM). SLIM scoring systems are built by solving an
integer program that directly encodes measures of accuracy (the 0-1 loss) and
sparsity (the $\ell_0$-seminorm) while restricting coefficients to coprime
integers. SLIM can seamlessly incorporate a wide range of operational
constraints related to accuracy and sparsity, and can produce highly tailored
models without parameter tuning. We provide bounds on the testing and training
accuracy of SLIM scoring systems, and present a new data reduction technique
that can improve scalability by eliminating a portion of the training data
beforehand. Our paper includes results from a collaboration with the
Massachusetts General Hospital Sleep Laboratory, where SLIM was used to create
a highly tailored scoring system for sleep apnea screening
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04272</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04272</id><created>2015-02-14</created><authors><author><keyname>Mathew</keyname><forenames>Joshin John</forenames></author><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Spatial Stimuli Gradient Sketch Model</title><categories>cs.CV</categories><comments>accepted for publication in IEEE Signal Processing Letters, 2015</comments><journal-ref>Volume: 22 Issue: 9 On page(s): 1336-1339, 2015</journal-ref><doi>10.1109/LSP.2015.2404827</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The inability of automated edge detection methods inspired from primal sketch
models to accurately calculate object edges under the influence of pixel noise
is an open problem. Extending the principles of image perception i.e.
Weber-Fechner law, and Sheperd similarity law, we propose a new edge detection
method and formulation that use perceived brightness and neighbourhood
similarity calculations in the determination of robust object edges. The
robustness of the detected edges is benchmark against Sobel, SIS, Kirsch, and
Prewitt edge detection methods in an example face recognition problem showing
statistically significant improvement in recognition accuracy and pixel noise
tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04273</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04273</id><created>2015-02-14</created><authors><author><keyname>Kolte</keyname><forenames>Ritesh</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author><author><keyname>Permuter</keyname><forenames>Haim</forenames></author></authors><title>Multicoding Schemes for Interference Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The best known inner bound for the 2-user discrete memoryless interference
channel is the Han-Kobayashi rate region. The coding schemes that achieve this
region are based on rate-splitting and superposition coding. In this paper, we
develop a multicoding scheme to achieve the same rate region. A key advantage
of the multicoding nature of the proposed coding scheme is that it can be
naturally extended to more general settings, such as when encoders have state
information or can overhear each other. In particular, we extend our coding
scheme to characterize the capacity region of the state-dependent deterministic
Z-interference channel when noncausal state information is available at the
interfering transmitter. We specialize our results to the case of the linear
deterministic model with on/off interference which models a wireless system
where a cognitive transmitter is noncausally aware of the times it interferes
with a primary transmission. For this special case, we provide an explicit
expression for the capacity region and discuss some interesting properties of
the optimal strategy. We also extend our multicoding scheme to find the
capacity region of the deterministic Z-interference channel when the signal of
the interfering transmitter can be overheard at the other transmitter (a.k.a.
unidirectional partial cribbing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04275</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04275</id><created>2015-02-14</created><authors><author><keyname>Zhu</keyname><forenames>Yukun</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author></authors><title>segDeepM: Exploiting Segmentation and Context in Deep Neural Networks
  for Object Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approach that exploits object segmentation in
order to improve the accuracy of object detection. We frame the problem as
inference in a Markov Random Field, in which each detection hypothesis scores
object appearance as well as contextual information using Convolutional Neural
Networks, and allows the hypothesis to choose and score a segment out of a
large pool of accurate object segmentation proposals. This enables the detector
to incorporate additional evidence when it is available and thus results in
more accurate detections. Our experiments show an improvement of 4.1% in mAP
over the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current
state-of-the-art, demonstrating the power of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04280</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04280</id><created>2015-02-14</created><updated>2015-07-27</updated><authors><author><keyname>Chen</keyname><forenames>Yu-Fang</forenames></author><author><keyname>Hong</keyname><forenames>Chih-Duo</forenames></author><author><keyname>Wang</keyname><forenames>Bow-Yaw</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Counterexample-Guided Polynomial Loop Invariant Generation by Lagrange
  Interpolation</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply multivariate Lagrange interpolation to synthesize polynomial
quantitative loop invariants for probabilistic programs. We reduce the
computation of an quantitative loop invariant to solving constraints over
program variables and unknown coefficients. Lagrange interpolation allows us to
find constraints with less unknown coefficients. Counterexample-guided
refinement furthermore generates linear constraints that pinpoint the desired
quantitative invariants. We evaluate our technique by several case studies with
polynomial quantitative loop invariants in the experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04281</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04281</id><created>2015-02-14</created><authors><author><keyname>Mitliagkas</keyname><forenames>Ioannis</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author></authors><title>FrogWild! -- Fast PageRank Approximations on Graph Engines</title><categories>cs.DC cs.IT cs.SI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose FrogWild, a novel algorithm for fast approximation of high
PageRank vertices, geared towards reducing network costs of running traditional
PageRank algorithms. Our algorithm can be seen as a quantized version of power
iteration that performs multiple parallel random walks over a directed graph.
One important innovation is that we introduce a modification to the GraphLab
framework that only partially synchronizes mirror vertices. This partial
synchronization vastly reduces the network traffic generated by traditional
PageRank algorithms, thus greatly reducing the per-iteration cost of PageRank.
On the other hand, this partial synchronization also creates dependencies
between the random walks used to estimate PageRank. Our main theoretical
innovation is the analysis of the correlations introduced by this partial
synchronization process and a bound establishing that our approximation is
close to the true PageRank vector.
  We implement our algorithm in GraphLab and compare it against the default
PageRank implementation. We show that our algorithm is very fast, performing
each iteration in less than one second on the Twitter graph and can be up to 7x
faster compared to the standard GraphLab PageRank implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04297</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04297</id><created>2015-02-15</created><authors><author><keyname>Tomanek</keyname><forenames>Martin</forenames></author><author><keyname>Cermak</keyname><forenames>Radim</forenames></author><author><keyname>Smutny</keyname><forenames>Zdenek</forenames></author></authors><title>A Conceptual Framework for Web Development Projects Based on Project
  Management and Agile Development Principles</title><categories>cs.SE</categories><comments>Conference: 10th European Conference on Management Leadership and
  Governance (ECMLG 2014), At Zagreb</comments><doi>10.13140/2.1.1262.4165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Companies implement different frameworks and best practices with the
objective to improve the project management success rate and improve the
business adaptability to the changing business environment. Project management
framework (PRINCE2) and agile development framework (Scrum) proved in many
cases that they can meet these objectives. However, both frameworks are based
on different principles and the use of both frameworks together should be
carefully considered. A large amount of money and effort has been invested by
companies into establishing their project management environment and processes
that follow the classical phased approach where requirements are defined
upfront and fixed. But companies want to react more quickly to new global
challenges and to the changing business environment. These business
requirements then result in the failure of many running projects. Therefore
there is a need to enhance the current project management environment so that
it is more agile and adoptive to changes. The objective of this paper is to
create a conceptual framework that aggregates principles and processes from
both frameworks (PRINCE2 and Scrum) with emphasis on their use in web
development projects. This paper will discuss the advantages and disadvantages
of using the two abovementioned frameworks. Different groups of readers can
benefit from the results of this paper. It will help corporate management to
decide how a company should set up its own specific framework for managing
agile product development projects. Project managers will have a better
understanding of agile development principles and how it fits in the classic
project management framework. Last but not least, it will help product
developers to work in more agile ways and survive in the controlled and complex
project environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1502.04300</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1502.04300</id><created>2015-02-15</created><authors><author><keyname>Wang</keyname><forenames>Ju-Chiang</forenames></author><author><keyname>Gu</keyname><forenames>Hung-Yan</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author></authors><title>Mandarin Singing Voice Synthesis Based on Harmonic Plus Noise Model and
  Singing Expression Analysis</title><categories>cs.SD</categories><comments>8 pages, technical report</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The purpose of this study is to investigate how humans interpret musical
scores expressively, and then design machines that sing like humans. We
consider six factors that have a strong influence on the expression of human
singing. The factors are related to the acoustic, phonetic, and musical
features of a real singing signal. Given real singing voices recorded following
the MIDI scores and lyrics, our analysis module can extract the expression
parameters from the real singing signals semi-automatically. The expression
parameters are used to control the singing voice synthesis (SVS) system for
Mandarin Chinese, which is based on the harmonic plus noise model (HNM). The
results of perceptual experiments show that integrating the expression factors
into the SVS system yields a notable improvement in perceptual naturalness,
clearness, and expressiveness. By one-to-one mapping of the real singing signal
and expression controls to the synthesizer, our SVS system can simulate the
interpretation of a real singer with the timbre of a speaker.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="72000" completeListSize="102538">1122234|73001</resumptionToken>
</ListRecords>
</OAI-PMH>
